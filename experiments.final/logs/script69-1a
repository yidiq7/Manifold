+ RUN=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='-2 -1'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output69
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0
+ date
Mon Oct 26 09:07:24 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7370ad1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7370ad8ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73780fcb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73780fce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7370aa9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7370aa9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7370aa9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7370aa9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7370aa9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7370aa92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7370aa9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f737097ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73709331e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7370933c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73709338c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73708edf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73708edb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72fa40a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73708edea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72fa3c5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72fa3c5ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72fa3c5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72fa3a7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d441dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d441df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d441a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d43f2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d437ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d4393400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d43499d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d4319ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d430af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d430a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d42ec620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d427aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72d430aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.96752623e-05
Iter: 2 loss: 2.75545099e-05
Iter: 3 loss: 2.61911737e-05
Iter: 4 loss: 2.54467595e-05
Iter: 5 loss: 2.65683666e-05
Iter: 6 loss: 2.50904741e-05
Iter: 7 loss: 2.45499141e-05
Iter: 8 loss: 2.68027215e-05
Iter: 9 loss: 2.44342627e-05
Iter: 10 loss: 2.38967696e-05
Iter: 11 loss: 2.54767674e-05
Iter: 12 loss: 2.37307595e-05
Iter: 13 loss: 2.34304862e-05
Iter: 14 loss: 2.31247795e-05
Iter: 15 loss: 2.30658079e-05
Iter: 16 loss: 2.28228055e-05
Iter: 17 loss: 2.28208592e-05
Iter: 18 loss: 2.24950363e-05
Iter: 19 loss: 2.17658198e-05
Iter: 20 loss: 3.17420308e-05
Iter: 21 loss: 2.17255765e-05
Iter: 22 loss: 2.11885417e-05
Iter: 23 loss: 2.23806201e-05
Iter: 24 loss: 2.0983658e-05
Iter: 25 loss: 2.08801448e-05
Iter: 26 loss: 2.08258371e-05
Iter: 27 loss: 2.06512523e-05
Iter: 28 loss: 2.06044388e-05
Iter: 29 loss: 2.04965545e-05
Iter: 30 loss: 2.03164782e-05
Iter: 31 loss: 1.99849892e-05
Iter: 32 loss: 2.77037252e-05
Iter: 33 loss: 1.99846108e-05
Iter: 34 loss: 1.97474328e-05
Iter: 35 loss: 2.31979593e-05
Iter: 36 loss: 1.97471891e-05
Iter: 37 loss: 1.94444292e-05
Iter: 38 loss: 1.93508349e-05
Iter: 39 loss: 1.91722647e-05
Iter: 40 loss: 1.88627673e-05
Iter: 41 loss: 2.14829051e-05
Iter: 42 loss: 1.88450504e-05
Iter: 43 loss: 1.86347406e-05
Iter: 44 loss: 1.91756189e-05
Iter: 45 loss: 1.85624904e-05
Iter: 46 loss: 1.84239252e-05
Iter: 47 loss: 1.93151791e-05
Iter: 48 loss: 1.8408904e-05
Iter: 49 loss: 1.83247848e-05
Iter: 50 loss: 1.81825617e-05
Iter: 51 loss: 1.8182267e-05
Iter: 52 loss: 1.81233263e-05
Iter: 53 loss: 1.81004543e-05
Iter: 54 loss: 1.80098832e-05
Iter: 55 loss: 1.77884922e-05
Iter: 56 loss: 2.00571485e-05
Iter: 57 loss: 1.77620859e-05
Iter: 58 loss: 1.7538172e-05
Iter: 59 loss: 1.76035282e-05
Iter: 60 loss: 1.73773042e-05
Iter: 61 loss: 1.71850406e-05
Iter: 62 loss: 1.93396809e-05
Iter: 63 loss: 1.7181439e-05
Iter: 64 loss: 1.71969223e-05
Iter: 65 loss: 1.71262727e-05
Iter: 66 loss: 1.71013744e-05
Iter: 67 loss: 1.7041737e-05
Iter: 68 loss: 1.76975918e-05
Iter: 69 loss: 1.70356798e-05
Iter: 70 loss: 1.69798368e-05
Iter: 71 loss: 1.74808883e-05
Iter: 72 loss: 1.69772175e-05
Iter: 73 loss: 1.69021259e-05
Iter: 74 loss: 1.68409897e-05
Iter: 75 loss: 1.68187144e-05
Iter: 76 loss: 1.67342841e-05
Iter: 77 loss: 1.80069692e-05
Iter: 78 loss: 1.67341786e-05
Iter: 79 loss: 1.66763239e-05
Iter: 80 loss: 1.65687597e-05
Iter: 81 loss: 1.90008741e-05
Iter: 82 loss: 1.65684833e-05
Iter: 83 loss: 1.64800986e-05
Iter: 84 loss: 1.70622661e-05
Iter: 85 loss: 1.64708472e-05
Iter: 86 loss: 1.6383623e-05
Iter: 87 loss: 1.64715711e-05
Iter: 88 loss: 1.63346249e-05
Iter: 89 loss: 1.62898141e-05
Iter: 90 loss: 1.62898068e-05
Iter: 91 loss: 1.62373053e-05
Iter: 92 loss: 1.61875141e-05
Iter: 93 loss: 1.61755815e-05
Iter: 94 loss: 1.61059834e-05
Iter: 95 loss: 1.59805641e-05
Iter: 96 loss: 1.90240316e-05
Iter: 97 loss: 1.59806077e-05
Iter: 98 loss: 1.58374496e-05
Iter: 99 loss: 1.67566e-05
Iter: 100 loss: 1.58217626e-05
Iter: 101 loss: 1.58285038e-05
Iter: 102 loss: 1.57721115e-05
Iter: 103 loss: 1.57459144e-05
Iter: 104 loss: 1.56737515e-05
Iter: 105 loss: 1.60960026e-05
Iter: 106 loss: 1.56533697e-05
Iter: 107 loss: 1.564993e-05
Iter: 108 loss: 1.56166861e-05
Iter: 109 loss: 1.55906637e-05
Iter: 110 loss: 1.55840316e-05
Iter: 111 loss: 1.5567728e-05
Iter: 112 loss: 1.55305679e-05
Iter: 113 loss: 1.58041657e-05
Iter: 114 loss: 1.55275975e-05
Iter: 115 loss: 1.5498732e-05
Iter: 116 loss: 1.54102945e-05
Iter: 117 loss: 1.56269962e-05
Iter: 118 loss: 1.53598885e-05
Iter: 119 loss: 1.52742577e-05
Iter: 120 loss: 1.52742541e-05
Iter: 121 loss: 1.52115381e-05
Iter: 122 loss: 1.56985298e-05
Iter: 123 loss: 1.52070697e-05
Iter: 124 loss: 1.51687927e-05
Iter: 125 loss: 1.51952354e-05
Iter: 126 loss: 1.51448412e-05
Iter: 127 loss: 1.50871392e-05
Iter: 128 loss: 1.55888338e-05
Iter: 129 loss: 1.50842825e-05
Iter: 130 loss: 1.50479782e-05
Iter: 131 loss: 1.49592106e-05
Iter: 132 loss: 1.5863292e-05
Iter: 133 loss: 1.49485995e-05
Iter: 134 loss: 1.48550043e-05
Iter: 135 loss: 1.50144951e-05
Iter: 136 loss: 1.48131203e-05
Iter: 137 loss: 1.48246554e-05
Iter: 138 loss: 1.47704723e-05
Iter: 139 loss: 1.47380033e-05
Iter: 140 loss: 1.4690705e-05
Iter: 141 loss: 1.46894326e-05
Iter: 142 loss: 1.4662879e-05
Iter: 143 loss: 1.49342122e-05
Iter: 144 loss: 1.46621896e-05
Iter: 145 loss: 1.46293314e-05
Iter: 146 loss: 1.46292514e-05
Iter: 147 loss: 1.46029397e-05
Iter: 148 loss: 1.45707199e-05
Iter: 149 loss: 1.49430034e-05
Iter: 150 loss: 1.45701761e-05
Iter: 151 loss: 1.45389686e-05
Iter: 152 loss: 1.44830847e-05
Iter: 153 loss: 1.5843445e-05
Iter: 154 loss: 1.44830865e-05
Iter: 155 loss: 1.44356727e-05
Iter: 156 loss: 1.44972764e-05
Iter: 157 loss: 1.44115729e-05
Iter: 158 loss: 1.43625239e-05
Iter: 159 loss: 1.48621039e-05
Iter: 160 loss: 1.43609795e-05
Iter: 161 loss: 1.43327261e-05
Iter: 162 loss: 1.44007709e-05
Iter: 163 loss: 1.43226298e-05
Iter: 164 loss: 1.42839954e-05
Iter: 165 loss: 1.445403e-05
Iter: 166 loss: 1.42762192e-05
Iter: 167 loss: 1.42559256e-05
Iter: 168 loss: 1.4206702e-05
Iter: 169 loss: 1.47313731e-05
Iter: 170 loss: 1.42013168e-05
Iter: 171 loss: 1.41255587e-05
Iter: 172 loss: 1.4111668e-05
Iter: 173 loss: 1.40608918e-05
Iter: 174 loss: 1.40846314e-05
Iter: 175 loss: 1.40308148e-05
Iter: 176 loss: 1.39991162e-05
Iter: 177 loss: 1.40051825e-05
Iter: 178 loss: 1.39755e-05
Iter: 179 loss: 1.3947918e-05
Iter: 180 loss: 1.39325039e-05
Iter: 181 loss: 1.39203639e-05
Iter: 182 loss: 1.39144122e-05
Iter: 183 loss: 1.39015528e-05
Iter: 184 loss: 1.38887499e-05
Iter: 185 loss: 1.38613113e-05
Iter: 186 loss: 1.42838762e-05
Iter: 187 loss: 1.38601772e-05
Iter: 188 loss: 1.38187479e-05
Iter: 189 loss: 1.41074215e-05
Iter: 190 loss: 1.38148043e-05
Iter: 191 loss: 1.3787665e-05
Iter: 192 loss: 1.37353218e-05
Iter: 193 loss: 1.48166364e-05
Iter: 194 loss: 1.37350034e-05
Iter: 195 loss: 1.36811304e-05
Iter: 196 loss: 1.42584049e-05
Iter: 197 loss: 1.36798772e-05
Iter: 198 loss: 1.36495164e-05
Iter: 199 loss: 1.38117648e-05
Iter: 200 loss: 1.36450262e-05
Iter: 201 loss: 1.36149729e-05
Iter: 202 loss: 1.3734335e-05
Iter: 203 loss: 1.36082381e-05
Iter: 204 loss: 1.35953287e-05
Iter: 205 loss: 1.35597975e-05
Iter: 206 loss: 1.37612742e-05
Iter: 207 loss: 1.35494147e-05
Iter: 208 loss: 1.35117634e-05
Iter: 209 loss: 1.3881503e-05
Iter: 210 loss: 1.35104674e-05
Iter: 211 loss: 1.3481e-05
Iter: 212 loss: 1.3714789e-05
Iter: 213 loss: 1.34788697e-05
Iter: 214 loss: 1.34536085e-05
Iter: 215 loss: 1.34024658e-05
Iter: 216 loss: 1.4342153e-05
Iter: 217 loss: 1.34016409e-05
Iter: 218 loss: 1.33745953e-05
Iter: 219 loss: 1.33729209e-05
Iter: 220 loss: 1.33459816e-05
Iter: 221 loss: 1.34204893e-05
Iter: 222 loss: 1.33373596e-05
Iter: 223 loss: 1.33234962e-05
Iter: 224 loss: 1.34545644e-05
Iter: 225 loss: 1.33229805e-05
Iter: 226 loss: 1.33084923e-05
Iter: 227 loss: 1.32757577e-05
Iter: 228 loss: 1.37063516e-05
Iter: 229 loss: 1.32735786e-05
Iter: 230 loss: 1.32409587e-05
Iter: 231 loss: 1.3397992e-05
Iter: 232 loss: 1.32352634e-05
Iter: 233 loss: 1.32017976e-05
Iter: 234 loss: 1.32879231e-05
Iter: 235 loss: 1.31902816e-05
Iter: 236 loss: 1.31672959e-05
Iter: 237 loss: 1.31671932e-05
Iter: 238 loss: 1.31514453e-05
Iter: 239 loss: 1.31162033e-05
Iter: 240 loss: 1.36143462e-05
Iter: 241 loss: 1.31143861e-05
Iter: 242 loss: 1.30825229e-05
Iter: 243 loss: 1.31507786e-05
Iter: 244 loss: 1.30700828e-05
Iter: 245 loss: 1.30551907e-05
Iter: 246 loss: 1.3050796e-05
Iter: 247 loss: 1.30342632e-05
Iter: 248 loss: 1.29993223e-05
Iter: 249 loss: 1.35691944e-05
Iter: 250 loss: 1.29981536e-05
Iter: 251 loss: 1.29615501e-05
Iter: 252 loss: 1.31021898e-05
Iter: 253 loss: 1.29530017e-05
Iter: 254 loss: 1.29270384e-05
Iter: 255 loss: 1.29261407e-05
Iter: 256 loss: 1.2914601e-05
Iter: 257 loss: 1.29207438e-05
Iter: 258 loss: 1.29069658e-05
Iter: 259 loss: 1.28880083e-05
Iter: 260 loss: 1.29279624e-05
Iter: 261 loss: 1.2880384e-05
Iter: 262 loss: 1.28663714e-05
Iter: 263 loss: 1.28417187e-05
Iter: 264 loss: 1.28416377e-05
Iter: 265 loss: 1.28100719e-05
Iter: 266 loss: 1.30775979e-05
Iter: 267 loss: 1.28082484e-05
Iter: 268 loss: 1.27867697e-05
Iter: 269 loss: 1.29067248e-05
Iter: 270 loss: 1.27836383e-05
Iter: 271 loss: 1.27571366e-05
Iter: 272 loss: 1.2727869e-05
Iter: 273 loss: 1.27237463e-05
Iter: 274 loss: 1.26900795e-05
Iter: 275 loss: 1.26869209e-05
Iter: 276 loss: 1.26620416e-05
Iter: 277 loss: 1.26496052e-05
Iter: 278 loss: 1.26426057e-05
Iter: 279 loss: 1.26242212e-05
Iter: 280 loss: 1.26308751e-05
Iter: 281 loss: 1.26113409e-05
Iter: 282 loss: 1.25956203e-05
Iter: 283 loss: 1.25789e-05
Iter: 284 loss: 1.25761835e-05
Iter: 285 loss: 1.25576908e-05
Iter: 286 loss: 1.25562701e-05
Iter: 287 loss: 1.25397301e-05
Iter: 288 loss: 1.25260103e-05
Iter: 289 loss: 1.25210718e-05
Iter: 290 loss: 1.24951976e-05
Iter: 291 loss: 1.27477379e-05
Iter: 292 loss: 1.24942389e-05
Iter: 293 loss: 1.24809858e-05
Iter: 294 loss: 1.24595354e-05
Iter: 295 loss: 1.24594571e-05
Iter: 296 loss: 1.24373164e-05
Iter: 297 loss: 1.26448267e-05
Iter: 298 loss: 1.24363087e-05
Iter: 299 loss: 1.24219014e-05
Iter: 300 loss: 1.24983653e-05
Iter: 301 loss: 1.24195394e-05
Iter: 302 loss: 1.24032786e-05
Iter: 303 loss: 1.2415423e-05
Iter: 304 loss: 1.23933096e-05
Iter: 305 loss: 1.23783157e-05
Iter: 306 loss: 1.235336e-05
Iter: 307 loss: 1.23532564e-05
Iter: 308 loss: 1.23334212e-05
Iter: 309 loss: 1.23332347e-05
Iter: 310 loss: 1.23135651e-05
Iter: 311 loss: 1.23597674e-05
Iter: 312 loss: 1.23062127e-05
Iter: 313 loss: 1.22919519e-05
Iter: 314 loss: 1.22820429e-05
Iter: 315 loss: 1.22768979e-05
Iter: 316 loss: 1.22687125e-05
Iter: 317 loss: 1.22667243e-05
Iter: 318 loss: 1.22564834e-05
Iter: 319 loss: 1.22434412e-05
Iter: 320 loss: 1.22424481e-05
Iter: 321 loss: 1.22254569e-05
Iter: 322 loss: 1.24192056e-05
Iter: 323 loss: 1.22251768e-05
Iter: 324 loss: 1.22136025e-05
Iter: 325 loss: 1.21972225e-05
Iter: 326 loss: 1.21965641e-05
Iter: 327 loss: 1.21790572e-05
Iter: 328 loss: 1.22702841e-05
Iter: 329 loss: 1.21761559e-05
Iter: 330 loss: 1.21609628e-05
Iter: 331 loss: 1.22084839e-05
Iter: 332 loss: 1.21565026e-05
Iter: 333 loss: 1.21400781e-05
Iter: 334 loss: 1.22371475e-05
Iter: 335 loss: 1.21379826e-05
Iter: 336 loss: 1.21284329e-05
Iter: 337 loss: 1.21083094e-05
Iter: 338 loss: 1.24384833e-05
Iter: 339 loss: 1.21076137e-05
Iter: 340 loss: 1.20889626e-05
Iter: 341 loss: 1.22563224e-05
Iter: 342 loss: 1.20880177e-05
Iter: 343 loss: 1.20708337e-05
Iter: 344 loss: 1.21856119e-05
Iter: 345 loss: 1.20689774e-05
Iter: 346 loss: 1.20577033e-05
Iter: 347 loss: 1.20361065e-05
Iter: 348 loss: 1.24967155e-05
Iter: 349 loss: 1.20360392e-05
Iter: 350 loss: 1.20248178e-05
Iter: 351 loss: 1.20234417e-05
Iter: 352 loss: 1.20114191e-05
Iter: 353 loss: 1.203378e-05
Iter: 354 loss: 1.20062568e-05
Iter: 355 loss: 1.19983197e-05
Iter: 356 loss: 1.20784462e-05
Iter: 357 loss: 1.19980523e-05
Iter: 358 loss: 1.19912074e-05
Iter: 359 loss: 1.19756369e-05
Iter: 360 loss: 1.21860521e-05
Iter: 361 loss: 1.19748083e-05
Iter: 362 loss: 1.19564502e-05
Iter: 363 loss: 1.20503064e-05
Iter: 364 loss: 1.19535071e-05
Iter: 365 loss: 1.19380265e-05
Iter: 366 loss: 1.19825218e-05
Iter: 367 loss: 1.19332144e-05
Iter: 368 loss: 1.19171546e-05
Iter: 369 loss: 1.20349105e-05
Iter: 370 loss: 1.19158121e-05
Iter: 371 loss: 1.19042888e-05
Iter: 372 loss: 1.18809694e-05
Iter: 373 loss: 1.23072987e-05
Iter: 374 loss: 1.18805929e-05
Iter: 375 loss: 1.18644602e-05
Iter: 376 loss: 1.20386558e-05
Iter: 377 loss: 1.18640037e-05
Iter: 378 loss: 1.18517528e-05
Iter: 379 loss: 1.19796705e-05
Iter: 380 loss: 1.1851449e-05
Iter: 381 loss: 1.18431308e-05
Iter: 382 loss: 1.18283133e-05
Iter: 383 loss: 1.18283297e-05
Iter: 384 loss: 1.1815554e-05
Iter: 385 loss: 1.19563438e-05
Iter: 386 loss: 1.18153612e-05
Iter: 387 loss: 1.1801716e-05
Iter: 388 loss: 1.18330718e-05
Iter: 389 loss: 1.1796511e-05
Iter: 390 loss: 1.17867103e-05
Iter: 391 loss: 1.18611133e-05
Iter: 392 loss: 1.17859381e-05
Iter: 393 loss: 1.17767049e-05
Iter: 394 loss: 1.17696391e-05
Iter: 395 loss: 1.17667278e-05
Iter: 396 loss: 1.17563177e-05
Iter: 397 loss: 1.17906202e-05
Iter: 398 loss: 1.17533264e-05
Iter: 399 loss: 1.17421823e-05
Iter: 400 loss: 1.17587806e-05
Iter: 401 loss: 1.17367526e-05
Iter: 402 loss: 1.17250274e-05
Iter: 403 loss: 1.18600956e-05
Iter: 404 loss: 1.17248728e-05
Iter: 405 loss: 1.17169457e-05
Iter: 406 loss: 1.17015743e-05
Iter: 407 loss: 1.20126579e-05
Iter: 408 loss: 1.17013296e-05
Iter: 409 loss: 1.16873834e-05
Iter: 410 loss: 1.17494801e-05
Iter: 411 loss: 1.16845968e-05
Iter: 412 loss: 1.16754045e-05
Iter: 413 loss: 1.16751844e-05
Iter: 414 loss: 1.166927e-05
Iter: 415 loss: 1.16580868e-05
Iter: 416 loss: 1.19105425e-05
Iter: 417 loss: 1.16580895e-05
Iter: 418 loss: 1.16474121e-05
Iter: 419 loss: 1.1719173e-05
Iter: 420 loss: 1.16462852e-05
Iter: 421 loss: 1.16350875e-05
Iter: 422 loss: 1.16987758e-05
Iter: 423 loss: 1.1633434e-05
Iter: 424 loss: 1.16265855e-05
Iter: 425 loss: 1.16545843e-05
Iter: 426 loss: 1.16250121e-05
Iter: 427 loss: 1.16171195e-05
Iter: 428 loss: 1.16134806e-05
Iter: 429 loss: 1.16095571e-05
Iter: 430 loss: 1.16009014e-05
Iter: 431 loss: 1.16264309e-05
Iter: 432 loss: 1.15983366e-05
Iter: 433 loss: 1.15896182e-05
Iter: 434 loss: 1.16084884e-05
Iter: 435 loss: 1.15862877e-05
Iter: 436 loss: 1.15787943e-05
Iter: 437 loss: 1.16765887e-05
Iter: 438 loss: 1.15787025e-05
Iter: 439 loss: 1.1572869e-05
Iter: 440 loss: 1.15618323e-05
Iter: 441 loss: 1.18058497e-05
Iter: 442 loss: 1.1561784e-05
Iter: 443 loss: 1.15513667e-05
Iter: 444 loss: 1.15793719e-05
Iter: 445 loss: 1.15478706e-05
Iter: 446 loss: 1.15408611e-05
Iter: 447 loss: 1.15405273e-05
Iter: 448 loss: 1.15348957e-05
Iter: 449 loss: 1.15243474e-05
Iter: 450 loss: 1.17587624e-05
Iter: 451 loss: 1.15243693e-05
Iter: 452 loss: 1.15143139e-05
Iter: 453 loss: 1.15760495e-05
Iter: 454 loss: 1.15131661e-05
Iter: 455 loss: 1.15048342e-05
Iter: 456 loss: 1.16037827e-05
Iter: 457 loss: 1.15046696e-05
Iter: 458 loss: 1.1500626e-05
Iter: 459 loss: 1.15096727e-05
Iter: 460 loss: 1.14990744e-05
Iter: 461 loss: 1.14935465e-05
Iter: 462 loss: 1.1493561e-05
Iter: 463 loss: 1.14891345e-05
Iter: 464 loss: 1.14827517e-05
Iter: 465 loss: 1.14850245e-05
Iter: 466 loss: 1.14781396e-05
Iter: 467 loss: 1.1468449e-05
Iter: 468 loss: 1.14957e-05
Iter: 469 loss: 1.14654158e-05
Iter: 470 loss: 1.14586946e-05
Iter: 471 loss: 1.14586765e-05
Iter: 472 loss: 1.14535469e-05
Iter: 473 loss: 1.14439936e-05
Iter: 474 loss: 1.16660867e-05
Iter: 475 loss: 1.14439572e-05
Iter: 476 loss: 1.14342474e-05
Iter: 477 loss: 1.14511895e-05
Iter: 478 loss: 1.14301138e-05
Iter: 479 loss: 1.14250251e-05
Iter: 480 loss: 1.14239265e-05
Iter: 481 loss: 1.14192753e-05
Iter: 482 loss: 1.14104196e-05
Iter: 483 loss: 1.1600323e-05
Iter: 484 loss: 1.14104014e-05
Iter: 485 loss: 1.14018785e-05
Iter: 486 loss: 1.14468276e-05
Iter: 487 loss: 1.1400507e-05
Iter: 488 loss: 1.13938804e-05
Iter: 489 loss: 1.14895211e-05
Iter: 490 loss: 1.13937986e-05
Iter: 491 loss: 1.13898168e-05
Iter: 492 loss: 1.1393111e-05
Iter: 493 loss: 1.13876176e-05
Iter: 494 loss: 1.13812866e-05
Iter: 495 loss: 1.13855103e-05
Iter: 496 loss: 1.1377324e-05
Iter: 497 loss: 1.13709293e-05
Iter: 498 loss: 1.1371867e-05
Iter: 499 loss: 1.13661026e-05
Iter: 500 loss: 1.13564984e-05
Iter: 501 loss: 1.13893193e-05
Iter: 502 loss: 1.13538863e-05
Iter: 503 loss: 1.13466713e-05
Iter: 504 loss: 1.14269069e-05
Iter: 505 loss: 1.13465594e-05
Iter: 506 loss: 1.13398928e-05
Iter: 507 loss: 1.13287379e-05
Iter: 508 loss: 1.13286842e-05
Iter: 509 loss: 1.13185779e-05
Iter: 510 loss: 1.13529059e-05
Iter: 511 loss: 1.1315884e-05
Iter: 512 loss: 1.13117103e-05
Iter: 513 loss: 1.131099e-05
Iter: 514 loss: 1.13070137e-05
Iter: 515 loss: 1.12990774e-05
Iter: 516 loss: 1.14615377e-05
Iter: 517 loss: 1.12990438e-05
Iter: 518 loss: 1.1291032e-05
Iter: 519 loss: 1.13201586e-05
Iter: 520 loss: 1.12889902e-05
Iter: 521 loss: 1.12820017e-05
Iter: 522 loss: 1.13870801e-05
Iter: 523 loss: 1.12820071e-05
Iter: 524 loss: 1.12775879e-05
Iter: 525 loss: 1.1278933e-05
Iter: 526 loss: 1.12744265e-05
Iter: 527 loss: 1.12668458e-05
Iter: 528 loss: 1.1277607e-05
Iter: 529 loss: 1.12631369e-05
Iter: 530 loss: 1.12562211e-05
Iter: 531 loss: 1.12560374e-05
Iter: 532 loss: 1.12505786e-05
Iter: 533 loss: 1.12406724e-05
Iter: 534 loss: 1.12847283e-05
Iter: 535 loss: 1.12387825e-05
Iter: 536 loss: 1.12321068e-05
Iter: 537 loss: 1.13007027e-05
Iter: 538 loss: 1.12318903e-05
Iter: 539 loss: 1.12254775e-05
Iter: 540 loss: 1.12175585e-05
Iter: 541 loss: 1.12168782e-05
Iter: 542 loss: 1.120867e-05
Iter: 543 loss: 1.12215212e-05
Iter: 544 loss: 1.12046291e-05
Iter: 545 loss: 1.11998961e-05
Iter: 546 loss: 1.11993759e-05
Iter: 547 loss: 1.11946683e-05
Iter: 548 loss: 1.1186934e-05
Iter: 549 loss: 1.11870067e-05
Iter: 550 loss: 1.11791287e-05
Iter: 551 loss: 1.12028656e-05
Iter: 552 loss: 1.11767458e-05
Iter: 553 loss: 1.11696509e-05
Iter: 554 loss: 1.12821317e-05
Iter: 555 loss: 1.11696645e-05
Iter: 556 loss: 1.11648615e-05
Iter: 557 loss: 1.11653271e-05
Iter: 558 loss: 1.11610025e-05
Iter: 559 loss: 1.11529243e-05
Iter: 560 loss: 1.1174483e-05
Iter: 561 loss: 1.11502723e-05
Iter: 562 loss: 1.11442987e-05
Iter: 563 loss: 1.11423242e-05
Iter: 564 loss: 1.11387908e-05
Iter: 565 loss: 1.11299241e-05
Iter: 566 loss: 1.11714526e-05
Iter: 567 loss: 1.11283325e-05
Iter: 568 loss: 1.11222198e-05
Iter: 569 loss: 1.11776426e-05
Iter: 570 loss: 1.11219833e-05
Iter: 571 loss: 1.11160462e-05
Iter: 572 loss: 1.11110567e-05
Iter: 573 loss: 1.11093505e-05
Iter: 574 loss: 1.11024501e-05
Iter: 575 loss: 1.11061609e-05
Iter: 576 loss: 1.10979017e-05
Iter: 577 loss: 1.10921137e-05
Iter: 578 loss: 1.10919191e-05
Iter: 579 loss: 1.10861729e-05
Iter: 580 loss: 1.10794172e-05
Iter: 581 loss: 1.10786459e-05
Iter: 582 loss: 1.10709043e-05
Iter: 583 loss: 1.10843266e-05
Iter: 584 loss: 1.10674082e-05
Iter: 585 loss: 1.10605934e-05
Iter: 586 loss: 1.10604387e-05
Iter: 587 loss: 1.10553701e-05
Iter: 588 loss: 1.10572155e-05
Iter: 589 loss: 1.10520095e-05
Iter: 590 loss: 1.104476e-05
Iter: 591 loss: 1.10695946e-05
Iter: 592 loss: 1.10428427e-05
Iter: 593 loss: 1.10374676e-05
Iter: 594 loss: 1.10327555e-05
Iter: 595 loss: 1.10314977e-05
Iter: 596 loss: 1.10222863e-05
Iter: 597 loss: 1.10666642e-05
Iter: 598 loss: 1.10206574e-05
Iter: 599 loss: 1.10141555e-05
Iter: 600 loss: 1.10656465e-05
Iter: 601 loss: 1.10136652e-05
Iter: 602 loss: 1.10069832e-05
Iter: 603 loss: 1.10039418e-05
Iter: 604 loss: 1.10006586e-05
Iter: 605 loss: 1.09932171e-05
Iter: 606 loss: 1.09894954e-05
Iter: 607 loss: 1.09860684e-05
Iter: 608 loss: 1.09794819e-05
Iter: 609 loss: 1.09789544e-05
Iter: 610 loss: 1.09722641e-05
Iter: 611 loss: 1.09679149e-05
Iter: 612 loss: 1.09653929e-05
Iter: 613 loss: 1.09577177e-05
Iter: 614 loss: 1.0959724e-05
Iter: 615 loss: 1.09522007e-05
Iter: 616 loss: 1.09446182e-05
Iter: 617 loss: 1.09443336e-05
Iter: 618 loss: 1.093843e-05
Iter: 619 loss: 1.09409284e-05
Iter: 620 loss: 1.09344392e-05
Iter: 621 loss: 1.09263565e-05
Iter: 622 loss: 1.09586417e-05
Iter: 623 loss: 1.09245757e-05
Iter: 624 loss: 1.09187858e-05
Iter: 625 loss: 1.09122357e-05
Iter: 626 loss: 1.09114135e-05
Iter: 627 loss: 1.0900123e-05
Iter: 628 loss: 1.09462653e-05
Iter: 629 loss: 1.08975646e-05
Iter: 630 loss: 1.08884406e-05
Iter: 631 loss: 1.09499797e-05
Iter: 632 loss: 1.08874738e-05
Iter: 633 loss: 1.08774702e-05
Iter: 634 loss: 1.08756922e-05
Iter: 635 loss: 1.08688037e-05
Iter: 636 loss: 1.08583063e-05
Iter: 637 loss: 1.085027e-05
Iter: 638 loss: 1.08469139e-05
Iter: 639 loss: 1.08378372e-05
Iter: 640 loss: 1.08368176e-05
Iter: 641 loss: 1.08271688e-05
Iter: 642 loss: 1.08248496e-05
Iter: 643 loss: 1.08186696e-05
Iter: 644 loss: 1.08081467e-05
Iter: 645 loss: 1.08020049e-05
Iter: 646 loss: 1.07975611e-05
Iter: 647 loss: 1.07880487e-05
Iter: 648 loss: 1.07868345e-05
Iter: 649 loss: 1.07778505e-05
Iter: 650 loss: 1.07791147e-05
Iter: 651 loss: 1.0771194e-05
Iter: 652 loss: 1.07582382e-05
Iter: 653 loss: 1.08092609e-05
Iter: 654 loss: 1.07552323e-05
Iter: 655 loss: 1.07452888e-05
Iter: 656 loss: 1.07345386e-05
Iter: 657 loss: 1.07329215e-05
Iter: 658 loss: 1.07148862e-05
Iter: 659 loss: 1.0795e-05
Iter: 660 loss: 1.07113137e-05
Iter: 661 loss: 1.06971311e-05
Iter: 662 loss: 1.0781846e-05
Iter: 663 loss: 1.06952175e-05
Iter: 664 loss: 1.06792704e-05
Iter: 665 loss: 1.06853504e-05
Iter: 666 loss: 1.06680091e-05
Iter: 667 loss: 1.0653238e-05
Iter: 668 loss: 1.06364168e-05
Iter: 669 loss: 1.06344178e-05
Iter: 670 loss: 1.06201942e-05
Iter: 671 loss: 1.06191628e-05
Iter: 672 loss: 1.06041489e-05
Iter: 673 loss: 1.06079042e-05
Iter: 674 loss: 1.05930321e-05
Iter: 675 loss: 1.05779063e-05
Iter: 676 loss: 1.0566494e-05
Iter: 677 loss: 1.05613963e-05
Iter: 678 loss: 1.05489989e-05
Iter: 679 loss: 1.05471699e-05
Iter: 680 loss: 1.0533915e-05
Iter: 681 loss: 1.05313675e-05
Iter: 682 loss: 1.0522449e-05
Iter: 683 loss: 1.05027348e-05
Iter: 684 loss: 1.06061389e-05
Iter: 685 loss: 1.04996807e-05
Iter: 686 loss: 1.04859937e-05
Iter: 687 loss: 1.04749261e-05
Iter: 688 loss: 1.04708579e-05
Iter: 689 loss: 1.04490027e-05
Iter: 690 loss: 1.05314666e-05
Iter: 691 loss: 1.04436649e-05
Iter: 692 loss: 1.04263872e-05
Iter: 693 loss: 1.05568151e-05
Iter: 694 loss: 1.04251658e-05
Iter: 695 loss: 1.04071069e-05
Iter: 696 loss: 1.0416441e-05
Iter: 697 loss: 1.03950715e-05
Iter: 698 loss: 1.03801558e-05
Iter: 699 loss: 1.03638231e-05
Iter: 700 loss: 1.03613493e-05
Iter: 701 loss: 1.03460789e-05
Iter: 702 loss: 1.0345665e-05
Iter: 703 loss: 1.03294005e-05
Iter: 704 loss: 1.0340952e-05
Iter: 705 loss: 1.03193324e-05
Iter: 706 loss: 1.03036609e-05
Iter: 707 loss: 1.02971426e-05
Iter: 708 loss: 1.02889089e-05
Iter: 709 loss: 1.0278286e-05
Iter: 710 loss: 1.02768754e-05
Iter: 711 loss: 1.02648792e-05
Iter: 712 loss: 1.02577524e-05
Iter: 713 loss: 1.02528529e-05
Iter: 714 loss: 1.02341382e-05
Iter: 715 loss: 1.03612238e-05
Iter: 716 loss: 1.02323429e-05
Iter: 717 loss: 1.0219841e-05
Iter: 718 loss: 1.02190097e-05
Iter: 719 loss: 1.02095464e-05
Iter: 720 loss: 1.01927235e-05
Iter: 721 loss: 1.02151826e-05
Iter: 722 loss: 1.01842916e-05
Iter: 723 loss: 1.01685437e-05
Iter: 724 loss: 1.02860395e-05
Iter: 725 loss: 1.01673377e-05
Iter: 726 loss: 1.01504684e-05
Iter: 727 loss: 1.01778633e-05
Iter: 728 loss: 1.01427786e-05
Iter: 729 loss: 1.01314054e-05
Iter: 730 loss: 1.01178484e-05
Iter: 731 loss: 1.01166133e-05
Iter: 732 loss: 1.01045107e-05
Iter: 733 loss: 1.01043388e-05
Iter: 734 loss: 1.00919406e-05
Iter: 735 loss: 1.01185433e-05
Iter: 736 loss: 1.00871675e-05
Iter: 737 loss: 1.00775214e-05
Iter: 738 loss: 1.00747111e-05
Iter: 739 loss: 1.00690504e-05
Iter: 740 loss: 1.0059397e-05
Iter: 741 loss: 1.01941005e-05
Iter: 742 loss: 1.0059377e-05
Iter: 743 loss: 1.00486759e-05
Iter: 744 loss: 1.00572488e-05
Iter: 745 loss: 1.00423804e-05
Iter: 746 loss: 1.00323487e-05
Iter: 747 loss: 1.01140949e-05
Iter: 748 loss: 1.00317739e-05
Iter: 749 loss: 1.00243724e-05
Iter: 750 loss: 1.00150073e-05
Iter: 751 loss: 1.00143006e-05
Iter: 752 loss: 9.99783697e-06
Iter: 753 loss: 1.003334e-05
Iter: 754 loss: 9.9913359e-06
Iter: 755 loss: 9.97929419e-06
Iter: 756 loss: 1.00579982e-05
Iter: 757 loss: 9.97810093e-06
Iter: 758 loss: 9.96488598e-06
Iter: 759 loss: 9.98804808e-06
Iter: 760 loss: 9.95907249e-06
Iter: 761 loss: 9.94876791e-06
Iter: 762 loss: 9.93771482e-06
Iter: 763 loss: 9.93594949e-06
Iter: 764 loss: 9.92501191e-06
Iter: 765 loss: 9.92497553e-06
Iter: 766 loss: 9.91460365e-06
Iter: 767 loss: 9.94361562e-06
Iter: 768 loss: 9.91120396e-06
Iter: 769 loss: 9.90285571e-06
Iter: 770 loss: 9.89732507e-06
Iter: 771 loss: 9.89403816e-06
Iter: 772 loss: 9.88372267e-06
Iter: 773 loss: 1.00060024e-05
Iter: 774 loss: 9.88360716e-06
Iter: 775 loss: 9.873198e-06
Iter: 776 loss: 9.89440196e-06
Iter: 777 loss: 9.86886698e-06
Iter: 778 loss: 9.86073792e-06
Iter: 779 loss: 9.90140506e-06
Iter: 780 loss: 9.85942643e-06
Iter: 781 loss: 9.85175393e-06
Iter: 782 loss: 9.85385213e-06
Iter: 783 loss: 9.84618691e-06
Iter: 784 loss: 9.83603422e-06
Iter: 785 loss: 9.85786755e-06
Iter: 786 loss: 9.83206155e-06
Iter: 787 loss: 9.82450638e-06
Iter: 788 loss: 9.85785937e-06
Iter: 789 loss: 9.82297479e-06
Iter: 790 loss: 9.81437643e-06
Iter: 791 loss: 9.85469069e-06
Iter: 792 loss: 9.81280482e-06
Iter: 793 loss: 9.80810637e-06
Iter: 794 loss: 9.79893048e-06
Iter: 795 loss: 9.97968e-06
Iter: 796 loss: 9.79877e-06
Iter: 797 loss: 9.78931166e-06
Iter: 798 loss: 9.913746e-06
Iter: 799 loss: 9.78926619e-06
Iter: 800 loss: 9.78063508e-06
Iter: 801 loss: 9.85746738e-06
Iter: 802 loss: 9.78015396e-06
Iter: 803 loss: 9.77574928e-06
Iter: 804 loss: 9.76654155e-06
Iter: 805 loss: 9.92092646e-06
Iter: 806 loss: 9.76635602e-06
Iter: 807 loss: 9.75719831e-06
Iter: 808 loss: 9.8760047e-06
Iter: 809 loss: 9.75712283e-06
Iter: 810 loss: 9.75005e-06
Iter: 811 loss: 9.83233895e-06
Iter: 812 loss: 9.74988416e-06
Iter: 813 loss: 9.74726208e-06
Iter: 814 loss: 9.75081093e-06
Iter: 815 loss: 9.74577506e-06
Iter: 816 loss: 9.74226532e-06
Iter: 817 loss: 9.74616705e-06
Iter: 818 loss: 9.74034083e-06
Iter: 819 loss: 9.73695205e-06
Iter: 820 loss: 9.74735303e-06
Iter: 821 loss: 9.7359507e-06
Iter: 822 loss: 9.73272836e-06
Iter: 823 loss: 9.73239094e-06
Iter: 824 loss: 9.73009628e-06
Iter: 825 loss: 9.72621092e-06
Iter: 826 loss: 9.72611269e-06
Iter: 827 loss: 9.72457383e-06
Iter: 828 loss: 9.72045382e-06
Iter: 829 loss: 9.756689e-06
Iter: 830 loss: 9.71987538e-06
Iter: 831 loss: 9.71385089e-06
Iter: 832 loss: 9.73002716e-06
Iter: 833 loss: 9.71178451e-06
Iter: 834 loss: 9.71268219e-06
Iter: 835 loss: 9.70970541e-06
Iter: 836 loss: 9.70834299e-06
Iter: 837 loss: 9.70501787e-06
Iter: 838 loss: 9.73967872e-06
Iter: 839 loss: 9.70457586e-06
Iter: 840 loss: 9.70179e-06
Iter: 841 loss: 9.7223874e-06
Iter: 842 loss: 9.70158453e-06
Iter: 843 loss: 9.6988424e-06
Iter: 844 loss: 9.72752696e-06
Iter: 845 loss: 9.69880057e-06
Iter: 846 loss: 9.69641223e-06
Iter: 847 loss: 9.69186476e-06
Iter: 848 loss: 9.79581364e-06
Iter: 849 loss: 9.69189e-06
Iter: 850 loss: 9.68504901e-06
Iter: 851 loss: 9.71745612e-06
Iter: 852 loss: 9.68371933e-06
Iter: 853 loss: 9.67788765e-06
Iter: 854 loss: 9.67638334e-06
Iter: 855 loss: 9.67262258e-06
Iter: 856 loss: 9.66309835e-06
Iter: 857 loss: 9.67031701e-06
Iter: 858 loss: 9.65727668e-06
Iter: 859 loss: 9.6560334e-06
Iter: 860 loss: 9.65329309e-06
Iter: 861 loss: 9.65016261e-06
Iter: 862 loss: 9.64285755e-06
Iter: 863 loss: 9.7290158e-06
Iter: 864 loss: 9.64220453e-06
Iter: 865 loss: 9.63374896e-06
Iter: 866 loss: 9.62915783e-06
Iter: 867 loss: 9.62528e-06
Iter: 868 loss: 9.61527167e-06
Iter: 869 loss: 9.61522164e-06
Iter: 870 loss: 9.60422403e-06
Iter: 871 loss: 9.62883132e-06
Iter: 872 loss: 9.59991849e-06
Iter: 873 loss: 9.59494537e-06
Iter: 874 loss: 9.59298359e-06
Iter: 875 loss: 9.59037e-06
Iter: 876 loss: 9.58403689e-06
Iter: 877 loss: 9.68227596e-06
Iter: 878 loss: 9.58404144e-06
Iter: 879 loss: 9.57834709e-06
Iter: 880 loss: 9.58568944e-06
Iter: 881 loss: 9.57546945e-06
Iter: 882 loss: 9.57119573e-06
Iter: 883 loss: 9.59756289e-06
Iter: 884 loss: 9.57072189e-06
Iter: 885 loss: 9.56667827e-06
Iter: 886 loss: 9.56218355e-06
Iter: 887 loss: 9.56156691e-06
Iter: 888 loss: 9.55366431e-06
Iter: 889 loss: 9.57398606e-06
Iter: 890 loss: 9.55105588e-06
Iter: 891 loss: 9.54582538e-06
Iter: 892 loss: 9.58330656e-06
Iter: 893 loss: 9.54536881e-06
Iter: 894 loss: 9.53887866e-06
Iter: 895 loss: 9.54036e-06
Iter: 896 loss: 9.53419658e-06
Iter: 897 loss: 9.52964092e-06
Iter: 898 loss: 9.52668779e-06
Iter: 899 loss: 9.52491428e-06
Iter: 900 loss: 9.51747e-06
Iter: 901 loss: 9.5260566e-06
Iter: 902 loss: 9.51351376e-06
Iter: 903 loss: 9.50765207e-06
Iter: 904 loss: 9.50687354e-06
Iter: 905 loss: 9.50299818e-06
Iter: 906 loss: 9.49498826e-06
Iter: 907 loss: 9.63025468e-06
Iter: 908 loss: 9.49475e-06
Iter: 909 loss: 9.48948582e-06
Iter: 910 loss: 9.48951856e-06
Iter: 911 loss: 9.48479646e-06
Iter: 912 loss: 9.50386675e-06
Iter: 913 loss: 9.48372508e-06
Iter: 914 loss: 9.48e-06
Iter: 915 loss: 9.48297566e-06
Iter: 916 loss: 9.47771696e-06
Iter: 917 loss: 9.47245826e-06
Iter: 918 loss: 9.48381603e-06
Iter: 919 loss: 9.47034459e-06
Iter: 920 loss: 9.4656607e-06
Iter: 921 loss: 9.46959699e-06
Iter: 922 loss: 9.4628258e-06
Iter: 923 loss: 9.45676766e-06
Iter: 924 loss: 9.45966076e-06
Iter: 925 loss: 9.45270313e-06
Iter: 926 loss: 9.44553e-06
Iter: 927 loss: 9.55639644e-06
Iter: 928 loss: 9.44556814e-06
Iter: 929 loss: 9.44130625e-06
Iter: 930 loss: 9.43102e-06
Iter: 931 loss: 9.53859308e-06
Iter: 932 loss: 9.42985571e-06
Iter: 933 loss: 9.41938742e-06
Iter: 934 loss: 9.44274e-06
Iter: 935 loss: 9.41542e-06
Iter: 936 loss: 9.41105827e-06
Iter: 937 loss: 9.41001235e-06
Iter: 938 loss: 9.40406608e-06
Iter: 939 loss: 9.40231439e-06
Iter: 940 loss: 9.398781e-06
Iter: 941 loss: 9.3936942e-06
Iter: 942 loss: 9.3986e-06
Iter: 943 loss: 9.39091933e-06
Iter: 944 loss: 9.38710673e-06
Iter: 945 loss: 9.38684389e-06
Iter: 946 loss: 9.38348603e-06
Iter: 947 loss: 9.38518679e-06
Iter: 948 loss: 9.38115e-06
Iter: 949 loss: 9.37727418e-06
Iter: 950 loss: 9.39366873e-06
Iter: 951 loss: 9.37661662e-06
Iter: 952 loss: 9.37276945e-06
Iter: 953 loss: 9.37293e-06
Iter: 954 loss: 9.36979541e-06
Iter: 955 loss: 9.36482138e-06
Iter: 956 loss: 9.37277855e-06
Iter: 957 loss: 9.36249307e-06
Iter: 958 loss: 9.35865137e-06
Iter: 959 loss: 9.39738857e-06
Iter: 960 loss: 9.3585877e-06
Iter: 961 loss: 9.3545741e-06
Iter: 962 loss: 9.35519711e-06
Iter: 963 loss: 9.35150456e-06
Iter: 964 loss: 9.3473991e-06
Iter: 965 loss: 9.34113359e-06
Iter: 966 loss: 9.34099535e-06
Iter: 967 loss: 9.3328e-06
Iter: 968 loss: 9.3448607e-06
Iter: 969 loss: 9.32878629e-06
Iter: 970 loss: 9.32886178e-06
Iter: 971 loss: 9.32446619e-06
Iter: 972 loss: 9.32154398e-06
Iter: 973 loss: 9.31532668e-06
Iter: 974 loss: 9.41226426e-06
Iter: 975 loss: 9.31505747e-06
Iter: 976 loss: 9.31017712e-06
Iter: 977 loss: 9.34289437e-06
Iter: 978 loss: 9.30955321e-06
Iter: 979 loss: 9.30671376e-06
Iter: 980 loss: 9.30661e-06
Iter: 981 loss: 9.30448914e-06
Iter: 982 loss: 9.30385795e-06
Iter: 983 loss: 9.30259921e-06
Iter: 984 loss: 9.29925e-06
Iter: 985 loss: 9.31111845e-06
Iter: 986 loss: 9.29835369e-06
Iter: 987 loss: 9.29521229e-06
Iter: 988 loss: 9.2964965e-06
Iter: 989 loss: 9.29312591e-06
Iter: 990 loss: 9.28919872e-06
Iter: 991 loss: 9.29507405e-06
Iter: 992 loss: 9.28722511e-06
Iter: 993 loss: 9.28419377e-06
Iter: 994 loss: 9.28414738e-06
Iter: 995 loss: 9.28155714e-06
Iter: 996 loss: 9.27743076e-06
Iter: 997 loss: 9.27737256e-06
Iter: 998 loss: 9.27324618e-06
Iter: 999 loss: 9.27137808e-06
Iter: 1000 loss: 9.26920711e-06
Iter: 1001 loss: 9.26314897e-06
Iter: 1002 loss: 9.28868576e-06
Iter: 1003 loss: 9.26194662e-06
Iter: 1004 loss: 9.25663608e-06
Iter: 1005 loss: 9.25657332e-06
Iter: 1006 loss: 9.25372115e-06
Iter: 1007 loss: 9.24782489e-06
Iter: 1008 loss: 9.34081709e-06
Iter: 1009 loss: 9.24767664e-06
Iter: 1010 loss: 9.24368396e-06
Iter: 1011 loss: 9.24367123e-06
Iter: 1012 loss: 9.24003598e-06
Iter: 1013 loss: 9.26e-06
Iter: 1014 loss: 9.23948e-06
Iter: 1015 loss: 9.23702555e-06
Iter: 1016 loss: 9.24313917e-06
Iter: 1017 loss: 9.23613879e-06
Iter: 1018 loss: 9.23354673e-06
Iter: 1019 loss: 9.23801417e-06
Iter: 1020 loss: 9.23236439e-06
Iter: 1021 loss: 9.2295968e-06
Iter: 1022 loss: 9.23068e-06
Iter: 1023 loss: 9.22771414e-06
Iter: 1024 loss: 9.22391e-06
Iter: 1025 loss: 9.2323462e-06
Iter: 1026 loss: 9.22252366e-06
Iter: 1027 loss: 9.21863557e-06
Iter: 1028 loss: 9.25516724e-06
Iter: 1029 loss: 9.21837636e-06
Iter: 1030 loss: 9.21557148e-06
Iter: 1031 loss: 9.20895218e-06
Iter: 1032 loss: 9.2878081e-06
Iter: 1033 loss: 9.20829734e-06
Iter: 1034 loss: 9.20157709e-06
Iter: 1035 loss: 9.22305571e-06
Iter: 1036 loss: 9.19953527e-06
Iter: 1037 loss: 9.19625381e-06
Iter: 1038 loss: 9.19612103e-06
Iter: 1039 loss: 9.19214472e-06
Iter: 1040 loss: 9.19348e-06
Iter: 1041 loss: 9.18936894e-06
Iter: 1042 loss: 9.18620117e-06
Iter: 1043 loss: 9.18296064e-06
Iter: 1044 loss: 9.18234582e-06
Iter: 1045 loss: 9.17979924e-06
Iter: 1046 loss: 9.17900798e-06
Iter: 1047 loss: 9.17624311e-06
Iter: 1048 loss: 9.17591751e-06
Iter: 1049 loss: 9.17396756e-06
Iter: 1050 loss: 9.17022317e-06
Iter: 1051 loss: 9.18734804e-06
Iter: 1052 loss: 9.16945646e-06
Iter: 1053 loss: 9.1662514e-06
Iter: 1054 loss: 9.16839235e-06
Iter: 1055 loss: 9.16425597e-06
Iter: 1056 loss: 9.16078352e-06
Iter: 1057 loss: 9.16951467e-06
Iter: 1058 loss: 9.15951e-06
Iter: 1059 loss: 9.15645433e-06
Iter: 1060 loss: 9.18031e-06
Iter: 1061 loss: 9.15630335e-06
Iter: 1062 loss: 9.15330747e-06
Iter: 1063 loss: 9.15282271e-06
Iter: 1064 loss: 9.1508191e-06
Iter: 1065 loss: 9.14734483e-06
Iter: 1066 loss: 9.14219709e-06
Iter: 1067 loss: 9.14213342e-06
Iter: 1068 loss: 9.13506301e-06
Iter: 1069 loss: 9.15722e-06
Iter: 1070 loss: 9.13294389e-06
Iter: 1071 loss: 9.12988435e-06
Iter: 1072 loss: 9.12865653e-06
Iter: 1073 loss: 9.12603628e-06
Iter: 1074 loss: 9.11928146e-06
Iter: 1075 loss: 9.18169098e-06
Iter: 1076 loss: 9.11837e-06
Iter: 1077 loss: 9.11302595e-06
Iter: 1078 loss: 9.18557271e-06
Iter: 1079 loss: 9.11300504e-06
Iter: 1080 loss: 9.1079728e-06
Iter: 1081 loss: 9.1367483e-06
Iter: 1082 loss: 9.10723065e-06
Iter: 1083 loss: 9.10423569e-06
Iter: 1084 loss: 9.11301504e-06
Iter: 1085 loss: 9.10316157e-06
Iter: 1086 loss: 9.09969276e-06
Iter: 1087 loss: 9.09983464e-06
Iter: 1088 loss: 9.09699156e-06
Iter: 1089 loss: 9.09238315e-06
Iter: 1090 loss: 9.10351264e-06
Iter: 1091 loss: 9.09088885e-06
Iter: 1092 loss: 9.08672155e-06
Iter: 1093 loss: 9.10214658e-06
Iter: 1094 loss: 9.08564289e-06
Iter: 1095 loss: 9.08145194e-06
Iter: 1096 loss: 9.10485e-06
Iter: 1097 loss: 9.08095717e-06
Iter: 1098 loss: 9.07783397e-06
Iter: 1099 loss: 9.07239701e-06
Iter: 1100 loss: 9.07244248e-06
Iter: 1101 loss: 9.06667083e-06
Iter: 1102 loss: 9.07171852e-06
Iter: 1103 loss: 9.06326841e-06
Iter: 1104 loss: 9.05914203e-06
Iter: 1105 loss: 9.05898196e-06
Iter: 1106 loss: 9.05386332e-06
Iter: 1107 loss: 9.051505e-06
Iter: 1108 loss: 9.0490239e-06
Iter: 1109 loss: 9.04385706e-06
Iter: 1110 loss: 9.04268472e-06
Iter: 1111 loss: 9.03947694e-06
Iter: 1112 loss: 9.03718865e-06
Iter: 1113 loss: 9.03554428e-06
Iter: 1114 loss: 9.03283399e-06
Iter: 1115 loss: 9.03139608e-06
Iter: 1116 loss: 9.0300764e-06
Iter: 1117 loss: 9.0260437e-06
Iter: 1118 loss: 9.04904846e-06
Iter: 1119 loss: 9.02549436e-06
Iter: 1120 loss: 9.02272313e-06
Iter: 1121 loss: 9.02340616e-06
Iter: 1122 loss: 9.02067768e-06
Iter: 1123 loss: 9.01675503e-06
Iter: 1124 loss: 9.02372449e-06
Iter: 1125 loss: 9.01506883e-06
Iter: 1126 loss: 9.0111007e-06
Iter: 1127 loss: 9.04494482e-06
Iter: 1128 loss: 9.01086787e-06
Iter: 1129 loss: 9.00711439e-06
Iter: 1130 loss: 9.00271152e-06
Iter: 1131 loss: 9.00223131e-06
Iter: 1132 loss: 8.99634e-06
Iter: 1133 loss: 8.99476709e-06
Iter: 1134 loss: 8.99103725e-06
Iter: 1135 loss: 8.9846144e-06
Iter: 1136 loss: 9.03513046e-06
Iter: 1137 loss: 8.9841742e-06
Iter: 1138 loss: 8.97939935e-06
Iter: 1139 loss: 8.97949758e-06
Iter: 1140 loss: 8.97694736e-06
Iter: 1141 loss: 8.97089558e-06
Iter: 1142 loss: 9.03617729e-06
Iter: 1143 loss: 8.97020618e-06
Iter: 1144 loss: 8.96581878e-06
Iter: 1145 loss: 8.96573238e-06
Iter: 1146 loss: 8.96064921e-06
Iter: 1147 loss: 8.96457186e-06
Iter: 1148 loss: 8.95760877e-06
Iter: 1149 loss: 8.95308676e-06
Iter: 1150 loss: 8.98515736e-06
Iter: 1151 loss: 8.9527166e-06
Iter: 1152 loss: 8.94885852e-06
Iter: 1153 loss: 8.94515506e-06
Iter: 1154 loss: 8.94415462e-06
Iter: 1155 loss: 8.93825109e-06
Iter: 1156 loss: 8.97072096e-06
Iter: 1157 loss: 8.93738616e-06
Iter: 1158 loss: 8.93322431e-06
Iter: 1159 loss: 8.95780249e-06
Iter: 1160 loss: 8.9327832e-06
Iter: 1161 loss: 8.92867e-06
Iter: 1162 loss: 8.93355354e-06
Iter: 1163 loss: 8.92659227e-06
Iter: 1164 loss: 8.92278695e-06
Iter: 1165 loss: 8.91792934e-06
Iter: 1166 loss: 8.91750733e-06
Iter: 1167 loss: 8.91031414e-06
Iter: 1168 loss: 8.9164887e-06
Iter: 1169 loss: 8.90609681e-06
Iter: 1170 loss: 8.90285355e-06
Iter: 1171 loss: 8.9012392e-06
Iter: 1172 loss: 8.89666899e-06
Iter: 1173 loss: 8.88822069e-06
Iter: 1174 loss: 9.07429603e-06
Iter: 1175 loss: 8.88822433e-06
Iter: 1176 loss: 8.88120849e-06
Iter: 1177 loss: 8.90914725e-06
Iter: 1178 loss: 8.87961869e-06
Iter: 1179 loss: 8.87356873e-06
Iter: 1180 loss: 8.87348506e-06
Iter: 1181 loss: 8.8703182e-06
Iter: 1182 loss: 8.87198621e-06
Iter: 1183 loss: 8.8680772e-06
Iter: 1184 loss: 8.86301677e-06
Iter: 1185 loss: 8.86737325e-06
Iter: 1186 loss: 8.85993e-06
Iter: 1187 loss: 8.85489226e-06
Iter: 1188 loss: 8.86316138e-06
Iter: 1189 loss: 8.85264581e-06
Iter: 1190 loss: 8.84620567e-06
Iter: 1191 loss: 8.86119415e-06
Iter: 1192 loss: 8.84385372e-06
Iter: 1193 loss: 8.83691609e-06
Iter: 1194 loss: 8.88098839e-06
Iter: 1195 loss: 8.83608482e-06
Iter: 1196 loss: 8.83122448e-06
Iter: 1197 loss: 8.82494714e-06
Iter: 1198 loss: 8.82458426e-06
Iter: 1199 loss: 8.81735468e-06
Iter: 1200 loss: 8.8292918e-06
Iter: 1201 loss: 8.8142333e-06
Iter: 1202 loss: 8.80962216e-06
Iter: 1203 loss: 8.80967946e-06
Iter: 1204 loss: 8.80466359e-06
Iter: 1205 loss: 8.81203323e-06
Iter: 1206 loss: 8.80237531e-06
Iter: 1207 loss: 8.7983135e-06
Iter: 1208 loss: 8.79185063e-06
Iter: 1209 loss: 8.7918761e-06
Iter: 1210 loss: 8.78926403e-06
Iter: 1211 loss: 8.78738592e-06
Iter: 1212 loss: 8.78377432e-06
Iter: 1213 loss: 8.77931234e-06
Iter: 1214 loss: 8.77881303e-06
Iter: 1215 loss: 8.77276e-06
Iter: 1216 loss: 8.83001849e-06
Iter: 1217 loss: 8.77270759e-06
Iter: 1218 loss: 8.76912054e-06
Iter: 1219 loss: 8.76622471e-06
Iter: 1220 loss: 8.76526065e-06
Iter: 1221 loss: 8.75995465e-06
Iter: 1222 loss: 8.79827e-06
Iter: 1223 loss: 8.75958904e-06
Iter: 1224 loss: 8.7560029e-06
Iter: 1225 loss: 8.77719322e-06
Iter: 1226 loss: 8.75557544e-06
Iter: 1227 loss: 8.75191654e-06
Iter: 1228 loss: 8.74718899e-06
Iter: 1229 loss: 8.74690159e-06
Iter: 1230 loss: 8.74081525e-06
Iter: 1231 loss: 8.74229e-06
Iter: 1232 loss: 8.73636236e-06
Iter: 1233 loss: 8.72955388e-06
Iter: 1234 loss: 8.76300328e-06
Iter: 1235 loss: 8.72828332e-06
Iter: 1236 loss: 8.72455075e-06
Iter: 1237 loss: 8.72416877e-06
Iter: 1238 loss: 8.72195506e-06
Iter: 1239 loss: 8.71710108e-06
Iter: 1240 loss: 8.78805076e-06
Iter: 1241 loss: 8.71684097e-06
Iter: 1242 loss: 8.71398061e-06
Iter: 1243 loss: 8.71382e-06
Iter: 1244 loss: 8.71039265e-06
Iter: 1245 loss: 8.71010798e-06
Iter: 1246 loss: 8.70758413e-06
Iter: 1247 loss: 8.70411e-06
Iter: 1248 loss: 8.73315912e-06
Iter: 1249 loss: 8.70392614e-06
Iter: 1250 loss: 8.7006847e-06
Iter: 1251 loss: 8.69597898e-06
Iter: 1252 loss: 8.69591167e-06
Iter: 1253 loss: 8.69070209e-06
Iter: 1254 loss: 8.72928831e-06
Iter: 1255 loss: 8.69035102e-06
Iter: 1256 loss: 8.68594361e-06
Iter: 1257 loss: 8.70299118e-06
Iter: 1258 loss: 8.68512e-06
Iter: 1259 loss: 8.68069674e-06
Iter: 1260 loss: 8.68966436e-06
Iter: 1261 loss: 8.67893687e-06
Iter: 1262 loss: 8.6756736e-06
Iter: 1263 loss: 8.67178278e-06
Iter: 1264 loss: 8.671459e-06
Iter: 1265 loss: 8.66537084e-06
Iter: 1266 loss: 8.67548897e-06
Iter: 1267 loss: 8.66269147e-06
Iter: 1268 loss: 8.66086339e-06
Iter: 1269 loss: 8.65955826e-06
Iter: 1270 loss: 8.65648872e-06
Iter: 1271 loss: 8.6510081e-06
Iter: 1272 loss: 8.78142328e-06
Iter: 1273 loss: 8.65098264e-06
Iter: 1274 loss: 8.64668436e-06
Iter: 1275 loss: 8.67301696e-06
Iter: 1276 loss: 8.64616686e-06
Iter: 1277 loss: 8.64200592e-06
Iter: 1278 loss: 8.68223105e-06
Iter: 1279 loss: 8.64188769e-06
Iter: 1280 loss: 8.6397331e-06
Iter: 1281 loss: 8.64171489e-06
Iter: 1282 loss: 8.63857622e-06
Iter: 1283 loss: 8.63532841e-06
Iter: 1284 loss: 8.63718e-06
Iter: 1285 loss: 8.6332393e-06
Iter: 1286 loss: 8.63017704e-06
Iter: 1287 loss: 8.6348191e-06
Iter: 1288 loss: 8.62863453e-06
Iter: 1289 loss: 8.62461275e-06
Iter: 1290 loss: 8.64213143e-06
Iter: 1291 loss: 8.62384513e-06
Iter: 1292 loss: 8.62018351e-06
Iter: 1293 loss: 8.63523383e-06
Iter: 1294 loss: 8.61940134e-06
Iter: 1295 loss: 8.61632361e-06
Iter: 1296 loss: 8.6114851e-06
Iter: 1297 loss: 8.61146691e-06
Iter: 1298 loss: 8.60642467e-06
Iter: 1299 loss: 8.61877379e-06
Iter: 1300 loss: 8.60459568e-06
Iter: 1301 loss: 8.60114051e-06
Iter: 1302 loss: 8.6485561e-06
Iter: 1303 loss: 8.60118053e-06
Iter: 1304 loss: 8.59718875e-06
Iter: 1305 loss: 8.60369073e-06
Iter: 1306 loss: 8.5953634e-06
Iter: 1307 loss: 8.59234478e-06
Iter: 1308 loss: 8.58915882e-06
Iter: 1309 loss: 8.58866e-06
Iter: 1310 loss: 8.58560452e-06
Iter: 1311 loss: 8.58505791e-06
Iter: 1312 loss: 8.58256863e-06
Iter: 1313 loss: 8.57878149e-06
Iter: 1314 loss: 8.57871783e-06
Iter: 1315 loss: 8.57394298e-06
Iter: 1316 loss: 8.61899207e-06
Iter: 1317 loss: 8.57363921e-06
Iter: 1318 loss: 8.57112263e-06
Iter: 1319 loss: 8.56865336e-06
Iter: 1320 loss: 8.56810675e-06
Iter: 1321 loss: 8.56469342e-06
Iter: 1322 loss: 8.60826185e-06
Iter: 1323 loss: 8.56460792e-06
Iter: 1324 loss: 8.56229326e-06
Iter: 1325 loss: 8.56980751e-06
Iter: 1326 loss: 8.56152474e-06
Iter: 1327 loss: 8.55904455e-06
Iter: 1328 loss: 8.55693361e-06
Iter: 1329 loss: 8.55623875e-06
Iter: 1330 loss: 8.55237613e-06
Iter: 1331 loss: 8.54908467e-06
Iter: 1332 loss: 8.54807513e-06
Iter: 1333 loss: 8.54185237e-06
Iter: 1334 loss: 8.57968644e-06
Iter: 1335 loss: 8.54103837e-06
Iter: 1336 loss: 8.53666825e-06
Iter: 1337 loss: 8.53656638e-06
Iter: 1338 loss: 8.53414804e-06
Iter: 1339 loss: 8.52902e-06
Iter: 1340 loss: 8.61145236e-06
Iter: 1341 loss: 8.52890298e-06
Iter: 1342 loss: 8.52769881e-06
Iter: 1343 loss: 8.52644735e-06
Iter: 1344 loss: 8.52412632e-06
Iter: 1345 loss: 8.52169615e-06
Iter: 1346 loss: 8.52132689e-06
Iter: 1347 loss: 8.51830828e-06
Iter: 1348 loss: 8.5500451e-06
Iter: 1349 loss: 8.51828e-06
Iter: 1350 loss: 8.51565e-06
Iter: 1351 loss: 8.51059576e-06
Iter: 1352 loss: 8.61024e-06
Iter: 1353 loss: 8.51053301e-06
Iter: 1354 loss: 8.50575907e-06
Iter: 1355 loss: 8.55943836e-06
Iter: 1356 loss: 8.50575543e-06
Iter: 1357 loss: 8.50177548e-06
Iter: 1358 loss: 8.51580535e-06
Iter: 1359 loss: 8.50067772e-06
Iter: 1360 loss: 8.49680691e-06
Iter: 1361 loss: 8.50179e-06
Iter: 1362 loss: 8.49481148e-06
Iter: 1363 loss: 8.49177377e-06
Iter: 1364 loss: 8.48974378e-06
Iter: 1365 loss: 8.48856325e-06
Iter: 1366 loss: 8.48436503e-06
Iter: 1367 loss: 8.49594835e-06
Iter: 1368 loss: 8.48304626e-06
Iter: 1369 loss: 8.48111449e-06
Iter: 1370 loss: 8.48043055e-06
Iter: 1371 loss: 8.47833326e-06
Iter: 1372 loss: 8.4735093e-06
Iter: 1373 loss: 8.54716745e-06
Iter: 1374 loss: 8.47331e-06
Iter: 1375 loss: 8.4699e-06
Iter: 1376 loss: 8.51756886e-06
Iter: 1377 loss: 8.46986404e-06
Iter: 1378 loss: 8.46602506e-06
Iter: 1379 loss: 8.47320371e-06
Iter: 1380 loss: 8.46434523e-06
Iter: 1381 loss: 8.46192324e-06
Iter: 1382 loss: 8.47332e-06
Iter: 1383 loss: 8.46135299e-06
Iter: 1384 loss: 8.45837e-06
Iter: 1385 loss: 8.45780323e-06
Iter: 1386 loss: 8.4558551e-06
Iter: 1387 loss: 8.45279646e-06
Iter: 1388 loss: 8.4587582e-06
Iter: 1389 loss: 8.45155682e-06
Iter: 1390 loss: 8.44762326e-06
Iter: 1391 loss: 8.46809326e-06
Iter: 1392 loss: 8.44698934e-06
Iter: 1393 loss: 8.44355418e-06
Iter: 1394 loss: 8.4516887e-06
Iter: 1395 loss: 8.44223541e-06
Iter: 1396 loss: 8.43892485e-06
Iter: 1397 loss: 8.43510134e-06
Iter: 1398 loss: 8.43464204e-06
Iter: 1399 loss: 8.42984082e-06
Iter: 1400 loss: 8.43900489e-06
Iter: 1401 loss: 8.42769077e-06
Iter: 1402 loss: 8.42502868e-06
Iter: 1403 loss: 8.42487316e-06
Iter: 1404 loss: 8.4217827e-06
Iter: 1405 loss: 8.42377267e-06
Iter: 1406 loss: 8.41983638e-06
Iter: 1407 loss: 8.41717429e-06
Iter: 1408 loss: 8.41630572e-06
Iter: 1409 loss: 8.41484598e-06
Iter: 1410 loss: 8.41124711e-06
Iter: 1411 loss: 8.41115798e-06
Iter: 1412 loss: 8.40918801e-06
Iter: 1413 loss: 8.40716166e-06
Iter: 1414 loss: 8.40677785e-06
Iter: 1415 loss: 8.40273242e-06
Iter: 1416 loss: 8.42242753e-06
Iter: 1417 loss: 8.40207576e-06
Iter: 1418 loss: 8.39946551e-06
Iter: 1419 loss: 8.39732638e-06
Iter: 1420 loss: 8.39654058e-06
Iter: 1421 loss: 8.39373e-06
Iter: 1422 loss: 8.39366749e-06
Iter: 1423 loss: 8.39160566e-06
Iter: 1424 loss: 8.39454e-06
Iter: 1425 loss: 8.39062e-06
Iter: 1426 loss: 8.38800133e-06
Iter: 1427 loss: 8.38566666e-06
Iter: 1428 loss: 8.38504457e-06
Iter: 1429 loss: 8.38093092e-06
Iter: 1430 loss: 8.37989319e-06
Iter: 1431 loss: 8.37731e-06
Iter: 1432 loss: 8.37178868e-06
Iter: 1433 loss: 8.4035828e-06
Iter: 1434 loss: 8.37098742e-06
Iter: 1435 loss: 8.36626168e-06
Iter: 1436 loss: 8.43667203e-06
Iter: 1437 loss: 8.36627351e-06
Iter: 1438 loss: 8.36380241e-06
Iter: 1439 loss: 8.35928131e-06
Iter: 1440 loss: 8.46557123e-06
Iter: 1441 loss: 8.35925766e-06
Iter: 1442 loss: 8.35762057e-06
Iter: 1443 loss: 8.3567038e-06
Iter: 1444 loss: 8.35450192e-06
Iter: 1445 loss: 8.35078936e-06
Iter: 1446 loss: 8.35079118e-06
Iter: 1447 loss: 8.34702e-06
Iter: 1448 loss: 8.3941668e-06
Iter: 1449 loss: 8.34708226e-06
Iter: 1450 loss: 8.34428829e-06
Iter: 1451 loss: 8.33934428e-06
Iter: 1452 loss: 8.45771501e-06
Iter: 1453 loss: 8.33918057e-06
Iter: 1454 loss: 8.3343748e-06
Iter: 1455 loss: 8.39068434e-06
Iter: 1456 loss: 8.33440936e-06
Iter: 1457 loss: 8.32987644e-06
Iter: 1458 loss: 8.33427e-06
Iter: 1459 loss: 8.32739352e-06
Iter: 1460 loss: 8.32206388e-06
Iter: 1461 loss: 8.33616105e-06
Iter: 1462 loss: 8.32027e-06
Iter: 1463 loss: 8.31670877e-06
Iter: 1464 loss: 8.31273155e-06
Iter: 1465 loss: 8.31215584e-06
Iter: 1466 loss: 8.30596764e-06
Iter: 1467 loss: 8.31767829e-06
Iter: 1468 loss: 8.30332e-06
Iter: 1469 loss: 8.30184217e-06
Iter: 1470 loss: 8.2999486e-06
Iter: 1471 loss: 8.29680357e-06
Iter: 1472 loss: 8.29046e-06
Iter: 1473 loss: 8.4034e-06
Iter: 1474 loss: 8.29024702e-06
Iter: 1475 loss: 8.28592238e-06
Iter: 1476 loss: 8.28585235e-06
Iter: 1477 loss: 8.28112843e-06
Iter: 1478 loss: 8.28245629e-06
Iter: 1479 loss: 8.27768235e-06
Iter: 1480 loss: 8.2740562e-06
Iter: 1481 loss: 8.30250428e-06
Iter: 1482 loss: 8.27373515e-06
Iter: 1483 loss: 8.27018539e-06
Iter: 1484 loss: 8.26875839e-06
Iter: 1485 loss: 8.26686664e-06
Iter: 1486 loss: 8.2633087e-06
Iter: 1487 loss: 8.27451095e-06
Iter: 1488 loss: 8.2622264e-06
Iter: 1489 loss: 8.25767893e-06
Iter: 1490 loss: 8.2806946e-06
Iter: 1491 loss: 8.25704592e-06
Iter: 1492 loss: 8.25341249e-06
Iter: 1493 loss: 8.25627467e-06
Iter: 1494 loss: 8.25116e-06
Iter: 1495 loss: 8.24569906e-06
Iter: 1496 loss: 8.23904884e-06
Iter: 1497 loss: 8.23842311e-06
Iter: 1498 loss: 8.23066785e-06
Iter: 1499 loss: 8.24374365e-06
Iter: 1500 loss: 8.22709444e-06
Iter: 1501 loss: 8.22099082e-06
Iter: 1502 loss: 8.29937198e-06
Iter: 1503 loss: 8.22087623e-06
Iter: 1504 loss: 8.21522644e-06
Iter: 1505 loss: 8.2523e-06
Iter: 1506 loss: 8.21453796e-06
Iter: 1507 loss: 8.21190497e-06
Iter: 1508 loss: 8.21054618e-06
Iter: 1509 loss: 8.20935475e-06
Iter: 1510 loss: 8.2045035e-06
Iter: 1511 loss: 8.24434937e-06
Iter: 1512 loss: 8.20413516e-06
Iter: 1513 loss: 8.20112746e-06
Iter: 1514 loss: 8.19894922e-06
Iter: 1515 loss: 8.19783781e-06
Iter: 1516 loss: 8.19188e-06
Iter: 1517 loss: 8.21734193e-06
Iter: 1518 loss: 8.19065281e-06
Iter: 1519 loss: 8.18652643e-06
Iter: 1520 loss: 8.18307581e-06
Iter: 1521 loss: 8.18195622e-06
Iter: 1522 loss: 8.17786531e-06
Iter: 1523 loss: 8.17760611e-06
Iter: 1524 loss: 8.17454384e-06
Iter: 1525 loss: 8.17469845e-06
Iter: 1526 loss: 8.17212913e-06
Iter: 1527 loss: 8.16748434e-06
Iter: 1528 loss: 8.17227829e-06
Iter: 1529 loss: 8.16493412e-06
Iter: 1530 loss: 8.15953899e-06
Iter: 1531 loss: 8.15245312e-06
Iter: 1532 loss: 8.1519629e-06
Iter: 1533 loss: 8.14373379e-06
Iter: 1534 loss: 8.19141587e-06
Iter: 1535 loss: 8.14254781e-06
Iter: 1536 loss: 8.13864699e-06
Iter: 1537 loss: 8.13809675e-06
Iter: 1538 loss: 8.13467705e-06
Iter: 1539 loss: 8.12791768e-06
Iter: 1540 loss: 8.25038478e-06
Iter: 1541 loss: 8.12778762e-06
Iter: 1542 loss: 8.12586768e-06
Iter: 1543 loss: 8.12410326e-06
Iter: 1544 loss: 8.12153576e-06
Iter: 1545 loss: 8.1170092e-06
Iter: 1546 loss: 8.22908441e-06
Iter: 1547 loss: 8.11701193e-06
Iter: 1548 loss: 8.11245081e-06
Iter: 1549 loss: 8.16773172e-06
Iter: 1550 loss: 8.11231257e-06
Iter: 1551 loss: 8.10874e-06
Iter: 1552 loss: 8.10344136e-06
Iter: 1553 loss: 8.10336314e-06
Iter: 1554 loss: 8.09792618e-06
Iter: 1555 loss: 8.15055773e-06
Iter: 1556 loss: 8.09777703e-06
Iter: 1557 loss: 8.09225912e-06
Iter: 1558 loss: 8.10209895e-06
Iter: 1559 loss: 8.08987716e-06
Iter: 1560 loss: 8.08455661e-06
Iter: 1561 loss: 8.09373159e-06
Iter: 1562 loss: 8.08229e-06
Iter: 1563 loss: 8.0776e-06
Iter: 1564 loss: 8.07298238e-06
Iter: 1565 loss: 8.07209108e-06
Iter: 1566 loss: 8.06381286e-06
Iter: 1567 loss: 8.07279775e-06
Iter: 1568 loss: 8.05928084e-06
Iter: 1569 loss: 8.05786658e-06
Iter: 1570 loss: 8.05518266e-06
Iter: 1571 loss: 8.05090349e-06
Iter: 1572 loss: 8.04630508e-06
Iter: 1573 loss: 8.04568481e-06
Iter: 1574 loss: 8.0412683e-06
Iter: 1575 loss: 8.09431731e-06
Iter: 1576 loss: 8.04117e-06
Iter: 1577 loss: 8.03605599e-06
Iter: 1578 loss: 8.03272724e-06
Iter: 1579 loss: 8.03077819e-06
Iter: 1580 loss: 8.02628165e-06
Iter: 1581 loss: 8.07723791e-06
Iter: 1582 loss: 8.02618251e-06
Iter: 1583 loss: 8.02224531e-06
Iter: 1584 loss: 8.01887e-06
Iter: 1585 loss: 8.01776332e-06
Iter: 1586 loss: 8.0128666e-06
Iter: 1587 loss: 8.03674448e-06
Iter: 1588 loss: 8.01207625e-06
Iter: 1589 loss: 8.00798625e-06
Iter: 1590 loss: 8.05310174e-06
Iter: 1591 loss: 8.00787438e-06
Iter: 1592 loss: 8.00488851e-06
Iter: 1593 loss: 8.00330417e-06
Iter: 1594 loss: 8.00193538e-06
Iter: 1595 loss: 7.9966776e-06
Iter: 1596 loss: 7.99777081e-06
Iter: 1597 loss: 7.99293593e-06
Iter: 1598 loss: 7.98720794e-06
Iter: 1599 loss: 7.98687597e-06
Iter: 1600 loss: 7.98273777e-06
Iter: 1601 loss: 7.97597386e-06
Iter: 1602 loss: 8.03297735e-06
Iter: 1603 loss: 7.97567e-06
Iter: 1604 loss: 7.96941458e-06
Iter: 1605 loss: 8.03288913e-06
Iter: 1606 loss: 7.96926361e-06
Iter: 1607 loss: 7.96653785e-06
Iter: 1608 loss: 7.96489348e-06
Iter: 1609 loss: 7.96375207e-06
Iter: 1610 loss: 7.95891447e-06
Iter: 1611 loss: 8.00228554e-06
Iter: 1612 loss: 7.95861e-06
Iter: 1613 loss: 7.95590313e-06
Iter: 1614 loss: 7.95419146e-06
Iter: 1615 loss: 7.95312644e-06
Iter: 1616 loss: 7.94781681e-06
Iter: 1617 loss: 7.96751374e-06
Iter: 1618 loss: 7.94648804e-06
Iter: 1619 loss: 7.94301468e-06
Iter: 1620 loss: 7.94030257e-06
Iter: 1621 loss: 7.93925483e-06
Iter: 1622 loss: 7.93551408e-06
Iter: 1623 loss: 7.9354013e-06
Iter: 1624 loss: 7.93242816e-06
Iter: 1625 loss: 7.93690651e-06
Iter: 1626 loss: 7.93107483e-06
Iter: 1627 loss: 7.92809715e-06
Iter: 1628 loss: 7.92794162e-06
Iter: 1629 loss: 7.92557876e-06
Iter: 1630 loss: 7.92100764e-06
Iter: 1631 loss: 7.91926868e-06
Iter: 1632 loss: 7.9168467e-06
Iter: 1633 loss: 7.91096954e-06
Iter: 1634 loss: 7.92455558e-06
Iter: 1635 loss: 7.90884769e-06
Iter: 1636 loss: 7.90588274e-06
Iter: 1637 loss: 7.90508784e-06
Iter: 1638 loss: 7.90217e-06
Iter: 1639 loss: 7.89578e-06
Iter: 1640 loss: 7.99048394e-06
Iter: 1641 loss: 7.89556361e-06
Iter: 1642 loss: 7.89400474e-06
Iter: 1643 loss: 7.89214e-06
Iter: 1644 loss: 7.88972284e-06
Iter: 1645 loss: 7.88469697e-06
Iter: 1646 loss: 7.96349559e-06
Iter: 1647 loss: 7.88449415e-06
Iter: 1648 loss: 7.87963199e-06
Iter: 1649 loss: 7.95143205e-06
Iter: 1650 loss: 7.87956924e-06
Iter: 1651 loss: 7.87633871e-06
Iter: 1652 loss: 7.87172303e-06
Iter: 1653 loss: 7.87148747e-06
Iter: 1654 loss: 7.86695819e-06
Iter: 1655 loss: 7.91103048e-06
Iter: 1656 loss: 7.86674354e-06
Iter: 1657 loss: 7.8623043e-06
Iter: 1658 loss: 7.87437057e-06
Iter: 1659 loss: 7.86082273e-06
Iter: 1660 loss: 7.85657539e-06
Iter: 1661 loss: 7.86003e-06
Iter: 1662 loss: 7.85397879e-06
Iter: 1663 loss: 7.84964686e-06
Iter: 1664 loss: 7.84685562e-06
Iter: 1665 loss: 7.84506483e-06
Iter: 1666 loss: 7.83734413e-06
Iter: 1667 loss: 7.84598706e-06
Iter: 1668 loss: 7.83315409e-06
Iter: 1669 loss: 7.83022733e-06
Iter: 1670 loss: 7.82905e-06
Iter: 1671 loss: 7.82449479e-06
Iter: 1672 loss: 7.82115603e-06
Iter: 1673 loss: 7.81976087e-06
Iter: 1674 loss: 7.81588915e-06
Iter: 1675 loss: 7.86661258e-06
Iter: 1676 loss: 7.81588824e-06
Iter: 1677 loss: 7.81160816e-06
Iter: 1678 loss: 7.80797382e-06
Iter: 1679 loss: 7.80679329e-06
Iter: 1680 loss: 7.80273331e-06
Iter: 1681 loss: 7.8445355e-06
Iter: 1682 loss: 7.80262144e-06
Iter: 1683 loss: 7.79860238e-06
Iter: 1684 loss: 7.79127367e-06
Iter: 1685 loss: 7.79121274e-06
Iter: 1686 loss: 7.78453341e-06
Iter: 1687 loss: 7.83009637e-06
Iter: 1688 loss: 7.78379126e-06
Iter: 1689 loss: 7.77889e-06
Iter: 1690 loss: 7.84198528e-06
Iter: 1691 loss: 7.77881269e-06
Iter: 1692 loss: 7.77531659e-06
Iter: 1693 loss: 7.77572677e-06
Iter: 1694 loss: 7.77266905e-06
Iter: 1695 loss: 7.76774323e-06
Iter: 1696 loss: 7.76758316e-06
Iter: 1697 loss: 7.76365232e-06
Iter: 1698 loss: 7.7572131e-06
Iter: 1699 loss: 7.7575678e-06
Iter: 1700 loss: 7.75212266e-06
Iter: 1701 loss: 7.74412e-06
Iter: 1702 loss: 7.78710637e-06
Iter: 1703 loss: 7.74290675e-06
Iter: 1704 loss: 7.73823194e-06
Iter: 1705 loss: 7.73784359e-06
Iter: 1706 loss: 7.735287e-06
Iter: 1707 loss: 7.73047759e-06
Iter: 1708 loss: 7.8243047e-06
Iter: 1709 loss: 7.73037846e-06
Iter: 1710 loss: 7.72390376e-06
Iter: 1711 loss: 7.80517439e-06
Iter: 1712 loss: 7.72384919e-06
Iter: 1713 loss: 7.7209088e-06
Iter: 1714 loss: 7.7183322e-06
Iter: 1715 loss: 7.71751729e-06
Iter: 1716 loss: 7.71191e-06
Iter: 1717 loss: 7.7467148e-06
Iter: 1718 loss: 7.7112727e-06
Iter: 1719 loss: 7.70848783e-06
Iter: 1720 loss: 7.70221322e-06
Iter: 1721 loss: 7.78483263e-06
Iter: 1722 loss: 7.70183215e-06
Iter: 1723 loss: 7.69686449e-06
Iter: 1724 loss: 7.69649341e-06
Iter: 1725 loss: 7.69179132e-06
Iter: 1726 loss: 7.70624865e-06
Iter: 1727 loss: 7.69048893e-06
Iter: 1728 loss: 7.68673726e-06
Iter: 1729 loss: 7.68600512e-06
Iter: 1730 loss: 7.68335849e-06
Iter: 1731 loss: 7.67696656e-06
Iter: 1732 loss: 7.67708e-06
Iter: 1733 loss: 7.67190249e-06
Iter: 1734 loss: 7.66421545e-06
Iter: 1735 loss: 7.67274651e-06
Iter: 1736 loss: 7.66017365e-06
Iter: 1737 loss: 7.65620098e-06
Iter: 1738 loss: 7.65547429e-06
Iter: 1739 loss: 7.65045297e-06
Iter: 1740 loss: 7.64410743e-06
Iter: 1741 loss: 7.64361721e-06
Iter: 1742 loss: 7.64058586e-06
Iter: 1743 loss: 7.64000833e-06
Iter: 1744 loss: 7.63635762e-06
Iter: 1745 loss: 7.62829495e-06
Iter: 1746 loss: 7.74460204e-06
Iter: 1747 loss: 7.62795435e-06
Iter: 1748 loss: 7.62560558e-06
Iter: 1749 loss: 7.62429227e-06
Iter: 1750 loss: 7.62173886e-06
Iter: 1751 loss: 7.61759e-06
Iter: 1752 loss: 7.61758474e-06
Iter: 1753 loss: 7.61195406e-06
Iter: 1754 loss: 7.6073e-06
Iter: 1755 loss: 7.6057122e-06
Iter: 1756 loss: 7.60157491e-06
Iter: 1757 loss: 7.60040075e-06
Iter: 1758 loss: 7.59678e-06
Iter: 1759 loss: 7.59811383e-06
Iter: 1760 loss: 7.59431259e-06
Iter: 1761 loss: 7.58943861e-06
Iter: 1762 loss: 7.57848647e-06
Iter: 1763 loss: 7.72334e-06
Iter: 1764 loss: 7.57778071e-06
Iter: 1765 loss: 7.56963027e-06
Iter: 1766 loss: 7.56958434e-06
Iter: 1767 loss: 7.56494865e-06
Iter: 1768 loss: 7.57713542e-06
Iter: 1769 loss: 7.56346162e-06
Iter: 1770 loss: 7.55809106e-06
Iter: 1771 loss: 7.628204e-06
Iter: 1772 loss: 7.55805877e-06
Iter: 1773 loss: 7.55493238e-06
Iter: 1774 loss: 7.55228575e-06
Iter: 1775 loss: 7.55157453e-06
Iter: 1776 loss: 7.5460307e-06
Iter: 1777 loss: 7.60412649e-06
Iter: 1778 loss: 7.54598295e-06
Iter: 1779 loss: 7.54335269e-06
Iter: 1780 loss: 7.5365233e-06
Iter: 1781 loss: 7.58514489e-06
Iter: 1782 loss: 7.53506e-06
Iter: 1783 loss: 7.5307662e-06
Iter: 1784 loss: 7.52959431e-06
Iter: 1785 loss: 7.5268058e-06
Iter: 1786 loss: 7.52551387e-06
Iter: 1787 loss: 7.52409278e-06
Iter: 1788 loss: 7.52016513e-06
Iter: 1789 loss: 7.52036385e-06
Iter: 1790 loss: 7.51718289e-06
Iter: 1791 loss: 7.51290509e-06
Iter: 1792 loss: 7.50506115e-06
Iter: 1793 loss: 7.67884194e-06
Iter: 1794 loss: 7.50505251e-06
Iter: 1795 loss: 7.50179152e-06
Iter: 1796 loss: 7.49995615e-06
Iter: 1797 loss: 7.49421497e-06
Iter: 1798 loss: 7.49141964e-06
Iter: 1799 loss: 7.48869388e-06
Iter: 1800 loss: 7.48415368e-06
Iter: 1801 loss: 7.47995637e-06
Iter: 1802 loss: 7.47888134e-06
Iter: 1803 loss: 7.47860668e-06
Iter: 1804 loss: 7.47632384e-06
Iter: 1805 loss: 7.47326521e-06
Iter: 1806 loss: 7.47086e-06
Iter: 1807 loss: 7.46990099e-06
Iter: 1808 loss: 7.46772321e-06
Iter: 1809 loss: 7.46694241e-06
Iter: 1810 loss: 7.46423575e-06
Iter: 1811 loss: 7.46185651e-06
Iter: 1812 loss: 7.46106252e-06
Iter: 1813 loss: 7.4561267e-06
Iter: 1814 loss: 7.47188369e-06
Iter: 1815 loss: 7.45472153e-06
Iter: 1816 loss: 7.44800127e-06
Iter: 1817 loss: 7.47800732e-06
Iter: 1818 loss: 7.44662157e-06
Iter: 1819 loss: 7.44243789e-06
Iter: 1820 loss: 7.44704357e-06
Iter: 1821 loss: 7.44014051e-06
Iter: 1822 loss: 7.43529654e-06
Iter: 1823 loss: 7.44995214e-06
Iter: 1824 loss: 7.43388773e-06
Iter: 1825 loss: 7.43015426e-06
Iter: 1826 loss: 7.43127885e-06
Iter: 1827 loss: 7.4274285e-06
Iter: 1828 loss: 7.42451903e-06
Iter: 1829 loss: 7.42415341e-06
Iter: 1830 loss: 7.42261909e-06
Iter: 1831 loss: 7.41793883e-06
Iter: 1832 loss: 7.42712109e-06
Iter: 1833 loss: 7.41498889e-06
Iter: 1834 loss: 7.40658925e-06
Iter: 1835 loss: 7.44702766e-06
Iter: 1836 loss: 7.40504311e-06
Iter: 1837 loss: 7.3993333e-06
Iter: 1838 loss: 7.3959518e-06
Iter: 1839 loss: 7.39357e-06
Iter: 1840 loss: 7.40014411e-06
Iter: 1841 loss: 7.39125107e-06
Iter: 1842 loss: 7.38895e-06
Iter: 1843 loss: 7.38627932e-06
Iter: 1844 loss: 7.38604285e-06
Iter: 1845 loss: 7.38279232e-06
Iter: 1846 loss: 7.38277868e-06
Iter: 1847 loss: 7.38012113e-06
Iter: 1848 loss: 7.37703522e-06
Iter: 1849 loss: 7.3767178e-06
Iter: 1850 loss: 7.37236769e-06
Iter: 1851 loss: 7.39537927e-06
Iter: 1852 loss: 7.37171149e-06
Iter: 1853 loss: 7.36763741e-06
Iter: 1854 loss: 7.38221297e-06
Iter: 1855 loss: 7.36674656e-06
Iter: 1856 loss: 7.36298216e-06
Iter: 1857 loss: 7.3585e-06
Iter: 1858 loss: 7.35805406e-06
Iter: 1859 loss: 7.3551887e-06
Iter: 1860 loss: 7.35519734e-06
Iter: 1861 loss: 7.35218964e-06
Iter: 1862 loss: 7.35541562e-06
Iter: 1863 loss: 7.35046842e-06
Iter: 1864 loss: 7.34735931e-06
Iter: 1865 loss: 7.34172863e-06
Iter: 1866 loss: 7.48182674e-06
Iter: 1867 loss: 7.34173136e-06
Iter: 1868 loss: 7.33530396e-06
Iter: 1869 loss: 7.32882427e-06
Iter: 1870 loss: 7.32754143e-06
Iter: 1871 loss: 7.31698128e-06
Iter: 1872 loss: 7.38976e-06
Iter: 1873 loss: 7.31600903e-06
Iter: 1874 loss: 7.31068076e-06
Iter: 1875 loss: 7.31068212e-06
Iter: 1876 loss: 7.30783813e-06
Iter: 1877 loss: 7.30775e-06
Iter: 1878 loss: 7.30587453e-06
Iter: 1879 loss: 7.30072725e-06
Iter: 1880 loss: 7.33207e-06
Iter: 1881 loss: 7.29934254e-06
Iter: 1882 loss: 7.29751173e-06
Iter: 1883 loss: 7.29580961e-06
Iter: 1884 loss: 7.29289968e-06
Iter: 1885 loss: 7.28864597e-06
Iter: 1886 loss: 7.28847499e-06
Iter: 1887 loss: 7.28184114e-06
Iter: 1888 loss: 7.30228567e-06
Iter: 1889 loss: 7.27995121e-06
Iter: 1890 loss: 7.27548922e-06
Iter: 1891 loss: 7.30571e-06
Iter: 1892 loss: 7.2750272e-06
Iter: 1893 loss: 7.2708267e-06
Iter: 1894 loss: 7.27521729e-06
Iter: 1895 loss: 7.26855069e-06
Iter: 1896 loss: 7.26518647e-06
Iter: 1897 loss: 7.26076269e-06
Iter: 1898 loss: 7.26051576e-06
Iter: 1899 loss: 7.25871314e-06
Iter: 1900 loss: 7.25712425e-06
Iter: 1901 loss: 7.25493e-06
Iter: 1902 loss: 7.24984557e-06
Iter: 1903 loss: 7.31705677e-06
Iter: 1904 loss: 7.24950905e-06
Iter: 1905 loss: 7.24347865e-06
Iter: 1906 loss: 7.24078382e-06
Iter: 1907 loss: 7.2375642e-06
Iter: 1908 loss: 7.2304224e-06
Iter: 1909 loss: 7.28899795e-06
Iter: 1910 loss: 7.23005e-06
Iter: 1911 loss: 7.22957157e-06
Iter: 1912 loss: 7.22734512e-06
Iter: 1913 loss: 7.22620416e-06
Iter: 1914 loss: 7.22295499e-06
Iter: 1915 loss: 7.24317579e-06
Iter: 1916 loss: 7.22206141e-06
Iter: 1917 loss: 7.21892047e-06
Iter: 1918 loss: 7.21884635e-06
Iter: 1919 loss: 7.21618335e-06
Iter: 1920 loss: 7.21778724e-06
Iter: 1921 loss: 7.21439937e-06
Iter: 1922 loss: 7.21093465e-06
Iter: 1923 loss: 7.2117773e-06
Iter: 1924 loss: 7.20831531e-06
Iter: 1925 loss: 7.20408343e-06
Iter: 1926 loss: 7.23250832e-06
Iter: 1927 loss: 7.20371736e-06
Iter: 1928 loss: 7.19980835e-06
Iter: 1929 loss: 7.19480886e-06
Iter: 1930 loss: 7.19441459e-06
Iter: 1931 loss: 7.19159e-06
Iter: 1932 loss: 7.19158197e-06
Iter: 1933 loss: 7.18871524e-06
Iter: 1934 loss: 7.19378204e-06
Iter: 1935 loss: 7.18745969e-06
Iter: 1936 loss: 7.18488764e-06
Iter: 1937 loss: 7.17862622e-06
Iter: 1938 loss: 7.25057816e-06
Iter: 1939 loss: 7.178126e-06
Iter: 1940 loss: 7.16962722e-06
Iter: 1941 loss: 7.18833917e-06
Iter: 1942 loss: 7.16635259e-06
Iter: 1943 loss: 7.16444083e-06
Iter: 1944 loss: 7.16265913e-06
Iter: 1945 loss: 7.15910619e-06
Iter: 1946 loss: 7.15877832e-06
Iter: 1947 loss: 7.15599117e-06
Iter: 1948 loss: 7.15306896e-06
Iter: 1949 loss: 7.16526847e-06
Iter: 1950 loss: 7.1524687e-06
Iter: 1951 loss: 7.14852831e-06
Iter: 1952 loss: 7.15839724e-06
Iter: 1953 loss: 7.14713042e-06
Iter: 1954 loss: 7.14413545e-06
Iter: 1955 loss: 7.1538152e-06
Iter: 1956 loss: 7.14324142e-06
Iter: 1957 loss: 7.14006046e-06
Iter: 1958 loss: 7.14018461e-06
Iter: 1959 loss: 7.13742065e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4
+ date
Mon Oct 26 09:18:44 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.4_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.4_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8e14488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8d71d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8dae0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8d15840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8d1f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8d15ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8ce3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8c9c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8c9c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8c6a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8c2c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8c157b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8c15ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8be96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8b94d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8b94840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8bb9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8b0a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8b167b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8ac9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8b21950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8ab9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8a70730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8a80840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8a80378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8a56378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de89a7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de89cc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de89cc0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de89778c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de894c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8934048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8934ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de8916510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4de88e7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dbf4212f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.11599249e-05
Iter: 2 loss: 3.90475179e-05
Iter: 3 loss: 3.87756481e-05
Iter: 4 loss: 3.76705211e-05
Iter: 5 loss: 3.81247337e-05
Iter: 6 loss: 3.69065747e-05
Iter: 7 loss: 3.62683204e-05
Iter: 8 loss: 4.51106607e-05
Iter: 9 loss: 3.62664978e-05
Iter: 10 loss: 3.58161415e-05
Iter: 11 loss: 3.59331025e-05
Iter: 12 loss: 3.54892181e-05
Iter: 13 loss: 3.49476977e-05
Iter: 14 loss: 3.51818126e-05
Iter: 15 loss: 3.45769295e-05
Iter: 16 loss: 3.4149045e-05
Iter: 17 loss: 3.41487503e-05
Iter: 18 loss: 3.37074707e-05
Iter: 19 loss: 3.28303322e-05
Iter: 20 loss: 4.93472035e-05
Iter: 21 loss: 3.28199421e-05
Iter: 22 loss: 3.20514555e-05
Iter: 23 loss: 3.34409742e-05
Iter: 24 loss: 3.17184968e-05
Iter: 25 loss: 3.15368598e-05
Iter: 26 loss: 3.14047938e-05
Iter: 27 loss: 3.10941359e-05
Iter: 28 loss: 3.08885865e-05
Iter: 29 loss: 3.07708033e-05
Iter: 30 loss: 3.0527066e-05
Iter: 31 loss: 3.01418859e-05
Iter: 32 loss: 3.01377186e-05
Iter: 33 loss: 2.9594943e-05
Iter: 34 loss: 3.25794244e-05
Iter: 35 loss: 2.951649e-05
Iter: 36 loss: 2.89807722e-05
Iter: 37 loss: 3.74526426e-05
Iter: 38 loss: 2.89807977e-05
Iter: 39 loss: 2.87175335e-05
Iter: 40 loss: 2.85412316e-05
Iter: 41 loss: 2.84429043e-05
Iter: 42 loss: 2.80429103e-05
Iter: 43 loss: 3.2428281e-05
Iter: 44 loss: 2.80342592e-05
Iter: 45 loss: 2.78459247e-05
Iter: 46 loss: 2.79012966e-05
Iter: 47 loss: 2.77107465e-05
Iter: 48 loss: 2.74875056e-05
Iter: 49 loss: 2.79317755e-05
Iter: 50 loss: 2.73959049e-05
Iter: 51 loss: 2.71902172e-05
Iter: 52 loss: 2.95393984e-05
Iter: 53 loss: 2.71866083e-05
Iter: 54 loss: 2.70472447e-05
Iter: 55 loss: 2.67382802e-05
Iter: 56 loss: 3.11410331e-05
Iter: 57 loss: 2.67228206e-05
Iter: 58 loss: 2.63823968e-05
Iter: 59 loss: 2.655762e-05
Iter: 60 loss: 2.61566529e-05
Iter: 61 loss: 2.62386347e-05
Iter: 62 loss: 2.60100605e-05
Iter: 63 loss: 2.5874433e-05
Iter: 64 loss: 2.56974108e-05
Iter: 65 loss: 2.56859821e-05
Iter: 66 loss: 2.5526826e-05
Iter: 67 loss: 2.54684455e-05
Iter: 68 loss: 2.53800572e-05
Iter: 69 loss: 2.52146638e-05
Iter: 70 loss: 2.71972312e-05
Iter: 71 loss: 2.52125938e-05
Iter: 72 loss: 2.50507e-05
Iter: 73 loss: 2.62483609e-05
Iter: 74 loss: 2.50376979e-05
Iter: 75 loss: 2.49447949e-05
Iter: 76 loss: 2.48513297e-05
Iter: 77 loss: 2.48324468e-05
Iter: 78 loss: 2.4618e-05
Iter: 79 loss: 2.56008734e-05
Iter: 80 loss: 2.45773153e-05
Iter: 81 loss: 2.44573621e-05
Iter: 82 loss: 2.44684616e-05
Iter: 83 loss: 2.43647119e-05
Iter: 84 loss: 2.4237961e-05
Iter: 85 loss: 2.55832983e-05
Iter: 86 loss: 2.42345886e-05
Iter: 87 loss: 2.41478701e-05
Iter: 88 loss: 2.45687625e-05
Iter: 89 loss: 2.41326743e-05
Iter: 90 loss: 2.40772024e-05
Iter: 91 loss: 2.39649107e-05
Iter: 92 loss: 2.60220149e-05
Iter: 93 loss: 2.39632445e-05
Iter: 94 loss: 2.38229586e-05
Iter: 95 loss: 2.39301808e-05
Iter: 96 loss: 2.37374115e-05
Iter: 97 loss: 2.37066051e-05
Iter: 98 loss: 2.36553369e-05
Iter: 99 loss: 2.35734951e-05
Iter: 100 loss: 2.3359069e-05
Iter: 101 loss: 2.49948062e-05
Iter: 102 loss: 2.33164392e-05
Iter: 103 loss: 2.3134613e-05
Iter: 104 loss: 2.42376118e-05
Iter: 105 loss: 2.31121812e-05
Iter: 106 loss: 2.30725163e-05
Iter: 107 loss: 2.30615296e-05
Iter: 108 loss: 2.30012593e-05
Iter: 109 loss: 2.29235266e-05
Iter: 110 loss: 2.29184e-05
Iter: 111 loss: 2.28531117e-05
Iter: 112 loss: 2.3746521e-05
Iter: 113 loss: 2.28527679e-05
Iter: 114 loss: 2.27856144e-05
Iter: 115 loss: 2.26995653e-05
Iter: 116 loss: 2.26932552e-05
Iter: 117 loss: 2.25876938e-05
Iter: 118 loss: 2.27768905e-05
Iter: 119 loss: 2.25419608e-05
Iter: 120 loss: 2.241334e-05
Iter: 121 loss: 2.34157524e-05
Iter: 122 loss: 2.24040632e-05
Iter: 123 loss: 2.23085608e-05
Iter: 124 loss: 2.24227606e-05
Iter: 125 loss: 2.22581075e-05
Iter: 126 loss: 2.21787959e-05
Iter: 127 loss: 2.2166405e-05
Iter: 128 loss: 2.21114642e-05
Iter: 129 loss: 2.20331058e-05
Iter: 130 loss: 2.22743329e-05
Iter: 131 loss: 2.20100628e-05
Iter: 132 loss: 2.19382509e-05
Iter: 133 loss: 2.19380308e-05
Iter: 134 loss: 2.18892455e-05
Iter: 135 loss: 2.17518336e-05
Iter: 136 loss: 2.24415689e-05
Iter: 137 loss: 2.17064116e-05
Iter: 138 loss: 2.15670443e-05
Iter: 139 loss: 2.25306849e-05
Iter: 140 loss: 2.15537912e-05
Iter: 141 loss: 2.15166219e-05
Iter: 142 loss: 2.14899374e-05
Iter: 143 loss: 2.14506017e-05
Iter: 144 loss: 2.13835337e-05
Iter: 145 loss: 2.13834064e-05
Iter: 146 loss: 2.13515523e-05
Iter: 147 loss: 2.13433523e-05
Iter: 148 loss: 2.13197418e-05
Iter: 149 loss: 2.12525356e-05
Iter: 150 loss: 2.15666805e-05
Iter: 151 loss: 2.12286504e-05
Iter: 152 loss: 2.11461174e-05
Iter: 153 loss: 2.22813869e-05
Iter: 154 loss: 2.11458864e-05
Iter: 155 loss: 2.10630533e-05
Iter: 156 loss: 2.1122265e-05
Iter: 157 loss: 2.10117196e-05
Iter: 158 loss: 2.09059e-05
Iter: 159 loss: 2.09077407e-05
Iter: 160 loss: 2.0821446e-05
Iter: 161 loss: 2.07139583e-05
Iter: 162 loss: 2.09697973e-05
Iter: 163 loss: 2.0675061e-05
Iter: 164 loss: 2.06232726e-05
Iter: 165 loss: 2.06230961e-05
Iter: 166 loss: 2.0568008e-05
Iter: 167 loss: 2.06476197e-05
Iter: 168 loss: 2.05410179e-05
Iter: 169 loss: 2.05037468e-05
Iter: 170 loss: 2.04208609e-05
Iter: 171 loss: 2.16019562e-05
Iter: 172 loss: 2.04166681e-05
Iter: 173 loss: 2.03452837e-05
Iter: 174 loss: 2.03448726e-05
Iter: 175 loss: 2.02582669e-05
Iter: 176 loss: 2.03470463e-05
Iter: 177 loss: 2.02100309e-05
Iter: 178 loss: 2.01579187e-05
Iter: 179 loss: 2.04597072e-05
Iter: 180 loss: 2.0151112e-05
Iter: 181 loss: 2.00921877e-05
Iter: 182 loss: 2.01438834e-05
Iter: 183 loss: 2.00578597e-05
Iter: 184 loss: 2.00191753e-05
Iter: 185 loss: 2.00459654e-05
Iter: 186 loss: 1.99951228e-05
Iter: 187 loss: 1.99594451e-05
Iter: 188 loss: 1.99591577e-05
Iter: 189 loss: 1.99338665e-05
Iter: 190 loss: 1.98914513e-05
Iter: 191 loss: 1.9891233e-05
Iter: 192 loss: 1.98211419e-05
Iter: 193 loss: 1.98111011e-05
Iter: 194 loss: 1.97617901e-05
Iter: 195 loss: 1.96826513e-05
Iter: 196 loss: 1.98078869e-05
Iter: 197 loss: 1.96460478e-05
Iter: 198 loss: 1.96149158e-05
Iter: 199 loss: 1.96011115e-05
Iter: 200 loss: 1.9558187e-05
Iter: 201 loss: 1.95106659e-05
Iter: 202 loss: 1.95039138e-05
Iter: 203 loss: 1.94640779e-05
Iter: 204 loss: 1.9429619e-05
Iter: 205 loss: 1.94189597e-05
Iter: 206 loss: 1.93928099e-05
Iter: 207 loss: 1.93823271e-05
Iter: 208 loss: 1.93407377e-05
Iter: 209 loss: 1.92730531e-05
Iter: 210 loss: 1.92727548e-05
Iter: 211 loss: 1.92305088e-05
Iter: 212 loss: 1.92287516e-05
Iter: 213 loss: 1.91918352e-05
Iter: 214 loss: 1.91063864e-05
Iter: 215 loss: 2.01706625e-05
Iter: 216 loss: 1.90999417e-05
Iter: 217 loss: 1.90427472e-05
Iter: 218 loss: 1.9772282e-05
Iter: 219 loss: 1.90422652e-05
Iter: 220 loss: 1.89944876e-05
Iter: 221 loss: 1.9364652e-05
Iter: 222 loss: 1.89909078e-05
Iter: 223 loss: 1.89642706e-05
Iter: 224 loss: 1.8943e-05
Iter: 225 loss: 1.89350467e-05
Iter: 226 loss: 1.889126e-05
Iter: 227 loss: 1.89330858e-05
Iter: 228 loss: 1.88663143e-05
Iter: 229 loss: 1.88077756e-05
Iter: 230 loss: 1.87634687e-05
Iter: 231 loss: 1.87446676e-05
Iter: 232 loss: 1.87312835e-05
Iter: 233 loss: 1.86973011e-05
Iter: 234 loss: 1.86642228e-05
Iter: 235 loss: 1.86093475e-05
Iter: 236 loss: 1.86092348e-05
Iter: 237 loss: 1.85674144e-05
Iter: 238 loss: 1.86034122e-05
Iter: 239 loss: 1.85429217e-05
Iter: 240 loss: 1.85365279e-05
Iter: 241 loss: 1.85178433e-05
Iter: 242 loss: 1.85025456e-05
Iter: 243 loss: 1.8470706e-05
Iter: 244 loss: 1.90118153e-05
Iter: 245 loss: 1.84698911e-05
Iter: 246 loss: 1.84305609e-05
Iter: 247 loss: 1.88225786e-05
Iter: 248 loss: 1.84294204e-05
Iter: 249 loss: 1.84033815e-05
Iter: 250 loss: 1.83347402e-05
Iter: 251 loss: 1.88197791e-05
Iter: 252 loss: 1.83195189e-05
Iter: 253 loss: 1.82901094e-05
Iter: 254 loss: 1.82753083e-05
Iter: 255 loss: 1.82361182e-05
Iter: 256 loss: 1.82181157e-05
Iter: 257 loss: 1.81987707e-05
Iter: 258 loss: 1.81665655e-05
Iter: 259 loss: 1.8264871e-05
Iter: 260 loss: 1.81569922e-05
Iter: 261 loss: 1.81221531e-05
Iter: 262 loss: 1.81196774e-05
Iter: 263 loss: 1.80934148e-05
Iter: 264 loss: 1.80546012e-05
Iter: 265 loss: 1.85180434e-05
Iter: 266 loss: 1.80541938e-05
Iter: 267 loss: 1.8014327e-05
Iter: 268 loss: 1.80512434e-05
Iter: 269 loss: 1.79912095e-05
Iter: 270 loss: 1.79516919e-05
Iter: 271 loss: 1.78712817e-05
Iter: 272 loss: 1.93367014e-05
Iter: 273 loss: 1.78701193e-05
Iter: 274 loss: 1.78588198e-05
Iter: 275 loss: 1.7831384e-05
Iter: 276 loss: 1.77953843e-05
Iter: 277 loss: 1.78067548e-05
Iter: 278 loss: 1.77696638e-05
Iter: 279 loss: 1.77514594e-05
Iter: 280 loss: 1.80283787e-05
Iter: 281 loss: 1.77514485e-05
Iter: 282 loss: 1.77349575e-05
Iter: 283 loss: 1.76991234e-05
Iter: 284 loss: 1.82582844e-05
Iter: 285 loss: 1.76979192e-05
Iter: 286 loss: 1.76647191e-05
Iter: 287 loss: 1.7836428e-05
Iter: 288 loss: 1.76595931e-05
Iter: 289 loss: 1.76204812e-05
Iter: 290 loss: 1.77805505e-05
Iter: 291 loss: 1.76117683e-05
Iter: 292 loss: 1.75813184e-05
Iter: 293 loss: 1.75356145e-05
Iter: 294 loss: 1.75346504e-05
Iter: 295 loss: 1.74805173e-05
Iter: 296 loss: 1.77782749e-05
Iter: 297 loss: 1.74728702e-05
Iter: 298 loss: 1.74354627e-05
Iter: 299 loss: 1.75950554e-05
Iter: 300 loss: 1.74278084e-05
Iter: 301 loss: 1.73999397e-05
Iter: 302 loss: 1.78128357e-05
Iter: 303 loss: 1.73999906e-05
Iter: 304 loss: 1.73838889e-05
Iter: 305 loss: 1.73546759e-05
Iter: 306 loss: 1.80377119e-05
Iter: 307 loss: 1.73546487e-05
Iter: 308 loss: 1.73255066e-05
Iter: 309 loss: 1.74726592e-05
Iter: 310 loss: 1.73207336e-05
Iter: 311 loss: 1.72813125e-05
Iter: 312 loss: 1.7397404e-05
Iter: 313 loss: 1.72694308e-05
Iter: 314 loss: 1.72400323e-05
Iter: 315 loss: 1.72639229e-05
Iter: 316 loss: 1.72225155e-05
Iter: 317 loss: 1.71774191e-05
Iter: 318 loss: 1.73374974e-05
Iter: 319 loss: 1.71659558e-05
Iter: 320 loss: 1.71422e-05
Iter: 321 loss: 1.71186384e-05
Iter: 322 loss: 1.71135835e-05
Iter: 323 loss: 1.70900312e-05
Iter: 324 loss: 1.70874773e-05
Iter: 325 loss: 1.70726325e-05
Iter: 326 loss: 1.70522071e-05
Iter: 327 loss: 1.70511794e-05
Iter: 328 loss: 1.70235089e-05
Iter: 329 loss: 1.70645289e-05
Iter: 330 loss: 1.70101848e-05
Iter: 331 loss: 1.69741e-05
Iter: 332 loss: 1.69663344e-05
Iter: 333 loss: 1.69426348e-05
Iter: 334 loss: 1.69088562e-05
Iter: 335 loss: 1.69069481e-05
Iter: 336 loss: 1.68734259e-05
Iter: 337 loss: 1.68324368e-05
Iter: 338 loss: 1.68286606e-05
Iter: 339 loss: 1.67961698e-05
Iter: 340 loss: 1.6999802e-05
Iter: 341 loss: 1.67924882e-05
Iter: 342 loss: 1.67751277e-05
Iter: 343 loss: 1.67748149e-05
Iter: 344 loss: 1.67630169e-05
Iter: 345 loss: 1.67389881e-05
Iter: 346 loss: 1.71833271e-05
Iter: 347 loss: 1.67386224e-05
Iter: 348 loss: 1.67112485e-05
Iter: 349 loss: 1.70433341e-05
Iter: 350 loss: 1.67109047e-05
Iter: 351 loss: 1.66940936e-05
Iter: 352 loss: 1.66575446e-05
Iter: 353 loss: 1.7216722e-05
Iter: 354 loss: 1.66559748e-05
Iter: 355 loss: 1.66312093e-05
Iter: 356 loss: 1.66296377e-05
Iter: 357 loss: 1.66038644e-05
Iter: 358 loss: 1.65646743e-05
Iter: 359 loss: 1.6563914e-05
Iter: 360 loss: 1.65282909e-05
Iter: 361 loss: 1.67660692e-05
Iter: 362 loss: 1.65245474e-05
Iter: 363 loss: 1.6500675e-05
Iter: 364 loss: 1.65174097e-05
Iter: 365 loss: 1.64856483e-05
Iter: 366 loss: 1.64661851e-05
Iter: 367 loss: 1.67372054e-05
Iter: 368 loss: 1.64660323e-05
Iter: 369 loss: 1.64455596e-05
Iter: 370 loss: 1.64780122e-05
Iter: 371 loss: 1.64359826e-05
Iter: 372 loss: 1.64153316e-05
Iter: 373 loss: 1.63753284e-05
Iter: 374 loss: 1.72017644e-05
Iter: 375 loss: 1.63753029e-05
Iter: 376 loss: 1.63486093e-05
Iter: 377 loss: 1.63442455e-05
Iter: 378 loss: 1.63187979e-05
Iter: 379 loss: 1.62993038e-05
Iter: 380 loss: 1.62910492e-05
Iter: 381 loss: 1.62695233e-05
Iter: 382 loss: 1.62694669e-05
Iter: 383 loss: 1.62527758e-05
Iter: 384 loss: 1.6220607e-05
Iter: 385 loss: 1.68972656e-05
Iter: 386 loss: 1.62205943e-05
Iter: 387 loss: 1.62065517e-05
Iter: 388 loss: 1.62049982e-05
Iter: 389 loss: 1.61910284e-05
Iter: 390 loss: 1.62008e-05
Iter: 391 loss: 1.61822045e-05
Iter: 392 loss: 1.61671e-05
Iter: 393 loss: 1.61485623e-05
Iter: 394 loss: 1.61468633e-05
Iter: 395 loss: 1.61104e-05
Iter: 396 loss: 1.61240914e-05
Iter: 397 loss: 1.60850541e-05
Iter: 398 loss: 1.60477102e-05
Iter: 399 loss: 1.63833374e-05
Iter: 400 loss: 1.60458803e-05
Iter: 401 loss: 1.60183681e-05
Iter: 402 loss: 1.62961114e-05
Iter: 403 loss: 1.60175587e-05
Iter: 404 loss: 1.60019245e-05
Iter: 405 loss: 1.59816373e-05
Iter: 406 loss: 1.59804567e-05
Iter: 407 loss: 1.5966114e-05
Iter: 408 loss: 1.59653682e-05
Iter: 409 loss: 1.5950216e-05
Iter: 410 loss: 1.59389274e-05
Iter: 411 loss: 1.59341471e-05
Iter: 412 loss: 1.59152041e-05
Iter: 413 loss: 1.60406053e-05
Iter: 414 loss: 1.59132887e-05
Iter: 415 loss: 1.58926759e-05
Iter: 416 loss: 1.58799412e-05
Iter: 417 loss: 1.5871512e-05
Iter: 418 loss: 1.58475814e-05
Iter: 419 loss: 1.5893107e-05
Iter: 420 loss: 1.58376097e-05
Iter: 421 loss: 1.58040712e-05
Iter: 422 loss: 1.59713127e-05
Iter: 423 loss: 1.57982831e-05
Iter: 424 loss: 1.57788982e-05
Iter: 425 loss: 1.57841096e-05
Iter: 426 loss: 1.57649374e-05
Iter: 427 loss: 1.57459144e-05
Iter: 428 loss: 1.58186886e-05
Iter: 429 loss: 1.57412778e-05
Iter: 430 loss: 1.57246741e-05
Iter: 431 loss: 1.57308496e-05
Iter: 432 loss: 1.57130635e-05
Iter: 433 loss: 1.56942806e-05
Iter: 434 loss: 1.5694286e-05
Iter: 435 loss: 1.56796596e-05
Iter: 436 loss: 1.56524948e-05
Iter: 437 loss: 1.62791657e-05
Iter: 438 loss: 1.5652533e-05
Iter: 439 loss: 1.56285769e-05
Iter: 440 loss: 1.59164501e-05
Iter: 441 loss: 1.56282e-05
Iter: 442 loss: 1.56038495e-05
Iter: 443 loss: 1.56565911e-05
Iter: 444 loss: 1.55945163e-05
Iter: 445 loss: 1.55775797e-05
Iter: 446 loss: 1.56299084e-05
Iter: 447 loss: 1.55726848e-05
Iter: 448 loss: 1.55536673e-05
Iter: 449 loss: 1.56056958e-05
Iter: 450 loss: 1.5547459e-05
Iter: 451 loss: 1.5536003e-05
Iter: 452 loss: 1.55361431e-05
Iter: 453 loss: 1.55270245e-05
Iter: 454 loss: 1.55133439e-05
Iter: 455 loss: 1.57192771e-05
Iter: 456 loss: 1.55133494e-05
Iter: 457 loss: 1.5503676e-05
Iter: 458 loss: 1.5481457e-05
Iter: 459 loss: 1.57566174e-05
Iter: 460 loss: 1.54797599e-05
Iter: 461 loss: 1.5449803e-05
Iter: 462 loss: 1.5565618e-05
Iter: 463 loss: 1.54428126e-05
Iter: 464 loss: 1.54142672e-05
Iter: 465 loss: 1.54378868e-05
Iter: 466 loss: 1.5397447e-05
Iter: 467 loss: 1.53837209e-05
Iter: 468 loss: 1.53801266e-05
Iter: 469 loss: 1.53658566e-05
Iter: 470 loss: 1.53569781e-05
Iter: 471 loss: 1.53513174e-05
Iter: 472 loss: 1.53382207e-05
Iter: 473 loss: 1.5410973e-05
Iter: 474 loss: 1.53363726e-05
Iter: 475 loss: 1.53227775e-05
Iter: 476 loss: 1.53872152e-05
Iter: 477 loss: 1.53202873e-05
Iter: 478 loss: 1.53093752e-05
Iter: 479 loss: 1.53010405e-05
Iter: 480 loss: 1.52975736e-05
Iter: 481 loss: 1.52763569e-05
Iter: 482 loss: 1.53763631e-05
Iter: 483 loss: 1.52724097e-05
Iter: 484 loss: 1.52562461e-05
Iter: 485 loss: 1.52297653e-05
Iter: 486 loss: 1.5229778e-05
Iter: 487 loss: 1.52152388e-05
Iter: 488 loss: 1.52113516e-05
Iter: 489 loss: 1.51991599e-05
Iter: 490 loss: 1.51875238e-05
Iter: 491 loss: 1.51848399e-05
Iter: 492 loss: 1.51710174e-05
Iter: 493 loss: 1.52177072e-05
Iter: 494 loss: 1.51673266e-05
Iter: 495 loss: 1.51527274e-05
Iter: 496 loss: 1.51465338e-05
Iter: 497 loss: 1.51389013e-05
Iter: 498 loss: 1.51217582e-05
Iter: 499 loss: 1.5377449e-05
Iter: 500 loss: 1.51216818e-05
Iter: 501 loss: 1.51030727e-05
Iter: 502 loss: 1.51044787e-05
Iter: 503 loss: 1.50886153e-05
Iter: 504 loss: 1.50666965e-05
Iter: 505 loss: 1.50837259e-05
Iter: 506 loss: 1.5053507e-05
Iter: 507 loss: 1.50285123e-05
Iter: 508 loss: 1.53703368e-05
Iter: 509 loss: 1.50284031e-05
Iter: 510 loss: 1.50141641e-05
Iter: 511 loss: 1.50101459e-05
Iter: 512 loss: 1.50015912e-05
Iter: 513 loss: 1.49854159e-05
Iter: 514 loss: 1.51804734e-05
Iter: 515 loss: 1.49852249e-05
Iter: 516 loss: 1.49759207e-05
Iter: 517 loss: 1.49629486e-05
Iter: 518 loss: 1.49624093e-05
Iter: 519 loss: 1.49534462e-05
Iter: 520 loss: 1.49532734e-05
Iter: 521 loss: 1.49441339e-05
Iter: 522 loss: 1.4930919e-05
Iter: 523 loss: 1.49305633e-05
Iter: 524 loss: 1.49118587e-05
Iter: 525 loss: 1.49118277e-05
Iter: 526 loss: 1.48969939e-05
Iter: 527 loss: 1.48675945e-05
Iter: 528 loss: 1.49615826e-05
Iter: 529 loss: 1.48594399e-05
Iter: 530 loss: 1.48400695e-05
Iter: 531 loss: 1.49672805e-05
Iter: 532 loss: 1.48381441e-05
Iter: 533 loss: 1.48189411e-05
Iter: 534 loss: 1.49517409e-05
Iter: 535 loss: 1.48171939e-05
Iter: 536 loss: 1.48076797e-05
Iter: 537 loss: 1.48068748e-05
Iter: 538 loss: 1.47997316e-05
Iter: 539 loss: 1.47893652e-05
Iter: 540 loss: 1.49523712e-05
Iter: 541 loss: 1.47893306e-05
Iter: 542 loss: 1.47809697e-05
Iter: 543 loss: 1.47665651e-05
Iter: 544 loss: 1.47665405e-05
Iter: 545 loss: 1.47500223e-05
Iter: 546 loss: 1.49287371e-05
Iter: 547 loss: 1.47496758e-05
Iter: 548 loss: 1.47361588e-05
Iter: 549 loss: 1.47146402e-05
Iter: 550 loss: 1.47143437e-05
Iter: 551 loss: 1.46990242e-05
Iter: 552 loss: 1.46990042e-05
Iter: 553 loss: 1.46848561e-05
Iter: 554 loss: 1.47034825e-05
Iter: 555 loss: 1.46776656e-05
Iter: 556 loss: 1.466715e-05
Iter: 557 loss: 1.46702805e-05
Iter: 558 loss: 1.4659694e-05
Iter: 559 loss: 1.46462062e-05
Iter: 560 loss: 1.46851135e-05
Iter: 561 loss: 1.46420534e-05
Iter: 562 loss: 1.46272614e-05
Iter: 563 loss: 1.46313214e-05
Iter: 564 loss: 1.46166194e-05
Iter: 565 loss: 1.45996037e-05
Iter: 566 loss: 1.45995955e-05
Iter: 567 loss: 1.45883932e-05
Iter: 568 loss: 1.45696031e-05
Iter: 569 loss: 1.4569574e-05
Iter: 570 loss: 1.45585691e-05
Iter: 571 loss: 1.45566555e-05
Iter: 572 loss: 1.45477652e-05
Iter: 573 loss: 1.45422982e-05
Iter: 574 loss: 1.45387385e-05
Iter: 575 loss: 1.45294334e-05
Iter: 576 loss: 1.46353614e-05
Iter: 577 loss: 1.4529247e-05
Iter: 578 loss: 1.45216691e-05
Iter: 579 loss: 1.45130771e-05
Iter: 580 loss: 1.45118966e-05
Iter: 581 loss: 1.45006943e-05
Iter: 582 loss: 1.45284685e-05
Iter: 583 loss: 1.44967489e-05
Iter: 584 loss: 1.4481001e-05
Iter: 585 loss: 1.45281883e-05
Iter: 586 loss: 1.44761925e-05
Iter: 587 loss: 1.44626465e-05
Iter: 588 loss: 1.44422083e-05
Iter: 589 loss: 1.44417209e-05
Iter: 590 loss: 1.44184223e-05
Iter: 591 loss: 1.45695194e-05
Iter: 592 loss: 1.44158357e-05
Iter: 593 loss: 1.43984653e-05
Iter: 594 loss: 1.44255155e-05
Iter: 595 loss: 1.43904635e-05
Iter: 596 loss: 1.43818506e-05
Iter: 597 loss: 1.43802135e-05
Iter: 598 loss: 1.43718125e-05
Iter: 599 loss: 1.43585312e-05
Iter: 600 loss: 1.43584466e-05
Iter: 601 loss: 1.43464486e-05
Iter: 602 loss: 1.45037639e-05
Iter: 603 loss: 1.43463712e-05
Iter: 604 loss: 1.4333049e-05
Iter: 605 loss: 1.43226607e-05
Iter: 606 loss: 1.4318548e-05
Iter: 607 loss: 1.43020743e-05
Iter: 608 loss: 1.44215301e-05
Iter: 609 loss: 1.43007292e-05
Iter: 610 loss: 1.42852514e-05
Iter: 611 loss: 1.42976241e-05
Iter: 612 loss: 1.42758217e-05
Iter: 613 loss: 1.4263187e-05
Iter: 614 loss: 1.42914832e-05
Iter: 615 loss: 1.42583058e-05
Iter: 616 loss: 1.42459958e-05
Iter: 617 loss: 1.43793113e-05
Iter: 618 loss: 1.42455947e-05
Iter: 619 loss: 1.42376921e-05
Iter: 620 loss: 1.42271929e-05
Iter: 621 loss: 1.42266799e-05
Iter: 622 loss: 1.42120725e-05
Iter: 623 loss: 1.4230407e-05
Iter: 624 loss: 1.42046156e-05
Iter: 625 loss: 1.4182282e-05
Iter: 626 loss: 1.41843484e-05
Iter: 627 loss: 1.41649589e-05
Iter: 628 loss: 1.41454029e-05
Iter: 629 loss: 1.41451392e-05
Iter: 630 loss: 1.41243454e-05
Iter: 631 loss: 1.4125575e-05
Iter: 632 loss: 1.41081764e-05
Iter: 633 loss: 1.40950069e-05
Iter: 634 loss: 1.42566887e-05
Iter: 635 loss: 1.40948932e-05
Iter: 636 loss: 1.40832944e-05
Iter: 637 loss: 1.41072705e-05
Iter: 638 loss: 1.4078686e-05
Iter: 639 loss: 1.40703578e-05
Iter: 640 loss: 1.40888606e-05
Iter: 641 loss: 1.40671327e-05
Iter: 642 loss: 1.40564443e-05
Iter: 643 loss: 1.40829306e-05
Iter: 644 loss: 1.40526454e-05
Iter: 645 loss: 1.40437696e-05
Iter: 646 loss: 1.40337834e-05
Iter: 647 loss: 1.403243e-05
Iter: 648 loss: 1.40176553e-05
Iter: 649 loss: 1.42208128e-05
Iter: 650 loss: 1.40176317e-05
Iter: 651 loss: 1.40059765e-05
Iter: 652 loss: 1.39858148e-05
Iter: 653 loss: 1.39857448e-05
Iter: 654 loss: 1.39649237e-05
Iter: 655 loss: 1.40587763e-05
Iter: 656 loss: 1.39609556e-05
Iter: 657 loss: 1.39451658e-05
Iter: 658 loss: 1.39970607e-05
Iter: 659 loss: 1.39408685e-05
Iter: 660 loss: 1.39288877e-05
Iter: 661 loss: 1.4004836e-05
Iter: 662 loss: 1.39275071e-05
Iter: 663 loss: 1.39148488e-05
Iter: 664 loss: 1.39755939e-05
Iter: 665 loss: 1.3912635e-05
Iter: 666 loss: 1.39044305e-05
Iter: 667 loss: 1.38985697e-05
Iter: 668 loss: 1.38956557e-05
Iter: 669 loss: 1.38765045e-05
Iter: 670 loss: 1.39435087e-05
Iter: 671 loss: 1.38714695e-05
Iter: 672 loss: 1.38566793e-05
Iter: 673 loss: 1.38623927e-05
Iter: 674 loss: 1.38464184e-05
Iter: 675 loss: 1.3828344e-05
Iter: 676 loss: 1.39890853e-05
Iter: 677 loss: 1.38273936e-05
Iter: 678 loss: 1.38175074e-05
Iter: 679 loss: 1.38090354e-05
Iter: 680 loss: 1.38061769e-05
Iter: 681 loss: 1.3796217e-05
Iter: 682 loss: 1.37959523e-05
Iter: 683 loss: 1.37875113e-05
Iter: 684 loss: 1.37780016e-05
Iter: 685 loss: 1.37766119e-05
Iter: 686 loss: 1.37635416e-05
Iter: 687 loss: 1.37695915e-05
Iter: 688 loss: 1.37545685e-05
Iter: 689 loss: 1.37345214e-05
Iter: 690 loss: 1.37598854e-05
Iter: 691 loss: 1.3724125e-05
Iter: 692 loss: 1.37012803e-05
Iter: 693 loss: 1.37975348e-05
Iter: 694 loss: 1.36964081e-05
Iter: 695 loss: 1.36800736e-05
Iter: 696 loss: 1.36798226e-05
Iter: 697 loss: 1.36714807e-05
Iter: 698 loss: 1.36649105e-05
Iter: 699 loss: 1.36622821e-05
Iter: 700 loss: 1.36526342e-05
Iter: 701 loss: 1.3652405e-05
Iter: 702 loss: 1.36465205e-05
Iter: 703 loss: 1.36352155e-05
Iter: 704 loss: 1.38974074e-05
Iter: 705 loss: 1.36352701e-05
Iter: 706 loss: 1.3621453e-05
Iter: 707 loss: 1.37544848e-05
Iter: 708 loss: 1.36208228e-05
Iter: 709 loss: 1.36107774e-05
Iter: 710 loss: 1.35958016e-05
Iter: 711 loss: 1.3595416e-05
Iter: 712 loss: 1.35823284e-05
Iter: 713 loss: 1.35823302e-05
Iter: 714 loss: 1.35697983e-05
Iter: 715 loss: 1.3567721e-05
Iter: 716 loss: 1.35589435e-05
Iter: 717 loss: 1.35461487e-05
Iter: 718 loss: 1.35619157e-05
Iter: 719 loss: 1.3539372e-05
Iter: 720 loss: 1.3526198e-05
Iter: 721 loss: 1.35782111e-05
Iter: 722 loss: 1.35232131e-05
Iter: 723 loss: 1.35091313e-05
Iter: 724 loss: 1.35055452e-05
Iter: 725 loss: 1.34967504e-05
Iter: 726 loss: 1.34842776e-05
Iter: 727 loss: 1.34829234e-05
Iter: 728 loss: 1.34719139e-05
Iter: 729 loss: 1.34524507e-05
Iter: 730 loss: 1.39314725e-05
Iter: 731 loss: 1.34523962e-05
Iter: 732 loss: 1.34390884e-05
Iter: 733 loss: 1.34379243e-05
Iter: 734 loss: 1.34283455e-05
Iter: 735 loss: 1.34168749e-05
Iter: 736 loss: 1.34156508e-05
Iter: 737 loss: 1.34055335e-05
Iter: 738 loss: 1.34055e-05
Iter: 739 loss: 1.33980266e-05
Iter: 740 loss: 1.33894864e-05
Iter: 741 loss: 1.33883532e-05
Iter: 742 loss: 1.337712e-05
Iter: 743 loss: 1.34209968e-05
Iter: 744 loss: 1.3374487e-05
Iter: 745 loss: 1.33603298e-05
Iter: 746 loss: 1.3371553e-05
Iter: 747 loss: 1.33518806e-05
Iter: 748 loss: 1.3336854e-05
Iter: 749 loss: 1.33217745e-05
Iter: 750 loss: 1.33186331e-05
Iter: 751 loss: 1.32940122e-05
Iter: 752 loss: 1.33749663e-05
Iter: 753 loss: 1.32872183e-05
Iter: 754 loss: 1.32704081e-05
Iter: 755 loss: 1.33653739e-05
Iter: 756 loss: 1.32681462e-05
Iter: 757 loss: 1.32584428e-05
Iter: 758 loss: 1.33902e-05
Iter: 759 loss: 1.32585237e-05
Iter: 760 loss: 1.32485548e-05
Iter: 761 loss: 1.32567593e-05
Iter: 762 loss: 1.32427413e-05
Iter: 763 loss: 1.32345276e-05
Iter: 764 loss: 1.32788882e-05
Iter: 765 loss: 1.32332716e-05
Iter: 766 loss: 1.32218884e-05
Iter: 767 loss: 1.32063e-05
Iter: 768 loss: 1.32055793e-05
Iter: 769 loss: 1.31874567e-05
Iter: 770 loss: 1.33093845e-05
Iter: 771 loss: 1.31856696e-05
Iter: 772 loss: 1.3168361e-05
Iter: 773 loss: 1.32033119e-05
Iter: 774 loss: 1.31614443e-05
Iter: 775 loss: 1.31502147e-05
Iter: 776 loss: 1.31797897e-05
Iter: 777 loss: 1.31466231e-05
Iter: 778 loss: 1.31350116e-05
Iter: 779 loss: 1.32179357e-05
Iter: 780 loss: 1.31339912e-05
Iter: 781 loss: 1.31270517e-05
Iter: 782 loss: 1.31182223e-05
Iter: 783 loss: 1.31173892e-05
Iter: 784 loss: 1.31054094e-05
Iter: 785 loss: 1.31327633e-05
Iter: 786 loss: 1.31009019e-05
Iter: 787 loss: 1.30867584e-05
Iter: 788 loss: 1.30837334e-05
Iter: 789 loss: 1.30745611e-05
Iter: 790 loss: 1.30526987e-05
Iter: 791 loss: 1.31819706e-05
Iter: 792 loss: 1.30498593e-05
Iter: 793 loss: 1.30300359e-05
Iter: 794 loss: 1.32475343e-05
Iter: 795 loss: 1.30295684e-05
Iter: 796 loss: 1.30197368e-05
Iter: 797 loss: 1.30294156e-05
Iter: 798 loss: 1.30141852e-05
Iter: 799 loss: 1.30028457e-05
Iter: 800 loss: 1.31089128e-05
Iter: 801 loss: 1.30022745e-05
Iter: 802 loss: 1.29973105e-05
Iter: 803 loss: 1.29943819e-05
Iter: 804 loss: 1.29923246e-05
Iter: 805 loss: 1.29828159e-05
Iter: 806 loss: 1.30223762e-05
Iter: 807 loss: 1.2980654e-05
Iter: 808 loss: 1.29726195e-05
Iter: 809 loss: 1.29664058e-05
Iter: 810 loss: 1.29637237e-05
Iter: 811 loss: 1.29516357e-05
Iter: 812 loss: 1.30863809e-05
Iter: 813 loss: 1.29513992e-05
Iter: 814 loss: 1.29411856e-05
Iter: 815 loss: 1.29228365e-05
Iter: 816 loss: 1.33753501e-05
Iter: 817 loss: 1.29229029e-05
Iter: 818 loss: 1.29031014e-05
Iter: 819 loss: 1.29667642e-05
Iter: 820 loss: 1.28976408e-05
Iter: 821 loss: 1.28801348e-05
Iter: 822 loss: 1.29431555e-05
Iter: 823 loss: 1.28756983e-05
Iter: 824 loss: 1.2862698e-05
Iter: 825 loss: 1.29135269e-05
Iter: 826 loss: 1.28596e-05
Iter: 827 loss: 1.28504271e-05
Iter: 828 loss: 1.28505144e-05
Iter: 829 loss: 1.28427246e-05
Iter: 830 loss: 1.28299007e-05
Iter: 831 loss: 1.28298498e-05
Iter: 832 loss: 1.28159145e-05
Iter: 833 loss: 1.30364006e-05
Iter: 834 loss: 1.28158499e-05
Iter: 835 loss: 1.28058173e-05
Iter: 836 loss: 1.27891062e-05
Iter: 837 loss: 1.27890935e-05
Iter: 838 loss: 1.27740332e-05
Iter: 839 loss: 1.27739459e-05
Iter: 840 loss: 1.27636922e-05
Iter: 841 loss: 1.27530084e-05
Iter: 842 loss: 1.27510066e-05
Iter: 843 loss: 1.27405e-05
Iter: 844 loss: 1.27404946e-05
Iter: 845 loss: 1.27327912e-05
Iter: 846 loss: 1.27361554e-05
Iter: 847 loss: 1.27274225e-05
Iter: 848 loss: 1.27199091e-05
Iter: 849 loss: 1.27120829e-05
Iter: 850 loss: 1.2710605e-05
Iter: 851 loss: 1.2694637e-05
Iter: 852 loss: 1.26966952e-05
Iter: 853 loss: 1.26824125e-05
Iter: 854 loss: 1.26610112e-05
Iter: 855 loss: 1.27405565e-05
Iter: 856 loss: 1.26557588e-05
Iter: 857 loss: 1.26416617e-05
Iter: 858 loss: 1.26415061e-05
Iter: 859 loss: 1.2628494e-05
Iter: 860 loss: 1.26477535e-05
Iter: 861 loss: 1.26221857e-05
Iter: 862 loss: 1.2614908e-05
Iter: 863 loss: 1.27061876e-05
Iter: 864 loss: 1.2614777e-05
Iter: 865 loss: 1.2607803e-05
Iter: 866 loss: 1.26010236e-05
Iter: 867 loss: 1.25995348e-05
Iter: 868 loss: 1.25911502e-05
Iter: 869 loss: 1.26538544e-05
Iter: 870 loss: 1.25904871e-05
Iter: 871 loss: 1.25816186e-05
Iter: 872 loss: 1.25808783e-05
Iter: 873 loss: 1.25743427e-05
Iter: 874 loss: 1.25638489e-05
Iter: 875 loss: 1.2592669e-05
Iter: 876 loss: 1.25604074e-05
Iter: 877 loss: 1.25455936e-05
Iter: 878 loss: 1.25707847e-05
Iter: 879 loss: 1.25390161e-05
Iter: 880 loss: 1.2525802e-05
Iter: 881 loss: 1.25345505e-05
Iter: 882 loss: 1.25175538e-05
Iter: 883 loss: 1.2507262e-05
Iter: 884 loss: 1.25431116e-05
Iter: 885 loss: 1.25044207e-05
Iter: 886 loss: 1.24938797e-05
Iter: 887 loss: 1.24850558e-05
Iter: 888 loss: 1.24820817e-05
Iter: 889 loss: 1.24705457e-05
Iter: 890 loss: 1.24705793e-05
Iter: 891 loss: 1.24594553e-05
Iter: 892 loss: 1.25128799e-05
Iter: 893 loss: 1.24575727e-05
Iter: 894 loss: 1.24479411e-05
Iter: 895 loss: 1.24478574e-05
Iter: 896 loss: 1.24404069e-05
Iter: 897 loss: 1.24251319e-05
Iter: 898 loss: 1.2531128e-05
Iter: 899 loss: 1.24238941e-05
Iter: 900 loss: 1.24170519e-05
Iter: 901 loss: 1.24204416e-05
Iter: 902 loss: 1.24126473e-05
Iter: 903 loss: 1.24041762e-05
Iter: 904 loss: 1.24648413e-05
Iter: 905 loss: 1.24033195e-05
Iter: 906 loss: 1.23980453e-05
Iter: 907 loss: 1.23958034e-05
Iter: 908 loss: 1.23928439e-05
Iter: 909 loss: 1.23848113e-05
Iter: 910 loss: 1.24508324e-05
Iter: 911 loss: 1.23844256e-05
Iter: 912 loss: 1.23776827e-05
Iter: 913 loss: 1.23644804e-05
Iter: 914 loss: 1.2617108e-05
Iter: 915 loss: 1.23641812e-05
Iter: 916 loss: 1.23468553e-05
Iter: 917 loss: 1.23780155e-05
Iter: 918 loss: 1.23391656e-05
Iter: 919 loss: 1.23220452e-05
Iter: 920 loss: 1.23759164e-05
Iter: 921 loss: 1.23171149e-05
Iter: 922 loss: 1.23032805e-05
Iter: 923 loss: 1.23430136e-05
Iter: 924 loss: 1.22988013e-05
Iter: 925 loss: 1.22959136e-05
Iter: 926 loss: 1.22925849e-05
Iter: 927 loss: 1.22880883e-05
Iter: 928 loss: 1.22824722e-05
Iter: 929 loss: 1.22821248e-05
Iter: 930 loss: 1.22741985e-05
Iter: 931 loss: 1.23507471e-05
Iter: 932 loss: 1.22738575e-05
Iter: 933 loss: 1.22664305e-05
Iter: 934 loss: 1.22529427e-05
Iter: 935 loss: 1.25772822e-05
Iter: 936 loss: 1.22529382e-05
Iter: 937 loss: 1.22430156e-05
Iter: 938 loss: 1.22427418e-05
Iter: 939 loss: 1.2235254e-05
Iter: 940 loss: 1.2227214e-05
Iter: 941 loss: 1.22259735e-05
Iter: 942 loss: 1.22164729e-05
Iter: 943 loss: 1.23369182e-05
Iter: 944 loss: 1.22163256e-05
Iter: 945 loss: 1.22087431e-05
Iter: 946 loss: 1.22136571e-05
Iter: 947 loss: 1.22041174e-05
Iter: 948 loss: 1.21973371e-05
Iter: 949 loss: 1.21996491e-05
Iter: 950 loss: 1.21926532e-05
Iter: 951 loss: 1.21843523e-05
Iter: 952 loss: 1.21779904e-05
Iter: 953 loss: 1.2175301e-05
Iter: 954 loss: 1.21598878e-05
Iter: 955 loss: 1.21837656e-05
Iter: 956 loss: 1.21527009e-05
Iter: 957 loss: 1.21356888e-05
Iter: 958 loss: 1.22448009e-05
Iter: 959 loss: 1.21338871e-05
Iter: 960 loss: 1.21319899e-05
Iter: 961 loss: 1.21280318e-05
Iter: 962 loss: 1.21241e-05
Iter: 963 loss: 1.21177145e-05
Iter: 964 loss: 1.21175945e-05
Iter: 965 loss: 1.21105932e-05
Iter: 966 loss: 1.2214341e-05
Iter: 967 loss: 1.21104549e-05
Iter: 968 loss: 1.21055473e-05
Iter: 969 loss: 1.21013745e-05
Iter: 970 loss: 1.20999775e-05
Iter: 971 loss: 1.2093984e-05
Iter: 972 loss: 1.21470603e-05
Iter: 973 loss: 1.20937357e-05
Iter: 974 loss: 1.20870955e-05
Iter: 975 loss: 1.20765317e-05
Iter: 976 loss: 1.20764089e-05
Iter: 977 loss: 1.20668929e-05
Iter: 978 loss: 1.21771791e-05
Iter: 979 loss: 1.20667437e-05
Iter: 980 loss: 1.205849e-05
Iter: 981 loss: 1.20851346e-05
Iter: 982 loss: 1.20560835e-05
Iter: 983 loss: 1.20502882e-05
Iter: 984 loss: 1.20483601e-05
Iter: 985 loss: 1.20449831e-05
Iter: 986 loss: 1.20363766e-05
Iter: 987 loss: 1.2031297e-05
Iter: 988 loss: 1.20276454e-05
Iter: 989 loss: 1.20121758e-05
Iter: 990 loss: 1.20272653e-05
Iter: 991 loss: 1.20033474e-05
Iter: 992 loss: 1.19838387e-05
Iter: 993 loss: 1.20610703e-05
Iter: 994 loss: 1.19794295e-05
Iter: 995 loss: 1.19686883e-05
Iter: 996 loss: 1.21404792e-05
Iter: 997 loss: 1.1968672e-05
Iter: 998 loss: 1.19611759e-05
Iter: 999 loss: 1.20721324e-05
Iter: 1000 loss: 1.19611177e-05
Iter: 1001 loss: 1.1957778e-05
Iter: 1002 loss: 1.1954784e-05
Iter: 1003 loss: 1.19538945e-05
Iter: 1004 loss: 1.19466149e-05
Iter: 1005 loss: 1.1976259e-05
Iter: 1006 loss: 1.1945036e-05
Iter: 1007 loss: 1.19385113e-05
Iter: 1008 loss: 1.19281358e-05
Iter: 1009 loss: 1.19280521e-05
Iter: 1010 loss: 1.19188189e-05
Iter: 1011 loss: 1.19187207e-05
Iter: 1012 loss: 1.19089045e-05
Iter: 1013 loss: 1.1899383e-05
Iter: 1014 loss: 1.18972284e-05
Iter: 1015 loss: 1.1885305e-05
Iter: 1016 loss: 1.19235683e-05
Iter: 1017 loss: 1.18819535e-05
Iter: 1018 loss: 1.18723601e-05
Iter: 1019 loss: 1.19929518e-05
Iter: 1020 loss: 1.18723692e-05
Iter: 1021 loss: 1.18648368e-05
Iter: 1022 loss: 1.18652124e-05
Iter: 1023 loss: 1.18589587e-05
Iter: 1024 loss: 1.18515127e-05
Iter: 1025 loss: 1.18451735e-05
Iter: 1026 loss: 1.1843209e-05
Iter: 1027 loss: 1.18307407e-05
Iter: 1028 loss: 1.18507924e-05
Iter: 1029 loss: 1.18249836e-05
Iter: 1030 loss: 1.18100943e-05
Iter: 1031 loss: 1.18417856e-05
Iter: 1032 loss: 1.18043354e-05
Iter: 1033 loss: 1.18023236e-05
Iter: 1034 loss: 1.17970449e-05
Iter: 1035 loss: 1.17904856e-05
Iter: 1036 loss: 1.1781407e-05
Iter: 1037 loss: 1.17810878e-05
Iter: 1038 loss: 1.17744121e-05
Iter: 1039 loss: 1.17742584e-05
Iter: 1040 loss: 1.17690797e-05
Iter: 1041 loss: 1.17680793e-05
Iter: 1042 loss: 1.17645741e-05
Iter: 1043 loss: 1.17580112e-05
Iter: 1044 loss: 1.17549234e-05
Iter: 1045 loss: 1.17515938e-05
Iter: 1046 loss: 1.17436311e-05
Iter: 1047 loss: 1.17435065e-05
Iter: 1048 loss: 1.17391864e-05
Iter: 1049 loss: 1.1729644e-05
Iter: 1050 loss: 1.18519838e-05
Iter: 1051 loss: 1.17289464e-05
Iter: 1052 loss: 1.17148666e-05
Iter: 1053 loss: 1.17438603e-05
Iter: 1054 loss: 1.17093605e-05
Iter: 1055 loss: 1.17013678e-05
Iter: 1056 loss: 1.17010286e-05
Iter: 1057 loss: 1.1693669e-05
Iter: 1058 loss: 1.16986894e-05
Iter: 1059 loss: 1.16890023e-05
Iter: 1060 loss: 1.1682735e-05
Iter: 1061 loss: 1.16735755e-05
Iter: 1062 loss: 1.16734636e-05
Iter: 1063 loss: 1.16605133e-05
Iter: 1064 loss: 1.1689559e-05
Iter: 1065 loss: 1.1655764e-05
Iter: 1066 loss: 1.16445726e-05
Iter: 1067 loss: 1.1723394e-05
Iter: 1068 loss: 1.1643473e-05
Iter: 1069 loss: 1.16327046e-05
Iter: 1070 loss: 1.17500476e-05
Iter: 1071 loss: 1.16324654e-05
Iter: 1072 loss: 1.16262945e-05
Iter: 1073 loss: 1.1623375e-05
Iter: 1074 loss: 1.16203655e-05
Iter: 1075 loss: 1.16114643e-05
Iter: 1076 loss: 1.1705004e-05
Iter: 1077 loss: 1.16112224e-05
Iter: 1078 loss: 1.16060364e-05
Iter: 1079 loss: 1.16019355e-05
Iter: 1080 loss: 1.16002739e-05
Iter: 1081 loss: 1.15953844e-05
Iter: 1082 loss: 1.16399606e-05
Iter: 1083 loss: 1.1595208e-05
Iter: 1084 loss: 1.15890534e-05
Iter: 1085 loss: 1.15833882e-05
Iter: 1086 loss: 1.1581973e-05
Iter: 1087 loss: 1.15732164e-05
Iter: 1088 loss: 1.15850562e-05
Iter: 1089 loss: 1.15686034e-05
Iter: 1090 loss: 1.15583043e-05
Iter: 1091 loss: 1.15574294e-05
Iter: 1092 loss: 1.15496678e-05
Iter: 1093 loss: 1.15393705e-05
Iter: 1094 loss: 1.15392959e-05
Iter: 1095 loss: 1.15286821e-05
Iter: 1096 loss: 1.15380026e-05
Iter: 1097 loss: 1.15225675e-05
Iter: 1098 loss: 1.15159974e-05
Iter: 1099 loss: 1.15130388e-05
Iter: 1100 loss: 1.15096718e-05
Iter: 1101 loss: 1.15001594e-05
Iter: 1102 loss: 1.15072125e-05
Iter: 1103 loss: 1.14943978e-05
Iter: 1104 loss: 1.14840186e-05
Iter: 1105 loss: 1.15890571e-05
Iter: 1106 loss: 1.1483693e-05
Iter: 1107 loss: 1.14737823e-05
Iter: 1108 loss: 1.15369749e-05
Iter: 1109 loss: 1.14725626e-05
Iter: 1110 loss: 1.14652121e-05
Iter: 1111 loss: 1.14595787e-05
Iter: 1112 loss: 1.14572731e-05
Iter: 1113 loss: 1.14502836e-05
Iter: 1114 loss: 1.14500835e-05
Iter: 1115 loss: 1.14434515e-05
Iter: 1116 loss: 1.14325849e-05
Iter: 1117 loss: 1.14325667e-05
Iter: 1118 loss: 1.14267914e-05
Iter: 1119 loss: 1.14266404e-05
Iter: 1120 loss: 1.14214708e-05
Iter: 1121 loss: 1.14251361e-05
Iter: 1122 loss: 1.14181148e-05
Iter: 1123 loss: 1.14118666e-05
Iter: 1124 loss: 1.14029117e-05
Iter: 1125 loss: 1.1402557e-05
Iter: 1126 loss: 1.13914884e-05
Iter: 1127 loss: 1.14334553e-05
Iter: 1128 loss: 1.1388759e-05
Iter: 1129 loss: 1.13833139e-05
Iter: 1130 loss: 1.13825681e-05
Iter: 1131 loss: 1.13775204e-05
Iter: 1132 loss: 1.13649257e-05
Iter: 1133 loss: 1.14815939e-05
Iter: 1134 loss: 1.13631013e-05
Iter: 1135 loss: 1.13492679e-05
Iter: 1136 loss: 1.13860824e-05
Iter: 1137 loss: 1.13446495e-05
Iter: 1138 loss: 1.13348306e-05
Iter: 1139 loss: 1.14295171e-05
Iter: 1140 loss: 1.1334454e-05
Iter: 1141 loss: 1.13268497e-05
Iter: 1142 loss: 1.14161649e-05
Iter: 1143 loss: 1.13266915e-05
Iter: 1144 loss: 1.13213991e-05
Iter: 1145 loss: 1.13143415e-05
Iter: 1146 loss: 1.13139813e-05
Iter: 1147 loss: 1.13066762e-05
Iter: 1148 loss: 1.14129498e-05
Iter: 1149 loss: 1.13066599e-05
Iter: 1150 loss: 1.12990519e-05
Iter: 1151 loss: 1.12945427e-05
Iter: 1152 loss: 1.12913822e-05
Iter: 1153 loss: 1.12837606e-05
Iter: 1154 loss: 1.1308277e-05
Iter: 1155 loss: 1.12816242e-05
Iter: 1156 loss: 1.12718044e-05
Iter: 1157 loss: 1.13044807e-05
Iter: 1158 loss: 1.12692496e-05
Iter: 1159 loss: 1.12612761e-05
Iter: 1160 loss: 1.12597136e-05
Iter: 1161 loss: 1.12547077e-05
Iter: 1162 loss: 1.12467842e-05
Iter: 1163 loss: 1.12583066e-05
Iter: 1164 loss: 1.12430325e-05
Iter: 1165 loss: 1.12363659e-05
Iter: 1166 loss: 1.13333281e-05
Iter: 1167 loss: 1.12363068e-05
Iter: 1168 loss: 1.12285788e-05
Iter: 1169 loss: 1.12212192e-05
Iter: 1170 loss: 1.12193766e-05
Iter: 1171 loss: 1.12090929e-05
Iter: 1172 loss: 1.1203123e-05
Iter: 1173 loss: 1.11989393e-05
Iter: 1174 loss: 1.11838217e-05
Iter: 1175 loss: 1.1262654e-05
Iter: 1176 loss: 1.11814352e-05
Iter: 1177 loss: 1.11718691e-05
Iter: 1178 loss: 1.1171398e-05
Iter: 1179 loss: 1.11654454e-05
Iter: 1180 loss: 1.1160646e-05
Iter: 1181 loss: 1.11588797e-05
Iter: 1182 loss: 1.11528716e-05
Iter: 1183 loss: 1.12349444e-05
Iter: 1184 loss: 1.11527934e-05
Iter: 1185 loss: 1.11466297e-05
Iter: 1186 loss: 1.1145873e-05
Iter: 1187 loss: 1.11414338e-05
Iter: 1188 loss: 1.11347e-05
Iter: 1189 loss: 1.11365534e-05
Iter: 1190 loss: 1.11298177e-05
Iter: 1191 loss: 1.11183235e-05
Iter: 1192 loss: 1.11843174e-05
Iter: 1193 loss: 1.11168301e-05
Iter: 1194 loss: 1.11090685e-05
Iter: 1195 loss: 1.11046338e-05
Iter: 1196 loss: 1.1101276e-05
Iter: 1197 loss: 1.10907085e-05
Iter: 1198 loss: 1.10915298e-05
Iter: 1199 loss: 1.10824676e-05
Iter: 1200 loss: 1.10737183e-05
Iter: 1201 loss: 1.10734818e-05
Iter: 1202 loss: 1.10647088e-05
Iter: 1203 loss: 1.10752917e-05
Iter: 1204 loss: 1.10601295e-05
Iter: 1205 loss: 1.10543751e-05
Iter: 1206 loss: 1.10459987e-05
Iter: 1207 loss: 1.10457167e-05
Iter: 1208 loss: 1.10336896e-05
Iter: 1209 loss: 1.10832034e-05
Iter: 1210 loss: 1.10310466e-05
Iter: 1211 loss: 1.10199398e-05
Iter: 1212 loss: 1.11787476e-05
Iter: 1213 loss: 1.10199344e-05
Iter: 1214 loss: 1.10132269e-05
Iter: 1215 loss: 1.1005618e-05
Iter: 1216 loss: 1.10046985e-05
Iter: 1217 loss: 1.09967541e-05
Iter: 1218 loss: 1.09966968e-05
Iter: 1219 loss: 1.09891253e-05
Iter: 1220 loss: 1.09838938e-05
Iter: 1221 loss: 1.09812963e-05
Iter: 1222 loss: 1.09738576e-05
Iter: 1223 loss: 1.10061628e-05
Iter: 1224 loss: 1.09724215e-05
Iter: 1225 loss: 1.09637522e-05
Iter: 1226 loss: 1.09964112e-05
Iter: 1227 loss: 1.09616958e-05
Iter: 1228 loss: 1.0956338e-05
Iter: 1229 loss: 1.0952811e-05
Iter: 1230 loss: 1.09506764e-05
Iter: 1231 loss: 1.09415269e-05
Iter: 1232 loss: 1.0937496e-05
Iter: 1233 loss: 1.09328321e-05
Iter: 1234 loss: 1.09250177e-05
Iter: 1235 loss: 1.09240982e-05
Iter: 1236 loss: 1.09158627e-05
Iter: 1237 loss: 1.09095963e-05
Iter: 1238 loss: 1.09069706e-05
Iter: 1239 loss: 1.08975428e-05
Iter: 1240 loss: 1.08915465e-05
Iter: 1241 loss: 1.08877302e-05
Iter: 1242 loss: 1.08790955e-05
Iter: 1243 loss: 1.08788645e-05
Iter: 1244 loss: 1.08712711e-05
Iter: 1245 loss: 1.09110388e-05
Iter: 1246 loss: 1.08700406e-05
Iter: 1247 loss: 1.08655122e-05
Iter: 1248 loss: 1.08623244e-05
Iter: 1249 loss: 1.086062e-05
Iter: 1250 loss: 1.08537006e-05
Iter: 1251 loss: 1.0940601e-05
Iter: 1252 loss: 1.08537133e-05
Iter: 1253 loss: 1.08477598e-05
Iter: 1254 loss: 1.08374261e-05
Iter: 1255 loss: 1.10932124e-05
Iter: 1256 loss: 1.08374497e-05
Iter: 1257 loss: 1.08297936e-05
Iter: 1258 loss: 1.08297809e-05
Iter: 1259 loss: 1.08219938e-05
Iter: 1260 loss: 1.08195018e-05
Iter: 1261 loss: 1.08150944e-05
Iter: 1262 loss: 1.08070117e-05
Iter: 1263 loss: 1.08164959e-05
Iter: 1264 loss: 1.08025888e-05
Iter: 1265 loss: 1.07943742e-05
Iter: 1266 loss: 1.0811229e-05
Iter: 1267 loss: 1.07909773e-05
Iter: 1268 loss: 1.07860451e-05
Iter: 1269 loss: 1.0785363e-05
Iter: 1270 loss: 1.07815558e-05
Iter: 1271 loss: 1.07736942e-05
Iter: 1272 loss: 1.09148586e-05
Iter: 1273 loss: 1.07736141e-05
Iter: 1274 loss: 1.07640481e-05
Iter: 1275 loss: 1.07611804e-05
Iter: 1276 loss: 1.07554861e-05
Iter: 1277 loss: 1.07478118e-05
Iter: 1278 loss: 1.07464584e-05
Iter: 1279 loss: 1.07389023e-05
Iter: 1280 loss: 1.07336364e-05
Iter: 1281 loss: 1.07308551e-05
Iter: 1282 loss: 1.072344e-05
Iter: 1283 loss: 1.07831929e-05
Iter: 1284 loss: 1.0722847e-05
Iter: 1285 loss: 1.07151018e-05
Iter: 1286 loss: 1.07399464e-05
Iter: 1287 loss: 1.07128872e-05
Iter: 1288 loss: 1.07073247e-05
Iter: 1289 loss: 1.07042142e-05
Iter: 1290 loss: 1.07018304e-05
Iter: 1291 loss: 1.06965672e-05
Iter: 1292 loss: 1.06964126e-05
Iter: 1293 loss: 1.06925781e-05
Iter: 1294 loss: 1.06838033e-05
Iter: 1295 loss: 1.0804275e-05
Iter: 1296 loss: 1.06832667e-05
Iter: 1297 loss: 1.06722182e-05
Iter: 1298 loss: 1.0689675e-05
Iter: 1299 loss: 1.06669659e-05
Iter: 1300 loss: 1.0655659e-05
Iter: 1301 loss: 1.07297819e-05
Iter: 1302 loss: 1.06544794e-05
Iter: 1303 loss: 1.06428924e-05
Iter: 1304 loss: 1.07050682e-05
Iter: 1305 loss: 1.06410225e-05
Iter: 1306 loss: 1.06348198e-05
Iter: 1307 loss: 1.06279285e-05
Iter: 1308 loss: 1.06269563e-05
Iter: 1309 loss: 1.06190764e-05
Iter: 1310 loss: 1.06801563e-05
Iter: 1311 loss: 1.06184643e-05
Iter: 1312 loss: 1.06107245e-05
Iter: 1313 loss: 1.06596954e-05
Iter: 1314 loss: 1.06098851e-05
Iter: 1315 loss: 1.06048974e-05
Iter: 1316 loss: 1.05997733e-05
Iter: 1317 loss: 1.05989402e-05
Iter: 1318 loss: 1.05931485e-05
Iter: 1319 loss: 1.05930285e-05
Iter: 1320 loss: 1.05877516e-05
Iter: 1321 loss: 1.05775052e-05
Iter: 1322 loss: 1.0793844e-05
Iter: 1323 loss: 1.05774106e-05
Iter: 1324 loss: 1.05673989e-05
Iter: 1325 loss: 1.06434409e-05
Iter: 1326 loss: 1.05666149e-05
Iter: 1327 loss: 1.05567779e-05
Iter: 1328 loss: 1.05923027e-05
Iter: 1329 loss: 1.05544668e-05
Iter: 1330 loss: 1.05487134e-05
Iter: 1331 loss: 1.05429845e-05
Iter: 1332 loss: 1.05419149e-05
Iter: 1333 loss: 1.05327863e-05
Iter: 1334 loss: 1.05645013e-05
Iter: 1335 loss: 1.05303379e-05
Iter: 1336 loss: 1.05252802e-05
Iter: 1337 loss: 1.05250128e-05
Iter: 1338 loss: 1.05201807e-05
Iter: 1339 loss: 1.0510772e-05
Iter: 1340 loss: 1.06950465e-05
Iter: 1341 loss: 1.05105737e-05
Iter: 1342 loss: 1.05003164e-05
Iter: 1343 loss: 1.05155832e-05
Iter: 1344 loss: 1.04954461e-05
Iter: 1345 loss: 1.04882201e-05
Iter: 1346 loss: 1.04877345e-05
Iter: 1347 loss: 1.04809023e-05
Iter: 1348 loss: 1.04705232e-05
Iter: 1349 loss: 1.04702976e-05
Iter: 1350 loss: 1.04623177e-05
Iter: 1351 loss: 1.0570453e-05
Iter: 1352 loss: 1.04622632e-05
Iter: 1353 loss: 1.04552719e-05
Iter: 1354 loss: 1.04849023e-05
Iter: 1355 loss: 1.04537658e-05
Iter: 1356 loss: 1.04496485e-05
Iter: 1357 loss: 1.0446387e-05
Iter: 1358 loss: 1.04451883e-05
Iter: 1359 loss: 1.04392775e-05
Iter: 1360 loss: 1.05183763e-05
Iter: 1361 loss: 1.04392475e-05
Iter: 1362 loss: 1.0434288e-05
Iter: 1363 loss: 1.04248757e-05
Iter: 1364 loss: 1.06281941e-05
Iter: 1365 loss: 1.04249448e-05
Iter: 1366 loss: 1.04144765e-05
Iter: 1367 loss: 1.04318042e-05
Iter: 1368 loss: 1.04096162e-05
Iter: 1369 loss: 1.03985894e-05
Iter: 1370 loss: 1.04769852e-05
Iter: 1371 loss: 1.03975153e-05
Iter: 1372 loss: 1.03872699e-05
Iter: 1373 loss: 1.0453773e-05
Iter: 1374 loss: 1.03861394e-05
Iter: 1375 loss: 1.03807761e-05
Iter: 1376 loss: 1.03733128e-05
Iter: 1377 loss: 1.03731163e-05
Iter: 1378 loss: 1.03662351e-05
Iter: 1379 loss: 1.04579831e-05
Iter: 1380 loss: 1.0366125e-05
Iter: 1381 loss: 1.03598013e-05
Iter: 1382 loss: 1.03835218e-05
Iter: 1383 loss: 1.03581897e-05
Iter: 1384 loss: 1.03534067e-05
Iter: 1385 loss: 1.03453658e-05
Iter: 1386 loss: 1.03452549e-05
Iter: 1387 loss: 1.03373441e-05
Iter: 1388 loss: 1.03370676e-05
Iter: 1389 loss: 1.03313514e-05
Iter: 1390 loss: 1.0321939e-05
Iter: 1391 loss: 1.03219099e-05
Iter: 1392 loss: 1.03148104e-05
Iter: 1393 loss: 1.04181236e-05
Iter: 1394 loss: 1.0314775e-05
Iter: 1395 loss: 1.03083075e-05
Iter: 1396 loss: 1.03174934e-05
Iter: 1397 loss: 1.03050588e-05
Iter: 1398 loss: 1.02994691e-05
Iter: 1399 loss: 1.02947288e-05
Iter: 1400 loss: 1.02931781e-05
Iter: 1401 loss: 1.02860377e-05
Iter: 1402 loss: 1.03365255e-05
Iter: 1403 loss: 1.02854065e-05
Iter: 1404 loss: 1.02801278e-05
Iter: 1405 loss: 1.03366874e-05
Iter: 1406 loss: 1.02800568e-05
Iter: 1407 loss: 1.02749709e-05
Iter: 1408 loss: 1.02644744e-05
Iter: 1409 loss: 1.04452474e-05
Iter: 1410 loss: 1.02641707e-05
Iter: 1411 loss: 1.02535359e-05
Iter: 1412 loss: 1.02951308e-05
Iter: 1413 loss: 1.02510658e-05
Iter: 1414 loss: 1.02433842e-05
Iter: 1415 loss: 1.02432496e-05
Iter: 1416 loss: 1.0238231e-05
Iter: 1417 loss: 1.02319973e-05
Iter: 1418 loss: 1.02313988e-05
Iter: 1419 loss: 1.02275244e-05
Iter: 1420 loss: 1.0227368e-05
Iter: 1421 loss: 1.02233225e-05
Iter: 1422 loss: 1.02204813e-05
Iter: 1423 loss: 1.02189915e-05
Iter: 1424 loss: 1.02140548e-05
Iter: 1425 loss: 1.0220936e-05
Iter: 1426 loss: 1.02115337e-05
Iter: 1427 loss: 1.02063077e-05
Iter: 1428 loss: 1.0269534e-05
Iter: 1429 loss: 1.02062249e-05
Iter: 1430 loss: 1.02024387e-05
Iter: 1431 loss: 1.01948181e-05
Iter: 1432 loss: 1.03347775e-05
Iter: 1433 loss: 1.01946598e-05
Iter: 1434 loss: 1.01845289e-05
Iter: 1435 loss: 1.01926462e-05
Iter: 1436 loss: 1.01784763e-05
Iter: 1437 loss: 1.01711121e-05
Iter: 1438 loss: 1.01706282e-05
Iter: 1439 loss: 1.0164732e-05
Iter: 1440 loss: 1.01795558e-05
Iter: 1441 loss: 1.01627829e-05
Iter: 1442 loss: 1.01590431e-05
Iter: 1443 loss: 1.01538935e-05
Iter: 1444 loss: 1.01535843e-05
Iter: 1445 loss: 1.0149537e-05
Iter: 1446 loss: 1.01491441e-05
Iter: 1447 loss: 1.01450805e-05
Iter: 1448 loss: 1.01427777e-05
Iter: 1449 loss: 1.01410624e-05
Iter: 1450 loss: 1.01358091e-05
Iter: 1451 loss: 1.01416535e-05
Iter: 1452 loss: 1.01329406e-05
Iter: 1453 loss: 1.01249298e-05
Iter: 1454 loss: 1.01815403e-05
Iter: 1455 loss: 1.01242586e-05
Iter: 1456 loss: 1.01195492e-05
Iter: 1457 loss: 1.01130636e-05
Iter: 1458 loss: 1.01127989e-05
Iter: 1459 loss: 1.01079131e-05
Iter: 1460 loss: 1.01076239e-05
Iter: 1461 loss: 1.01034229e-05
Iter: 1462 loss: 1.01008754e-05
Iter: 1463 loss: 1.00991319e-05
Iter: 1464 loss: 1.00941834e-05
Iter: 1465 loss: 1.00979141e-05
Iter: 1466 loss: 1.0091162e-05
Iter: 1467 loss: 1.00855577e-05
Iter: 1468 loss: 1.0111291e-05
Iter: 1469 loss: 1.00844663e-05
Iter: 1470 loss: 1.00781108e-05
Iter: 1471 loss: 1.01145397e-05
Iter: 1472 loss: 1.00772695e-05
Iter: 1473 loss: 1.00722573e-05
Iter: 1474 loss: 1.00653597e-05
Iter: 1475 loss: 1.00651296e-05
Iter: 1476 loss: 1.00576653e-05
Iter: 1477 loss: 1.01148935e-05
Iter: 1478 loss: 1.00570742e-05
Iter: 1479 loss: 1.00491134e-05
Iter: 1480 loss: 1.00762727e-05
Iter: 1481 loss: 1.00469179e-05
Iter: 1482 loss: 1.00418774e-05
Iter: 1483 loss: 1.00444513e-05
Iter: 1484 loss: 1.00384887e-05
Iter: 1485 loss: 1.00360139e-05
Iter: 1486 loss: 1.00353627e-05
Iter: 1487 loss: 1.0033219e-05
Iter: 1488 loss: 1.00277448e-05
Iter: 1489 loss: 1.00813195e-05
Iter: 1490 loss: 1.00271291e-05
Iter: 1491 loss: 1.00216312e-05
Iter: 1492 loss: 1.00758471e-05
Iter: 1493 loss: 1.00214784e-05
Iter: 1494 loss: 1.00158231e-05
Iter: 1495 loss: 1.00302623e-05
Iter: 1496 loss: 1.00139023e-05
Iter: 1497 loss: 1.00093439e-05
Iter: 1498 loss: 1.00023426e-05
Iter: 1499 loss: 1.0002228e-05
Iter: 1500 loss: 9.99347139e-06
Iter: 1501 loss: 1.00374691e-05
Iter: 1502 loss: 9.99199619e-06
Iter: 1503 loss: 9.98819542e-06
Iter: 1504 loss: 9.98786e-06
Iter: 1505 loss: 9.98494852e-06
Iter: 1506 loss: 9.98252744e-06
Iter: 1507 loss: 9.98177529e-06
Iter: 1508 loss: 9.97790903e-06
Iter: 1509 loss: 9.98210271e-06
Iter: 1510 loss: 9.97583084e-06
Iter: 1511 loss: 9.97115421e-06
Iter: 1512 loss: 1.00293637e-05
Iter: 1513 loss: 9.97109692e-06
Iter: 1514 loss: 9.96775816e-06
Iter: 1515 loss: 9.96144627e-06
Iter: 1516 loss: 1.00986126e-05
Iter: 1517 loss: 9.96127892e-06
Iter: 1518 loss: 9.95612936e-06
Iter: 1519 loss: 9.95613482e-06
Iter: 1520 loss: 9.95069e-06
Iter: 1521 loss: 9.94734e-06
Iter: 1522 loss: 9.94524635e-06
Iter: 1523 loss: 9.93994217e-06
Iter: 1524 loss: 9.94925722e-06
Iter: 1525 loss: 9.93757931e-06
Iter: 1526 loss: 9.93250796e-06
Iter: 1527 loss: 1.0011634e-05
Iter: 1528 loss: 9.93247158e-06
Iter: 1529 loss: 9.92978494e-06
Iter: 1530 loss: 9.92726564e-06
Iter: 1531 loss: 9.92652167e-06
Iter: 1532 loss: 9.92282e-06
Iter: 1533 loss: 9.92553123e-06
Iter: 1534 loss: 9.92060268e-06
Iter: 1535 loss: 9.91589332e-06
Iter: 1536 loss: 9.95756818e-06
Iter: 1537 loss: 9.91572415e-06
Iter: 1538 loss: 9.91071283e-06
Iter: 1539 loss: 9.9118879e-06
Iter: 1540 loss: 9.90696481e-06
Iter: 1541 loss: 9.9013123e-06
Iter: 1542 loss: 9.90309127e-06
Iter: 1543 loss: 9.89741784e-06
Iter: 1544 loss: 9.89222735e-06
Iter: 1545 loss: 9.89214277e-06
Iter: 1546 loss: 9.88776264e-06
Iter: 1547 loss: 9.88367174e-06
Iter: 1548 loss: 9.8825094e-06
Iter: 1549 loss: 9.87875046e-06
Iter: 1550 loss: 9.92440437e-06
Iter: 1551 loss: 9.87869134e-06
Iter: 1552 loss: 9.87522253e-06
Iter: 1553 loss: 9.8872606e-06
Iter: 1554 loss: 9.8743858e-06
Iter: 1555 loss: 9.87198109e-06
Iter: 1556 loss: 9.86715077e-06
Iter: 1557 loss: 9.95949813e-06
Iter: 1558 loss: 9.86704072e-06
Iter: 1559 loss: 9.86344e-06
Iter: 1560 loss: 9.863179e-06
Iter: 1561 loss: 9.8598066e-06
Iter: 1562 loss: 9.85459428e-06
Iter: 1563 loss: 9.85453153e-06
Iter: 1564 loss: 9.84797771e-06
Iter: 1565 loss: 9.85463e-06
Iter: 1566 loss: 9.84426697e-06
Iter: 1567 loss: 9.83903192e-06
Iter: 1568 loss: 9.91375873e-06
Iter: 1569 loss: 9.83900372e-06
Iter: 1570 loss: 9.83518294e-06
Iter: 1571 loss: 9.86155374e-06
Iter: 1572 loss: 9.83477366e-06
Iter: 1573 loss: 9.83256177e-06
Iter: 1574 loss: 9.82988058e-06
Iter: 1575 loss: 9.82967686e-06
Iter: 1576 loss: 9.82637812e-06
Iter: 1577 loss: 9.86257237e-06
Iter: 1578 loss: 9.82631354e-06
Iter: 1579 loss: 9.82294569e-06
Iter: 1580 loss: 9.82352e-06
Iter: 1581 loss: 9.82036272e-06
Iter: 1582 loss: 9.81647099e-06
Iter: 1583 loss: 9.81867925e-06
Iter: 1584 loss: 9.81406356e-06
Iter: 1585 loss: 9.80915e-06
Iter: 1586 loss: 9.87620115e-06
Iter: 1587 loss: 9.80909317e-06
Iter: 1588 loss: 9.806261e-06
Iter: 1589 loss: 9.80104414e-06
Iter: 1590 loss: 9.92816786e-06
Iter: 1591 loss: 9.80101413e-06
Iter: 1592 loss: 9.79801916e-06
Iter: 1593 loss: 9.79791912e-06
Iter: 1594 loss: 9.79490505e-06
Iter: 1595 loss: 9.79531433e-06
Iter: 1596 loss: 9.79261404e-06
Iter: 1597 loss: 9.78981188e-06
Iter: 1598 loss: 9.792e-06
Iter: 1599 loss: 9.78807657e-06
Iter: 1600 loss: 9.78454864e-06
Iter: 1601 loss: 9.79143351e-06
Iter: 1602 loss: 9.78300523e-06
Iter: 1603 loss: 9.77845229e-06
Iter: 1604 loss: 9.80952245e-06
Iter: 1605 loss: 9.77805394e-06
Iter: 1606 loss: 9.77409e-06
Iter: 1607 loss: 9.77002856e-06
Iter: 1608 loss: 9.76922274e-06
Iter: 1609 loss: 9.76508272e-06
Iter: 1610 loss: 9.81643916e-06
Iter: 1611 loss: 9.76503361e-06
Iter: 1612 loss: 9.7616421e-06
Iter: 1613 loss: 9.7754064e-06
Iter: 1614 loss: 9.76075899e-06
Iter: 1615 loss: 9.75854527e-06
Iter: 1616 loss: 9.75696094e-06
Iter: 1617 loss: 9.75609782e-06
Iter: 1618 loss: 9.75427611e-06
Iter: 1619 loss: 9.75400326e-06
Iter: 1620 loss: 9.75260173e-06
Iter: 1621 loss: 9.74959858e-06
Iter: 1622 loss: 9.79953802e-06
Iter: 1623 loss: 9.74950672e-06
Iter: 1624 loss: 9.74614704e-06
Iter: 1625 loss: 9.75657531e-06
Iter: 1626 loss: 9.74511568e-06
Iter: 1627 loss: 9.74014256e-06
Iter: 1628 loss: 9.75606326e-06
Iter: 1629 loss: 9.73873466e-06
Iter: 1630 loss: 9.73510942e-06
Iter: 1631 loss: 9.73291208e-06
Iter: 1632 loss: 9.73147871e-06
Iter: 1633 loss: 9.72662383e-06
Iter: 1634 loss: 9.74798422e-06
Iter: 1635 loss: 9.72569524e-06
Iter: 1636 loss: 9.72307862e-06
Iter: 1637 loss: 9.72299313e-06
Iter: 1638 loss: 9.72119778e-06
Iter: 1639 loss: 9.72004636e-06
Iter: 1640 loss: 9.71931513e-06
Iter: 1641 loss: 9.71717145e-06
Iter: 1642 loss: 9.72459748e-06
Iter: 1643 loss: 9.71661939e-06
Iter: 1644 loss: 9.71424e-06
Iter: 1645 loss: 9.72784255e-06
Iter: 1646 loss: 9.7139191e-06
Iter: 1647 loss: 9.71197733e-06
Iter: 1648 loss: 9.70923793e-06
Iter: 1649 loss: 9.70912151e-06
Iter: 1650 loss: 9.70643487e-06
Iter: 1651 loss: 9.70643305e-06
Iter: 1652 loss: 9.70390283e-06
Iter: 1653 loss: 9.70082e-06
Iter: 1654 loss: 9.70045858e-06
Iter: 1655 loss: 9.69764733e-06
Iter: 1656 loss: 9.70966084e-06
Iter: 1657 loss: 9.69707071e-06
Iter: 1658 loss: 9.695068e-06
Iter: 1659 loss: 9.69508073e-06
Iter: 1660 loss: 9.69375651e-06
Iter: 1661 loss: 9.69108169e-06
Iter: 1662 loss: 9.73323949e-06
Iter: 1663 loss: 9.69104258e-06
Iter: 1664 loss: 9.6878739e-06
Iter: 1665 loss: 9.7001639e-06
Iter: 1666 loss: 9.68709e-06
Iter: 1667 loss: 9.68475069e-06
Iter: 1668 loss: 9.70980363e-06
Iter: 1669 loss: 9.68470795e-06
Iter: 1670 loss: 9.68209679e-06
Iter: 1671 loss: 9.67994856e-06
Iter: 1672 loss: 9.67934648e-06
Iter: 1673 loss: 9.67549204e-06
Iter: 1674 loss: 9.68631321e-06
Iter: 1675 loss: 9.67438245e-06
Iter: 1676 loss: 9.67178312e-06
Iter: 1677 loss: 9.67176766e-06
Iter: 1678 loss: 9.6703061e-06
Iter: 1679 loss: 9.669272e-06
Iter: 1680 loss: 9.66875632e-06
Iter: 1681 loss: 9.66741663e-06
Iter: 1682 loss: 9.68780751e-06
Iter: 1683 loss: 9.66741391e-06
Iter: 1684 loss: 9.66592506e-06
Iter: 1685 loss: 9.66557309e-06
Iter: 1686 loss: 9.66474e-06
Iter: 1687 loss: 9.66318e-06
Iter: 1688 loss: 9.66196149e-06
Iter: 1689 loss: 9.66150219e-06
Iter: 1690 loss: 9.65944309e-06
Iter: 1691 loss: 9.6593576e-06
Iter: 1692 loss: 9.65753861e-06
Iter: 1693 loss: 9.65371e-06
Iter: 1694 loss: 9.7143693e-06
Iter: 1695 loss: 9.65361869e-06
Iter: 1696 loss: 9.64953506e-06
Iter: 1697 loss: 9.66977495e-06
Iter: 1698 loss: 9.64891296e-06
Iter: 1699 loss: 9.64651645e-06
Iter: 1700 loss: 9.68077256e-06
Iter: 1701 loss: 9.6465219e-06
Iter: 1702 loss: 9.64441278e-06
Iter: 1703 loss: 9.64946594e-06
Iter: 1704 loss: 9.64374703e-06
Iter: 1705 loss: 9.64228821e-06
Iter: 1706 loss: 9.64249e-06
Iter: 1707 loss: 9.64129322e-06
Iter: 1708 loss: 9.63942693e-06
Iter: 1709 loss: 9.65841718e-06
Iter: 1710 loss: 9.63934417e-06
Iter: 1711 loss: 9.63783532e-06
Iter: 1712 loss: 9.63609909e-06
Iter: 1713 loss: 9.63579805e-06
Iter: 1714 loss: 9.63387538e-06
Iter: 1715 loss: 9.65225081e-06
Iter: 1716 loss: 9.63383536e-06
Iter: 1717 loss: 9.63180173e-06
Iter: 1718 loss: 9.63518778e-06
Iter: 1719 loss: 9.63089315e-06
Iter: 1720 loss: 9.62915237e-06
Iter: 1721 loss: 9.62682134e-06
Iter: 1722 loss: 9.62664308e-06
Iter: 1723 loss: 9.62596187e-06
Iter: 1724 loss: 9.62532249e-06
Iter: 1725 loss: 9.62433478e-06
Iter: 1726 loss: 9.62330159e-06
Iter: 1727 loss: 9.62313334e-06
Iter: 1728 loss: 9.62168633e-06
Iter: 1729 loss: 9.62112e-06
Iter: 1730 loss: 9.6202757e-06
Iter: 1731 loss: 9.61787646e-06
Iter: 1732 loss: 9.62830836e-06
Iter: 1733 loss: 9.61731348e-06
Iter: 1734 loss: 9.61448313e-06
Iter: 1735 loss: 9.62847844e-06
Iter: 1736 loss: 9.6140393e-06
Iter: 1737 loss: 9.61208752e-06
Iter: 1738 loss: 9.61079604e-06
Iter: 1739 loss: 9.61013484e-06
Iter: 1740 loss: 9.60814668e-06
Iter: 1741 loss: 9.60804755e-06
Iter: 1742 loss: 9.60650686e-06
Iter: 1743 loss: 9.60721354e-06
Iter: 1744 loss: 9.60541183e-06
Iter: 1745 loss: 9.60431e-06
Iter: 1746 loss: 9.60996294e-06
Iter: 1747 loss: 9.60404759e-06
Iter: 1748 loss: 9.60284797e-06
Iter: 1749 loss: 9.60929901e-06
Iter: 1750 loss: 9.60263696e-06
Iter: 1751 loss: 9.60183752e-06
Iter: 1752 loss: 9.59974e-06
Iter: 1753 loss: 9.62653576e-06
Iter: 1754 loss: 9.59973113e-06
Iter: 1755 loss: 9.5978794e-06
Iter: 1756 loss: 9.59794852e-06
Iter: 1757 loss: 9.5958676e-06
Iter: 1758 loss: 9.59428326e-06
Iter: 1759 loss: 9.59372846e-06
Iter: 1760 loss: 9.59112367e-06
Iter: 1761 loss: 9.59454701e-06
Iter: 1762 loss: 9.58988767e-06
Iter: 1763 loss: 9.58766577e-06
Iter: 1764 loss: 9.61046135e-06
Iter: 1765 loss: 9.58762121e-06
Iter: 1766 loss: 9.58613236e-06
Iter: 1767 loss: 9.59741737e-06
Iter: 1768 loss: 9.58598321e-06
Iter: 1769 loss: 9.58487362e-06
Iter: 1770 loss: 9.58353121e-06
Iter: 1771 loss: 9.58345481e-06
Iter: 1772 loss: 9.58200872e-06
Iter: 1773 loss: 9.60147281e-06
Iter: 1774 loss: 9.58201508e-06
Iter: 1775 loss: 9.58073451e-06
Iter: 1776 loss: 9.58014243e-06
Iter: 1777 loss: 9.5794785e-06
Iter: 1778 loss: 9.57745397e-06
Iter: 1779 loss: 9.57887642e-06
Iter: 1780 loss: 9.57620523e-06
Iter: 1781 loss: 9.57392058e-06
Iter: 1782 loss: 9.60769103e-06
Iter: 1783 loss: 9.57386601e-06
Iter: 1784 loss: 9.57248812e-06
Iter: 1785 loss: 9.57068733e-06
Iter: 1786 loss: 9.57058182e-06
Iter: 1787 loss: 9.56899839e-06
Iter: 1788 loss: 9.58460441e-06
Iter: 1789 loss: 9.56888653e-06
Iter: 1790 loss: 9.56729764e-06
Iter: 1791 loss: 9.57107204e-06
Iter: 1792 loss: 9.56656186e-06
Iter: 1793 loss: 9.56543136e-06
Iter: 1794 loss: 9.56441e-06
Iter: 1795 loss: 9.56409713e-06
Iter: 1796 loss: 9.56203075e-06
Iter: 1797 loss: 9.5675714e-06
Iter: 1798 loss: 9.56110125e-06
Iter: 1799 loss: 9.55852374e-06
Iter: 1800 loss: 9.57465545e-06
Iter: 1801 loss: 9.5581272e-06
Iter: 1802 loss: 9.55568703e-06
Iter: 1803 loss: 9.55489759e-06
Iter: 1804 loss: 9.55344422e-06
Iter: 1805 loss: 9.55157884e-06
Iter: 1806 loss: 9.57705379e-06
Iter: 1807 loss: 9.55155338e-06
Iter: 1808 loss: 9.54987809e-06
Iter: 1809 loss: 9.55350333e-06
Iter: 1810 loss: 9.5492378e-06
Iter: 1811 loss: 9.54796815e-06
Iter: 1812 loss: 9.54895e-06
Iter: 1813 loss: 9.54718507e-06
Iter: 1814 loss: 9.54601364e-06
Iter: 1815 loss: 9.54599636e-06
Iter: 1816 loss: 9.54508141e-06
Iter: 1817 loss: 9.54294683e-06
Iter: 1818 loss: 9.56565236e-06
Iter: 1819 loss: 9.54267944e-06
Iter: 1820 loss: 9.54007464e-06
Iter: 1821 loss: 9.55066935e-06
Iter: 1822 loss: 9.53955168e-06
Iter: 1823 loss: 9.53643e-06
Iter: 1824 loss: 9.55790438e-06
Iter: 1825 loss: 9.53616654e-06
Iter: 1826 loss: 9.53429299e-06
Iter: 1827 loss: 9.53140807e-06
Iter: 1828 loss: 9.53135441e-06
Iter: 1829 loss: 9.5288533e-06
Iter: 1830 loss: 9.56093e-06
Iter: 1831 loss: 9.52882e-06
Iter: 1832 loss: 9.52714163e-06
Iter: 1833 loss: 9.54228562e-06
Iter: 1834 loss: 9.52708251e-06
Iter: 1835 loss: 9.52561459e-06
Iter: 1836 loss: 9.5249361e-06
Iter: 1837 loss: 9.52433675e-06
Iter: 1838 loss: 9.52254777e-06
Iter: 1839 loss: 9.52882874e-06
Iter: 1840 loss: 9.52213759e-06
Iter: 1841 loss: 9.52011942e-06
Iter: 1842 loss: 9.52952905e-06
Iter: 1843 loss: 9.51966922e-06
Iter: 1844 loss: 9.51813217e-06
Iter: 1845 loss: 9.51591301e-06
Iter: 1846 loss: 9.51584661e-06
Iter: 1847 loss: 9.51322545e-06
Iter: 1848 loss: 9.51320362e-06
Iter: 1849 loss: 9.51095444e-06
Iter: 1850 loss: 9.50894446e-06
Iter: 1851 loss: 9.50856884e-06
Iter: 1852 loss: 9.50672893e-06
Iter: 1853 loss: 9.51597212e-06
Iter: 1854 loss: 9.50641243e-06
Iter: 1855 loss: 9.50489539e-06
Iter: 1856 loss: 9.51955735e-06
Iter: 1857 loss: 9.50476351e-06
Iter: 1858 loss: 9.50363e-06
Iter: 1859 loss: 9.50089543e-06
Iter: 1860 loss: 9.53045856e-06
Iter: 1861 loss: 9.50048e-06
Iter: 1862 loss: 9.49758e-06
Iter: 1863 loss: 9.52293703e-06
Iter: 1864 loss: 9.49745845e-06
Iter: 1865 loss: 9.49499918e-06
Iter: 1866 loss: 9.5116884e-06
Iter: 1867 loss: 9.49473269e-06
Iter: 1868 loss: 9.49218247e-06
Iter: 1869 loss: 9.49129389e-06
Iter: 1870 loss: 9.48999e-06
Iter: 1871 loss: 9.48722663e-06
Iter: 1872 loss: 9.50030153e-06
Iter: 1873 loss: 9.48677553e-06
Iter: 1874 loss: 9.48488e-06
Iter: 1875 loss: 9.51328093e-06
Iter: 1876 loss: 9.48487104e-06
Iter: 1877 loss: 9.48401612e-06
Iter: 1878 loss: 9.48256547e-06
Iter: 1879 loss: 9.4824818e-06
Iter: 1880 loss: 9.48108209e-06
Iter: 1881 loss: 9.50447702e-06
Iter: 1882 loss: 9.48105298e-06
Iter: 1883 loss: 9.47963781e-06
Iter: 1884 loss: 9.47800345e-06
Iter: 1885 loss: 9.4777879e-06
Iter: 1886 loss: 9.47589433e-06
Iter: 1887 loss: 9.4769257e-06
Iter: 1888 loss: 9.47469925e-06
Iter: 1889 loss: 9.47246372e-06
Iter: 1890 loss: 9.47247554e-06
Iter: 1891 loss: 9.47075932e-06
Iter: 1892 loss: 9.46733417e-06
Iter: 1893 loss: 9.53177e-06
Iter: 1894 loss: 9.46721138e-06
Iter: 1895 loss: 9.46481578e-06
Iter: 1896 loss: 9.48701836e-06
Iter: 1897 loss: 9.46469572e-06
Iter: 1898 loss: 9.46315231e-06
Iter: 1899 loss: 9.47878652e-06
Iter: 1900 loss: 9.46305681e-06
Iter: 1901 loss: 9.4616571e-06
Iter: 1902 loss: 9.46332875e-06
Iter: 1903 loss: 9.46093e-06
Iter: 1904 loss: 9.45941702e-06
Iter: 1905 loss: 9.45839565e-06
Iter: 1906 loss: 9.45776264e-06
Iter: 1907 loss: 9.45557076e-06
Iter: 1908 loss: 9.48978868e-06
Iter: 1909 loss: 9.45559805e-06
Iter: 1910 loss: 9.45408465e-06
Iter: 1911 loss: 9.45219745e-06
Iter: 1912 loss: 9.45207285e-06
Iter: 1913 loss: 9.45063766e-06
Iter: 1914 loss: 9.45049e-06
Iter: 1915 loss: 9.44912e-06
Iter: 1916 loss: 9.44862222e-06
Iter: 1917 loss: 9.44781641e-06
Iter: 1918 loss: 9.44632666e-06
Iter: 1919 loss: 9.44664407e-06
Iter: 1920 loss: 9.44513158e-06
Iter: 1921 loss: 9.44465592e-06
Iter: 1922 loss: 9.4442812e-06
Iter: 1923 loss: 9.44353815e-06
Iter: 1924 loss: 9.4416e-06
Iter: 1925 loss: 9.45798729e-06
Iter: 1926 loss: 9.44128533e-06
Iter: 1927 loss: 9.43869236e-06
Iter: 1928 loss: 9.44276144e-06
Iter: 1929 loss: 9.43748e-06
Iter: 1930 loss: 9.43525083e-06
Iter: 1931 loss: 9.43523901e-06
Iter: 1932 loss: 9.4336483e-06
Iter: 1933 loss: 9.43577288e-06
Iter: 1934 loss: 9.43276245e-06
Iter: 1935 loss: 9.43100895e-06
Iter: 1936 loss: 9.43070063e-06
Iter: 1937 loss: 9.42944826e-06
Iter: 1938 loss: 9.42828228e-06
Iter: 1939 loss: 9.42812e-06
Iter: 1940 loss: 9.42727e-06
Iter: 1941 loss: 9.42613678e-06
Iter: 1942 loss: 9.42606766e-06
Iter: 1943 loss: 9.42474799e-06
Iter: 1944 loss: 9.43559098e-06
Iter: 1945 loss: 9.42474253e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8
+ date
Mon Oct 26 09:29:57 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.8_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.8_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c1edf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c26ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c26e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c26e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c135510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c1357b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c116d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c09e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c0b97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c087c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c087ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c058d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c027598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f791c027c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f790006a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f790006a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f79000341e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78947bca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78947f49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7894783e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78947816a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7894789ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78946f98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78946f9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f789471e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78946ef9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f789467b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7894660a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7894660488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78946008c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7894600620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78945f4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78945d4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78945f4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78945a76a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f789453be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.65183761e-05
Iter: 2 loss: 6.3975247e-05
Iter: 3 loss: 9.48335728e-05
Iter: 4 loss: 6.39433711e-05
Iter: 5 loss: 6.24228414e-05
Iter: 6 loss: 6.64391337e-05
Iter: 7 loss: 6.19088387e-05
Iter: 8 loss: 6.08935152e-05
Iter: 9 loss: 6.34120297e-05
Iter: 10 loss: 6.05365931e-05
Iter: 11 loss: 5.96262835e-05
Iter: 12 loss: 6.40224171e-05
Iter: 13 loss: 5.94653975e-05
Iter: 14 loss: 5.85334092e-05
Iter: 15 loss: 5.93706463e-05
Iter: 16 loss: 5.79903353e-05
Iter: 17 loss: 5.69908152e-05
Iter: 18 loss: 5.78925792e-05
Iter: 19 loss: 5.64104339e-05
Iter: 20 loss: 5.5287288e-05
Iter: 21 loss: 7.16373906e-05
Iter: 22 loss: 5.52857819e-05
Iter: 23 loss: 5.47509117e-05
Iter: 24 loss: 5.34256978e-05
Iter: 25 loss: 6.64027102e-05
Iter: 26 loss: 5.32526283e-05
Iter: 27 loss: 5.21574075e-05
Iter: 28 loss: 6.04248e-05
Iter: 29 loss: 5.20724752e-05
Iter: 30 loss: 5.14466228e-05
Iter: 31 loss: 5.14118328e-05
Iter: 32 loss: 5.11156686e-05
Iter: 33 loss: 5.0356608e-05
Iter: 34 loss: 5.67785537e-05
Iter: 35 loss: 5.02275579e-05
Iter: 36 loss: 4.92530271e-05
Iter: 37 loss: 5.10072932e-05
Iter: 38 loss: 4.88319674e-05
Iter: 39 loss: 4.81127026e-05
Iter: 40 loss: 4.80397066e-05
Iter: 41 loss: 4.73859473e-05
Iter: 42 loss: 4.67300561e-05
Iter: 43 loss: 4.65973426e-05
Iter: 44 loss: 4.5992776e-05
Iter: 45 loss: 4.59921248e-05
Iter: 46 loss: 4.55964255e-05
Iter: 47 loss: 4.67614809e-05
Iter: 48 loss: 4.5474917e-05
Iter: 49 loss: 4.5065226e-05
Iter: 50 loss: 4.51639025e-05
Iter: 51 loss: 4.47649873e-05
Iter: 52 loss: 4.44791549e-05
Iter: 53 loss: 4.44785474e-05
Iter: 54 loss: 4.41879092e-05
Iter: 55 loss: 4.37902781e-05
Iter: 56 loss: 4.37717172e-05
Iter: 57 loss: 4.33276364e-05
Iter: 58 loss: 4.29726024e-05
Iter: 59 loss: 4.28383901e-05
Iter: 60 loss: 4.23490128e-05
Iter: 61 loss: 4.23473721e-05
Iter: 62 loss: 4.18517193e-05
Iter: 63 loss: 4.35102629e-05
Iter: 64 loss: 4.17165866e-05
Iter: 65 loss: 4.14792739e-05
Iter: 66 loss: 4.11585861e-05
Iter: 67 loss: 4.11421352e-05
Iter: 68 loss: 4.08093911e-05
Iter: 69 loss: 4.31202861e-05
Iter: 70 loss: 4.07783737e-05
Iter: 71 loss: 4.0437717e-05
Iter: 72 loss: 4.32401575e-05
Iter: 73 loss: 4.04170569e-05
Iter: 74 loss: 4.02365258e-05
Iter: 75 loss: 4.00666031e-05
Iter: 76 loss: 4.00246063e-05
Iter: 77 loss: 3.96948381e-05
Iter: 78 loss: 4.22147496e-05
Iter: 79 loss: 3.96699979e-05
Iter: 80 loss: 3.94147864e-05
Iter: 81 loss: 3.97989133e-05
Iter: 82 loss: 3.92937327e-05
Iter: 83 loss: 3.90305577e-05
Iter: 84 loss: 3.95687239e-05
Iter: 85 loss: 3.89249981e-05
Iter: 86 loss: 3.87280525e-05
Iter: 87 loss: 3.87274704e-05
Iter: 88 loss: 3.86166357e-05
Iter: 89 loss: 3.83546394e-05
Iter: 90 loss: 4.13922971e-05
Iter: 91 loss: 3.83310835e-05
Iter: 92 loss: 3.80735946e-05
Iter: 93 loss: 3.85915046e-05
Iter: 94 loss: 3.7968879e-05
Iter: 95 loss: 3.78170516e-05
Iter: 96 loss: 3.77913311e-05
Iter: 97 loss: 3.76331154e-05
Iter: 98 loss: 3.73039256e-05
Iter: 99 loss: 4.29313186e-05
Iter: 100 loss: 3.72962168e-05
Iter: 101 loss: 3.69770896e-05
Iter: 102 loss: 3.73806033e-05
Iter: 103 loss: 3.68124238e-05
Iter: 104 loss: 3.67493267e-05
Iter: 105 loss: 3.66681052e-05
Iter: 106 loss: 3.65352171e-05
Iter: 107 loss: 3.64225925e-05
Iter: 108 loss: 3.63856088e-05
Iter: 109 loss: 3.62404171e-05
Iter: 110 loss: 3.71090064e-05
Iter: 111 loss: 3.62221799e-05
Iter: 112 loss: 3.60656231e-05
Iter: 113 loss: 3.641886e-05
Iter: 114 loss: 3.60064332e-05
Iter: 115 loss: 3.5861005e-05
Iter: 116 loss: 3.60168051e-05
Iter: 117 loss: 3.57809622e-05
Iter: 118 loss: 3.56008568e-05
Iter: 119 loss: 3.64816915e-05
Iter: 120 loss: 3.55695374e-05
Iter: 121 loss: 3.53720825e-05
Iter: 122 loss: 3.56721248e-05
Iter: 123 loss: 3.52783e-05
Iter: 124 loss: 3.51318522e-05
Iter: 125 loss: 3.48594585e-05
Iter: 126 loss: 4.0994717e-05
Iter: 127 loss: 3.4859142e-05
Iter: 128 loss: 3.46754932e-05
Iter: 129 loss: 3.46742381e-05
Iter: 130 loss: 3.45181434e-05
Iter: 131 loss: 3.58156067e-05
Iter: 132 loss: 3.45089575e-05
Iter: 133 loss: 3.44271248e-05
Iter: 134 loss: 3.42340099e-05
Iter: 135 loss: 3.64729967e-05
Iter: 136 loss: 3.42167259e-05
Iter: 137 loss: 3.39652688e-05
Iter: 138 loss: 3.46468587e-05
Iter: 139 loss: 3.38826248e-05
Iter: 140 loss: 3.3729375e-05
Iter: 141 loss: 3.37066958e-05
Iter: 142 loss: 3.36112389e-05
Iter: 143 loss: 3.34137076e-05
Iter: 144 loss: 3.68355468e-05
Iter: 145 loss: 3.34093711e-05
Iter: 146 loss: 3.32962445e-05
Iter: 147 loss: 3.32785e-05
Iter: 148 loss: 3.31912561e-05
Iter: 149 loss: 3.3154065e-05
Iter: 150 loss: 3.31087795e-05
Iter: 151 loss: 3.3007891e-05
Iter: 152 loss: 3.37527454e-05
Iter: 153 loss: 3.29999421e-05
Iter: 154 loss: 3.29192626e-05
Iter: 155 loss: 3.33611024e-05
Iter: 156 loss: 3.2907461e-05
Iter: 157 loss: 3.28340393e-05
Iter: 158 loss: 3.268008e-05
Iter: 159 loss: 3.52439929e-05
Iter: 160 loss: 3.26758854e-05
Iter: 161 loss: 3.24768407e-05
Iter: 162 loss: 3.25018191e-05
Iter: 163 loss: 3.23254098e-05
Iter: 164 loss: 3.2242926e-05
Iter: 165 loss: 3.22028536e-05
Iter: 166 loss: 3.20730942e-05
Iter: 167 loss: 3.19411884e-05
Iter: 168 loss: 3.19156643e-05
Iter: 169 loss: 3.17878294e-05
Iter: 170 loss: 3.17874437e-05
Iter: 171 loss: 3.16856713e-05
Iter: 172 loss: 3.16160294e-05
Iter: 173 loss: 3.1601543e-05
Iter: 174 loss: 3.15141806e-05
Iter: 175 loss: 3.1505584e-05
Iter: 176 loss: 3.14417921e-05
Iter: 177 loss: 3.13350756e-05
Iter: 178 loss: 3.1358315e-05
Iter: 179 loss: 3.12564371e-05
Iter: 180 loss: 3.10762262e-05
Iter: 181 loss: 3.22306878e-05
Iter: 182 loss: 3.10567484e-05
Iter: 183 loss: 3.09619318e-05
Iter: 184 loss: 3.10447685e-05
Iter: 185 loss: 3.09061652e-05
Iter: 186 loss: 3.08034978e-05
Iter: 187 loss: 3.18218881e-05
Iter: 188 loss: 3.08002309e-05
Iter: 189 loss: 3.07204828e-05
Iter: 190 loss: 3.07991868e-05
Iter: 191 loss: 3.06754e-05
Iter: 192 loss: 3.0599731e-05
Iter: 193 loss: 3.05341928e-05
Iter: 194 loss: 3.05136946e-05
Iter: 195 loss: 3.04049827e-05
Iter: 196 loss: 3.06307811e-05
Iter: 197 loss: 3.0361547e-05
Iter: 198 loss: 3.02767075e-05
Iter: 199 loss: 3.02725e-05
Iter: 200 loss: 3.02056978e-05
Iter: 201 loss: 3.00315969e-05
Iter: 202 loss: 3.136327e-05
Iter: 203 loss: 2.99970561e-05
Iter: 204 loss: 2.98257037e-05
Iter: 205 loss: 3.0694071e-05
Iter: 206 loss: 2.97975475e-05
Iter: 207 loss: 2.97706083e-05
Iter: 208 loss: 2.972927e-05
Iter: 209 loss: 2.96840481e-05
Iter: 210 loss: 2.95856134e-05
Iter: 211 loss: 3.10868199e-05
Iter: 212 loss: 2.95818209e-05
Iter: 213 loss: 2.9528177e-05
Iter: 214 loss: 2.95230184e-05
Iter: 215 loss: 2.94679612e-05
Iter: 216 loss: 2.93830417e-05
Iter: 217 loss: 2.93816447e-05
Iter: 218 loss: 2.92933928e-05
Iter: 219 loss: 3.00841584e-05
Iter: 220 loss: 2.92893692e-05
Iter: 221 loss: 2.92010791e-05
Iter: 222 loss: 2.9351002e-05
Iter: 223 loss: 2.91616889e-05
Iter: 224 loss: 2.90659627e-05
Iter: 225 loss: 2.90614953e-05
Iter: 226 loss: 2.898819e-05
Iter: 227 loss: 2.88673946e-05
Iter: 228 loss: 2.89300442e-05
Iter: 229 loss: 2.878743e-05
Iter: 230 loss: 2.8716584e-05
Iter: 231 loss: 2.87152925e-05
Iter: 232 loss: 2.8641889e-05
Iter: 233 loss: 2.87642251e-05
Iter: 234 loss: 2.86089853e-05
Iter: 235 loss: 2.85491369e-05
Iter: 236 loss: 2.84416947e-05
Iter: 237 loss: 3.1027892e-05
Iter: 238 loss: 2.84416565e-05
Iter: 239 loss: 2.83389163e-05
Iter: 240 loss: 2.93124212e-05
Iter: 241 loss: 2.83350291e-05
Iter: 242 loss: 2.82152105e-05
Iter: 243 loss: 2.86092109e-05
Iter: 244 loss: 2.81822322e-05
Iter: 245 loss: 2.81155917e-05
Iter: 246 loss: 2.81417088e-05
Iter: 247 loss: 2.80695112e-05
Iter: 248 loss: 2.79959186e-05
Iter: 249 loss: 2.90777334e-05
Iter: 250 loss: 2.79958e-05
Iter: 251 loss: 2.7951326e-05
Iter: 252 loss: 2.78799453e-05
Iter: 253 loss: 2.78795196e-05
Iter: 254 loss: 2.78255666e-05
Iter: 255 loss: 2.78208445e-05
Iter: 256 loss: 2.77827603e-05
Iter: 257 loss: 2.77428662e-05
Iter: 258 loss: 2.77358558e-05
Iter: 259 loss: 2.76628089e-05
Iter: 260 loss: 2.7658878e-05
Iter: 261 loss: 2.76032424e-05
Iter: 262 loss: 2.74943595e-05
Iter: 263 loss: 2.7488155e-05
Iter: 264 loss: 2.74054382e-05
Iter: 265 loss: 2.73738187e-05
Iter: 266 loss: 2.73300757e-05
Iter: 267 loss: 2.72800207e-05
Iter: 268 loss: 2.71799909e-05
Iter: 269 loss: 2.91070373e-05
Iter: 270 loss: 2.7178985e-05
Iter: 271 loss: 2.70910859e-05
Iter: 272 loss: 2.7289665e-05
Iter: 273 loss: 2.70581841e-05
Iter: 274 loss: 2.70152505e-05
Iter: 275 loss: 2.70095989e-05
Iter: 276 loss: 2.69684297e-05
Iter: 277 loss: 2.68880249e-05
Iter: 278 loss: 2.85136757e-05
Iter: 279 loss: 2.6887421e-05
Iter: 280 loss: 2.68137919e-05
Iter: 281 loss: 2.75189759e-05
Iter: 282 loss: 2.6811e-05
Iter: 283 loss: 2.6721149e-05
Iter: 284 loss: 2.6646574e-05
Iter: 285 loss: 2.66209609e-05
Iter: 286 loss: 2.65499039e-05
Iter: 287 loss: 2.65498311e-05
Iter: 288 loss: 2.64870905e-05
Iter: 289 loss: 2.65581621e-05
Iter: 290 loss: 2.64534647e-05
Iter: 291 loss: 2.63996953e-05
Iter: 292 loss: 2.64670562e-05
Iter: 293 loss: 2.63717029e-05
Iter: 294 loss: 2.63183101e-05
Iter: 295 loss: 2.63777529e-05
Iter: 296 loss: 2.62894027e-05
Iter: 297 loss: 2.6237145e-05
Iter: 298 loss: 2.64483479e-05
Iter: 299 loss: 2.62254634e-05
Iter: 300 loss: 2.61541663e-05
Iter: 301 loss: 2.63805396e-05
Iter: 302 loss: 2.61336099e-05
Iter: 303 loss: 2.60847955e-05
Iter: 304 loss: 2.59749067e-05
Iter: 305 loss: 2.74689373e-05
Iter: 306 loss: 2.59684421e-05
Iter: 307 loss: 2.59072094e-05
Iter: 308 loss: 2.58963373e-05
Iter: 309 loss: 2.58353321e-05
Iter: 310 loss: 2.60410161e-05
Iter: 311 loss: 2.58188538e-05
Iter: 312 loss: 2.57843494e-05
Iter: 313 loss: 2.57781721e-05
Iter: 314 loss: 2.5754809e-05
Iter: 315 loss: 2.57082793e-05
Iter: 316 loss: 2.63260117e-05
Iter: 317 loss: 2.57080137e-05
Iter: 318 loss: 2.56804651e-05
Iter: 319 loss: 2.56428048e-05
Iter: 320 loss: 2.56409439e-05
Iter: 321 loss: 2.55871728e-05
Iter: 322 loss: 2.62025733e-05
Iter: 323 loss: 2.55861596e-05
Iter: 324 loss: 2.55504729e-05
Iter: 325 loss: 2.54891656e-05
Iter: 326 loss: 2.5489e-05
Iter: 327 loss: 2.53996841e-05
Iter: 328 loss: 2.55524683e-05
Iter: 329 loss: 2.53596518e-05
Iter: 330 loss: 2.5283367e-05
Iter: 331 loss: 2.54945808e-05
Iter: 332 loss: 2.52587233e-05
Iter: 333 loss: 2.52300197e-05
Iter: 334 loss: 2.52204136e-05
Iter: 335 loss: 2.51931669e-05
Iter: 336 loss: 2.51305828e-05
Iter: 337 loss: 2.59107455e-05
Iter: 338 loss: 2.51260462e-05
Iter: 339 loss: 2.50639296e-05
Iter: 340 loss: 2.52346435e-05
Iter: 341 loss: 2.50438097e-05
Iter: 342 loss: 2.49909608e-05
Iter: 343 loss: 2.49906989e-05
Iter: 344 loss: 2.49537989e-05
Iter: 345 loss: 2.48790602e-05
Iter: 346 loss: 2.62574595e-05
Iter: 347 loss: 2.4877987e-05
Iter: 348 loss: 2.48249598e-05
Iter: 349 loss: 2.48231117e-05
Iter: 350 loss: 2.47749049e-05
Iter: 351 loss: 2.47619173e-05
Iter: 352 loss: 2.47318821e-05
Iter: 353 loss: 2.47004464e-05
Iter: 354 loss: 2.46998134e-05
Iter: 355 loss: 2.46716609e-05
Iter: 356 loss: 2.46443542e-05
Iter: 357 loss: 2.4637915e-05
Iter: 358 loss: 2.4604622e-05
Iter: 359 loss: 2.47129738e-05
Iter: 360 loss: 2.45953561e-05
Iter: 361 loss: 2.45575338e-05
Iter: 362 loss: 2.45219489e-05
Iter: 363 loss: 2.45132032e-05
Iter: 364 loss: 2.44601979e-05
Iter: 365 loss: 2.514882e-05
Iter: 366 loss: 2.44600233e-05
Iter: 367 loss: 2.44027833e-05
Iter: 368 loss: 2.44360817e-05
Iter: 369 loss: 2.43657396e-05
Iter: 370 loss: 2.43129234e-05
Iter: 371 loss: 2.42636033e-05
Iter: 372 loss: 2.42512724e-05
Iter: 373 loss: 2.42483657e-05
Iter: 374 loss: 2.4221672e-05
Iter: 375 loss: 2.41971065e-05
Iter: 376 loss: 2.4171648e-05
Iter: 377 loss: 2.41668531e-05
Iter: 378 loss: 2.41343514e-05
Iter: 379 loss: 2.42270089e-05
Iter: 380 loss: 2.41238758e-05
Iter: 381 loss: 2.4081879e-05
Iter: 382 loss: 2.42638926e-05
Iter: 383 loss: 2.40730806e-05
Iter: 384 loss: 2.40461741e-05
Iter: 385 loss: 2.40712143e-05
Iter: 386 loss: 2.40305599e-05
Iter: 387 loss: 2.39839501e-05
Iter: 388 loss: 2.4087225e-05
Iter: 389 loss: 2.39660694e-05
Iter: 390 loss: 2.39260698e-05
Iter: 391 loss: 2.39005967e-05
Iter: 392 loss: 2.38849261e-05
Iter: 393 loss: 2.38253197e-05
Iter: 394 loss: 2.40828267e-05
Iter: 395 loss: 2.38131943e-05
Iter: 396 loss: 2.37741369e-05
Iter: 397 loss: 2.39240071e-05
Iter: 398 loss: 2.37650493e-05
Iter: 399 loss: 2.37319291e-05
Iter: 400 loss: 2.4179164e-05
Iter: 401 loss: 2.37317836e-05
Iter: 402 loss: 2.37124314e-05
Iter: 403 loss: 2.36681626e-05
Iter: 404 loss: 2.42533279e-05
Iter: 405 loss: 2.36653759e-05
Iter: 406 loss: 2.36181859e-05
Iter: 407 loss: 2.39056644e-05
Iter: 408 loss: 2.36125725e-05
Iter: 409 loss: 2.35551397e-05
Iter: 410 loss: 2.37612949e-05
Iter: 411 loss: 2.35408152e-05
Iter: 412 loss: 2.35030384e-05
Iter: 413 loss: 2.34801355e-05
Iter: 414 loss: 2.34647086e-05
Iter: 415 loss: 2.3426288e-05
Iter: 416 loss: 2.34247127e-05
Iter: 417 loss: 2.34002728e-05
Iter: 418 loss: 2.33733444e-05
Iter: 419 loss: 2.33693318e-05
Iter: 420 loss: 2.33403025e-05
Iter: 421 loss: 2.33398823e-05
Iter: 422 loss: 2.33237788e-05
Iter: 423 loss: 2.32968214e-05
Iter: 424 loss: 2.32969087e-05
Iter: 425 loss: 2.32639504e-05
Iter: 426 loss: 2.33448955e-05
Iter: 427 loss: 2.32521088e-05
Iter: 428 loss: 2.3211107e-05
Iter: 429 loss: 2.31851081e-05
Iter: 430 loss: 2.31689155e-05
Iter: 431 loss: 2.31368194e-05
Iter: 432 loss: 2.31300037e-05
Iter: 433 loss: 2.30982187e-05
Iter: 434 loss: 2.3037579e-05
Iter: 435 loss: 2.43516406e-05
Iter: 436 loss: 2.3037479e-05
Iter: 437 loss: 2.29804136e-05
Iter: 438 loss: 2.31924205e-05
Iter: 439 loss: 2.29665147e-05
Iter: 440 loss: 2.29368288e-05
Iter: 441 loss: 2.2934335e-05
Iter: 442 loss: 2.29154e-05
Iter: 443 loss: 2.28769313e-05
Iter: 444 loss: 2.35833068e-05
Iter: 445 loss: 2.28764511e-05
Iter: 446 loss: 2.28505305e-05
Iter: 447 loss: 2.28495737e-05
Iter: 448 loss: 2.28236549e-05
Iter: 449 loss: 2.27976288e-05
Iter: 450 loss: 2.27926721e-05
Iter: 451 loss: 2.27637756e-05
Iter: 452 loss: 2.32208113e-05
Iter: 453 loss: 2.2763692e-05
Iter: 454 loss: 2.27365636e-05
Iter: 455 loss: 2.26901593e-05
Iter: 456 loss: 2.26900756e-05
Iter: 457 loss: 2.26433822e-05
Iter: 458 loss: 2.28045574e-05
Iter: 459 loss: 2.26311549e-05
Iter: 460 loss: 2.25889453e-05
Iter: 461 loss: 2.27320088e-05
Iter: 462 loss: 2.25776275e-05
Iter: 463 loss: 2.2554781e-05
Iter: 464 loss: 2.28416175e-05
Iter: 465 loss: 2.25545318e-05
Iter: 466 loss: 2.25300373e-05
Iter: 467 loss: 2.25312488e-05
Iter: 468 loss: 2.25108015e-05
Iter: 469 loss: 2.24843607e-05
Iter: 470 loss: 2.24625037e-05
Iter: 471 loss: 2.24547148e-05
Iter: 472 loss: 2.2427268e-05
Iter: 473 loss: 2.24257619e-05
Iter: 474 loss: 2.23982879e-05
Iter: 475 loss: 2.2351056e-05
Iter: 476 loss: 2.2351016e-05
Iter: 477 loss: 2.2309423e-05
Iter: 478 loss: 2.26628181e-05
Iter: 479 loss: 2.23071638e-05
Iter: 480 loss: 2.22656417e-05
Iter: 481 loss: 2.23932948e-05
Iter: 482 loss: 2.22536219e-05
Iter: 483 loss: 2.223103e-05
Iter: 484 loss: 2.23556472e-05
Iter: 485 loss: 2.2227714e-05
Iter: 486 loss: 2.22026902e-05
Iter: 487 loss: 2.22229173e-05
Iter: 488 loss: 2.21877017e-05
Iter: 489 loss: 2.21648261e-05
Iter: 490 loss: 2.21502687e-05
Iter: 491 loss: 2.21413757e-05
Iter: 492 loss: 2.21019291e-05
Iter: 493 loss: 2.22242925e-05
Iter: 494 loss: 2.20903694e-05
Iter: 495 loss: 2.20520378e-05
Iter: 496 loss: 2.21627815e-05
Iter: 497 loss: 2.20400452e-05
Iter: 498 loss: 2.19989961e-05
Iter: 499 loss: 2.23781517e-05
Iter: 500 loss: 2.19971116e-05
Iter: 501 loss: 2.19743342e-05
Iter: 502 loss: 2.1930693e-05
Iter: 503 loss: 2.28479403e-05
Iter: 504 loss: 2.19304675e-05
Iter: 505 loss: 2.19084432e-05
Iter: 506 loss: 2.19052799e-05
Iter: 507 loss: 2.18826026e-05
Iter: 508 loss: 2.19003559e-05
Iter: 509 loss: 2.18688765e-05
Iter: 510 loss: 2.18481055e-05
Iter: 511 loss: 2.18435725e-05
Iter: 512 loss: 2.18302157e-05
Iter: 513 loss: 2.17958477e-05
Iter: 514 loss: 2.21274913e-05
Iter: 515 loss: 2.1794629e-05
Iter: 516 loss: 2.17724119e-05
Iter: 517 loss: 2.17614179e-05
Iter: 518 loss: 2.17508041e-05
Iter: 519 loss: 2.17133202e-05
Iter: 520 loss: 2.20014281e-05
Iter: 521 loss: 2.1710408e-05
Iter: 522 loss: 2.16900371e-05
Iter: 523 loss: 2.16578028e-05
Iter: 524 loss: 2.16574572e-05
Iter: 525 loss: 2.16188018e-05
Iter: 526 loss: 2.18351233e-05
Iter: 527 loss: 2.1613434e-05
Iter: 528 loss: 2.15884884e-05
Iter: 529 loss: 2.1665277e-05
Iter: 530 loss: 2.15813689e-05
Iter: 531 loss: 2.15639684e-05
Iter: 532 loss: 2.15638756e-05
Iter: 533 loss: 2.15492655e-05
Iter: 534 loss: 2.15119508e-05
Iter: 535 loss: 2.18136374e-05
Iter: 536 loss: 2.15052278e-05
Iter: 537 loss: 2.14675438e-05
Iter: 538 loss: 2.18357018e-05
Iter: 539 loss: 2.14661122e-05
Iter: 540 loss: 2.14305801e-05
Iter: 541 loss: 2.16398839e-05
Iter: 542 loss: 2.14261145e-05
Iter: 543 loss: 2.14038701e-05
Iter: 544 loss: 2.13693838e-05
Iter: 545 loss: 2.13688563e-05
Iter: 546 loss: 2.13500607e-05
Iter: 547 loss: 2.13449657e-05
Iter: 548 loss: 2.1329186e-05
Iter: 549 loss: 2.13142812e-05
Iter: 550 loss: 2.13106759e-05
Iter: 551 loss: 2.12911236e-05
Iter: 552 loss: 2.12911254e-05
Iter: 553 loss: 2.12767773e-05
Iter: 554 loss: 2.12468876e-05
Iter: 555 loss: 2.17581019e-05
Iter: 556 loss: 2.12461582e-05
Iter: 557 loss: 2.12161e-05
Iter: 558 loss: 2.13492131e-05
Iter: 559 loss: 2.12098657e-05
Iter: 560 loss: 2.11768202e-05
Iter: 561 loss: 2.12048581e-05
Iter: 562 loss: 2.11571314e-05
Iter: 563 loss: 2.11283441e-05
Iter: 564 loss: 2.11283477e-05
Iter: 565 loss: 2.11023889e-05
Iter: 566 loss: 2.10844446e-05
Iter: 567 loss: 2.10750295e-05
Iter: 568 loss: 2.10512626e-05
Iter: 569 loss: 2.1132395e-05
Iter: 570 loss: 2.10447761e-05
Iter: 571 loss: 2.10243343e-05
Iter: 572 loss: 2.12461982e-05
Iter: 573 loss: 2.10238704e-05
Iter: 574 loss: 2.10094549e-05
Iter: 575 loss: 2.09873942e-05
Iter: 576 loss: 2.09872378e-05
Iter: 577 loss: 2.09692189e-05
Iter: 578 loss: 2.0969077e-05
Iter: 579 loss: 2.0950436e-05
Iter: 580 loss: 2.09322079e-05
Iter: 581 loss: 2.0928348e-05
Iter: 582 loss: 2.09079662e-05
Iter: 583 loss: 2.09080026e-05
Iter: 584 loss: 2.08896745e-05
Iter: 585 loss: 2.08646507e-05
Iter: 586 loss: 2.0863361e-05
Iter: 587 loss: 2.08379206e-05
Iter: 588 loss: 2.08997953e-05
Iter: 589 loss: 2.08286292e-05
Iter: 590 loss: 2.08020392e-05
Iter: 591 loss: 2.08822094e-05
Iter: 592 loss: 2.07937701e-05
Iter: 593 loss: 2.07717458e-05
Iter: 594 loss: 2.0944437e-05
Iter: 595 loss: 2.07702178e-05
Iter: 596 loss: 2.07459e-05
Iter: 597 loss: 2.07759349e-05
Iter: 598 loss: 2.07331886e-05
Iter: 599 loss: 2.07100657e-05
Iter: 600 loss: 2.06786881e-05
Iter: 601 loss: 2.06769619e-05
Iter: 602 loss: 2.06519544e-05
Iter: 603 loss: 2.0648411e-05
Iter: 604 loss: 2.06304576e-05
Iter: 605 loss: 2.06084314e-05
Iter: 606 loss: 2.06062268e-05
Iter: 607 loss: 2.05846536e-05
Iter: 608 loss: 2.07715111e-05
Iter: 609 loss: 2.05835786e-05
Iter: 610 loss: 2.05611432e-05
Iter: 611 loss: 2.06192308e-05
Iter: 612 loss: 2.05534961e-05
Iter: 613 loss: 2.05392225e-05
Iter: 614 loss: 2.06027089e-05
Iter: 615 loss: 2.05364267e-05
Iter: 616 loss: 2.0518788e-05
Iter: 617 loss: 2.05183278e-05
Iter: 618 loss: 2.0504649e-05
Iter: 619 loss: 2.0484249e-05
Iter: 620 loss: 2.04583757e-05
Iter: 621 loss: 2.04564112e-05
Iter: 622 loss: 2.0415946e-05
Iter: 623 loss: 2.06425957e-05
Iter: 624 loss: 2.04101889e-05
Iter: 625 loss: 2.03798627e-05
Iter: 626 loss: 2.05282049e-05
Iter: 627 loss: 2.03748514e-05
Iter: 628 loss: 2.0347572e-05
Iter: 629 loss: 2.05910583e-05
Iter: 630 loss: 2.03462405e-05
Iter: 631 loss: 2.03322597e-05
Iter: 632 loss: 2.031439e-05
Iter: 633 loss: 2.03132331e-05
Iter: 634 loss: 2.02997962e-05
Iter: 635 loss: 2.02991196e-05
Iter: 636 loss: 2.02859792e-05
Iter: 637 loss: 2.02654956e-05
Iter: 638 loss: 2.02652664e-05
Iter: 639 loss: 2.02406336e-05
Iter: 640 loss: 2.03012896e-05
Iter: 641 loss: 2.02320516e-05
Iter: 642 loss: 2.02017891e-05
Iter: 643 loss: 2.03938216e-05
Iter: 644 loss: 2.01981966e-05
Iter: 645 loss: 2.01783932e-05
Iter: 646 loss: 2.01961175e-05
Iter: 647 loss: 2.01667099e-05
Iter: 648 loss: 2.01375606e-05
Iter: 649 loss: 2.02634164e-05
Iter: 650 loss: 2.01315306e-05
Iter: 651 loss: 2.01149705e-05
Iter: 652 loss: 2.01000512e-05
Iter: 653 loss: 2.00959039e-05
Iter: 654 loss: 2.00740069e-05
Iter: 655 loss: 2.01908642e-05
Iter: 656 loss: 2.00706854e-05
Iter: 657 loss: 2.00502509e-05
Iter: 658 loss: 2.00899267e-05
Iter: 659 loss: 2.00417162e-05
Iter: 660 loss: 2.00212708e-05
Iter: 661 loss: 2.027954e-05
Iter: 662 loss: 2.00210543e-05
Iter: 663 loss: 2.00061222e-05
Iter: 664 loss: 1.99748974e-05
Iter: 665 loss: 2.04943572e-05
Iter: 666 loss: 1.99741753e-05
Iter: 667 loss: 1.99485439e-05
Iter: 668 loss: 1.99483911e-05
Iter: 669 loss: 1.99230089e-05
Iter: 670 loss: 1.99323749e-05
Iter: 671 loss: 1.99053211e-05
Iter: 672 loss: 1.98849339e-05
Iter: 673 loss: 1.9937901e-05
Iter: 674 loss: 1.98781127e-05
Iter: 675 loss: 1.98621601e-05
Iter: 676 loss: 1.98622038e-05
Iter: 677 loss: 1.98522466e-05
Iter: 678 loss: 1.98403577e-05
Iter: 679 loss: 1.98389753e-05
Iter: 680 loss: 1.98186972e-05
Iter: 681 loss: 1.99677816e-05
Iter: 682 loss: 1.981694e-05
Iter: 683 loss: 1.98030903e-05
Iter: 684 loss: 1.9773e-05
Iter: 685 loss: 2.02254614e-05
Iter: 686 loss: 1.97717036e-05
Iter: 687 loss: 1.97353329e-05
Iter: 688 loss: 1.98981706e-05
Iter: 689 loss: 1.97281879e-05
Iter: 690 loss: 1.96976944e-05
Iter: 691 loss: 1.98410344e-05
Iter: 692 loss: 1.96919846e-05
Iter: 693 loss: 1.96754227e-05
Iter: 694 loss: 1.96752608e-05
Iter: 695 loss: 1.96625333e-05
Iter: 696 loss: 1.96442888e-05
Iter: 697 loss: 1.96436777e-05
Iter: 698 loss: 1.9627234e-05
Iter: 699 loss: 1.97491208e-05
Iter: 700 loss: 1.96258297e-05
Iter: 701 loss: 1.96076471e-05
Iter: 702 loss: 1.96478177e-05
Iter: 703 loss: 1.96006804e-05
Iter: 704 loss: 1.95857647e-05
Iter: 705 loss: 1.9567804e-05
Iter: 706 loss: 1.95658904e-05
Iter: 707 loss: 1.95476987e-05
Iter: 708 loss: 1.95467255e-05
Iter: 709 loss: 1.9531657e-05
Iter: 710 loss: 1.95205957e-05
Iter: 711 loss: 1.95154316e-05
Iter: 712 loss: 1.94970489e-05
Iter: 713 loss: 1.97613699e-05
Iter: 714 loss: 1.94970798e-05
Iter: 715 loss: 1.94844233e-05
Iter: 716 loss: 1.94598942e-05
Iter: 717 loss: 1.99441893e-05
Iter: 718 loss: 1.94596814e-05
Iter: 719 loss: 1.9435447e-05
Iter: 720 loss: 1.95304583e-05
Iter: 721 loss: 1.94300119e-05
Iter: 722 loss: 1.94052736e-05
Iter: 723 loss: 1.94574641e-05
Iter: 724 loss: 1.93954511e-05
Iter: 725 loss: 1.93737069e-05
Iter: 726 loss: 1.96473593e-05
Iter: 727 loss: 1.93736087e-05
Iter: 728 loss: 1.93526757e-05
Iter: 729 loss: 1.93426022e-05
Iter: 730 loss: 1.93323904e-05
Iter: 731 loss: 1.9311221e-05
Iter: 732 loss: 1.93771921e-05
Iter: 733 loss: 1.93050946e-05
Iter: 734 loss: 1.92828138e-05
Iter: 735 loss: 1.9472418e-05
Iter: 736 loss: 1.92815678e-05
Iter: 737 loss: 1.92685329e-05
Iter: 738 loss: 1.92488551e-05
Iter: 739 loss: 1.92486204e-05
Iter: 740 loss: 1.92371117e-05
Iter: 741 loss: 1.92354055e-05
Iter: 742 loss: 1.92237894e-05
Iter: 743 loss: 1.92139833e-05
Iter: 744 loss: 1.92106381e-05
Iter: 745 loss: 1.91956169e-05
Iter: 746 loss: 1.93733322e-05
Iter: 747 loss: 1.9195375e-05
Iter: 748 loss: 1.91816034e-05
Iter: 749 loss: 1.91555664e-05
Iter: 750 loss: 1.97122699e-05
Iter: 751 loss: 1.91553772e-05
Iter: 752 loss: 1.91278868e-05
Iter: 753 loss: 1.91893414e-05
Iter: 754 loss: 1.91175313e-05
Iter: 755 loss: 1.90903284e-05
Iter: 756 loss: 1.92516673e-05
Iter: 757 loss: 1.90870323e-05
Iter: 758 loss: 1.90689225e-05
Iter: 759 loss: 1.92140487e-05
Iter: 760 loss: 1.90676419e-05
Iter: 761 loss: 1.90487808e-05
Iter: 762 loss: 1.9070927e-05
Iter: 763 loss: 1.90390347e-05
Iter: 764 loss: 1.90227365e-05
Iter: 765 loss: 1.90183564e-05
Iter: 766 loss: 1.90084065e-05
Iter: 767 loss: 1.89881866e-05
Iter: 768 loss: 1.89880138e-05
Iter: 769 loss: 1.89730963e-05
Iter: 770 loss: 1.89397942e-05
Iter: 771 loss: 1.93972301e-05
Iter: 772 loss: 1.89377315e-05
Iter: 773 loss: 1.89105976e-05
Iter: 774 loss: 1.89106104e-05
Iter: 775 loss: 1.8884326e-05
Iter: 776 loss: 1.89256498e-05
Iter: 777 loss: 1.88720769e-05
Iter: 778 loss: 1.88553768e-05
Iter: 779 loss: 1.90135142e-05
Iter: 780 loss: 1.88546874e-05
Iter: 781 loss: 1.88397353e-05
Iter: 782 loss: 1.88326903e-05
Iter: 783 loss: 1.88251561e-05
Iter: 784 loss: 1.88090562e-05
Iter: 785 loss: 1.8807943e-05
Iter: 786 loss: 1.87956393e-05
Iter: 787 loss: 1.87733676e-05
Iter: 788 loss: 1.88786362e-05
Iter: 789 loss: 1.87693586e-05
Iter: 790 loss: 1.87479491e-05
Iter: 791 loss: 1.88241575e-05
Iter: 792 loss: 1.87421665e-05
Iter: 793 loss: 1.87160895e-05
Iter: 794 loss: 1.88207341e-05
Iter: 795 loss: 1.87101723e-05
Iter: 796 loss: 1.86923098e-05
Iter: 797 loss: 1.86826401e-05
Iter: 798 loss: 1.86747748e-05
Iter: 799 loss: 1.86617362e-05
Iter: 800 loss: 1.86600755e-05
Iter: 801 loss: 1.86484904e-05
Iter: 802 loss: 1.86251382e-05
Iter: 803 loss: 1.90613373e-05
Iter: 804 loss: 1.86249235e-05
Iter: 805 loss: 1.86040179e-05
Iter: 806 loss: 1.87568749e-05
Iter: 807 loss: 1.86022189e-05
Iter: 808 loss: 1.85809531e-05
Iter: 809 loss: 1.86900543e-05
Iter: 810 loss: 1.85775025e-05
Iter: 811 loss: 1.85642311e-05
Iter: 812 loss: 1.85898461e-05
Iter: 813 loss: 1.85588669e-05
Iter: 814 loss: 1.8538256e-05
Iter: 815 loss: 1.85364261e-05
Iter: 816 loss: 1.8521403e-05
Iter: 817 loss: 1.84974051e-05
Iter: 818 loss: 1.84925884e-05
Iter: 819 loss: 1.84765904e-05
Iter: 820 loss: 1.84504242e-05
Iter: 821 loss: 1.86450416e-05
Iter: 822 loss: 1.84483797e-05
Iter: 823 loss: 1.84279561e-05
Iter: 824 loss: 1.85163226e-05
Iter: 825 loss: 1.8423545e-05
Iter: 826 loss: 1.84066521e-05
Iter: 827 loss: 1.85848221e-05
Iter: 828 loss: 1.84061628e-05
Iter: 829 loss: 1.83952943e-05
Iter: 830 loss: 1.83762568e-05
Iter: 831 loss: 1.8376204e-05
Iter: 832 loss: 1.83583397e-05
Iter: 833 loss: 1.86364632e-05
Iter: 834 loss: 1.8358407e-05
Iter: 835 loss: 1.83396351e-05
Iter: 836 loss: 1.83144475e-05
Iter: 837 loss: 1.8313116e-05
Iter: 838 loss: 1.82885669e-05
Iter: 839 loss: 1.83872871e-05
Iter: 840 loss: 1.82831682e-05
Iter: 841 loss: 1.82619187e-05
Iter: 842 loss: 1.85430872e-05
Iter: 843 loss: 1.82619442e-05
Iter: 844 loss: 1.82502881e-05
Iter: 845 loss: 1.82501044e-05
Iter: 846 loss: 1.8240944e-05
Iter: 847 loss: 1.82220301e-05
Iter: 848 loss: 1.83170832e-05
Iter: 849 loss: 1.82189669e-05
Iter: 850 loss: 1.8207571e-05
Iter: 851 loss: 1.8188628e-05
Iter: 852 loss: 1.81884807e-05
Iter: 853 loss: 1.81636642e-05
Iter: 854 loss: 1.82176282e-05
Iter: 855 loss: 1.81540818e-05
Iter: 856 loss: 1.81242522e-05
Iter: 857 loss: 1.82699805e-05
Iter: 858 loss: 1.81191062e-05
Iter: 859 loss: 1.80970783e-05
Iter: 860 loss: 1.83771917e-05
Iter: 861 loss: 1.80969346e-05
Iter: 862 loss: 1.80803472e-05
Iter: 863 loss: 1.80548304e-05
Iter: 864 loss: 1.80544012e-05
Iter: 865 loss: 1.80366e-05
Iter: 866 loss: 1.80364023e-05
Iter: 867 loss: 1.80200332e-05
Iter: 868 loss: 1.80412171e-05
Iter: 869 loss: 1.80116967e-05
Iter: 870 loss: 1.79995914e-05
Iter: 871 loss: 1.79894469e-05
Iter: 872 loss: 1.79859489e-05
Iter: 873 loss: 1.79688213e-05
Iter: 874 loss: 1.79686558e-05
Iter: 875 loss: 1.79564286e-05
Iter: 876 loss: 1.79412673e-05
Iter: 877 loss: 1.79396939e-05
Iter: 878 loss: 1.79178496e-05
Iter: 879 loss: 1.81511459e-05
Iter: 880 loss: 1.79172057e-05
Iter: 881 loss: 1.79037161e-05
Iter: 882 loss: 1.78757509e-05
Iter: 883 loss: 1.83617158e-05
Iter: 884 loss: 1.78749287e-05
Iter: 885 loss: 1.78452392e-05
Iter: 886 loss: 1.79848939e-05
Iter: 887 loss: 1.78400369e-05
Iter: 888 loss: 1.78185619e-05
Iter: 889 loss: 1.7966282e-05
Iter: 890 loss: 1.78166538e-05
Iter: 891 loss: 1.78021655e-05
Iter: 892 loss: 1.79517538e-05
Iter: 893 loss: 1.78017763e-05
Iter: 894 loss: 1.77882921e-05
Iter: 895 loss: 1.77744132e-05
Iter: 896 loss: 1.77717811e-05
Iter: 897 loss: 1.77538204e-05
Iter: 898 loss: 1.78284481e-05
Iter: 899 loss: 1.77498077e-05
Iter: 900 loss: 1.7727034e-05
Iter: 901 loss: 1.77952024e-05
Iter: 902 loss: 1.77201673e-05
Iter: 903 loss: 1.77018628e-05
Iter: 904 loss: 1.76764297e-05
Iter: 905 loss: 1.76753565e-05
Iter: 906 loss: 1.76632075e-05
Iter: 907 loss: 1.7657916e-05
Iter: 908 loss: 1.76446301e-05
Iter: 909 loss: 1.76296144e-05
Iter: 910 loss: 1.76276826e-05
Iter: 911 loss: 1.76121284e-05
Iter: 912 loss: 1.76119684e-05
Iter: 913 loss: 1.76012072e-05
Iter: 914 loss: 1.75785681e-05
Iter: 915 loss: 1.79574927e-05
Iter: 916 loss: 1.75779187e-05
Iter: 917 loss: 1.75533478e-05
Iter: 918 loss: 1.7606515e-05
Iter: 919 loss: 1.75436944e-05
Iter: 920 loss: 1.75165405e-05
Iter: 921 loss: 1.76518806e-05
Iter: 922 loss: 1.75120567e-05
Iter: 923 loss: 1.74913112e-05
Iter: 924 loss: 1.76977592e-05
Iter: 925 loss: 1.749066e-05
Iter: 926 loss: 1.74727102e-05
Iter: 927 loss: 1.74965553e-05
Iter: 928 loss: 1.74635843e-05
Iter: 929 loss: 1.74498164e-05
Iter: 930 loss: 1.74593188e-05
Iter: 931 loss: 1.74412453e-05
Iter: 932 loss: 1.74236229e-05
Iter: 933 loss: 1.76217618e-05
Iter: 934 loss: 1.74232482e-05
Iter: 935 loss: 1.74130837e-05
Iter: 936 loss: 1.73941571e-05
Iter: 937 loss: 1.78317023e-05
Iter: 938 loss: 1.73940316e-05
Iter: 939 loss: 1.73783046e-05
Iter: 940 loss: 1.73782391e-05
Iter: 941 loss: 1.73606968e-05
Iter: 942 loss: 1.73404405e-05
Iter: 943 loss: 1.73381122e-05
Iter: 944 loss: 1.73186236e-05
Iter: 945 loss: 1.7318529e-05
Iter: 946 loss: 1.73029784e-05
Iter: 947 loss: 1.72861346e-05
Iter: 948 loss: 1.72835316e-05
Iter: 949 loss: 1.72648906e-05
Iter: 950 loss: 1.72818545e-05
Iter: 951 loss: 1.7254084e-05
Iter: 952 loss: 1.7232509e-05
Iter: 953 loss: 1.73272874e-05
Iter: 954 loss: 1.72280779e-05
Iter: 955 loss: 1.72106229e-05
Iter: 956 loss: 1.73612698e-05
Iter: 957 loss: 1.72096716e-05
Iter: 958 loss: 1.71928368e-05
Iter: 959 loss: 1.72281652e-05
Iter: 960 loss: 1.71860302e-05
Iter: 961 loss: 1.71695829e-05
Iter: 962 loss: 1.71492065e-05
Iter: 963 loss: 1.71474403e-05
Iter: 964 loss: 1.71284919e-05
Iter: 965 loss: 1.71271095e-05
Iter: 966 loss: 1.71150041e-05
Iter: 967 loss: 1.7095248e-05
Iter: 968 loss: 1.70950389e-05
Iter: 969 loss: 1.70793883e-05
Iter: 970 loss: 1.726759e-05
Iter: 971 loss: 1.70792009e-05
Iter: 972 loss: 1.70629337e-05
Iter: 973 loss: 1.70880521e-05
Iter: 974 loss: 1.7055263e-05
Iter: 975 loss: 1.70439889e-05
Iter: 976 loss: 1.71197826e-05
Iter: 977 loss: 1.70428593e-05
Iter: 978 loss: 1.70302083e-05
Iter: 979 loss: 1.70140447e-05
Iter: 980 loss: 1.7012937e-05
Iter: 981 loss: 1.69912782e-05
Iter: 982 loss: 1.6989412e-05
Iter: 983 loss: 1.69733121e-05
Iter: 984 loss: 1.69452433e-05
Iter: 985 loss: 1.71208503e-05
Iter: 986 loss: 1.694194e-05
Iter: 987 loss: 1.69203704e-05
Iter: 988 loss: 1.70751737e-05
Iter: 989 loss: 1.69184423e-05
Iter: 990 loss: 1.69005525e-05
Iter: 991 loss: 1.69993727e-05
Iter: 992 loss: 1.68976949e-05
Iter: 993 loss: 1.68855913e-05
Iter: 994 loss: 1.6878832e-05
Iter: 995 loss: 1.68733277e-05
Iter: 996 loss: 1.68628885e-05
Iter: 997 loss: 1.68626211e-05
Iter: 998 loss: 1.68531e-05
Iter: 999 loss: 1.68329861e-05
Iter: 1000 loss: 1.71627e-05
Iter: 1001 loss: 1.68323859e-05
Iter: 1002 loss: 1.68105489e-05
Iter: 1003 loss: 1.69043633e-05
Iter: 1004 loss: 1.68061888e-05
Iter: 1005 loss: 1.67810849e-05
Iter: 1006 loss: 1.69381601e-05
Iter: 1007 loss: 1.67781454e-05
Iter: 1008 loss: 1.67641556e-05
Iter: 1009 loss: 1.67864546e-05
Iter: 1010 loss: 1.67575672e-05
Iter: 1011 loss: 1.67369799e-05
Iter: 1012 loss: 1.67764301e-05
Iter: 1013 loss: 1.67284707e-05
Iter: 1014 loss: 1.67153921e-05
Iter: 1015 loss: 1.67085454e-05
Iter: 1016 loss: 1.67025937e-05
Iter: 1017 loss: 1.66844766e-05
Iter: 1018 loss: 1.67455655e-05
Iter: 1019 loss: 1.66796126e-05
Iter: 1020 loss: 1.66611153e-05
Iter: 1021 loss: 1.67575017e-05
Iter: 1022 loss: 1.66582213e-05
Iter: 1023 loss: 1.66406226e-05
Iter: 1024 loss: 1.67462422e-05
Iter: 1025 loss: 1.66385289e-05
Iter: 1026 loss: 1.66237078e-05
Iter: 1027 loss: 1.66088903e-05
Iter: 1028 loss: 1.66058817e-05
Iter: 1029 loss: 1.65889469e-05
Iter: 1030 loss: 1.68548431e-05
Iter: 1031 loss: 1.65888941e-05
Iter: 1032 loss: 1.65725014e-05
Iter: 1033 loss: 1.65613055e-05
Iter: 1034 loss: 1.65554702e-05
Iter: 1035 loss: 1.65396341e-05
Iter: 1036 loss: 1.65888941e-05
Iter: 1037 loss: 1.65350466e-05
Iter: 1038 loss: 1.65217098e-05
Iter: 1039 loss: 1.65217352e-05
Iter: 1040 loss: 1.65136389e-05
Iter: 1041 loss: 1.65041729e-05
Iter: 1042 loss: 1.65032052e-05
Iter: 1043 loss: 1.64841622e-05
Iter: 1044 loss: 1.65557976e-05
Iter: 1045 loss: 1.64796093e-05
Iter: 1046 loss: 1.64659359e-05
Iter: 1047 loss: 1.64454286e-05
Iter: 1048 loss: 1.64450266e-05
Iter: 1049 loss: 1.64197554e-05
Iter: 1050 loss: 1.65180554e-05
Iter: 1051 loss: 1.64138746e-05
Iter: 1052 loss: 1.63935656e-05
Iter: 1053 loss: 1.65650963e-05
Iter: 1054 loss: 1.63925361e-05
Iter: 1055 loss: 1.63787608e-05
Iter: 1056 loss: 1.6510563e-05
Iter: 1057 loss: 1.63782297e-05
Iter: 1058 loss: 1.63679088e-05
Iter: 1059 loss: 1.63605109e-05
Iter: 1060 loss: 1.63569457e-05
Iter: 1061 loss: 1.63442801e-05
Iter: 1062 loss: 1.64314552e-05
Iter: 1063 loss: 1.63431323e-05
Iter: 1064 loss: 1.63284603e-05
Iter: 1065 loss: 1.63348213e-05
Iter: 1066 loss: 1.63184886e-05
Iter: 1067 loss: 1.6304286e-05
Iter: 1068 loss: 1.62975484e-05
Iter: 1069 loss: 1.62904489e-05
Iter: 1070 loss: 1.62760989e-05
Iter: 1071 loss: 1.62751603e-05
Iter: 1072 loss: 1.62644246e-05
Iter: 1073 loss: 1.62529868e-05
Iter: 1074 loss: 1.62509696e-05
Iter: 1075 loss: 1.62362358e-05
Iter: 1076 loss: 1.64626435e-05
Iter: 1077 loss: 1.62362194e-05
Iter: 1078 loss: 1.62283577e-05
Iter: 1079 loss: 1.62115339e-05
Iter: 1080 loss: 1.64870144e-05
Iter: 1081 loss: 1.62110373e-05
Iter: 1082 loss: 1.61929183e-05
Iter: 1083 loss: 1.6259648e-05
Iter: 1084 loss: 1.61884273e-05
Iter: 1085 loss: 1.61715616e-05
Iter: 1086 loss: 1.62558954e-05
Iter: 1087 loss: 1.61688e-05
Iter: 1088 loss: 1.61529424e-05
Iter: 1089 loss: 1.62526976e-05
Iter: 1090 loss: 1.61511598e-05
Iter: 1091 loss: 1.61350836e-05
Iter: 1092 loss: 1.6129894e-05
Iter: 1093 loss: 1.61206481e-05
Iter: 1094 loss: 1.61027347e-05
Iter: 1095 loss: 1.61937387e-05
Iter: 1096 loss: 1.60998679e-05
Iter: 1097 loss: 1.60815471e-05
Iter: 1098 loss: 1.61764183e-05
Iter: 1099 loss: 1.60787367e-05
Iter: 1100 loss: 1.60685086e-05
Iter: 1101 loss: 1.60544332e-05
Iter: 1102 loss: 1.60539785e-05
Iter: 1103 loss: 1.60444724e-05
Iter: 1104 loss: 1.60430354e-05
Iter: 1105 loss: 1.60336876e-05
Iter: 1106 loss: 1.60215532e-05
Iter: 1107 loss: 1.60207783e-05
Iter: 1108 loss: 1.60075178e-05
Iter: 1109 loss: 1.6199454e-05
Iter: 1110 loss: 1.60074233e-05
Iter: 1111 loss: 1.59969131e-05
Iter: 1112 loss: 1.59734591e-05
Iter: 1113 loss: 1.63084114e-05
Iter: 1114 loss: 1.59724023e-05
Iter: 1115 loss: 1.5947644e-05
Iter: 1116 loss: 1.60225536e-05
Iter: 1117 loss: 1.59404517e-05
Iter: 1118 loss: 1.59193842e-05
Iter: 1119 loss: 1.60669442e-05
Iter: 1120 loss: 1.59176961e-05
Iter: 1121 loss: 1.59035353e-05
Iter: 1122 loss: 1.6055e-05
Iter: 1123 loss: 1.5903177e-05
Iter: 1124 loss: 1.58917701e-05
Iter: 1125 loss: 1.58980602e-05
Iter: 1126 loss: 1.58842922e-05
Iter: 1127 loss: 1.58718685e-05
Iter: 1128 loss: 1.5883641e-05
Iter: 1129 loss: 1.58646799e-05
Iter: 1130 loss: 1.58487383e-05
Iter: 1131 loss: 1.59815063e-05
Iter: 1132 loss: 1.58477451e-05
Iter: 1133 loss: 1.58372968e-05
Iter: 1134 loss: 1.58164075e-05
Iter: 1135 loss: 1.62140168e-05
Iter: 1136 loss: 1.58160801e-05
Iter: 1137 loss: 1.58016228e-05
Iter: 1138 loss: 1.58008206e-05
Iter: 1139 loss: 1.57858722e-05
Iter: 1140 loss: 1.57839695e-05
Iter: 1141 loss: 1.57733739e-05
Iter: 1142 loss: 1.57621143e-05
Iter: 1143 loss: 1.57620198e-05
Iter: 1144 loss: 1.57530285e-05
Iter: 1145 loss: 1.57377854e-05
Iter: 1146 loss: 1.57376635e-05
Iter: 1147 loss: 1.57212817e-05
Iter: 1148 loss: 1.57353497e-05
Iter: 1149 loss: 1.57115137e-05
Iter: 1150 loss: 1.56919232e-05
Iter: 1151 loss: 1.58156508e-05
Iter: 1152 loss: 1.56896913e-05
Iter: 1153 loss: 1.56741553e-05
Iter: 1154 loss: 1.57977374e-05
Iter: 1155 loss: 1.56731548e-05
Iter: 1156 loss: 1.56584611e-05
Iter: 1157 loss: 1.56709903e-05
Iter: 1158 loss: 1.564987e-05
Iter: 1159 loss: 1.56345359e-05
Iter: 1160 loss: 1.56562419e-05
Iter: 1161 loss: 1.56269416e-05
Iter: 1162 loss: 1.56139104e-05
Iter: 1163 loss: 1.56139286e-05
Iter: 1164 loss: 1.56058231e-05
Iter: 1165 loss: 1.55901889e-05
Iter: 1166 loss: 1.59201154e-05
Iter: 1167 loss: 1.55900379e-05
Iter: 1168 loss: 1.55778162e-05
Iter: 1169 loss: 1.57595241e-05
Iter: 1170 loss: 1.55778016e-05
Iter: 1171 loss: 1.5564945e-05
Iter: 1172 loss: 1.55768575e-05
Iter: 1173 loss: 1.55575017e-05
Iter: 1174 loss: 1.55461712e-05
Iter: 1175 loss: 1.56023361e-05
Iter: 1176 loss: 1.55442431e-05
Iter: 1177 loss: 1.55299895e-05
Iter: 1178 loss: 1.55171e-05
Iter: 1179 loss: 1.55135021e-05
Iter: 1180 loss: 1.54954232e-05
Iter: 1181 loss: 1.54981535e-05
Iter: 1182 loss: 1.54817426e-05
Iter: 1183 loss: 1.54626068e-05
Iter: 1184 loss: 1.56346541e-05
Iter: 1185 loss: 1.54616719e-05
Iter: 1186 loss: 1.54496029e-05
Iter: 1187 loss: 1.55621656e-05
Iter: 1188 loss: 1.54490863e-05
Iter: 1189 loss: 1.5438538e-05
Iter: 1190 loss: 1.54625504e-05
Iter: 1191 loss: 1.54346471e-05
Iter: 1192 loss: 1.54242771e-05
Iter: 1193 loss: 1.54201534e-05
Iter: 1194 loss: 1.54147638e-05
Iter: 1195 loss: 1.54017343e-05
Iter: 1196 loss: 1.5579164e-05
Iter: 1197 loss: 1.54016561e-05
Iter: 1198 loss: 1.53907185e-05
Iter: 1199 loss: 1.53724868e-05
Iter: 1200 loss: 1.5372425e-05
Iter: 1201 loss: 1.53567562e-05
Iter: 1202 loss: 1.54824083e-05
Iter: 1203 loss: 1.53556175e-05
Iter: 1204 loss: 1.53394103e-05
Iter: 1205 loss: 1.54070785e-05
Iter: 1206 loss: 1.5335987e-05
Iter: 1207 loss: 1.5326219e-05
Iter: 1208 loss: 1.53564288e-05
Iter: 1209 loss: 1.53232813e-05
Iter: 1210 loss: 1.53117744e-05
Iter: 1211 loss: 1.5329857e-05
Iter: 1212 loss: 1.53065703e-05
Iter: 1213 loss: 1.52977627e-05
Iter: 1214 loss: 1.52866924e-05
Iter: 1215 loss: 1.52859666e-05
Iter: 1216 loss: 1.52689172e-05
Iter: 1217 loss: 1.53294932e-05
Iter: 1218 loss: 1.52645243e-05
Iter: 1219 loss: 1.52479297e-05
Iter: 1220 loss: 1.53691672e-05
Iter: 1221 loss: 1.52466137e-05
Iter: 1222 loss: 1.52311977e-05
Iter: 1223 loss: 1.52862194e-05
Iter: 1224 loss: 1.5227417e-05
Iter: 1225 loss: 1.52143675e-05
Iter: 1226 loss: 1.52116108e-05
Iter: 1227 loss: 1.52028597e-05
Iter: 1228 loss: 1.51905015e-05
Iter: 1229 loss: 1.5190406e-05
Iter: 1230 loss: 1.51803724e-05
Iter: 1231 loss: 1.51730155e-05
Iter: 1232 loss: 1.51697077e-05
Iter: 1233 loss: 1.5158831e-05
Iter: 1234 loss: 1.5188265e-05
Iter: 1235 loss: 1.51551021e-05
Iter: 1236 loss: 1.51419972e-05
Iter: 1237 loss: 1.52398916e-05
Iter: 1238 loss: 1.51409822e-05
Iter: 1239 loss: 1.51322674e-05
Iter: 1240 loss: 1.513188e-05
Iter: 1241 loss: 1.51251643e-05
Iter: 1242 loss: 1.51099484e-05
Iter: 1243 loss: 1.51656968e-05
Iter: 1244 loss: 1.51063387e-05
Iter: 1245 loss: 1.5095764e-05
Iter: 1246 loss: 1.50764417e-05
Iter: 1247 loss: 1.55267189e-05
Iter: 1248 loss: 1.50764272e-05
Iter: 1249 loss: 1.50571723e-05
Iter: 1250 loss: 1.52154853e-05
Iter: 1251 loss: 1.5055919e-05
Iter: 1252 loss: 1.50438664e-05
Iter: 1253 loss: 1.51663162e-05
Iter: 1254 loss: 1.50434425e-05
Iter: 1255 loss: 1.50337864e-05
Iter: 1256 loss: 1.50737778e-05
Iter: 1257 loss: 1.50316228e-05
Iter: 1258 loss: 1.50225733e-05
Iter: 1259 loss: 1.50153737e-05
Iter: 1260 loss: 1.5012618e-05
Iter: 1261 loss: 1.50007836e-05
Iter: 1262 loss: 1.513604e-05
Iter: 1263 loss: 1.50005271e-05
Iter: 1264 loss: 1.49888674e-05
Iter: 1265 loss: 1.49839234e-05
Iter: 1266 loss: 1.49777752e-05
Iter: 1267 loss: 1.49640673e-05
Iter: 1268 loss: 1.49763109e-05
Iter: 1269 loss: 1.49560619e-05
Iter: 1270 loss: 1.49432899e-05
Iter: 1271 loss: 1.49429889e-05
Iter: 1272 loss: 1.49359457e-05
Iter: 1273 loss: 1.49332136e-05
Iter: 1274 loss: 1.49292355e-05
Iter: 1275 loss: 1.49183752e-05
Iter: 1276 loss: 1.4991123e-05
Iter: 1277 loss: 1.49172993e-05
Iter: 1278 loss: 1.49099451e-05
Iter: 1279 loss: 1.48948484e-05
Iter: 1280 loss: 1.51592358e-05
Iter: 1281 loss: 1.48943909e-05
Iter: 1282 loss: 1.48778136e-05
Iter: 1283 loss: 1.49428452e-05
Iter: 1284 loss: 1.48738636e-05
Iter: 1285 loss: 1.48577537e-05
Iter: 1286 loss: 1.49672251e-05
Iter: 1287 loss: 1.48561412e-05
Iter: 1288 loss: 1.48415165e-05
Iter: 1289 loss: 1.49179377e-05
Iter: 1290 loss: 1.48392683e-05
Iter: 1291 loss: 1.48270592e-05
Iter: 1292 loss: 1.48275394e-05
Iter: 1293 loss: 1.48173967e-05
Iter: 1294 loss: 1.48060217e-05
Iter: 1295 loss: 1.49295411e-05
Iter: 1296 loss: 1.48056151e-05
Iter: 1297 loss: 1.47953106e-05
Iter: 1298 loss: 1.48055133e-05
Iter: 1299 loss: 1.47894607e-05
Iter: 1300 loss: 1.47802602e-05
Iter: 1301 loss: 1.47757964e-05
Iter: 1302 loss: 1.47713272e-05
Iter: 1303 loss: 1.47626288e-05
Iter: 1304 loss: 1.47619457e-05
Iter: 1305 loss: 1.47548544e-05
Iter: 1306 loss: 1.47441333e-05
Iter: 1307 loss: 1.47439005e-05
Iter: 1308 loss: 1.47301862e-05
Iter: 1309 loss: 1.48728168e-05
Iter: 1310 loss: 1.47297897e-05
Iter: 1311 loss: 1.47206501e-05
Iter: 1312 loss: 1.47044266e-05
Iter: 1313 loss: 1.51033428e-05
Iter: 1314 loss: 1.47043102e-05
Iter: 1315 loss: 1.46893726e-05
Iter: 1316 loss: 1.47507599e-05
Iter: 1317 loss: 1.46861394e-05
Iter: 1318 loss: 1.4674164e-05
Iter: 1319 loss: 1.47822229e-05
Iter: 1320 loss: 1.46736566e-05
Iter: 1321 loss: 1.46639395e-05
Iter: 1322 loss: 1.4717335e-05
Iter: 1323 loss: 1.46624852e-05
Iter: 1324 loss: 1.46534639e-05
Iter: 1325 loss: 1.46492603e-05
Iter: 1326 loss: 1.46447856e-05
Iter: 1327 loss: 1.46329176e-05
Iter: 1328 loss: 1.47007486e-05
Iter: 1329 loss: 1.46312241e-05
Iter: 1330 loss: 1.46184666e-05
Iter: 1331 loss: 1.46484363e-05
Iter: 1332 loss: 1.46138036e-05
Iter: 1333 loss: 1.46035354e-05
Iter: 1334 loss: 1.45944396e-05
Iter: 1335 loss: 1.4591923e-05
Iter: 1336 loss: 1.45840313e-05
Iter: 1337 loss: 1.45824142e-05
Iter: 1338 loss: 1.45748754e-05
Iter: 1339 loss: 1.45669601e-05
Iter: 1340 loss: 1.4565594e-05
Iter: 1341 loss: 1.45565173e-05
Iter: 1342 loss: 1.4700232e-05
Iter: 1343 loss: 1.45565355e-05
Iter: 1344 loss: 1.45502554e-05
Iter: 1345 loss: 1.45376216e-05
Iter: 1346 loss: 1.4777369e-05
Iter: 1347 loss: 1.45375634e-05
Iter: 1348 loss: 1.45229978e-05
Iter: 1349 loss: 1.45322874e-05
Iter: 1350 loss: 1.45135973e-05
Iter: 1351 loss: 1.44979385e-05
Iter: 1352 loss: 1.46742568e-05
Iter: 1353 loss: 1.44975711e-05
Iter: 1354 loss: 1.44862879e-05
Iter: 1355 loss: 1.45786962e-05
Iter: 1356 loss: 1.44856785e-05
Iter: 1357 loss: 1.44765072e-05
Iter: 1358 loss: 1.44773221e-05
Iter: 1359 loss: 1.44694222e-05
Iter: 1360 loss: 1.44594514e-05
Iter: 1361 loss: 1.45103604e-05
Iter: 1362 loss: 1.4457999e-05
Iter: 1363 loss: 1.44486639e-05
Iter: 1364 loss: 1.44915584e-05
Iter: 1365 loss: 1.4446895e-05
Iter: 1366 loss: 1.44399964e-05
Iter: 1367 loss: 1.44277801e-05
Iter: 1368 loss: 1.44277674e-05
Iter: 1369 loss: 1.44179294e-05
Iter: 1370 loss: 1.44177375e-05
Iter: 1371 loss: 1.44076803e-05
Iter: 1372 loss: 1.43996649e-05
Iter: 1373 loss: 1.43966645e-05
Iter: 1374 loss: 1.43865454e-05
Iter: 1375 loss: 1.43865682e-05
Iter: 1376 loss: 1.43791567e-05
Iter: 1377 loss: 1.43672514e-05
Iter: 1378 loss: 1.43671987e-05
Iter: 1379 loss: 1.43549314e-05
Iter: 1380 loss: 1.43749949e-05
Iter: 1381 loss: 1.43494144e-05
Iter: 1382 loss: 1.43384368e-05
Iter: 1383 loss: 1.4434534e-05
Iter: 1384 loss: 1.43377674e-05
Iter: 1385 loss: 1.43289499e-05
Iter: 1386 loss: 1.43863181e-05
Iter: 1387 loss: 1.43278248e-05
Iter: 1388 loss: 1.43191673e-05
Iter: 1389 loss: 1.43195393e-05
Iter: 1390 loss: 1.43123088e-05
Iter: 1391 loss: 1.43014076e-05
Iter: 1392 loss: 1.43317866e-05
Iter: 1393 loss: 1.42980061e-05
Iter: 1394 loss: 1.42857989e-05
Iter: 1395 loss: 1.43535735e-05
Iter: 1396 loss: 1.42840563e-05
Iter: 1397 loss: 1.42748013e-05
Iter: 1398 loss: 1.42629124e-05
Iter: 1399 loss: 1.42620947e-05
Iter: 1400 loss: 1.42542631e-05
Iter: 1401 loss: 1.4253711e-05
Iter: 1402 loss: 1.42459303e-05
Iter: 1403 loss: 1.42474064e-05
Iter: 1404 loss: 1.42400531e-05
Iter: 1405 loss: 1.42328863e-05
Iter: 1406 loss: 1.42934432e-05
Iter: 1407 loss: 1.4232508e-05
Iter: 1408 loss: 1.42256531e-05
Iter: 1409 loss: 1.4215937e-05
Iter: 1410 loss: 1.42154586e-05
Iter: 1411 loss: 1.42034733e-05
Iter: 1412 loss: 1.42025747e-05
Iter: 1413 loss: 1.41938835e-05
Iter: 1414 loss: 1.41784885e-05
Iter: 1415 loss: 1.42899826e-05
Iter: 1416 loss: 1.41772107e-05
Iter: 1417 loss: 1.41667197e-05
Iter: 1418 loss: 1.42948793e-05
Iter: 1419 loss: 1.41665441e-05
Iter: 1420 loss: 1.41585761e-05
Iter: 1421 loss: 1.41692553e-05
Iter: 1422 loss: 1.41546516e-05
Iter: 1423 loss: 1.41471119e-05
Iter: 1424 loss: 1.4159059e-05
Iter: 1425 loss: 1.41436676e-05
Iter: 1426 loss: 1.41349756e-05
Iter: 1427 loss: 1.42011077e-05
Iter: 1428 loss: 1.41344299e-05
Iter: 1429 loss: 1.41278852e-05
Iter: 1430 loss: 1.41183564e-05
Iter: 1431 loss: 1.41182809e-05
Iter: 1432 loss: 1.41088349e-05
Iter: 1433 loss: 1.42038043e-05
Iter: 1434 loss: 1.41085375e-05
Iter: 1435 loss: 1.40976972e-05
Iter: 1436 loss: 1.41040509e-05
Iter: 1437 loss: 1.40907405e-05
Iter: 1438 loss: 1.40810234e-05
Iter: 1439 loss: 1.4147945e-05
Iter: 1440 loss: 1.40801967e-05
Iter: 1441 loss: 1.40705733e-05
Iter: 1442 loss: 1.40683687e-05
Iter: 1443 loss: 1.40622342e-05
Iter: 1444 loss: 1.40524371e-05
Iter: 1445 loss: 1.40524717e-05
Iter: 1446 loss: 1.40445936e-05
Iter: 1447 loss: 1.40331922e-05
Iter: 1448 loss: 1.41113569e-05
Iter: 1449 loss: 1.40319962e-05
Iter: 1450 loss: 1.4023266e-05
Iter: 1451 loss: 1.41012224e-05
Iter: 1452 loss: 1.40229067e-05
Iter: 1453 loss: 1.40149368e-05
Iter: 1454 loss: 1.40217026e-05
Iter: 1455 loss: 1.40101702e-05
Iter: 1456 loss: 1.40008124e-05
Iter: 1457 loss: 1.40098837e-05
Iter: 1458 loss: 1.3995641e-05
Iter: 1459 loss: 1.39856347e-05
Iter: 1460 loss: 1.4102081e-05
Iter: 1461 loss: 1.39853373e-05
Iter: 1462 loss: 1.3978377e-05
Iter: 1463 loss: 1.39696785e-05
Iter: 1464 loss: 1.39689964e-05
Iter: 1465 loss: 1.3959474e-05
Iter: 1466 loss: 1.4025105e-05
Iter: 1467 loss: 1.39586919e-05
Iter: 1468 loss: 1.3948551e-05
Iter: 1469 loss: 1.39851763e-05
Iter: 1470 loss: 1.39459617e-05
Iter: 1471 loss: 1.39397143e-05
Iter: 1472 loss: 1.39569584e-05
Iter: 1473 loss: 1.39375097e-05
Iter: 1474 loss: 1.39287904e-05
Iter: 1475 loss: 1.39293024e-05
Iter: 1476 loss: 1.39219737e-05
Iter: 1477 loss: 1.39122276e-05
Iter: 1478 loss: 1.39050708e-05
Iter: 1479 loss: 1.39017866e-05
Iter: 1480 loss: 1.38891101e-05
Iter: 1481 loss: 1.39776675e-05
Iter: 1482 loss: 1.38877913e-05
Iter: 1483 loss: 1.38784635e-05
Iter: 1484 loss: 1.39722388e-05
Iter: 1485 loss: 1.38781488e-05
Iter: 1486 loss: 1.3870057e-05
Iter: 1487 loss: 1.38837113e-05
Iter: 1488 loss: 1.38663263e-05
Iter: 1489 loss: 1.38583709e-05
Iter: 1490 loss: 1.38680161e-05
Iter: 1491 loss: 1.38542055e-05
Iter: 1492 loss: 1.38467967e-05
Iter: 1493 loss: 1.393398e-05
Iter: 1494 loss: 1.38466103e-05
Iter: 1495 loss: 1.38404484e-05
Iter: 1496 loss: 1.38297873e-05
Iter: 1497 loss: 1.38297528e-05
Iter: 1498 loss: 1.38177274e-05
Iter: 1499 loss: 1.38677924e-05
Iter: 1500 loss: 1.38150735e-05
Iter: 1501 loss: 1.38024152e-05
Iter: 1502 loss: 1.39010199e-05
Iter: 1503 loss: 1.38015066e-05
Iter: 1504 loss: 1.37948746e-05
Iter: 1505 loss: 1.38042706e-05
Iter: 1506 loss: 1.37915213e-05
Iter: 1507 loss: 1.37823899e-05
Iter: 1508 loss: 1.38003252e-05
Iter: 1509 loss: 1.377837e-05
Iter: 1510 loss: 1.37714542e-05
Iter: 1511 loss: 1.37648258e-05
Iter: 1512 loss: 1.37631e-05
Iter: 1513 loss: 1.37524494e-05
Iter: 1514 loss: 1.37933139e-05
Iter: 1515 loss: 1.3749891e-05
Iter: 1516 loss: 1.37398265e-05
Iter: 1517 loss: 1.38248524e-05
Iter: 1518 loss: 1.3739299e-05
Iter: 1519 loss: 1.37297538e-05
Iter: 1520 loss: 1.37493598e-05
Iter: 1521 loss: 1.37260504e-05
Iter: 1522 loss: 1.37169727e-05
Iter: 1523 loss: 1.37252546e-05
Iter: 1524 loss: 1.37116094e-05
Iter: 1525 loss: 1.37032621e-05
Iter: 1526 loss: 1.38127834e-05
Iter: 1527 loss: 1.37032921e-05
Iter: 1528 loss: 1.3696721e-05
Iter: 1529 loss: 1.36906656e-05
Iter: 1530 loss: 1.36890922e-05
Iter: 1531 loss: 1.36807867e-05
Iter: 1532 loss: 1.37069255e-05
Iter: 1533 loss: 1.36784884e-05
Iter: 1534 loss: 1.36701656e-05
Iter: 1535 loss: 1.37591869e-05
Iter: 1536 loss: 1.36700273e-05
Iter: 1537 loss: 1.36650215e-05
Iter: 1538 loss: 1.36616272e-05
Iter: 1539 loss: 1.36597137e-05
Iter: 1540 loss: 1.36495482e-05
Iter: 1541 loss: 1.3685506e-05
Iter: 1542 loss: 1.36468307e-05
Iter: 1543 loss: 1.36394874e-05
Iter: 1544 loss: 1.36287035e-05
Iter: 1545 loss: 1.36284689e-05
Iter: 1546 loss: 1.36153612e-05
Iter: 1547 loss: 1.36711751e-05
Iter: 1548 loss: 1.36126437e-05
Iter: 1549 loss: 1.36032177e-05
Iter: 1550 loss: 1.37350817e-05
Iter: 1551 loss: 1.3603174e-05
Iter: 1552 loss: 1.35959481e-05
Iter: 1553 loss: 1.36187573e-05
Iter: 1554 loss: 1.35939135e-05
Iter: 1555 loss: 1.35874307e-05
Iter: 1556 loss: 1.35900718e-05
Iter: 1557 loss: 1.35831669e-05
Iter: 1558 loss: 1.3576343e-05
Iter: 1559 loss: 1.36430044e-05
Iter: 1560 loss: 1.35760438e-05
Iter: 1561 loss: 1.35694245e-05
Iter: 1562 loss: 1.35630125e-05
Iter: 1563 loss: 1.35615355e-05
Iter: 1564 loss: 1.35522423e-05
Iter: 1565 loss: 1.35716582e-05
Iter: 1566 loss: 1.35484224e-05
Iter: 1567 loss: 1.35398268e-05
Iter: 1568 loss: 1.35398877e-05
Iter: 1569 loss: 1.3534599e-05
Iter: 1570 loss: 1.35301398e-05
Iter: 1571 loss: 1.35287128e-05
Iter: 1572 loss: 1.35196233e-05
Iter: 1573 loss: 1.3586513e-05
Iter: 1574 loss: 1.3518913e-05
Iter: 1575 loss: 1.3513687e-05
Iter: 1576 loss: 1.35049104e-05
Iter: 1577 loss: 1.35049013e-05
Iter: 1578 loss: 1.34944694e-05
Iter: 1579 loss: 1.35187911e-05
Iter: 1580 loss: 1.3490554e-05
Iter: 1581 loss: 1.34804886e-05
Iter: 1582 loss: 1.35875471e-05
Iter: 1583 loss: 1.34802694e-05
Iter: 1584 loss: 1.34715456e-05
Iter: 1585 loss: 1.35016817e-05
Iter: 1586 loss: 1.34692646e-05
Iter: 1587 loss: 1.34618158e-05
Iter: 1588 loss: 1.3466647e-05
Iter: 1589 loss: 1.34569091e-05
Iter: 1590 loss: 1.34496322e-05
Iter: 1591 loss: 1.35199898e-05
Iter: 1592 loss: 1.34493839e-05
Iter: 1593 loss: 1.34425445e-05
Iter: 1594 loss: 1.34409183e-05
Iter: 1595 loss: 1.34363872e-05
Iter: 1596 loss: 1.34287129e-05
Iter: 1597 loss: 1.34381207e-05
Iter: 1598 loss: 1.34248767e-05
Iter: 1599 loss: 1.34182155e-05
Iter: 1600 loss: 1.3418221e-05
Iter: 1601 loss: 1.34133925e-05
Iter: 1602 loss: 1.34056063e-05
Iter: 1603 loss: 1.34055299e-05
Iter: 1604 loss: 1.33952308e-05
Iter: 1605 loss: 1.3489238e-05
Iter: 1606 loss: 1.33948179e-05
Iter: 1607 loss: 1.33884778e-05
Iter: 1608 loss: 1.33783424e-05
Iter: 1609 loss: 1.3378286e-05
Iter: 1610 loss: 1.33678514e-05
Iter: 1611 loss: 1.34070151e-05
Iter: 1612 loss: 1.33653357e-05
Iter: 1613 loss: 1.3357434e-05
Iter: 1614 loss: 1.34491993e-05
Iter: 1615 loss: 1.33572776e-05
Iter: 1616 loss: 1.33508092e-05
Iter: 1617 loss: 1.33757057e-05
Iter: 1618 loss: 1.33492795e-05
Iter: 1619 loss: 1.33434623e-05
Iter: 1620 loss: 1.33445528e-05
Iter: 1621 loss: 1.33390631e-05
Iter: 1622 loss: 1.33320736e-05
Iter: 1623 loss: 1.33795766e-05
Iter: 1624 loss: 1.33312897e-05
Iter: 1625 loss: 1.33237536e-05
Iter: 1626 loss: 1.33228414e-05
Iter: 1627 loss: 1.33173762e-05
Iter: 1628 loss: 1.33085796e-05
Iter: 1629 loss: 1.33163521e-05
Iter: 1630 loss: 1.33035919e-05
Iter: 1631 loss: 1.32971636e-05
Iter: 1632 loss: 1.3296878e-05
Iter: 1633 loss: 1.32918121e-05
Iter: 1634 loss: 1.32848345e-05
Iter: 1635 loss: 1.32846008e-05
Iter: 1636 loss: 1.32766245e-05
Iter: 1637 loss: 1.33813637e-05
Iter: 1638 loss: 1.32765372e-05
Iter: 1639 loss: 1.32714476e-05
Iter: 1640 loss: 1.32620935e-05
Iter: 1641 loss: 1.34771462e-05
Iter: 1642 loss: 1.32620571e-05
Iter: 1643 loss: 1.32521754e-05
Iter: 1644 loss: 1.32753494e-05
Iter: 1645 loss: 1.32485911e-05
Iter: 1646 loss: 1.32396071e-05
Iter: 1647 loss: 1.3343968e-05
Iter: 1648 loss: 1.32395135e-05
Iter: 1649 loss: 1.32322166e-05
Iter: 1650 loss: 1.32663881e-05
Iter: 1651 loss: 1.32309142e-05
Iter: 1652 loss: 1.32250461e-05
Iter: 1653 loss: 1.32282776e-05
Iter: 1654 loss: 1.32211935e-05
Iter: 1655 loss: 1.3215119e-05
Iter: 1656 loss: 1.32565046e-05
Iter: 1657 loss: 1.3214476e-05
Iter: 1658 loss: 1.32081132e-05
Iter: 1659 loss: 1.32143286e-05
Iter: 1660 loss: 1.32045952e-05
Iter: 1661 loss: 1.31985944e-05
Iter: 1662 loss: 1.31965608e-05
Iter: 1663 loss: 1.31930892e-05
Iter: 1664 loss: 1.31863981e-05
Iter: 1665 loss: 1.31864599e-05
Iter: 1666 loss: 1.31802954e-05
Iter: 1667 loss: 1.31710531e-05
Iter: 1668 loss: 1.31709867e-05
Iter: 1669 loss: 1.31624574e-05
Iter: 1670 loss: 1.31624665e-05
Iter: 1671 loss: 1.31571314e-05
Iter: 1672 loss: 1.31475445e-05
Iter: 1673 loss: 1.33842841e-05
Iter: 1674 loss: 1.31475217e-05
Iter: 1675 loss: 1.31380193e-05
Iter: 1676 loss: 1.3160743e-05
Iter: 1677 loss: 1.31347406e-05
Iter: 1678 loss: 1.31274737e-05
Iter: 1679 loss: 1.32273417e-05
Iter: 1680 loss: 1.31274292e-05
Iter: 1681 loss: 1.31215793e-05
Iter: 1682 loss: 1.3145228e-05
Iter: 1683 loss: 1.31202723e-05
Iter: 1684 loss: 1.31144643e-05
Iter: 1685 loss: 1.31133947e-05
Iter: 1686 loss: 1.31095585e-05
Iter: 1687 loss: 1.3101826e-05
Iter: 1688 loss: 1.3144032e-05
Iter: 1689 loss: 1.31006291e-05
Iter: 1690 loss: 1.30925819e-05
Iter: 1691 loss: 1.3112247e-05
Iter: 1692 loss: 1.30896051e-05
Iter: 1693 loss: 1.30835951e-05
Iter: 1694 loss: 1.30816898e-05
Iter: 1695 loss: 1.30781855e-05
Iter: 1696 loss: 1.3072422e-05
Iter: 1697 loss: 1.30722092e-05
Iter: 1698 loss: 1.30670451e-05
Iter: 1699 loss: 1.30619519e-05
Iter: 1700 loss: 1.30608933e-05
Iter: 1701 loss: 1.30550725e-05
Iter: 1702 loss: 1.30551416e-05
Iter: 1703 loss: 1.30507615e-05
Iter: 1704 loss: 1.30417075e-05
Iter: 1705 loss: 1.31986453e-05
Iter: 1706 loss: 1.30415774e-05
Iter: 1707 loss: 1.30312983e-05
Iter: 1708 loss: 1.30479566e-05
Iter: 1709 loss: 1.30266289e-05
Iter: 1710 loss: 1.30181115e-05
Iter: 1711 loss: 1.31342395e-05
Iter: 1712 loss: 1.30180688e-05
Iter: 1713 loss: 1.30115295e-05
Iter: 1714 loss: 1.30455664e-05
Iter: 1715 loss: 1.30104036e-05
Iter: 1716 loss: 1.30045337e-05
Iter: 1717 loss: 1.30063845e-05
Iter: 1718 loss: 1.30003191e-05
Iter: 1719 loss: 1.2993688e-05
Iter: 1720 loss: 1.30271665e-05
Iter: 1721 loss: 1.29925802e-05
Iter: 1722 loss: 1.29857171e-05
Iter: 1723 loss: 1.30087392e-05
Iter: 1724 loss: 1.298386e-05
Iter: 1725 loss: 1.29787986e-05
Iter: 1726 loss: 1.29722139e-05
Iter: 1727 loss: 1.29716918e-05
Iter: 1728 loss: 1.29651326e-05
Iter: 1729 loss: 1.2965078e-05
Iter: 1730 loss: 1.29588752e-05
Iter: 1731 loss: 1.29546524e-05
Iter: 1732 loss: 1.29523341e-05
Iter: 1733 loss: 1.2946065e-05
Iter: 1734 loss: 1.30337266e-05
Iter: 1735 loss: 1.2945974e-05
Iter: 1736 loss: 1.29407345e-05
Iter: 1737 loss: 1.29322325e-05
Iter: 1738 loss: 1.29322034e-05
Iter: 1739 loss: 1.29238342e-05
Iter: 1740 loss: 1.29368709e-05
Iter: 1741 loss: 1.29199816e-05
Iter: 1742 loss: 1.2911898e-05
Iter: 1743 loss: 1.29795226e-05
Iter: 1744 loss: 1.29115169e-05
Iter: 1745 loss: 1.29041582e-05
Iter: 1746 loss: 1.29411983e-05
Iter: 1747 loss: 1.29029177e-05
Iter: 1748 loss: 1.28961801e-05
Iter: 1749 loss: 1.29007876e-05
Iter: 1750 loss: 1.28918655e-05
Iter: 1751 loss: 1.28849797e-05
Iter: 1752 loss: 1.29164282e-05
Iter: 1753 loss: 1.28835727e-05
Iter: 1754 loss: 1.28765314e-05
Iter: 1755 loss: 1.29084801e-05
Iter: 1756 loss: 1.28753018e-05
Iter: 1757 loss: 1.28701859e-05
Iter: 1758 loss: 1.28642278e-05
Iter: 1759 loss: 1.28635347e-05
Iter: 1760 loss: 1.28580186e-05
Iter: 1761 loss: 1.28579541e-05
Iter: 1762 loss: 1.28526081e-05
Iter: 1763 loss: 1.28507882e-05
Iter: 1764 loss: 1.2847735e-05
Iter: 1765 loss: 1.28420688e-05
Iter: 1766 loss: 1.28911524e-05
Iter: 1767 loss: 1.28417969e-05
Iter: 1768 loss: 1.28359834e-05
Iter: 1769 loss: 1.28271186e-05
Iter: 1770 loss: 1.2827094e-05
Iter: 1771 loss: 1.28182573e-05
Iter: 1772 loss: 1.2832229e-05
Iter: 1773 loss: 1.28141091e-05
Iter: 1774 loss: 1.28060601e-05
Iter: 1775 loss: 1.28727779e-05
Iter: 1776 loss: 1.28056117e-05
Iter: 1777 loss: 1.27988105e-05
Iter: 1778 loss: 1.28449883e-05
Iter: 1779 loss: 1.27981311e-05
Iter: 1780 loss: 1.27926251e-05
Iter: 1781 loss: 1.2798384e-05
Iter: 1782 loss: 1.27895846e-05
Iter: 1783 loss: 1.27838821e-05
Iter: 1784 loss: 1.27988696e-05
Iter: 1785 loss: 1.27820513e-05
Iter: 1786 loss: 1.2775412e-05
Iter: 1787 loss: 1.28086049e-05
Iter: 1788 loss: 1.27744024e-05
Iter: 1789 loss: 1.27692274e-05
Iter: 1790 loss: 1.27615422e-05
Iter: 1791 loss: 1.27613839e-05
Iter: 1792 loss: 1.27550202e-05
Iter: 1793 loss: 1.27549501e-05
Iter: 1794 loss: 1.2748731e-05
Iter: 1795 loss: 1.27521216e-05
Iter: 1796 loss: 1.27446237e-05
Iter: 1797 loss: 1.27393932e-05
Iter: 1798 loss: 1.27761232e-05
Iter: 1799 loss: 1.27389631e-05
Iter: 1800 loss: 1.2733648e-05
Iter: 1801 loss: 1.27287394e-05
Iter: 1802 loss: 1.27273834e-05
Iter: 1803 loss: 1.27207441e-05
Iter: 1804 loss: 1.27217045e-05
Iter: 1805 loss: 1.27157855e-05
Iter: 1806 loss: 1.27074209e-05
Iter: 1807 loss: 1.27596722e-05
Iter: 1808 loss: 1.27064131e-05
Iter: 1809 loss: 1.26986779e-05
Iter: 1810 loss: 1.2753364e-05
Iter: 1811 loss: 1.26978994e-05
Iter: 1812 loss: 1.26915384e-05
Iter: 1813 loss: 1.26956475e-05
Iter: 1814 loss: 1.26873556e-05
Iter: 1815 loss: 1.26800551e-05
Iter: 1816 loss: 1.2703782e-05
Iter: 1817 loss: 1.26778896e-05
Iter: 1818 loss: 1.26713676e-05
Iter: 1819 loss: 1.27259627e-05
Iter: 1820 loss: 1.26708374e-05
Iter: 1821 loss: 1.26663326e-05
Iter: 1822 loss: 1.26604773e-05
Iter: 1823 loss: 1.26601117e-05
Iter: 1824 loss: 1.26544264e-05
Iter: 1825 loss: 1.27313106e-05
Iter: 1826 loss: 1.26544082e-05
Iter: 1827 loss: 1.2648552e-05
Iter: 1828 loss: 1.26537161e-05
Iter: 1829 loss: 1.26453042e-05
Iter: 1830 loss: 1.26399655e-05
Iter: 1831 loss: 1.26632485e-05
Iter: 1832 loss: 1.26390169e-05
Iter: 1833 loss: 1.26326577e-05
Iter: 1834 loss: 1.26299738e-05
Iter: 1835 loss: 1.26266732e-05
Iter: 1836 loss: 1.26194973e-05
Iter: 1837 loss: 1.26167888e-05
Iter: 1838 loss: 1.26128198e-05
Iter: 1839 loss: 1.26037066e-05
Iter: 1840 loss: 1.26749655e-05
Iter: 1841 loss: 1.26030864e-05
Iter: 1842 loss: 1.25960305e-05
Iter: 1843 loss: 1.26642608e-05
Iter: 1844 loss: 1.25957213e-05
Iter: 1845 loss: 1.2590096e-05
Iter: 1846 loss: 1.2593413e-05
Iter: 1847 loss: 1.25864262e-05
Iter: 1848 loss: 1.25796232e-05
Iter: 1849 loss: 1.25933057e-05
Iter: 1850 loss: 1.25767983e-05
Iter: 1851 loss: 1.25699389e-05
Iter: 1852 loss: 1.26336072e-05
Iter: 1853 loss: 1.25696588e-05
Iter: 1854 loss: 1.25646629e-05
Iter: 1855 loss: 1.2557618e-05
Iter: 1856 loss: 1.25573861e-05
Iter: 1857 loss: 1.25507504e-05
Iter: 1858 loss: 1.26272371e-05
Iter: 1859 loss: 1.25506176e-05
Iter: 1860 loss: 1.25440201e-05
Iter: 1861 loss: 1.25611268e-05
Iter: 1862 loss: 1.25417191e-05
Iter: 1863 loss: 1.25371507e-05
Iter: 1864 loss: 1.2550704e-05
Iter: 1865 loss: 1.25358192e-05
Iter: 1866 loss: 1.25298502e-05
Iter: 1867 loss: 1.25314282e-05
Iter: 1868 loss: 1.2525541e-05
Iter: 1869 loss: 1.2519442e-05
Iter: 1870 loss: 1.25137649e-05
Iter: 1871 loss: 1.25123952e-05
Iter: 1872 loss: 1.25030474e-05
Iter: 1873 loss: 1.25657507e-05
Iter: 1874 loss: 1.25021197e-05
Iter: 1875 loss: 1.24950666e-05
Iter: 1876 loss: 1.25767583e-05
Iter: 1877 loss: 1.24949256e-05
Iter: 1878 loss: 1.24895887e-05
Iter: 1879 loss: 1.24939234e-05
Iter: 1880 loss: 1.24864673e-05
Iter: 1881 loss: 1.24805802e-05
Iter: 1882 loss: 1.24925127e-05
Iter: 1883 loss: 1.24781282e-05
Iter: 1884 loss: 1.24722956e-05
Iter: 1885 loss: 1.25278875e-05
Iter: 1886 loss: 1.2472141e-05
Iter: 1887 loss: 1.24677272e-05
Iter: 1888 loss: 1.24610524e-05
Iter: 1889 loss: 1.2460996e-05
Iter: 1890 loss: 1.24543358e-05
Iter: 1891 loss: 1.25060124e-05
Iter: 1892 loss: 1.24537992e-05
Iter: 1893 loss: 1.24467551e-05
Iter: 1894 loss: 1.24677081e-05
Iter: 1895 loss: 1.24445751e-05
Iter: 1896 loss: 1.24391518e-05
Iter: 1897 loss: 1.2448133e-05
Iter: 1898 loss: 1.24367789e-05
Iter: 1899 loss: 1.24292292e-05
Iter: 1900 loss: 1.24433645e-05
Iter: 1901 loss: 1.24259359e-05
Iter: 1902 loss: 1.24205226e-05
Iter: 1903 loss: 1.24145545e-05
Iter: 1904 loss: 1.24137241e-05
Iter: 1905 loss: 1.24056187e-05
Iter: 1906 loss: 1.24539511e-05
Iter: 1907 loss: 1.24047456e-05
Iter: 1908 loss: 1.23983446e-05
Iter: 1909 loss: 1.24692197e-05
Iter: 1910 loss: 1.23982154e-05
Iter: 1911 loss: 1.23928976e-05
Iter: 1912 loss: 1.2394381e-05
Iter: 1913 loss: 1.2389175e-05
Iter: 1914 loss: 1.23820073e-05
Iter: 1915 loss: 1.23926184e-05
Iter: 1916 loss: 1.23784957e-05
Iter: 1917 loss: 1.23716682e-05
Iter: 1918 loss: 1.24552116e-05
Iter: 1919 loss: 1.23716482e-05
Iter: 1920 loss: 1.23664904e-05
Iter: 1921 loss: 1.23601094e-05
Iter: 1922 loss: 1.23595801e-05
Iter: 1923 loss: 1.23528316e-05
Iter: 1924 loss: 1.23958871e-05
Iter: 1925 loss: 1.2352e-05
Iter: 1926 loss: 1.23451009e-05
Iter: 1927 loss: 1.23839582e-05
Iter: 1928 loss: 1.23441023e-05
Iter: 1929 loss: 1.23398468e-05
Iter: 1930 loss: 1.23432255e-05
Iter: 1931 loss: 1.23372138e-05
Iter: 1932 loss: 1.23305199e-05
Iter: 1933 loss: 1.23440041e-05
Iter: 1934 loss: 1.23277332e-05
Iter: 1935 loss: 1.23219379e-05
Iter: 1936 loss: 1.2312812e-05
Iter: 1937 loss: 1.23126711e-05
Iter: 1938 loss: 1.23032432e-05
Iter: 1939 loss: 1.23790815e-05
Iter: 1940 loss: 1.2302592e-05
Iter: 1941 loss: 1.22959918e-05
Iter: 1942 loss: 1.2372283e-05
Iter: 1943 loss: 1.229593e-05
Iter: 1944 loss: 1.22904494e-05
Iter: 1945 loss: 1.22933307e-05
Iter: 1946 loss: 1.22867e-05
Iter: 1947 loss: 1.22799529e-05
Iter: 1948 loss: 1.22931924e-05
Iter: 1949 loss: 1.2277108e-05
Iter: 1950 loss: 1.22712972e-05
Iter: 1951 loss: 1.23400596e-05
Iter: 1952 loss: 1.22712481e-05
Iter: 1953 loss: 1.22665442e-05
Iter: 1954 loss: 1.22601459e-05
Iter: 1955 loss: 1.22598121e-05
Iter: 1956 loss: 1.2252176e-05
Iter: 1957 loss: 1.22798647e-05
Iter: 1958 loss: 1.22503934e-05
Iter: 1959 loss: 1.2242197e-05
Iter: 1960 loss: 1.23080463e-05
Iter: 1961 loss: 1.22417514e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2
+ date
Mon Oct 26 09:41:58 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.2_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.2_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7045730d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe704549ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe704524d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe704524c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7044bb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7044bb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7044bb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe704439048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe704423510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe704404f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7043cc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7043cbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7043db400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7043db840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70432fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70432f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7042e5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7043110d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7042c7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70427df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70427c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70427c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7042178c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7042177b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70420a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70420a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70417a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70414d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70414d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70414d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7040ff7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7040cc7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7040ccd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7040cd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6e3c28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6e3c28ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.34540069e-05
Iter: 2 loss: 1.14110671e-05
Iter: 3 loss: 1.13864471e-05
Iter: 4 loss: 1.04058181e-05
Iter: 5 loss: 1.28327974e-05
Iter: 6 loss: 1.00597845e-05
Iter: 7 loss: 9.40140217e-06
Iter: 8 loss: 1.22572674e-05
Iter: 9 loss: 9.26725261e-06
Iter: 10 loss: 8.70072563e-06
Iter: 11 loss: 9.25567565e-06
Iter: 12 loss: 8.37944754e-06
Iter: 13 loss: 7.85642624e-06
Iter: 14 loss: 9.55869291e-06
Iter: 15 loss: 7.71119903e-06
Iter: 16 loss: 7.2609846e-06
Iter: 17 loss: 8.800047e-06
Iter: 18 loss: 7.14122507e-06
Iter: 19 loss: 6.63505034e-06
Iter: 20 loss: 8.64249887e-06
Iter: 21 loss: 6.51972141e-06
Iter: 22 loss: 6.20674291e-06
Iter: 23 loss: 5.71472856e-06
Iter: 24 loss: 5.70930524e-06
Iter: 25 loss: 5.49705328e-06
Iter: 26 loss: 5.47735908e-06
Iter: 27 loss: 5.26626354e-06
Iter: 28 loss: 6.26614428e-06
Iter: 29 loss: 5.22798473e-06
Iter: 30 loss: 5.12255247e-06
Iter: 31 loss: 5.01545901e-06
Iter: 32 loss: 4.99474436e-06
Iter: 33 loss: 4.81688858e-06
Iter: 34 loss: 4.73801674e-06
Iter: 35 loss: 4.64780805e-06
Iter: 36 loss: 4.4930166e-06
Iter: 37 loss: 4.48719e-06
Iter: 38 loss: 4.31041963e-06
Iter: 39 loss: 4.79784921e-06
Iter: 40 loss: 4.25315375e-06
Iter: 41 loss: 4.13308635e-06
Iter: 42 loss: 4.49314894e-06
Iter: 43 loss: 4.09681252e-06
Iter: 44 loss: 3.96361975e-06
Iter: 45 loss: 4.23154688e-06
Iter: 46 loss: 3.90942387e-06
Iter: 47 loss: 3.82730832e-06
Iter: 48 loss: 4.07242487e-06
Iter: 49 loss: 3.80229e-06
Iter: 50 loss: 3.7172149e-06
Iter: 51 loss: 4.05158926e-06
Iter: 52 loss: 3.69764484e-06
Iter: 53 loss: 3.61823868e-06
Iter: 54 loss: 3.94948347e-06
Iter: 55 loss: 3.60131298e-06
Iter: 56 loss: 3.54428221e-06
Iter: 57 loss: 3.47337846e-06
Iter: 58 loss: 3.46737215e-06
Iter: 59 loss: 3.36945072e-06
Iter: 60 loss: 3.62308674e-06
Iter: 61 loss: 3.33605522e-06
Iter: 62 loss: 3.2582434e-06
Iter: 63 loss: 3.25595329e-06
Iter: 64 loss: 3.21360949e-06
Iter: 65 loss: 3.11515032e-06
Iter: 66 loss: 4.32997876e-06
Iter: 67 loss: 3.10761311e-06
Iter: 68 loss: 3.02302533e-06
Iter: 69 loss: 3.30255671e-06
Iter: 70 loss: 2.99969111e-06
Iter: 71 loss: 2.9411076e-06
Iter: 72 loss: 3.51503741e-06
Iter: 73 loss: 2.93905987e-06
Iter: 74 loss: 2.91797733e-06
Iter: 75 loss: 2.91378478e-06
Iter: 76 loss: 2.892152e-06
Iter: 77 loss: 2.84830412e-06
Iter: 78 loss: 3.64881816e-06
Iter: 79 loss: 2.84760017e-06
Iter: 80 loss: 2.81734765e-06
Iter: 81 loss: 2.81667189e-06
Iter: 82 loss: 2.7927656e-06
Iter: 83 loss: 2.74415743e-06
Iter: 84 loss: 3.63711297e-06
Iter: 85 loss: 2.74336435e-06
Iter: 86 loss: 2.71285035e-06
Iter: 87 loss: 2.71231465e-06
Iter: 88 loss: 2.68307281e-06
Iter: 89 loss: 2.71787098e-06
Iter: 90 loss: 2.66760981e-06
Iter: 91 loss: 2.63958032e-06
Iter: 92 loss: 2.73826117e-06
Iter: 93 loss: 2.63239e-06
Iter: 94 loss: 2.61104378e-06
Iter: 95 loss: 2.59684589e-06
Iter: 96 loss: 2.5888562e-06
Iter: 97 loss: 2.57084457e-06
Iter: 98 loss: 2.56990688e-06
Iter: 99 loss: 2.55176838e-06
Iter: 100 loss: 2.55607915e-06
Iter: 101 loss: 2.53845838e-06
Iter: 102 loss: 2.51671236e-06
Iter: 103 loss: 2.47419894e-06
Iter: 104 loss: 3.32994864e-06
Iter: 105 loss: 2.47391358e-06
Iter: 106 loss: 2.43102613e-06
Iter: 107 loss: 2.64511118e-06
Iter: 108 loss: 2.42379861e-06
Iter: 109 loss: 2.4414735e-06
Iter: 110 loss: 2.41316e-06
Iter: 111 loss: 2.40453073e-06
Iter: 112 loss: 2.38385928e-06
Iter: 113 loss: 2.60699062e-06
Iter: 114 loss: 2.38167513e-06
Iter: 115 loss: 2.36737651e-06
Iter: 116 loss: 2.36703499e-06
Iter: 117 loss: 2.3554926e-06
Iter: 118 loss: 2.3480402e-06
Iter: 119 loss: 2.34357503e-06
Iter: 120 loss: 2.32606135e-06
Iter: 121 loss: 2.3278576e-06
Iter: 122 loss: 2.31254717e-06
Iter: 123 loss: 2.29115562e-06
Iter: 124 loss: 2.2911006e-06
Iter: 125 loss: 2.28175122e-06
Iter: 126 loss: 2.27080523e-06
Iter: 127 loss: 2.26958218e-06
Iter: 128 loss: 2.24974883e-06
Iter: 129 loss: 2.28399904e-06
Iter: 130 loss: 2.2408808e-06
Iter: 131 loss: 2.22732115e-06
Iter: 132 loss: 2.35000584e-06
Iter: 133 loss: 2.22672656e-06
Iter: 134 loss: 2.21421669e-06
Iter: 135 loss: 2.27482724e-06
Iter: 136 loss: 2.21207051e-06
Iter: 137 loss: 2.20405218e-06
Iter: 138 loss: 2.18965351e-06
Iter: 139 loss: 2.535558e-06
Iter: 140 loss: 2.18964624e-06
Iter: 141 loss: 2.17378e-06
Iter: 142 loss: 2.21748473e-06
Iter: 143 loss: 2.16870421e-06
Iter: 144 loss: 2.16383705e-06
Iter: 145 loss: 2.16055059e-06
Iter: 146 loss: 2.15278533e-06
Iter: 147 loss: 2.13588282e-06
Iter: 148 loss: 2.38834036e-06
Iter: 149 loss: 2.13513249e-06
Iter: 150 loss: 2.12565214e-06
Iter: 151 loss: 2.12562304e-06
Iter: 152 loss: 2.11664815e-06
Iter: 153 loss: 2.11776023e-06
Iter: 154 loss: 2.10982876e-06
Iter: 155 loss: 2.10241183e-06
Iter: 156 loss: 2.11952192e-06
Iter: 157 loss: 2.09962559e-06
Iter: 158 loss: 2.09067025e-06
Iter: 159 loss: 2.16209583e-06
Iter: 160 loss: 2.09001337e-06
Iter: 161 loss: 2.08437223e-06
Iter: 162 loss: 2.07536505e-06
Iter: 163 loss: 2.07529911e-06
Iter: 164 loss: 2.06281447e-06
Iter: 165 loss: 2.10600547e-06
Iter: 166 loss: 2.05955371e-06
Iter: 167 loss: 2.0483626e-06
Iter: 168 loss: 2.11394536e-06
Iter: 169 loss: 2.04682328e-06
Iter: 170 loss: 2.03908849e-06
Iter: 171 loss: 2.12319742e-06
Iter: 172 loss: 2.0389225e-06
Iter: 173 loss: 2.03345417e-06
Iter: 174 loss: 2.02197089e-06
Iter: 175 loss: 2.20730658e-06
Iter: 176 loss: 2.0216703e-06
Iter: 177 loss: 2.01373678e-06
Iter: 178 loss: 2.03234458e-06
Iter: 179 loss: 2.01081139e-06
Iter: 180 loss: 2.0073162e-06
Iter: 181 loss: 2.0058169e-06
Iter: 182 loss: 2.00113391e-06
Iter: 183 loss: 1.99466558e-06
Iter: 184 loss: 1.99438136e-06
Iter: 185 loss: 1.98774046e-06
Iter: 186 loss: 2.0013e-06
Iter: 187 loss: 1.98508815e-06
Iter: 188 loss: 1.97671716e-06
Iter: 189 loss: 2.03266541e-06
Iter: 190 loss: 1.9758811e-06
Iter: 191 loss: 1.97110012e-06
Iter: 192 loss: 1.96475958e-06
Iter: 193 loss: 1.96438759e-06
Iter: 194 loss: 1.95794223e-06
Iter: 195 loss: 1.95762459e-06
Iter: 196 loss: 1.95446387e-06
Iter: 197 loss: 1.94759059e-06
Iter: 198 loss: 2.05236984e-06
Iter: 199 loss: 1.94735753e-06
Iter: 200 loss: 1.94109634e-06
Iter: 201 loss: 2.00768682e-06
Iter: 202 loss: 1.94097129e-06
Iter: 203 loss: 1.93615551e-06
Iter: 204 loss: 1.95410303e-06
Iter: 205 loss: 1.93499363e-06
Iter: 206 loss: 1.92951347e-06
Iter: 207 loss: 1.94443419e-06
Iter: 208 loss: 1.92771677e-06
Iter: 209 loss: 1.92222569e-06
Iter: 210 loss: 1.91652043e-06
Iter: 211 loss: 1.91549361e-06
Iter: 212 loss: 1.90749915e-06
Iter: 213 loss: 1.91368326e-06
Iter: 214 loss: 1.90265837e-06
Iter: 215 loss: 1.90280525e-06
Iter: 216 loss: 1.8983003e-06
Iter: 217 loss: 1.89572893e-06
Iter: 218 loss: 1.88988065e-06
Iter: 219 loss: 1.9682102e-06
Iter: 220 loss: 1.88954539e-06
Iter: 221 loss: 1.88481613e-06
Iter: 222 loss: 1.9253489e-06
Iter: 223 loss: 1.88451122e-06
Iter: 224 loss: 1.87974319e-06
Iter: 225 loss: 1.89468813e-06
Iter: 226 loss: 1.87835417e-06
Iter: 227 loss: 1.87513228e-06
Iter: 228 loss: 1.87598482e-06
Iter: 229 loss: 1.87276919e-06
Iter: 230 loss: 1.86812281e-06
Iter: 231 loss: 1.91062281e-06
Iter: 232 loss: 1.86788247e-06
Iter: 233 loss: 1.86460579e-06
Iter: 234 loss: 1.85688339e-06
Iter: 235 loss: 1.94852828e-06
Iter: 236 loss: 1.85623981e-06
Iter: 237 loss: 1.84982503e-06
Iter: 238 loss: 1.92650555e-06
Iter: 239 loss: 1.84972851e-06
Iter: 240 loss: 1.84526016e-06
Iter: 241 loss: 1.8965286e-06
Iter: 242 loss: 1.84519058e-06
Iter: 243 loss: 1.84240616e-06
Iter: 244 loss: 1.84956752e-06
Iter: 245 loss: 1.84141754e-06
Iter: 246 loss: 1.83870202e-06
Iter: 247 loss: 1.83541556e-06
Iter: 248 loss: 1.8351119e-06
Iter: 249 loss: 1.8315435e-06
Iter: 250 loss: 1.845621e-06
Iter: 251 loss: 1.83072871e-06
Iter: 252 loss: 1.8270764e-06
Iter: 253 loss: 1.87580315e-06
Iter: 254 loss: 1.82704707e-06
Iter: 255 loss: 1.82439248e-06
Iter: 256 loss: 1.81861424e-06
Iter: 257 loss: 1.90315325e-06
Iter: 258 loss: 1.81828625e-06
Iter: 259 loss: 1.81420342e-06
Iter: 260 loss: 1.86026807e-06
Iter: 261 loss: 1.81406813e-06
Iter: 262 loss: 1.80964764e-06
Iter: 263 loss: 1.81881478e-06
Iter: 264 loss: 1.80782331e-06
Iter: 265 loss: 1.80521499e-06
Iter: 266 loss: 1.81171845e-06
Iter: 267 loss: 1.80431357e-06
Iter: 268 loss: 1.80081747e-06
Iter: 269 loss: 1.81452458e-06
Iter: 270 loss: 1.80008578e-06
Iter: 271 loss: 1.79820199e-06
Iter: 272 loss: 1.79609924e-06
Iter: 273 loss: 1.79584822e-06
Iter: 274 loss: 1.79225049e-06
Iter: 275 loss: 1.79957476e-06
Iter: 276 loss: 1.79080951e-06
Iter: 277 loss: 1.78731671e-06
Iter: 278 loss: 1.7873615e-06
Iter: 279 loss: 1.78514301e-06
Iter: 280 loss: 1.7833521e-06
Iter: 281 loss: 1.78273808e-06
Iter: 282 loss: 1.77823358e-06
Iter: 283 loss: 1.77852439e-06
Iter: 284 loss: 1.77477818e-06
Iter: 285 loss: 1.77136292e-06
Iter: 286 loss: 1.79590029e-06
Iter: 287 loss: 1.77107336e-06
Iter: 288 loss: 1.76760466e-06
Iter: 289 loss: 1.79499852e-06
Iter: 290 loss: 1.76735557e-06
Iter: 291 loss: 1.76600543e-06
Iter: 292 loss: 1.76414778e-06
Iter: 293 loss: 1.76407457e-06
Iter: 294 loss: 1.76189428e-06
Iter: 295 loss: 1.77568711e-06
Iter: 296 loss: 1.76163303e-06
Iter: 297 loss: 1.75873333e-06
Iter: 298 loss: 1.75904643e-06
Iter: 299 loss: 1.75651212e-06
Iter: 300 loss: 1.75385344e-06
Iter: 301 loss: 1.76678168e-06
Iter: 302 loss: 1.75339619e-06
Iter: 303 loss: 1.75021205e-06
Iter: 304 loss: 1.75738285e-06
Iter: 305 loss: 1.74900015e-06
Iter: 306 loss: 1.74663182e-06
Iter: 307 loss: 1.74231639e-06
Iter: 308 loss: 1.84429587e-06
Iter: 309 loss: 1.74230104e-06
Iter: 310 loss: 1.74030174e-06
Iter: 311 loss: 1.73985177e-06
Iter: 312 loss: 1.73781689e-06
Iter: 313 loss: 1.74603906e-06
Iter: 314 loss: 1.73736623e-06
Iter: 315 loss: 1.73593207e-06
Iter: 316 loss: 1.73551371e-06
Iter: 317 loss: 1.73460933e-06
Iter: 318 loss: 1.73215972e-06
Iter: 319 loss: 1.73471676e-06
Iter: 320 loss: 1.73079218e-06
Iter: 321 loss: 1.72824775e-06
Iter: 322 loss: 1.74280615e-06
Iter: 323 loss: 1.72797115e-06
Iter: 324 loss: 1.72496368e-06
Iter: 325 loss: 1.73566161e-06
Iter: 326 loss: 1.72421483e-06
Iter: 327 loss: 1.72206421e-06
Iter: 328 loss: 1.71761951e-06
Iter: 329 loss: 1.7917489e-06
Iter: 330 loss: 1.71747342e-06
Iter: 331 loss: 1.71696115e-06
Iter: 332 loss: 1.71594797e-06
Iter: 333 loss: 1.71436454e-06
Iter: 334 loss: 1.71270347e-06
Iter: 335 loss: 1.71245335e-06
Iter: 336 loss: 1.71102204e-06
Iter: 337 loss: 1.73136129e-06
Iter: 338 loss: 1.71099691e-06
Iter: 339 loss: 1.70949284e-06
Iter: 340 loss: 1.70770249e-06
Iter: 341 loss: 1.70745966e-06
Iter: 342 loss: 1.70459521e-06
Iter: 343 loss: 1.70358146e-06
Iter: 344 loss: 1.70195506e-06
Iter: 345 loss: 1.70063765e-06
Iter: 346 loss: 1.70019337e-06
Iter: 347 loss: 1.69838904e-06
Iter: 348 loss: 1.69664349e-06
Iter: 349 loss: 1.69625434e-06
Iter: 350 loss: 1.69391785e-06
Iter: 351 loss: 1.70093722e-06
Iter: 352 loss: 1.6931707e-06
Iter: 353 loss: 1.69106249e-06
Iter: 354 loss: 1.70026215e-06
Iter: 355 loss: 1.69060513e-06
Iter: 356 loss: 1.68951487e-06
Iter: 357 loss: 1.70653902e-06
Iter: 358 loss: 1.68951101e-06
Iter: 359 loss: 1.68829115e-06
Iter: 360 loss: 1.6856892e-06
Iter: 361 loss: 1.72752129e-06
Iter: 362 loss: 1.6855945e-06
Iter: 363 loss: 1.68293059e-06
Iter: 364 loss: 1.68757481e-06
Iter: 365 loss: 1.68175529e-06
Iter: 366 loss: 1.67997473e-06
Iter: 367 loss: 1.67992744e-06
Iter: 368 loss: 1.67798078e-06
Iter: 369 loss: 1.67541771e-06
Iter: 370 loss: 1.67527446e-06
Iter: 371 loss: 1.67384064e-06
Iter: 372 loss: 1.67376618e-06
Iter: 373 loss: 1.67233486e-06
Iter: 374 loss: 1.67028861e-06
Iter: 375 loss: 1.67024336e-06
Iter: 376 loss: 1.66844188e-06
Iter: 377 loss: 1.67149824e-06
Iter: 378 loss: 1.66762811e-06
Iter: 379 loss: 1.66622556e-06
Iter: 380 loss: 1.66618497e-06
Iter: 381 loss: 1.6649617e-06
Iter: 382 loss: 1.66298264e-06
Iter: 383 loss: 1.66296445e-06
Iter: 384 loss: 1.66062546e-06
Iter: 385 loss: 1.66435041e-06
Iter: 386 loss: 1.65951269e-06
Iter: 387 loss: 1.6569893e-06
Iter: 388 loss: 1.68006341e-06
Iter: 389 loss: 1.65682798e-06
Iter: 390 loss: 1.65551535e-06
Iter: 391 loss: 1.67587791e-06
Iter: 392 loss: 1.65550068e-06
Iter: 393 loss: 1.65451888e-06
Iter: 394 loss: 1.65255324e-06
Iter: 395 loss: 1.68845372e-06
Iter: 396 loss: 1.65249844e-06
Iter: 397 loss: 1.65091319e-06
Iter: 398 loss: 1.65347774e-06
Iter: 399 loss: 1.65016263e-06
Iter: 400 loss: 1.64886342e-06
Iter: 401 loss: 1.64882897e-06
Iter: 402 loss: 1.64769767e-06
Iter: 403 loss: 1.64608127e-06
Iter: 404 loss: 1.64601977e-06
Iter: 405 loss: 1.64464814e-06
Iter: 406 loss: 1.64463529e-06
Iter: 407 loss: 1.64347318e-06
Iter: 408 loss: 1.64114317e-06
Iter: 409 loss: 1.68717247e-06
Iter: 410 loss: 1.64112066e-06
Iter: 411 loss: 1.6386665e-06
Iter: 412 loss: 1.64746075e-06
Iter: 413 loss: 1.63807556e-06
Iter: 414 loss: 1.63658581e-06
Iter: 415 loss: 1.63646632e-06
Iter: 416 loss: 1.63575305e-06
Iter: 417 loss: 1.63402137e-06
Iter: 418 loss: 1.65653501e-06
Iter: 419 loss: 1.63390382e-06
Iter: 420 loss: 1.63207437e-06
Iter: 421 loss: 1.64401717e-06
Iter: 422 loss: 1.63192044e-06
Iter: 423 loss: 1.63039863e-06
Iter: 424 loss: 1.64245512e-06
Iter: 425 loss: 1.63031291e-06
Iter: 426 loss: 1.62890956e-06
Iter: 427 loss: 1.63261689e-06
Iter: 428 loss: 1.62846379e-06
Iter: 429 loss: 1.62734261e-06
Iter: 430 loss: 1.62526726e-06
Iter: 431 loss: 1.67420239e-06
Iter: 432 loss: 1.62527181e-06
Iter: 433 loss: 1.62257038e-06
Iter: 434 loss: 1.62878564e-06
Iter: 435 loss: 1.62154424e-06
Iter: 436 loss: 1.62112156e-06
Iter: 437 loss: 1.62034632e-06
Iter: 438 loss: 1.61967853e-06
Iter: 439 loss: 1.61827825e-06
Iter: 440 loss: 1.64133837e-06
Iter: 441 loss: 1.61824619e-06
Iter: 442 loss: 1.61695084e-06
Iter: 443 loss: 1.61694902e-06
Iter: 444 loss: 1.61621642e-06
Iter: 445 loss: 1.6143523e-06
Iter: 446 loss: 1.63076618e-06
Iter: 447 loss: 1.61408548e-06
Iter: 448 loss: 1.61263017e-06
Iter: 449 loss: 1.61257935e-06
Iter: 450 loss: 1.61110233e-06
Iter: 451 loss: 1.6134527e-06
Iter: 452 loss: 1.61042794e-06
Iter: 453 loss: 1.60904062e-06
Iter: 454 loss: 1.60683908e-06
Iter: 455 loss: 1.60681248e-06
Iter: 456 loss: 1.6053998e-06
Iter: 457 loss: 1.60533716e-06
Iter: 458 loss: 1.6040093e-06
Iter: 459 loss: 1.60790444e-06
Iter: 460 loss: 1.60362083e-06
Iter: 461 loss: 1.60245486e-06
Iter: 462 loss: 1.60527679e-06
Iter: 463 loss: 1.60201512e-06
Iter: 464 loss: 1.60110824e-06
Iter: 465 loss: 1.59933347e-06
Iter: 466 loss: 1.63318248e-06
Iter: 467 loss: 1.59930141e-06
Iter: 468 loss: 1.59760168e-06
Iter: 469 loss: 1.618346e-06
Iter: 470 loss: 1.5975736e-06
Iter: 471 loss: 1.59604451e-06
Iter: 472 loss: 1.60613081e-06
Iter: 473 loss: 1.59588978e-06
Iter: 474 loss: 1.59486058e-06
Iter: 475 loss: 1.59410649e-06
Iter: 476 loss: 1.59376043e-06
Iter: 477 loss: 1.59221054e-06
Iter: 478 loss: 1.61100274e-06
Iter: 479 loss: 1.59218462e-06
Iter: 480 loss: 1.59134606e-06
Iter: 481 loss: 1.58992066e-06
Iter: 482 loss: 1.58989462e-06
Iter: 483 loss: 1.58880925e-06
Iter: 484 loss: 1.58878947e-06
Iter: 485 loss: 1.58756075e-06
Iter: 486 loss: 1.58716489e-06
Iter: 487 loss: 1.58642456e-06
Iter: 488 loss: 1.58541627e-06
Iter: 489 loss: 1.58524961e-06
Iter: 490 loss: 1.5844879e-06
Iter: 491 loss: 1.58308535e-06
Iter: 492 loss: 1.58308444e-06
Iter: 493 loss: 1.58182013e-06
Iter: 494 loss: 1.58153284e-06
Iter: 495 loss: 1.58074442e-06
Iter: 496 loss: 1.57909813e-06
Iter: 497 loss: 1.582153e-06
Iter: 498 loss: 1.57837758e-06
Iter: 499 loss: 1.57688771e-06
Iter: 500 loss: 1.57715988e-06
Iter: 501 loss: 1.57575573e-06
Iter: 502 loss: 1.57465865e-06
Iter: 503 loss: 1.57461773e-06
Iter: 504 loss: 1.57355021e-06
Iter: 505 loss: 1.57518093e-06
Iter: 506 loss: 1.573031e-06
Iter: 507 loss: 1.5722203e-06
Iter: 508 loss: 1.57341401e-06
Iter: 509 loss: 1.57180193e-06
Iter: 510 loss: 1.57058116e-06
Iter: 511 loss: 1.57311592e-06
Iter: 512 loss: 1.57006843e-06
Iter: 513 loss: 1.56922079e-06
Iter: 514 loss: 1.56816645e-06
Iter: 515 loss: 1.56809665e-06
Iter: 516 loss: 1.56684905e-06
Iter: 517 loss: 1.56681631e-06
Iter: 518 loss: 1.56588851e-06
Iter: 519 loss: 1.5643335e-06
Iter: 520 loss: 1.56432816e-06
Iter: 521 loss: 1.56275462e-06
Iter: 522 loss: 1.56405713e-06
Iter: 523 loss: 1.56179954e-06
Iter: 524 loss: 1.56175281e-06
Iter: 525 loss: 1.56104329e-06
Iter: 526 loss: 1.56031717e-06
Iter: 527 loss: 1.55890962e-06
Iter: 528 loss: 1.5858011e-06
Iter: 529 loss: 1.5588962e-06
Iter: 530 loss: 1.55765338e-06
Iter: 531 loss: 1.56299291e-06
Iter: 532 loss: 1.55741975e-06
Iter: 533 loss: 1.55600458e-06
Iter: 534 loss: 1.55550265e-06
Iter: 535 loss: 1.55469615e-06
Iter: 536 loss: 1.55420753e-06
Iter: 537 loss: 1.55390603e-06
Iter: 538 loss: 1.5531582e-06
Iter: 539 loss: 1.55149655e-06
Iter: 540 loss: 1.57137572e-06
Iter: 541 loss: 1.55134762e-06
Iter: 542 loss: 1.55019802e-06
Iter: 543 loss: 1.56820113e-06
Iter: 544 loss: 1.5501937e-06
Iter: 545 loss: 1.54905706e-06
Iter: 546 loss: 1.55115185e-06
Iter: 547 loss: 1.54857321e-06
Iter: 548 loss: 1.54768168e-06
Iter: 549 loss: 1.54709949e-06
Iter: 550 loss: 1.54679105e-06
Iter: 551 loss: 1.54552538e-06
Iter: 552 loss: 1.54923362e-06
Iter: 553 loss: 1.54512691e-06
Iter: 554 loss: 1.54385827e-06
Iter: 555 loss: 1.56366e-06
Iter: 556 loss: 1.5438502e-06
Iter: 557 loss: 1.54319423e-06
Iter: 558 loss: 1.541242e-06
Iter: 559 loss: 1.54901863e-06
Iter: 560 loss: 1.54044858e-06
Iter: 561 loss: 1.54113923e-06
Iter: 562 loss: 1.5397801e-06
Iter: 563 loss: 1.53892336e-06
Iter: 564 loss: 1.5377251e-06
Iter: 565 loss: 1.53772612e-06
Iter: 566 loss: 1.5366752e-06
Iter: 567 loss: 1.53715609e-06
Iter: 568 loss: 1.53595215e-06
Iter: 569 loss: 1.53473945e-06
Iter: 570 loss: 1.53808992e-06
Iter: 571 loss: 1.53430335e-06
Iter: 572 loss: 1.53295696e-06
Iter: 573 loss: 1.54430484e-06
Iter: 574 loss: 1.53285566e-06
Iter: 575 loss: 1.53169071e-06
Iter: 576 loss: 1.53576855e-06
Iter: 577 loss: 1.53139445e-06
Iter: 578 loss: 1.53030771e-06
Iter: 579 loss: 1.53131623e-06
Iter: 580 loss: 1.5297162e-06
Iter: 581 loss: 1.52859593e-06
Iter: 582 loss: 1.53223573e-06
Iter: 583 loss: 1.52833627e-06
Iter: 584 loss: 1.52699727e-06
Iter: 585 loss: 1.53001656e-06
Iter: 586 loss: 1.52647738e-06
Iter: 587 loss: 1.52570817e-06
Iter: 588 loss: 1.52493828e-06
Iter: 589 loss: 1.52481198e-06
Iter: 590 loss: 1.52429698e-06
Iter: 591 loss: 1.52407983e-06
Iter: 592 loss: 1.52348866e-06
Iter: 593 loss: 1.52220332e-06
Iter: 594 loss: 1.54141333e-06
Iter: 595 loss: 1.52214284e-06
Iter: 596 loss: 1.52104235e-06
Iter: 597 loss: 1.53110773e-06
Iter: 598 loss: 1.52102552e-06
Iter: 599 loss: 1.51986762e-06
Iter: 600 loss: 1.52464679e-06
Iter: 601 loss: 1.51961706e-06
Iter: 602 loss: 1.51887798e-06
Iter: 603 loss: 1.51764243e-06
Iter: 604 loss: 1.51764471e-06
Iter: 605 loss: 1.5164776e-06
Iter: 606 loss: 1.51890481e-06
Iter: 607 loss: 1.5160374e-06
Iter: 608 loss: 1.51538529e-06
Iter: 609 loss: 1.51531071e-06
Iter: 610 loss: 1.51461268e-06
Iter: 611 loss: 1.51381346e-06
Iter: 612 loss: 1.51373899e-06
Iter: 613 loss: 1.51268409e-06
Iter: 614 loss: 1.51977724e-06
Iter: 615 loss: 1.51253857e-06
Iter: 616 loss: 1.51173799e-06
Iter: 617 loss: 1.51342601e-06
Iter: 618 loss: 1.51138784e-06
Iter: 619 loss: 1.5102521e-06
Iter: 620 loss: 1.51055531e-06
Iter: 621 loss: 1.50941241e-06
Iter: 622 loss: 1.50810024e-06
Iter: 623 loss: 1.50988922e-06
Iter: 624 loss: 1.50745234e-06
Iter: 625 loss: 1.50683115e-06
Iter: 626 loss: 1.50674873e-06
Iter: 627 loss: 1.50610526e-06
Iter: 628 loss: 1.50538654e-06
Iter: 629 loss: 1.50524045e-06
Iter: 630 loss: 1.50461187e-06
Iter: 631 loss: 1.50803328e-06
Iter: 632 loss: 1.50448568e-06
Iter: 633 loss: 1.50357437e-06
Iter: 634 loss: 1.50569622e-06
Iter: 635 loss: 1.50324763e-06
Iter: 636 loss: 1.50278186e-06
Iter: 637 loss: 1.50165056e-06
Iter: 638 loss: 1.51648737e-06
Iter: 639 loss: 1.50157553e-06
Iter: 640 loss: 1.50003621e-06
Iter: 641 loss: 1.5030912e-06
Iter: 642 loss: 1.49936056e-06
Iter: 643 loss: 1.49814082e-06
Iter: 644 loss: 1.4981556e-06
Iter: 645 loss: 1.49700304e-06
Iter: 646 loss: 1.50155483e-06
Iter: 647 loss: 1.49669745e-06
Iter: 648 loss: 1.49611537e-06
Iter: 649 loss: 1.4961e-06
Iter: 650 loss: 1.49563107e-06
Iter: 651 loss: 1.49477501e-06
Iter: 652 loss: 1.50122469e-06
Iter: 653 loss: 1.49471521e-06
Iter: 654 loss: 1.49406765e-06
Iter: 655 loss: 1.49386062e-06
Iter: 656 loss: 1.49349648e-06
Iter: 657 loss: 1.49262e-06
Iter: 658 loss: 1.496403e-06
Iter: 659 loss: 1.49246603e-06
Iter: 660 loss: 1.49163895e-06
Iter: 661 loss: 1.49368884e-06
Iter: 662 loss: 1.49131859e-06
Iter: 663 loss: 1.49011339e-06
Iter: 664 loss: 1.4910795e-06
Iter: 665 loss: 1.48937875e-06
Iter: 666 loss: 1.48845868e-06
Iter: 667 loss: 1.49097309e-06
Iter: 668 loss: 1.48814945e-06
Iter: 669 loss: 1.48717595e-06
Iter: 670 loss: 1.49734683e-06
Iter: 671 loss: 1.48716992e-06
Iter: 672 loss: 1.486629e-06
Iter: 673 loss: 1.48545064e-06
Iter: 674 loss: 1.50457072e-06
Iter: 675 loss: 1.48541051e-06
Iter: 676 loss: 1.48446361e-06
Iter: 677 loss: 1.48481536e-06
Iter: 678 loss: 1.48379195e-06
Iter: 679 loss: 1.48240133e-06
Iter: 680 loss: 1.48595336e-06
Iter: 681 loss: 1.48192851e-06
Iter: 682 loss: 1.48059428e-06
Iter: 683 loss: 1.49184098e-06
Iter: 684 loss: 1.48053232e-06
Iter: 685 loss: 1.47982712e-06
Iter: 686 loss: 1.47977494e-06
Iter: 687 loss: 1.47919604e-06
Iter: 688 loss: 1.47775916e-06
Iter: 689 loss: 1.49863308e-06
Iter: 690 loss: 1.47774676e-06
Iter: 691 loss: 1.47662558e-06
Iter: 692 loss: 1.48141851e-06
Iter: 693 loss: 1.47640117e-06
Iter: 694 loss: 1.47539743e-06
Iter: 695 loss: 1.48487254e-06
Iter: 696 loss: 1.47537799e-06
Iter: 697 loss: 1.47479716e-06
Iter: 698 loss: 1.47953938e-06
Iter: 699 loss: 1.47474452e-06
Iter: 700 loss: 1.47421792e-06
Iter: 701 loss: 1.47350067e-06
Iter: 702 loss: 1.47345554e-06
Iter: 703 loss: 1.47263268e-06
Iter: 704 loss: 1.48063748e-06
Iter: 705 loss: 1.47259107e-06
Iter: 706 loss: 1.47172034e-06
Iter: 707 loss: 1.47220771e-06
Iter: 708 loss: 1.47117839e-06
Iter: 709 loss: 1.47051844e-06
Iter: 710 loss: 1.47643573e-06
Iter: 711 loss: 1.47048809e-06
Iter: 712 loss: 1.46987327e-06
Iter: 713 loss: 1.46885509e-06
Iter: 714 loss: 1.46884145e-06
Iter: 715 loss: 1.46769037e-06
Iter: 716 loss: 1.46814307e-06
Iter: 717 loss: 1.46689194e-06
Iter: 718 loss: 1.46579737e-06
Iter: 719 loss: 1.46933826e-06
Iter: 720 loss: 1.46550713e-06
Iter: 721 loss: 1.46423133e-06
Iter: 722 loss: 1.46732282e-06
Iter: 723 loss: 1.46381944e-06
Iter: 724 loss: 1.46286936e-06
Iter: 725 loss: 1.469994e-06
Iter: 726 loss: 1.4628381e-06
Iter: 727 loss: 1.46192963e-06
Iter: 728 loss: 1.47059222e-06
Iter: 729 loss: 1.46187017e-06
Iter: 730 loss: 1.46134494e-06
Iter: 731 loss: 1.46044863e-06
Iter: 732 loss: 1.46044476e-06
Iter: 733 loss: 1.45955869e-06
Iter: 734 loss: 1.46778984e-06
Iter: 735 loss: 1.45955323e-06
Iter: 736 loss: 1.45890681e-06
Iter: 737 loss: 1.45996773e-06
Iter: 738 loss: 1.45859474e-06
Iter: 739 loss: 1.45806052e-06
Iter: 740 loss: 1.45804881e-06
Iter: 741 loss: 1.45758895e-06
Iter: 742 loss: 1.45702734e-06
Iter: 743 loss: 1.45701119e-06
Iter: 744 loss: 1.45607282e-06
Iter: 745 loss: 1.46177467e-06
Iter: 746 loss: 1.45597323e-06
Iter: 747 loss: 1.45522336e-06
Iter: 748 loss: 1.4543441e-06
Iter: 749 loss: 1.45428476e-06
Iter: 750 loss: 1.45368335e-06
Iter: 751 loss: 1.45360468e-06
Iter: 752 loss: 1.4530508e-06
Iter: 753 loss: 1.4518655e-06
Iter: 754 loss: 1.46856166e-06
Iter: 755 loss: 1.45177739e-06
Iter: 756 loss: 1.45060494e-06
Iter: 757 loss: 1.45297213e-06
Iter: 758 loss: 1.4501577e-06
Iter: 759 loss: 1.44959654e-06
Iter: 760 loss: 1.44955322e-06
Iter: 761 loss: 1.44895921e-06
Iter: 762 loss: 1.44902242e-06
Iter: 763 loss: 1.44849253e-06
Iter: 764 loss: 1.44769228e-06
Iter: 765 loss: 1.4469847e-06
Iter: 766 loss: 1.44680007e-06
Iter: 767 loss: 1.44581566e-06
Iter: 768 loss: 1.45717991e-06
Iter: 769 loss: 1.44580099e-06
Iter: 770 loss: 1.44496244e-06
Iter: 771 loss: 1.44676505e-06
Iter: 772 loss: 1.4446407e-06
Iter: 773 loss: 1.4437062e-06
Iter: 774 loss: 1.45153058e-06
Iter: 775 loss: 1.44366606e-06
Iter: 776 loss: 1.44324474e-06
Iter: 777 loss: 1.44391049e-06
Iter: 778 loss: 1.44304386e-06
Iter: 779 loss: 1.44245541e-06
Iter: 780 loss: 1.44276294e-06
Iter: 781 loss: 1.44205524e-06
Iter: 782 loss: 1.44145588e-06
Iter: 783 loss: 1.44253409e-06
Iter: 784 loss: 1.44118417e-06
Iter: 785 loss: 1.44042303e-06
Iter: 786 loss: 1.44500893e-06
Iter: 787 loss: 1.44033038e-06
Iter: 788 loss: 1.43980094e-06
Iter: 789 loss: 1.43888542e-06
Iter: 790 loss: 1.46154571e-06
Iter: 791 loss: 1.43888178e-06
Iter: 792 loss: 1.43777174e-06
Iter: 793 loss: 1.44011653e-06
Iter: 794 loss: 1.4373702e-06
Iter: 795 loss: 1.43704779e-06
Iter: 796 loss: 1.43681962e-06
Iter: 797 loss: 1.43638886e-06
Iter: 798 loss: 1.43551028e-06
Iter: 799 loss: 1.45214926e-06
Iter: 800 loss: 1.43553029e-06
Iter: 801 loss: 1.4347562e-06
Iter: 802 loss: 1.43719978e-06
Iter: 803 loss: 1.43452098e-06
Iter: 804 loss: 1.43384057e-06
Iter: 805 loss: 1.43964076e-06
Iter: 806 loss: 1.43378952e-06
Iter: 807 loss: 1.4331788e-06
Iter: 808 loss: 1.43527825e-06
Iter: 809 loss: 1.4330584e-06
Iter: 810 loss: 1.43231205e-06
Iter: 811 loss: 1.43271427e-06
Iter: 812 loss: 1.4318581e-06
Iter: 813 loss: 1.43115926e-06
Iter: 814 loss: 1.43649322e-06
Iter: 815 loss: 1.43111106e-06
Iter: 816 loss: 1.43053069e-06
Iter: 817 loss: 1.43043951e-06
Iter: 818 loss: 1.42998101e-06
Iter: 819 loss: 1.4293812e-06
Iter: 820 loss: 1.43376042e-06
Iter: 821 loss: 1.4293604e-06
Iter: 822 loss: 1.42867896e-06
Iter: 823 loss: 1.4284858e-06
Iter: 824 loss: 1.42808494e-06
Iter: 825 loss: 1.42743545e-06
Iter: 826 loss: 1.42744659e-06
Iter: 827 loss: 1.42696126e-06
Iter: 828 loss: 1.4261127e-06
Iter: 829 loss: 1.43075238e-06
Iter: 830 loss: 1.42600936e-06
Iter: 831 loss: 1.42514557e-06
Iter: 832 loss: 1.43215073e-06
Iter: 833 loss: 1.42513e-06
Iter: 834 loss: 1.42464137e-06
Iter: 835 loss: 1.42373142e-06
Iter: 836 loss: 1.44088824e-06
Iter: 837 loss: 1.42371061e-06
Iter: 838 loss: 1.42283034e-06
Iter: 839 loss: 1.42828253e-06
Iter: 840 loss: 1.42270937e-06
Iter: 841 loss: 1.42191504e-06
Iter: 842 loss: 1.42897443e-06
Iter: 843 loss: 1.42186184e-06
Iter: 844 loss: 1.42123395e-06
Iter: 845 loss: 1.42400063e-06
Iter: 846 loss: 1.42110935e-06
Iter: 847 loss: 1.42062527e-06
Iter: 848 loss: 1.42111378e-06
Iter: 849 loss: 1.42040199e-06
Iter: 850 loss: 1.41983094e-06
Iter: 851 loss: 1.42232011e-06
Iter: 852 loss: 1.41972157e-06
Iter: 853 loss: 1.41919668e-06
Iter: 854 loss: 1.41873375e-06
Iter: 855 loss: 1.41857299e-06
Iter: 856 loss: 1.41793009e-06
Iter: 857 loss: 1.41794021e-06
Iter: 858 loss: 1.41747569e-06
Iter: 859 loss: 1.41665214e-06
Iter: 860 loss: 1.4361774e-06
Iter: 861 loss: 1.41666169e-06
Iter: 862 loss: 1.4157572e-06
Iter: 863 loss: 1.4160986e-06
Iter: 864 loss: 1.41513783e-06
Iter: 865 loss: 1.41505518e-06
Iter: 866 loss: 1.41469059e-06
Iter: 867 loss: 1.41426403e-06
Iter: 868 loss: 1.41375915e-06
Iter: 869 loss: 1.41369367e-06
Iter: 870 loss: 1.41307794e-06
Iter: 871 loss: 1.41276394e-06
Iter: 872 loss: 1.41244982e-06
Iter: 873 loss: 1.41167902e-06
Iter: 874 loss: 1.4189543e-06
Iter: 875 loss: 1.41166504e-06
Iter: 876 loss: 1.41086616e-06
Iter: 877 loss: 1.41422356e-06
Iter: 878 loss: 1.41071791e-06
Iter: 879 loss: 1.41003045e-06
Iter: 880 loss: 1.41204009e-06
Iter: 881 loss: 1.40983639e-06
Iter: 882 loss: 1.40929467e-06
Iter: 883 loss: 1.41035423e-06
Iter: 884 loss: 1.40910186e-06
Iter: 885 loss: 1.40838438e-06
Iter: 886 loss: 1.40966029e-06
Iter: 887 loss: 1.40808015e-06
Iter: 888 loss: 1.40749853e-06
Iter: 889 loss: 1.40879729e-06
Iter: 890 loss: 1.40727411e-06
Iter: 891 loss: 1.40659085e-06
Iter: 892 loss: 1.41071973e-06
Iter: 893 loss: 1.40653833e-06
Iter: 894 loss: 1.40613645e-06
Iter: 895 loss: 1.405381e-06
Iter: 896 loss: 1.40539396e-06
Iter: 897 loss: 1.40455609e-06
Iter: 898 loss: 1.4057207e-06
Iter: 899 loss: 1.40415955e-06
Iter: 900 loss: 1.40380735e-06
Iter: 901 loss: 1.40360476e-06
Iter: 902 loss: 1.40321288e-06
Iter: 903 loss: 1.40227098e-06
Iter: 904 loss: 1.41302166e-06
Iter: 905 loss: 1.4021997e-06
Iter: 906 loss: 1.40115162e-06
Iter: 907 loss: 1.40490238e-06
Iter: 908 loss: 1.4009197e-06
Iter: 909 loss: 1.40054817e-06
Iter: 910 loss: 1.40049497e-06
Iter: 911 loss: 1.40003976e-06
Iter: 912 loss: 1.39967187e-06
Iter: 913 loss: 1.39952965e-06
Iter: 914 loss: 1.39886993e-06
Iter: 915 loss: 1.4037854e-06
Iter: 916 loss: 1.3988182e-06
Iter: 917 loss: 1.3984328e-06
Iter: 918 loss: 1.39962719e-06
Iter: 919 loss: 1.39833082e-06
Iter: 920 loss: 1.39784811e-06
Iter: 921 loss: 1.39736994e-06
Iter: 922 loss: 1.39726308e-06
Iter: 923 loss: 1.39670578e-06
Iter: 924 loss: 1.39671477e-06
Iter: 925 loss: 1.3962657e-06
Iter: 926 loss: 1.39597034e-06
Iter: 927 loss: 1.39575388e-06
Iter: 928 loss: 1.39514782e-06
Iter: 929 loss: 1.39470853e-06
Iter: 930 loss: 1.39443716e-06
Iter: 931 loss: 1.39365704e-06
Iter: 932 loss: 1.40150541e-06
Iter: 933 loss: 1.39363055e-06
Iter: 934 loss: 1.39291024e-06
Iter: 935 loss: 1.39918825e-06
Iter: 936 loss: 1.39284407e-06
Iter: 937 loss: 1.39250722e-06
Iter: 938 loss: 1.39180383e-06
Iter: 939 loss: 1.40275711e-06
Iter: 940 loss: 1.39179338e-06
Iter: 941 loss: 1.39095141e-06
Iter: 942 loss: 1.39537042e-06
Iter: 943 loss: 1.39083249e-06
Iter: 944 loss: 1.39030453e-06
Iter: 945 loss: 1.39030476e-06
Iter: 946 loss: 1.38987525e-06
Iter: 947 loss: 1.3892527e-06
Iter: 948 loss: 1.38926043e-06
Iter: 949 loss: 1.38841801e-06
Iter: 950 loss: 1.39510848e-06
Iter: 951 loss: 1.38839164e-06
Iter: 952 loss: 1.38793462e-06
Iter: 953 loss: 1.38970017e-06
Iter: 954 loss: 1.38783616e-06
Iter: 955 loss: 1.3873921e-06
Iter: 956 loss: 1.38712721e-06
Iter: 957 loss: 1.38695e-06
Iter: 958 loss: 1.38635903e-06
Iter: 959 loss: 1.39318638e-06
Iter: 960 loss: 1.38631663e-06
Iter: 961 loss: 1.38593418e-06
Iter: 962 loss: 1.38545943e-06
Iter: 963 loss: 1.38540474e-06
Iter: 964 loss: 1.38478595e-06
Iter: 965 loss: 1.38461144e-06
Iter: 966 loss: 1.38418636e-06
Iter: 967 loss: 1.38362816e-06
Iter: 968 loss: 1.38359974e-06
Iter: 969 loss: 1.38298901e-06
Iter: 970 loss: 1.38330938e-06
Iter: 971 loss: 1.38260737e-06
Iter: 972 loss: 1.38208725e-06
Iter: 973 loss: 1.38118e-06
Iter: 974 loss: 1.40330553e-06
Iter: 975 loss: 1.38117275e-06
Iter: 976 loss: 1.38063479e-06
Iter: 977 loss: 1.38057862e-06
Iter: 978 loss: 1.37999086e-06
Iter: 979 loss: 1.3823651e-06
Iter: 980 loss: 1.37991e-06
Iter: 981 loss: 1.37948041e-06
Iter: 982 loss: 1.37939389e-06
Iter: 983 loss: 1.3791423e-06
Iter: 984 loss: 1.37843085e-06
Iter: 985 loss: 1.38132191e-06
Iter: 986 loss: 1.37826373e-06
Iter: 987 loss: 1.37771576e-06
Iter: 988 loss: 1.37905204e-06
Iter: 989 loss: 1.37754955e-06
Iter: 990 loss: 1.37699658e-06
Iter: 991 loss: 1.37831898e-06
Iter: 992 loss: 1.37684333e-06
Iter: 993 loss: 1.37625796e-06
Iter: 994 loss: 1.37911297e-06
Iter: 995 loss: 1.37618963e-06
Iter: 996 loss: 1.37578149e-06
Iter: 997 loss: 1.37515735e-06
Iter: 998 loss: 1.37517713e-06
Iter: 999 loss: 1.37441509e-06
Iter: 1000 loss: 1.37516759e-06
Iter: 1001 loss: 1.37400798e-06
Iter: 1002 loss: 1.37388554e-06
Iter: 1003 loss: 1.3735837e-06
Iter: 1004 loss: 1.37328732e-06
Iter: 1005 loss: 1.37265283e-06
Iter: 1006 loss: 1.38339624e-06
Iter: 1007 loss: 1.37264396e-06
Iter: 1008 loss: 1.37186589e-06
Iter: 1009 loss: 1.37156371e-06
Iter: 1010 loss: 1.37112966e-06
Iter: 1011 loss: 1.37072141e-06
Iter: 1012 loss: 1.37054064e-06
Iter: 1013 loss: 1.36995436e-06
Iter: 1014 loss: 1.37036989e-06
Iter: 1015 loss: 1.36960648e-06
Iter: 1016 loss: 1.36925917e-06
Iter: 1017 loss: 1.37147845e-06
Iter: 1018 loss: 1.36922074e-06
Iter: 1019 loss: 1.36884887e-06
Iter: 1020 loss: 1.36853691e-06
Iter: 1021 loss: 1.36846552e-06
Iter: 1022 loss: 1.36783046e-06
Iter: 1023 loss: 1.37082418e-06
Iter: 1024 loss: 1.36770416e-06
Iter: 1025 loss: 1.36720064e-06
Iter: 1026 loss: 1.36919539e-06
Iter: 1027 loss: 1.36707627e-06
Iter: 1028 loss: 1.36651522e-06
Iter: 1029 loss: 1.36653557e-06
Iter: 1030 loss: 1.36606877e-06
Iter: 1031 loss: 1.36524955e-06
Iter: 1032 loss: 1.3652143e-06
Iter: 1033 loss: 1.36464826e-06
Iter: 1034 loss: 1.36373785e-06
Iter: 1035 loss: 1.36605445e-06
Iter: 1036 loss: 1.36341362e-06
Iter: 1037 loss: 1.36323649e-06
Iter: 1038 loss: 1.36295887e-06
Iter: 1039 loss: 1.36274491e-06
Iter: 1040 loss: 1.3621418e-06
Iter: 1041 loss: 1.36664903e-06
Iter: 1042 loss: 1.36198241e-06
Iter: 1043 loss: 1.36122298e-06
Iter: 1044 loss: 1.36338122e-06
Iter: 1045 loss: 1.36097299e-06
Iter: 1046 loss: 1.3606583e-06
Iter: 1047 loss: 1.3605495e-06
Iter: 1048 loss: 1.36013773e-06
Iter: 1049 loss: 1.35934681e-06
Iter: 1050 loss: 1.37660993e-06
Iter: 1051 loss: 1.35933328e-06
Iter: 1052 loss: 1.35887603e-06
Iter: 1053 loss: 1.35887399e-06
Iter: 1054 loss: 1.35847154e-06
Iter: 1055 loss: 1.35788662e-06
Iter: 1056 loss: 1.35788343e-06
Iter: 1057 loss: 1.35733569e-06
Iter: 1058 loss: 1.36521567e-06
Iter: 1059 loss: 1.35731898e-06
Iter: 1060 loss: 1.35695927e-06
Iter: 1061 loss: 1.35769869e-06
Iter: 1062 loss: 1.35678033e-06
Iter: 1063 loss: 1.35634082e-06
Iter: 1064 loss: 1.35604228e-06
Iter: 1065 loss: 1.35588311e-06
Iter: 1066 loss: 1.35513892e-06
Iter: 1067 loss: 1.35587175e-06
Iter: 1068 loss: 1.35475864e-06
Iter: 1069 loss: 1.3539036e-06
Iter: 1070 loss: 1.35454377e-06
Iter: 1071 loss: 1.3534052e-06
Iter: 1072 loss: 1.35304708e-06
Iter: 1073 loss: 1.3528138e-06
Iter: 1074 loss: 1.35244863e-06
Iter: 1075 loss: 1.35157961e-06
Iter: 1076 loss: 1.36011909e-06
Iter: 1077 loss: 1.35153e-06
Iter: 1078 loss: 1.35071582e-06
Iter: 1079 loss: 1.35354412e-06
Iter: 1080 loss: 1.35054177e-06
Iter: 1081 loss: 1.35006144e-06
Iter: 1082 loss: 1.34999345e-06
Iter: 1083 loss: 1.34967058e-06
Iter: 1084 loss: 1.34905008e-06
Iter: 1085 loss: 1.34902882e-06
Iter: 1086 loss: 1.34867742e-06
Iter: 1087 loss: 1.34867059e-06
Iter: 1088 loss: 1.34833397e-06
Iter: 1089 loss: 1.34776553e-06
Iter: 1090 loss: 1.34773518e-06
Iter: 1091 loss: 1.34720347e-06
Iter: 1092 loss: 1.35437563e-06
Iter: 1093 loss: 1.34718221e-06
Iter: 1094 loss: 1.34672143e-06
Iter: 1095 loss: 1.34648883e-06
Iter: 1096 loss: 1.34628499e-06
Iter: 1097 loss: 1.34561515e-06
Iter: 1098 loss: 1.34799484e-06
Iter: 1099 loss: 1.34545166e-06
Iter: 1100 loss: 1.34486527e-06
Iter: 1101 loss: 1.34494439e-06
Iter: 1102 loss: 1.34437357e-06
Iter: 1103 loss: 1.34377717e-06
Iter: 1104 loss: 1.34929428e-06
Iter: 1105 loss: 1.34377592e-06
Iter: 1106 loss: 1.34319441e-06
Iter: 1107 loss: 1.34631739e-06
Iter: 1108 loss: 1.34309403e-06
Iter: 1109 loss: 1.34274148e-06
Iter: 1110 loss: 1.34210154e-06
Iter: 1111 loss: 1.35832715e-06
Iter: 1112 loss: 1.3420879e-06
Iter: 1113 loss: 1.34151446e-06
Iter: 1114 loss: 1.34585628e-06
Iter: 1115 loss: 1.34149911e-06
Iter: 1116 loss: 1.34075481e-06
Iter: 1117 loss: 1.34261745e-06
Iter: 1118 loss: 1.34049151e-06
Iter: 1119 loss: 1.34006928e-06
Iter: 1120 loss: 1.33999163e-06
Iter: 1121 loss: 1.33966932e-06
Iter: 1122 loss: 1.33921253e-06
Iter: 1123 loss: 1.3392073e-06
Iter: 1124 loss: 1.33887602e-06
Iter: 1125 loss: 1.338277e-06
Iter: 1126 loss: 1.33827803e-06
Iter: 1127 loss: 1.33777417e-06
Iter: 1128 loss: 1.33776462e-06
Iter: 1129 loss: 1.33738672e-06
Iter: 1130 loss: 1.33697904e-06
Iter: 1131 loss: 1.33692254e-06
Iter: 1132 loss: 1.33628851e-06
Iter: 1133 loss: 1.33848948e-06
Iter: 1134 loss: 1.336114e-06
Iter: 1135 loss: 1.33544575e-06
Iter: 1136 loss: 1.33521962e-06
Iter: 1137 loss: 1.33485082e-06
Iter: 1138 loss: 1.33413323e-06
Iter: 1139 loss: 1.33938079e-06
Iter: 1140 loss: 1.3340873e-06
Iter: 1141 loss: 1.33343519e-06
Iter: 1142 loss: 1.33926119e-06
Iter: 1143 loss: 1.33337426e-06
Iter: 1144 loss: 1.33308538e-06
Iter: 1145 loss: 1.33245771e-06
Iter: 1146 loss: 1.34339052e-06
Iter: 1147 loss: 1.33242918e-06
Iter: 1148 loss: 1.3319617e-06
Iter: 1149 loss: 1.33195056e-06
Iter: 1150 loss: 1.33142657e-06
Iter: 1151 loss: 1.33215349e-06
Iter: 1152 loss: 1.33114133e-06
Iter: 1153 loss: 1.33070671e-06
Iter: 1154 loss: 1.33020126e-06
Iter: 1155 loss: 1.33014237e-06
Iter: 1156 loss: 1.32964817e-06
Iter: 1157 loss: 1.32962191e-06
Iter: 1158 loss: 1.32918331e-06
Iter: 1159 loss: 1.32837567e-06
Iter: 1160 loss: 1.34375148e-06
Iter: 1161 loss: 1.3283244e-06
Iter: 1162 loss: 1.32786431e-06
Iter: 1163 loss: 1.32778678e-06
Iter: 1164 loss: 1.32743116e-06
Iter: 1165 loss: 1.32684613e-06
Iter: 1166 loss: 1.32684249e-06
Iter: 1167 loss: 1.32604623e-06
Iter: 1168 loss: 1.32780701e-06
Iter: 1169 loss: 1.32574883e-06
Iter: 1170 loss: 1.32512218e-06
Iter: 1171 loss: 1.3317931e-06
Iter: 1172 loss: 1.32509933e-06
Iter: 1173 loss: 1.32469e-06
Iter: 1174 loss: 1.32491084e-06
Iter: 1175 loss: 1.32441983e-06
Iter: 1176 loss: 1.32360867e-06
Iter: 1177 loss: 1.32516107e-06
Iter: 1178 loss: 1.32324806e-06
Iter: 1179 loss: 1.32280024e-06
Iter: 1180 loss: 1.32239575e-06
Iter: 1181 loss: 1.32226182e-06
Iter: 1182 loss: 1.32209539e-06
Iter: 1183 loss: 1.32193202e-06
Iter: 1184 loss: 1.32163962e-06
Iter: 1185 loss: 1.3211004e-06
Iter: 1186 loss: 1.32110915e-06
Iter: 1187 loss: 1.32060359e-06
Iter: 1188 loss: 1.32225432e-06
Iter: 1189 loss: 1.3204608e-06
Iter: 1190 loss: 1.31995193e-06
Iter: 1191 loss: 1.32348623e-06
Iter: 1192 loss: 1.31990896e-06
Iter: 1193 loss: 1.31950617e-06
Iter: 1194 loss: 1.31876038e-06
Iter: 1195 loss: 1.33198682e-06
Iter: 1196 loss: 1.31871684e-06
Iter: 1197 loss: 1.31795446e-06
Iter: 1198 loss: 1.31793081e-06
Iter: 1199 loss: 1.31759452e-06
Iter: 1200 loss: 1.31686204e-06
Iter: 1201 loss: 1.33086075e-06
Iter: 1202 loss: 1.31688239e-06
Iter: 1203 loss: 1.31607067e-06
Iter: 1204 loss: 1.32158584e-06
Iter: 1205 loss: 1.31598654e-06
Iter: 1206 loss: 1.31546403e-06
Iter: 1207 loss: 1.31665502e-06
Iter: 1208 loss: 1.31523484e-06
Iter: 1209 loss: 1.3147494e-06
Iter: 1210 loss: 1.31741763e-06
Iter: 1211 loss: 1.31469e-06
Iter: 1212 loss: 1.31418665e-06
Iter: 1213 loss: 1.31629736e-06
Iter: 1214 loss: 1.31404249e-06
Iter: 1215 loss: 1.31364777e-06
Iter: 1216 loss: 1.31284241e-06
Iter: 1217 loss: 1.32590253e-06
Iter: 1218 loss: 1.31279444e-06
Iter: 1219 loss: 1.31258605e-06
Iter: 1220 loss: 1.31235117e-06
Iter: 1221 loss: 1.3119901e-06
Iter: 1222 loss: 1.31108482e-06
Iter: 1223 loss: 1.32113666e-06
Iter: 1224 loss: 1.3109925e-06
Iter: 1225 loss: 1.31017919e-06
Iter: 1226 loss: 1.3127692e-06
Iter: 1227 loss: 1.30993237e-06
Iter: 1228 loss: 1.30967589e-06
Iter: 1229 loss: 1.3095364e-06
Iter: 1230 loss: 1.30927128e-06
Iter: 1231 loss: 1.30882677e-06
Iter: 1232 loss: 1.30883018e-06
Iter: 1233 loss: 1.30838259e-06
Iter: 1234 loss: 1.31146567e-06
Iter: 1235 loss: 1.30837793e-06
Iter: 1236 loss: 1.30785418e-06
Iter: 1237 loss: 1.30805984e-06
Iter: 1238 loss: 1.3075155e-06
Iter: 1239 loss: 1.30699e-06
Iter: 1240 loss: 1.30637181e-06
Iter: 1241 loss: 1.30631054e-06
Iter: 1242 loss: 1.30547733e-06
Iter: 1243 loss: 1.31783e-06
Iter: 1244 loss: 1.30547858e-06
Iter: 1245 loss: 1.30501098e-06
Iter: 1246 loss: 1.30615808e-06
Iter: 1247 loss: 1.30483863e-06
Iter: 1248 loss: 1.30421267e-06
Iter: 1249 loss: 1.30652074e-06
Iter: 1250 loss: 1.30405306e-06
Iter: 1251 loss: 1.3036647e-06
Iter: 1252 loss: 1.30362059e-06
Iter: 1253 loss: 1.30329681e-06
Iter: 1254 loss: 1.30299793e-06
Iter: 1255 loss: 1.30298042e-06
Iter: 1256 loss: 1.30268972e-06
Iter: 1257 loss: 1.30198828e-06
Iter: 1258 loss: 1.3078452e-06
Iter: 1259 loss: 1.30187061e-06
Iter: 1260 loss: 1.30101694e-06
Iter: 1261 loss: 1.30204467e-06
Iter: 1262 loss: 1.30060221e-06
Iter: 1263 loss: 1.30047988e-06
Iter: 1264 loss: 1.3001727e-06
Iter: 1265 loss: 1.29988962e-06
Iter: 1266 loss: 1.29910541e-06
Iter: 1267 loss: 1.30562694e-06
Iter: 1268 loss: 1.29898319e-06
Iter: 1269 loss: 1.29850741e-06
Iter: 1270 loss: 1.29845682e-06
Iter: 1271 loss: 1.2980139e-06
Iter: 1272 loss: 1.29820114e-06
Iter: 1273 loss: 1.29768648e-06
Iter: 1274 loss: 1.29715306e-06
Iter: 1275 loss: 1.29670411e-06
Iter: 1276 loss: 1.29656269e-06
Iter: 1277 loss: 1.29591967e-06
Iter: 1278 loss: 1.300956e-06
Iter: 1279 loss: 1.29586726e-06
Iter: 1280 loss: 1.29528382e-06
Iter: 1281 loss: 1.30135072e-06
Iter: 1282 loss: 1.29529235e-06
Iter: 1283 loss: 1.2948924e-06
Iter: 1284 loss: 1.29528735e-06
Iter: 1285 loss: 1.29466787e-06
Iter: 1286 loss: 1.29416799e-06
Iter: 1287 loss: 1.29412547e-06
Iter: 1288 loss: 1.29374189e-06
Iter: 1289 loss: 1.29354612e-06
Iter: 1290 loss: 1.29345574e-06
Iter: 1291 loss: 1.29319142e-06
Iter: 1292 loss: 1.29256227e-06
Iter: 1293 loss: 1.29777413e-06
Iter: 1294 loss: 1.29245836e-06
Iter: 1295 loss: 1.29177715e-06
Iter: 1296 loss: 1.29521061e-06
Iter: 1297 loss: 1.29167427e-06
Iter: 1298 loss: 1.29129842e-06
Iter: 1299 loss: 1.29128875e-06
Iter: 1300 loss: 1.29098225e-06
Iter: 1301 loss: 1.29020509e-06
Iter: 1302 loss: 1.2963344e-06
Iter: 1303 loss: 1.29010709e-06
Iter: 1304 loss: 1.28968372e-06
Iter: 1305 loss: 1.28963802e-06
Iter: 1306 loss: 1.28913223e-06
Iter: 1307 loss: 1.28914303e-06
Iter: 1308 loss: 1.28877241e-06
Iter: 1309 loss: 1.28819283e-06
Iter: 1310 loss: 1.28836859e-06
Iter: 1311 loss: 1.28782608e-06
Iter: 1312 loss: 1.28721058e-06
Iter: 1313 loss: 1.28741749e-06
Iter: 1314 loss: 1.28680426e-06
Iter: 1315 loss: 1.28593388e-06
Iter: 1316 loss: 1.28928764e-06
Iter: 1317 loss: 1.28571219e-06
Iter: 1318 loss: 1.28534271e-06
Iter: 1319 loss: 1.285199e-06
Iter: 1320 loss: 1.28489683e-06
Iter: 1321 loss: 1.28465433e-06
Iter: 1322 loss: 1.2845594e-06
Iter: 1323 loss: 1.28403553e-06
Iter: 1324 loss: 1.28584861e-06
Iter: 1325 loss: 1.28388274e-06
Iter: 1326 loss: 1.28352065e-06
Iter: 1327 loss: 1.28890304e-06
Iter: 1328 loss: 1.2835244e-06
Iter: 1329 loss: 1.28325883e-06
Iter: 1330 loss: 1.28275451e-06
Iter: 1331 loss: 1.29224748e-06
Iter: 1332 loss: 1.28277736e-06
Iter: 1333 loss: 1.28229397e-06
Iter: 1334 loss: 1.28375177e-06
Iter: 1335 loss: 1.28215606e-06
Iter: 1336 loss: 1.28172087e-06
Iter: 1337 loss: 1.28757597e-06
Iter: 1338 loss: 1.28172564e-06
Iter: 1339 loss: 1.28140255e-06
Iter: 1340 loss: 1.28084162e-06
Iter: 1341 loss: 1.29339867e-06
Iter: 1342 loss: 1.28082911e-06
Iter: 1343 loss: 1.28029774e-06
Iter: 1344 loss: 1.28837246e-06
Iter: 1345 loss: 1.28029455e-06
Iter: 1346 loss: 1.2797808e-06
Iter: 1347 loss: 1.27924523e-06
Iter: 1348 loss: 1.27914006e-06
Iter: 1349 loss: 1.2785448e-06
Iter: 1350 loss: 1.27815292e-06
Iter: 1351 loss: 1.27791736e-06
Iter: 1352 loss: 1.2771352e-06
Iter: 1353 loss: 1.28508009e-06
Iter: 1354 loss: 1.27710678e-06
Iter: 1355 loss: 1.27664589e-06
Iter: 1356 loss: 1.28081729e-06
Iter: 1357 loss: 1.27661724e-06
Iter: 1358 loss: 1.2760529e-06
Iter: 1359 loss: 1.27714691e-06
Iter: 1360 loss: 1.27581779e-06
Iter: 1361 loss: 1.2754881e-06
Iter: 1362 loss: 1.27538124e-06
Iter: 1363 loss: 1.27520184e-06
Iter: 1364 loss: 1.2746375e-06
Iter: 1365 loss: 1.28031729e-06
Iter: 1366 loss: 1.2746325e-06
Iter: 1367 loss: 1.2743501e-06
Iter: 1368 loss: 1.27352814e-06
Iter: 1369 loss: 1.27907845e-06
Iter: 1370 loss: 1.27332294e-06
Iter: 1371 loss: 1.27297278e-06
Iter: 1372 loss: 1.27286035e-06
Iter: 1373 loss: 1.2725186e-06
Iter: 1374 loss: 1.27413773e-06
Iter: 1375 loss: 1.27243811e-06
Iter: 1376 loss: 1.2720933e-06
Iter: 1377 loss: 1.27216276e-06
Iter: 1378 loss: 1.27183876e-06
Iter: 1379 loss: 1.27136173e-06
Iter: 1380 loss: 1.27149497e-06
Iter: 1381 loss: 1.27102089e-06
Iter: 1382 loss: 1.27054091e-06
Iter: 1383 loss: 1.2769874e-06
Iter: 1384 loss: 1.27052556e-06
Iter: 1385 loss: 1.27007775e-06
Iter: 1386 loss: 1.26955661e-06
Iter: 1387 loss: 1.26949828e-06
Iter: 1388 loss: 1.26889279e-06
Iter: 1389 loss: 1.26976988e-06
Iter: 1390 loss: 1.26855434e-06
Iter: 1391 loss: 1.26802547e-06
Iter: 1392 loss: 1.27311114e-06
Iter: 1393 loss: 1.26801854e-06
Iter: 1394 loss: 1.26744726e-06
Iter: 1395 loss: 1.2692476e-06
Iter: 1396 loss: 1.26723228e-06
Iter: 1397 loss: 1.26697728e-06
Iter: 1398 loss: 1.26699649e-06
Iter: 1399 loss: 1.2667532e-06
Iter: 1400 loss: 1.26632062e-06
Iter: 1401 loss: 1.27061287e-06
Iter: 1402 loss: 1.26631585e-06
Iter: 1403 loss: 1.26605289e-06
Iter: 1404 loss: 1.26543341e-06
Iter: 1405 loss: 1.27285739e-06
Iter: 1406 loss: 1.26540783e-06
Iter: 1407 loss: 1.26473719e-06
Iter: 1408 loss: 1.26607699e-06
Iter: 1409 loss: 1.26446662e-06
Iter: 1410 loss: 1.26400164e-06
Iter: 1411 loss: 1.26394798e-06
Iter: 1412 loss: 1.26359862e-06
Iter: 1413 loss: 1.26292821e-06
Iter: 1414 loss: 1.27661372e-06
Iter: 1415 loss: 1.26293298e-06
Iter: 1416 loss: 1.26249324e-06
Iter: 1417 loss: 1.26247539e-06
Iter: 1418 loss: 1.2621864e-06
Iter: 1419 loss: 1.26228349e-06
Iter: 1420 loss: 1.26197767e-06
Iter: 1421 loss: 1.2614496e-06
Iter: 1422 loss: 1.26183511e-06
Iter: 1423 loss: 1.26114185e-06
Iter: 1424 loss: 1.26064788e-06
Iter: 1425 loss: 1.26010502e-06
Iter: 1426 loss: 1.26003511e-06
Iter: 1427 loss: 1.25925271e-06
Iter: 1428 loss: 1.26488385e-06
Iter: 1429 loss: 1.25914494e-06
Iter: 1430 loss: 1.25864108e-06
Iter: 1431 loss: 1.25864119e-06
Iter: 1432 loss: 1.25834686e-06
Iter: 1433 loss: 1.25787142e-06
Iter: 1434 loss: 1.257863e-06
Iter: 1435 loss: 1.25766087e-06
Iter: 1436 loss: 1.25756344e-06
Iter: 1437 loss: 1.25736301e-06
Iter: 1438 loss: 1.25679617e-06
Iter: 1439 loss: 1.26061514e-06
Iter: 1440 loss: 1.2566594e-06
Iter: 1441 loss: 1.25613508e-06
Iter: 1442 loss: 1.25998849e-06
Iter: 1443 loss: 1.25608267e-06
Iter: 1444 loss: 1.25561246e-06
Iter: 1445 loss: 1.26040413e-06
Iter: 1446 loss: 1.25558449e-06
Iter: 1447 loss: 1.25528163e-06
Iter: 1448 loss: 1.2548187e-06
Iter: 1449 loss: 1.25482563e-06
Iter: 1450 loss: 1.25435463e-06
Iter: 1451 loss: 1.25727729e-06
Iter: 1452 loss: 1.25429165e-06
Iter: 1453 loss: 1.25374277e-06
Iter: 1454 loss: 1.25535541e-06
Iter: 1455 loss: 1.25358793e-06
Iter: 1456 loss: 1.25320139e-06
Iter: 1457 loss: 1.25397514e-06
Iter: 1458 loss: 1.2530835e-06
Iter: 1459 loss: 1.25262159e-06
Iter: 1460 loss: 1.25264637e-06
Iter: 1461 loss: 1.25225165e-06
Iter: 1462 loss: 1.25171437e-06
Iter: 1463 loss: 1.25196823e-06
Iter: 1464 loss: 1.25134875e-06
Iter: 1465 loss: 1.25096631e-06
Iter: 1466 loss: 1.25094959e-06
Iter: 1467 loss: 1.25049451e-06
Iter: 1468 loss: 1.2504488e-06
Iter: 1469 loss: 1.25009319e-06
Iter: 1470 loss: 1.24973189e-06
Iter: 1471 loss: 1.25217241e-06
Iter: 1472 loss: 1.24967892e-06
Iter: 1473 loss: 1.24926578e-06
Iter: 1474 loss: 1.24958126e-06
Iter: 1475 loss: 1.2489304e-06
Iter: 1476 loss: 1.24860196e-06
Iter: 1477 loss: 1.24828318e-06
Iter: 1478 loss: 1.24822077e-06
Iter: 1479 loss: 1.24803239e-06
Iter: 1480 loss: 1.24795486e-06
Iter: 1481 loss: 1.24770986e-06
Iter: 1482 loss: 1.24716848e-06
Iter: 1483 loss: 1.25308361e-06
Iter: 1484 loss: 1.24711812e-06
Iter: 1485 loss: 1.24655594e-06
Iter: 1486 loss: 1.24783787e-06
Iter: 1487 loss: 1.24638473e-06
Iter: 1488 loss: 1.24579446e-06
Iter: 1489 loss: 1.25310646e-06
Iter: 1490 loss: 1.24579424e-06
Iter: 1491 loss: 1.24540509e-06
Iter: 1492 loss: 1.24476571e-06
Iter: 1493 loss: 1.24475378e-06
Iter: 1494 loss: 1.2441277e-06
Iter: 1495 loss: 1.24925953e-06
Iter: 1496 loss: 1.24410985e-06
Iter: 1497 loss: 1.24359713e-06
Iter: 1498 loss: 1.24567748e-06
Iter: 1499 loss: 1.24349901e-06
Iter: 1500 loss: 1.24313897e-06
Iter: 1501 loss: 1.24649205e-06
Iter: 1502 loss: 1.24311464e-06
Iter: 1503 loss: 1.24277335e-06
Iter: 1504 loss: 1.24286544e-06
Iter: 1505 loss: 1.24251824e-06
Iter: 1506 loss: 1.24224186e-06
Iter: 1507 loss: 1.24366045e-06
Iter: 1508 loss: 1.24221492e-06
Iter: 1509 loss: 1.24178814e-06
Iter: 1510 loss: 1.2415386e-06
Iter: 1511 loss: 1.24136557e-06
Iter: 1512 loss: 1.24095186e-06
Iter: 1513 loss: 1.24038786e-06
Iter: 1514 loss: 1.24036046e-06
Iter: 1515 loss: 1.2398973e-06
Iter: 1516 loss: 1.23985114e-06
Iter: 1517 loss: 1.23941322e-06
Iter: 1518 loss: 1.24054873e-06
Iter: 1519 loss: 1.23924588e-06
Iter: 1520 loss: 1.23892653e-06
Iter: 1521 loss: 1.23864038e-06
Iter: 1522 loss: 1.23857751e-06
Iter: 1523 loss: 1.23820678e-06
Iter: 1524 loss: 1.23818688e-06
Iter: 1525 loss: 1.23785071e-06
Iter: 1526 loss: 1.23734071e-06
Iter: 1527 loss: 1.23732912e-06
Iter: 1528 loss: 1.23681502e-06
Iter: 1529 loss: 1.23627444e-06
Iter: 1530 loss: 1.23619463e-06
Iter: 1531 loss: 1.23578945e-06
Iter: 1532 loss: 1.23567133e-06
Iter: 1533 loss: 1.23531333e-06
Iter: 1534 loss: 1.23716018e-06
Iter: 1535 loss: 1.23525183e-06
Iter: 1536 loss: 1.2348778e-06
Iter: 1537 loss: 1.23527172e-06
Iter: 1538 loss: 1.23465384e-06
Iter: 1539 loss: 1.2343155e-06
Iter: 1540 loss: 1.23625694e-06
Iter: 1541 loss: 1.23424024e-06
Iter: 1542 loss: 1.23394307e-06
Iter: 1543 loss: 1.23497045e-06
Iter: 1544 loss: 1.23383472e-06
Iter: 1545 loss: 1.23358791e-06
Iter: 1546 loss: 1.23314135e-06
Iter: 1547 loss: 1.24024257e-06
Iter: 1548 loss: 1.23311497e-06
Iter: 1549 loss: 1.23254426e-06
Iter: 1550 loss: 1.2362425e-06
Iter: 1551 loss: 1.23247514e-06
Iter: 1552 loss: 1.23183918e-06
Iter: 1553 loss: 1.23544942e-06
Iter: 1554 loss: 1.23175425e-06
Iter: 1555 loss: 1.23143514e-06
Iter: 1556 loss: 1.23090899e-06
Iter: 1557 loss: 1.24325754e-06
Iter: 1558 loss: 1.23090126e-06
Iter: 1559 loss: 1.23053076e-06
Iter: 1560 loss: 1.23047926e-06
Iter: 1561 loss: 1.23016366e-06
Iter: 1562 loss: 1.23004634e-06
Iter: 1563 loss: 1.22987899e-06
Iter: 1564 loss: 1.22952213e-06
Iter: 1565 loss: 1.22894426e-06
Iter: 1566 loss: 1.22890719e-06
Iter: 1567 loss: 1.22844108e-06
Iter: 1568 loss: 1.22844153e-06
Iter: 1569 loss: 1.2279703e-06
Iter: 1570 loss: 1.23145537e-06
Iter: 1571 loss: 1.22795916e-06
Iter: 1572 loss: 1.22767506e-06
Iter: 1573 loss: 1.2277975e-06
Iter: 1574 loss: 1.22745769e-06
Iter: 1575 loss: 1.22708116e-06
Iter: 1576 loss: 1.22864901e-06
Iter: 1577 loss: 1.22701499e-06
Iter: 1578 loss: 1.22660015e-06
Iter: 1579 loss: 1.22737083e-06
Iter: 1580 loss: 1.2264527e-06
Iter: 1581 loss: 1.22613358e-06
Iter: 1582 loss: 1.22567644e-06
Iter: 1583 loss: 1.22564802e-06
Iter: 1584 loss: 1.22534288e-06
Iter: 1585 loss: 1.22533629e-06
Iter: 1586 loss: 1.22500433e-06
Iter: 1587 loss: 1.22565984e-06
Iter: 1588 loss: 1.22489291e-06
Iter: 1589 loss: 1.22455629e-06
Iter: 1590 loss: 1.22396523e-06
Iter: 1591 loss: 1.23766768e-06
Iter: 1592 loss: 1.22394545e-06
Iter: 1593 loss: 1.22359643e-06
Iter: 1594 loss: 1.2235497e-06
Iter: 1595 loss: 1.22314782e-06
Iter: 1596 loss: 1.22281199e-06
Iter: 1597 loss: 1.22270717e-06
Iter: 1598 loss: 1.22222639e-06
Iter: 1599 loss: 1.22222343e-06
Iter: 1600 loss: 1.22184485e-06
Iter: 1601 loss: 1.22140705e-06
Iter: 1602 loss: 1.22400991e-06
Iter: 1603 loss: 1.22133122e-06
Iter: 1604 loss: 1.22099789e-06
Iter: 1605 loss: 1.22102051e-06
Iter: 1606 loss: 1.22076153e-06
Iter: 1607 loss: 1.22039592e-06
Iter: 1608 loss: 1.22035203e-06
Iter: 1609 loss: 1.2200851e-06
Iter: 1610 loss: 1.22007862e-06
Iter: 1611 loss: 1.21982134e-06
Iter: 1612 loss: 1.21949518e-06
Iter: 1613 loss: 1.21947437e-06
Iter: 1614 loss: 1.21895653e-06
Iter: 1615 loss: 1.22042911e-06
Iter: 1616 loss: 1.21880578e-06
Iter: 1617 loss: 1.2184563e-06
Iter: 1618 loss: 1.2194032e-06
Iter: 1619 loss: 1.21832807e-06
Iter: 1620 loss: 1.21805169e-06
Iter: 1621 loss: 1.2180418e-06
Iter: 1622 loss: 1.21788696e-06
Iter: 1623 loss: 1.21742482e-06
Iter: 1624 loss: 1.21977268e-06
Iter: 1625 loss: 1.21727135e-06
Iter: 1626 loss: 1.2167651e-06
Iter: 1627 loss: 1.22062738e-06
Iter: 1628 loss: 1.21675384e-06
Iter: 1629 loss: 1.21638459e-06
Iter: 1630 loss: 1.22154461e-06
Iter: 1631 loss: 1.21636799e-06
Iter: 1632 loss: 1.21609105e-06
Iter: 1633 loss: 1.21563721e-06
Iter: 1634 loss: 1.21562368e-06
Iter: 1635 loss: 1.21506832e-06
Iter: 1636 loss: 1.21526955e-06
Iter: 1637 loss: 1.21471146e-06
Iter: 1638 loss: 1.21476432e-06
Iter: 1639 loss: 1.21450012e-06
Iter: 1640 loss: 1.21425933e-06
Iter: 1641 loss: 1.2138828e-06
Iter: 1642 loss: 1.21389144e-06
Iter: 1643 loss: 1.21356175e-06
Iter: 1644 loss: 1.21478615e-06
Iter: 1645 loss: 1.21349422e-06
Iter: 1646 loss: 1.213112e-06
Iter: 1647 loss: 1.21552262e-06
Iter: 1648 loss: 1.21306562e-06
Iter: 1649 loss: 1.21280311e-06
Iter: 1650 loss: 1.21229778e-06
Iter: 1651 loss: 1.22211736e-06
Iter: 1652 loss: 1.21232176e-06
Iter: 1653 loss: 1.21177482e-06
Iter: 1654 loss: 1.2155715e-06
Iter: 1655 loss: 1.21170854e-06
Iter: 1656 loss: 1.21125152e-06
Iter: 1657 loss: 1.21432072e-06
Iter: 1658 loss: 1.2111941e-06
Iter: 1659 loss: 1.21083599e-06
Iter: 1660 loss: 1.21189385e-06
Iter: 1661 loss: 1.21069661e-06
Iter: 1662 loss: 1.21039534e-06
Iter: 1663 loss: 1.2101259e-06
Iter: 1664 loss: 1.2100254e-06
Iter: 1665 loss: 1.20971481e-06
Iter: 1666 loss: 1.21312121e-06
Iter: 1667 loss: 1.20970913e-06
Iter: 1668 loss: 1.20933964e-06
Iter: 1669 loss: 1.20995264e-06
Iter: 1670 loss: 1.20917366e-06
Iter: 1671 loss: 1.2089256e-06
Iter: 1672 loss: 1.20840446e-06
Iter: 1673 loss: 1.20839604e-06
Iter: 1674 loss: 1.20782829e-06
Iter: 1675 loss: 1.21275616e-06
Iter: 1676 loss: 1.20780271e-06
Iter: 1677 loss: 1.2073624e-06
Iter: 1678 loss: 1.21423511e-06
Iter: 1679 loss: 1.20736831e-06
Iter: 1680 loss: 1.20717357e-06
Iter: 1681 loss: 1.2067452e-06
Iter: 1682 loss: 1.21619473e-06
Iter: 1683 loss: 1.2067303e-06
Iter: 1684 loss: 1.20656091e-06
Iter: 1685 loss: 1.20653522e-06
Iter: 1686 loss: 1.20631807e-06
Iter: 1687 loss: 1.20579148e-06
Iter: 1688 loss: 1.21032338e-06
Iter: 1689 loss: 1.20567631e-06
Iter: 1690 loss: 1.20536151e-06
Iter: 1691 loss: 1.20996265e-06
Iter: 1692 loss: 1.20536652e-06
Iter: 1693 loss: 1.20497225e-06
Iter: 1694 loss: 1.20549271e-06
Iter: 1695 loss: 1.2047874e-06
Iter: 1696 loss: 1.20429149e-06
Iter: 1697 loss: 1.20502295e-06
Iter: 1698 loss: 1.2040872e-06
Iter: 1699 loss: 1.2036553e-06
Iter: 1700 loss: 1.20426898e-06
Iter: 1701 loss: 1.20345294e-06
Iter: 1702 loss: 1.20312507e-06
Iter: 1703 loss: 1.20597656e-06
Iter: 1704 loss: 1.2030938e-06
Iter: 1705 loss: 1.20272375e-06
Iter: 1706 loss: 1.20314587e-06
Iter: 1707 loss: 1.20255288e-06
Iter: 1708 loss: 1.20222512e-06
Iter: 1709 loss: 1.20224593e-06
Iter: 1710 loss: 1.2019575e-06
Iter: 1711 loss: 1.20161098e-06
Iter: 1712 loss: 1.20171353e-06
Iter: 1713 loss: 1.20133836e-06
Iter: 1714 loss: 1.20130494e-06
Iter: 1715 loss: 1.20104073e-06
Iter: 1716 loss: 1.20088623e-06
Iter: 1717 loss: 1.20038283e-06
Iter: 1718 loss: 1.20380344e-06
Iter: 1719 loss: 1.20025993e-06
Iter: 1720 loss: 1.20000072e-06
Iter: 1721 loss: 1.19993206e-06
Iter: 1722 loss: 1.19960566e-06
Iter: 1723 loss: 1.19926494e-06
Iter: 1724 loss: 1.19919582e-06
Iter: 1725 loss: 1.19879769e-06
Iter: 1726 loss: 1.1997696e-06
Iter: 1727 loss: 1.19869696e-06
Iter: 1728 loss: 1.19832634e-06
Iter: 1729 loss: 1.20218397e-06
Iter: 1730 loss: 1.19835022e-06
Iter: 1731 loss: 1.19812285e-06
Iter: 1732 loss: 1.19775541e-06
Iter: 1733 loss: 1.1977744e-06
Iter: 1734 loss: 1.19743618e-06
Iter: 1735 loss: 1.20021491e-06
Iter: 1736 loss: 1.19742083e-06
Iter: 1737 loss: 1.19709648e-06
Iter: 1738 loss: 1.19670335e-06
Iter: 1739 loss: 1.19666788e-06
Iter: 1740 loss: 1.1962901e-06
Iter: 1741 loss: 1.19627657e-06
Iter: 1742 loss: 1.19597314e-06
Iter: 1743 loss: 1.19563958e-06
Iter: 1744 loss: 1.19557558e-06
Iter: 1745 loss: 1.19516676e-06
Iter: 1746 loss: 1.19624178e-06
Iter: 1747 loss: 1.1949852e-06
Iter: 1748 loss: 1.19492506e-06
Iter: 1749 loss: 1.19478204e-06
Iter: 1750 loss: 1.19463948e-06
Iter: 1751 loss: 1.1943182e-06
Iter: 1752 loss: 1.20111667e-06
Iter: 1753 loss: 1.19432525e-06
Iter: 1754 loss: 1.19397828e-06
Iter: 1755 loss: 1.19484434e-06
Iter: 1756 loss: 1.19388631e-06
Iter: 1757 loss: 1.1933937e-06
Iter: 1758 loss: 1.19463596e-06
Iter: 1759 loss: 1.19320885e-06
Iter: 1760 loss: 1.19293622e-06
Iter: 1761 loss: 1.19264473e-06
Iter: 1762 loss: 1.19261608e-06
Iter: 1763 loss: 1.19235335e-06
Iter: 1764 loss: 1.19232254e-06
Iter: 1765 loss: 1.19213587e-06
Iter: 1766 loss: 1.19185063e-06
Iter: 1767 loss: 1.1918429e-06
Iter: 1768 loss: 1.19149763e-06
Iter: 1769 loss: 1.19341075e-06
Iter: 1770 loss: 1.19147489e-06
Iter: 1771 loss: 1.19117749e-06
Iter: 1772 loss: 1.19165202e-06
Iter: 1773 loss: 1.19105698e-06
Iter: 1774 loss: 1.19076321e-06
Iter: 1775 loss: 1.19167657e-06
Iter: 1776 loss: 1.19063918e-06
Iter: 1777 loss: 1.19028482e-06
Iter: 1778 loss: 1.19030608e-06
Iter: 1779 loss: 1.18997775e-06
Iter: 1780 loss: 1.1895695e-06
Iter: 1781 loss: 1.18945104e-06
Iter: 1782 loss: 1.18913636e-06
Iter: 1783 loss: 1.18905314e-06
Iter: 1784 loss: 1.18884918e-06
Iter: 1785 loss: 1.18865592e-06
Iter: 1786 loss: 1.18824119e-06
Iter: 1787 loss: 1.19339711e-06
Iter: 1788 loss: 1.18820537e-06
Iter: 1789 loss: 1.18800847e-06
Iter: 1790 loss: 1.18798982e-06
Iter: 1791 loss: 1.18777427e-06
Iter: 1792 loss: 1.18757293e-06
Iter: 1793 loss: 1.18755406e-06
Iter: 1794 loss: 1.18724279e-06
Iter: 1795 loss: 1.18713729e-06
Iter: 1796 loss: 1.18697915e-06
Iter: 1797 loss: 1.18659977e-06
Iter: 1798 loss: 1.18658954e-06
Iter: 1799 loss: 1.18635853e-06
Iter: 1800 loss: 1.18588162e-06
Iter: 1801 loss: 1.19511299e-06
Iter: 1802 loss: 1.18586672e-06
Iter: 1803 loss: 1.18546723e-06
Iter: 1804 loss: 1.1899657e-06
Iter: 1805 loss: 1.18546109e-06
Iter: 1806 loss: 1.18505818e-06
Iter: 1807 loss: 1.18632875e-06
Iter: 1808 loss: 1.18492267e-06
Iter: 1809 loss: 1.18461344e-06
Iter: 1810 loss: 1.18588753e-06
Iter: 1811 loss: 1.18454705e-06
Iter: 1812 loss: 1.1843199e-06
Iter: 1813 loss: 1.18421406e-06
Iter: 1814 loss: 1.18404421e-06
Iter: 1815 loss: 1.18373089e-06
Iter: 1816 loss: 1.18581465e-06
Iter: 1817 loss: 1.18367768e-06
Iter: 1818 loss: 1.18335629e-06
Iter: 1819 loss: 1.18656158e-06
Iter: 1820 loss: 1.18334151e-06
Iter: 1821 loss: 1.18314847e-06
Iter: 1822 loss: 1.18266246e-06
Iter: 1823 loss: 1.18700768e-06
Iter: 1824 loss: 1.18257844e-06
Iter: 1825 loss: 1.18228854e-06
Iter: 1826 loss: 1.1822226e-06
Iter: 1827 loss: 1.1819709e-06
Iter: 1828 loss: 1.18135597e-06
Iter: 1829 loss: 1.18674916e-06
Iter: 1830 loss: 1.18128878e-06
Iter: 1831 loss: 1.18093578e-06
Iter: 1832 loss: 1.18090225e-06
Iter: 1833 loss: 1.18055755e-06
Iter: 1834 loss: 1.18137814e-06
Iter: 1835 loss: 1.18043545e-06
Iter: 1836 loss: 1.18015828e-06
Iter: 1837 loss: 1.1798661e-06
Iter: 1838 loss: 1.17982574e-06
Iter: 1839 loss: 1.17951731e-06
Iter: 1840 loss: 1.17950674e-06
Iter: 1841 loss: 1.17920115e-06
Iter: 1842 loss: 1.17907507e-06
Iter: 1843 loss: 1.17892523e-06
Iter: 1844 loss: 1.17850755e-06
Iter: 1845 loss: 1.18026605e-06
Iter: 1846 loss: 1.17840079e-06
Iter: 1847 loss: 1.1780262e-06
Iter: 1848 loss: 1.17801e-06
Iter: 1849 loss: 1.17773516e-06
Iter: 1850 loss: 1.17759703e-06
Iter: 1851 loss: 1.17753461e-06
Iter: 1852 loss: 1.17731918e-06
Iter: 1853 loss: 1.17703064e-06
Iter: 1854 loss: 1.17701086e-06
Iter: 1855 loss: 1.17676e-06
Iter: 1856 loss: 1.17878199e-06
Iter: 1857 loss: 1.17672073e-06
Iter: 1858 loss: 1.17642503e-06
Iter: 1859 loss: 1.17664422e-06
Iter: 1860 loss: 1.17626564e-06
Iter: 1861 loss: 1.17597688e-06
Iter: 1862 loss: 1.17547847e-06
Iter: 1863 loss: 1.17548007e-06
Iter: 1864 loss: 1.17537797e-06
Iter: 1865 loss: 1.17522382e-06
Iter: 1866 loss: 1.17496882e-06
Iter: 1867 loss: 1.17463026e-06
Iter: 1868 loss: 1.17460615e-06
Iter: 1869 loss: 1.17421678e-06
Iter: 1870 loss: 1.17474121e-06
Iter: 1871 loss: 1.17404511e-06
Iter: 1872 loss: 1.17365676e-06
Iter: 1873 loss: 1.17806007e-06
Iter: 1874 loss: 1.17366324e-06
Iter: 1875 loss: 1.17338732e-06
Iter: 1876 loss: 1.17310196e-06
Iter: 1877 loss: 1.17306047e-06
Iter: 1878 loss: 1.17264563e-06
Iter: 1879 loss: 1.17519801e-06
Iter: 1880 loss: 1.17259788e-06
Iter: 1881 loss: 1.1722608e-06
Iter: 1882 loss: 1.17242234e-06
Iter: 1883 loss: 1.17202762e-06
Iter: 1884 loss: 1.17166417e-06
Iter: 1885 loss: 1.17551315e-06
Iter: 1886 loss: 1.17167019e-06
Iter: 1887 loss: 1.17126069e-06
Iter: 1888 loss: 1.17125592e-06
Iter: 1889 loss: 1.17095124e-06
Iter: 1890 loss: 1.17064747e-06
Iter: 1891 loss: 1.17121635e-06
Iter: 1892 loss: 1.17051854e-06
Iter: 1893 loss: 1.17014088e-06
Iter: 1894 loss: 1.17298941e-06
Iter: 1895 loss: 1.17011678e-06
Iter: 1896 loss: 1.16994818e-06
Iter: 1897 loss: 1.16949741e-06
Iter: 1898 loss: 1.17267814e-06
Iter: 1899 loss: 1.16941021e-06
Iter: 1900 loss: 1.16918966e-06
Iter: 1901 loss: 1.16912679e-06
Iter: 1902 loss: 1.16882734e-06
Iter: 1903 loss: 1.16954357e-06
Iter: 1904 loss: 1.16875617e-06
Iter: 1905 loss: 1.16852527e-06
Iter: 1906 loss: 1.16810452e-06
Iter: 1907 loss: 1.16810395e-06
Iter: 1908 loss: 1.16779427e-06
Iter: 1909 loss: 1.16778483e-06
Iter: 1910 loss: 1.1674739e-06
Iter: 1911 loss: 1.16700585e-06
Iter: 1912 loss: 1.16697856e-06
Iter: 1913 loss: 1.16660476e-06
Iter: 1914 loss: 1.16941931e-06
Iter: 1915 loss: 1.16655087e-06
Iter: 1916 loss: 1.16620276e-06
Iter: 1917 loss: 1.16741774e-06
Iter: 1918 loss: 1.1660909e-06
Iter: 1919 loss: 1.16591536e-06
Iter: 1920 loss: 1.16591e-06
Iter: 1921 loss: 1.16574847e-06
Iter: 1922 loss: 1.16535671e-06
Iter: 1923 loss: 1.17018044e-06
Iter: 1924 loss: 1.16531214e-06
Iter: 1925 loss: 1.16509534e-06
Iter: 1926 loss: 1.16507385e-06
Iter: 1927 loss: 1.16488241e-06
Iter: 1928 loss: 1.16436559e-06
Iter: 1929 loss: 1.16872116e-06
Iter: 1930 loss: 1.16434217e-06
Iter: 1931 loss: 1.16381216e-06
Iter: 1932 loss: 1.16609328e-06
Iter: 1933 loss: 1.16371382e-06
Iter: 1934 loss: 1.16338515e-06
Iter: 1935 loss: 1.16628576e-06
Iter: 1936 loss: 1.16336457e-06
Iter: 1937 loss: 1.16302897e-06
Iter: 1938 loss: 1.16398337e-06
Iter: 1939 loss: 1.16292927e-06
Iter: 1940 loss: 1.16270826e-06
Iter: 1941 loss: 1.16244087e-06
Iter: 1942 loss: 1.16240744e-06
Iter: 1943 loss: 1.162128e-06
Iter: 1944 loss: 1.16212527e-06
Iter: 1945 loss: 1.16187152e-06
Iter: 1946 loss: 1.16146305e-06
Iter: 1947 loss: 1.16146816e-06
Iter: 1948 loss: 1.16096589e-06
Iter: 1949 loss: 1.16174488e-06
Iter: 1950 loss: 1.16070873e-06
Iter: 1951 loss: 1.1603596e-06
Iter: 1952 loss: 1.16514275e-06
Iter: 1953 loss: 1.16035449e-06
Iter: 1954 loss: 1.16005072e-06
Iter: 1955 loss: 1.16189017e-06
Iter: 1956 loss: 1.16001524e-06
Iter: 1957 loss: 1.15974467e-06
Iter: 1958 loss: 1.16010676e-06
Iter: 1959 loss: 1.15961075e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6
+ date
Mon Oct 26 09:53:18 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.6_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.6_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.6_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.6_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2860254f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f286029f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f286029f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28602cc268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f286020d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2860206b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2860206510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f286017e048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2860206ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2860157c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28600c7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28600d7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28600de400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28600de840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2860093c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2860093b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f286004b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2840722b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28406e17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28406ecf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28406ef840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28406ef158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28406838c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2840683620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2840632510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2840642620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f284060aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28405a4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28405a40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28405a47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f284057c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f284051e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f284051eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28404e5ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f284050fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f284050fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.26378943e-05
Iter: 2 loss: 1.85454646e-05
Iter: 3 loss: 1.85213921e-05
Iter: 4 loss: 1.72526488e-05
Iter: 5 loss: 3.19875508e-05
Iter: 6 loss: 1.72328691e-05
Iter: 7 loss: 1.65426773e-05
Iter: 8 loss: 1.58416733e-05
Iter: 9 loss: 1.57061404e-05
Iter: 10 loss: 1.44572059e-05
Iter: 11 loss: 1.86994039e-05
Iter: 12 loss: 1.41203382e-05
Iter: 13 loss: 1.29386408e-05
Iter: 14 loss: 1.54490626e-05
Iter: 15 loss: 1.24754079e-05
Iter: 16 loss: 1.13911756e-05
Iter: 17 loss: 1.57586073e-05
Iter: 18 loss: 1.11496602e-05
Iter: 19 loss: 1.0206385e-05
Iter: 20 loss: 1.24586177e-05
Iter: 21 loss: 9.86197665e-06
Iter: 22 loss: 9.32375588e-06
Iter: 23 loss: 9.87870771e-06
Iter: 24 loss: 9.02466854e-06
Iter: 25 loss: 8.65574475e-06
Iter: 26 loss: 8.61900298e-06
Iter: 27 loss: 8.43774797e-06
Iter: 28 loss: 8.0128093e-06
Iter: 29 loss: 1.30828448e-05
Iter: 30 loss: 7.97698067e-06
Iter: 31 loss: 7.44952604e-06
Iter: 32 loss: 9.18596e-06
Iter: 33 loss: 7.30361035e-06
Iter: 34 loss: 6.91414607e-06
Iter: 35 loss: 8.28990505e-06
Iter: 36 loss: 6.81364691e-06
Iter: 37 loss: 6.42089435e-06
Iter: 38 loss: 7.1400973e-06
Iter: 39 loss: 6.25190114e-06
Iter: 40 loss: 6.04033312e-06
Iter: 41 loss: 6.02089267e-06
Iter: 42 loss: 5.81648874e-06
Iter: 43 loss: 6.86699195e-06
Iter: 44 loss: 5.78350728e-06
Iter: 45 loss: 5.6526942e-06
Iter: 46 loss: 5.7357438e-06
Iter: 47 loss: 5.5693572e-06
Iter: 48 loss: 5.40332894e-06
Iter: 49 loss: 5.91511161e-06
Iter: 50 loss: 5.35441268e-06
Iter: 51 loss: 5.20773665e-06
Iter: 52 loss: 5.66817e-06
Iter: 53 loss: 5.16511e-06
Iter: 54 loss: 5.02400144e-06
Iter: 55 loss: 5.29647878e-06
Iter: 56 loss: 4.96516714e-06
Iter: 57 loss: 4.81865754e-06
Iter: 58 loss: 5.15005604e-06
Iter: 59 loss: 4.7634303e-06
Iter: 60 loss: 4.61850641e-06
Iter: 61 loss: 4.56455382e-06
Iter: 62 loss: 4.48452693e-06
Iter: 63 loss: 4.42770124e-06
Iter: 64 loss: 4.39661744e-06
Iter: 65 loss: 4.32377374e-06
Iter: 66 loss: 4.19147818e-06
Iter: 67 loss: 7.3432675e-06
Iter: 68 loss: 4.1914127e-06
Iter: 69 loss: 4.08306505e-06
Iter: 70 loss: 4.53687244e-06
Iter: 71 loss: 4.06004756e-06
Iter: 72 loss: 3.95224743e-06
Iter: 73 loss: 3.94518793e-06
Iter: 74 loss: 3.86377224e-06
Iter: 75 loss: 3.74734304e-06
Iter: 76 loss: 4.30473801e-06
Iter: 77 loss: 3.72641739e-06
Iter: 78 loss: 3.72688555e-06
Iter: 79 loss: 3.67956318e-06
Iter: 80 loss: 3.64445532e-06
Iter: 81 loss: 3.59861951e-06
Iter: 82 loss: 3.59566957e-06
Iter: 83 loss: 3.53991368e-06
Iter: 84 loss: 3.74810043e-06
Iter: 85 loss: 3.52634811e-06
Iter: 86 loss: 3.4630234e-06
Iter: 87 loss: 3.59343267e-06
Iter: 88 loss: 3.43775559e-06
Iter: 89 loss: 3.39334065e-06
Iter: 90 loss: 3.67343318e-06
Iter: 91 loss: 3.38827886e-06
Iter: 92 loss: 3.34791844e-06
Iter: 93 loss: 3.33993648e-06
Iter: 94 loss: 3.31309343e-06
Iter: 95 loss: 3.26628697e-06
Iter: 96 loss: 3.54015128e-06
Iter: 97 loss: 3.26020927e-06
Iter: 98 loss: 3.22060259e-06
Iter: 99 loss: 3.19512469e-06
Iter: 100 loss: 3.17965964e-06
Iter: 101 loss: 3.1307975e-06
Iter: 102 loss: 3.13018427e-06
Iter: 103 loss: 3.10453652e-06
Iter: 104 loss: 3.06026209e-06
Iter: 105 loss: 3.06020547e-06
Iter: 106 loss: 3.01153341e-06
Iter: 107 loss: 3.16891146e-06
Iter: 108 loss: 2.99784756e-06
Iter: 109 loss: 2.95024324e-06
Iter: 110 loss: 2.96572102e-06
Iter: 111 loss: 2.91646461e-06
Iter: 112 loss: 2.86707154e-06
Iter: 113 loss: 3.10966493e-06
Iter: 114 loss: 2.85863985e-06
Iter: 115 loss: 2.85332317e-06
Iter: 116 loss: 2.84122393e-06
Iter: 117 loss: 2.8226e-06
Iter: 118 loss: 2.81074836e-06
Iter: 119 loss: 2.80327572e-06
Iter: 120 loss: 2.7829642e-06
Iter: 121 loss: 2.76542619e-06
Iter: 122 loss: 2.75997809e-06
Iter: 123 loss: 2.73403361e-06
Iter: 124 loss: 3.14797603e-06
Iter: 125 loss: 2.73404657e-06
Iter: 126 loss: 2.70995361e-06
Iter: 127 loss: 2.72991906e-06
Iter: 128 loss: 2.69563247e-06
Iter: 129 loss: 2.67305859e-06
Iter: 130 loss: 2.73221031e-06
Iter: 131 loss: 2.66541088e-06
Iter: 132 loss: 2.64093433e-06
Iter: 133 loss: 2.72629586e-06
Iter: 134 loss: 2.63450875e-06
Iter: 135 loss: 2.61761261e-06
Iter: 136 loss: 2.65931158e-06
Iter: 137 loss: 2.61155355e-06
Iter: 138 loss: 2.59023182e-06
Iter: 139 loss: 2.57087981e-06
Iter: 140 loss: 2.56564272e-06
Iter: 141 loss: 2.5428576e-06
Iter: 142 loss: 2.7767046e-06
Iter: 143 loss: 2.54219412e-06
Iter: 144 loss: 2.52447e-06
Iter: 145 loss: 2.67778e-06
Iter: 146 loss: 2.52353357e-06
Iter: 147 loss: 2.50864537e-06
Iter: 148 loss: 2.49117147e-06
Iter: 149 loss: 2.48917695e-06
Iter: 150 loss: 2.46876903e-06
Iter: 151 loss: 2.46920581e-06
Iter: 152 loss: 2.45248043e-06
Iter: 153 loss: 2.43475483e-06
Iter: 154 loss: 2.43469e-06
Iter: 155 loss: 2.42284432e-06
Iter: 156 loss: 2.42279657e-06
Iter: 157 loss: 2.41605244e-06
Iter: 158 loss: 2.4015585e-06
Iter: 159 loss: 2.6273915e-06
Iter: 160 loss: 2.40103964e-06
Iter: 161 loss: 2.3864377e-06
Iter: 162 loss: 2.37126937e-06
Iter: 163 loss: 2.36858546e-06
Iter: 164 loss: 2.35796688e-06
Iter: 165 loss: 2.35390098e-06
Iter: 166 loss: 2.3400296e-06
Iter: 167 loss: 2.34255572e-06
Iter: 168 loss: 2.32958291e-06
Iter: 169 loss: 2.3155219e-06
Iter: 170 loss: 2.36273854e-06
Iter: 171 loss: 2.31173226e-06
Iter: 172 loss: 2.29970897e-06
Iter: 173 loss: 2.35671541e-06
Iter: 174 loss: 2.29753778e-06
Iter: 175 loss: 2.28623207e-06
Iter: 176 loss: 2.28725457e-06
Iter: 177 loss: 2.27750525e-06
Iter: 178 loss: 2.26440125e-06
Iter: 179 loss: 2.34108848e-06
Iter: 180 loss: 2.26272368e-06
Iter: 181 loss: 2.25189933e-06
Iter: 182 loss: 2.23402299e-06
Iter: 183 loss: 2.23395455e-06
Iter: 184 loss: 2.23579809e-06
Iter: 185 loss: 2.22530707e-06
Iter: 186 loss: 2.21886899e-06
Iter: 187 loss: 2.20548e-06
Iter: 188 loss: 2.43684053e-06
Iter: 189 loss: 2.20516881e-06
Iter: 190 loss: 2.19191361e-06
Iter: 191 loss: 2.249712e-06
Iter: 192 loss: 2.18925197e-06
Iter: 193 loss: 2.18526907e-06
Iter: 194 loss: 2.18303421e-06
Iter: 195 loss: 2.17844513e-06
Iter: 196 loss: 2.1669639e-06
Iter: 197 loss: 2.27458031e-06
Iter: 198 loss: 2.16534499e-06
Iter: 199 loss: 2.15335422e-06
Iter: 200 loss: 2.16036392e-06
Iter: 201 loss: 2.1454839e-06
Iter: 202 loss: 2.13170597e-06
Iter: 203 loss: 2.18635159e-06
Iter: 204 loss: 2.12859345e-06
Iter: 205 loss: 2.11761494e-06
Iter: 206 loss: 2.20596803e-06
Iter: 207 loss: 2.11693668e-06
Iter: 208 loss: 2.10630651e-06
Iter: 209 loss: 2.1505432e-06
Iter: 210 loss: 2.10396547e-06
Iter: 211 loss: 2.09408745e-06
Iter: 212 loss: 2.11401948e-06
Iter: 213 loss: 2.08995289e-06
Iter: 214 loss: 2.08249e-06
Iter: 215 loss: 2.16836111e-06
Iter: 216 loss: 2.08233314e-06
Iter: 217 loss: 2.07694734e-06
Iter: 218 loss: 2.06890036e-06
Iter: 219 loss: 2.06868981e-06
Iter: 220 loss: 2.05738752e-06
Iter: 221 loss: 2.11032398e-06
Iter: 222 loss: 2.05526499e-06
Iter: 223 loss: 2.0484008e-06
Iter: 224 loss: 2.04638241e-06
Iter: 225 loss: 2.04223079e-06
Iter: 226 loss: 2.03481113e-06
Iter: 227 loss: 2.03446893e-06
Iter: 228 loss: 2.02976889e-06
Iter: 229 loss: 2.02123056e-06
Iter: 230 loss: 2.22502013e-06
Iter: 231 loss: 2.02119713e-06
Iter: 232 loss: 2.01645139e-06
Iter: 233 loss: 2.01591911e-06
Iter: 234 loss: 2.01028024e-06
Iter: 235 loss: 2.00867657e-06
Iter: 236 loss: 2.00517661e-06
Iter: 237 loss: 1.99859051e-06
Iter: 238 loss: 1.98668658e-06
Iter: 239 loss: 2.28000272e-06
Iter: 240 loss: 1.98670341e-06
Iter: 241 loss: 1.97765985e-06
Iter: 242 loss: 2.04218873e-06
Iter: 243 loss: 1.97682857e-06
Iter: 244 loss: 1.9697286e-06
Iter: 245 loss: 2.04597382e-06
Iter: 246 loss: 1.96945621e-06
Iter: 247 loss: 1.96371116e-06
Iter: 248 loss: 1.98219777e-06
Iter: 249 loss: 1.96201086e-06
Iter: 250 loss: 1.95750295e-06
Iter: 251 loss: 1.97113559e-06
Iter: 252 loss: 1.95621578e-06
Iter: 253 loss: 1.95030361e-06
Iter: 254 loss: 1.95136067e-06
Iter: 255 loss: 1.94588711e-06
Iter: 256 loss: 1.93972164e-06
Iter: 257 loss: 1.9511931e-06
Iter: 258 loss: 1.93710207e-06
Iter: 259 loss: 1.92955986e-06
Iter: 260 loss: 1.9430031e-06
Iter: 261 loss: 1.9262543e-06
Iter: 262 loss: 1.92094126e-06
Iter: 263 loss: 1.97386589e-06
Iter: 264 loss: 1.92073094e-06
Iter: 265 loss: 1.91548247e-06
Iter: 266 loss: 1.92503671e-06
Iter: 267 loss: 1.91322101e-06
Iter: 268 loss: 1.9083966e-06
Iter: 269 loss: 1.91547315e-06
Iter: 270 loss: 1.90603203e-06
Iter: 271 loss: 1.90418348e-06
Iter: 272 loss: 1.90352807e-06
Iter: 273 loss: 1.90165599e-06
Iter: 274 loss: 1.89570301e-06
Iter: 275 loss: 1.90115566e-06
Iter: 276 loss: 1.89077127e-06
Iter: 277 loss: 1.88307013e-06
Iter: 278 loss: 1.92898e-06
Iter: 279 loss: 1.88206309e-06
Iter: 280 loss: 1.87498199e-06
Iter: 281 loss: 1.89689649e-06
Iter: 282 loss: 1.87285104e-06
Iter: 283 loss: 1.86700208e-06
Iter: 284 loss: 1.86695547e-06
Iter: 285 loss: 1.86313923e-06
Iter: 286 loss: 1.86496368e-06
Iter: 287 loss: 1.86051125e-06
Iter: 288 loss: 1.85627823e-06
Iter: 289 loss: 1.88650142e-06
Iter: 290 loss: 1.85589829e-06
Iter: 291 loss: 1.85218471e-06
Iter: 292 loss: 1.84739667e-06
Iter: 293 loss: 1.84706687e-06
Iter: 294 loss: 1.84158034e-06
Iter: 295 loss: 1.86367697e-06
Iter: 296 loss: 1.84029079e-06
Iter: 297 loss: 1.83441e-06
Iter: 298 loss: 1.85158228e-06
Iter: 299 loss: 1.83258771e-06
Iter: 300 loss: 1.82793735e-06
Iter: 301 loss: 1.87052547e-06
Iter: 302 loss: 1.82768531e-06
Iter: 303 loss: 1.82321992e-06
Iter: 304 loss: 1.82221322e-06
Iter: 305 loss: 1.81937298e-06
Iter: 306 loss: 1.81708492e-06
Iter: 307 loss: 1.81690518e-06
Iter: 308 loss: 1.81438736e-06
Iter: 309 loss: 1.81155247e-06
Iter: 310 loss: 1.81118139e-06
Iter: 311 loss: 1.80739016e-06
Iter: 312 loss: 1.8058131e-06
Iter: 313 loss: 1.80383108e-06
Iter: 314 loss: 1.79885558e-06
Iter: 315 loss: 1.8007479e-06
Iter: 316 loss: 1.79539734e-06
Iter: 317 loss: 1.79218e-06
Iter: 318 loss: 1.79194058e-06
Iter: 319 loss: 1.78802588e-06
Iter: 320 loss: 1.78875212e-06
Iter: 321 loss: 1.78510834e-06
Iter: 322 loss: 1.78180824e-06
Iter: 323 loss: 1.81354665e-06
Iter: 324 loss: 1.78165601e-06
Iter: 325 loss: 1.77895663e-06
Iter: 326 loss: 1.77986794e-06
Iter: 327 loss: 1.77699235e-06
Iter: 328 loss: 1.77351239e-06
Iter: 329 loss: 1.77339189e-06
Iter: 330 loss: 1.77062475e-06
Iter: 331 loss: 1.76686581e-06
Iter: 332 loss: 1.79544759e-06
Iter: 333 loss: 1.76661104e-06
Iter: 334 loss: 1.76273375e-06
Iter: 335 loss: 1.7700072e-06
Iter: 336 loss: 1.76100957e-06
Iter: 337 loss: 1.75681816e-06
Iter: 338 loss: 1.7821269e-06
Iter: 339 loss: 1.75634239e-06
Iter: 340 loss: 1.75398964e-06
Iter: 341 loss: 1.76256674e-06
Iter: 342 loss: 1.75345292e-06
Iter: 343 loss: 1.75011337e-06
Iter: 344 loss: 1.74966419e-06
Iter: 345 loss: 1.74730621e-06
Iter: 346 loss: 1.74436229e-06
Iter: 347 loss: 1.74487457e-06
Iter: 348 loss: 1.74224158e-06
Iter: 349 loss: 1.7383627e-06
Iter: 350 loss: 1.73818603e-06
Iter: 351 loss: 1.73521971e-06
Iter: 352 loss: 1.73237117e-06
Iter: 353 loss: 1.73231433e-06
Iter: 354 loss: 1.72951343e-06
Iter: 355 loss: 1.73997796e-06
Iter: 356 loss: 1.72885757e-06
Iter: 357 loss: 1.72671741e-06
Iter: 358 loss: 1.72845409e-06
Iter: 359 loss: 1.72545845e-06
Iter: 360 loss: 1.7224254e-06
Iter: 361 loss: 1.73169747e-06
Iter: 362 loss: 1.72155933e-06
Iter: 363 loss: 1.71863678e-06
Iter: 364 loss: 1.71626277e-06
Iter: 365 loss: 1.71545094e-06
Iter: 366 loss: 1.71171973e-06
Iter: 367 loss: 1.72978571e-06
Iter: 368 loss: 1.71105648e-06
Iter: 369 loss: 1.70723501e-06
Iter: 370 loss: 1.73397416e-06
Iter: 371 loss: 1.70686258e-06
Iter: 372 loss: 1.70445901e-06
Iter: 373 loss: 1.71490728e-06
Iter: 374 loss: 1.70399562e-06
Iter: 375 loss: 1.70202952e-06
Iter: 376 loss: 1.70921919e-06
Iter: 377 loss: 1.70152305e-06
Iter: 378 loss: 1.69900704e-06
Iter: 379 loss: 1.70038913e-06
Iter: 380 loss: 1.69734608e-06
Iter: 381 loss: 1.69528789e-06
Iter: 382 loss: 1.69314012e-06
Iter: 383 loss: 1.69280861e-06
Iter: 384 loss: 1.68890278e-06
Iter: 385 loss: 1.69294299e-06
Iter: 386 loss: 1.68673455e-06
Iter: 387 loss: 1.68276529e-06
Iter: 388 loss: 1.71083616e-06
Iter: 389 loss: 1.68240695e-06
Iter: 390 loss: 1.67924418e-06
Iter: 391 loss: 1.71870192e-06
Iter: 392 loss: 1.67926078e-06
Iter: 393 loss: 1.67730741e-06
Iter: 394 loss: 1.67528788e-06
Iter: 395 loss: 1.67492624e-06
Iter: 396 loss: 1.67179724e-06
Iter: 397 loss: 1.70222779e-06
Iter: 398 loss: 1.67168969e-06
Iter: 399 loss: 1.66957443e-06
Iter: 400 loss: 1.66687619e-06
Iter: 401 loss: 1.66669668e-06
Iter: 402 loss: 1.66349378e-06
Iter: 403 loss: 1.68051452e-06
Iter: 404 loss: 1.66301993e-06
Iter: 405 loss: 1.66072414e-06
Iter: 406 loss: 1.68936117e-06
Iter: 407 loss: 1.66068253e-06
Iter: 408 loss: 1.65855727e-06
Iter: 409 loss: 1.65699259e-06
Iter: 410 loss: 1.65624124e-06
Iter: 411 loss: 1.65395909e-06
Iter: 412 loss: 1.65391441e-06
Iter: 413 loss: 1.65230335e-06
Iter: 414 loss: 1.65082895e-06
Iter: 415 loss: 1.65048164e-06
Iter: 416 loss: 1.64750304e-06
Iter: 417 loss: 1.64615e-06
Iter: 418 loss: 1.64464586e-06
Iter: 419 loss: 1.64180369e-06
Iter: 420 loss: 1.64366702e-06
Iter: 421 loss: 1.64001131e-06
Iter: 422 loss: 1.63642812e-06
Iter: 423 loss: 1.66943903e-06
Iter: 424 loss: 1.63622417e-06
Iter: 425 loss: 1.63437846e-06
Iter: 426 loss: 1.63436926e-06
Iter: 427 loss: 1.63299433e-06
Iter: 428 loss: 1.63073742e-06
Iter: 429 loss: 1.63071661e-06
Iter: 430 loss: 1.62872379e-06
Iter: 431 loss: 1.65696611e-06
Iter: 432 loss: 1.62867559e-06
Iter: 433 loss: 1.62680089e-06
Iter: 434 loss: 1.62274864e-06
Iter: 435 loss: 1.68717213e-06
Iter: 436 loss: 1.62263564e-06
Iter: 437 loss: 1.61913636e-06
Iter: 438 loss: 1.64771347e-06
Iter: 439 loss: 1.6189399e-06
Iter: 440 loss: 1.61627713e-06
Iter: 441 loss: 1.6428138e-06
Iter: 442 loss: 1.61614707e-06
Iter: 443 loss: 1.61388959e-06
Iter: 444 loss: 1.61615617e-06
Iter: 445 loss: 1.61257901e-06
Iter: 446 loss: 1.61080686e-06
Iter: 447 loss: 1.61079799e-06
Iter: 448 loss: 1.6095762e-06
Iter: 449 loss: 1.60705713e-06
Iter: 450 loss: 1.64986909e-06
Iter: 451 loss: 1.60700415e-06
Iter: 452 loss: 1.60402226e-06
Iter: 453 loss: 1.61584944e-06
Iter: 454 loss: 1.60338573e-06
Iter: 455 loss: 1.60087552e-06
Iter: 456 loss: 1.60000934e-06
Iter: 457 loss: 1.59860224e-06
Iter: 458 loss: 1.59617377e-06
Iter: 459 loss: 1.62004653e-06
Iter: 460 loss: 1.59602564e-06
Iter: 461 loss: 1.59390606e-06
Iter: 462 loss: 1.6132858e-06
Iter: 463 loss: 1.59381113e-06
Iter: 464 loss: 1.59200465e-06
Iter: 465 loss: 1.58994499e-06
Iter: 466 loss: 1.5897001e-06
Iter: 467 loss: 1.5878411e-06
Iter: 468 loss: 1.6153615e-06
Iter: 469 loss: 1.58784792e-06
Iter: 470 loss: 1.58618013e-06
Iter: 471 loss: 1.58441389e-06
Iter: 472 loss: 1.58409375e-06
Iter: 473 loss: 1.58140767e-06
Iter: 474 loss: 1.58606667e-06
Iter: 475 loss: 1.58029e-06
Iter: 476 loss: 1.57891986e-06
Iter: 477 loss: 1.57866975e-06
Iter: 478 loss: 1.57741488e-06
Iter: 479 loss: 1.57663203e-06
Iter: 480 loss: 1.57614795e-06
Iter: 481 loss: 1.57407078e-06
Iter: 482 loss: 1.58976945e-06
Iter: 483 loss: 1.57389741e-06
Iter: 484 loss: 1.5723524e-06
Iter: 485 loss: 1.56955457e-06
Iter: 486 loss: 1.63798882e-06
Iter: 487 loss: 1.56955775e-06
Iter: 488 loss: 1.56655415e-06
Iter: 489 loss: 1.57804254e-06
Iter: 490 loss: 1.56582598e-06
Iter: 491 loss: 1.56315832e-06
Iter: 492 loss: 1.57271052e-06
Iter: 493 loss: 1.56250121e-06
Iter: 494 loss: 1.56046735e-06
Iter: 495 loss: 1.56305327e-06
Iter: 496 loss: 1.55945088e-06
Iter: 497 loss: 1.55816292e-06
Iter: 498 loss: 1.55797579e-06
Iter: 499 loss: 1.55703651e-06
Iter: 500 loss: 1.55509804e-06
Iter: 501 loss: 1.59086608e-06
Iter: 502 loss: 1.55504938e-06
Iter: 503 loss: 1.5525269e-06
Iter: 504 loss: 1.56202987e-06
Iter: 505 loss: 1.55184375e-06
Iter: 506 loss: 1.54932741e-06
Iter: 507 loss: 1.56369674e-06
Iter: 508 loss: 1.54893155e-06
Iter: 509 loss: 1.54744816e-06
Iter: 510 loss: 1.54463646e-06
Iter: 511 loss: 1.60671925e-06
Iter: 512 loss: 1.54462737e-06
Iter: 513 loss: 1.54356849e-06
Iter: 514 loss: 1.54278177e-06
Iter: 515 loss: 1.54149802e-06
Iter: 516 loss: 1.54090662e-06
Iter: 517 loss: 1.54025088e-06
Iter: 518 loss: 1.53830115e-06
Iter: 519 loss: 1.55371822e-06
Iter: 520 loss: 1.5382135e-06
Iter: 521 loss: 1.53719964e-06
Iter: 522 loss: 1.53596761e-06
Iter: 523 loss: 1.53586586e-06
Iter: 524 loss: 1.53403721e-06
Iter: 525 loss: 1.53558699e-06
Iter: 526 loss: 1.53293706e-06
Iter: 527 loss: 1.53088774e-06
Iter: 528 loss: 1.54515646e-06
Iter: 529 loss: 1.53073165e-06
Iter: 530 loss: 1.52899349e-06
Iter: 531 loss: 1.52753034e-06
Iter: 532 loss: 1.52699283e-06
Iter: 533 loss: 1.52544817e-06
Iter: 534 loss: 1.52510188e-06
Iter: 535 loss: 1.52424298e-06
Iter: 536 loss: 1.52229541e-06
Iter: 537 loss: 1.55383657e-06
Iter: 538 loss: 1.52225289e-06
Iter: 539 loss: 1.52027258e-06
Iter: 540 loss: 1.54159761e-06
Iter: 541 loss: 1.52027019e-06
Iter: 542 loss: 1.51852805e-06
Iter: 543 loss: 1.51970789e-06
Iter: 544 loss: 1.51750828e-06
Iter: 545 loss: 1.51567326e-06
Iter: 546 loss: 1.51777385e-06
Iter: 547 loss: 1.51472307e-06
Iter: 548 loss: 1.51399581e-06
Iter: 549 loss: 1.51371114e-06
Iter: 550 loss: 1.51287986e-06
Iter: 551 loss: 1.51140716e-06
Iter: 552 loss: 1.51141148e-06
Iter: 553 loss: 1.50931464e-06
Iter: 554 loss: 1.523126e-06
Iter: 555 loss: 1.50914275e-06
Iter: 556 loss: 1.50797803e-06
Iter: 557 loss: 1.50612993e-06
Iter: 558 loss: 1.50611254e-06
Iter: 559 loss: 1.5039285e-06
Iter: 560 loss: 1.51038284e-06
Iter: 561 loss: 1.50332437e-06
Iter: 562 loss: 1.5010304e-06
Iter: 563 loss: 1.51126403e-06
Iter: 564 loss: 1.50052119e-06
Iter: 565 loss: 1.49905532e-06
Iter: 566 loss: 1.50744609e-06
Iter: 567 loss: 1.49885545e-06
Iter: 568 loss: 1.49736968e-06
Iter: 569 loss: 1.50467849e-06
Iter: 570 loss: 1.49714037e-06
Iter: 571 loss: 1.49577079e-06
Iter: 572 loss: 1.49432253e-06
Iter: 573 loss: 1.49406583e-06
Iter: 574 loss: 1.49250707e-06
Iter: 575 loss: 1.50409892e-06
Iter: 576 loss: 1.49236143e-06
Iter: 577 loss: 1.49047071e-06
Iter: 578 loss: 1.48998265e-06
Iter: 579 loss: 1.48881225e-06
Iter: 580 loss: 1.48716856e-06
Iter: 581 loss: 1.49408027e-06
Iter: 582 loss: 1.48683421e-06
Iter: 583 loss: 1.48512095e-06
Iter: 584 loss: 1.50203141e-06
Iter: 585 loss: 1.48511253e-06
Iter: 586 loss: 1.4840316e-06
Iter: 587 loss: 1.48445429e-06
Iter: 588 loss: 1.48331901e-06
Iter: 589 loss: 1.48175263e-06
Iter: 590 loss: 1.48629681e-06
Iter: 591 loss: 1.48128879e-06
Iter: 592 loss: 1.47996639e-06
Iter: 593 loss: 1.47827427e-06
Iter: 594 loss: 1.4781856e-06
Iter: 595 loss: 1.47650042e-06
Iter: 596 loss: 1.48056279e-06
Iter: 597 loss: 1.47591436e-06
Iter: 598 loss: 1.47389756e-06
Iter: 599 loss: 1.48720528e-06
Iter: 600 loss: 1.47372918e-06
Iter: 601 loss: 1.47234425e-06
Iter: 602 loss: 1.47845958e-06
Iter: 603 loss: 1.47207777e-06
Iter: 604 loss: 1.47043863e-06
Iter: 605 loss: 1.47422634e-06
Iter: 606 loss: 1.46980824e-06
Iter: 607 loss: 1.46836192e-06
Iter: 608 loss: 1.46847242e-06
Iter: 609 loss: 1.46724483e-06
Iter: 610 loss: 1.46567277e-06
Iter: 611 loss: 1.47458104e-06
Iter: 612 loss: 1.46548916e-06
Iter: 613 loss: 1.46387276e-06
Iter: 614 loss: 1.46771401e-06
Iter: 615 loss: 1.46327693e-06
Iter: 616 loss: 1.46207731e-06
Iter: 617 loss: 1.46376306e-06
Iter: 618 loss: 1.46149682e-06
Iter: 619 loss: 1.45958529e-06
Iter: 620 loss: 1.46938328e-06
Iter: 621 loss: 1.45928425e-06
Iter: 622 loss: 1.4583577e-06
Iter: 623 loss: 1.46081879e-06
Iter: 624 loss: 1.45802619e-06
Iter: 625 loss: 1.45677484e-06
Iter: 626 loss: 1.45567196e-06
Iter: 627 loss: 1.45537751e-06
Iter: 628 loss: 1.45370336e-06
Iter: 629 loss: 1.45560045e-06
Iter: 630 loss: 1.45278841e-06
Iter: 631 loss: 1.45113677e-06
Iter: 632 loss: 1.45363208e-06
Iter: 633 loss: 1.45033187e-06
Iter: 634 loss: 1.44836986e-06
Iter: 635 loss: 1.46007778e-06
Iter: 636 loss: 1.44811713e-06
Iter: 637 loss: 1.44667342e-06
Iter: 638 loss: 1.46224738e-06
Iter: 639 loss: 1.44661067e-06
Iter: 640 loss: 1.44559067e-06
Iter: 641 loss: 1.44780461e-06
Iter: 642 loss: 1.44520925e-06
Iter: 643 loss: 1.44417709e-06
Iter: 644 loss: 1.44239289e-06
Iter: 645 loss: 1.44239482e-06
Iter: 646 loss: 1.44093019e-06
Iter: 647 loss: 1.44088733e-06
Iter: 648 loss: 1.43972829e-06
Iter: 649 loss: 1.44087358e-06
Iter: 650 loss: 1.43906129e-06
Iter: 651 loss: 1.43784246e-06
Iter: 652 loss: 1.44518708e-06
Iter: 653 loss: 1.43772616e-06
Iter: 654 loss: 1.43636839e-06
Iter: 655 loss: 1.43834541e-06
Iter: 656 loss: 1.4356508e-06
Iter: 657 loss: 1.43476291e-06
Iter: 658 loss: 1.43821217e-06
Iter: 659 loss: 1.43456509e-06
Iter: 660 loss: 1.43349575e-06
Iter: 661 loss: 1.43330089e-06
Iter: 662 loss: 1.43255465e-06
Iter: 663 loss: 1.43141961e-06
Iter: 664 loss: 1.43005138e-06
Iter: 665 loss: 1.4299103e-06
Iter: 666 loss: 1.42776662e-06
Iter: 667 loss: 1.43915008e-06
Iter: 668 loss: 1.42740805e-06
Iter: 669 loss: 1.42580848e-06
Iter: 670 loss: 1.435831e-06
Iter: 671 loss: 1.42561385e-06
Iter: 672 loss: 1.42402314e-06
Iter: 673 loss: 1.43322131e-06
Iter: 674 loss: 1.42379372e-06
Iter: 675 loss: 1.42263093e-06
Iter: 676 loss: 1.42374097e-06
Iter: 677 loss: 1.42193585e-06
Iter: 678 loss: 1.42036538e-06
Iter: 679 loss: 1.41880355e-06
Iter: 680 loss: 1.41841861e-06
Iter: 681 loss: 1.41725911e-06
Iter: 682 loss: 1.41711723e-06
Iter: 683 loss: 1.41601981e-06
Iter: 684 loss: 1.41583348e-06
Iter: 685 loss: 1.41503733e-06
Iter: 686 loss: 1.4140428e-06
Iter: 687 loss: 1.41402313e-06
Iter: 688 loss: 1.41327973e-06
Iter: 689 loss: 1.41260989e-06
Iter: 690 loss: 1.41238297e-06
Iter: 691 loss: 1.4113067e-06
Iter: 692 loss: 1.41492615e-06
Iter: 693 loss: 1.4109969e-06
Iter: 694 loss: 1.40948168e-06
Iter: 695 loss: 1.40886152e-06
Iter: 696 loss: 1.40804411e-06
Iter: 697 loss: 1.40652594e-06
Iter: 698 loss: 1.40566704e-06
Iter: 699 loss: 1.40496763e-06
Iter: 700 loss: 1.40318343e-06
Iter: 701 loss: 1.4201056e-06
Iter: 702 loss: 1.40308248e-06
Iter: 703 loss: 1.40176212e-06
Iter: 704 loss: 1.41243572e-06
Iter: 705 loss: 1.40172278e-06
Iter: 706 loss: 1.4004454e-06
Iter: 707 loss: 1.40377244e-06
Iter: 708 loss: 1.40003385e-06
Iter: 709 loss: 1.39910367e-06
Iter: 710 loss: 1.40020347e-06
Iter: 711 loss: 1.39855706e-06
Iter: 712 loss: 1.3974352e-06
Iter: 713 loss: 1.39839119e-06
Iter: 714 loss: 1.39674114e-06
Iter: 715 loss: 1.39555573e-06
Iter: 716 loss: 1.40742463e-06
Iter: 717 loss: 1.39551014e-06
Iter: 718 loss: 1.39448593e-06
Iter: 719 loss: 1.39578879e-06
Iter: 720 loss: 1.39396229e-06
Iter: 721 loss: 1.39278325e-06
Iter: 722 loss: 1.40383463e-06
Iter: 723 loss: 1.39273709e-06
Iter: 724 loss: 1.39200563e-06
Iter: 725 loss: 1.39050303e-06
Iter: 726 loss: 1.41763371e-06
Iter: 727 loss: 1.39047859e-06
Iter: 728 loss: 1.38937401e-06
Iter: 729 loss: 1.38930716e-06
Iter: 730 loss: 1.38861151e-06
Iter: 731 loss: 1.38760095e-06
Iter: 732 loss: 1.38755854e-06
Iter: 733 loss: 1.38631231e-06
Iter: 734 loss: 1.38837481e-06
Iter: 735 loss: 1.38570329e-06
Iter: 736 loss: 1.38430846e-06
Iter: 737 loss: 1.38700216e-06
Iter: 738 loss: 1.38372945e-06
Iter: 739 loss: 1.38247265e-06
Iter: 740 loss: 1.38243763e-06
Iter: 741 loss: 1.38160294e-06
Iter: 742 loss: 1.38107248e-06
Iter: 743 loss: 1.38075018e-06
Iter: 744 loss: 1.37940037e-06
Iter: 745 loss: 1.38141081e-06
Iter: 746 loss: 1.37879374e-06
Iter: 747 loss: 1.37755706e-06
Iter: 748 loss: 1.38372195e-06
Iter: 749 loss: 1.37736856e-06
Iter: 750 loss: 1.37616485e-06
Iter: 751 loss: 1.38061796e-06
Iter: 752 loss: 1.37592144e-06
Iter: 753 loss: 1.37470511e-06
Iter: 754 loss: 1.37934103e-06
Iter: 755 loss: 1.37439338e-06
Iter: 756 loss: 1.37319807e-06
Iter: 757 loss: 1.37694053e-06
Iter: 758 loss: 1.37285826e-06
Iter: 759 loss: 1.37219172e-06
Iter: 760 loss: 1.37264169e-06
Iter: 761 loss: 1.37185771e-06
Iter: 762 loss: 1.37091206e-06
Iter: 763 loss: 1.37470408e-06
Iter: 764 loss: 1.37072016e-06
Iter: 765 loss: 1.37000234e-06
Iter: 766 loss: 1.36832568e-06
Iter: 767 loss: 1.38860514e-06
Iter: 768 loss: 1.36821973e-06
Iter: 769 loss: 1.36622293e-06
Iter: 770 loss: 1.38041662e-06
Iter: 771 loss: 1.36602011e-06
Iter: 772 loss: 1.36499352e-06
Iter: 773 loss: 1.37547329e-06
Iter: 774 loss: 1.36496863e-06
Iter: 775 loss: 1.36375945e-06
Iter: 776 loss: 1.36539938e-06
Iter: 777 loss: 1.36316407e-06
Iter: 778 loss: 1.36221502e-06
Iter: 779 loss: 1.36360359e-06
Iter: 780 loss: 1.36177584e-06
Iter: 781 loss: 1.36056201e-06
Iter: 782 loss: 1.36159281e-06
Iter: 783 loss: 1.35984146e-06
Iter: 784 loss: 1.35849496e-06
Iter: 785 loss: 1.37025518e-06
Iter: 786 loss: 1.35843868e-06
Iter: 787 loss: 1.35757637e-06
Iter: 788 loss: 1.36407209e-06
Iter: 789 loss: 1.35752566e-06
Iter: 790 loss: 1.35686537e-06
Iter: 791 loss: 1.35888854e-06
Iter: 792 loss: 1.35664436e-06
Iter: 793 loss: 1.35595155e-06
Iter: 794 loss: 1.35510936e-06
Iter: 795 loss: 1.3550241e-06
Iter: 796 loss: 1.35395658e-06
Iter: 797 loss: 1.36244262e-06
Iter: 798 loss: 1.3538571e-06
Iter: 799 loss: 1.35276537e-06
Iter: 800 loss: 1.35216271e-06
Iter: 801 loss: 1.35164123e-06
Iter: 802 loss: 1.35043911e-06
Iter: 803 loss: 1.35137316e-06
Iter: 804 loss: 1.34969446e-06
Iter: 805 loss: 1.34840479e-06
Iter: 806 loss: 1.35404639e-06
Iter: 807 loss: 1.34815195e-06
Iter: 808 loss: 1.34740992e-06
Iter: 809 loss: 1.34736683e-06
Iter: 810 loss: 1.3467677e-06
Iter: 811 loss: 1.34628908e-06
Iter: 812 loss: 1.34608831e-06
Iter: 813 loss: 1.34519894e-06
Iter: 814 loss: 1.34446987e-06
Iter: 815 loss: 1.34418349e-06
Iter: 816 loss: 1.34245124e-06
Iter: 817 loss: 1.35575533e-06
Iter: 818 loss: 1.34231493e-06
Iter: 819 loss: 1.3414442e-06
Iter: 820 loss: 1.3505138e-06
Iter: 821 loss: 1.34145239e-06
Iter: 822 loss: 1.34078675e-06
Iter: 823 loss: 1.34203719e-06
Iter: 824 loss: 1.34048321e-06
Iter: 825 loss: 1.33958304e-06
Iter: 826 loss: 1.33994729e-06
Iter: 827 loss: 1.33898698e-06
Iter: 828 loss: 1.33795845e-06
Iter: 829 loss: 1.33846106e-06
Iter: 830 loss: 1.33731203e-06
Iter: 831 loss: 1.33601168e-06
Iter: 832 loss: 1.34763e-06
Iter: 833 loss: 1.33593949e-06
Iter: 834 loss: 1.3352078e-06
Iter: 835 loss: 1.33482013e-06
Iter: 836 loss: 1.3345259e-06
Iter: 837 loss: 1.33338722e-06
Iter: 838 loss: 1.33312847e-06
Iter: 839 loss: 1.33237893e-06
Iter: 840 loss: 1.33163462e-06
Iter: 841 loss: 1.33160756e-06
Iter: 842 loss: 1.33068488e-06
Iter: 843 loss: 1.33101082e-06
Iter: 844 loss: 1.3300496e-06
Iter: 845 loss: 1.32897594e-06
Iter: 846 loss: 1.32976811e-06
Iter: 847 loss: 1.32833566e-06
Iter: 848 loss: 1.32734942e-06
Iter: 849 loss: 1.33289439e-06
Iter: 850 loss: 1.32719606e-06
Iter: 851 loss: 1.32627724e-06
Iter: 852 loss: 1.33096592e-06
Iter: 853 loss: 1.32611763e-06
Iter: 854 loss: 1.32540049e-06
Iter: 855 loss: 1.33050617e-06
Iter: 856 loss: 1.32532944e-06
Iter: 857 loss: 1.32470745e-06
Iter: 858 loss: 1.32491573e-06
Iter: 859 loss: 1.32422258e-06
Iter: 860 loss: 1.32334833e-06
Iter: 861 loss: 1.32399168e-06
Iter: 862 loss: 1.32280172e-06
Iter: 863 loss: 1.32200034e-06
Iter: 864 loss: 1.32624564e-06
Iter: 865 loss: 1.32186051e-06
Iter: 866 loss: 1.32090759e-06
Iter: 867 loss: 1.32067134e-06
Iter: 868 loss: 1.32005221e-06
Iter: 869 loss: 1.31899321e-06
Iter: 870 loss: 1.32087803e-06
Iter: 871 loss: 1.3185188e-06
Iter: 872 loss: 1.3174481e-06
Iter: 873 loss: 1.31703302e-06
Iter: 874 loss: 1.31645743e-06
Iter: 875 loss: 1.31594231e-06
Iter: 876 loss: 1.31558215e-06
Iter: 877 loss: 1.31489492e-06
Iter: 878 loss: 1.31419142e-06
Iter: 879 loss: 1.31406455e-06
Iter: 880 loss: 1.31316688e-06
Iter: 881 loss: 1.31385968e-06
Iter: 882 loss: 1.31260845e-06
Iter: 883 loss: 1.31152797e-06
Iter: 884 loss: 1.32261448e-06
Iter: 885 loss: 1.31153e-06
Iter: 886 loss: 1.31072443e-06
Iter: 887 loss: 1.31549973e-06
Iter: 888 loss: 1.31061529e-06
Iter: 889 loss: 1.30996875e-06
Iter: 890 loss: 1.31160823e-06
Iter: 891 loss: 1.30970534e-06
Iter: 892 loss: 1.30900469e-06
Iter: 893 loss: 1.30885746e-06
Iter: 894 loss: 1.30838771e-06
Iter: 895 loss: 1.30739409e-06
Iter: 896 loss: 1.31028628e-06
Iter: 897 loss: 1.30709054e-06
Iter: 898 loss: 1.3062031e-06
Iter: 899 loss: 1.31123795e-06
Iter: 900 loss: 1.3060943e-06
Iter: 901 loss: 1.30528872e-06
Iter: 902 loss: 1.30478031e-06
Iter: 903 loss: 1.30444801e-06
Iter: 904 loss: 1.30346405e-06
Iter: 905 loss: 1.30365424e-06
Iter: 906 loss: 1.30274407e-06
Iter: 907 loss: 1.30155263e-06
Iter: 908 loss: 1.31229501e-06
Iter: 909 loss: 1.30152671e-06
Iter: 910 loss: 1.30050125e-06
Iter: 911 loss: 1.3079075e-06
Iter: 912 loss: 1.30040394e-06
Iter: 913 loss: 1.29962018e-06
Iter: 914 loss: 1.29857085e-06
Iter: 915 loss: 1.2985131e-06
Iter: 916 loss: 1.29766943e-06
Iter: 917 loss: 1.30334377e-06
Iter: 918 loss: 1.29757041e-06
Iter: 919 loss: 1.29660248e-06
Iter: 920 loss: 1.30085994e-06
Iter: 921 loss: 1.2964183e-06
Iter: 922 loss: 1.29565706e-06
Iter: 923 loss: 1.30120111e-06
Iter: 924 loss: 1.29557088e-06
Iter: 925 loss: 1.2950378e-06
Iter: 926 loss: 1.2953783e-06
Iter: 927 loss: 1.29471539e-06
Iter: 928 loss: 1.29395573e-06
Iter: 929 loss: 1.29420062e-06
Iter: 930 loss: 1.29345983e-06
Iter: 931 loss: 1.29256784e-06
Iter: 932 loss: 1.29688146e-06
Iter: 933 loss: 1.29240391e-06
Iter: 934 loss: 1.29150885e-06
Iter: 935 loss: 1.29280147e-06
Iter: 936 loss: 1.29108446e-06
Iter: 937 loss: 1.29000341e-06
Iter: 938 loss: 1.29002728e-06
Iter: 939 loss: 1.28923773e-06
Iter: 940 loss: 1.28820329e-06
Iter: 941 loss: 1.28871147e-06
Iter: 942 loss: 1.28744296e-06
Iter: 943 loss: 1.28722536e-06
Iter: 944 loss: 1.2868245e-06
Iter: 945 loss: 1.28633928e-06
Iter: 946 loss: 1.28578199e-06
Iter: 947 loss: 1.28569854e-06
Iter: 948 loss: 1.2848509e-06
Iter: 949 loss: 1.28428974e-06
Iter: 950 loss: 1.28395573e-06
Iter: 951 loss: 1.28337024e-06
Iter: 952 loss: 1.28326224e-06
Iter: 953 loss: 1.28269141e-06
Iter: 954 loss: 1.28427496e-06
Iter: 955 loss: 1.28247814e-06
Iter: 956 loss: 1.28185832e-06
Iter: 957 loss: 1.28326178e-06
Iter: 958 loss: 1.28162992e-06
Iter: 959 loss: 1.2809063e-06
Iter: 960 loss: 1.28026181e-06
Iter: 961 loss: 1.28012266e-06
Iter: 962 loss: 1.27899966e-06
Iter: 963 loss: 1.28671195e-06
Iter: 964 loss: 1.27890405e-06
Iter: 965 loss: 1.27819339e-06
Iter: 966 loss: 1.28179477e-06
Iter: 967 loss: 1.27808482e-06
Iter: 968 loss: 1.27744465e-06
Iter: 969 loss: 1.2770588e-06
Iter: 970 loss: 1.27678231e-06
Iter: 971 loss: 1.27585758e-06
Iter: 972 loss: 1.27648593e-06
Iter: 973 loss: 1.27527915e-06
Iter: 974 loss: 1.2741375e-06
Iter: 975 loss: 1.27902331e-06
Iter: 976 loss: 1.27394742e-06
Iter: 977 loss: 1.27322971e-06
Iter: 978 loss: 1.27323597e-06
Iter: 979 loss: 1.27270425e-06
Iter: 980 loss: 1.27144847e-06
Iter: 981 loss: 1.28277236e-06
Iter: 982 loss: 1.27124804e-06
Iter: 983 loss: 1.27010753e-06
Iter: 984 loss: 1.28231272e-06
Iter: 985 loss: 1.27010776e-06
Iter: 986 loss: 1.26946566e-06
Iter: 987 loss: 1.27950148e-06
Iter: 988 loss: 1.2694627e-06
Iter: 989 loss: 1.26887971e-06
Iter: 990 loss: 1.26874329e-06
Iter: 991 loss: 1.2683787e-06
Iter: 992 loss: 1.26746795e-06
Iter: 993 loss: 1.27043177e-06
Iter: 994 loss: 1.26718237e-06
Iter: 995 loss: 1.26655664e-06
Iter: 996 loss: 1.26644852e-06
Iter: 997 loss: 1.26595296e-06
Iter: 998 loss: 1.26495536e-06
Iter: 999 loss: 1.27073315e-06
Iter: 1000 loss: 1.26484633e-06
Iter: 1001 loss: 1.26407531e-06
Iter: 1002 loss: 1.26614589e-06
Iter: 1003 loss: 1.26381883e-06
Iter: 1004 loss: 1.26305258e-06
Iter: 1005 loss: 1.26279588e-06
Iter: 1006 loss: 1.26230611e-06
Iter: 1007 loss: 1.26143141e-06
Iter: 1008 loss: 1.26306577e-06
Iter: 1009 loss: 1.2610692e-06
Iter: 1010 loss: 1.26006944e-06
Iter: 1011 loss: 1.26750433e-06
Iter: 1012 loss: 1.26000828e-06
Iter: 1013 loss: 1.25908264e-06
Iter: 1014 loss: 1.26401824e-06
Iter: 1015 loss: 1.25892416e-06
Iter: 1016 loss: 1.25849533e-06
Iter: 1017 loss: 1.25760653e-06
Iter: 1018 loss: 1.27586566e-06
Iter: 1019 loss: 1.257598e-06
Iter: 1020 loss: 1.25692441e-06
Iter: 1021 loss: 1.25691406e-06
Iter: 1022 loss: 1.25613315e-06
Iter: 1023 loss: 1.25804627e-06
Iter: 1024 loss: 1.25588201e-06
Iter: 1025 loss: 1.25531142e-06
Iter: 1026 loss: 1.25795304e-06
Iter: 1027 loss: 1.25521092e-06
Iter: 1028 loss: 1.25469865e-06
Iter: 1029 loss: 1.25441397e-06
Iter: 1030 loss: 1.25420502e-06
Iter: 1031 loss: 1.25337783e-06
Iter: 1032 loss: 1.2551875e-06
Iter: 1033 loss: 1.25310635e-06
Iter: 1034 loss: 1.2521017e-06
Iter: 1035 loss: 1.25598331e-06
Iter: 1036 loss: 1.25188376e-06
Iter: 1037 loss: 1.25109773e-06
Iter: 1038 loss: 1.25242218e-06
Iter: 1039 loss: 1.25072836e-06
Iter: 1040 loss: 1.24994278e-06
Iter: 1041 loss: 1.25062638e-06
Iter: 1042 loss: 1.24950384e-06
Iter: 1043 loss: 1.24859184e-06
Iter: 1044 loss: 1.25082408e-06
Iter: 1045 loss: 1.24831388e-06
Iter: 1046 loss: 1.24780172e-06
Iter: 1047 loss: 1.24777125e-06
Iter: 1048 loss: 1.24734049e-06
Iter: 1049 loss: 1.24623568e-06
Iter: 1050 loss: 1.25483598e-06
Iter: 1051 loss: 1.24598841e-06
Iter: 1052 loss: 1.2446958e-06
Iter: 1053 loss: 1.25290853e-06
Iter: 1054 loss: 1.24456221e-06
Iter: 1055 loss: 1.24416204e-06
Iter: 1056 loss: 1.24406472e-06
Iter: 1057 loss: 1.24360054e-06
Iter: 1058 loss: 1.24292956e-06
Iter: 1059 loss: 1.24291e-06
Iter: 1060 loss: 1.24215535e-06
Iter: 1061 loss: 1.24934672e-06
Iter: 1062 loss: 1.24214648e-06
Iter: 1063 loss: 1.24165103e-06
Iter: 1064 loss: 1.24112603e-06
Iter: 1065 loss: 1.24104133e-06
Iter: 1066 loss: 1.24042708e-06
Iter: 1067 loss: 1.24956784e-06
Iter: 1068 loss: 1.24040935e-06
Iter: 1069 loss: 1.23992447e-06
Iter: 1070 loss: 1.23950122e-06
Iter: 1071 loss: 1.23938867e-06
Iter: 1072 loss: 1.23843688e-06
Iter: 1073 loss: 1.23985751e-06
Iter: 1074 loss: 1.23797531e-06
Iter: 1075 loss: 1.23704672e-06
Iter: 1076 loss: 1.23987729e-06
Iter: 1077 loss: 1.23677262e-06
Iter: 1078 loss: 1.23596692e-06
Iter: 1079 loss: 1.24028588e-06
Iter: 1080 loss: 1.23581731e-06
Iter: 1081 loss: 1.23484574e-06
Iter: 1082 loss: 1.23759605e-06
Iter: 1083 loss: 1.23457528e-06
Iter: 1084 loss: 1.23404448e-06
Iter: 1085 loss: 1.23321092e-06
Iter: 1086 loss: 1.23320069e-06
Iter: 1087 loss: 1.2325645e-06
Iter: 1088 loss: 1.2325512e-06
Iter: 1089 loss: 1.23197219e-06
Iter: 1090 loss: 1.23490781e-06
Iter: 1091 loss: 1.23191171e-06
Iter: 1092 loss: 1.23150562e-06
Iter: 1093 loss: 1.23142945e-06
Iter: 1094 loss: 1.2312131e-06
Iter: 1095 loss: 1.23051348e-06
Iter: 1096 loss: 1.23173845e-06
Iter: 1097 loss: 1.23020573e-06
Iter: 1098 loss: 1.22965548e-06
Iter: 1099 loss: 1.23040536e-06
Iter: 1100 loss: 1.22933625e-06
Iter: 1101 loss: 1.2284961e-06
Iter: 1102 loss: 1.23153268e-06
Iter: 1103 loss: 1.22826771e-06
Iter: 1104 loss: 1.22764732e-06
Iter: 1105 loss: 1.22789993e-06
Iter: 1106 loss: 1.22715505e-06
Iter: 1107 loss: 1.22626705e-06
Iter: 1108 loss: 1.22797053e-06
Iter: 1109 loss: 1.22590768e-06
Iter: 1110 loss: 1.22502468e-06
Iter: 1111 loss: 1.22809024e-06
Iter: 1112 loss: 1.22473671e-06
Iter: 1113 loss: 1.22410393e-06
Iter: 1114 loss: 1.22409074e-06
Iter: 1115 loss: 1.22362519e-06
Iter: 1116 loss: 1.22278061e-06
Iter: 1117 loss: 1.22278425e-06
Iter: 1118 loss: 1.22183292e-06
Iter: 1119 loss: 1.22415031e-06
Iter: 1120 loss: 1.22150777e-06
Iter: 1121 loss: 1.22134509e-06
Iter: 1122 loss: 1.22106076e-06
Iter: 1123 loss: 1.22077563e-06
Iter: 1124 loss: 1.22013694e-06
Iter: 1125 loss: 1.22915662e-06
Iter: 1126 loss: 1.22009828e-06
Iter: 1127 loss: 1.21929077e-06
Iter: 1128 loss: 1.2245572e-06
Iter: 1129 loss: 1.219198e-06
Iter: 1130 loss: 1.21867606e-06
Iter: 1131 loss: 1.21899416e-06
Iter: 1132 loss: 1.21827463e-06
Iter: 1133 loss: 1.21762059e-06
Iter: 1134 loss: 1.22077768e-06
Iter: 1135 loss: 1.2175318e-06
Iter: 1136 loss: 1.21677635e-06
Iter: 1137 loss: 1.2170035e-06
Iter: 1138 loss: 1.21625658e-06
Iter: 1139 loss: 1.21557639e-06
Iter: 1140 loss: 1.21698133e-06
Iter: 1141 loss: 1.21525591e-06
Iter: 1142 loss: 1.21444407e-06
Iter: 1143 loss: 1.2172967e-06
Iter: 1144 loss: 1.2142566e-06
Iter: 1145 loss: 1.2137159e-06
Iter: 1146 loss: 1.21935886e-06
Iter: 1147 loss: 1.21368134e-06
Iter: 1148 loss: 1.21307414e-06
Iter: 1149 loss: 1.21261155e-06
Iter: 1150 loss: 1.2124558e-06
Iter: 1151 loss: 1.21176345e-06
Iter: 1152 loss: 1.21319817e-06
Iter: 1153 loss: 1.2115172e-06
Iter: 1154 loss: 1.21098969e-06
Iter: 1155 loss: 1.21836342e-06
Iter: 1156 loss: 1.21097105e-06
Iter: 1157 loss: 1.21039295e-06
Iter: 1158 loss: 1.20994628e-06
Iter: 1159 loss: 1.20974119e-06
Iter: 1160 loss: 1.20915e-06
Iter: 1161 loss: 1.21296625e-06
Iter: 1162 loss: 1.20910272e-06
Iter: 1163 loss: 1.20846255e-06
Iter: 1164 loss: 1.20836717e-06
Iter: 1165 loss: 1.20794562e-06
Iter: 1166 loss: 1.20722325e-06
Iter: 1167 loss: 1.21121411e-06
Iter: 1168 loss: 1.20712809e-06
Iter: 1169 loss: 1.20651498e-06
Iter: 1170 loss: 1.20891195e-06
Iter: 1171 loss: 1.20634911e-06
Iter: 1172 loss: 1.2059e-06
Iter: 1173 loss: 1.20590573e-06
Iter: 1174 loss: 1.20555592e-06
Iter: 1175 loss: 1.20491063e-06
Iter: 1176 loss: 1.20619211e-06
Iter: 1177 loss: 1.20466404e-06
Iter: 1178 loss: 1.20381571e-06
Iter: 1179 loss: 1.20495019e-06
Iter: 1180 loss: 1.20337438e-06
Iter: 1181 loss: 1.20241259e-06
Iter: 1182 loss: 1.21450523e-06
Iter: 1183 loss: 1.20240077e-06
Iter: 1184 loss: 1.20197024e-06
Iter: 1185 loss: 1.20140544e-06
Iter: 1186 loss: 1.20135815e-06
Iter: 1187 loss: 1.20075595e-06
Iter: 1188 loss: 1.20549839e-06
Iter: 1189 loss: 1.20074174e-06
Iter: 1190 loss: 1.20008474e-06
Iter: 1191 loss: 1.20464358e-06
Iter: 1192 loss: 1.20005916e-06
Iter: 1193 loss: 1.1996882e-06
Iter: 1194 loss: 1.19915342e-06
Iter: 1195 loss: 1.19915205e-06
Iter: 1196 loss: 1.19838285e-06
Iter: 1197 loss: 1.20438926e-06
Iter: 1198 loss: 1.1983384e-06
Iter: 1199 loss: 1.1977944e-06
Iter: 1200 loss: 1.1974962e-06
Iter: 1201 loss: 1.19726576e-06
Iter: 1202 loss: 1.19665594e-06
Iter: 1203 loss: 1.20596428e-06
Iter: 1204 loss: 1.19666197e-06
Iter: 1205 loss: 1.19622757e-06
Iter: 1206 loss: 1.19582148e-06
Iter: 1207 loss: 1.19572815e-06
Iter: 1208 loss: 1.19505694e-06
Iter: 1209 loss: 1.19695267e-06
Iter: 1210 loss: 1.19485037e-06
Iter: 1211 loss: 1.19412937e-06
Iter: 1212 loss: 1.19605784e-06
Iter: 1213 loss: 1.19390779e-06
Iter: 1214 loss: 1.19333447e-06
Iter: 1215 loss: 1.20121535e-06
Iter: 1216 loss: 1.19334356e-06
Iter: 1217 loss: 1.19286244e-06
Iter: 1218 loss: 1.19269589e-06
Iter: 1219 loss: 1.19245487e-06
Iter: 1220 loss: 1.19182846e-06
Iter: 1221 loss: 1.19161859e-06
Iter: 1222 loss: 1.19123672e-06
Iter: 1223 loss: 1.19085132e-06
Iter: 1224 loss: 1.19071933e-06
Iter: 1225 loss: 1.19039055e-06
Iter: 1226 loss: 1.18975834e-06
Iter: 1227 loss: 1.2023072e-06
Iter: 1228 loss: 1.18974754e-06
Iter: 1229 loss: 1.18911737e-06
Iter: 1230 loss: 1.1941022e-06
Iter: 1231 loss: 1.18907428e-06
Iter: 1232 loss: 1.1884822e-06
Iter: 1233 loss: 1.18918967e-06
Iter: 1234 loss: 1.18822686e-06
Iter: 1235 loss: 1.18778655e-06
Iter: 1236 loss: 1.19019728e-06
Iter: 1237 loss: 1.18771368e-06
Iter: 1238 loss: 1.18723869e-06
Iter: 1239 loss: 1.18724972e-06
Iter: 1240 loss: 1.18685853e-06
Iter: 1241 loss: 1.18630328e-06
Iter: 1242 loss: 1.18649939e-06
Iter: 1243 loss: 1.18591856e-06
Iter: 1244 loss: 1.18516243e-06
Iter: 1245 loss: 1.18946764e-06
Iter: 1246 loss: 1.1850583e-06
Iter: 1247 loss: 1.18453113e-06
Iter: 1248 loss: 1.18851131e-06
Iter: 1249 loss: 1.18451487e-06
Iter: 1250 loss: 1.18395474e-06
Iter: 1251 loss: 1.18388061e-06
Iter: 1252 loss: 1.18350295e-06
Iter: 1253 loss: 1.18276012e-06
Iter: 1254 loss: 1.18426158e-06
Iter: 1255 loss: 1.18251342e-06
Iter: 1256 loss: 1.182191e-06
Iter: 1257 loss: 1.18216849e-06
Iter: 1258 loss: 1.1818513e-06
Iter: 1259 loss: 1.18126093e-06
Iter: 1260 loss: 1.19422566e-06
Iter: 1261 loss: 1.18126627e-06
Iter: 1262 loss: 1.18070125e-06
Iter: 1263 loss: 1.1843731e-06
Iter: 1264 loss: 1.18064793e-06
Iter: 1265 loss: 1.18015123e-06
Iter: 1266 loss: 1.18102321e-06
Iter: 1267 loss: 1.17992295e-06
Iter: 1268 loss: 1.17938544e-06
Iter: 1269 loss: 1.17967329e-06
Iter: 1270 loss: 1.17903573e-06
Iter: 1271 loss: 1.17827403e-06
Iter: 1272 loss: 1.18250966e-06
Iter: 1273 loss: 1.17816978e-06
Iter: 1274 loss: 1.17762283e-06
Iter: 1275 loss: 1.17680418e-06
Iter: 1276 loss: 1.1767637e-06
Iter: 1277 loss: 1.17596369e-06
Iter: 1278 loss: 1.185364e-06
Iter: 1279 loss: 1.17594459e-06
Iter: 1280 loss: 1.17537274e-06
Iter: 1281 loss: 1.17747709e-06
Iter: 1282 loss: 1.17526633e-06
Iter: 1283 loss: 1.17469619e-06
Iter: 1284 loss: 1.17829416e-06
Iter: 1285 loss: 1.1746165e-06
Iter: 1286 loss: 1.17424952e-06
Iter: 1287 loss: 1.17422496e-06
Iter: 1288 loss: 1.17394484e-06
Iter: 1289 loss: 1.1734503e-06
Iter: 1290 loss: 1.17587e-06
Iter: 1291 loss: 1.1733797e-06
Iter: 1292 loss: 1.17274271e-06
Iter: 1293 loss: 1.17329569e-06
Iter: 1294 loss: 1.17231798e-06
Iter: 1295 loss: 1.17189541e-06
Iter: 1296 loss: 1.1727094e-06
Iter: 1297 loss: 1.17169407e-06
Iter: 1298 loss: 1.17114587e-06
Iter: 1299 loss: 1.17263926e-06
Iter: 1300 loss: 1.17098432e-06
Iter: 1301 loss: 1.17041145e-06
Iter: 1302 loss: 1.17080117e-06
Iter: 1303 loss: 1.1700813e-06
Iter: 1304 loss: 1.16950264e-06
Iter: 1305 loss: 1.1754662e-06
Iter: 1306 loss: 1.16945228e-06
Iter: 1307 loss: 1.16903288e-06
Iter: 1308 loss: 1.16848685e-06
Iter: 1309 loss: 1.16841898e-06
Iter: 1310 loss: 1.16778483e-06
Iter: 1311 loss: 1.17008585e-06
Iter: 1312 loss: 1.167645e-06
Iter: 1313 loss: 1.16704859e-06
Iter: 1314 loss: 1.17110551e-06
Iter: 1315 loss: 1.1669714e-06
Iter: 1316 loss: 1.16654883e-06
Iter: 1317 loss: 1.17012246e-06
Iter: 1318 loss: 1.16652404e-06
Iter: 1319 loss: 1.16617525e-06
Iter: 1320 loss: 1.16563672e-06
Iter: 1321 loss: 1.1656175e-06
Iter: 1322 loss: 1.16496858e-06
Iter: 1323 loss: 1.16891192e-06
Iter: 1324 loss: 1.16487035e-06
Iter: 1325 loss: 1.16427304e-06
Iter: 1326 loss: 1.17042759e-06
Iter: 1327 loss: 1.16425031e-06
Iter: 1328 loss: 1.16396154e-06
Iter: 1329 loss: 1.16348463e-06
Iter: 1330 loss: 1.17552986e-06
Iter: 1331 loss: 1.16347007e-06
Iter: 1332 loss: 1.16293734e-06
Iter: 1333 loss: 1.16929778e-06
Iter: 1334 loss: 1.16291631e-06
Iter: 1335 loss: 1.16254103e-06
Iter: 1336 loss: 1.1623207e-06
Iter: 1337 loss: 1.16217223e-06
Iter: 1338 loss: 1.16167848e-06
Iter: 1339 loss: 1.16608658e-06
Iter: 1340 loss: 1.16167e-06
Iter: 1341 loss: 1.1612367e-06
Iter: 1342 loss: 1.16112392e-06
Iter: 1343 loss: 1.16082651e-06
Iter: 1344 loss: 1.16023489e-06
Iter: 1345 loss: 1.16017145e-06
Iter: 1346 loss: 1.15976877e-06
Iter: 1347 loss: 1.15904959e-06
Iter: 1348 loss: 1.1663343e-06
Iter: 1349 loss: 1.15899616e-06
Iter: 1350 loss: 1.15840112e-06
Iter: 1351 loss: 1.16178649e-06
Iter: 1352 loss: 1.1583295e-06
Iter: 1353 loss: 1.1577381e-06
Iter: 1354 loss: 1.1584475e-06
Iter: 1355 loss: 1.15748367e-06
Iter: 1356 loss: 1.15687794e-06
Iter: 1357 loss: 1.15687749e-06
Iter: 1358 loss: 1.15638431e-06
Iter: 1359 loss: 1.15608577e-06
Iter: 1360 loss: 1.15596913e-06
Iter: 1361 loss: 1.15568878e-06
Iter: 1362 loss: 1.15510602e-06
Iter: 1363 loss: 1.16586466e-06
Iter: 1364 loss: 1.1551075e-06
Iter: 1365 loss: 1.15457294e-06
Iter: 1366 loss: 1.15855346e-06
Iter: 1367 loss: 1.15455532e-06
Iter: 1368 loss: 1.15413741e-06
Iter: 1369 loss: 1.15505452e-06
Iter: 1370 loss: 1.15397984e-06
Iter: 1371 loss: 1.1535692e-06
Iter: 1372 loss: 1.15332193e-06
Iter: 1373 loss: 1.15316209e-06
Iter: 1374 loss: 1.15246212e-06
Iter: 1375 loss: 1.15898933e-06
Iter: 1376 loss: 1.15242278e-06
Iter: 1377 loss: 1.15198509e-06
Iter: 1378 loss: 1.1512e-06
Iter: 1379 loss: 1.16940203e-06
Iter: 1380 loss: 1.15121122e-06
Iter: 1381 loss: 1.15036346e-06
Iter: 1382 loss: 1.15639921e-06
Iter: 1383 loss: 1.15031389e-06
Iter: 1384 loss: 1.14975637e-06
Iter: 1385 loss: 1.15495095e-06
Iter: 1386 loss: 1.14974705e-06
Iter: 1387 loss: 1.14919635e-06
Iter: 1388 loss: 1.14967e-06
Iter: 1389 loss: 1.14886893e-06
Iter: 1390 loss: 1.14833426e-06
Iter: 1391 loss: 1.15022044e-06
Iter: 1392 loss: 1.1481718e-06
Iter: 1393 loss: 1.14778163e-06
Iter: 1394 loss: 1.14920078e-06
Iter: 1395 loss: 1.14769296e-06
Iter: 1396 loss: 1.14713339e-06
Iter: 1397 loss: 1.14818511e-06
Iter: 1398 loss: 1.14686384e-06
Iter: 1399 loss: 1.14649379e-06
Iter: 1400 loss: 1.14605245e-06
Iter: 1401 loss: 1.14602312e-06
Iter: 1402 loss: 1.14524812e-06
Iter: 1403 loss: 1.15079126e-06
Iter: 1404 loss: 1.14518423e-06
Iter: 1405 loss: 1.14463228e-06
Iter: 1406 loss: 1.14497107e-06
Iter: 1407 loss: 1.14427144e-06
Iter: 1408 loss: 1.14372176e-06
Iter: 1409 loss: 1.14640375e-06
Iter: 1410 loss: 1.14364184e-06
Iter: 1411 loss: 1.14303486e-06
Iter: 1412 loss: 1.14388536e-06
Iter: 1413 loss: 1.1427145e-06
Iter: 1414 loss: 1.1422253e-06
Iter: 1415 loss: 1.14168915e-06
Iter: 1416 loss: 1.14163208e-06
Iter: 1417 loss: 1.14097872e-06
Iter: 1418 loss: 1.15043667e-06
Iter: 1419 loss: 1.14099953e-06
Iter: 1420 loss: 1.14051159e-06
Iter: 1421 loss: 1.14446721e-06
Iter: 1422 loss: 1.14049044e-06
Iter: 1423 loss: 1.14010697e-06
Iter: 1424 loss: 1.13983958e-06
Iter: 1425 loss: 1.1396985e-06
Iter: 1426 loss: 1.139093e-06
Iter: 1427 loss: 1.1407152e-06
Iter: 1428 loss: 1.13890587e-06
Iter: 1429 loss: 1.13844442e-06
Iter: 1430 loss: 1.1384609e-06
Iter: 1431 loss: 1.13808051e-06
Iter: 1432 loss: 1.13755982e-06
Iter: 1433 loss: 1.13752378e-06
Iter: 1434 loss: 1.13704323e-06
Iter: 1435 loss: 1.13857755e-06
Iter: 1436 loss: 1.13694648e-06
Iter: 1437 loss: 1.13633837e-06
Iter: 1438 loss: 1.13819874e-06
Iter: 1439 loss: 1.13617511e-06
Iter: 1440 loss: 1.13572287e-06
Iter: 1441 loss: 1.1357032e-06
Iter: 1442 loss: 1.13535589e-06
Iter: 1443 loss: 1.13487556e-06
Iter: 1444 loss: 1.14206546e-06
Iter: 1445 loss: 1.1348568e-06
Iter: 1446 loss: 1.13447084e-06
Iter: 1447 loss: 1.13385772e-06
Iter: 1448 loss: 1.13384522e-06
Iter: 1449 loss: 1.13299586e-06
Iter: 1450 loss: 1.13395481e-06
Iter: 1451 loss: 1.13252759e-06
Iter: 1452 loss: 1.13196984e-06
Iter: 1453 loss: 1.13195347e-06
Iter: 1454 loss: 1.13139424e-06
Iter: 1455 loss: 1.1335469e-06
Iter: 1456 loss: 1.1312884e-06
Iter: 1457 loss: 1.13085048e-06
Iter: 1458 loss: 1.1309304e-06
Iter: 1459 loss: 1.13052442e-06
Iter: 1460 loss: 1.13005251e-06
Iter: 1461 loss: 1.13336114e-06
Iter: 1462 loss: 1.13001249e-06
Iter: 1463 loss: 1.12952353e-06
Iter: 1464 loss: 1.13200167e-06
Iter: 1465 loss: 1.12944826e-06
Iter: 1466 loss: 1.1291279e-06
Iter: 1467 loss: 1.12875477e-06
Iter: 1468 loss: 1.12871066e-06
Iter: 1469 loss: 1.12819453e-06
Iter: 1470 loss: 1.13065357e-06
Iter: 1471 loss: 1.12815246e-06
Iter: 1472 loss: 1.12755583e-06
Iter: 1473 loss: 1.12885141e-06
Iter: 1474 loss: 1.12738576e-06
Iter: 1475 loss: 1.12697796e-06
Iter: 1476 loss: 1.12690736e-06
Iter: 1477 loss: 1.12664725e-06
Iter: 1478 loss: 1.12590772e-06
Iter: 1479 loss: 1.12893531e-06
Iter: 1480 loss: 1.12572729e-06
Iter: 1481 loss: 1.12525925e-06
Iter: 1482 loss: 1.12553516e-06
Iter: 1483 loss: 1.12498776e-06
Iter: 1484 loss: 1.12436464e-06
Iter: 1485 loss: 1.12419877e-06
Iter: 1486 loss: 1.12381599e-06
Iter: 1487 loss: 1.12353575e-06
Iter: 1488 loss: 1.12338557e-06
Iter: 1489 loss: 1.12303678e-06
Iter: 1490 loss: 1.12239911e-06
Iter: 1491 loss: 1.12239672e-06
Iter: 1492 loss: 1.12182272e-06
Iter: 1493 loss: 1.12622388e-06
Iter: 1494 loss: 1.12176167e-06
Iter: 1495 loss: 1.1213408e-06
Iter: 1496 loss: 1.12686382e-06
Iter: 1497 loss: 1.12132886e-06
Iter: 1498 loss: 1.12099076e-06
Iter: 1499 loss: 1.12049747e-06
Iter: 1500 loss: 1.12048144e-06
Iter: 1501 loss: 1.12003374e-06
Iter: 1502 loss: 1.12145676e-06
Iter: 1503 loss: 1.11987447e-06
Iter: 1504 loss: 1.11941358e-06
Iter: 1505 loss: 1.12242924e-06
Iter: 1506 loss: 1.1193647e-06
Iter: 1507 loss: 1.11890904e-06
Iter: 1508 loss: 1.11864438e-06
Iter: 1509 loss: 1.11843156e-06
Iter: 1510 loss: 1.1179809e-06
Iter: 1511 loss: 1.12271755e-06
Iter: 1512 loss: 1.11798636e-06
Iter: 1513 loss: 1.11750364e-06
Iter: 1514 loss: 1.11783379e-06
Iter: 1515 loss: 1.11728218e-06
Iter: 1516 loss: 1.11678969e-06
Iter: 1517 loss: 1.11618863e-06
Iter: 1518 loss: 1.11614474e-06
Iter: 1519 loss: 1.11560644e-06
Iter: 1520 loss: 1.11559029e-06
Iter: 1521 loss: 1.11507632e-06
Iter: 1522 loss: 1.11754207e-06
Iter: 1523 loss: 1.11500162e-06
Iter: 1524 loss: 1.11461384e-06
Iter: 1525 loss: 1.11422469e-06
Iter: 1526 loss: 1.11415375e-06
Iter: 1527 loss: 1.11378631e-06
Iter: 1528 loss: 1.11377881e-06
Iter: 1529 loss: 1.11338238e-06
Iter: 1530 loss: 1.11335248e-06
Iter: 1531 loss: 1.11307713e-06
Iter: 1532 loss: 1.11259965e-06
Iter: 1533 loss: 1.11300574e-06
Iter: 1534 loss: 1.11230577e-06
Iter: 1535 loss: 1.11177224e-06
Iter: 1536 loss: 1.1129282e-06
Iter: 1537 loss: 1.11161978e-06
Iter: 1538 loss: 1.1109837e-06
Iter: 1539 loss: 1.11458689e-06
Iter: 1540 loss: 1.11087854e-06
Iter: 1541 loss: 1.11043676e-06
Iter: 1542 loss: 1.11000213e-06
Iter: 1543 loss: 1.10990982e-06
Iter: 1544 loss: 1.10930102e-06
Iter: 1545 loss: 1.11840984e-06
Iter: 1546 loss: 1.10927e-06
Iter: 1547 loss: 1.1088672e-06
Iter: 1548 loss: 1.10927067e-06
Iter: 1549 loss: 1.1086222e-06
Iter: 1550 loss: 1.10825545e-06
Iter: 1551 loss: 1.1075947e-06
Iter: 1552 loss: 1.1249914e-06
Iter: 1553 loss: 1.10756378e-06
Iter: 1554 loss: 1.10745827e-06
Iter: 1555 loss: 1.10713313e-06
Iter: 1556 loss: 1.10682458e-06
Iter: 1557 loss: 1.10655594e-06
Iter: 1558 loss: 1.10643236e-06
Iter: 1559 loss: 1.10590281e-06
Iter: 1560 loss: 1.10646511e-06
Iter: 1561 loss: 1.10564281e-06
Iter: 1562 loss: 1.10511769e-06
Iter: 1563 loss: 1.10510598e-06
Iter: 1564 loss: 1.10485064e-06
Iter: 1565 loss: 1.10441215e-06
Iter: 1566 loss: 1.10441385e-06
Iter: 1567 loss: 1.10392784e-06
Iter: 1568 loss: 1.10616156e-06
Iter: 1569 loss: 1.10382814e-06
Iter: 1570 loss: 1.10345536e-06
Iter: 1571 loss: 1.10531107e-06
Iter: 1572 loss: 1.10338146e-06
Iter: 1573 loss: 1.10296162e-06
Iter: 1574 loss: 1.10312942e-06
Iter: 1575 loss: 1.10267706e-06
Iter: 1576 loss: 1.10230212e-06
Iter: 1577 loss: 1.10368546e-06
Iter: 1578 loss: 1.1021898e-06
Iter: 1579 loss: 1.10172e-06
Iter: 1580 loss: 1.10285464e-06
Iter: 1581 loss: 1.10157612e-06
Iter: 1582 loss: 1.10112228e-06
Iter: 1583 loss: 1.10099859e-06
Iter: 1584 loss: 1.10068754e-06
Iter: 1585 loss: 1.10015867e-06
Iter: 1586 loss: 1.10164751e-06
Iter: 1587 loss: 1.1000252e-06
Iter: 1588 loss: 1.09957068e-06
Iter: 1589 loss: 1.09958091e-06
Iter: 1590 loss: 1.09923621e-06
Iter: 1591 loss: 1.09861048e-06
Iter: 1592 loss: 1.11380723e-06
Iter: 1593 loss: 1.09860946e-06
Iter: 1594 loss: 1.09854648e-06
Iter: 1595 loss: 1.09836947e-06
Iter: 1596 loss: 1.09815221e-06
Iter: 1597 loss: 1.09774589e-06
Iter: 1598 loss: 1.10725443e-06
Iter: 1599 loss: 1.09775658e-06
Iter: 1600 loss: 1.09720395e-06
Iter: 1601 loss: 1.09771031e-06
Iter: 1602 loss: 1.09695247e-06
Iter: 1603 loss: 1.09640177e-06
Iter: 1604 loss: 1.09919051e-06
Iter: 1605 loss: 1.09628559e-06
Iter: 1606 loss: 1.09573421e-06
Iter: 1607 loss: 1.09750158e-06
Iter: 1608 loss: 1.09562325e-06
Iter: 1609 loss: 1.09510188e-06
Iter: 1610 loss: 1.09581049e-06
Iter: 1611 loss: 1.09482971e-06
Iter: 1612 loss: 1.09444636e-06
Iter: 1613 loss: 1.09621305e-06
Iter: 1614 loss: 1.0943495e-06
Iter: 1615 loss: 1.09383109e-06
Iter: 1616 loss: 1.09449491e-06
Iter: 1617 loss: 1.09358007e-06
Iter: 1618 loss: 1.09320763e-06
Iter: 1619 loss: 1.0928693e-06
Iter: 1620 loss: 1.09276641e-06
Iter: 1621 loss: 1.09227972e-06
Iter: 1622 loss: 1.0922837e-06
Iter: 1623 loss: 1.09187908e-06
Iter: 1624 loss: 1.09418943e-06
Iter: 1625 loss: 1.09181246e-06
Iter: 1626 loss: 1.09158736e-06
Iter: 1627 loss: 1.09141206e-06
Iter: 1628 loss: 1.09133782e-06
Iter: 1629 loss: 1.09091957e-06
Iter: 1630 loss: 1.0955572e-06
Iter: 1631 loss: 1.09090752e-06
Iter: 1632 loss: 1.09069697e-06
Iter: 1633 loss: 1.09020789e-06
Iter: 1634 loss: 1.09596306e-06
Iter: 1635 loss: 1.09019811e-06
Iter: 1636 loss: 1.08962388e-06
Iter: 1637 loss: 1.09448604e-06
Iter: 1638 loss: 1.08957875e-06
Iter: 1639 loss: 1.08918221e-06
Iter: 1640 loss: 1.09070447e-06
Iter: 1641 loss: 1.08905238e-06
Iter: 1642 loss: 1.08862196e-06
Iter: 1643 loss: 1.08902975e-06
Iter: 1644 loss: 1.08838935e-06
Iter: 1645 loss: 1.08793461e-06
Iter: 1646 loss: 1.08977838e-06
Iter: 1647 loss: 1.08787549e-06
Iter: 1648 loss: 1.08756842e-06
Iter: 1649 loss: 1.08957454e-06
Iter: 1650 loss: 1.08752943e-06
Iter: 1651 loss: 1.08720837e-06
Iter: 1652 loss: 1.08668837e-06
Iter: 1653 loss: 1.0867e-06
Iter: 1654 loss: 1.0860814e-06
Iter: 1655 loss: 1.08823861e-06
Iter: 1656 loss: 1.08591632e-06
Iter: 1657 loss: 1.08556821e-06
Iter: 1658 loss: 1.08555992e-06
Iter: 1659 loss: 1.08520271e-06
Iter: 1660 loss: 1.0846843e-06
Iter: 1661 loss: 1.08466224e-06
Iter: 1662 loss: 1.08437496e-06
Iter: 1663 loss: 1.08431686e-06
Iter: 1664 loss: 1.08404834e-06
Iter: 1665 loss: 1.08373058e-06
Iter: 1666 loss: 1.08373399e-06
Iter: 1667 loss: 1.08334734e-06
Iter: 1668 loss: 1.0834101e-06
Iter: 1669 loss: 1.08309166e-06
Iter: 1670 loss: 1.08265078e-06
Iter: 1671 loss: 1.08802783e-06
Iter: 1672 loss: 1.08265351e-06
Iter: 1673 loss: 1.08235542e-06
Iter: 1674 loss: 1.08284871e-06
Iter: 1675 loss: 1.08224697e-06
Iter: 1676 loss: 1.08183667e-06
Iter: 1677 loss: 1.0816899e-06
Iter: 1678 loss: 1.08147651e-06
Iter: 1679 loss: 1.08102131e-06
Iter: 1680 loss: 1.084573e-06
Iter: 1681 loss: 1.08099061e-06
Iter: 1682 loss: 1.08051972e-06
Iter: 1683 loss: 1.08111976e-06
Iter: 1684 loss: 1.08030076e-06
Iter: 1685 loss: 1.07987262e-06
Iter: 1686 loss: 1.08041343e-06
Iter: 1687 loss: 1.07965548e-06
Iter: 1688 loss: 1.07924825e-06
Iter: 1689 loss: 1.08048198e-06
Iter: 1690 loss: 1.07911274e-06
Iter: 1691 loss: 1.07861854e-06
Iter: 1692 loss: 1.0831e-06
Iter: 1693 loss: 1.07860069e-06
Iter: 1694 loss: 1.07842766e-06
Iter: 1695 loss: 1.07871233e-06
Iter: 1696 loss: 1.07832307e-06
Iter: 1697 loss: 1.07798633e-06
Iter: 1698 loss: 1.07808978e-06
Iter: 1699 loss: 1.07774008e-06
Iter: 1700 loss: 1.07738697e-06
Iter: 1701 loss: 1.07722e-06
Iter: 1702 loss: 1.07702874e-06
Iter: 1703 loss: 1.07657195e-06
Iter: 1704 loss: 1.07843721e-06
Iter: 1705 loss: 1.07644792e-06
Iter: 1706 loss: 1.07590654e-06
Iter: 1707 loss: 1.07792505e-06
Iter: 1708 loss: 1.07578182e-06
Iter: 1709 loss: 1.07533253e-06
Iter: 1710 loss: 1.07629955e-06
Iter: 1711 loss: 1.07515734e-06
Iter: 1712 loss: 1.07470203e-06
Iter: 1713 loss: 1.0754361e-06
Iter: 1714 loss: 1.07447181e-06
Iter: 1715 loss: 1.07417043e-06
Iter: 1716 loss: 1.07855226e-06
Iter: 1717 loss: 1.07414473e-06
Iter: 1718 loss: 1.07386177e-06
Iter: 1719 loss: 1.07335802e-06
Iter: 1720 loss: 1.08536517e-06
Iter: 1721 loss: 1.07336746e-06
Iter: 1722 loss: 1.07282244e-06
Iter: 1723 loss: 1.07578467e-06
Iter: 1724 loss: 1.07271717e-06
Iter: 1725 loss: 1.07240214e-06
Iter: 1726 loss: 1.07239816e-06
Iter: 1727 loss: 1.07211247e-06
Iter: 1728 loss: 1.07175038e-06
Iter: 1729 loss: 1.07170331e-06
Iter: 1730 loss: 1.07141841e-06
Iter: 1731 loss: 1.07140374e-06
Iter: 1732 loss: 1.07116716e-06
Iter: 1733 loss: 1.07058054e-06
Iter: 1734 loss: 1.07865014e-06
Iter: 1735 loss: 1.07055848e-06
Iter: 1736 loss: 1.06999119e-06
Iter: 1737 loss: 1.07101528e-06
Iter: 1738 loss: 1.06977723e-06
Iter: 1739 loss: 1.06934715e-06
Iter: 1740 loss: 1.06934931e-06
Iter: 1741 loss: 1.06900802e-06
Iter: 1742 loss: 1.06940888e-06
Iter: 1743 loss: 1.06884477e-06
Iter: 1744 loss: 1.06849689e-06
Iter: 1745 loss: 1.06880964e-06
Iter: 1746 loss: 1.06829111e-06
Iter: 1747 loss: 1.0678566e-06
Iter: 1748 loss: 1.06937682e-06
Iter: 1749 loss: 1.06773098e-06
Iter: 1750 loss: 1.06729112e-06
Iter: 1751 loss: 1.06931623e-06
Iter: 1752 loss: 1.06724974e-06
Iter: 1753 loss: 1.06686389e-06
Iter: 1754 loss: 1.0666156e-06
Iter: 1755 loss: 1.06648486e-06
Iter: 1756 loss: 1.06604341e-06
Iter: 1757 loss: 1.068453e-06
Iter: 1758 loss: 1.06597054e-06
Iter: 1759 loss: 1.06544348e-06
Iter: 1760 loss: 1.06750144e-06
Iter: 1761 loss: 1.06531922e-06
Iter: 1762 loss: 1.06502318e-06
Iter: 1763 loss: 1.06656796e-06
Iter: 1764 loss: 1.06497328e-06
Iter: 1765 loss: 1.06466064e-06
Iter: 1766 loss: 1.06539142e-06
Iter: 1767 loss: 1.06455627e-06
Iter: 1768 loss: 1.06427672e-06
Iter: 1769 loss: 1.06375205e-06
Iter: 1770 loss: 1.07342362e-06
Iter: 1771 loss: 1.06374159e-06
Iter: 1772 loss: 1.06327172e-06
Iter: 1773 loss: 1.06754578e-06
Iter: 1774 loss: 1.063274e-06
Iter: 1775 loss: 1.06278742e-06
Iter: 1776 loss: 1.06458583e-06
Iter: 1777 loss: 1.06267805e-06
Iter: 1778 loss: 1.0623088e-06
Iter: 1779 loss: 1.06295067e-06
Iter: 1780 loss: 1.06215168e-06
Iter: 1781 loss: 1.06175378e-06
Iter: 1782 loss: 1.06198308e-06
Iter: 1783 loss: 1.06150515e-06
Iter: 1784 loss: 1.06091397e-06
Iter: 1785 loss: 1.06335892e-06
Iter: 1786 loss: 1.06078664e-06
Iter: 1787 loss: 1.06028824e-06
Iter: 1788 loss: 1.06234188e-06
Iter: 1789 loss: 1.06018729e-06
Iter: 1790 loss: 1.05979416e-06
Iter: 1791 loss: 1.05948629e-06
Iter: 1792 loss: 1.0593584e-06
Iter: 1793 loss: 1.05915365e-06
Iter: 1794 loss: 1.05906133e-06
Iter: 1795 loss: 1.05881361e-06
Iter: 1796 loss: 1.05850506e-06
Iter: 1797 loss: 1.05845766e-06
Iter: 1798 loss: 1.058198e-06
Iter: 1799 loss: 1.05817389e-06
Iter: 1800 loss: 1.05799006e-06
Iter: 1801 loss: 1.05752292e-06
Iter: 1802 loss: 1.06276661e-06
Iter: 1803 loss: 1.05747336e-06
Iter: 1804 loss: 1.05691356e-06
Iter: 1805 loss: 1.05884192e-06
Iter: 1806 loss: 1.05680886e-06
Iter: 1807 loss: 1.05642391e-06
Iter: 1808 loss: 1.06117864e-06
Iter: 1809 loss: 1.0564188e-06
Iter: 1810 loss: 1.05613617e-06
Iter: 1811 loss: 1.0562253e-06
Iter: 1812 loss: 1.05595325e-06
Iter: 1813 loss: 1.05557422e-06
Iter: 1814 loss: 1.05630568e-06
Iter: 1815 loss: 1.05545507e-06
Iter: 1816 loss: 1.05505615e-06
Iter: 1817 loss: 1.05624144e-06
Iter: 1818 loss: 1.05494189e-06
Iter: 1819 loss: 1.05458753e-06
Iter: 1820 loss: 1.05667539e-06
Iter: 1821 loss: 1.05455229e-06
Iter: 1822 loss: 1.05423896e-06
Iter: 1823 loss: 1.05382151e-06
Iter: 1824 loss: 1.05382958e-06
Iter: 1825 loss: 1.05328832e-06
Iter: 1826 loss: 1.05890228e-06
Iter: 1827 loss: 1.05325978e-06
Iter: 1828 loss: 1.05283925e-06
Iter: 1829 loss: 1.05502477e-06
Iter: 1830 loss: 1.0527724e-06
Iter: 1831 loss: 1.05248114e-06
Iter: 1832 loss: 1.05293464e-06
Iter: 1833 loss: 1.05235586e-06
Iter: 1834 loss: 1.05188428e-06
Iter: 1835 loss: 1.05253321e-06
Iter: 1836 loss: 1.05168579e-06
Iter: 1837 loss: 1.05138884e-06
Iter: 1838 loss: 1.05101913e-06
Iter: 1839 loss: 1.05100662e-06
Iter: 1840 loss: 1.05056722e-06
Iter: 1841 loss: 1.05613231e-06
Iter: 1842 loss: 1.05056e-06
Iter: 1843 loss: 1.05022491e-06
Iter: 1844 loss: 1.05123536e-06
Iter: 1845 loss: 1.05014226e-06
Iter: 1846 loss: 1.0498419e-06
Iter: 1847 loss: 1.04981962e-06
Iter: 1848 loss: 1.04956735e-06
Iter: 1849 loss: 1.04922572e-06
Iter: 1850 loss: 1.05208733e-06
Iter: 1851 loss: 1.0492131e-06
Iter: 1852 loss: 1.04890501e-06
Iter: 1853 loss: 1.04931348e-06
Iter: 1854 loss: 1.04873925e-06
Iter: 1855 loss: 1.04831872e-06
Iter: 1856 loss: 1.04904052e-06
Iter: 1857 loss: 1.04815115e-06
Iter: 1858 loss: 1.04778451e-06
Iter: 1859 loss: 1.04852813e-06
Iter: 1860 loss: 1.04764592e-06
Iter: 1861 loss: 1.04721653e-06
Iter: 1862 loss: 1.05035474e-06
Iter: 1863 loss: 1.04719015e-06
Iter: 1864 loss: 1.04686933e-06
Iter: 1865 loss: 1.047041e-06
Iter: 1866 loss: 1.04663479e-06
Iter: 1867 loss: 1.04630681e-06
Iter: 1868 loss: 1.05064976e-06
Iter: 1869 loss: 1.04630806e-06
Iter: 1870 loss: 1.04608421e-06
Iter: 1871 loss: 1.04559626e-06
Iter: 1872 loss: 1.05112417e-06
Iter: 1873 loss: 1.04554124e-06
Iter: 1874 loss: 1.04518335e-06
Iter: 1875 loss: 1.04870446e-06
Iter: 1876 loss: 1.0451397e-06
Iter: 1877 loss: 1.04476635e-06
Iter: 1878 loss: 1.04698529e-06
Iter: 1879 loss: 1.04472792e-06
Iter: 1880 loss: 1.04444462e-06
Iter: 1881 loss: 1.04433627e-06
Iter: 1882 loss: 1.0441795e-06
Iter: 1883 loss: 1.04376659e-06
Iter: 1884 loss: 1.04537037e-06
Iter: 1885 loss: 1.04369678e-06
Iter: 1886 loss: 1.04330127e-06
Iter: 1887 loss: 1.04478056e-06
Iter: 1888 loss: 1.04321839e-06
Iter: 1889 loss: 1.04291826e-06
Iter: 1890 loss: 1.04409992e-06
Iter: 1891 loss: 1.04283777e-06
Iter: 1892 loss: 1.04254582e-06
Iter: 1893 loss: 1.04255378e-06
Iter: 1894 loss: 1.04233038e-06
Iter: 1895 loss: 1.04208107e-06
Iter: 1896 loss: 1.04205878e-06
Iter: 1897 loss: 1.04181436e-06
Iter: 1898 loss: 1.04174126e-06
Iter: 1899 loss: 1.04158676e-06
Iter: 1900 loss: 1.04136757e-06
Iter: 1901 loss: 1.04137416e-06
Iter: 1902 loss: 1.04118226e-06
Iter: 1903 loss: 1.04083256e-06
Iter: 1904 loss: 1.04638843e-06
Iter: 1905 loss: 1.040808e-06
Iter: 1906 loss: 1.04039646e-06
Iter: 1907 loss: 1.04076355e-06
Iter: 1908 loss: 1.04011531e-06
Iter: 1909 loss: 1.03981324e-06
Iter: 1910 loss: 1.03981574e-06
Iter: 1911 loss: 1.03953585e-06
Iter: 1912 loss: 1.03937521e-06
Iter: 1913 loss: 1.03926141e-06
Iter: 1914 loss: 1.03889715e-06
Iter: 1915 loss: 1.03991147e-06
Iter: 1916 loss: 1.03877733e-06
Iter: 1917 loss: 1.03841853e-06
Iter: 1918 loss: 1.04024821e-06
Iter: 1919 loss: 1.03833383e-06
Iter: 1920 loss: 1.03805837e-06
Iter: 1921 loss: 1.03925106e-06
Iter: 1922 loss: 1.0380013e-06
Iter: 1923 loss: 1.03776415e-06
Iter: 1924 loss: 1.0378559e-06
Iter: 1925 loss: 1.03758509e-06
Iter: 1926 loss: 1.03721732e-06
Iter: 1927 loss: 1.03870661e-06
Iter: 1928 loss: 1.03713887e-06
Iter: 1929 loss: 1.03675973e-06
Iter: 1930 loss: 1.0389931e-06
Iter: 1931 loss: 1.03672983e-06
Iter: 1932 loss: 1.03655691e-06
Iter: 1933 loss: 1.03712478e-06
Iter: 1934 loss: 1.03648404e-06
Iter: 1935 loss: 1.03617799e-06
Iter: 1936 loss: 1.03577986e-06
Iter: 1937 loss: 1.03577827e-06
Iter: 1938 loss: 1.03535945e-06
Iter: 1939 loss: 1.03601792e-06
Iter: 1940 loss: 1.03515254e-06
Iter: 1941 loss: 1.03481943e-06
Iter: 1942 loss: 1.03739046e-06
Iter: 1943 loss: 1.03479806e-06
Iter: 1944 loss: 1.03444108e-06
Iter: 1945 loss: 1.0355468e-06
Iter: 1946 loss: 1.03432535e-06
Iter: 1947 loss: 1.03403431e-06
Iter: 1948 loss: 1.0338365e-06
Iter: 1949 loss: 1.03372372e-06
Iter: 1950 loss: 1.03331149e-06
Iter: 1951 loss: 1.03708862e-06
Iter: 1952 loss: 1.03328568e-06
Iter: 1953 loss: 1.03299828e-06
Iter: 1954 loss: 1.03408354e-06
Iter: 1955 loss: 1.03294622e-06
Iter: 1956 loss: 1.0326371e-06
Iter: 1957 loss: 1.03259083e-06
Iter: 1958 loss: 1.03236232e-06
Iter: 1959 loss: 1.03195475e-06
Iter: 1960 loss: 1.03463435e-06
Iter: 1961 loss: 1.0319277e-06
Iter: 1962 loss: 1.03161028e-06
Iter: 1963 loss: 1.03394905e-06
Iter: 1964 loss: 1.03159505e-06
Iter: 1965 loss: 1.03135073e-06
Iter: 1966 loss: 1.03117372e-06
Iter: 1967 loss: 1.03107595e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2
+ date
Mon Oct 26 10:05:23 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b166eaf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b1667ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b1669dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b16746bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b1675f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b1675f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b165e29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b16570840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b16570598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b16551598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b1650b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b16524730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b165242f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b164dc620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b164409d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b164dc488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b164720d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b164208c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b16420598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b163b9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b163dc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b163dcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b1634c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b1632c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b1632c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b1631af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b162bf7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aec6186a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aec6182f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aec618598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aec5d17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aec5821e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aec582158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aec59d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aec5617b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aec5618c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000222045448
Iter: 2 loss: 0.000183915516
Iter: 3 loss: 0.000183667144
Iter: 4 loss: 0.00016018044
Iter: 5 loss: 0.000310706324
Iter: 6 loss: 0.000157439441
Iter: 7 loss: 0.000141160432
Iter: 8 loss: 0.000114759416
Iter: 9 loss: 0.000114535993
Iter: 10 loss: 8.84457404e-05
Iter: 11 loss: 0.00022089883
Iter: 12 loss: 8.35395113e-05
Iter: 13 loss: 6.36560289e-05
Iter: 14 loss: 0.000357425539
Iter: 15 loss: 6.3617008e-05
Iter: 16 loss: 5.23282652e-05
Iter: 17 loss: 7.55725923e-05
Iter: 18 loss: 4.78497277e-05
Iter: 19 loss: 4.0134044e-05
Iter: 20 loss: 8.99710649e-05
Iter: 21 loss: 3.92397124e-05
Iter: 22 loss: 3.39471699e-05
Iter: 23 loss: 7.07076542e-05
Iter: 24 loss: 3.34444048e-05
Iter: 25 loss: 3.0953066e-05
Iter: 26 loss: 3.54089e-05
Iter: 27 loss: 2.98705891e-05
Iter: 28 loss: 2.71364552e-05
Iter: 29 loss: 4.49354957e-05
Iter: 30 loss: 2.68374752e-05
Iter: 31 loss: 2.50563971e-05
Iter: 32 loss: 2.71699228e-05
Iter: 33 loss: 2.41179805e-05
Iter: 34 loss: 2.17636789e-05
Iter: 35 loss: 3.12458615e-05
Iter: 36 loss: 2.12144078e-05
Iter: 37 loss: 2.14003412e-05
Iter: 38 loss: 2.01110306e-05
Iter: 39 loss: 1.93970955e-05
Iter: 40 loss: 2.04978751e-05
Iter: 41 loss: 1.90605861e-05
Iter: 42 loss: 1.84498585e-05
Iter: 43 loss: 1.76040267e-05
Iter: 44 loss: 1.75672903e-05
Iter: 45 loss: 1.66311656e-05
Iter: 46 loss: 2.10200051e-05
Iter: 47 loss: 1.64594912e-05
Iter: 48 loss: 1.57468676e-05
Iter: 49 loss: 1.85219578e-05
Iter: 50 loss: 1.55807429e-05
Iter: 51 loss: 1.51255681e-05
Iter: 52 loss: 2.23077877e-05
Iter: 53 loss: 1.51255063e-05
Iter: 54 loss: 1.47478131e-05
Iter: 55 loss: 1.43677316e-05
Iter: 56 loss: 1.42919398e-05
Iter: 57 loss: 1.38307623e-05
Iter: 58 loss: 1.69388568e-05
Iter: 59 loss: 1.37845927e-05
Iter: 60 loss: 1.34222637e-05
Iter: 61 loss: 1.46675111e-05
Iter: 62 loss: 1.33262474e-05
Iter: 63 loss: 1.29331356e-05
Iter: 64 loss: 1.28794309e-05
Iter: 65 loss: 1.26021969e-05
Iter: 66 loss: 1.21627618e-05
Iter: 67 loss: 1.67888429e-05
Iter: 68 loss: 1.21514495e-05
Iter: 69 loss: 1.18045045e-05
Iter: 70 loss: 1.19895212e-05
Iter: 71 loss: 1.15761713e-05
Iter: 72 loss: 1.12652724e-05
Iter: 73 loss: 1.40888442e-05
Iter: 74 loss: 1.12512489e-05
Iter: 75 loss: 1.09078364e-05
Iter: 76 loss: 1.22552583e-05
Iter: 77 loss: 1.08285049e-05
Iter: 78 loss: 1.07027599e-05
Iter: 79 loss: 1.04622686e-05
Iter: 80 loss: 1.55914095e-05
Iter: 81 loss: 1.0461049e-05
Iter: 82 loss: 1.0163234e-05
Iter: 83 loss: 1.15166476e-05
Iter: 84 loss: 1.01064697e-05
Iter: 85 loss: 9.88707143e-06
Iter: 86 loss: 1.12895177e-05
Iter: 87 loss: 9.86261875e-06
Iter: 88 loss: 9.6779313e-06
Iter: 89 loss: 1.02150498e-05
Iter: 90 loss: 9.62079503e-06
Iter: 91 loss: 9.38848825e-06
Iter: 92 loss: 9.65133768e-06
Iter: 93 loss: 9.26369557e-06
Iter: 94 loss: 9.09398295e-06
Iter: 95 loss: 9.40067366e-06
Iter: 96 loss: 9.02051e-06
Iter: 97 loss: 8.82170389e-06
Iter: 98 loss: 9.95051869e-06
Iter: 99 loss: 8.79429263e-06
Iter: 100 loss: 8.63969308e-06
Iter: 101 loss: 8.76642844e-06
Iter: 102 loss: 8.5473e-06
Iter: 103 loss: 8.40407574e-06
Iter: 104 loss: 9.55986252e-06
Iter: 105 loss: 8.39471613e-06
Iter: 106 loss: 8.26705582e-06
Iter: 107 loss: 8.26155883e-06
Iter: 108 loss: 8.16320153e-06
Iter: 109 loss: 8.02921568e-06
Iter: 110 loss: 9.73016722e-06
Iter: 111 loss: 8.02803333e-06
Iter: 112 loss: 7.90205922e-06
Iter: 113 loss: 8.49618573e-06
Iter: 114 loss: 7.87907629e-06
Iter: 115 loss: 7.82294228e-06
Iter: 116 loss: 7.68100654e-06
Iter: 117 loss: 8.96656e-06
Iter: 118 loss: 7.65965342e-06
Iter: 119 loss: 7.51504285e-06
Iter: 120 loss: 8.73224417e-06
Iter: 121 loss: 7.50687877e-06
Iter: 122 loss: 7.40805672e-06
Iter: 123 loss: 7.80780283e-06
Iter: 124 loss: 7.38606468e-06
Iter: 125 loss: 7.27716451e-06
Iter: 126 loss: 7.77285e-06
Iter: 127 loss: 7.25645077e-06
Iter: 128 loss: 7.17056537e-06
Iter: 129 loss: 7.43245255e-06
Iter: 130 loss: 7.14497037e-06
Iter: 131 loss: 7.06038236e-06
Iter: 132 loss: 7.05614548e-06
Iter: 133 loss: 6.99164957e-06
Iter: 134 loss: 6.89739772e-06
Iter: 135 loss: 7.45376747e-06
Iter: 136 loss: 6.88511818e-06
Iter: 137 loss: 6.79205459e-06
Iter: 138 loss: 7.02256602e-06
Iter: 139 loss: 6.75919773e-06
Iter: 140 loss: 6.68274288e-06
Iter: 141 loss: 6.8812551e-06
Iter: 142 loss: 6.65681546e-06
Iter: 143 loss: 6.58113322e-06
Iter: 144 loss: 6.80075436e-06
Iter: 145 loss: 6.55787608e-06
Iter: 146 loss: 6.48097557e-06
Iter: 147 loss: 6.83116923e-06
Iter: 148 loss: 6.46635363e-06
Iter: 149 loss: 6.43810336e-06
Iter: 150 loss: 6.43166e-06
Iter: 151 loss: 6.41146835e-06
Iter: 152 loss: 6.34721164e-06
Iter: 153 loss: 6.43380145e-06
Iter: 154 loss: 6.3001271e-06
Iter: 155 loss: 6.21967502e-06
Iter: 156 loss: 6.73371505e-06
Iter: 157 loss: 6.21078e-06
Iter: 158 loss: 6.1360729e-06
Iter: 159 loss: 6.36170626e-06
Iter: 160 loss: 6.11359155e-06
Iter: 161 loss: 6.05896048e-06
Iter: 162 loss: 6.0589291e-06
Iter: 163 loss: 6.01984539e-06
Iter: 164 loss: 6.05475452e-06
Iter: 165 loss: 5.99711e-06
Iter: 166 loss: 5.95406527e-06
Iter: 167 loss: 6.01243846e-06
Iter: 168 loss: 5.9324052e-06
Iter: 169 loss: 5.87701061e-06
Iter: 170 loss: 5.95047732e-06
Iter: 171 loss: 5.84878035e-06
Iter: 172 loss: 5.80113192e-06
Iter: 173 loss: 6.24438599e-06
Iter: 174 loss: 5.79904463e-06
Iter: 175 loss: 5.75742342e-06
Iter: 176 loss: 5.75555441e-06
Iter: 177 loss: 5.72361569e-06
Iter: 178 loss: 5.66877679e-06
Iter: 179 loss: 5.8501646e-06
Iter: 180 loss: 5.65360597e-06
Iter: 181 loss: 5.61250727e-06
Iter: 182 loss: 6.00208568e-06
Iter: 183 loss: 5.61097613e-06
Iter: 184 loss: 5.58118245e-06
Iter: 185 loss: 5.849849e-06
Iter: 186 loss: 5.57968906e-06
Iter: 187 loss: 5.55198085e-06
Iter: 188 loss: 5.52263e-06
Iter: 189 loss: 5.51773883e-06
Iter: 190 loss: 5.48642765e-06
Iter: 191 loss: 5.47463105e-06
Iter: 192 loss: 5.45735429e-06
Iter: 193 loss: 5.411036e-06
Iter: 194 loss: 5.53575364e-06
Iter: 195 loss: 5.39559142e-06
Iter: 196 loss: 5.3559e-06
Iter: 197 loss: 5.64849188e-06
Iter: 198 loss: 5.35266508e-06
Iter: 199 loss: 5.31601609e-06
Iter: 200 loss: 5.53700374e-06
Iter: 201 loss: 5.31145588e-06
Iter: 202 loss: 5.28635428e-06
Iter: 203 loss: 5.26718213e-06
Iter: 204 loss: 5.25930727e-06
Iter: 205 loss: 5.22294931e-06
Iter: 206 loss: 5.49567903e-06
Iter: 207 loss: 5.22001801e-06
Iter: 208 loss: 5.19176592e-06
Iter: 209 loss: 5.19725381e-06
Iter: 210 loss: 5.17074477e-06
Iter: 211 loss: 5.13086479e-06
Iter: 212 loss: 5.40217661e-06
Iter: 213 loss: 5.12703082e-06
Iter: 214 loss: 5.10232212e-06
Iter: 215 loss: 5.14315525e-06
Iter: 216 loss: 5.09113852e-06
Iter: 217 loss: 5.06250717e-06
Iter: 218 loss: 5.09164e-06
Iter: 219 loss: 5.04640502e-06
Iter: 220 loss: 5.02536477e-06
Iter: 221 loss: 5.02338298e-06
Iter: 222 loss: 5.006e-06
Iter: 223 loss: 5.0078811e-06
Iter: 224 loss: 4.99257931e-06
Iter: 225 loss: 4.96841858e-06
Iter: 226 loss: 4.93038397e-06
Iter: 227 loss: 4.9299183e-06
Iter: 228 loss: 4.89554395e-06
Iter: 229 loss: 5.04256741e-06
Iter: 230 loss: 4.88839169e-06
Iter: 231 loss: 4.85557393e-06
Iter: 232 loss: 4.92877e-06
Iter: 233 loss: 4.84317206e-06
Iter: 234 loss: 4.82334508e-06
Iter: 235 loss: 4.82324867e-06
Iter: 236 loss: 4.80360677e-06
Iter: 237 loss: 4.8074794e-06
Iter: 238 loss: 4.78901075e-06
Iter: 239 loss: 4.77012418e-06
Iter: 240 loss: 4.77759886e-06
Iter: 241 loss: 4.75718252e-06
Iter: 242 loss: 4.72874854e-06
Iter: 243 loss: 4.86113731e-06
Iter: 244 loss: 4.72343891e-06
Iter: 245 loss: 4.70317809e-06
Iter: 246 loss: 4.8097304e-06
Iter: 247 loss: 4.69989027e-06
Iter: 248 loss: 4.68254439e-06
Iter: 249 loss: 4.70511395e-06
Iter: 250 loss: 4.67376367e-06
Iter: 251 loss: 4.64974664e-06
Iter: 252 loss: 4.66179699e-06
Iter: 253 loss: 4.63381e-06
Iter: 254 loss: 4.61426816e-06
Iter: 255 loss: 4.91019182e-06
Iter: 256 loss: 4.61433e-06
Iter: 257 loss: 4.59587682e-06
Iter: 258 loss: 4.67937298e-06
Iter: 259 loss: 4.59236071e-06
Iter: 260 loss: 4.5791985e-06
Iter: 261 loss: 4.57373335e-06
Iter: 262 loss: 4.5667648e-06
Iter: 263 loss: 4.5465431e-06
Iter: 264 loss: 4.54356359e-06
Iter: 265 loss: 4.52949507e-06
Iter: 266 loss: 4.50652806e-06
Iter: 267 loss: 4.54761448e-06
Iter: 268 loss: 4.49655e-06
Iter: 269 loss: 4.47105049e-06
Iter: 270 loss: 4.58142767e-06
Iter: 271 loss: 4.46604463e-06
Iter: 272 loss: 4.45176101e-06
Iter: 273 loss: 4.6744035e-06
Iter: 274 loss: 4.4518024e-06
Iter: 275 loss: 4.43637464e-06
Iter: 276 loss: 4.4158196e-06
Iter: 277 loss: 4.41468273e-06
Iter: 278 loss: 4.39311771e-06
Iter: 279 loss: 4.49419804e-06
Iter: 280 loss: 4.38919324e-06
Iter: 281 loss: 4.37076551e-06
Iter: 282 loss: 4.41803968e-06
Iter: 283 loss: 4.36439268e-06
Iter: 284 loss: 4.3426835e-06
Iter: 285 loss: 4.41727434e-06
Iter: 286 loss: 4.33657806e-06
Iter: 287 loss: 4.3181235e-06
Iter: 288 loss: 4.36583377e-06
Iter: 289 loss: 4.3118207e-06
Iter: 290 loss: 4.29450574e-06
Iter: 291 loss: 4.36123128e-06
Iter: 292 loss: 4.29052398e-06
Iter: 293 loss: 4.27788564e-06
Iter: 294 loss: 4.37434483e-06
Iter: 295 loss: 4.27697069e-06
Iter: 296 loss: 4.26087172e-06
Iter: 297 loss: 4.264788e-06
Iter: 298 loss: 4.24907648e-06
Iter: 299 loss: 4.23777055e-06
Iter: 300 loss: 4.27517716e-06
Iter: 301 loss: 4.23457504e-06
Iter: 302 loss: 4.22329231e-06
Iter: 303 loss: 4.20808738e-06
Iter: 304 loss: 4.20720789e-06
Iter: 305 loss: 4.18826494e-06
Iter: 306 loss: 4.2735287e-06
Iter: 307 loss: 4.18459786e-06
Iter: 308 loss: 4.16652892e-06
Iter: 309 loss: 4.18628497e-06
Iter: 310 loss: 4.15671093e-06
Iter: 311 loss: 4.14983515e-06
Iter: 312 loss: 4.1466842e-06
Iter: 313 loss: 4.13870339e-06
Iter: 314 loss: 4.12361624e-06
Iter: 315 loss: 4.44644547e-06
Iter: 316 loss: 4.1235744e-06
Iter: 317 loss: 4.10681969e-06
Iter: 318 loss: 4.16201647e-06
Iter: 319 loss: 4.1023095e-06
Iter: 320 loss: 4.08803362e-06
Iter: 321 loss: 4.19999469e-06
Iter: 322 loss: 4.08702726e-06
Iter: 323 loss: 4.07509333e-06
Iter: 324 loss: 4.09950462e-06
Iter: 325 loss: 4.07039397e-06
Iter: 326 loss: 4.05795436e-06
Iter: 327 loss: 4.07412244e-06
Iter: 328 loss: 4.05161518e-06
Iter: 329 loss: 4.03755166e-06
Iter: 330 loss: 4.12123063e-06
Iter: 331 loss: 4.03581862e-06
Iter: 332 loss: 4.02591832e-06
Iter: 333 loss: 4.181843e-06
Iter: 334 loss: 4.02590967e-06
Iter: 335 loss: 4.01994248e-06
Iter: 336 loss: 4.00792396e-06
Iter: 337 loss: 4.22974517e-06
Iter: 338 loss: 4.00774161e-06
Iter: 339 loss: 3.99388136e-06
Iter: 340 loss: 4.02875685e-06
Iter: 341 loss: 3.98915654e-06
Iter: 342 loss: 3.97464237e-06
Iter: 343 loss: 4.02162595e-06
Iter: 344 loss: 3.97070835e-06
Iter: 345 loss: 3.95892812e-06
Iter: 346 loss: 3.9670922e-06
Iter: 347 loss: 3.95149164e-06
Iter: 348 loss: 3.93756454e-06
Iter: 349 loss: 3.95643156e-06
Iter: 350 loss: 3.93045548e-06
Iter: 351 loss: 3.92348375e-06
Iter: 352 loss: 3.92223592e-06
Iter: 353 loss: 3.91390495e-06
Iter: 354 loss: 3.90351306e-06
Iter: 355 loss: 3.90268906e-06
Iter: 356 loss: 3.89189472e-06
Iter: 357 loss: 3.93079836e-06
Iter: 358 loss: 3.8892058e-06
Iter: 359 loss: 3.87831369e-06
Iter: 360 loss: 3.8957005e-06
Iter: 361 loss: 3.87325053e-06
Iter: 362 loss: 3.86201827e-06
Iter: 363 loss: 3.98673774e-06
Iter: 364 loss: 3.86174679e-06
Iter: 365 loss: 3.85473095e-06
Iter: 366 loss: 3.85081421e-06
Iter: 367 loss: 3.84772875e-06
Iter: 368 loss: 3.83747283e-06
Iter: 369 loss: 3.98125758e-06
Iter: 370 loss: 3.83743827e-06
Iter: 371 loss: 3.82862072e-06
Iter: 372 loss: 3.84507439e-06
Iter: 373 loss: 3.8248445e-06
Iter: 374 loss: 3.81765722e-06
Iter: 375 loss: 3.8082776e-06
Iter: 376 loss: 3.80770848e-06
Iter: 377 loss: 3.79627204e-06
Iter: 378 loss: 3.86216743e-06
Iter: 379 loss: 3.79458834e-06
Iter: 380 loss: 3.78489585e-06
Iter: 381 loss: 3.80462029e-06
Iter: 382 loss: 3.78100594e-06
Iter: 383 loss: 3.77130118e-06
Iter: 384 loss: 3.78664936e-06
Iter: 385 loss: 3.76672824e-06
Iter: 386 loss: 3.75577406e-06
Iter: 387 loss: 3.75380591e-06
Iter: 388 loss: 3.74638785e-06
Iter: 389 loss: 3.73796593e-06
Iter: 390 loss: 3.73716853e-06
Iter: 391 loss: 3.72837781e-06
Iter: 392 loss: 3.73102944e-06
Iter: 393 loss: 3.72207887e-06
Iter: 394 loss: 3.71172678e-06
Iter: 395 loss: 3.72329623e-06
Iter: 396 loss: 3.70621365e-06
Iter: 397 loss: 3.69734289e-06
Iter: 398 loss: 3.71665578e-06
Iter: 399 loss: 3.69390318e-06
Iter: 400 loss: 3.68236442e-06
Iter: 401 loss: 3.75018294e-06
Iter: 402 loss: 3.6807869e-06
Iter: 403 loss: 3.67332177e-06
Iter: 404 loss: 3.71161e-06
Iter: 405 loss: 3.67215e-06
Iter: 406 loss: 3.6642607e-06
Iter: 407 loss: 3.70425869e-06
Iter: 408 loss: 3.66291852e-06
Iter: 409 loss: 3.65822279e-06
Iter: 410 loss: 3.65269489e-06
Iter: 411 loss: 3.65207188e-06
Iter: 412 loss: 3.64287462e-06
Iter: 413 loss: 3.65573601e-06
Iter: 414 loss: 3.63852405e-06
Iter: 415 loss: 3.62962192e-06
Iter: 416 loss: 3.66033328e-06
Iter: 417 loss: 3.62734136e-06
Iter: 418 loss: 3.618562e-06
Iter: 419 loss: 3.63952063e-06
Iter: 420 loss: 3.61543e-06
Iter: 421 loss: 3.60581453e-06
Iter: 422 loss: 3.62415881e-06
Iter: 423 loss: 3.60163085e-06
Iter: 424 loss: 3.59289629e-06
Iter: 425 loss: 3.59700198e-06
Iter: 426 loss: 3.58686816e-06
Iter: 427 loss: 3.57825479e-06
Iter: 428 loss: 3.68225346e-06
Iter: 429 loss: 3.57807971e-06
Iter: 430 loss: 3.57091676e-06
Iter: 431 loss: 3.6155443e-06
Iter: 432 loss: 3.57008344e-06
Iter: 433 loss: 3.56464807e-06
Iter: 434 loss: 3.56374926e-06
Iter: 435 loss: 3.5601513e-06
Iter: 436 loss: 3.55307816e-06
Iter: 437 loss: 3.55106977e-06
Iter: 438 loss: 3.54679105e-06
Iter: 439 loss: 3.53968608e-06
Iter: 440 loss: 3.53940823e-06
Iter: 441 loss: 3.53457835e-06
Iter: 442 loss: 3.54628219e-06
Iter: 443 loss: 3.53283622e-06
Iter: 444 loss: 3.52525376e-06
Iter: 445 loss: 3.52846541e-06
Iter: 446 loss: 3.5201133e-06
Iter: 447 loss: 3.5140215e-06
Iter: 448 loss: 3.52025427e-06
Iter: 449 loss: 3.51064364e-06
Iter: 450 loss: 3.50464325e-06
Iter: 451 loss: 3.5132789e-06
Iter: 452 loss: 3.5015828e-06
Iter: 453 loss: 3.4926959e-06
Iter: 454 loss: 3.50119717e-06
Iter: 455 loss: 3.48756521e-06
Iter: 456 loss: 3.4798195e-06
Iter: 457 loss: 3.52427537e-06
Iter: 458 loss: 3.47872219e-06
Iter: 459 loss: 3.47151899e-06
Iter: 460 loss: 3.48625258e-06
Iter: 461 loss: 3.46860429e-06
Iter: 462 loss: 3.46174261e-06
Iter: 463 loss: 3.46989736e-06
Iter: 464 loss: 3.45807e-06
Iter: 465 loss: 3.45004582e-06
Iter: 466 loss: 3.45725539e-06
Iter: 467 loss: 3.44552336e-06
Iter: 468 loss: 3.43620923e-06
Iter: 469 loss: 3.50871778e-06
Iter: 470 loss: 3.43548891e-06
Iter: 471 loss: 3.42817907e-06
Iter: 472 loss: 3.50522487e-06
Iter: 473 loss: 3.42794692e-06
Iter: 474 loss: 3.42422413e-06
Iter: 475 loss: 3.41779241e-06
Iter: 476 loss: 3.41784198e-06
Iter: 477 loss: 3.41027044e-06
Iter: 478 loss: 3.47257492e-06
Iter: 479 loss: 3.40979705e-06
Iter: 480 loss: 3.40377687e-06
Iter: 481 loss: 3.43042962e-06
Iter: 482 loss: 3.40264273e-06
Iter: 483 loss: 3.39815551e-06
Iter: 484 loss: 3.43455781e-06
Iter: 485 loss: 3.39785311e-06
Iter: 486 loss: 3.39274879e-06
Iter: 487 loss: 3.38978884e-06
Iter: 488 loss: 3.38757218e-06
Iter: 489 loss: 3.3828785e-06
Iter: 490 loss: 3.38047812e-06
Iter: 491 loss: 3.37822621e-06
Iter: 492 loss: 3.37098481e-06
Iter: 493 loss: 3.4163204e-06
Iter: 494 loss: 3.3702172e-06
Iter: 495 loss: 3.36385756e-06
Iter: 496 loss: 3.36550579e-06
Iter: 497 loss: 3.35915183e-06
Iter: 498 loss: 3.35260529e-06
Iter: 499 loss: 3.39777671e-06
Iter: 500 loss: 3.35191908e-06
Iter: 501 loss: 3.34632205e-06
Iter: 502 loss: 3.35152185e-06
Iter: 503 loss: 3.34300557e-06
Iter: 504 loss: 3.33629646e-06
Iter: 505 loss: 3.36198059e-06
Iter: 506 loss: 3.33465641e-06
Iter: 507 loss: 3.32902482e-06
Iter: 508 loss: 3.33308935e-06
Iter: 509 loss: 3.32543118e-06
Iter: 510 loss: 3.31999854e-06
Iter: 511 loss: 3.31981028e-06
Iter: 512 loss: 3.31691e-06
Iter: 513 loss: 3.31044043e-06
Iter: 514 loss: 3.40747238e-06
Iter: 515 loss: 3.31021511e-06
Iter: 516 loss: 3.3036124e-06
Iter: 517 loss: 3.3746644e-06
Iter: 518 loss: 3.30346074e-06
Iter: 519 loss: 3.29856266e-06
Iter: 520 loss: 3.34673814e-06
Iter: 521 loss: 3.29835848e-06
Iter: 522 loss: 3.29452496e-06
Iter: 523 loss: 3.30386615e-06
Iter: 524 loss: 3.29307636e-06
Iter: 525 loss: 3.28918804e-06
Iter: 526 loss: 3.28344549e-06
Iter: 527 loss: 3.28328952e-06
Iter: 528 loss: 3.27790508e-06
Iter: 529 loss: 3.28185797e-06
Iter: 530 loss: 3.27447924e-06
Iter: 531 loss: 3.26936333e-06
Iter: 532 loss: 3.35090817e-06
Iter: 533 loss: 3.26937788e-06
Iter: 534 loss: 3.26524969e-06
Iter: 535 loss: 3.26208e-06
Iter: 536 loss: 3.26071836e-06
Iter: 537 loss: 3.25387327e-06
Iter: 538 loss: 3.28053375e-06
Iter: 539 loss: 3.25233464e-06
Iter: 540 loss: 3.24641292e-06
Iter: 541 loss: 3.25897099e-06
Iter: 542 loss: 3.24423036e-06
Iter: 543 loss: 3.23744712e-06
Iter: 544 loss: 3.26614509e-06
Iter: 545 loss: 3.23609424e-06
Iter: 546 loss: 3.23123663e-06
Iter: 547 loss: 3.25122664e-06
Iter: 548 loss: 3.23015547e-06
Iter: 549 loss: 3.22531105e-06
Iter: 550 loss: 3.25394672e-06
Iter: 551 loss: 3.22464439e-06
Iter: 552 loss: 3.22090364e-06
Iter: 553 loss: 3.21476818e-06
Iter: 554 loss: 3.21476432e-06
Iter: 555 loss: 3.21490643e-06
Iter: 556 loss: 3.21171228e-06
Iter: 557 loss: 3.20911158e-06
Iter: 558 loss: 3.2066223e-06
Iter: 559 loss: 3.206153e-06
Iter: 560 loss: 3.20179402e-06
Iter: 561 loss: 3.20774075e-06
Iter: 562 loss: 3.19961259e-06
Iter: 563 loss: 3.195099e-06
Iter: 564 loss: 3.2003004e-06
Iter: 565 loss: 3.19272021e-06
Iter: 566 loss: 3.18832713e-06
Iter: 567 loss: 3.18429943e-06
Iter: 568 loss: 3.1832476e-06
Iter: 569 loss: 3.1764323e-06
Iter: 570 loss: 3.24966959e-06
Iter: 571 loss: 3.17630702e-06
Iter: 572 loss: 3.17102854e-06
Iter: 573 loss: 3.19178775e-06
Iter: 574 loss: 3.16975502e-06
Iter: 575 loss: 3.16476962e-06
Iter: 576 loss: 3.16441265e-06
Iter: 577 loss: 3.16071282e-06
Iter: 578 loss: 3.15507123e-06
Iter: 579 loss: 3.17644481e-06
Iter: 580 loss: 3.15371358e-06
Iter: 581 loss: 3.14911267e-06
Iter: 582 loss: 3.19412834e-06
Iter: 583 loss: 3.14895215e-06
Iter: 584 loss: 3.14496697e-06
Iter: 585 loss: 3.15185616e-06
Iter: 586 loss: 3.1431307e-06
Iter: 587 loss: 3.13867508e-06
Iter: 588 loss: 3.15842226e-06
Iter: 589 loss: 3.13776013e-06
Iter: 590 loss: 3.13530063e-06
Iter: 591 loss: 3.1569457e-06
Iter: 592 loss: 3.1352165e-06
Iter: 593 loss: 3.132232e-06
Iter: 594 loss: 3.12817929e-06
Iter: 595 loss: 3.12795692e-06
Iter: 596 loss: 3.12350085e-06
Iter: 597 loss: 3.13387136e-06
Iter: 598 loss: 3.12185875e-06
Iter: 599 loss: 3.11817689e-06
Iter: 600 loss: 3.1437794e-06
Iter: 601 loss: 3.11788676e-06
Iter: 602 loss: 3.11482745e-06
Iter: 603 loss: 3.10867449e-06
Iter: 604 loss: 3.22774167e-06
Iter: 605 loss: 3.10864334e-06
Iter: 606 loss: 3.10168502e-06
Iter: 607 loss: 3.12970451e-06
Iter: 608 loss: 3.10017685e-06
Iter: 609 loss: 3.09441384e-06
Iter: 610 loss: 3.13141163e-06
Iter: 611 loss: 3.09383563e-06
Iter: 612 loss: 3.08964673e-06
Iter: 613 loss: 3.11658277e-06
Iter: 614 loss: 3.0892179e-06
Iter: 615 loss: 3.08548692e-06
Iter: 616 loss: 3.08554945e-06
Iter: 617 loss: 3.08259928e-06
Iter: 618 loss: 3.07726214e-06
Iter: 619 loss: 3.07955042e-06
Iter: 620 loss: 3.07367668e-06
Iter: 621 loss: 3.06979746e-06
Iter: 622 loss: 3.06962352e-06
Iter: 623 loss: 3.06602101e-06
Iter: 624 loss: 3.07210939e-06
Iter: 625 loss: 3.06441734e-06
Iter: 626 loss: 3.06118227e-06
Iter: 627 loss: 3.08130325e-06
Iter: 628 loss: 3.06072479e-06
Iter: 629 loss: 3.05695471e-06
Iter: 630 loss: 3.06421589e-06
Iter: 631 loss: 3.05545336e-06
Iter: 632 loss: 3.0527633e-06
Iter: 633 loss: 3.05210915e-06
Iter: 634 loss: 3.05037815e-06
Iter: 635 loss: 3.04676769e-06
Iter: 636 loss: 3.05558024e-06
Iter: 637 loss: 3.04543437e-06
Iter: 638 loss: 3.04170317e-06
Iter: 639 loss: 3.05213462e-06
Iter: 640 loss: 3.04055e-06
Iter: 641 loss: 3.03583101e-06
Iter: 642 loss: 3.04060904e-06
Iter: 643 loss: 3.03324396e-06
Iter: 644 loss: 3.02936405e-06
Iter: 645 loss: 3.02665239e-06
Iter: 646 loss: 3.02533135e-06
Iter: 647 loss: 3.01887439e-06
Iter: 648 loss: 3.06404104e-06
Iter: 649 loss: 3.01831028e-06
Iter: 650 loss: 3.01395721e-06
Iter: 651 loss: 3.02596504e-06
Iter: 652 loss: 3.01257387e-06
Iter: 653 loss: 3.00765578e-06
Iter: 654 loss: 3.03858178e-06
Iter: 655 loss: 3.0071219e-06
Iter: 656 loss: 3.00407987e-06
Iter: 657 loss: 3.00020156e-06
Iter: 658 loss: 2.99981821e-06
Iter: 659 loss: 2.99387602e-06
Iter: 660 loss: 3.02216722e-06
Iter: 661 loss: 2.99277895e-06
Iter: 662 loss: 2.98927694e-06
Iter: 663 loss: 2.98928944e-06
Iter: 664 loss: 2.98631949e-06
Iter: 665 loss: 2.99270096e-06
Iter: 666 loss: 2.98511759e-06
Iter: 667 loss: 2.98260625e-06
Iter: 668 loss: 3.01033378e-06
Iter: 669 loss: 2.98254054e-06
Iter: 670 loss: 2.98049645e-06
Iter: 671 loss: 2.97544375e-06
Iter: 672 loss: 3.02834565e-06
Iter: 673 loss: 2.97482961e-06
Iter: 674 loss: 2.97119959e-06
Iter: 675 loss: 2.99374506e-06
Iter: 676 loss: 2.97081442e-06
Iter: 677 loss: 2.96681401e-06
Iter: 678 loss: 2.97441989e-06
Iter: 679 loss: 2.96512e-06
Iter: 680 loss: 2.96159851e-06
Iter: 681 loss: 2.98087571e-06
Iter: 682 loss: 2.961051e-06
Iter: 683 loss: 2.95830978e-06
Iter: 684 loss: 2.96100029e-06
Iter: 685 loss: 2.95673e-06
Iter: 686 loss: 2.95301743e-06
Iter: 687 loss: 2.9562525e-06
Iter: 688 loss: 2.95098562e-06
Iter: 689 loss: 2.94633901e-06
Iter: 690 loss: 2.94956953e-06
Iter: 691 loss: 2.94335632e-06
Iter: 692 loss: 2.938134e-06
Iter: 693 loss: 2.96300277e-06
Iter: 694 loss: 2.93712333e-06
Iter: 695 loss: 2.9333205e-06
Iter: 696 loss: 2.9796829e-06
Iter: 697 loss: 2.93329367e-06
Iter: 698 loss: 2.93043377e-06
Iter: 699 loss: 2.92933555e-06
Iter: 700 loss: 2.92772688e-06
Iter: 701 loss: 2.92336017e-06
Iter: 702 loss: 2.92835739e-06
Iter: 703 loss: 2.92097093e-06
Iter: 704 loss: 2.92023083e-06
Iter: 705 loss: 2.91895958e-06
Iter: 706 loss: 2.91682454e-06
Iter: 707 loss: 2.91432843e-06
Iter: 708 loss: 2.9140424e-06
Iter: 709 loss: 2.91007927e-06
Iter: 710 loss: 2.93306039e-06
Iter: 711 loss: 2.90955631e-06
Iter: 712 loss: 2.90738444e-06
Iter: 713 loss: 2.90355115e-06
Iter: 714 loss: 2.99717726e-06
Iter: 715 loss: 2.90356093e-06
Iter: 716 loss: 2.8989034e-06
Iter: 717 loss: 2.90937669e-06
Iter: 718 loss: 2.89717809e-06
Iter: 719 loss: 2.89265927e-06
Iter: 720 loss: 2.90908019e-06
Iter: 721 loss: 2.89153331e-06
Iter: 722 loss: 2.88714614e-06
Iter: 723 loss: 2.93075914e-06
Iter: 724 loss: 2.88701676e-06
Iter: 725 loss: 2.88419733e-06
Iter: 726 loss: 2.88183355e-06
Iter: 727 loss: 2.88107208e-06
Iter: 728 loss: 2.87694911e-06
Iter: 729 loss: 2.90304502e-06
Iter: 730 loss: 2.87643752e-06
Iter: 731 loss: 2.87297371e-06
Iter: 732 loss: 2.87836019e-06
Iter: 733 loss: 2.87136527e-06
Iter: 734 loss: 2.86748832e-06
Iter: 735 loss: 2.87344e-06
Iter: 736 loss: 2.86572367e-06
Iter: 737 loss: 2.86211161e-06
Iter: 738 loss: 2.86798513e-06
Iter: 739 loss: 2.8603713e-06
Iter: 740 loss: 2.8569284e-06
Iter: 741 loss: 2.89459581e-06
Iter: 742 loss: 2.8568893e-06
Iter: 743 loss: 2.85371e-06
Iter: 744 loss: 2.85978922e-06
Iter: 745 loss: 2.8523375e-06
Iter: 746 loss: 2.85058695e-06
Iter: 747 loss: 2.8505126e-06
Iter: 748 loss: 2.84889143e-06
Iter: 749 loss: 2.845567e-06
Iter: 750 loss: 2.89884656e-06
Iter: 751 loss: 2.84549037e-06
Iter: 752 loss: 2.84240832e-06
Iter: 753 loss: 2.87795956e-06
Iter: 754 loss: 2.84233e-06
Iter: 755 loss: 2.84041334e-06
Iter: 756 loss: 2.83896543e-06
Iter: 757 loss: 2.8383547e-06
Iter: 758 loss: 2.83481404e-06
Iter: 759 loss: 2.83039162e-06
Iter: 760 loss: 2.83003783e-06
Iter: 761 loss: 2.82739802e-06
Iter: 762 loss: 2.82681776e-06
Iter: 763 loss: 2.82387487e-06
Iter: 764 loss: 2.82237647e-06
Iter: 765 loss: 2.82095789e-06
Iter: 766 loss: 2.81787e-06
Iter: 767 loss: 2.82203519e-06
Iter: 768 loss: 2.81636858e-06
Iter: 769 loss: 2.81277016e-06
Iter: 770 loss: 2.82763835e-06
Iter: 771 loss: 2.81201847e-06
Iter: 772 loss: 2.80801441e-06
Iter: 773 loss: 2.81727e-06
Iter: 774 loss: 2.80660538e-06
Iter: 775 loss: 2.8032739e-06
Iter: 776 loss: 2.81750818e-06
Iter: 777 loss: 2.80261679e-06
Iter: 778 loss: 2.79977894e-06
Iter: 779 loss: 2.79833421e-06
Iter: 780 loss: 2.79699361e-06
Iter: 781 loss: 2.79732603e-06
Iter: 782 loss: 2.79496726e-06
Iter: 783 loss: 2.7936685e-06
Iter: 784 loss: 2.79252299e-06
Iter: 785 loss: 2.79214464e-06
Iter: 786 loss: 2.78979178e-06
Iter: 787 loss: 2.79045753e-06
Iter: 788 loss: 2.78806328e-06
Iter: 789 loss: 2.7849278e-06
Iter: 790 loss: 2.79786468e-06
Iter: 791 loss: 2.78426046e-06
Iter: 792 loss: 2.78174207e-06
Iter: 793 loss: 2.78901871e-06
Iter: 794 loss: 2.78093739e-06
Iter: 795 loss: 2.77858248e-06
Iter: 796 loss: 2.77696449e-06
Iter: 797 loss: 2.77605659e-06
Iter: 798 loss: 2.77241293e-06
Iter: 799 loss: 2.77881099e-06
Iter: 800 loss: 2.77085883e-06
Iter: 801 loss: 2.76787523e-06
Iter: 802 loss: 2.81322923e-06
Iter: 803 loss: 2.76789478e-06
Iter: 804 loss: 2.76496371e-06
Iter: 805 loss: 2.76315905e-06
Iter: 806 loss: 2.76193396e-06
Iter: 807 loss: 2.75837988e-06
Iter: 808 loss: 2.76872015e-06
Iter: 809 loss: 2.75723892e-06
Iter: 810 loss: 2.75380489e-06
Iter: 811 loss: 2.75489538e-06
Iter: 812 loss: 2.75120283e-06
Iter: 813 loss: 2.7472e-06
Iter: 814 loss: 2.76267565e-06
Iter: 815 loss: 2.7462961e-06
Iter: 816 loss: 2.7431488e-06
Iter: 817 loss: 2.78654534e-06
Iter: 818 loss: 2.74315653e-06
Iter: 819 loss: 2.74079821e-06
Iter: 820 loss: 2.76869469e-06
Iter: 821 loss: 2.74076979e-06
Iter: 822 loss: 2.73934279e-06
Iter: 823 loss: 2.73737e-06
Iter: 824 loss: 2.73732849e-06
Iter: 825 loss: 2.73493197e-06
Iter: 826 loss: 2.74504691e-06
Iter: 827 loss: 2.7344895e-06
Iter: 828 loss: 2.73232649e-06
Iter: 829 loss: 2.73981732e-06
Iter: 830 loss: 2.73184696e-06
Iter: 831 loss: 2.72945454e-06
Iter: 832 loss: 2.72605166e-06
Iter: 833 loss: 2.72598345e-06
Iter: 834 loss: 2.72283751e-06
Iter: 835 loss: 2.7332112e-06
Iter: 836 loss: 2.72198486e-06
Iter: 837 loss: 2.718627e-06
Iter: 838 loss: 2.74439185e-06
Iter: 839 loss: 2.71845056e-06
Iter: 840 loss: 2.71588624e-06
Iter: 841 loss: 2.71675435e-06
Iter: 842 loss: 2.71404633e-06
Iter: 843 loss: 2.71108775e-06
Iter: 844 loss: 2.72450825e-06
Iter: 845 loss: 2.7105084e-06
Iter: 846 loss: 2.70757528e-06
Iter: 847 loss: 2.72037551e-06
Iter: 848 loss: 2.70687906e-06
Iter: 849 loss: 2.70445889e-06
Iter: 850 loss: 2.70516534e-06
Iter: 851 loss: 2.70265264e-06
Iter: 852 loss: 2.6996297e-06
Iter: 853 loss: 2.697215e-06
Iter: 854 loss: 2.69632892e-06
Iter: 855 loss: 2.69536486e-06
Iter: 856 loss: 2.69423845e-06
Iter: 857 loss: 2.69201973e-06
Iter: 858 loss: 2.69738257e-06
Iter: 859 loss: 2.69116117e-06
Iter: 860 loss: 2.68959138e-06
Iter: 861 loss: 2.68642088e-06
Iter: 862 loss: 2.75127195e-06
Iter: 863 loss: 2.6863645e-06
Iter: 864 loss: 2.68370923e-06
Iter: 865 loss: 2.71785257e-06
Iter: 866 loss: 2.68364738e-06
Iter: 867 loss: 2.6814223e-06
Iter: 868 loss: 2.68743065e-06
Iter: 869 loss: 2.68069357e-06
Iter: 870 loss: 2.67865153e-06
Iter: 871 loss: 2.68148256e-06
Iter: 872 loss: 2.67759788e-06
Iter: 873 loss: 2.67526138e-06
Iter: 874 loss: 2.67782116e-06
Iter: 875 loss: 2.67401492e-06
Iter: 876 loss: 2.6712778e-06
Iter: 877 loss: 2.67380301e-06
Iter: 878 loss: 2.66967868e-06
Iter: 879 loss: 2.66643474e-06
Iter: 880 loss: 2.68421854e-06
Iter: 881 loss: 2.66595271e-06
Iter: 882 loss: 2.66327879e-06
Iter: 883 loss: 2.67870109e-06
Iter: 884 loss: 2.66285087e-06
Iter: 885 loss: 2.6606258e-06
Iter: 886 loss: 2.65797439e-06
Iter: 887 loss: 2.65766e-06
Iter: 888 loss: 2.65433209e-06
Iter: 889 loss: 2.65427798e-06
Iter: 890 loss: 2.65257336e-06
Iter: 891 loss: 2.64983805e-06
Iter: 892 loss: 2.64980508e-06
Iter: 893 loss: 2.65021686e-06
Iter: 894 loss: 2.64825621e-06
Iter: 895 loss: 2.64736536e-06
Iter: 896 loss: 2.64504115e-06
Iter: 897 loss: 2.66658162e-06
Iter: 898 loss: 2.64479513e-06
Iter: 899 loss: 2.64195114e-06
Iter: 900 loss: 2.64316895e-06
Iter: 901 loss: 2.64021901e-06
Iter: 902 loss: 2.63822403e-06
Iter: 903 loss: 2.63805714e-06
Iter: 904 loss: 2.63616857e-06
Iter: 905 loss: 2.63428728e-06
Iter: 906 loss: 2.63400716e-06
Iter: 907 loss: 2.63137463e-06
Iter: 908 loss: 2.63884976e-06
Iter: 909 loss: 2.6305504e-06
Iter: 910 loss: 2.62784533e-06
Iter: 911 loss: 2.63740048e-06
Iter: 912 loss: 2.62717185e-06
Iter: 913 loss: 2.62480194e-06
Iter: 914 loss: 2.62755975e-06
Iter: 915 loss: 2.62360845e-06
Iter: 916 loss: 2.62051549e-06
Iter: 917 loss: 2.63326569e-06
Iter: 918 loss: 2.61989317e-06
Iter: 919 loss: 2.61726086e-06
Iter: 920 loss: 2.62487038e-06
Iter: 921 loss: 2.61648029e-06
Iter: 922 loss: 2.61433115e-06
Iter: 923 loss: 2.62058757e-06
Iter: 924 loss: 2.61372452e-06
Iter: 925 loss: 2.61137302e-06
Iter: 926 loss: 2.62086019e-06
Iter: 927 loss: 2.61089554e-06
Iter: 928 loss: 2.60926709e-06
Iter: 929 loss: 2.6092348e-06
Iter: 930 loss: 2.60790353e-06
Iter: 931 loss: 2.60481283e-06
Iter: 932 loss: 2.65244489e-06
Iter: 933 loss: 2.60468209e-06
Iter: 934 loss: 2.60224829e-06
Iter: 935 loss: 2.60415482e-06
Iter: 936 loss: 2.60082493e-06
Iter: 937 loss: 2.59802869e-06
Iter: 938 loss: 2.6177504e-06
Iter: 939 loss: 2.59778926e-06
Iter: 940 loss: 2.59568333e-06
Iter: 941 loss: 2.61717628e-06
Iter: 942 loss: 2.59566718e-06
Iter: 943 loss: 2.59420426e-06
Iter: 944 loss: 2.5909178e-06
Iter: 945 loss: 2.63707216e-06
Iter: 946 loss: 2.59071589e-06
Iter: 947 loss: 2.58764453e-06
Iter: 948 loss: 2.61090395e-06
Iter: 949 loss: 2.58748628e-06
Iter: 950 loss: 2.58446289e-06
Iter: 951 loss: 2.59096123e-06
Iter: 952 loss: 2.58332216e-06
Iter: 953 loss: 2.58082764e-06
Iter: 954 loss: 2.59765102e-06
Iter: 955 loss: 2.58050795e-06
Iter: 956 loss: 2.57876536e-06
Iter: 957 loss: 2.58357704e-06
Iter: 958 loss: 2.5782208e-06
Iter: 959 loss: 2.57598e-06
Iter: 960 loss: 2.57806096e-06
Iter: 961 loss: 2.57469378e-06
Iter: 962 loss: 2.57230909e-06
Iter: 963 loss: 2.5837353e-06
Iter: 964 loss: 2.57188367e-06
Iter: 965 loss: 2.56995372e-06
Iter: 966 loss: 2.56992325e-06
Iter: 967 loss: 2.56882777e-06
Iter: 968 loss: 2.56900967e-06
Iter: 969 loss: 2.56804765e-06
Iter: 970 loss: 2.56646035e-06
Iter: 971 loss: 2.56327803e-06
Iter: 972 loss: 2.62246613e-06
Iter: 973 loss: 2.56327985e-06
Iter: 974 loss: 2.56060025e-06
Iter: 975 loss: 2.57859983e-06
Iter: 976 loss: 2.56035446e-06
Iter: 977 loss: 2.55816371e-06
Iter: 978 loss: 2.5708764e-06
Iter: 979 loss: 2.55786472e-06
Iter: 980 loss: 2.55560394e-06
Iter: 981 loss: 2.56349381e-06
Iter: 982 loss: 2.5550296e-06
Iter: 983 loss: 2.5533318e-06
Iter: 984 loss: 2.55077771e-06
Iter: 985 loss: 2.5507336e-06
Iter: 986 loss: 2.54780571e-06
Iter: 987 loss: 2.55892019e-06
Iter: 988 loss: 2.54708493e-06
Iter: 989 loss: 2.54456472e-06
Iter: 990 loss: 2.56498265e-06
Iter: 991 loss: 2.54442261e-06
Iter: 992 loss: 2.54209613e-06
Iter: 993 loss: 2.54476936e-06
Iter: 994 loss: 2.54078077e-06
Iter: 995 loss: 2.53868689e-06
Iter: 996 loss: 2.5605118e-06
Iter: 997 loss: 2.53868029e-06
Iter: 998 loss: 2.5373788e-06
Iter: 999 loss: 2.54196266e-06
Iter: 1000 loss: 2.53701432e-06
Iter: 1001 loss: 2.53546068e-06
Iter: 1002 loss: 2.54354927e-06
Iter: 1003 loss: 2.53527537e-06
Iter: 1004 loss: 2.53365465e-06
Iter: 1005 loss: 2.53172539e-06
Iter: 1006 loss: 2.53159169e-06
Iter: 1007 loss: 2.52954851e-06
Iter: 1008 loss: 2.5361e-06
Iter: 1009 loss: 2.529016e-06
Iter: 1010 loss: 2.52693826e-06
Iter: 1011 loss: 2.53009421e-06
Iter: 1012 loss: 2.52592326e-06
Iter: 1013 loss: 2.52370455e-06
Iter: 1014 loss: 2.52537711e-06
Iter: 1015 loss: 2.52227483e-06
Iter: 1016 loss: 2.52003065e-06
Iter: 1017 loss: 2.54158203e-06
Iter: 1018 loss: 2.51997608e-06
Iter: 1019 loss: 2.5178565e-06
Iter: 1020 loss: 2.52592781e-06
Iter: 1021 loss: 2.51742676e-06
Iter: 1022 loss: 2.5160416e-06
Iter: 1023 loss: 2.5132781e-06
Iter: 1024 loss: 2.57092e-06
Iter: 1025 loss: 2.51330016e-06
Iter: 1026 loss: 2.51049414e-06
Iter: 1027 loss: 2.52075733e-06
Iter: 1028 loss: 2.50976018e-06
Iter: 1029 loss: 2.50718949e-06
Iter: 1030 loss: 2.52307905e-06
Iter: 1031 loss: 2.50683865e-06
Iter: 1032 loss: 2.50472476e-06
Iter: 1033 loss: 2.5077311e-06
Iter: 1034 loss: 2.50372113e-06
Iter: 1035 loss: 2.50253879e-06
Iter: 1036 loss: 2.50224957e-06
Iter: 1037 loss: 2.50109088e-06
Iter: 1038 loss: 2.50332459e-06
Iter: 1039 loss: 2.50057769e-06
Iter: 1040 loss: 2.49932577e-06
Iter: 1041 loss: 2.49894242e-06
Iter: 1042 loss: 2.49809591e-06
Iter: 1043 loss: 2.49672121e-06
Iter: 1044 loss: 2.49637515e-06
Iter: 1045 loss: 2.49551977e-06
Iter: 1046 loss: 2.49303775e-06
Iter: 1047 loss: 2.50395806e-06
Iter: 1048 loss: 2.49254595e-06
Iter: 1049 loss: 2.49089817e-06
Iter: 1050 loss: 2.4922349e-06
Iter: 1051 loss: 2.48989227e-06
Iter: 1052 loss: 2.48749802e-06
Iter: 1053 loss: 2.4951396e-06
Iter: 1054 loss: 2.48682613e-06
Iter: 1055 loss: 2.48489368e-06
Iter: 1056 loss: 2.49990944e-06
Iter: 1057 loss: 2.4847609e-06
Iter: 1058 loss: 2.48310266e-06
Iter: 1059 loss: 2.4863923e-06
Iter: 1060 loss: 2.4824767e-06
Iter: 1061 loss: 2.48079959e-06
Iter: 1062 loss: 2.48209949e-06
Iter: 1063 loss: 2.47988237e-06
Iter: 1064 loss: 2.47795106e-06
Iter: 1065 loss: 2.47704497e-06
Iter: 1066 loss: 2.47604885e-06
Iter: 1067 loss: 2.47319872e-06
Iter: 1068 loss: 2.48143169e-06
Iter: 1069 loss: 2.47237176e-06
Iter: 1070 loss: 2.47040043e-06
Iter: 1071 loss: 2.49180721e-06
Iter: 1072 loss: 2.47034632e-06
Iter: 1073 loss: 2.46974832e-06
Iter: 1074 loss: 2.46955028e-06
Iter: 1075 loss: 2.46881564e-06
Iter: 1076 loss: 2.46655827e-06
Iter: 1077 loss: 2.47404068e-06
Iter: 1078 loss: 2.46549325e-06
Iter: 1079 loss: 2.46367836e-06
Iter: 1080 loss: 2.48765946e-06
Iter: 1081 loss: 2.46364152e-06
Iter: 1082 loss: 2.46209e-06
Iter: 1083 loss: 2.46586887e-06
Iter: 1084 loss: 2.46145419e-06
Iter: 1085 loss: 2.45980664e-06
Iter: 1086 loss: 2.46035961e-06
Iter: 1087 loss: 2.45871456e-06
Iter: 1088 loss: 2.45682827e-06
Iter: 1089 loss: 2.45969613e-06
Iter: 1090 loss: 2.45587853e-06
Iter: 1091 loss: 2.45341516e-06
Iter: 1092 loss: 2.46926561e-06
Iter: 1093 loss: 2.4531314e-06
Iter: 1094 loss: 2.45130059e-06
Iter: 1095 loss: 2.45478304e-06
Iter: 1096 loss: 2.45057072e-06
Iter: 1097 loss: 2.44851026e-06
Iter: 1098 loss: 2.45978435e-06
Iter: 1099 loss: 2.44821717e-06
Iter: 1100 loss: 2.44682406e-06
Iter: 1101 loss: 2.44567173e-06
Iter: 1102 loss: 2.44528383e-06
Iter: 1103 loss: 2.44354351e-06
Iter: 1104 loss: 2.45681963e-06
Iter: 1105 loss: 2.44339117e-06
Iter: 1106 loss: 2.44170246e-06
Iter: 1107 loss: 2.44287958e-06
Iter: 1108 loss: 2.44061812e-06
Iter: 1109 loss: 2.43958448e-06
Iter: 1110 loss: 2.43939417e-06
Iter: 1111 loss: 2.43836303e-06
Iter: 1112 loss: 2.4367389e-06
Iter: 1113 loss: 2.4367564e-06
Iter: 1114 loss: 2.43513932e-06
Iter: 1115 loss: 2.43285058e-06
Iter: 1116 loss: 2.432776e-06
Iter: 1117 loss: 2.43029626e-06
Iter: 1118 loss: 2.45902947e-06
Iter: 1119 loss: 2.4302617e-06
Iter: 1120 loss: 2.42841816e-06
Iter: 1121 loss: 2.44331022e-06
Iter: 1122 loss: 2.42831379e-06
Iter: 1123 loss: 2.42708484e-06
Iter: 1124 loss: 2.42624219e-06
Iter: 1125 loss: 2.42574447e-06
Iter: 1126 loss: 2.42372562e-06
Iter: 1127 loss: 2.42639e-06
Iter: 1128 loss: 2.42268152e-06
Iter: 1129 loss: 2.42149827e-06
Iter: 1130 loss: 2.42131819e-06
Iter: 1131 loss: 2.42049873e-06
Iter: 1132 loss: 2.41955377e-06
Iter: 1133 loss: 2.41941439e-06
Iter: 1134 loss: 2.41752218e-06
Iter: 1135 loss: 2.42330248e-06
Iter: 1136 loss: 2.41705197e-06
Iter: 1137 loss: 2.4155911e-06
Iter: 1138 loss: 2.41585394e-06
Iter: 1139 loss: 2.41450334e-06
Iter: 1140 loss: 2.4122869e-06
Iter: 1141 loss: 2.41648627e-06
Iter: 1142 loss: 2.41141038e-06
Iter: 1143 loss: 2.41071257e-06
Iter: 1144 loss: 2.41020371e-06
Iter: 1145 loss: 2.40919189e-06
Iter: 1146 loss: 2.40750046e-06
Iter: 1147 loss: 2.40749614e-06
Iter: 1148 loss: 2.40577924e-06
Iter: 1149 loss: 2.40935742e-06
Iter: 1150 loss: 2.40506097e-06
Iter: 1151 loss: 2.40339432e-06
Iter: 1152 loss: 2.404214e-06
Iter: 1153 loss: 2.40224654e-06
Iter: 1154 loss: 2.40054896e-06
Iter: 1155 loss: 2.40741019e-06
Iter: 1156 loss: 2.40015743e-06
Iter: 1157 loss: 2.39825067e-06
Iter: 1158 loss: 2.40701638e-06
Iter: 1159 loss: 2.39787914e-06
Iter: 1160 loss: 2.39613837e-06
Iter: 1161 loss: 2.39941414e-06
Iter: 1162 loss: 2.39530891e-06
Iter: 1163 loss: 2.39392284e-06
Iter: 1164 loss: 2.39373071e-06
Iter: 1165 loss: 2.39274664e-06
Iter: 1166 loss: 2.39087944e-06
Iter: 1167 loss: 2.39087376e-06
Iter: 1168 loss: 2.38969e-06
Iter: 1169 loss: 2.38870962e-06
Iter: 1170 loss: 2.38845769e-06
Iter: 1171 loss: 2.3868256e-06
Iter: 1172 loss: 2.39906694e-06
Iter: 1173 loss: 2.38674829e-06
Iter: 1174 loss: 2.38523648e-06
Iter: 1175 loss: 2.38360758e-06
Iter: 1176 loss: 2.38337452e-06
Iter: 1177 loss: 2.38312373e-06
Iter: 1178 loss: 2.38236453e-06
Iter: 1179 loss: 2.38134589e-06
Iter: 1180 loss: 2.38038456e-06
Iter: 1181 loss: 2.38015855e-06
Iter: 1182 loss: 2.37877e-06
Iter: 1183 loss: 2.37754193e-06
Iter: 1184 loss: 2.37717836e-06
Iter: 1185 loss: 2.3754144e-06
Iter: 1186 loss: 2.38863095e-06
Iter: 1187 loss: 2.37530639e-06
Iter: 1188 loss: 2.37361746e-06
Iter: 1189 loss: 2.37337235e-06
Iter: 1190 loss: 2.37217e-06
Iter: 1191 loss: 2.37007407e-06
Iter: 1192 loss: 2.37521772e-06
Iter: 1193 loss: 2.3693342e-06
Iter: 1194 loss: 2.36712958e-06
Iter: 1195 loss: 2.3793e-06
Iter: 1196 loss: 2.36681649e-06
Iter: 1197 loss: 2.36495111e-06
Iter: 1198 loss: 2.37868767e-06
Iter: 1199 loss: 2.36476399e-06
Iter: 1200 loss: 2.36372443e-06
Iter: 1201 loss: 2.36312735e-06
Iter: 1202 loss: 2.36257756e-06
Iter: 1203 loss: 2.3612763e-06
Iter: 1204 loss: 2.38087569e-06
Iter: 1205 loss: 2.36127153e-06
Iter: 1206 loss: 2.36007281e-06
Iter: 1207 loss: 2.3583641e-06
Iter: 1208 loss: 2.35831703e-06
Iter: 1209 loss: 2.35615653e-06
Iter: 1210 loss: 2.36323558e-06
Iter: 1211 loss: 2.35559014e-06
Iter: 1212 loss: 2.35407197e-06
Iter: 1213 loss: 2.36685946e-06
Iter: 1214 loss: 2.35401444e-06
Iter: 1215 loss: 2.35281595e-06
Iter: 1216 loss: 2.36013875e-06
Iter: 1217 loss: 2.35269317e-06
Iter: 1218 loss: 2.35144694e-06
Iter: 1219 loss: 2.35642983e-06
Iter: 1220 loss: 2.3511684e-06
Iter: 1221 loss: 2.35043535e-06
Iter: 1222 loss: 2.34842901e-06
Iter: 1223 loss: 2.36275605e-06
Iter: 1224 loss: 2.34793833e-06
Iter: 1225 loss: 2.34563095e-06
Iter: 1226 loss: 2.35374728e-06
Iter: 1227 loss: 2.3450184e-06
Iter: 1228 loss: 2.34299569e-06
Iter: 1229 loss: 2.36464484e-06
Iter: 1230 loss: 2.34290064e-06
Iter: 1231 loss: 2.34129766e-06
Iter: 1232 loss: 2.34199479e-06
Iter: 1233 loss: 2.34030426e-06
Iter: 1234 loss: 2.33880883e-06
Iter: 1235 loss: 2.34964546e-06
Iter: 1236 loss: 2.33860828e-06
Iter: 1237 loss: 2.33719e-06
Iter: 1238 loss: 2.34247659e-06
Iter: 1239 loss: 2.33681749e-06
Iter: 1240 loss: 2.33520927e-06
Iter: 1241 loss: 2.33482e-06
Iter: 1242 loss: 2.33387209e-06
Iter: 1243 loss: 2.3327e-06
Iter: 1244 loss: 2.33264791e-06
Iter: 1245 loss: 2.33178116e-06
Iter: 1246 loss: 2.32976981e-06
Iter: 1247 loss: 2.35600442e-06
Iter: 1248 loss: 2.32955244e-06
Iter: 1249 loss: 2.32753359e-06
Iter: 1250 loss: 2.33824699e-06
Iter: 1251 loss: 2.32712955e-06
Iter: 1252 loss: 2.32621824e-06
Iter: 1253 loss: 2.32607181e-06
Iter: 1254 loss: 2.32482512e-06
Iter: 1255 loss: 2.32410775e-06
Iter: 1256 loss: 2.32355296e-06
Iter: 1257 loss: 2.32224534e-06
Iter: 1258 loss: 2.32024536e-06
Iter: 1259 loss: 2.32017896e-06
Iter: 1260 loss: 2.31818922e-06
Iter: 1261 loss: 2.32491448e-06
Iter: 1262 loss: 2.31766217e-06
Iter: 1263 loss: 2.31562035e-06
Iter: 1264 loss: 2.33181731e-06
Iter: 1265 loss: 2.31548483e-06
Iter: 1266 loss: 2.31402441e-06
Iter: 1267 loss: 2.31797094e-06
Iter: 1268 loss: 2.31359104e-06
Iter: 1269 loss: 2.31182139e-06
Iter: 1270 loss: 2.31319245e-06
Iter: 1271 loss: 2.31076206e-06
Iter: 1272 loss: 2.30941396e-06
Iter: 1273 loss: 2.32963157e-06
Iter: 1274 loss: 2.30941555e-06
Iter: 1275 loss: 2.30832256e-06
Iter: 1276 loss: 2.30784372e-06
Iter: 1277 loss: 2.30720116e-06
Iter: 1278 loss: 2.30546402e-06
Iter: 1279 loss: 2.31666309e-06
Iter: 1280 loss: 2.30526757e-06
Iter: 1281 loss: 2.30398155e-06
Iter: 1282 loss: 2.30425485e-06
Iter: 1283 loss: 2.30304659e-06
Iter: 1284 loss: 2.30137493e-06
Iter: 1285 loss: 2.31138483e-06
Iter: 1286 loss: 2.30122259e-06
Iter: 1287 loss: 2.29982493e-06
Iter: 1288 loss: 2.30104774e-06
Iter: 1289 loss: 2.29901298e-06
Iter: 1290 loss: 2.29810075e-06
Iter: 1291 loss: 2.29800753e-06
Iter: 1292 loss: 2.29726038e-06
Iter: 1293 loss: 2.29570787e-06
Iter: 1294 loss: 2.32291859e-06
Iter: 1295 loss: 2.29568946e-06
Iter: 1296 loss: 2.29429861e-06
Iter: 1297 loss: 2.29396e-06
Iter: 1298 loss: 2.29310831e-06
Iter: 1299 loss: 2.29131456e-06
Iter: 1300 loss: 2.29886336e-06
Iter: 1301 loss: 2.29095963e-06
Iter: 1302 loss: 2.2892059e-06
Iter: 1303 loss: 2.29381021e-06
Iter: 1304 loss: 2.28865974e-06
Iter: 1305 loss: 2.28683507e-06
Iter: 1306 loss: 2.29769012e-06
Iter: 1307 loss: 2.28660588e-06
Iter: 1308 loss: 2.28499903e-06
Iter: 1309 loss: 2.28851491e-06
Iter: 1310 loss: 2.28443764e-06
Iter: 1311 loss: 2.28308136e-06
Iter: 1312 loss: 2.28936415e-06
Iter: 1313 loss: 2.28283716e-06
Iter: 1314 loss: 2.28121394e-06
Iter: 1315 loss: 2.28274121e-06
Iter: 1316 loss: 2.28029739e-06
Iter: 1317 loss: 2.27898067e-06
Iter: 1318 loss: 2.29147645e-06
Iter: 1319 loss: 2.27886767e-06
Iter: 1320 loss: 2.27788178e-06
Iter: 1321 loss: 2.27727423e-06
Iter: 1322 loss: 2.27680584e-06
Iter: 1323 loss: 2.2753452e-06
Iter: 1324 loss: 2.28498425e-06
Iter: 1325 loss: 2.27522605e-06
Iter: 1326 loss: 2.27392457e-06
Iter: 1327 loss: 2.27522128e-06
Iter: 1328 loss: 2.27320334e-06
Iter: 1329 loss: 2.27158102e-06
Iter: 1330 loss: 2.27101305e-06
Iter: 1331 loss: 2.27012401e-06
Iter: 1332 loss: 2.27066653e-06
Iter: 1333 loss: 2.26953125e-06
Iter: 1334 loss: 2.26886868e-06
Iter: 1335 loss: 2.26707607e-06
Iter: 1336 loss: 2.28251065e-06
Iter: 1337 loss: 2.26676275e-06
Iter: 1338 loss: 2.26509223e-06
Iter: 1339 loss: 2.27084274e-06
Iter: 1340 loss: 2.26463499e-06
Iter: 1341 loss: 2.26327143e-06
Iter: 1342 loss: 2.26384532e-06
Iter: 1343 loss: 2.26227303e-06
Iter: 1344 loss: 2.2605459e-06
Iter: 1345 loss: 2.27385681e-06
Iter: 1346 loss: 2.26044858e-06
Iter: 1347 loss: 2.25887766e-06
Iter: 1348 loss: 2.26178554e-06
Iter: 1349 loss: 2.25820736e-06
Iter: 1350 loss: 2.25687836e-06
Iter: 1351 loss: 2.26246448e-06
Iter: 1352 loss: 2.25659574e-06
Iter: 1353 loss: 2.25517169e-06
Iter: 1354 loss: 2.26028783e-06
Iter: 1355 loss: 2.25479107e-06
Iter: 1356 loss: 2.25363215e-06
Iter: 1357 loss: 2.25647591e-06
Iter: 1358 loss: 2.25312442e-06
Iter: 1359 loss: 2.25196345e-06
Iter: 1360 loss: 2.2548943e-06
Iter: 1361 loss: 2.25151189e-06
Iter: 1362 loss: 2.25017357e-06
Iter: 1363 loss: 2.25239455e-06
Iter: 1364 loss: 2.24956057e-06
Iter: 1365 loss: 2.24833479e-06
Iter: 1366 loss: 2.25505369e-06
Iter: 1367 loss: 2.24810105e-06
Iter: 1368 loss: 2.24685641e-06
Iter: 1369 loss: 2.24530413e-06
Iter: 1370 loss: 2.24522159e-06
Iter: 1371 loss: 2.24534e-06
Iter: 1372 loss: 2.24419273e-06
Iter: 1373 loss: 2.24357655e-06
Iter: 1374 loss: 2.24206724e-06
Iter: 1375 loss: 2.25705571e-06
Iter: 1376 loss: 2.24190444e-06
Iter: 1377 loss: 2.24028827e-06
Iter: 1378 loss: 2.24083306e-06
Iter: 1379 loss: 2.23914799e-06
Iter: 1380 loss: 2.23692177e-06
Iter: 1381 loss: 2.24368523e-06
Iter: 1382 loss: 2.23628422e-06
Iter: 1383 loss: 2.23479788e-06
Iter: 1384 loss: 2.24045607e-06
Iter: 1385 loss: 2.23438838e-06
Iter: 1386 loss: 2.23317875e-06
Iter: 1387 loss: 2.23319e-06
Iter: 1388 loss: 2.23225061e-06
Iter: 1389 loss: 2.2317754e-06
Iter: 1390 loss: 2.23136749e-06
Iter: 1391 loss: 2.2301374e-06
Iter: 1392 loss: 2.23981806e-06
Iter: 1393 loss: 2.23003167e-06
Iter: 1394 loss: 2.22903464e-06
Iter: 1395 loss: 2.23163261e-06
Iter: 1396 loss: 2.22873541e-06
Iter: 1397 loss: 2.22768813e-06
Iter: 1398 loss: 2.22828817e-06
Iter: 1399 loss: 2.22700805e-06
Iter: 1400 loss: 2.22555445e-06
Iter: 1401 loss: 2.230571e-06
Iter: 1402 loss: 2.22509016e-06
Iter: 1403 loss: 2.2241943e-06
Iter: 1404 loss: 2.23644065e-06
Iter: 1405 loss: 2.22416247e-06
Iter: 1406 loss: 2.22309905e-06
Iter: 1407 loss: 2.22273e-06
Iter: 1408 loss: 2.22218796e-06
Iter: 1409 loss: 2.22125573e-06
Iter: 1410 loss: 2.22334711e-06
Iter: 1411 loss: 2.22086692e-06
Iter: 1412 loss: 2.21977666e-06
Iter: 1413 loss: 2.21879463e-06
Iter: 1414 loss: 2.21857772e-06
Iter: 1415 loss: 2.21700088e-06
Iter: 1416 loss: 2.22834569e-06
Iter: 1417 loss: 2.21683763e-06
Iter: 1418 loss: 2.21551977e-06
Iter: 1419 loss: 2.21333448e-06
Iter: 1420 loss: 2.21329788e-06
Iter: 1421 loss: 2.21388905e-06
Iter: 1422 loss: 2.21237315e-06
Iter: 1423 loss: 2.21160121e-06
Iter: 1424 loss: 2.21046685e-06
Iter: 1425 loss: 2.21041728e-06
Iter: 1426 loss: 2.20924721e-06
Iter: 1427 loss: 2.22002518e-06
Iter: 1428 loss: 2.2091906e-06
Iter: 1429 loss: 2.20805805e-06
Iter: 1430 loss: 2.20858169e-06
Iter: 1431 loss: 2.20736797e-06
Iter: 1432 loss: 2.20600668e-06
Iter: 1433 loss: 2.20730021e-06
Iter: 1434 loss: 2.20514085e-06
Iter: 1435 loss: 2.20403263e-06
Iter: 1436 loss: 2.20403422e-06
Iter: 1437 loss: 2.2032184e-06
Iter: 1438 loss: 2.20844436e-06
Iter: 1439 loss: 2.20316338e-06
Iter: 1440 loss: 2.20256879e-06
Iter: 1441 loss: 2.20146921e-06
Iter: 1442 loss: 2.20147717e-06
Iter: 1443 loss: 2.20028483e-06
Iter: 1444 loss: 2.20137872e-06
Iter: 1445 loss: 2.19964113e-06
Iter: 1446 loss: 2.19817139e-06
Iter: 1447 loss: 2.20381571e-06
Iter: 1448 loss: 2.197924e-06
Iter: 1449 loss: 2.19641834e-06
Iter: 1450 loss: 2.19631215e-06
Iter: 1451 loss: 2.19526783e-06
Iter: 1452 loss: 2.19345657e-06
Iter: 1453 loss: 2.20343736e-06
Iter: 1454 loss: 2.19314506e-06
Iter: 1455 loss: 2.19213234e-06
Iter: 1456 loss: 2.20644506e-06
Iter: 1457 loss: 2.19208914e-06
Iter: 1458 loss: 2.19100593e-06
Iter: 1459 loss: 2.19029562e-06
Iter: 1460 loss: 2.18987134e-06
Iter: 1461 loss: 2.18836635e-06
Iter: 1462 loss: 2.19215713e-06
Iter: 1463 loss: 2.18780951e-06
Iter: 1464 loss: 2.18604373e-06
Iter: 1465 loss: 2.19598087e-06
Iter: 1466 loss: 2.18578907e-06
Iter: 1467 loss: 2.18474543e-06
Iter: 1468 loss: 2.18455671e-06
Iter: 1469 loss: 2.18382e-06
Iter: 1470 loss: 2.18292553e-06
Iter: 1471 loss: 2.18282457e-06
Iter: 1472 loss: 2.18212881e-06
Iter: 1473 loss: 2.18165701e-06
Iter: 1474 loss: 2.18145533e-06
Iter: 1475 loss: 2.18040555e-06
Iter: 1476 loss: 2.18001492e-06
Iter: 1477 loss: 2.17946604e-06
Iter: 1478 loss: 2.17788102e-06
Iter: 1479 loss: 2.17976185e-06
Iter: 1480 loss: 2.17699085e-06
Iter: 1481 loss: 2.17555544e-06
Iter: 1482 loss: 2.18472292e-06
Iter: 1483 loss: 2.17536353e-06
Iter: 1484 loss: 2.17390516e-06
Iter: 1485 loss: 2.17575939e-06
Iter: 1486 loss: 2.17319393e-06
Iter: 1487 loss: 2.17182105e-06
Iter: 1488 loss: 2.17646516e-06
Iter: 1489 loss: 2.17147e-06
Iter: 1490 loss: 2.17010961e-06
Iter: 1491 loss: 2.18014566e-06
Iter: 1492 loss: 2.16997387e-06
Iter: 1493 loss: 2.168847e-06
Iter: 1494 loss: 2.16852504e-06
Iter: 1495 loss: 2.1678668e-06
Iter: 1496 loss: 2.16643275e-06
Iter: 1497 loss: 2.17277329e-06
Iter: 1498 loss: 2.16609374e-06
Iter: 1499 loss: 2.16497574e-06
Iter: 1500 loss: 2.17410457e-06
Iter: 1501 loss: 2.16492458e-06
Iter: 1502 loss: 2.16416493e-06
Iter: 1503 loss: 2.16498461e-06
Iter: 1504 loss: 2.16376702e-06
Iter: 1505 loss: 2.16234503e-06
Iter: 1506 loss: 2.16462627e-06
Iter: 1507 loss: 2.16170565e-06
Iter: 1508 loss: 2.16086801e-06
Iter: 1509 loss: 2.16132048e-06
Iter: 1510 loss: 2.16032777e-06
Iter: 1511 loss: 2.1591477e-06
Iter: 1512 loss: 2.16016679e-06
Iter: 1513 loss: 2.15851696e-06
Iter: 1514 loss: 2.15716227e-06
Iter: 1515 loss: 2.15997375e-06
Iter: 1516 loss: 2.15657656e-06
Iter: 1517 loss: 2.15540922e-06
Iter: 1518 loss: 2.15663567e-06
Iter: 1519 loss: 2.15474643e-06
Iter: 1520 loss: 2.15318596e-06
Iter: 1521 loss: 2.16514218e-06
Iter: 1522 loss: 2.15309865e-06
Iter: 1523 loss: 2.15205046e-06
Iter: 1524 loss: 2.15452928e-06
Iter: 1525 loss: 2.15163709e-06
Iter: 1526 loss: 2.15046657e-06
Iter: 1527 loss: 2.15739283e-06
Iter: 1528 loss: 2.15035675e-06
Iter: 1529 loss: 2.14948068e-06
Iter: 1530 loss: 2.14860802e-06
Iter: 1531 loss: 2.14839929e-06
Iter: 1532 loss: 2.14737656e-06
Iter: 1533 loss: 2.16006538e-06
Iter: 1534 loss: 2.14739475e-06
Iter: 1535 loss: 2.14637839e-06
Iter: 1536 loss: 2.14629608e-06
Iter: 1537 loss: 2.14556667e-06
Iter: 1538 loss: 2.14473903e-06
Iter: 1539 loss: 2.14465126e-06
Iter: 1540 loss: 2.1440992e-06
Iter: 1541 loss: 2.14319516e-06
Iter: 1542 loss: 2.14322836e-06
Iter: 1543 loss: 2.14218881e-06
Iter: 1544 loss: 2.14215743e-06
Iter: 1545 loss: 2.14141255e-06
Iter: 1546 loss: 2.13979183e-06
Iter: 1547 loss: 2.14689044e-06
Iter: 1548 loss: 2.13947214e-06
Iter: 1549 loss: 2.1383762e-06
Iter: 1550 loss: 2.13670546e-06
Iter: 1551 loss: 2.13669682e-06
Iter: 1552 loss: 2.13508588e-06
Iter: 1553 loss: 2.15594946e-06
Iter: 1554 loss: 2.13505905e-06
Iter: 1555 loss: 2.13377189e-06
Iter: 1556 loss: 2.13544809e-06
Iter: 1557 loss: 2.13312023e-06
Iter: 1558 loss: 2.13221301e-06
Iter: 1559 loss: 2.13215981e-06
Iter: 1560 loss: 2.13138878e-06
Iter: 1561 loss: 2.13003409e-06
Iter: 1562 loss: 2.13006979e-06
Iter: 1563 loss: 2.12860732e-06
Iter: 1564 loss: 2.13837643e-06
Iter: 1565 loss: 2.12848136e-06
Iter: 1566 loss: 2.12728651e-06
Iter: 1567 loss: 2.1314745e-06
Iter: 1568 loss: 2.12696932e-06
Iter: 1569 loss: 2.12621876e-06
Iter: 1570 loss: 2.13790941e-06
Iter: 1571 loss: 2.12622081e-06
Iter: 1572 loss: 2.12541909e-06
Iter: 1573 loss: 2.1252531e-06
Iter: 1574 loss: 2.12479654e-06
Iter: 1575 loss: 2.12405075e-06
Iter: 1576 loss: 2.12323721e-06
Iter: 1577 loss: 2.12311261e-06
Iter: 1578 loss: 2.12170721e-06
Iter: 1579 loss: 2.13066278e-06
Iter: 1580 loss: 2.12153714e-06
Iter: 1581 loss: 2.12048189e-06
Iter: 1582 loss: 2.12084797e-06
Iter: 1583 loss: 2.11971724e-06
Iter: 1584 loss: 2.11842394e-06
Iter: 1585 loss: 2.12242503e-06
Iter: 1586 loss: 2.11800693e-06
Iter: 1587 loss: 2.11671227e-06
Iter: 1588 loss: 2.12312671e-06
Iter: 1589 loss: 2.11646238e-06
Iter: 1590 loss: 2.1155679e-06
Iter: 1591 loss: 2.11703468e-06
Iter: 1592 loss: 2.11518523e-06
Iter: 1593 loss: 2.1140329e-06
Iter: 1594 loss: 2.11683732e-06
Iter: 1595 loss: 2.11360907e-06
Iter: 1596 loss: 2.11239058e-06
Iter: 1597 loss: 2.12485406e-06
Iter: 1598 loss: 2.11237921e-06
Iter: 1599 loss: 2.11181668e-06
Iter: 1600 loss: 2.11039151e-06
Iter: 1601 loss: 2.12787154e-06
Iter: 1602 loss: 2.11030783e-06
Iter: 1603 loss: 2.11012866e-06
Iter: 1604 loss: 2.10961275e-06
Iter: 1605 loss: 2.10892927e-06
Iter: 1606 loss: 2.10989538e-06
Iter: 1607 loss: 2.10862208e-06
Iter: 1608 loss: 2.10795e-06
Iter: 1609 loss: 2.10801795e-06
Iter: 1610 loss: 2.10735061e-06
Iter: 1611 loss: 2.10629605e-06
Iter: 1612 loss: 2.10614189e-06
Iter: 1613 loss: 2.10539292e-06
Iter: 1614 loss: 2.10420285e-06
Iter: 1615 loss: 2.1046169e-06
Iter: 1616 loss: 2.10335907e-06
Iter: 1617 loss: 2.10186636e-06
Iter: 1618 loss: 2.11281713e-06
Iter: 1619 loss: 2.1016956e-06
Iter: 1620 loss: 2.10050666e-06
Iter: 1621 loss: 2.1038968e-06
Iter: 1622 loss: 2.10018084e-06
Iter: 1623 loss: 2.09882592e-06
Iter: 1624 loss: 2.10162671e-06
Iter: 1625 loss: 2.09832615e-06
Iter: 1626 loss: 2.09716336e-06
Iter: 1627 loss: 2.09973132e-06
Iter: 1628 loss: 2.09666814e-06
Iter: 1629 loss: 2.09559198e-06
Iter: 1630 loss: 2.10619328e-06
Iter: 1631 loss: 2.09549898e-06
Iter: 1632 loss: 2.09483778e-06
Iter: 1633 loss: 2.09605605e-06
Iter: 1634 loss: 2.09454947e-06
Iter: 1635 loss: 2.0935106e-06
Iter: 1636 loss: 2.0941352e-06
Iter: 1637 loss: 2.0927987e-06
Iter: 1638 loss: 2.09202381e-06
Iter: 1639 loss: 2.10290409e-06
Iter: 1640 loss: 2.09200516e-06
Iter: 1641 loss: 2.09126279e-06
Iter: 1642 loss: 2.09289055e-06
Iter: 1643 loss: 2.091017e-06
Iter: 1644 loss: 2.09044651e-06
Iter: 1645 loss: 2.08933466e-06
Iter: 1646 loss: 2.10815e-06
Iter: 1647 loss: 2.08930624e-06
Iter: 1648 loss: 2.08830738e-06
Iter: 1649 loss: 2.09545487e-06
Iter: 1650 loss: 2.08823258e-06
Iter: 1651 loss: 2.08712845e-06
Iter: 1652 loss: 2.08875804e-06
Iter: 1653 loss: 2.08655842e-06
Iter: 1654 loss: 2.08541383e-06
Iter: 1655 loss: 2.08628171e-06
Iter: 1656 loss: 2.08470738e-06
Iter: 1657 loss: 2.08352208e-06
Iter: 1658 loss: 2.08746769e-06
Iter: 1659 loss: 2.08316396e-06
Iter: 1660 loss: 2.08206598e-06
Iter: 1661 loss: 2.08739402e-06
Iter: 1662 loss: 2.08190522e-06
Iter: 1663 loss: 2.08093843e-06
Iter: 1664 loss: 2.08364145e-06
Iter: 1665 loss: 2.08059282e-06
Iter: 1666 loss: 2.07965604e-06
Iter: 1667 loss: 2.08027313e-06
Iter: 1668 loss: 2.07911648e-06
Iter: 1669 loss: 2.07784478e-06
Iter: 1670 loss: 2.08472147e-06
Iter: 1671 loss: 2.07765834e-06
Iter: 1672 loss: 2.07673065e-06
Iter: 1673 loss: 2.07842049e-06
Iter: 1674 loss: 2.07632752e-06
Iter: 1675 loss: 2.07561698e-06
Iter: 1676 loss: 2.07563062e-06
Iter: 1677 loss: 2.07512835e-06
Iter: 1678 loss: 2.07393964e-06
Iter: 1679 loss: 2.08357619e-06
Iter: 1680 loss: 2.07370476e-06
Iter: 1681 loss: 2.07248172e-06
Iter: 1682 loss: 2.07663425e-06
Iter: 1683 loss: 2.07217181e-06
Iter: 1684 loss: 2.07133326e-06
Iter: 1685 loss: 2.08325264e-06
Iter: 1686 loss: 2.07131689e-06
Iter: 1687 loss: 2.07056678e-06
Iter: 1688 loss: 2.06973232e-06
Iter: 1689 loss: 2.06963568e-06
Iter: 1690 loss: 2.06867617e-06
Iter: 1691 loss: 2.07907692e-06
Iter: 1692 loss: 2.06867253e-06
Iter: 1693 loss: 2.06783329e-06
Iter: 1694 loss: 2.06769937e-06
Iter: 1695 loss: 2.06712411e-06
Iter: 1696 loss: 2.06600021e-06
Iter: 1697 loss: 2.06859158e-06
Iter: 1698 loss: 2.06567756e-06
Iter: 1699 loss: 2.06472032e-06
Iter: 1700 loss: 2.06519917e-06
Iter: 1701 loss: 2.06412801e-06
Iter: 1702 loss: 2.06279083e-06
Iter: 1703 loss: 2.07142398e-06
Iter: 1704 loss: 2.06266532e-06
Iter: 1705 loss: 2.06163e-06
Iter: 1706 loss: 2.06188e-06
Iter: 1707 loss: 2.06090272e-06
Iter: 1708 loss: 2.05967672e-06
Iter: 1709 loss: 2.06699497e-06
Iter: 1710 loss: 2.05951119e-06
Iter: 1711 loss: 2.05845299e-06
Iter: 1712 loss: 2.06113623e-06
Iter: 1713 loss: 2.05802189e-06
Iter: 1714 loss: 2.05713513e-06
Iter: 1715 loss: 2.06315849e-06
Iter: 1716 loss: 2.05701599e-06
Iter: 1717 loss: 2.05625929e-06
Iter: 1718 loss: 2.06481127e-06
Iter: 1719 loss: 2.05631204e-06
Iter: 1720 loss: 2.05586e-06
Iter: 1721 loss: 2.05472543e-06
Iter: 1722 loss: 2.05836864e-06
Iter: 1723 loss: 2.05419951e-06
Iter: 1724 loss: 2.0536761e-06
Iter: 1725 loss: 2.05336619e-06
Iter: 1726 loss: 2.05280594e-06
Iter: 1727 loss: 2.05204287e-06
Iter: 1728 loss: 2.0520215e-06
Iter: 1729 loss: 2.05116567e-06
Iter: 1730 loss: 2.05889296e-06
Iter: 1731 loss: 2.0511518e-06
Iter: 1732 loss: 2.05037759e-06
Iter: 1733 loss: 2.0522541e-06
Iter: 1734 loss: 2.05013771e-06
Iter: 1735 loss: 2.04945e-06
Iter: 1736 loss: 2.04798948e-06
Iter: 1737 loss: 2.07634412e-06
Iter: 1738 loss: 2.04798721e-06
Iter: 1739 loss: 2.04677053e-06
Iter: 1740 loss: 2.06034815e-06
Iter: 1741 loss: 2.04674052e-06
Iter: 1742 loss: 2.0457087e-06
Iter: 1743 loss: 2.04573121e-06
Iter: 1744 loss: 2.0448897e-06
Iter: 1745 loss: 2.04399862e-06
Iter: 1746 loss: 2.04400794e-06
Iter: 1747 loss: 2.04323806e-06
Iter: 1748 loss: 2.04377784e-06
Iter: 1749 loss: 2.04269531e-06
Iter: 1750 loss: 2.04215485e-06
Iter: 1751 loss: 2.04217235e-06
Iter: 1752 loss: 2.04161915e-06
Iter: 1753 loss: 2.04046933e-06
Iter: 1754 loss: 2.06652089e-06
Iter: 1755 loss: 2.04045136e-06
Iter: 1756 loss: 2.03964942e-06
Iter: 1757 loss: 2.04262051e-06
Iter: 1758 loss: 2.03948775e-06
Iter: 1759 loss: 2.03863101e-06
Iter: 1760 loss: 2.0414866e-06
Iter: 1761 loss: 2.03835134e-06
Iter: 1762 loss: 2.03753279e-06
Iter: 1763 loss: 2.03884429e-06
Iter: 1764 loss: 2.03718105e-06
Iter: 1765 loss: 2.03626382e-06
Iter: 1766 loss: 2.0394848e-06
Iter: 1767 loss: 2.03603486e-06
Iter: 1768 loss: 2.03513082e-06
Iter: 1769 loss: 2.03605123e-06
Iter: 1770 loss: 2.0345733e-06
Iter: 1771 loss: 2.03376703e-06
Iter: 1772 loss: 2.03578065e-06
Iter: 1773 loss: 2.0334744e-06
Iter: 1774 loss: 2.03249692e-06
Iter: 1775 loss: 2.03458194e-06
Iter: 1776 loss: 2.03204127e-06
Iter: 1777 loss: 2.03106697e-06
Iter: 1778 loss: 2.03160857e-06
Iter: 1779 loss: 2.03045056e-06
Iter: 1780 loss: 2.02937304e-06
Iter: 1781 loss: 2.03566583e-06
Iter: 1782 loss: 2.02925185e-06
Iter: 1783 loss: 2.02832e-06
Iter: 1784 loss: 2.02919318e-06
Iter: 1785 loss: 2.02783303e-06
Iter: 1786 loss: 2.02654746e-06
Iter: 1787 loss: 2.0421412e-06
Iter: 1788 loss: 2.02651609e-06
Iter: 1789 loss: 2.02612887e-06
Iter: 1790 loss: 2.02530873e-06
Iter: 1791 loss: 2.03841137e-06
Iter: 1792 loss: 2.02531737e-06
Iter: 1793 loss: 2.02409524e-06
Iter: 1794 loss: 2.02881574e-06
Iter: 1795 loss: 2.02381898e-06
Iter: 1796 loss: 2.02305773e-06
Iter: 1797 loss: 2.02685874e-06
Iter: 1798 loss: 2.02295041e-06
Iter: 1799 loss: 2.02212664e-06
Iter: 1800 loss: 2.02271758e-06
Iter: 1801 loss: 2.02163028e-06
Iter: 1802 loss: 2.02068804e-06
Iter: 1803 loss: 2.02106185e-06
Iter: 1804 loss: 2.02000751e-06
Iter: 1805 loss: 2.01882403e-06
Iter: 1806 loss: 2.02368892e-06
Iter: 1807 loss: 2.01859075e-06
Iter: 1808 loss: 2.01758667e-06
Iter: 1809 loss: 2.02472847e-06
Iter: 1810 loss: 2.01752846e-06
Iter: 1811 loss: 2.01679768e-06
Iter: 1812 loss: 2.01610055e-06
Iter: 1813 loss: 2.01591956e-06
Iter: 1814 loss: 2.01472176e-06
Iter: 1815 loss: 2.0223938e-06
Iter: 1816 loss: 2.01464695e-06
Iter: 1817 loss: 2.01356988e-06
Iter: 1818 loss: 2.01414423e-06
Iter: 1819 loss: 2.01284297e-06
Iter: 1820 loss: 2.01172816e-06
Iter: 1821 loss: 2.02006368e-06
Iter: 1822 loss: 2.0116795e-06
Iter: 1823 loss: 2.01104513e-06
Iter: 1824 loss: 2.0110142e-06
Iter: 1825 loss: 2.0106836e-06
Iter: 1826 loss: 2.00975046e-06
Iter: 1827 loss: 2.01314333e-06
Iter: 1828 loss: 2.00931913e-06
Iter: 1829 loss: 2.00810587e-06
Iter: 1830 loss: 2.01135117e-06
Iter: 1831 loss: 2.00769432e-06
Iter: 1832 loss: 2.00678096e-06
Iter: 1833 loss: 2.00678232e-06
Iter: 1834 loss: 2.00584304e-06
Iter: 1835 loss: 2.00572049e-06
Iter: 1836 loss: 2.00507293e-06
Iter: 1837 loss: 2.00402928e-06
Iter: 1838 loss: 2.00601289e-06
Iter: 1839 loss: 2.00363093e-06
Iter: 1840 loss: 2.00258205e-06
Iter: 1841 loss: 2.00848353e-06
Iter: 1842 loss: 2.0024554e-06
Iter: 1843 loss: 2.00133309e-06
Iter: 1844 loss: 2.00224986e-06
Iter: 1845 loss: 2.00064733e-06
Iter: 1846 loss: 1.99974102e-06
Iter: 1847 loss: 2.00140335e-06
Iter: 1848 loss: 1.99935039e-06
Iter: 1849 loss: 1.99832857e-06
Iter: 1850 loss: 2.00303703e-06
Iter: 1851 loss: 1.99802662e-06
Iter: 1852 loss: 1.99717806e-06
Iter: 1853 loss: 1.99828628e-06
Iter: 1854 loss: 1.99673059e-06
Iter: 1855 loss: 1.99581314e-06
Iter: 1856 loss: 2.00546037e-06
Iter: 1857 loss: 1.99577562e-06
Iter: 1858 loss: 1.99483202e-06
Iter: 1859 loss: 1.99795659e-06
Iter: 1860 loss: 1.9946051e-06
Iter: 1861 loss: 1.99406713e-06
Iter: 1862 loss: 1.99347096e-06
Iter: 1863 loss: 1.99333499e-06
Iter: 1864 loss: 1.99228907e-06
Iter: 1865 loss: 1.99308579e-06
Iter: 1866 loss: 1.99158194e-06
Iter: 1867 loss: 1.99064198e-06
Iter: 1868 loss: 1.99426222e-06
Iter: 1869 loss: 1.99030228e-06
Iter: 1870 loss: 1.98932594e-06
Iter: 1871 loss: 1.99788064e-06
Iter: 1872 loss: 1.98926682e-06
Iter: 1873 loss: 1.9884551e-06
Iter: 1874 loss: 1.9878853e-06
Iter: 1875 loss: 1.987622e-06
Iter: 1876 loss: 1.98626503e-06
Iter: 1877 loss: 1.98611338e-06
Iter: 1878 loss: 1.98510952e-06
Iter: 1879 loss: 1.98434054e-06
Iter: 1880 loss: 1.98414318e-06
Iter: 1881 loss: 1.98347971e-06
Iter: 1882 loss: 1.9825668e-06
Iter: 1883 loss: 1.98254975e-06
Iter: 1884 loss: 1.9812328e-06
Iter: 1885 loss: 1.98318457e-06
Iter: 1886 loss: 1.98059456e-06
Iter: 1887 loss: 1.97953409e-06
Iter: 1888 loss: 1.99262604e-06
Iter: 1889 loss: 1.97951636e-06
Iter: 1890 loss: 1.97875033e-06
Iter: 1891 loss: 1.98781368e-06
Iter: 1892 loss: 1.97874533e-06
Iter: 1893 loss: 1.97813551e-06
Iter: 1894 loss: 1.97800478e-06
Iter: 1895 loss: 1.97763165e-06
Iter: 1896 loss: 1.97688405e-06
Iter: 1897 loss: 1.97676923e-06
Iter: 1898 loss: 1.97623581e-06
Iter: 1899 loss: 1.97531563e-06
Iter: 1900 loss: 1.974616e-06
Iter: 1901 loss: 1.9743668e-06
Iter: 1902 loss: 1.97341706e-06
Iter: 1903 loss: 1.97334316e-06
Iter: 1904 loss: 1.97246072e-06
Iter: 1905 loss: 1.9730669e-06
Iter: 1906 loss: 1.97207646e-06
Iter: 1907 loss: 1.97109239e-06
Iter: 1908 loss: 1.97259897e-06
Iter: 1909 loss: 1.97051168e-06
Iter: 1910 loss: 1.96953056e-06
Iter: 1911 loss: 1.97320946e-06
Iter: 1912 loss: 1.96928977e-06
Iter: 1913 loss: 1.9684494e-06
Iter: 1914 loss: 1.96918199e-06
Iter: 1915 loss: 1.96797123e-06
Iter: 1916 loss: 1.96671863e-06
Iter: 1917 loss: 1.9737372e-06
Iter: 1918 loss: 1.96651763e-06
Iter: 1919 loss: 1.96577412e-06
Iter: 1920 loss: 1.96500605e-06
Iter: 1921 loss: 1.96482915e-06
Iter: 1922 loss: 1.96383712e-06
Iter: 1923 loss: 1.97829308e-06
Iter: 1924 loss: 1.96383871e-06
Iter: 1925 loss: 1.96308429e-06
Iter: 1926 loss: 1.97097279e-06
Iter: 1927 loss: 1.96306928e-06
Iter: 1928 loss: 1.96264386e-06
Iter: 1929 loss: 1.96180076e-06
Iter: 1930 loss: 1.97522763e-06
Iter: 1931 loss: 1.96174187e-06
Iter: 1932 loss: 1.9609231e-06
Iter: 1933 loss: 1.96917313e-06
Iter: 1934 loss: 1.96092651e-06
Iter: 1935 loss: 1.96023802e-06
Iter: 1936 loss: 1.95911025e-06
Iter: 1937 loss: 1.95912071e-06
Iter: 1938 loss: 1.95799794e-06
Iter: 1939 loss: 1.96645965e-06
Iter: 1940 loss: 1.95793314e-06
Iter: 1941 loss: 1.95692155e-06
Iter: 1942 loss: 1.9647573e-06
Iter: 1943 loss: 1.95686334e-06
Iter: 1944 loss: 1.95619964e-06
Iter: 1945 loss: 1.95563098e-06
Iter: 1946 loss: 1.95546295e-06
Iter: 1947 loss: 1.95439043e-06
Iter: 1948 loss: 1.95619123e-06
Iter: 1949 loss: 1.95395319e-06
Iter: 1950 loss: 1.95288976e-06
Iter: 1951 loss: 1.96335895e-06
Iter: 1952 loss: 1.95282564e-06
Iter: 1953 loss: 1.95195707e-06
Iter: 1954 loss: 1.95247821e-06
Iter: 1955 loss: 1.95145299e-06
Iter: 1956 loss: 1.95049142e-06
Iter: 1957 loss: 1.95403686e-06
Iter: 1958 loss: 1.95021312e-06
Iter: 1959 loss: 1.94926861e-06
Iter: 1960 loss: 1.95194843e-06
Iter: 1961 loss: 1.9489944e-06
Iter: 1962 loss: 1.94817835e-06
Iter: 1963 loss: 1.94818631e-06
Iter: 1964 loss: 1.94790459e-06
Iter: 1965 loss: 1.94705217e-06
Iter: 1966 loss: 1.95358803e-06
Iter: 1967 loss: 1.94684026e-06
Iter: 1968 loss: 1.94569702e-06
Iter: 1969 loss: 1.9469212e-06
Iter: 1970 loss: 1.94509357e-06
Iter: 1971 loss: 1.94382278e-06
Iter: 1972 loss: 1.95213443e-06
Iter: 1973 loss: 1.94370614e-06
Iter: 1974 loss: 1.94280801e-06
Iter: 1975 loss: 1.95078837e-06
Iter: 1976 loss: 1.94275776e-06
Iter: 1977 loss: 1.94216636e-06
Iter: 1978 loss: 1.94281756e-06
Iter: 1979 loss: 1.94183349e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4
+ date
Mon Oct 26 10:16:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.4_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.4_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8aae40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8abacc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8aaffe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8abd4bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8aa532f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8aa53840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8aab0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8aa2a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8aa17048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a9ee730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a9aa510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a9a6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a9856a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a985378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a91f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a9616a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a8dd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a887ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a845620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a866f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a85b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a85bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa8a85b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa6cb91598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa6cae9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa6cae9d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa6cb459d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa6caad378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa6caad0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa48354730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa4838f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa483451e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa483452f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa482ee730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa4830eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa4830e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000125218852
Iter: 2 loss: 0.000635432487
Iter: 3 loss: 9.66049556e-05
Iter: 4 loss: 8.99100341e-05
Iter: 5 loss: 8.27556214e-05
Iter: 6 loss: 8.15837266e-05
Iter: 7 loss: 7.2115683e-05
Iter: 8 loss: 0.00010246085
Iter: 9 loss: 6.94169139e-05
Iter: 10 loss: 6.54337782e-05
Iter: 11 loss: 6.51758673e-05
Iter: 12 loss: 6.21818617e-05
Iter: 13 loss: 5.76616949e-05
Iter: 14 loss: 6.37987105e-05
Iter: 15 loss: 5.53991958e-05
Iter: 16 loss: 5.27385346e-05
Iter: 17 loss: 5.24996831e-05
Iter: 18 loss: 5.05322969e-05
Iter: 19 loss: 4.81727511e-05
Iter: 20 loss: 4.32312954e-05
Iter: 21 loss: 0.000126348066
Iter: 22 loss: 4.31061089e-05
Iter: 23 loss: 3.90692294e-05
Iter: 24 loss: 4.5185152e-05
Iter: 25 loss: 3.7142665e-05
Iter: 26 loss: 3.38987193e-05
Iter: 27 loss: 7.19275122e-05
Iter: 28 loss: 3.38506252e-05
Iter: 29 loss: 3.22970336e-05
Iter: 30 loss: 3.54987278e-05
Iter: 31 loss: 3.16752157e-05
Iter: 32 loss: 2.95885602e-05
Iter: 33 loss: 2.85466358e-05
Iter: 34 loss: 2.75614675e-05
Iter: 35 loss: 2.58969339e-05
Iter: 36 loss: 4.45536716e-05
Iter: 37 loss: 2.58660784e-05
Iter: 38 loss: 2.43331306e-05
Iter: 39 loss: 2.60876586e-05
Iter: 40 loss: 2.35139269e-05
Iter: 41 loss: 2.25686381e-05
Iter: 42 loss: 3.1280968e-05
Iter: 43 loss: 2.25268723e-05
Iter: 44 loss: 2.18604837e-05
Iter: 45 loss: 3.21928746e-05
Iter: 46 loss: 2.18605273e-05
Iter: 47 loss: 2.13674721e-05
Iter: 48 loss: 2.10864109e-05
Iter: 49 loss: 2.08739184e-05
Iter: 50 loss: 2.10586095e-05
Iter: 51 loss: 2.0645215e-05
Iter: 52 loss: 2.04854477e-05
Iter: 53 loss: 1.99958522e-05
Iter: 54 loss: 2.11137e-05
Iter: 55 loss: 1.97058689e-05
Iter: 56 loss: 1.88266386e-05
Iter: 57 loss: 2.12963751e-05
Iter: 58 loss: 1.85468762e-05
Iter: 59 loss: 1.79266208e-05
Iter: 60 loss: 2.52138088e-05
Iter: 61 loss: 1.79174276e-05
Iter: 62 loss: 1.74882589e-05
Iter: 63 loss: 1.72883665e-05
Iter: 64 loss: 1.70768762e-05
Iter: 65 loss: 1.6487802e-05
Iter: 66 loss: 1.79423223e-05
Iter: 67 loss: 1.62792639e-05
Iter: 68 loss: 1.57738432e-05
Iter: 69 loss: 1.9121846e-05
Iter: 70 loss: 1.57211052e-05
Iter: 71 loss: 1.53391156e-05
Iter: 72 loss: 1.63450968e-05
Iter: 73 loss: 1.52102275e-05
Iter: 74 loss: 1.47918518e-05
Iter: 75 loss: 1.52616867e-05
Iter: 76 loss: 1.45662743e-05
Iter: 77 loss: 1.41047258e-05
Iter: 78 loss: 1.49675598e-05
Iter: 79 loss: 1.39083304e-05
Iter: 80 loss: 1.3768843e-05
Iter: 81 loss: 1.36605022e-05
Iter: 82 loss: 1.34820912e-05
Iter: 83 loss: 1.44270252e-05
Iter: 84 loss: 1.34547972e-05
Iter: 85 loss: 1.33173617e-05
Iter: 86 loss: 1.39085732e-05
Iter: 87 loss: 1.32886034e-05
Iter: 88 loss: 1.31901115e-05
Iter: 89 loss: 1.28958436e-05
Iter: 90 loss: 1.38117475e-05
Iter: 91 loss: 1.27505009e-05
Iter: 92 loss: 1.23705022e-05
Iter: 93 loss: 1.57148934e-05
Iter: 94 loss: 1.23516966e-05
Iter: 95 loss: 1.21345784e-05
Iter: 96 loss: 1.3662624e-05
Iter: 97 loss: 1.21149615e-05
Iter: 98 loss: 1.1910688e-05
Iter: 99 loss: 1.19765773e-05
Iter: 100 loss: 1.17656691e-05
Iter: 101 loss: 1.15550411e-05
Iter: 102 loss: 1.24821217e-05
Iter: 103 loss: 1.15131334e-05
Iter: 104 loss: 1.13140741e-05
Iter: 105 loss: 1.12258203e-05
Iter: 106 loss: 1.11251857e-05
Iter: 107 loss: 1.09109242e-05
Iter: 108 loss: 1.26742216e-05
Iter: 109 loss: 1.08973227e-05
Iter: 110 loss: 1.07363494e-05
Iter: 111 loss: 1.14690929e-05
Iter: 112 loss: 1.07054275e-05
Iter: 113 loss: 1.05701347e-05
Iter: 114 loss: 1.10519486e-05
Iter: 115 loss: 1.05356266e-05
Iter: 116 loss: 1.04575283e-05
Iter: 117 loss: 1.04513201e-05
Iter: 118 loss: 1.03766697e-05
Iter: 119 loss: 1.0513947e-05
Iter: 120 loss: 1.03445291e-05
Iter: 121 loss: 1.02574195e-05
Iter: 122 loss: 1.02564863e-05
Iter: 123 loss: 1.01873411e-05
Iter: 124 loss: 1.010902e-05
Iter: 125 loss: 1.00154693e-05
Iter: 126 loss: 1.00058678e-05
Iter: 127 loss: 9.89373348e-06
Iter: 128 loss: 1.03411094e-05
Iter: 129 loss: 9.86836312e-06
Iter: 130 loss: 9.75925e-06
Iter: 131 loss: 1.00212919e-05
Iter: 132 loss: 9.71956706e-06
Iter: 133 loss: 9.58237797e-06
Iter: 134 loss: 1.0118838e-05
Iter: 135 loss: 9.55066844e-06
Iter: 136 loss: 9.4612642e-06
Iter: 137 loss: 9.48360776e-06
Iter: 138 loss: 9.39591519e-06
Iter: 139 loss: 9.26629309e-06
Iter: 140 loss: 9.59848148e-06
Iter: 141 loss: 9.22152503e-06
Iter: 142 loss: 9.12026917e-06
Iter: 143 loss: 9.28746158e-06
Iter: 144 loss: 9.07416779e-06
Iter: 145 loss: 8.9491e-06
Iter: 146 loss: 9.70635665e-06
Iter: 147 loss: 8.93381275e-06
Iter: 148 loss: 8.87758415e-06
Iter: 149 loss: 9.52193841e-06
Iter: 150 loss: 8.87664646e-06
Iter: 151 loss: 8.82386303e-06
Iter: 152 loss: 9.32154762e-06
Iter: 153 loss: 8.82194854e-06
Iter: 154 loss: 8.78133869e-06
Iter: 155 loss: 8.78122319e-06
Iter: 156 loss: 8.74867328e-06
Iter: 157 loss: 8.69104e-06
Iter: 158 loss: 8.69027463e-06
Iter: 159 loss: 8.64478716e-06
Iter: 160 loss: 8.58825842e-06
Iter: 161 loss: 8.53132769e-06
Iter: 162 loss: 8.52010544e-06
Iter: 163 loss: 8.43224188e-06
Iter: 164 loss: 8.92669595e-06
Iter: 165 loss: 8.41995461e-06
Iter: 166 loss: 8.35784886e-06
Iter: 167 loss: 8.64590402e-06
Iter: 168 loss: 8.34637376e-06
Iter: 169 loss: 8.29213e-06
Iter: 170 loss: 8.48642412e-06
Iter: 171 loss: 8.27839449e-06
Iter: 172 loss: 8.21826325e-06
Iter: 173 loss: 8.26688392e-06
Iter: 174 loss: 8.18248373e-06
Iter: 175 loss: 8.1113194e-06
Iter: 176 loss: 8.2563356e-06
Iter: 177 loss: 8.08265941e-06
Iter: 178 loss: 8.02350496e-06
Iter: 179 loss: 8.15141266e-06
Iter: 180 loss: 8.00069574e-06
Iter: 181 loss: 7.92860556e-06
Iter: 182 loss: 8.13909264e-06
Iter: 183 loss: 7.90641116e-06
Iter: 184 loss: 7.87562567e-06
Iter: 185 loss: 7.87523913e-06
Iter: 186 loss: 7.83430278e-06
Iter: 187 loss: 7.89314072e-06
Iter: 188 loss: 7.81445669e-06
Iter: 189 loss: 7.78409049e-06
Iter: 190 loss: 7.89020669e-06
Iter: 191 loss: 7.7761415e-06
Iter: 192 loss: 7.75126045e-06
Iter: 193 loss: 7.72192652e-06
Iter: 194 loss: 7.71851228e-06
Iter: 195 loss: 7.67434e-06
Iter: 196 loss: 7.66238372e-06
Iter: 197 loss: 7.63538901e-06
Iter: 198 loss: 7.57813541e-06
Iter: 199 loss: 7.9986221e-06
Iter: 200 loss: 7.57348971e-06
Iter: 201 loss: 7.53029144e-06
Iter: 202 loss: 7.50884465e-06
Iter: 203 loss: 7.48838e-06
Iter: 204 loss: 7.44201225e-06
Iter: 205 loss: 7.44195586e-06
Iter: 206 loss: 7.40731593e-06
Iter: 207 loss: 7.44702447e-06
Iter: 208 loss: 7.38874132e-06
Iter: 209 loss: 7.34787955e-06
Iter: 210 loss: 7.34927289e-06
Iter: 211 loss: 7.31575892e-06
Iter: 212 loss: 7.26526559e-06
Iter: 213 loss: 7.50032e-06
Iter: 214 loss: 7.25605241e-06
Iter: 215 loss: 7.21553897e-06
Iter: 216 loss: 7.37046685e-06
Iter: 217 loss: 7.20605658e-06
Iter: 218 loss: 7.20272055e-06
Iter: 219 loss: 7.19159925e-06
Iter: 220 loss: 7.17423336e-06
Iter: 221 loss: 7.1417262e-06
Iter: 222 loss: 7.86912187e-06
Iter: 223 loss: 7.14158068e-06
Iter: 224 loss: 7.11653956e-06
Iter: 225 loss: 7.33984098e-06
Iter: 226 loss: 7.1152258e-06
Iter: 227 loss: 7.09725236e-06
Iter: 228 loss: 7.08366315e-06
Iter: 229 loss: 7.07778418e-06
Iter: 230 loss: 7.04857e-06
Iter: 231 loss: 7.01599856e-06
Iter: 232 loss: 7.01167755e-06
Iter: 233 loss: 6.97989162e-06
Iter: 234 loss: 7.39650477e-06
Iter: 235 loss: 6.97963696e-06
Iter: 236 loss: 6.9503817e-06
Iter: 237 loss: 6.95345761e-06
Iter: 238 loss: 6.92821231e-06
Iter: 239 loss: 6.89259105e-06
Iter: 240 loss: 7.11295e-06
Iter: 241 loss: 6.88860428e-06
Iter: 242 loss: 6.85871646e-06
Iter: 243 loss: 6.96774259e-06
Iter: 244 loss: 6.85127543e-06
Iter: 245 loss: 6.82259179e-06
Iter: 246 loss: 6.88623732e-06
Iter: 247 loss: 6.81167103e-06
Iter: 248 loss: 6.78744709e-06
Iter: 249 loss: 6.79542745e-06
Iter: 250 loss: 6.77013759e-06
Iter: 251 loss: 6.73547765e-06
Iter: 252 loss: 6.8618333e-06
Iter: 253 loss: 6.72678334e-06
Iter: 254 loss: 6.72726128e-06
Iter: 255 loss: 6.70988311e-06
Iter: 256 loss: 6.70310601e-06
Iter: 257 loss: 6.68570465e-06
Iter: 258 loss: 6.82982682e-06
Iter: 259 loss: 6.68268194e-06
Iter: 260 loss: 6.66241522e-06
Iter: 261 loss: 6.83794178e-06
Iter: 262 loss: 6.66128972e-06
Iter: 263 loss: 6.6488783e-06
Iter: 264 loss: 6.63260971e-06
Iter: 265 loss: 6.63134142e-06
Iter: 266 loss: 6.60771366e-06
Iter: 267 loss: 6.63712126e-06
Iter: 268 loss: 6.59534908e-06
Iter: 269 loss: 6.57208147e-06
Iter: 270 loss: 6.67846871e-06
Iter: 271 loss: 6.56769407e-06
Iter: 272 loss: 6.54370342e-06
Iter: 273 loss: 6.55490931e-06
Iter: 274 loss: 6.52737344e-06
Iter: 275 loss: 6.50467337e-06
Iter: 276 loss: 6.82015479e-06
Iter: 277 loss: 6.50458924e-06
Iter: 278 loss: 6.48722835e-06
Iter: 279 loss: 6.48814148e-06
Iter: 280 loss: 6.4735932e-06
Iter: 281 loss: 6.44757802e-06
Iter: 282 loss: 6.55929034e-06
Iter: 283 loss: 6.44206375e-06
Iter: 284 loss: 6.42422629e-06
Iter: 285 loss: 6.42372e-06
Iter: 286 loss: 6.41004863e-06
Iter: 287 loss: 6.40198323e-06
Iter: 288 loss: 6.39725113e-06
Iter: 289 loss: 6.38158963e-06
Iter: 290 loss: 6.38353822e-06
Iter: 291 loss: 6.37009543e-06
Iter: 292 loss: 6.36115601e-06
Iter: 293 loss: 6.36497043e-06
Iter: 294 loss: 6.35518609e-06
Iter: 295 loss: 6.34103162e-06
Iter: 296 loss: 6.37376797e-06
Iter: 297 loss: 6.33579111e-06
Iter: 298 loss: 6.32263163e-06
Iter: 299 loss: 6.30280829e-06
Iter: 300 loss: 6.30255818e-06
Iter: 301 loss: 6.28116959e-06
Iter: 302 loss: 6.38439042e-06
Iter: 303 loss: 6.27741338e-06
Iter: 304 loss: 6.25942175e-06
Iter: 305 loss: 6.33538502e-06
Iter: 306 loss: 6.25578e-06
Iter: 307 loss: 6.23795859e-06
Iter: 308 loss: 6.25159191e-06
Iter: 309 loss: 6.22668358e-06
Iter: 310 loss: 6.20941591e-06
Iter: 311 loss: 6.39679774e-06
Iter: 312 loss: 6.20891569e-06
Iter: 313 loss: 6.19530965e-06
Iter: 314 loss: 6.20775063e-06
Iter: 315 loss: 6.18753802e-06
Iter: 316 loss: 6.16890793e-06
Iter: 317 loss: 6.21330628e-06
Iter: 318 loss: 6.16213856e-06
Iter: 319 loss: 6.14739156e-06
Iter: 320 loss: 6.163335e-06
Iter: 321 loss: 6.13921657e-06
Iter: 322 loss: 6.14128385e-06
Iter: 323 loss: 6.13062593e-06
Iter: 324 loss: 6.1256419e-06
Iter: 325 loss: 6.11194537e-06
Iter: 326 loss: 6.19697266e-06
Iter: 327 loss: 6.1085766e-06
Iter: 328 loss: 6.09691142e-06
Iter: 329 loss: 6.19339926e-06
Iter: 330 loss: 6.09611561e-06
Iter: 331 loss: 6.08282744e-06
Iter: 332 loss: 6.07940729e-06
Iter: 333 loss: 6.07112543e-06
Iter: 334 loss: 6.05510286e-06
Iter: 335 loss: 6.0669272e-06
Iter: 336 loss: 6.04536126e-06
Iter: 337 loss: 6.02931141e-06
Iter: 338 loss: 6.05547757e-06
Iter: 339 loss: 6.02199725e-06
Iter: 340 loss: 6.00315843e-06
Iter: 341 loss: 6.0563e-06
Iter: 342 loss: 5.99722625e-06
Iter: 343 loss: 5.98144743e-06
Iter: 344 loss: 6.07943275e-06
Iter: 345 loss: 5.97961935e-06
Iter: 346 loss: 5.96578457e-06
Iter: 347 loss: 6.01170359e-06
Iter: 348 loss: 5.96214613e-06
Iter: 349 loss: 5.94825588e-06
Iter: 350 loss: 5.971463e-06
Iter: 351 loss: 5.94189896e-06
Iter: 352 loss: 5.92596916e-06
Iter: 353 loss: 5.99046098e-06
Iter: 354 loss: 5.92243941e-06
Iter: 355 loss: 5.91429352e-06
Iter: 356 loss: 6.03766057e-06
Iter: 357 loss: 5.91429762e-06
Iter: 358 loss: 5.90333684e-06
Iter: 359 loss: 5.90529817e-06
Iter: 360 loss: 5.89522188e-06
Iter: 361 loss: 5.88712555e-06
Iter: 362 loss: 5.8776277e-06
Iter: 363 loss: 5.87654267e-06
Iter: 364 loss: 5.86897568e-06
Iter: 365 loss: 5.8687092e-06
Iter: 366 loss: 5.8613864e-06
Iter: 367 loss: 5.84464215e-06
Iter: 368 loss: 6.06041476e-06
Iter: 369 loss: 5.84362806e-06
Iter: 370 loss: 5.82815801e-06
Iter: 371 loss: 5.90941409e-06
Iter: 372 loss: 5.82546545e-06
Iter: 373 loss: 5.81198856e-06
Iter: 374 loss: 5.83368183e-06
Iter: 375 loss: 5.80601227e-06
Iter: 376 loss: 5.79310745e-06
Iter: 377 loss: 5.84156714e-06
Iter: 378 loss: 5.78984645e-06
Iter: 379 loss: 5.77820629e-06
Iter: 380 loss: 5.80605683e-06
Iter: 381 loss: 5.77366336e-06
Iter: 382 loss: 5.75939339e-06
Iter: 383 loss: 5.82825032e-06
Iter: 384 loss: 5.75680679e-06
Iter: 385 loss: 5.74486603e-06
Iter: 386 loss: 5.77487572e-06
Iter: 387 loss: 5.74063779e-06
Iter: 388 loss: 5.72933277e-06
Iter: 389 loss: 5.77456e-06
Iter: 390 loss: 5.72690624e-06
Iter: 391 loss: 5.72258796e-06
Iter: 392 loss: 5.72055706e-06
Iter: 393 loss: 5.7170414e-06
Iter: 394 loss: 5.70699194e-06
Iter: 395 loss: 5.77343735e-06
Iter: 396 loss: 5.70465818e-06
Iter: 397 loss: 5.69548592e-06
Iter: 398 loss: 5.7327361e-06
Iter: 399 loss: 5.69354052e-06
Iter: 400 loss: 5.68602718e-06
Iter: 401 loss: 5.77370474e-06
Iter: 402 loss: 5.68593396e-06
Iter: 403 loss: 5.68050928e-06
Iter: 404 loss: 5.66668677e-06
Iter: 405 loss: 5.79265179e-06
Iter: 406 loss: 5.6649742e-06
Iter: 407 loss: 5.65111168e-06
Iter: 408 loss: 5.76268485e-06
Iter: 409 loss: 5.65020491e-06
Iter: 410 loss: 5.64000584e-06
Iter: 411 loss: 5.63782e-06
Iter: 412 loss: 5.63085905e-06
Iter: 413 loss: 5.61732668e-06
Iter: 414 loss: 5.71273722e-06
Iter: 415 loss: 5.61608977e-06
Iter: 416 loss: 5.60514354e-06
Iter: 417 loss: 5.64304901e-06
Iter: 418 loss: 5.60208855e-06
Iter: 419 loss: 5.59159e-06
Iter: 420 loss: 5.64608627e-06
Iter: 421 loss: 5.58984266e-06
Iter: 422 loss: 5.58171405e-06
Iter: 423 loss: 5.60161425e-06
Iter: 424 loss: 5.57870771e-06
Iter: 425 loss: 5.5754831e-06
Iter: 426 loss: 5.5743817e-06
Iter: 427 loss: 5.56948908e-06
Iter: 428 loss: 5.55981342e-06
Iter: 429 loss: 5.75353215e-06
Iter: 430 loss: 5.5596247e-06
Iter: 431 loss: 5.5520004e-06
Iter: 432 loss: 5.55841052e-06
Iter: 433 loss: 5.54720691e-06
Iter: 434 loss: 5.5406781e-06
Iter: 435 loss: 5.61577326e-06
Iter: 436 loss: 5.54056442e-06
Iter: 437 loss: 5.53438349e-06
Iter: 438 loss: 5.53570408e-06
Iter: 439 loss: 5.52988649e-06
Iter: 440 loss: 5.5223627e-06
Iter: 441 loss: 5.5151022e-06
Iter: 442 loss: 5.51353105e-06
Iter: 443 loss: 5.5002356e-06
Iter: 444 loss: 5.5438e-06
Iter: 445 loss: 5.4967395e-06
Iter: 446 loss: 5.4853258e-06
Iter: 447 loss: 5.50052482e-06
Iter: 448 loss: 5.47937043e-06
Iter: 449 loss: 5.46873025e-06
Iter: 450 loss: 5.55718907e-06
Iter: 451 loss: 5.46819319e-06
Iter: 452 loss: 5.46013416e-06
Iter: 453 loss: 5.47149284e-06
Iter: 454 loss: 5.45596231e-06
Iter: 455 loss: 5.44568502e-06
Iter: 456 loss: 5.51922494e-06
Iter: 457 loss: 5.44492377e-06
Iter: 458 loss: 5.4384368e-06
Iter: 459 loss: 5.47683658e-06
Iter: 460 loss: 5.43755914e-06
Iter: 461 loss: 5.43084889e-06
Iter: 462 loss: 5.48695107e-06
Iter: 463 loss: 5.43046781e-06
Iter: 464 loss: 5.42733278e-06
Iter: 465 loss: 5.41867848e-06
Iter: 466 loss: 5.47616219e-06
Iter: 467 loss: 5.41659847e-06
Iter: 468 loss: 5.40790643e-06
Iter: 469 loss: 5.47501941e-06
Iter: 470 loss: 5.40714973e-06
Iter: 471 loss: 5.40046221e-06
Iter: 472 loss: 5.45997e-06
Iter: 473 loss: 5.40012206e-06
Iter: 474 loss: 5.3943113e-06
Iter: 475 loss: 5.38298264e-06
Iter: 476 loss: 5.60947501e-06
Iter: 477 loss: 5.38283621e-06
Iter: 478 loss: 5.37133e-06
Iter: 479 loss: 5.42434736e-06
Iter: 480 loss: 5.3693484e-06
Iter: 481 loss: 5.36074231e-06
Iter: 482 loss: 5.37256346e-06
Iter: 483 loss: 5.35655545e-06
Iter: 484 loss: 5.3470867e-06
Iter: 485 loss: 5.37256665e-06
Iter: 486 loss: 5.34403171e-06
Iter: 487 loss: 5.33618368e-06
Iter: 488 loss: 5.40462497e-06
Iter: 489 loss: 5.33582943e-06
Iter: 490 loss: 5.32978538e-06
Iter: 491 loss: 5.33541743e-06
Iter: 492 loss: 5.32660397e-06
Iter: 493 loss: 5.31843534e-06
Iter: 494 loss: 5.38236145e-06
Iter: 495 loss: 5.31777505e-06
Iter: 496 loss: 5.31394289e-06
Iter: 497 loss: 5.31359728e-06
Iter: 498 loss: 5.31066189e-06
Iter: 499 loss: 5.30289799e-06
Iter: 500 loss: 5.36214702e-06
Iter: 501 loss: 5.30122861e-06
Iter: 502 loss: 5.29378212e-06
Iter: 503 loss: 5.30697343e-06
Iter: 504 loss: 5.29050612e-06
Iter: 505 loss: 5.2848718e-06
Iter: 506 loss: 5.37219e-06
Iter: 507 loss: 5.28481769e-06
Iter: 508 loss: 5.28015516e-06
Iter: 509 loss: 5.28402688e-06
Iter: 510 loss: 5.27730663e-06
Iter: 511 loss: 5.2717678e-06
Iter: 512 loss: 5.26595431e-06
Iter: 513 loss: 5.26517624e-06
Iter: 514 loss: 5.25609039e-06
Iter: 515 loss: 5.29315048e-06
Iter: 516 loss: 5.25412543e-06
Iter: 517 loss: 5.2458463e-06
Iter: 518 loss: 5.24462575e-06
Iter: 519 loss: 5.23895051e-06
Iter: 520 loss: 5.22918799e-06
Iter: 521 loss: 5.32028389e-06
Iter: 522 loss: 5.22876735e-06
Iter: 523 loss: 5.22213941e-06
Iter: 524 loss: 5.24104416e-06
Iter: 525 loss: 5.22003074e-06
Iter: 526 loss: 5.21350557e-06
Iter: 527 loss: 5.25945688e-06
Iter: 528 loss: 5.21301399e-06
Iter: 529 loss: 5.20924277e-06
Iter: 530 loss: 5.2090395e-06
Iter: 531 loss: 5.2052219e-06
Iter: 532 loss: 5.20057438e-06
Iter: 533 loss: 5.20014601e-06
Iter: 534 loss: 5.19658e-06
Iter: 535 loss: 5.1966972e-06
Iter: 536 loss: 5.1937368e-06
Iter: 537 loss: 5.18883098e-06
Iter: 538 loss: 5.20356389e-06
Iter: 539 loss: 5.18742581e-06
Iter: 540 loss: 5.18190245e-06
Iter: 541 loss: 5.2139294e-06
Iter: 542 loss: 5.18109391e-06
Iter: 543 loss: 5.17717308e-06
Iter: 544 loss: 5.17406079e-06
Iter: 545 loss: 5.17283434e-06
Iter: 546 loss: 5.16631781e-06
Iter: 547 loss: 5.16791488e-06
Iter: 548 loss: 5.16157024e-06
Iter: 549 loss: 5.15352212e-06
Iter: 550 loss: 5.20153753e-06
Iter: 551 loss: 5.15259444e-06
Iter: 552 loss: 5.14632484e-06
Iter: 553 loss: 5.14724e-06
Iter: 554 loss: 5.1418283e-06
Iter: 555 loss: 5.13342e-06
Iter: 556 loss: 5.18512115e-06
Iter: 557 loss: 5.13247096e-06
Iter: 558 loss: 5.12705356e-06
Iter: 559 loss: 5.15104057e-06
Iter: 560 loss: 5.12604902e-06
Iter: 561 loss: 5.12280485e-06
Iter: 562 loss: 5.12265524e-06
Iter: 563 loss: 5.11921735e-06
Iter: 564 loss: 5.1208e-06
Iter: 565 loss: 5.11694498e-06
Iter: 566 loss: 5.11367216e-06
Iter: 567 loss: 5.10748e-06
Iter: 568 loss: 5.22877372e-06
Iter: 569 loss: 5.10736299e-06
Iter: 570 loss: 5.10142399e-06
Iter: 571 loss: 5.13962459e-06
Iter: 572 loss: 5.10091832e-06
Iter: 573 loss: 5.09632264e-06
Iter: 574 loss: 5.13995155e-06
Iter: 575 loss: 5.09618758e-06
Iter: 576 loss: 5.09214624e-06
Iter: 577 loss: 5.08924541e-06
Iter: 578 loss: 5.08815174e-06
Iter: 579 loss: 5.08266703e-06
Iter: 580 loss: 5.09427264e-06
Iter: 581 loss: 5.08069115e-06
Iter: 582 loss: 5.07522418e-06
Iter: 583 loss: 5.07920413e-06
Iter: 584 loss: 5.07194909e-06
Iter: 585 loss: 5.06485594e-06
Iter: 586 loss: 5.08254789e-06
Iter: 587 loss: 5.06230253e-06
Iter: 588 loss: 5.05526805e-06
Iter: 589 loss: 5.07013101e-06
Iter: 590 loss: 5.05251592e-06
Iter: 591 loss: 5.04544096e-06
Iter: 592 loss: 5.09031406e-06
Iter: 593 loss: 5.04481613e-06
Iter: 594 loss: 5.04243872e-06
Iter: 595 loss: 5.04201762e-06
Iter: 596 loss: 5.03905449e-06
Iter: 597 loss: 5.04273703e-06
Iter: 598 loss: 5.03742876e-06
Iter: 599 loss: 5.03413457e-06
Iter: 600 loss: 5.02780495e-06
Iter: 601 loss: 5.16536329e-06
Iter: 602 loss: 5.02792409e-06
Iter: 603 loss: 5.02319654e-06
Iter: 604 loss: 5.04725813e-06
Iter: 605 loss: 5.02237026e-06
Iter: 606 loss: 5.0185522e-06
Iter: 607 loss: 5.04272248e-06
Iter: 608 loss: 5.0181884e-06
Iter: 609 loss: 5.01414161e-06
Iter: 610 loss: 5.01749219e-06
Iter: 611 loss: 5.01151271e-06
Iter: 612 loss: 5.00757415e-06
Iter: 613 loss: 5.00969145e-06
Iter: 614 loss: 5.00498572e-06
Iter: 615 loss: 5.00011811e-06
Iter: 616 loss: 5.00962733e-06
Iter: 617 loss: 4.99806038e-06
Iter: 618 loss: 4.99241924e-06
Iter: 619 loss: 4.99766793e-06
Iter: 620 loss: 4.98914051e-06
Iter: 621 loss: 4.98274403e-06
Iter: 622 loss: 5.0065205e-06
Iter: 623 loss: 4.98114105e-06
Iter: 624 loss: 4.97525343e-06
Iter: 625 loss: 4.98798909e-06
Iter: 626 loss: 4.97277415e-06
Iter: 627 loss: 4.9681089e-06
Iter: 628 loss: 4.96807934e-06
Iter: 629 loss: 4.96434404e-06
Iter: 630 loss: 5.01336854e-06
Iter: 631 loss: 4.96430221e-06
Iter: 632 loss: 4.96232315e-06
Iter: 633 loss: 4.95843824e-06
Iter: 634 loss: 5.0390081e-06
Iter: 635 loss: 4.95836548e-06
Iter: 636 loss: 4.95409131e-06
Iter: 637 loss: 4.94896085e-06
Iter: 638 loss: 4.94830283e-06
Iter: 639 loss: 4.94306823e-06
Iter: 640 loss: 4.94302367e-06
Iter: 641 loss: 4.93939433e-06
Iter: 642 loss: 4.96172333e-06
Iter: 643 loss: 4.93901416e-06
Iter: 644 loss: 4.93607513e-06
Iter: 645 loss: 4.93143807e-06
Iter: 646 loss: 4.93140533e-06
Iter: 647 loss: 4.9263881e-06
Iter: 648 loss: 4.95478525e-06
Iter: 649 loss: 4.92555682e-06
Iter: 650 loss: 4.92145136e-06
Iter: 651 loss: 4.92285835e-06
Iter: 652 loss: 4.91835226e-06
Iter: 653 loss: 4.91263836e-06
Iter: 654 loss: 4.92912568e-06
Iter: 655 loss: 4.91064475e-06
Iter: 656 loss: 4.90530101e-06
Iter: 657 loss: 4.92166919e-06
Iter: 658 loss: 4.90360298e-06
Iter: 659 loss: 4.89887134e-06
Iter: 660 loss: 4.93064e-06
Iter: 661 loss: 4.89843478e-06
Iter: 662 loss: 4.89644663e-06
Iter: 663 loss: 4.89580725e-06
Iter: 664 loss: 4.89415197e-06
Iter: 665 loss: 4.89073045e-06
Iter: 666 loss: 4.9524192e-06
Iter: 667 loss: 4.89059266e-06
Iter: 668 loss: 4.88689466e-06
Iter: 669 loss: 4.88764044e-06
Iter: 670 loss: 4.88426076e-06
Iter: 671 loss: 4.88009e-06
Iter: 672 loss: 4.89082231e-06
Iter: 673 loss: 4.87856869e-06
Iter: 674 loss: 4.87426e-06
Iter: 675 loss: 4.91889205e-06
Iter: 676 loss: 4.87412763e-06
Iter: 677 loss: 4.8707343e-06
Iter: 678 loss: 4.86943281e-06
Iter: 679 loss: 4.86758336e-06
Iter: 680 loss: 4.86342196e-06
Iter: 681 loss: 4.86558474e-06
Iter: 682 loss: 4.86056206e-06
Iter: 683 loss: 4.85532246e-06
Iter: 684 loss: 4.88266051e-06
Iter: 685 loss: 4.85471082e-06
Iter: 686 loss: 4.85073906e-06
Iter: 687 loss: 4.84880366e-06
Iter: 688 loss: 4.8470165e-06
Iter: 689 loss: 4.84054181e-06
Iter: 690 loss: 4.8767497e-06
Iter: 691 loss: 4.83961503e-06
Iter: 692 loss: 4.83516487e-06
Iter: 693 loss: 4.84700195e-06
Iter: 694 loss: 4.83376743e-06
Iter: 695 loss: 4.8345637e-06
Iter: 696 loss: 4.83189524e-06
Iter: 697 loss: 4.83007898e-06
Iter: 698 loss: 4.82578071e-06
Iter: 699 loss: 4.87840134e-06
Iter: 700 loss: 4.82538871e-06
Iter: 701 loss: 4.82088353e-06
Iter: 702 loss: 4.83086387e-06
Iter: 703 loss: 4.81903817e-06
Iter: 704 loss: 4.81508414e-06
Iter: 705 loss: 4.81747884e-06
Iter: 706 loss: 4.81264306e-06
Iter: 707 loss: 4.80824e-06
Iter: 708 loss: 4.8616339e-06
Iter: 709 loss: 4.80825474e-06
Iter: 710 loss: 4.80447352e-06
Iter: 711 loss: 4.80509743e-06
Iter: 712 loss: 4.80167728e-06
Iter: 713 loss: 4.79759956e-06
Iter: 714 loss: 4.79979462e-06
Iter: 715 loss: 4.79491609e-06
Iter: 716 loss: 4.79018581e-06
Iter: 717 loss: 4.80640165e-06
Iter: 718 loss: 4.78890615e-06
Iter: 719 loss: 4.78434504e-06
Iter: 720 loss: 4.79233768e-06
Iter: 721 loss: 4.78224865e-06
Iter: 722 loss: 4.77803405e-06
Iter: 723 loss: 4.7846529e-06
Iter: 724 loss: 4.77601861e-06
Iter: 725 loss: 4.7706344e-06
Iter: 726 loss: 4.78955644e-06
Iter: 727 loss: 4.76922196e-06
Iter: 728 loss: 4.76789955e-06
Iter: 729 loss: 4.76716787e-06
Iter: 730 loss: 4.76436253e-06
Iter: 731 loss: 4.7623962e-06
Iter: 732 loss: 4.76138803e-06
Iter: 733 loss: 4.75812431e-06
Iter: 734 loss: 4.75545812e-06
Iter: 735 loss: 4.75452907e-06
Iter: 736 loss: 4.74973695e-06
Iter: 737 loss: 4.76596279e-06
Iter: 738 loss: 4.74872741e-06
Iter: 739 loss: 4.74521948e-06
Iter: 740 loss: 4.76005243e-06
Iter: 741 loss: 4.74445824e-06
Iter: 742 loss: 4.74006629e-06
Iter: 743 loss: 4.74964145e-06
Iter: 744 loss: 4.73836917e-06
Iter: 745 loss: 4.73449927e-06
Iter: 746 loss: 4.73325053e-06
Iter: 747 loss: 4.73078353e-06
Iter: 748 loss: 4.72589e-06
Iter: 749 loss: 4.73606269e-06
Iter: 750 loss: 4.7239032e-06
Iter: 751 loss: 4.7183762e-06
Iter: 752 loss: 4.73915634e-06
Iter: 753 loss: 4.71704698e-06
Iter: 754 loss: 4.71247495e-06
Iter: 755 loss: 4.71933708e-06
Iter: 756 loss: 4.71020758e-06
Iter: 757 loss: 4.70543182e-06
Iter: 758 loss: 4.72237525e-06
Iter: 759 loss: 4.70405666e-06
Iter: 760 loss: 4.70023269e-06
Iter: 761 loss: 4.73258206e-06
Iter: 762 loss: 4.70012947e-06
Iter: 763 loss: 4.69701718e-06
Iter: 764 loss: 4.69708903e-06
Iter: 765 loss: 4.69576389e-06
Iter: 766 loss: 4.69287716e-06
Iter: 767 loss: 4.71469048e-06
Iter: 768 loss: 4.6922346e-06
Iter: 769 loss: 4.68796406e-06
Iter: 770 loss: 4.69642146e-06
Iter: 771 loss: 4.68625e-06
Iter: 772 loss: 4.68231246e-06
Iter: 773 loss: 4.69460429e-06
Iter: 774 loss: 4.68119697e-06
Iter: 775 loss: 4.67801874e-06
Iter: 776 loss: 4.71947124e-06
Iter: 777 loss: 4.67801692e-06
Iter: 778 loss: 4.67574955e-06
Iter: 779 loss: 4.6715e-06
Iter: 780 loss: 4.77428966e-06
Iter: 781 loss: 4.67150403e-06
Iter: 782 loss: 4.66693382e-06
Iter: 783 loss: 4.68568805e-06
Iter: 784 loss: 4.66584333e-06
Iter: 785 loss: 4.66195343e-06
Iter: 786 loss: 4.66698384e-06
Iter: 787 loss: 4.66002075e-06
Iter: 788 loss: 4.65471e-06
Iter: 789 loss: 4.66727715e-06
Iter: 790 loss: 4.65278345e-06
Iter: 791 loss: 4.64868299e-06
Iter: 792 loss: 4.66197707e-06
Iter: 793 loss: 4.64743653e-06
Iter: 794 loss: 4.6431187e-06
Iter: 795 loss: 4.65625908e-06
Iter: 796 loss: 4.64181267e-06
Iter: 797 loss: 4.64455e-06
Iter: 798 loss: 4.6405512e-06
Iter: 799 loss: 4.63955166e-06
Iter: 800 loss: 4.63665037e-06
Iter: 801 loss: 4.64288132e-06
Iter: 802 loss: 4.63495871e-06
Iter: 803 loss: 4.63079596e-06
Iter: 804 loss: 4.64832419e-06
Iter: 805 loss: 4.6299615e-06
Iter: 806 loss: 4.62578282e-06
Iter: 807 loss: 4.63549168e-06
Iter: 808 loss: 4.62426e-06
Iter: 809 loss: 4.62121125e-06
Iter: 810 loss: 4.66220899e-06
Iter: 811 loss: 4.62122398e-06
Iter: 812 loss: 4.61867239e-06
Iter: 813 loss: 4.61824129e-06
Iter: 814 loss: 4.61665695e-06
Iter: 815 loss: 4.61386435e-06
Iter: 816 loss: 4.61416039e-06
Iter: 817 loss: 4.61152877e-06
Iter: 818 loss: 4.60798583e-06
Iter: 819 loss: 4.62342177e-06
Iter: 820 loss: 4.60721276e-06
Iter: 821 loss: 4.60398951e-06
Iter: 822 loss: 4.61044328e-06
Iter: 823 loss: 4.60261845e-06
Iter: 824 loss: 4.59859803e-06
Iter: 825 loss: 4.60293404e-06
Iter: 826 loss: 4.59650437e-06
Iter: 827 loss: 4.59277862e-06
Iter: 828 loss: 4.61668515e-06
Iter: 829 loss: 4.59224157e-06
Iter: 830 loss: 4.59145758e-06
Iter: 831 loss: 4.59075545e-06
Iter: 832 loss: 4.5889833e-06
Iter: 833 loss: 4.58506156e-06
Iter: 834 loss: 4.64100458e-06
Iter: 835 loss: 4.58470322e-06
Iter: 836 loss: 4.58242175e-06
Iter: 837 loss: 4.58673094e-06
Iter: 838 loss: 4.58134446e-06
Iter: 839 loss: 4.57836632e-06
Iter: 840 loss: 4.58153045e-06
Iter: 841 loss: 4.57671649e-06
Iter: 842 loss: 4.57344322e-06
Iter: 843 loss: 4.5986626e-06
Iter: 844 loss: 4.57324222e-06
Iter: 845 loss: 4.57030364e-06
Iter: 846 loss: 4.57967872e-06
Iter: 847 loss: 4.56963062e-06
Iter: 848 loss: 4.56754287e-06
Iter: 849 loss: 4.56592261e-06
Iter: 850 loss: 4.56527277e-06
Iter: 851 loss: 4.56216821e-06
Iter: 852 loss: 4.56742328e-06
Iter: 853 loss: 4.56083853e-06
Iter: 854 loss: 4.55737836e-06
Iter: 855 loss: 4.56908037e-06
Iter: 856 loss: 4.55640338e-06
Iter: 857 loss: 4.55291683e-06
Iter: 858 loss: 4.5612037e-06
Iter: 859 loss: 4.55163081e-06
Iter: 860 loss: 4.5480474e-06
Iter: 861 loss: 4.55173e-06
Iter: 862 loss: 4.54621068e-06
Iter: 863 loss: 4.54594419e-06
Iter: 864 loss: 4.54438259e-06
Iter: 865 loss: 4.54244127e-06
Iter: 866 loss: 4.5419356e-06
Iter: 867 loss: 4.54064866e-06
Iter: 868 loss: 4.53894108e-06
Iter: 869 loss: 4.53576195e-06
Iter: 870 loss: 4.61230866e-06
Iter: 871 loss: 4.53576649e-06
Iter: 872 loss: 4.53309713e-06
Iter: 873 loss: 4.56876842e-06
Iter: 874 loss: 4.53292841e-06
Iter: 875 loss: 4.53089e-06
Iter: 876 loss: 4.5307238e-06
Iter: 877 loss: 4.52919403e-06
Iter: 878 loss: 4.52549921e-06
Iter: 879 loss: 4.54139808e-06
Iter: 880 loss: 4.52459e-06
Iter: 881 loss: 4.5218203e-06
Iter: 882 loss: 4.52439326e-06
Iter: 883 loss: 4.52020595e-06
Iter: 884 loss: 4.51784035e-06
Iter: 885 loss: 4.51724645e-06
Iter: 886 loss: 4.51565347e-06
Iter: 887 loss: 4.51176038e-06
Iter: 888 loss: 4.52781114e-06
Iter: 889 loss: 4.51117194e-06
Iter: 890 loss: 4.50838797e-06
Iter: 891 loss: 4.51985397e-06
Iter: 892 loss: 4.50777497e-06
Iter: 893 loss: 4.50501375e-06
Iter: 894 loss: 4.50654443e-06
Iter: 895 loss: 4.50326479e-06
Iter: 896 loss: 4.5012107e-06
Iter: 897 loss: 4.50107291e-06
Iter: 898 loss: 4.49900836e-06
Iter: 899 loss: 4.51326378e-06
Iter: 900 loss: 4.4988e-06
Iter: 901 loss: 4.49781965e-06
Iter: 902 loss: 4.49522304e-06
Iter: 903 loss: 4.50975222e-06
Iter: 904 loss: 4.49441177e-06
Iter: 905 loss: 4.49143317e-06
Iter: 906 loss: 4.51254527e-06
Iter: 907 loss: 4.49111303e-06
Iter: 908 loss: 4.48835135e-06
Iter: 909 loss: 4.49435674e-06
Iter: 910 loss: 4.48735682e-06
Iter: 911 loss: 4.48510491e-06
Iter: 912 loss: 4.51619781e-06
Iter: 913 loss: 4.48513538e-06
Iter: 914 loss: 4.48352512e-06
Iter: 915 loss: 4.48076662e-06
Iter: 916 loss: 4.54933343e-06
Iter: 917 loss: 4.48067294e-06
Iter: 918 loss: 4.47722414e-06
Iter: 919 loss: 4.48966466e-06
Iter: 920 loss: 4.4763965e-06
Iter: 921 loss: 4.47366483e-06
Iter: 922 loss: 4.47758748e-06
Iter: 923 loss: 4.47223965e-06
Iter: 924 loss: 4.4685944e-06
Iter: 925 loss: 4.47934235e-06
Iter: 926 loss: 4.46739705e-06
Iter: 927 loss: 4.46472359e-06
Iter: 928 loss: 4.48334822e-06
Iter: 929 loss: 4.46446757e-06
Iter: 930 loss: 4.46195872e-06
Iter: 931 loss: 4.45832438e-06
Iter: 932 loss: 4.45832075e-06
Iter: 933 loss: 4.46457852e-06
Iter: 934 loss: 4.45676596e-06
Iter: 935 loss: 4.45592e-06
Iter: 936 loss: 4.45374826e-06
Iter: 937 loss: 4.46181775e-06
Iter: 938 loss: 4.45289288e-06
Iter: 939 loss: 4.45018304e-06
Iter: 940 loss: 4.45203659e-06
Iter: 941 loss: 4.4485887e-06
Iter: 942 loss: 4.44588e-06
Iter: 943 loss: 4.48209357e-06
Iter: 944 loss: 4.44595389e-06
Iter: 945 loss: 4.44411944e-06
Iter: 946 loss: 4.44745365e-06
Iter: 947 loss: 4.44363468e-06
Iter: 948 loss: 4.44099305e-06
Iter: 949 loss: 4.44343095e-06
Iter: 950 loss: 4.439441e-06
Iter: 951 loss: 4.43730187e-06
Iter: 952 loss: 4.43973704e-06
Iter: 953 loss: 4.43604313e-06
Iter: 954 loss: 4.43333829e-06
Iter: 955 loss: 4.43270255e-06
Iter: 956 loss: 4.43090039e-06
Iter: 957 loss: 4.42717192e-06
Iter: 958 loss: 4.45256956e-06
Iter: 959 loss: 4.42680175e-06
Iter: 960 loss: 4.42427336e-06
Iter: 961 loss: 4.43279532e-06
Iter: 962 loss: 4.4234107e-06
Iter: 963 loss: 4.42084456e-06
Iter: 964 loss: 4.42408964e-06
Iter: 965 loss: 4.41955035e-06
Iter: 966 loss: 4.41756947e-06
Iter: 967 loss: 4.41761313e-06
Iter: 968 loss: 4.41520933e-06
Iter: 969 loss: 4.42066539e-06
Iter: 970 loss: 4.4144258e-06
Iter: 971 loss: 4.41349493e-06
Iter: 972 loss: 4.41129396e-06
Iter: 973 loss: 4.43110093e-06
Iter: 974 loss: 4.41102793e-06
Iter: 975 loss: 4.40784697e-06
Iter: 976 loss: 4.41551356e-06
Iter: 977 loss: 4.40667418e-06
Iter: 978 loss: 4.40445456e-06
Iter: 979 loss: 4.40431177e-06
Iter: 980 loss: 4.40308395e-06
Iter: 981 loss: 4.40682743e-06
Iter: 982 loss: 4.40262102e-06
Iter: 983 loss: 4.40089752e-06
Iter: 984 loss: 4.39885116e-06
Iter: 985 loss: 4.39861651e-06
Iter: 986 loss: 4.39657742e-06
Iter: 987 loss: 4.40511e-06
Iter: 988 loss: 4.39599171e-06
Iter: 989 loss: 4.39378346e-06
Iter: 990 loss: 4.39129371e-06
Iter: 991 loss: 4.3908276e-06
Iter: 992 loss: 4.38780808e-06
Iter: 993 loss: 4.42958e-06
Iter: 994 loss: 4.38781e-06
Iter: 995 loss: 4.38557163e-06
Iter: 996 loss: 4.38713141e-06
Iter: 997 loss: 4.38413235e-06
Iter: 998 loss: 4.38176266e-06
Iter: 999 loss: 4.401556e-06
Iter: 1000 loss: 4.38155394e-06
Iter: 1001 loss: 4.38166489e-06
Iter: 1002 loss: 4.38074494e-06
Iter: 1003 loss: 4.38019561e-06
Iter: 1004 loss: 4.3786149e-06
Iter: 1005 loss: 4.37940525e-06
Iter: 1006 loss: 4.37727e-06
Iter: 1007 loss: 4.37455674e-06
Iter: 1008 loss: 4.3840123e-06
Iter: 1009 loss: 4.37388644e-06
Iter: 1010 loss: 4.37170411e-06
Iter: 1011 loss: 4.39047744e-06
Iter: 1012 loss: 4.37158951e-06
Iter: 1013 loss: 4.36958e-06
Iter: 1014 loss: 4.37823837e-06
Iter: 1015 loss: 4.36924438e-06
Iter: 1016 loss: 4.36765913e-06
Iter: 1017 loss: 4.372494e-06
Iter: 1018 loss: 4.36727623e-06
Iter: 1019 loss: 4.36588334e-06
Iter: 1020 loss: 4.36433766e-06
Iter: 1021 loss: 4.36414211e-06
Iter: 1022 loss: 4.36173377e-06
Iter: 1023 loss: 4.36988e-06
Iter: 1024 loss: 4.36112714e-06
Iter: 1025 loss: 4.3588152e-06
Iter: 1026 loss: 4.35800212e-06
Iter: 1027 loss: 4.35673337e-06
Iter: 1028 loss: 4.35427364e-06
Iter: 1029 loss: 4.35429502e-06
Iter: 1030 loss: 4.35239508e-06
Iter: 1031 loss: 4.35207403e-06
Iter: 1032 loss: 4.35067159e-06
Iter: 1033 loss: 4.35241691e-06
Iter: 1034 loss: 4.34984804e-06
Iter: 1035 loss: 4.34897083e-06
Iter: 1036 loss: 4.34747926e-06
Iter: 1037 loss: 4.34751291e-06
Iter: 1038 loss: 4.34640924e-06
Iter: 1039 loss: 4.34365711e-06
Iter: 1040 loss: 4.36415712e-06
Iter: 1041 loss: 4.34303456e-06
Iter: 1042 loss: 4.34081858e-06
Iter: 1043 loss: 4.34075628e-06
Iter: 1044 loss: 4.33923105e-06
Iter: 1045 loss: 4.33928381e-06
Iter: 1046 loss: 4.33834202e-06
Iter: 1047 loss: 4.33637251e-06
Iter: 1048 loss: 4.33637069e-06
Iter: 1049 loss: 4.33421792e-06
Iter: 1050 loss: 4.34595586e-06
Iter: 1051 loss: 4.33408741e-06
Iter: 1052 loss: 4.33243e-06
Iter: 1053 loss: 4.33493e-06
Iter: 1054 loss: 4.33173591e-06
Iter: 1055 loss: 4.32970592e-06
Iter: 1056 loss: 4.32830075e-06
Iter: 1057 loss: 4.32756315e-06
Iter: 1058 loss: 4.32553497e-06
Iter: 1059 loss: 4.3451405e-06
Iter: 1060 loss: 4.32533761e-06
Iter: 1061 loss: 4.32312117e-06
Iter: 1062 loss: 4.32305387e-06
Iter: 1063 loss: 4.32143224e-06
Iter: 1064 loss: 4.32259e-06
Iter: 1065 loss: 4.32061461e-06
Iter: 1066 loss: 4.31968419e-06
Iter: 1067 loss: 4.31899e-06
Iter: 1068 loss: 4.31866556e-06
Iter: 1069 loss: 4.31742956e-06
Iter: 1070 loss: 4.31425451e-06
Iter: 1071 loss: 4.33681453e-06
Iter: 1072 loss: 4.31332319e-06
Iter: 1073 loss: 4.31086255e-06
Iter: 1074 loss: 4.34713093e-06
Iter: 1075 loss: 4.31088529e-06
Iter: 1076 loss: 4.30889e-06
Iter: 1077 loss: 4.31375247e-06
Iter: 1078 loss: 4.30832733e-06
Iter: 1079 loss: 4.30539694e-06
Iter: 1080 loss: 4.31166518e-06
Iter: 1081 loss: 4.30425644e-06
Iter: 1082 loss: 4.3026771e-06
Iter: 1083 loss: 4.30188084e-06
Iter: 1084 loss: 4.30104592e-06
Iter: 1085 loss: 4.29870124e-06
Iter: 1086 loss: 4.30801128e-06
Iter: 1087 loss: 4.29810189e-06
Iter: 1088 loss: 4.29570719e-06
Iter: 1089 loss: 4.30502314e-06
Iter: 1090 loss: 4.29524334e-06
Iter: 1091 loss: 4.29323472e-06
Iter: 1092 loss: 4.29377087e-06
Iter: 1093 loss: 4.29189231e-06
Iter: 1094 loss: 4.28933163e-06
Iter: 1095 loss: 4.29502734e-06
Iter: 1096 loss: 4.28830435e-06
Iter: 1097 loss: 4.28537533e-06
Iter: 1098 loss: 4.2932852e-06
Iter: 1099 loss: 4.28431485e-06
Iter: 1100 loss: 4.28204476e-06
Iter: 1101 loss: 4.29705779e-06
Iter: 1102 loss: 4.28171506e-06
Iter: 1103 loss: 4.27953682e-06
Iter: 1104 loss: 4.27874602e-06
Iter: 1105 loss: 4.27751274e-06
Iter: 1106 loss: 4.27954637e-06
Iter: 1107 loss: 4.27660962e-06
Iter: 1108 loss: 4.27559962e-06
Iter: 1109 loss: 4.27478108e-06
Iter: 1110 loss: 4.27460145e-06
Iter: 1111 loss: 4.2734514e-06
Iter: 1112 loss: 4.27071927e-06
Iter: 1113 loss: 4.29222337e-06
Iter: 1114 loss: 4.27029227e-06
Iter: 1115 loss: 4.26801671e-06
Iter: 1116 loss: 4.2679867e-06
Iter: 1117 loss: 4.26668066e-06
Iter: 1118 loss: 4.27928262e-06
Iter: 1119 loss: 4.26664064e-06
Iter: 1120 loss: 4.26510769e-06
Iter: 1121 loss: 4.26271345e-06
Iter: 1122 loss: 4.26269162e-06
Iter: 1123 loss: 4.2606066e-06
Iter: 1124 loss: 4.26516635e-06
Iter: 1125 loss: 4.25965254e-06
Iter: 1126 loss: 4.25727421e-06
Iter: 1127 loss: 4.25529561e-06
Iter: 1128 loss: 4.25458529e-06
Iter: 1129 loss: 4.25283088e-06
Iter: 1130 loss: 4.2525312e-06
Iter: 1131 loss: 4.25126e-06
Iter: 1132 loss: 4.24914651e-06
Iter: 1133 loss: 4.24914106e-06
Iter: 1134 loss: 4.246157e-06
Iter: 1135 loss: 4.2643228e-06
Iter: 1136 loss: 4.24582458e-06
Iter: 1137 loss: 4.24476457e-06
Iter: 1138 loss: 4.24473956e-06
Iter: 1139 loss: 4.24336622e-06
Iter: 1140 loss: 4.24511336e-06
Iter: 1141 loss: 4.24277459e-06
Iter: 1142 loss: 4.24178597e-06
Iter: 1143 loss: 4.23963957e-06
Iter: 1144 loss: 4.27142413e-06
Iter: 1145 loss: 4.23953816e-06
Iter: 1146 loss: 4.23744405e-06
Iter: 1147 loss: 4.244107e-06
Iter: 1148 loss: 4.23679739e-06
Iter: 1149 loss: 4.23500478e-06
Iter: 1150 loss: 4.23849e-06
Iter: 1151 loss: 4.23426445e-06
Iter: 1152 loss: 4.23277379e-06
Iter: 1153 loss: 4.25237067e-06
Iter: 1154 loss: 4.23279471e-06
Iter: 1155 loss: 4.23141955e-06
Iter: 1156 loss: 4.22855828e-06
Iter: 1157 loss: 4.27544546e-06
Iter: 1158 loss: 4.22841e-06
Iter: 1159 loss: 4.22640233e-06
Iter: 1160 loss: 4.2424972e-06
Iter: 1161 loss: 4.22615494e-06
Iter: 1162 loss: 4.22414814e-06
Iter: 1163 loss: 4.22250105e-06
Iter: 1164 loss: 4.22188623e-06
Iter: 1165 loss: 4.21960522e-06
Iter: 1166 loss: 4.24833706e-06
Iter: 1167 loss: 4.2196607e-06
Iter: 1168 loss: 4.21783443e-06
Iter: 1169 loss: 4.21554887e-06
Iter: 1170 loss: 4.21541245e-06
Iter: 1171 loss: 4.2129227e-06
Iter: 1172 loss: 4.24611198e-06
Iter: 1173 loss: 4.21297364e-06
Iter: 1174 loss: 4.21262439e-06
Iter: 1175 loss: 4.21199775e-06
Iter: 1176 loss: 4.2109873e-06
Iter: 1177 loss: 4.20834158e-06
Iter: 1178 loss: 4.23429765e-06
Iter: 1179 loss: 4.20809e-06
Iter: 1180 loss: 4.20605647e-06
Iter: 1181 loss: 4.2122565e-06
Iter: 1182 loss: 4.2054412e-06
Iter: 1183 loss: 4.20396464e-06
Iter: 1184 loss: 4.20324841e-06
Iter: 1185 loss: 4.20251945e-06
Iter: 1186 loss: 4.20004562e-06
Iter: 1187 loss: 4.21079585e-06
Iter: 1188 loss: 4.19938078e-06
Iter: 1189 loss: 4.19714615e-06
Iter: 1190 loss: 4.21544792e-06
Iter: 1191 loss: 4.1970934e-06
Iter: 1192 loss: 4.19535354e-06
Iter: 1193 loss: 4.19350363e-06
Iter: 1194 loss: 4.19316757e-06
Iter: 1195 loss: 4.19065509e-06
Iter: 1196 loss: 4.1995786e-06
Iter: 1197 loss: 4.18983927e-06
Iter: 1198 loss: 4.18751188e-06
Iter: 1199 loss: 4.18749096e-06
Iter: 1200 loss: 4.18570198e-06
Iter: 1201 loss: 4.18279433e-06
Iter: 1202 loss: 4.20226024e-06
Iter: 1203 loss: 4.18258605e-06
Iter: 1204 loss: 4.17987667e-06
Iter: 1205 loss: 4.18725222e-06
Iter: 1206 loss: 4.17897172e-06
Iter: 1207 loss: 4.17711e-06
Iter: 1208 loss: 4.18802574e-06
Iter: 1209 loss: 4.17678439e-06
Iter: 1210 loss: 4.17568299e-06
Iter: 1211 loss: 4.17567935e-06
Iter: 1212 loss: 4.17433148e-06
Iter: 1213 loss: 4.17380079e-06
Iter: 1214 loss: 4.17299771e-06
Iter: 1215 loss: 4.17209321e-06
Iter: 1216 loss: 4.17154752e-06
Iter: 1217 loss: 4.1711246e-06
Iter: 1218 loss: 4.16929106e-06
Iter: 1219 loss: 4.16859939e-06
Iter: 1220 loss: 4.16758667e-06
Iter: 1221 loss: 4.16504554e-06
Iter: 1222 loss: 4.19431854e-06
Iter: 1223 loss: 4.16495e-06
Iter: 1224 loss: 4.16324383e-06
Iter: 1225 loss: 4.16746343e-06
Iter: 1226 loss: 4.1626954e-06
Iter: 1227 loss: 4.16098692e-06
Iter: 1228 loss: 4.1580397e-06
Iter: 1229 loss: 4.22771e-06
Iter: 1230 loss: 4.15810837e-06
Iter: 1231 loss: 4.15514569e-06
Iter: 1232 loss: 4.18329e-06
Iter: 1233 loss: 4.15500563e-06
Iter: 1234 loss: 4.15302566e-06
Iter: 1235 loss: 4.15218801e-06
Iter: 1236 loss: 4.15094109e-06
Iter: 1237 loss: 4.14781152e-06
Iter: 1238 loss: 4.16541116e-06
Iter: 1239 loss: 4.14731403e-06
Iter: 1240 loss: 4.14493343e-06
Iter: 1241 loss: 4.15035447e-06
Iter: 1242 loss: 4.14397846e-06
Iter: 1243 loss: 4.1415924e-06
Iter: 1244 loss: 4.16311741e-06
Iter: 1245 loss: 4.14135684e-06
Iter: 1246 loss: 4.1408648e-06
Iter: 1247 loss: 4.14034366e-06
Iter: 1248 loss: 4.13975522e-06
Iter: 1249 loss: 4.13806e-06
Iter: 1250 loss: 4.14504757e-06
Iter: 1251 loss: 4.13725775e-06
Iter: 1252 loss: 4.13529688e-06
Iter: 1253 loss: 4.14638907e-06
Iter: 1254 loss: 4.13495673e-06
Iter: 1255 loss: 4.13323369e-06
Iter: 1256 loss: 4.13725411e-06
Iter: 1257 loss: 4.13271755e-06
Iter: 1258 loss: 4.13080124e-06
Iter: 1259 loss: 4.14050464e-06
Iter: 1260 loss: 4.13055659e-06
Iter: 1261 loss: 4.1291396e-06
Iter: 1262 loss: 4.13096495e-06
Iter: 1263 loss: 4.12844656e-06
Iter: 1264 loss: 4.12684494e-06
Iter: 1265 loss: 4.12380268e-06
Iter: 1266 loss: 4.19029311e-06
Iter: 1267 loss: 4.12377949e-06
Iter: 1268 loss: 4.12167537e-06
Iter: 1269 loss: 4.1217736e-06
Iter: 1270 loss: 4.12014151e-06
Iter: 1271 loss: 4.11840574e-06
Iter: 1272 loss: 4.11810197e-06
Iter: 1273 loss: 4.11562087e-06
Iter: 1274 loss: 4.13116504e-06
Iter: 1275 loss: 4.11530891e-06
Iter: 1276 loss: 4.11306428e-06
Iter: 1277 loss: 4.11679e-06
Iter: 1278 loss: 4.11200745e-06
Iter: 1279 loss: 4.1121607e-06
Iter: 1280 loss: 4.11107158e-06
Iter: 1281 loss: 4.10998473e-06
Iter: 1282 loss: 4.11054043e-06
Iter: 1283 loss: 4.10913435e-06
Iter: 1284 loss: 4.10842858e-06
Iter: 1285 loss: 4.10636721e-06
Iter: 1286 loss: 4.13056114e-06
Iter: 1287 loss: 4.1062226e-06
Iter: 1288 loss: 4.10413668e-06
Iter: 1289 loss: 4.12024337e-06
Iter: 1290 loss: 4.10391112e-06
Iter: 1291 loss: 4.10250232e-06
Iter: 1292 loss: 4.11942437e-06
Iter: 1293 loss: 4.10251914e-06
Iter: 1294 loss: 4.10130724e-06
Iter: 1295 loss: 4.10072698e-06
Iter: 1296 loss: 4.10022267e-06
Iter: 1297 loss: 4.09838685e-06
Iter: 1298 loss: 4.09915629e-06
Iter: 1299 loss: 4.09711038e-06
Iter: 1300 loss: 4.09488075e-06
Iter: 1301 loss: 4.09985932e-06
Iter: 1302 loss: 4.09417135e-06
Iter: 1303 loss: 4.0920936e-06
Iter: 1304 loss: 4.09755194e-06
Iter: 1305 loss: 4.0913169e-06
Iter: 1306 loss: 4.08925962e-06
Iter: 1307 loss: 4.09222639e-06
Iter: 1308 loss: 4.0882278e-06
Iter: 1309 loss: 4.08591313e-06
Iter: 1310 loss: 4.09902941e-06
Iter: 1311 loss: 4.0856271e-06
Iter: 1312 loss: 4.08348205e-06
Iter: 1313 loss: 4.08283904e-06
Iter: 1314 loss: 4.08159212e-06
Iter: 1315 loss: 4.08491587e-06
Iter: 1316 loss: 4.08061169e-06
Iter: 1317 loss: 4.08007872e-06
Iter: 1318 loss: 4.07874086e-06
Iter: 1319 loss: 4.0895311e-06
Iter: 1320 loss: 4.07840253e-06
Iter: 1321 loss: 4.07667358e-06
Iter: 1322 loss: 4.07592961e-06
Iter: 1323 loss: 4.07505104e-06
Iter: 1324 loss: 4.07291509e-06
Iter: 1325 loss: 4.09334234e-06
Iter: 1326 loss: 4.07286734e-06
Iter: 1327 loss: 4.0713926e-06
Iter: 1328 loss: 4.08087863e-06
Iter: 1329 loss: 4.07125e-06
Iter: 1330 loss: 4.0699033e-06
Iter: 1331 loss: 4.06790787e-06
Iter: 1332 loss: 4.0678442e-06
Iter: 1333 loss: 4.06524305e-06
Iter: 1334 loss: 4.07314838e-06
Iter: 1335 loss: 4.06446361e-06
Iter: 1336 loss: 4.06228e-06
Iter: 1337 loss: 4.06613071e-06
Iter: 1338 loss: 4.06122308e-06
Iter: 1339 loss: 4.05869105e-06
Iter: 1340 loss: 4.06276649e-06
Iter: 1341 loss: 4.05749597e-06
Iter: 1342 loss: 4.05493029e-06
Iter: 1343 loss: 4.06177742e-06
Iter: 1344 loss: 4.05422679e-06
Iter: 1345 loss: 4.05125866e-06
Iter: 1346 loss: 4.05925039e-06
Iter: 1347 loss: 4.05038099e-06
Iter: 1348 loss: 4.04777347e-06
Iter: 1349 loss: 4.05836e-06
Iter: 1350 loss: 4.04726643e-06
Iter: 1351 loss: 4.04656e-06
Iter: 1352 loss: 4.04569073e-06
Iter: 1353 loss: 4.04515504e-06
Iter: 1354 loss: 4.04349066e-06
Iter: 1355 loss: 4.04845287e-06
Iter: 1356 loss: 4.04261482e-06
Iter: 1357 loss: 4.04025195e-06
Iter: 1358 loss: 4.0470477e-06
Iter: 1359 loss: 4.03948252e-06
Iter: 1360 loss: 4.03788e-06
Iter: 1361 loss: 4.06140225e-06
Iter: 1362 loss: 4.03791182e-06
Iter: 1363 loss: 4.03671856e-06
Iter: 1364 loss: 4.03986678e-06
Iter: 1365 loss: 4.0363193e-06
Iter: 1366 loss: 4.0349064e-06
Iter: 1367 loss: 4.03310969e-06
Iter: 1368 loss: 4.03292643e-06
Iter: 1369 loss: 4.03081913e-06
Iter: 1370 loss: 4.03876584e-06
Iter: 1371 loss: 4.03040258e-06
Iter: 1372 loss: 4.02821752e-06
Iter: 1373 loss: 4.02823116e-06
Iter: 1374 loss: 4.02655769e-06
Iter: 1375 loss: 4.02371643e-06
Iter: 1376 loss: 4.04282673e-06
Iter: 1377 loss: 4.02335627e-06
Iter: 1378 loss: 4.02143814e-06
Iter: 1379 loss: 4.02123214e-06
Iter: 1380 loss: 4.01989064e-06
Iter: 1381 loss: 4.01711441e-06
Iter: 1382 loss: 4.03053764e-06
Iter: 1383 loss: 4.01644593e-06
Iter: 1384 loss: 4.01708849e-06
Iter: 1385 loss: 4.01555553e-06
Iter: 1386 loss: 4.01460966e-06
Iter: 1387 loss: 4.01268653e-06
Iter: 1388 loss: 4.04748744e-06
Iter: 1389 loss: 4.01256966e-06
Iter: 1390 loss: 4.01081434e-06
Iter: 1391 loss: 4.0107243e-06
Iter: 1392 loss: 4.00950194e-06
Iter: 1393 loss: 4.00736e-06
Iter: 1394 loss: 4.0143932e-06
Iter: 1395 loss: 4.00685622e-06
Iter: 1396 loss: 4.00506451e-06
Iter: 1397 loss: 4.02719024e-06
Iter: 1398 loss: 4.00507088e-06
Iter: 1399 loss: 4.00384397e-06
Iter: 1400 loss: 4.00628051e-06
Iter: 1401 loss: 4.00329827e-06
Iter: 1402 loss: 4.00193e-06
Iter: 1403 loss: 4.00010595e-06
Iter: 1404 loss: 3.99985765e-06
Iter: 1405 loss: 3.99782948e-06
Iter: 1406 loss: 4.00871841e-06
Iter: 1407 loss: 3.99745795e-06
Iter: 1408 loss: 3.99545115e-06
Iter: 1409 loss: 3.99562123e-06
Iter: 1410 loss: 3.99371675e-06
Iter: 1411 loss: 3.99162309e-06
Iter: 1412 loss: 4.01205762e-06
Iter: 1413 loss: 3.99160672e-06
Iter: 1414 loss: 3.98986504e-06
Iter: 1415 loss: 3.98826251e-06
Iter: 1416 loss: 3.98780321e-06
Iter: 1417 loss: 3.98551538e-06
Iter: 1418 loss: 4.01141551e-06
Iter: 1419 loss: 3.98548855e-06
Iter: 1420 loss: 3.98611974e-06
Iter: 1421 loss: 3.9847173e-06
Iter: 1422 loss: 3.98430711e-06
Iter: 1423 loss: 3.9827e-06
Iter: 1424 loss: 3.98148677e-06
Iter: 1425 loss: 3.98052543e-06
Iter: 1426 loss: 3.97804843e-06
Iter: 1427 loss: 3.99852706e-06
Iter: 1428 loss: 3.97792383e-06
Iter: 1429 loss: 3.9761162e-06
Iter: 1430 loss: 3.98100337e-06
Iter: 1431 loss: 3.97552412e-06
Iter: 1432 loss: 3.97378653e-06
Iter: 1433 loss: 3.99393593e-06
Iter: 1434 loss: 3.97379199e-06
Iter: 1435 loss: 3.97246e-06
Iter: 1436 loss: 3.9748943e-06
Iter: 1437 loss: 3.9718243e-06
Iter: 1438 loss: 3.97040094e-06
Iter: 1439 loss: 3.96905443e-06
Iter: 1440 loss: 3.96871565e-06
Iter: 1441 loss: 3.96636551e-06
Iter: 1442 loss: 3.97151462e-06
Iter: 1443 loss: 3.96544146e-06
Iter: 1444 loss: 3.96319956e-06
Iter: 1445 loss: 3.96963333e-06
Iter: 1446 loss: 3.96245468e-06
Iter: 1447 loss: 3.96063751e-06
Iter: 1448 loss: 3.9678439e-06
Iter: 1449 loss: 3.96018459e-06
Iter: 1450 loss: 3.95806728e-06
Iter: 1451 loss: 3.95797815e-06
Iter: 1452 loss: 3.95636562e-06
Iter: 1453 loss: 3.95437928e-06
Iter: 1454 loss: 3.97530403e-06
Iter: 1455 loss: 3.95426514e-06
Iter: 1456 loss: 3.95375173e-06
Iter: 1457 loss: 3.95340339e-06
Iter: 1458 loss: 3.95239522e-06
Iter: 1459 loss: 3.94973813e-06
Iter: 1460 loss: 3.96128235e-06
Iter: 1461 loss: 3.94872677e-06
Iter: 1462 loss: 3.94682229e-06
Iter: 1463 loss: 3.9483939e-06
Iter: 1464 loss: 3.94576819e-06
Iter: 1465 loss: 3.94318522e-06
Iter: 1466 loss: 3.95207417e-06
Iter: 1467 loss: 3.94257131e-06
Iter: 1468 loss: 3.94108702e-06
Iter: 1469 loss: 3.95590678e-06
Iter: 1470 loss: 3.94096151e-06
Iter: 1471 loss: 3.93930895e-06
Iter: 1472 loss: 3.94380777e-06
Iter: 1473 loss: 3.93870323e-06
Iter: 1474 loss: 3.9373208e-06
Iter: 1475 loss: 3.93641676e-06
Iter: 1476 loss: 3.93587288e-06
Iter: 1477 loss: 3.93395385e-06
Iter: 1478 loss: 3.93802293e-06
Iter: 1479 loss: 3.93316623e-06
Iter: 1480 loss: 3.93065693e-06
Iter: 1481 loss: 3.93933669e-06
Iter: 1482 loss: 3.93000209e-06
Iter: 1483 loss: 3.92820493e-06
Iter: 1484 loss: 3.93269966e-06
Iter: 1485 loss: 3.92752781e-06
Iter: 1486 loss: 3.92566199e-06
Iter: 1487 loss: 3.92550191e-06
Iter: 1488 loss: 3.92411e-06
Iter: 1489 loss: 3.92149e-06
Iter: 1490 loss: 3.93651544e-06
Iter: 1491 loss: 3.92121228e-06
Iter: 1492 loss: 3.92207858e-06
Iter: 1493 loss: 3.92031916e-06
Iter: 1494 loss: 3.91974118e-06
Iter: 1495 loss: 3.91814592e-06
Iter: 1496 loss: 3.93012851e-06
Iter: 1497 loss: 3.91789354e-06
Iter: 1498 loss: 3.91653703e-06
Iter: 1499 loss: 3.91459434e-06
Iter: 1500 loss: 3.9144461e-06
Iter: 1501 loss: 3.91248068e-06
Iter: 1502 loss: 3.94428116e-06
Iter: 1503 loss: 3.91254753e-06
Iter: 1504 loss: 3.91131061e-06
Iter: 1505 loss: 3.9108063e-06
Iter: 1506 loss: 3.91024878e-06
Iter: 1507 loss: 3.90859168e-06
Iter: 1508 loss: 3.93315895e-06
Iter: 1509 loss: 3.90863897e-06
Iter: 1510 loss: 3.90745572e-06
Iter: 1511 loss: 3.91412459e-06
Iter: 1512 loss: 3.90738433e-06
Iter: 1513 loss: 3.90648e-06
Iter: 1514 loss: 3.90465357e-06
Iter: 1515 loss: 3.92829543e-06
Iter: 1516 loss: 3.90446257e-06
Iter: 1517 loss: 3.90234845e-06
Iter: 1518 loss: 3.91394951e-06
Iter: 1519 loss: 3.90206787e-06
Iter: 1520 loss: 3.90031619e-06
Iter: 1521 loss: 3.90314926e-06
Iter: 1522 loss: 3.89944762e-06
Iter: 1523 loss: 3.89749903e-06
Iter: 1524 loss: 3.91295271e-06
Iter: 1525 loss: 3.89735897e-06
Iter: 1526 loss: 3.89620527e-06
Iter: 1527 loss: 3.90167497e-06
Iter: 1528 loss: 3.89595562e-06
Iter: 1529 loss: 3.89526031e-06
Iter: 1530 loss: 3.895193e-06
Iter: 1531 loss: 3.89470188e-06
Iter: 1532 loss: 3.89330626e-06
Iter: 1533 loss: 3.89509569e-06
Iter: 1534 loss: 3.89216621e-06
Iter: 1535 loss: 3.8904027e-06
Iter: 1536 loss: 3.89677416e-06
Iter: 1537 loss: 3.88987428e-06
Iter: 1538 loss: 3.88818535e-06
Iter: 1539 loss: 3.89581692e-06
Iter: 1540 loss: 3.88779881e-06
Iter: 1541 loss: 3.8864373e-06
Iter: 1542 loss: 3.88847093e-06
Iter: 1543 loss: 3.88572e-06
Iter: 1544 loss: 3.88447188e-06
Iter: 1545 loss: 3.88452236e-06
Iter: 1546 loss: 3.88330136e-06
Iter: 1547 loss: 3.88517219e-06
Iter: 1548 loss: 3.88276521e-06
Iter: 1549 loss: 3.88177614e-06
Iter: 1550 loss: 3.88004628e-06
Iter: 1551 loss: 3.92409083e-06
Iter: 1552 loss: 3.88007174e-06
Iter: 1553 loss: 3.87829732e-06
Iter: 1554 loss: 3.88633907e-06
Iter: 1555 loss: 3.87799855e-06
Iter: 1556 loss: 3.87611271e-06
Iter: 1557 loss: 3.88101125e-06
Iter: 1558 loss: 3.8753933e-06
Iter: 1559 loss: 3.8739845e-06
Iter: 1560 loss: 3.8924245e-06
Iter: 1561 loss: 3.87397768e-06
Iter: 1562 loss: 3.8735343e-06
Iter: 1563 loss: 3.87362797e-06
Iter: 1564 loss: 3.87292766e-06
Iter: 1565 loss: 3.87157888e-06
Iter: 1566 loss: 3.89075558e-06
Iter: 1567 loss: 3.87156433e-06
Iter: 1568 loss: 3.87030605e-06
Iter: 1569 loss: 3.86934062e-06
Iter: 1570 loss: 3.86885631e-06
Iter: 1571 loss: 3.86756e-06
Iter: 1572 loss: 3.87276759e-06
Iter: 1573 loss: 3.86731062e-06
Iter: 1574 loss: 3.86581542e-06
Iter: 1575 loss: 3.86954707e-06
Iter: 1576 loss: 3.86524334e-06
Iter: 1577 loss: 3.86382e-06
Iter: 1578 loss: 3.86763259e-06
Iter: 1579 loss: 3.8633525e-06
Iter: 1580 loss: 3.86203646e-06
Iter: 1581 loss: 3.8620783e-06
Iter: 1582 loss: 3.86116608e-06
Iter: 1583 loss: 3.86100601e-06
Iter: 1584 loss: 3.8603248e-06
Iter: 1585 loss: 3.85941576e-06
Iter: 1586 loss: 3.85834755e-06
Iter: 1587 loss: 3.85821659e-06
Iter: 1588 loss: 3.85663589e-06
Iter: 1589 loss: 3.86318425e-06
Iter: 1590 loss: 3.85629937e-06
Iter: 1591 loss: 3.85481871e-06
Iter: 1592 loss: 3.86310512e-06
Iter: 1593 loss: 3.85453632e-06
Iter: 1594 loss: 3.85381463e-06
Iter: 1595 loss: 3.8538351e-06
Iter: 1596 loss: 3.85287058e-06
Iter: 1597 loss: 3.85654403e-06
Iter: 1598 loss: 3.85277599e-06
Iter: 1599 loss: 3.85218527e-06
Iter: 1600 loss: 3.85067324e-06
Iter: 1601 loss: 3.85986732e-06
Iter: 1602 loss: 3.85019484e-06
Iter: 1603 loss: 3.8484477e-06
Iter: 1604 loss: 3.85796102e-06
Iter: 1605 loss: 3.84826353e-06
Iter: 1606 loss: 3.84712348e-06
Iter: 1607 loss: 3.84758823e-06
Iter: 1608 loss: 3.84639407e-06
Iter: 1609 loss: 3.84464465e-06
Iter: 1610 loss: 3.8501521e-06
Iter: 1611 loss: 3.84418581e-06
Iter: 1612 loss: 3.84300893e-06
Iter: 1613 loss: 3.85229168e-06
Iter: 1614 loss: 3.84289706e-06
Iter: 1615 loss: 3.84162831e-06
Iter: 1616 loss: 3.84647728e-06
Iter: 1617 loss: 3.84125224e-06
Iter: 1618 loss: 3.84025134e-06
Iter: 1619 loss: 3.84048872e-06
Iter: 1620 loss: 3.83949236e-06
Iter: 1621 loss: 3.83845736e-06
Iter: 1622 loss: 3.83881888e-06
Iter: 1623 loss: 3.83773295e-06
Iter: 1624 loss: 3.836215e-06
Iter: 1625 loss: 3.84392479e-06
Iter: 1626 loss: 3.83594534e-06
Iter: 1627 loss: 3.83496808e-06
Iter: 1628 loss: 3.83627821e-06
Iter: 1629 loss: 3.83443421e-06
Iter: 1630 loss: 3.83395582e-06
Iter: 1631 loss: 3.83363204e-06
Iter: 1632 loss: 3.83313045e-06
Iter: 1633 loss: 3.83196e-06
Iter: 1634 loss: 3.84641226e-06
Iter: 1635 loss: 3.83191855e-06
Iter: 1636 loss: 3.83081897e-06
Iter: 1637 loss: 3.8325461e-06
Iter: 1638 loss: 3.83044835e-06
Iter: 1639 loss: 3.8291796e-06
Iter: 1640 loss: 3.82904955e-06
Iter: 1641 loss: 3.82818325e-06
Iter: 1642 loss: 3.82656526e-06
Iter: 1643 loss: 3.83229735e-06
Iter: 1644 loss: 3.82624785e-06
Iter: 1645 loss: 3.82494818e-06
Iter: 1646 loss: 3.82999724e-06
Iter: 1647 loss: 3.82462258e-06
Iter: 1648 loss: 3.82382041e-06
Iter: 1649 loss: 3.82377766e-06
Iter: 1650 loss: 3.82302142e-06
Iter: 1651 loss: 3.82214876e-06
Iter: 1652 loss: 3.8220328e-06
Iter: 1653 loss: 3.82092958e-06
Iter: 1654 loss: 3.82385588e-06
Iter: 1655 loss: 3.82065355e-06
Iter: 1656 loss: 3.81964719e-06
Iter: 1657 loss: 3.81945074e-06
Iter: 1658 loss: 3.8189587e-06
Iter: 1659 loss: 3.81748396e-06
Iter: 1660 loss: 3.8266312e-06
Iter: 1661 loss: 3.81718974e-06
Iter: 1662 loss: 3.81805739e-06
Iter: 1663 loss: 3.817e-06
Iter: 1664 loss: 3.8167218e-06
Iter: 1665 loss: 3.8158023e-06
Iter: 1666 loss: 3.81864e-06
Iter: 1667 loss: 3.8154817e-06
Iter: 1668 loss: 3.81424479e-06
Iter: 1669 loss: 3.8173348e-06
Iter: 1670 loss: 3.81376958e-06
Iter: 1671 loss: 3.81275413e-06
Iter: 1672 loss: 3.81611608e-06
Iter: 1673 loss: 3.81244104e-06
Iter: 1674 loss: 3.81133896e-06
Iter: 1675 loss: 3.81022664e-06
Iter: 1676 loss: 3.80993697e-06
Iter: 1677 loss: 3.80839128e-06
Iter: 1678 loss: 3.82123471e-06
Iter: 1679 loss: 3.80828988e-06
Iter: 1680 loss: 3.80711435e-06
Iter: 1681 loss: 3.81190284e-06
Iter: 1682 loss: 3.80688266e-06
Iter: 1683 loss: 3.80536858e-06
Iter: 1684 loss: 3.81482e-06
Iter: 1685 loss: 3.80516212e-06
Iter: 1686 loss: 3.80429356e-06
Iter: 1687 loss: 3.80357028e-06
Iter: 1688 loss: 3.80337906e-06
Iter: 1689 loss: 3.80191409e-06
Iter: 1690 loss: 3.80157917e-06
Iter: 1691 loss: 3.80064239e-06
Iter: 1692 loss: 3.79906146e-06
Iter: 1693 loss: 3.81983409e-06
Iter: 1694 loss: 3.79906351e-06
Iter: 1695 loss: 3.79834864e-06
Iter: 1696 loss: 3.80775987e-06
Iter: 1697 loss: 3.79831909e-06
Iter: 1698 loss: 3.79723224e-06
Iter: 1699 loss: 3.79770199e-06
Iter: 1700 loss: 3.79663243e-06
Iter: 1701 loss: 3.79592348e-06
Iter: 1702 loss: 3.79485118e-06
Iter: 1703 loss: 3.79483686e-06
Iter: 1704 loss: 3.7934019e-06
Iter: 1705 loss: 3.79766334e-06
Iter: 1706 loss: 3.79293851e-06
Iter: 1707 loss: 3.79180119e-06
Iter: 1708 loss: 3.79314497e-06
Iter: 1709 loss: 3.79114408e-06
Iter: 1710 loss: 3.78952404e-06
Iter: 1711 loss: 3.78921277e-06
Iter: 1712 loss: 3.78813775e-06
Iter: 1713 loss: 3.78654568e-06
Iter: 1714 loss: 3.8065948e-06
Iter: 1715 loss: 3.78657751e-06
Iter: 1716 loss: 3.7854104e-06
Iter: 1717 loss: 3.79705739e-06
Iter: 1718 loss: 3.78537925e-06
Iter: 1719 loss: 3.78418235e-06
Iter: 1720 loss: 3.78263098e-06
Iter: 1721 loss: 3.78247955e-06
Iter: 1722 loss: 3.78093932e-06
Iter: 1723 loss: 3.78626e-06
Iter: 1724 loss: 3.78054347e-06
Iter: 1725 loss: 3.77878678e-06
Iter: 1726 loss: 3.7777836e-06
Iter: 1727 loss: 3.77701826e-06
Iter: 1728 loss: 3.77556307e-06
Iter: 1729 loss: 3.77552396e-06
Iter: 1730 loss: 3.77465608e-06
Iter: 1731 loss: 3.77459037e-06
Iter: 1732 loss: 3.7741811e-06
Iter: 1733 loss: 3.77271022e-06
Iter: 1734 loss: 3.77466677e-06
Iter: 1735 loss: 3.77157266e-06
Iter: 1736 loss: 3.76958928e-06
Iter: 1737 loss: 3.78943741e-06
Iter: 1738 loss: 3.76962407e-06
Iter: 1739 loss: 3.76846e-06
Iter: 1740 loss: 3.76910202e-06
Iter: 1741 loss: 3.76777371e-06
Iter: 1742 loss: 3.76585945e-06
Iter: 1743 loss: 3.76751677e-06
Iter: 1744 loss: 3.76473963e-06
Iter: 1745 loss: 3.76267553e-06
Iter: 1746 loss: 3.76600383e-06
Iter: 1747 loss: 3.7618961e-06
Iter: 1748 loss: 3.76046273e-06
Iter: 1749 loss: 3.76036633e-06
Iter: 1750 loss: 3.75915033e-06
Iter: 1751 loss: 3.76247203e-06
Iter: 1752 loss: 3.75862987e-06
Iter: 1753 loss: 3.75765239e-06
Iter: 1754 loss: 3.75567379e-06
Iter: 1755 loss: 3.75569834e-06
Iter: 1756 loss: 3.75344302e-06
Iter: 1757 loss: 3.76434809e-06
Iter: 1758 loss: 3.7530433e-06
Iter: 1759 loss: 3.75150626e-06
Iter: 1760 loss: 3.75776676e-06
Iter: 1761 loss: 3.75112086e-06
Iter: 1762 loss: 3.75154718e-06
Iter: 1763 loss: 3.7505672e-06
Iter: 1764 loss: 3.75016862e-06
Iter: 1765 loss: 3.74879937e-06
Iter: 1766 loss: 3.75686659e-06
Iter: 1767 loss: 3.74849333e-06
Iter: 1768 loss: 3.74702108e-06
Iter: 1769 loss: 3.74616889e-06
Iter: 1770 loss: 3.74545289e-06
Iter: 1771 loss: 3.74387e-06
Iter: 1772 loss: 3.74383853e-06
Iter: 1773 loss: 3.74274941e-06
Iter: 1774 loss: 3.74206957e-06
Iter: 1775 loss: 3.74166848e-06
Iter: 1776 loss: 3.73965941e-06
Iter: 1777 loss: 3.74422825e-06
Iter: 1778 loss: 3.73875446e-06
Iter: 1779 loss: 3.73757348e-06
Iter: 1780 loss: 3.74368074e-06
Iter: 1781 loss: 3.73737157e-06
Iter: 1782 loss: 3.73621106e-06
Iter: 1783 loss: 3.74913543e-06
Iter: 1784 loss: 3.73612738e-06
Iter: 1785 loss: 3.73500802e-06
Iter: 1786 loss: 3.73490161e-06
Iter: 1787 loss: 3.73405624e-06
Iter: 1788 loss: 3.73297462e-06
Iter: 1789 loss: 3.7314021e-06
Iter: 1790 loss: 3.73128933e-06
Iter: 1791 loss: 3.72930958e-06
Iter: 1792 loss: 3.74295587e-06
Iter: 1793 loss: 3.72912086e-06
Iter: 1794 loss: 3.72843556e-06
Iter: 1795 loss: 3.72838394e-06
Iter: 1796 loss: 3.72736758e-06
Iter: 1797 loss: 3.72711156e-06
Iter: 1798 loss: 3.7264781e-06
Iter: 1799 loss: 3.72551403e-06
Iter: 1800 loss: 3.72424415e-06
Iter: 1801 loss: 3.72414024e-06
Iter: 1802 loss: 3.72282807e-06
Iter: 1803 loss: 3.72799104e-06
Iter: 1804 loss: 3.72248e-06
Iter: 1805 loss: 3.72107274e-06
Iter: 1806 loss: 3.72385102e-06
Iter: 1807 loss: 3.72042018e-06
Iter: 1808 loss: 3.71882129e-06
Iter: 1809 loss: 3.72276145e-06
Iter: 1810 loss: 3.71825513e-06
Iter: 1811 loss: 3.71670353e-06
Iter: 1812 loss: 3.71869555e-06
Iter: 1813 loss: 3.71588749e-06
Iter: 1814 loss: 3.71440319e-06
Iter: 1815 loss: 3.72652153e-06
Iter: 1816 loss: 3.71438114e-06
Iter: 1817 loss: 3.71291162e-06
Iter: 1818 loss: 3.71964893e-06
Iter: 1819 loss: 3.71259239e-06
Iter: 1820 loss: 3.71149099e-06
Iter: 1821 loss: 3.71093165e-06
Iter: 1822 loss: 3.71043916e-06
Iter: 1823 loss: 3.70899488e-06
Iter: 1824 loss: 3.70834323e-06
Iter: 1825 loss: 3.70772273e-06
Iter: 1826 loss: 3.70618909e-06
Iter: 1827 loss: 3.70616567e-06
Iter: 1828 loss: 3.70559133e-06
Iter: 1829 loss: 3.7055147e-06
Iter: 1830 loss: 3.70510452e-06
Iter: 1831 loss: 3.70401153e-06
Iter: 1832 loss: 3.7041691e-06
Iter: 1833 loss: 3.70292901e-06
Iter: 1834 loss: 3.70090356e-06
Iter: 1835 loss: 3.71247961e-06
Iter: 1836 loss: 3.70066937e-06
Iter: 1837 loss: 3.69913914e-06
Iter: 1838 loss: 3.70296493e-06
Iter: 1839 loss: 3.69862892e-06
Iter: 1840 loss: 3.69679651e-06
Iter: 1841 loss: 3.69936629e-06
Iter: 1842 loss: 3.69594272e-06
Iter: 1843 loss: 3.69443205e-06
Iter: 1844 loss: 3.70597195e-06
Iter: 1845 loss: 3.69442887e-06
Iter: 1846 loss: 3.69344548e-06
Iter: 1847 loss: 3.69266218e-06
Iter: 1848 loss: 3.69225381e-06
Iter: 1849 loss: 3.69156805e-06
Iter: 1850 loss: 3.69134773e-06
Iter: 1851 loss: 3.69047302e-06
Iter: 1852 loss: 3.68981773e-06
Iter: 1853 loss: 3.68957603e-06
Iter: 1854 loss: 3.68828887e-06
Iter: 1855 loss: 3.68651104e-06
Iter: 1856 loss: 3.68640281e-06
Iter: 1857 loss: 3.68487918e-06
Iter: 1858 loss: 3.70445241e-06
Iter: 1859 loss: 3.6848628e-06
Iter: 1860 loss: 3.68494e-06
Iter: 1861 loss: 3.68431847e-06
Iter: 1862 loss: 3.68387714e-06
Iter: 1863 loss: 3.68280212e-06
Iter: 1864 loss: 3.69230793e-06
Iter: 1865 loss: 3.68249e-06
Iter: 1866 loss: 3.68150359e-06
Iter: 1867 loss: 3.681615e-06
Iter: 1868 loss: 3.68066367e-06
Iter: 1869 loss: 3.67928988e-06
Iter: 1870 loss: 3.68492215e-06
Iter: 1871 loss: 3.6789138e-06
Iter: 1872 loss: 3.67796906e-06
Iter: 1873 loss: 3.67848679e-06
Iter: 1874 loss: 3.67713869e-06
Iter: 1875 loss: 3.67563325e-06
Iter: 1876 loss: 3.68526389e-06
Iter: 1877 loss: 3.67551047e-06
Iter: 1878 loss: 3.67448388e-06
Iter: 1879 loss: 3.67662187e-06
Iter: 1880 loss: 3.6742133e-06
Iter: 1881 loss: 3.67307257e-06
Iter: 1882 loss: 3.67348548e-06
Iter: 1883 loss: 3.67223856e-06
Iter: 1884 loss: 3.67170105e-06
Iter: 1885 loss: 3.67142934e-06
Iter: 1886 loss: 3.67095595e-06
Iter: 1887 loss: 3.66998438e-06
Iter: 1888 loss: 3.686318e-06
Iter: 1889 loss: 3.67002099e-06
Iter: 1890 loss: 3.66847939e-06
Iter: 1891 loss: 3.66820655e-06
Iter: 1892 loss: 3.66729068e-06
Iter: 1893 loss: 3.6674528e-06
Iter: 1894 loss: 3.66669769e-06
Iter: 1895 loss: 3.6660615e-06
Iter: 1896 loss: 3.66620225e-06
Iter: 1897 loss: 3.66551285e-06
Iter: 1898 loss: 3.66486984e-06
Iter: 1899 loss: 3.66333779e-06
Iter: 1900 loss: 3.67856092e-06
Iter: 1901 loss: 3.66308e-06
Iter: 1902 loss: 3.66175755e-06
Iter: 1903 loss: 3.6781953e-06
Iter: 1904 loss: 3.66178938e-06
Iter: 1905 loss: 3.66082486e-06
Iter: 1906 loss: 3.66133122e-06
Iter: 1907 loss: 3.66014592e-06
Iter: 1908 loss: 3.6590086e-06
Iter: 1909 loss: 3.66317613e-06
Iter: 1910 loss: 3.65869528e-06
Iter: 1911 loss: 3.65748338e-06
Iter: 1912 loss: 3.66080349e-06
Iter: 1913 loss: 3.65706774e-06
Iter: 1914 loss: 3.65595406e-06
Iter: 1915 loss: 3.65966775e-06
Iter: 1916 loss: 3.65560845e-06
Iter: 1917 loss: 3.65448477e-06
Iter: 1918 loss: 3.65969345e-06
Iter: 1919 loss: 3.65430492e-06
Iter: 1920 loss: 3.65306823e-06
Iter: 1921 loss: 3.65773667e-06
Iter: 1922 loss: 3.65267e-06
Iter: 1923 loss: 3.65199821e-06
Iter: 1924 loss: 3.65117648e-06
Iter: 1925 loss: 3.65105552e-06
Iter: 1926 loss: 3.64990115e-06
Iter: 1927 loss: 3.65848337e-06
Iter: 1928 loss: 3.64983316e-06
Iter: 1929 loss: 3.64871e-06
Iter: 1930 loss: 3.64877e-06
Iter: 1931 loss: 3.64838957e-06
Iter: 1932 loss: 3.64759012e-06
Iter: 1933 loss: 3.65355936e-06
Iter: 1934 loss: 3.64740345e-06
Iter: 1935 loss: 3.64626544e-06
Iter: 1936 loss: 3.64618018e-06
Iter: 1937 loss: 3.64542393e-06
Iter: 1938 loss: 3.64392645e-06
Iter: 1939 loss: 3.65768733e-06
Iter: 1940 loss: 3.64386233e-06
Iter: 1941 loss: 3.64285506e-06
Iter: 1942 loss: 3.64238622e-06
Iter: 1943 loss: 3.64194284e-06
Iter: 1944 loss: 3.64018956e-06
Iter: 1945 loss: 3.64975745e-06
Iter: 1946 loss: 3.63991785e-06
Iter: 1947 loss: 3.63870367e-06
Iter: 1948 loss: 3.64033031e-06
Iter: 1949 loss: 3.63811409e-06
Iter: 1950 loss: 3.63656227e-06
Iter: 1951 loss: 3.64348398e-06
Iter: 1952 loss: 3.63628442e-06
Iter: 1953 loss: 3.63549907e-06
Iter: 1954 loss: 3.63551135e-06
Iter: 1955 loss: 3.63481695e-06
Iter: 1956 loss: 3.63332606e-06
Iter: 1957 loss: 3.65728602e-06
Iter: 1958 loss: 3.63331174e-06
Iter: 1959 loss: 3.63197e-06
Iter: 1960 loss: 3.63649906e-06
Iter: 1961 loss: 3.63151275e-06
Iter: 1962 loss: 3.63243225e-06
Iter: 1963 loss: 3.63113804e-06
Iter: 1964 loss: 3.63079607e-06
Iter: 1965 loss: 3.62982928e-06
Iter: 1966 loss: 3.63300364e-06
Iter: 1967 loss: 3.62925402e-06
Iter: 1968 loss: 3.62806645e-06
Iter: 1969 loss: 3.62925744e-06
Iter: 1970 loss: 3.62745686e-06
Iter: 1971 loss: 3.62585797e-06
Iter: 1972 loss: 3.63113531e-06
Iter: 1973 loss: 3.62549372e-06
Iter: 1974 loss: 3.62415176e-06
Iter: 1975 loss: 3.63284107e-06
Iter: 1976 loss: 3.62403921e-06
Iter: 1977 loss: 3.62303399e-06
Iter: 1978 loss: 3.62246601e-06
Iter: 1979 loss: 3.6220149e-06
Iter: 1980 loss: 3.620391e-06
Iter: 1981 loss: 3.63231902e-06
Iter: 1982 loss: 3.62023047e-06
Iter: 1983 loss: 3.61935417e-06
Iter: 1984 loss: 3.62242508e-06
Iter: 1985 loss: 3.61923685e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8
+ date
Mon Oct 26 10:28:03 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.8_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.8_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf8450d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf998ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf877d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf911bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf911e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf7ccd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf7ccc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf76d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf76d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf6ff9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf715598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf73f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf73fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf6bc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf663bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf663f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf62c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf5d07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf592730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf59cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf598730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf5adb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf566a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf48e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf48b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf4b0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf4d7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf41d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf41d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf45f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cdf45f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cc4f70158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cc4f700d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cc4f70378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cc4f2f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3cc4f067b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.38081474e-05
Iter: 2 loss: 4.98444606e-05
Iter: 3 loss: 0.000172985805
Iter: 4 loss: 4.91690262e-05
Iter: 5 loss: 4.93427469e-05
Iter: 6 loss: 4.65992925e-05
Iter: 7 loss: 4.50904918e-05
Iter: 8 loss: 4.05130086e-05
Iter: 9 loss: 5.25277937e-05
Iter: 10 loss: 3.80115598e-05
Iter: 11 loss: 3.37561141e-05
Iter: 12 loss: 9.75763687e-05
Iter: 13 loss: 3.37520942e-05
Iter: 14 loss: 3.10380274e-05
Iter: 15 loss: 3.11971817e-05
Iter: 16 loss: 2.89237341e-05
Iter: 17 loss: 2.55847262e-05
Iter: 18 loss: 4.4352375e-05
Iter: 19 loss: 2.51221027e-05
Iter: 20 loss: 2.32652055e-05
Iter: 21 loss: 2.79358646e-05
Iter: 22 loss: 2.26088e-05
Iter: 23 loss: 2.13358835e-05
Iter: 24 loss: 2.44726762e-05
Iter: 25 loss: 2.08842375e-05
Iter: 26 loss: 1.97435984e-05
Iter: 27 loss: 2.14712582e-05
Iter: 28 loss: 1.92008411e-05
Iter: 29 loss: 1.81860269e-05
Iter: 30 loss: 2.22167982e-05
Iter: 31 loss: 1.79571489e-05
Iter: 32 loss: 1.72593718e-05
Iter: 33 loss: 1.7206843e-05
Iter: 34 loss: 1.66850368e-05
Iter: 35 loss: 1.57389532e-05
Iter: 36 loss: 1.95242937e-05
Iter: 37 loss: 1.55257403e-05
Iter: 38 loss: 1.50458236e-05
Iter: 39 loss: 1.51850772e-05
Iter: 40 loss: 1.47008759e-05
Iter: 41 loss: 1.40785105e-05
Iter: 42 loss: 1.79730378e-05
Iter: 43 loss: 1.40066168e-05
Iter: 44 loss: 1.40894481e-05
Iter: 45 loss: 1.38463165e-05
Iter: 46 loss: 1.37201096e-05
Iter: 47 loss: 1.3341185e-05
Iter: 48 loss: 1.44398582e-05
Iter: 49 loss: 1.31451598e-05
Iter: 50 loss: 1.26046343e-05
Iter: 51 loss: 1.44172536e-05
Iter: 52 loss: 1.24577264e-05
Iter: 53 loss: 1.2094557e-05
Iter: 54 loss: 1.28902911e-05
Iter: 55 loss: 1.19547976e-05
Iter: 56 loss: 1.15263865e-05
Iter: 57 loss: 1.44830756e-05
Iter: 58 loss: 1.14854338e-05
Iter: 59 loss: 1.12248554e-05
Iter: 60 loss: 1.18064445e-05
Iter: 61 loss: 1.11252666e-05
Iter: 62 loss: 1.08893146e-05
Iter: 63 loss: 1.21498088e-05
Iter: 64 loss: 1.08534496e-05
Iter: 65 loss: 1.0644897e-05
Iter: 66 loss: 1.05425415e-05
Iter: 67 loss: 1.04424034e-05
Iter: 68 loss: 1.01885435e-05
Iter: 69 loss: 1.28504935e-05
Iter: 70 loss: 1.01820369e-05
Iter: 71 loss: 1.00281159e-05
Iter: 72 loss: 1.01567603e-05
Iter: 73 loss: 9.93673e-06
Iter: 74 loss: 9.74641807e-06
Iter: 75 loss: 1.12190301e-05
Iter: 76 loss: 9.73256465e-06
Iter: 77 loss: 9.6705553e-06
Iter: 78 loss: 9.66436437e-06
Iter: 79 loss: 9.58572309e-06
Iter: 80 loss: 9.52105529e-06
Iter: 81 loss: 9.49800597e-06
Iter: 82 loss: 9.3951e-06
Iter: 83 loss: 9.30870829e-06
Iter: 84 loss: 9.2796e-06
Iter: 85 loss: 9.16406862e-06
Iter: 86 loss: 9.28175e-06
Iter: 87 loss: 9.09934624e-06
Iter: 88 loss: 8.95266e-06
Iter: 89 loss: 9.9536428e-06
Iter: 90 loss: 8.93817e-06
Iter: 91 loss: 8.85066947e-06
Iter: 92 loss: 9.06197e-06
Iter: 93 loss: 8.8192146e-06
Iter: 94 loss: 8.70244185e-06
Iter: 95 loss: 8.84412657e-06
Iter: 96 loss: 8.64115373e-06
Iter: 97 loss: 8.55915459e-06
Iter: 98 loss: 9.35288608e-06
Iter: 99 loss: 8.55627513e-06
Iter: 100 loss: 8.47550473e-06
Iter: 101 loss: 8.38484812e-06
Iter: 102 loss: 8.37278276e-06
Iter: 103 loss: 8.26202631e-06
Iter: 104 loss: 9.94936909e-06
Iter: 105 loss: 8.26210726e-06
Iter: 106 loss: 8.18762419e-06
Iter: 107 loss: 8.09401536e-06
Iter: 108 loss: 8.08646291e-06
Iter: 109 loss: 8.04144656e-06
Iter: 110 loss: 8.02046816e-06
Iter: 111 loss: 7.96978e-06
Iter: 112 loss: 8.36591244e-06
Iter: 113 loss: 7.96636414e-06
Iter: 114 loss: 7.94200423e-06
Iter: 115 loss: 7.8798239e-06
Iter: 116 loss: 8.40949178e-06
Iter: 117 loss: 7.86958117e-06
Iter: 118 loss: 7.78584945e-06
Iter: 119 loss: 7.96151926e-06
Iter: 120 loss: 7.75283115e-06
Iter: 121 loss: 7.68780046e-06
Iter: 122 loss: 7.93115214e-06
Iter: 123 loss: 7.67235e-06
Iter: 124 loss: 7.6062538e-06
Iter: 125 loss: 7.80677328e-06
Iter: 126 loss: 7.58624174e-06
Iter: 127 loss: 7.53571112e-06
Iter: 128 loss: 7.71448867e-06
Iter: 129 loss: 7.52270262e-06
Iter: 130 loss: 7.48067077e-06
Iter: 131 loss: 7.75446642e-06
Iter: 132 loss: 7.47637569e-06
Iter: 133 loss: 7.44228964e-06
Iter: 134 loss: 7.45352e-06
Iter: 135 loss: 7.41806e-06
Iter: 136 loss: 7.37359096e-06
Iter: 137 loss: 7.67911843e-06
Iter: 138 loss: 7.369179e-06
Iter: 139 loss: 7.34008609e-06
Iter: 140 loss: 7.39437201e-06
Iter: 141 loss: 7.32764465e-06
Iter: 142 loss: 7.28827763e-06
Iter: 143 loss: 7.34186597e-06
Iter: 144 loss: 7.26818325e-06
Iter: 145 loss: 7.26198687e-06
Iter: 146 loss: 7.24619713e-06
Iter: 147 loss: 7.23395e-06
Iter: 148 loss: 7.20369371e-06
Iter: 149 loss: 7.49462788e-06
Iter: 150 loss: 7.19948275e-06
Iter: 151 loss: 7.1586287e-06
Iter: 152 loss: 7.11926714e-06
Iter: 153 loss: 7.11037683e-06
Iter: 154 loss: 7.06379478e-06
Iter: 155 loss: 7.66225912e-06
Iter: 156 loss: 7.063456e-06
Iter: 157 loss: 7.03114392e-06
Iter: 158 loss: 7.00645069e-06
Iter: 159 loss: 6.99629436e-06
Iter: 160 loss: 6.94965729e-06
Iter: 161 loss: 7.49596529e-06
Iter: 162 loss: 6.94908249e-06
Iter: 163 loss: 6.91925698e-06
Iter: 164 loss: 6.91376135e-06
Iter: 165 loss: 6.89382796e-06
Iter: 166 loss: 6.84708493e-06
Iter: 167 loss: 7.20619482e-06
Iter: 168 loss: 6.84348743e-06
Iter: 169 loss: 6.82091741e-06
Iter: 170 loss: 6.86582962e-06
Iter: 171 loss: 6.81155643e-06
Iter: 172 loss: 6.78090373e-06
Iter: 173 loss: 6.82437894e-06
Iter: 174 loss: 6.76576474e-06
Iter: 175 loss: 6.73994055e-06
Iter: 176 loss: 6.93550191e-06
Iter: 177 loss: 6.73791419e-06
Iter: 178 loss: 6.72585429e-06
Iter: 179 loss: 6.72476926e-06
Iter: 180 loss: 6.71083126e-06
Iter: 181 loss: 6.68341318e-06
Iter: 182 loss: 7.22371306e-06
Iter: 183 loss: 6.68312623e-06
Iter: 184 loss: 6.65875e-06
Iter: 185 loss: 6.67240511e-06
Iter: 186 loss: 6.64263189e-06
Iter: 187 loss: 6.61233389e-06
Iter: 188 loss: 6.65172365e-06
Iter: 189 loss: 6.59667603e-06
Iter: 190 loss: 6.56464636e-06
Iter: 191 loss: 6.66495453e-06
Iter: 192 loss: 6.55541771e-06
Iter: 193 loss: 6.52369363e-06
Iter: 194 loss: 6.64224581e-06
Iter: 195 loss: 6.51600476e-06
Iter: 196 loss: 6.49019603e-06
Iter: 197 loss: 6.53192819e-06
Iter: 198 loss: 6.47838715e-06
Iter: 199 loss: 6.44284319e-06
Iter: 200 loss: 6.50758648e-06
Iter: 201 loss: 6.42748546e-06
Iter: 202 loss: 6.40028156e-06
Iter: 203 loss: 6.70728468e-06
Iter: 204 loss: 6.3997677e-06
Iter: 205 loss: 6.37808716e-06
Iter: 206 loss: 6.37544872e-06
Iter: 207 loss: 6.36007053e-06
Iter: 208 loss: 6.33695e-06
Iter: 209 loss: 6.60975275e-06
Iter: 210 loss: 6.3366474e-06
Iter: 211 loss: 6.32860429e-06
Iter: 212 loss: 6.3284856e-06
Iter: 213 loss: 6.3189e-06
Iter: 214 loss: 6.31689636e-06
Iter: 215 loss: 6.31091643e-06
Iter: 216 loss: 6.29798524e-06
Iter: 217 loss: 6.28076396e-06
Iter: 218 loss: 6.27979716e-06
Iter: 219 loss: 6.26143628e-06
Iter: 220 loss: 6.28542057e-06
Iter: 221 loss: 6.25225584e-06
Iter: 222 loss: 6.22670768e-06
Iter: 223 loss: 6.27738382e-06
Iter: 224 loss: 6.2159852e-06
Iter: 225 loss: 6.19608727e-06
Iter: 226 loss: 6.28416137e-06
Iter: 227 loss: 6.19222783e-06
Iter: 228 loss: 6.16856914e-06
Iter: 229 loss: 6.18547119e-06
Iter: 230 loss: 6.15375757e-06
Iter: 231 loss: 6.13364045e-06
Iter: 232 loss: 6.24661971e-06
Iter: 233 loss: 6.13070461e-06
Iter: 234 loss: 6.10919051e-06
Iter: 235 loss: 6.14360306e-06
Iter: 236 loss: 6.09912422e-06
Iter: 237 loss: 6.08074606e-06
Iter: 238 loss: 6.22552261e-06
Iter: 239 loss: 6.07932088e-06
Iter: 240 loss: 6.0647285e-06
Iter: 241 loss: 6.08578375e-06
Iter: 242 loss: 6.05774721e-06
Iter: 243 loss: 6.04436309e-06
Iter: 244 loss: 6.15193767e-06
Iter: 245 loss: 6.04345041e-06
Iter: 246 loss: 6.0343441e-06
Iter: 247 loss: 6.03430726e-06
Iter: 248 loss: 6.02949422e-06
Iter: 249 loss: 6.01710781e-06
Iter: 250 loss: 6.12132681e-06
Iter: 251 loss: 6.0150478e-06
Iter: 252 loss: 6.00165822e-06
Iter: 253 loss: 6.03777244e-06
Iter: 254 loss: 5.99706345e-06
Iter: 255 loss: 5.98137922e-06
Iter: 256 loss: 5.98169163e-06
Iter: 257 loss: 5.96891732e-06
Iter: 258 loss: 5.95437723e-06
Iter: 259 loss: 6.08989285e-06
Iter: 260 loss: 5.95379242e-06
Iter: 261 loss: 5.94086e-06
Iter: 262 loss: 5.93405457e-06
Iter: 263 loss: 5.928e-06
Iter: 264 loss: 5.91535718e-06
Iter: 265 loss: 5.91527487e-06
Iter: 266 loss: 5.90605805e-06
Iter: 267 loss: 5.89376396e-06
Iter: 268 loss: 5.8931e-06
Iter: 269 loss: 5.87671093e-06
Iter: 270 loss: 6.09950166e-06
Iter: 271 loss: 5.87666273e-06
Iter: 272 loss: 5.86738315e-06
Iter: 273 loss: 5.86904162e-06
Iter: 274 loss: 5.86044189e-06
Iter: 275 loss: 5.84537474e-06
Iter: 276 loss: 5.91073604e-06
Iter: 277 loss: 5.84235113e-06
Iter: 278 loss: 5.84172631e-06
Iter: 279 loss: 5.83726796e-06
Iter: 280 loss: 5.8336318e-06
Iter: 281 loss: 5.82557e-06
Iter: 282 loss: 5.93128243e-06
Iter: 283 loss: 5.82494886e-06
Iter: 284 loss: 5.81497261e-06
Iter: 285 loss: 5.80276355e-06
Iter: 286 loss: 5.80165124e-06
Iter: 287 loss: 5.78707431e-06
Iter: 288 loss: 5.92069728e-06
Iter: 289 loss: 5.7864645e-06
Iter: 290 loss: 5.77419632e-06
Iter: 291 loss: 5.76928824e-06
Iter: 292 loss: 5.76295679e-06
Iter: 293 loss: 5.74858859e-06
Iter: 294 loss: 5.8843425e-06
Iter: 295 loss: 5.74808109e-06
Iter: 296 loss: 5.73792249e-06
Iter: 297 loss: 5.75040212e-06
Iter: 298 loss: 5.73266698e-06
Iter: 299 loss: 5.72070348e-06
Iter: 300 loss: 5.74135538e-06
Iter: 301 loss: 5.71557848e-06
Iter: 302 loss: 5.70231896e-06
Iter: 303 loss: 5.77845231e-06
Iter: 304 loss: 5.70058728e-06
Iter: 305 loss: 5.69252234e-06
Iter: 306 loss: 5.7423963e-06
Iter: 307 loss: 5.69167787e-06
Iter: 308 loss: 5.68390533e-06
Iter: 309 loss: 5.68483574e-06
Iter: 310 loss: 5.67789266e-06
Iter: 311 loss: 5.6744675e-06
Iter: 312 loss: 5.67245479e-06
Iter: 313 loss: 5.66732797e-06
Iter: 314 loss: 5.66590643e-06
Iter: 315 loss: 5.66295694e-06
Iter: 316 loss: 5.65770733e-06
Iter: 317 loss: 5.64562652e-06
Iter: 318 loss: 5.80090364e-06
Iter: 319 loss: 5.64460424e-06
Iter: 320 loss: 5.63002868e-06
Iter: 321 loss: 5.74773912e-06
Iter: 322 loss: 5.62904552e-06
Iter: 323 loss: 5.62088462e-06
Iter: 324 loss: 5.62199421e-06
Iter: 325 loss: 5.61466823e-06
Iter: 326 loss: 5.60104854e-06
Iter: 327 loss: 5.64236188e-06
Iter: 328 loss: 5.5970163e-06
Iter: 329 loss: 5.58884949e-06
Iter: 330 loss: 5.62912919e-06
Iter: 331 loss: 5.58736383e-06
Iter: 332 loss: 5.5779683e-06
Iter: 333 loss: 5.57407839e-06
Iter: 334 loss: 5.56883424e-06
Iter: 335 loss: 5.55937095e-06
Iter: 336 loss: 5.66106337e-06
Iter: 337 loss: 5.55924635e-06
Iter: 338 loss: 5.55094721e-06
Iter: 339 loss: 5.56104e-06
Iter: 340 loss: 5.54652388e-06
Iter: 341 loss: 5.5392893e-06
Iter: 342 loss: 5.60122498e-06
Iter: 343 loss: 5.53891823e-06
Iter: 344 loss: 5.5336759e-06
Iter: 345 loss: 5.56896612e-06
Iter: 346 loss: 5.53325253e-06
Iter: 347 loss: 5.52692927e-06
Iter: 348 loss: 5.54527469e-06
Iter: 349 loss: 5.52476558e-06
Iter: 350 loss: 5.52050369e-06
Iter: 351 loss: 5.51335961e-06
Iter: 352 loss: 5.51328685e-06
Iter: 353 loss: 5.50621098e-06
Iter: 354 loss: 5.52046e-06
Iter: 355 loss: 5.50322602e-06
Iter: 356 loss: 5.49473e-06
Iter: 357 loss: 5.51415906e-06
Iter: 358 loss: 5.49167544e-06
Iter: 359 loss: 5.48199296e-06
Iter: 360 loss: 5.48961634e-06
Iter: 361 loss: 5.4761972e-06
Iter: 362 loss: 5.46545289e-06
Iter: 363 loss: 5.54185681e-06
Iter: 364 loss: 5.46456158e-06
Iter: 365 loss: 5.45636294e-06
Iter: 366 loss: 5.45403282e-06
Iter: 367 loss: 5.4489883e-06
Iter: 368 loss: 5.43856731e-06
Iter: 369 loss: 5.53293876e-06
Iter: 370 loss: 5.43819442e-06
Iter: 371 loss: 5.43067654e-06
Iter: 372 loss: 5.43065744e-06
Iter: 373 loss: 5.42467478e-06
Iter: 374 loss: 5.41625377e-06
Iter: 375 loss: 5.53384189e-06
Iter: 376 loss: 5.41630106e-06
Iter: 377 loss: 5.41121244e-06
Iter: 378 loss: 5.43193346e-06
Iter: 379 loss: 5.41023564e-06
Iter: 380 loss: 5.40461679e-06
Iter: 381 loss: 5.44967952e-06
Iter: 382 loss: 5.40413248e-06
Iter: 383 loss: 5.39997063e-06
Iter: 384 loss: 5.39375242e-06
Iter: 385 loss: 5.39361281e-06
Iter: 386 loss: 5.3890526e-06
Iter: 387 loss: 5.39083e-06
Iter: 388 loss: 5.3858821e-06
Iter: 389 loss: 5.37825053e-06
Iter: 390 loss: 5.3905751e-06
Iter: 391 loss: 5.3748663e-06
Iter: 392 loss: 5.36882681e-06
Iter: 393 loss: 5.40368046e-06
Iter: 394 loss: 5.36806874e-06
Iter: 395 loss: 5.36185053e-06
Iter: 396 loss: 5.36027801e-06
Iter: 397 loss: 5.35653726e-06
Iter: 398 loss: 5.34918763e-06
Iter: 399 loss: 5.41352347e-06
Iter: 400 loss: 5.34866649e-06
Iter: 401 loss: 5.34289347e-06
Iter: 402 loss: 5.33970069e-06
Iter: 403 loss: 5.33703769e-06
Iter: 404 loss: 5.32839249e-06
Iter: 405 loss: 5.39832536e-06
Iter: 406 loss: 5.32777e-06
Iter: 407 loss: 5.32114109e-06
Iter: 408 loss: 5.32343756e-06
Iter: 409 loss: 5.31649948e-06
Iter: 410 loss: 5.30880561e-06
Iter: 411 loss: 5.38602853e-06
Iter: 412 loss: 5.30844909e-06
Iter: 413 loss: 5.30640909e-06
Iter: 414 loss: 5.30523721e-06
Iter: 415 loss: 5.30290436e-06
Iter: 416 loss: 5.29854969e-06
Iter: 417 loss: 5.39709254e-06
Iter: 418 loss: 5.29856834e-06
Iter: 419 loss: 5.29454928e-06
Iter: 420 loss: 5.29924728e-06
Iter: 421 loss: 5.29225508e-06
Iter: 422 loss: 5.28747114e-06
Iter: 423 loss: 5.28293685e-06
Iter: 424 loss: 5.28174405e-06
Iter: 425 loss: 5.27534939e-06
Iter: 426 loss: 5.35764866e-06
Iter: 427 loss: 5.27520569e-06
Iter: 428 loss: 5.27012298e-06
Iter: 429 loss: 5.26298845e-06
Iter: 430 loss: 5.26259191e-06
Iter: 431 loss: 5.25605628e-06
Iter: 432 loss: 5.25596079e-06
Iter: 433 loss: 5.25204086e-06
Iter: 434 loss: 5.24497955e-06
Iter: 435 loss: 5.24499274e-06
Iter: 436 loss: 5.23752715e-06
Iter: 437 loss: 5.33894399e-06
Iter: 438 loss: 5.23738481e-06
Iter: 439 loss: 5.23251583e-06
Iter: 440 loss: 5.22724486e-06
Iter: 441 loss: 5.22637492e-06
Iter: 442 loss: 5.21733e-06
Iter: 443 loss: 5.29377348e-06
Iter: 444 loss: 5.2168225e-06
Iter: 445 loss: 5.21784204e-06
Iter: 446 loss: 5.21463062e-06
Iter: 447 loss: 5.21260063e-06
Iter: 448 loss: 5.20885396e-06
Iter: 449 loss: 5.28748387e-06
Iter: 450 loss: 5.20878848e-06
Iter: 451 loss: 5.20398953e-06
Iter: 452 loss: 5.20424601e-06
Iter: 453 loss: 5.2002415e-06
Iter: 454 loss: 5.19486e-06
Iter: 455 loss: 5.2058931e-06
Iter: 456 loss: 5.19255718e-06
Iter: 457 loss: 5.18673278e-06
Iter: 458 loss: 5.19243758e-06
Iter: 459 loss: 5.18327261e-06
Iter: 460 loss: 5.17655917e-06
Iter: 461 loss: 5.20124513e-06
Iter: 462 loss: 5.17485387e-06
Iter: 463 loss: 5.16793443e-06
Iter: 464 loss: 5.19695186e-06
Iter: 465 loss: 5.16650744e-06
Iter: 466 loss: 5.16155524e-06
Iter: 467 loss: 5.16660839e-06
Iter: 468 loss: 5.15892498e-06
Iter: 469 loss: 5.15266083e-06
Iter: 470 loss: 5.18302295e-06
Iter: 471 loss: 5.15151e-06
Iter: 472 loss: 5.14735711e-06
Iter: 473 loss: 5.1473221e-06
Iter: 474 loss: 5.1439788e-06
Iter: 475 loss: 5.13639861e-06
Iter: 476 loss: 5.16472301e-06
Iter: 477 loss: 5.1345387e-06
Iter: 478 loss: 5.13131681e-06
Iter: 479 loss: 5.13135274e-06
Iter: 480 loss: 5.12754286e-06
Iter: 481 loss: 5.13884333e-06
Iter: 482 loss: 5.1264833e-06
Iter: 483 loss: 5.1237048e-06
Iter: 484 loss: 5.11964481e-06
Iter: 485 loss: 5.11948656e-06
Iter: 486 loss: 5.1160996e-06
Iter: 487 loss: 5.13297255e-06
Iter: 488 loss: 5.11538e-06
Iter: 489 loss: 5.1112529e-06
Iter: 490 loss: 5.10413611e-06
Iter: 491 loss: 5.10421614e-06
Iter: 492 loss: 5.09833308e-06
Iter: 493 loss: 5.17738044e-06
Iter: 494 loss: 5.09830897e-06
Iter: 495 loss: 5.09372239e-06
Iter: 496 loss: 5.0944418e-06
Iter: 497 loss: 5.09017627e-06
Iter: 498 loss: 5.08481617e-06
Iter: 499 loss: 5.11805501e-06
Iter: 500 loss: 5.08417497e-06
Iter: 501 loss: 5.07990899e-06
Iter: 502 loss: 5.09317351e-06
Iter: 503 loss: 5.07853747e-06
Iter: 504 loss: 5.07460891e-06
Iter: 505 loss: 5.07908044e-06
Iter: 506 loss: 5.07238474e-06
Iter: 507 loss: 5.06725382e-06
Iter: 508 loss: 5.09147958e-06
Iter: 509 loss: 5.06622746e-06
Iter: 510 loss: 5.06244169e-06
Iter: 511 loss: 5.06839888e-06
Iter: 512 loss: 5.06039305e-06
Iter: 513 loss: 5.0596027e-06
Iter: 514 loss: 5.05809385e-06
Iter: 515 loss: 5.05617845e-06
Iter: 516 loss: 5.05227126e-06
Iter: 517 loss: 5.12174211e-06
Iter: 518 loss: 5.05217895e-06
Iter: 519 loss: 5.04959962e-06
Iter: 520 loss: 5.05199932e-06
Iter: 521 loss: 5.0482563e-06
Iter: 522 loss: 5.04409854e-06
Iter: 523 loss: 5.04627405e-06
Iter: 524 loss: 5.04141281e-06
Iter: 525 loss: 5.03712818e-06
Iter: 526 loss: 5.0647177e-06
Iter: 527 loss: 5.03672e-06
Iter: 528 loss: 5.03296542e-06
Iter: 529 loss: 5.02926923e-06
Iter: 530 loss: 5.02848161e-06
Iter: 531 loss: 5.0235e-06
Iter: 532 loss: 5.08164976e-06
Iter: 533 loss: 5.02325929e-06
Iter: 534 loss: 5.01941804e-06
Iter: 535 loss: 5.01558952e-06
Iter: 536 loss: 5.01483919e-06
Iter: 537 loss: 5.01043496e-06
Iter: 538 loss: 5.01044815e-06
Iter: 539 loss: 5.00663145e-06
Iter: 540 loss: 5.00049191e-06
Iter: 541 loss: 5.00046553e-06
Iter: 542 loss: 4.99394491e-06
Iter: 543 loss: 5.0558383e-06
Iter: 544 loss: 4.99369571e-06
Iter: 545 loss: 4.99011639e-06
Iter: 546 loss: 5.03875708e-06
Iter: 547 loss: 4.99013822e-06
Iter: 548 loss: 4.98653935e-06
Iter: 549 loss: 5.00163969e-06
Iter: 550 loss: 4.98576628e-06
Iter: 551 loss: 4.98411919e-06
Iter: 552 loss: 4.9802743e-06
Iter: 553 loss: 5.03404135e-06
Iter: 554 loss: 4.9801024e-06
Iter: 555 loss: 4.9756618e-06
Iter: 556 loss: 4.99531097e-06
Iter: 557 loss: 4.9748e-06
Iter: 558 loss: 4.97143674e-06
Iter: 559 loss: 4.97723659e-06
Iter: 560 loss: 4.96967823e-06
Iter: 561 loss: 4.96469147e-06
Iter: 562 loss: 4.9678265e-06
Iter: 563 loss: 4.96144867e-06
Iter: 564 loss: 4.95773247e-06
Iter: 565 loss: 4.98768168e-06
Iter: 566 loss: 4.95752283e-06
Iter: 567 loss: 4.95328686e-06
Iter: 568 loss: 4.95153336e-06
Iter: 569 loss: 4.94927099e-06
Iter: 570 loss: 4.94471e-06
Iter: 571 loss: 4.96169559e-06
Iter: 572 loss: 4.94366577e-06
Iter: 573 loss: 4.93801599e-06
Iter: 574 loss: 4.94600044e-06
Iter: 575 loss: 4.93531661e-06
Iter: 576 loss: 4.93161679e-06
Iter: 577 loss: 4.95718405e-06
Iter: 578 loss: 4.93123525e-06
Iter: 579 loss: 4.92733034e-06
Iter: 580 loss: 4.93092239e-06
Iter: 581 loss: 4.925e-06
Iter: 582 loss: 4.9256987e-06
Iter: 583 loss: 4.92284698e-06
Iter: 584 loss: 4.92189793e-06
Iter: 585 loss: 4.91911351e-06
Iter: 586 loss: 4.93512289e-06
Iter: 587 loss: 4.91825904e-06
Iter: 588 loss: 4.91475112e-06
Iter: 589 loss: 4.91730498e-06
Iter: 590 loss: 4.91251194e-06
Iter: 591 loss: 4.90851562e-06
Iter: 592 loss: 4.93837433e-06
Iter: 593 loss: 4.90821094e-06
Iter: 594 loss: 4.90512912e-06
Iter: 595 loss: 4.90951879e-06
Iter: 596 loss: 4.90348293e-06
Iter: 597 loss: 4.90034063e-06
Iter: 598 loss: 4.90607817e-06
Iter: 599 loss: 4.89904369e-06
Iter: 600 loss: 4.89513741e-06
Iter: 601 loss: 4.89859667e-06
Iter: 602 loss: 4.89259401e-06
Iter: 603 loss: 4.88862133e-06
Iter: 604 loss: 4.91955507e-06
Iter: 605 loss: 4.8884e-06
Iter: 606 loss: 4.88463957e-06
Iter: 607 loss: 4.88154137e-06
Iter: 608 loss: 4.88047499e-06
Iter: 609 loss: 4.87656689e-06
Iter: 610 loss: 4.9365226e-06
Iter: 611 loss: 4.87660327e-06
Iter: 612 loss: 4.87307e-06
Iter: 613 loss: 4.86814633e-06
Iter: 614 loss: 4.86796034e-06
Iter: 615 loss: 4.87769103e-06
Iter: 616 loss: 4.86669433e-06
Iter: 617 loss: 4.86559111e-06
Iter: 618 loss: 4.86309909e-06
Iter: 619 loss: 4.90061893e-06
Iter: 620 loss: 4.86312365e-06
Iter: 621 loss: 4.86019417e-06
Iter: 622 loss: 4.85615055e-06
Iter: 623 loss: 4.85593409e-06
Iter: 624 loss: 4.85212831e-06
Iter: 625 loss: 4.89129525e-06
Iter: 626 loss: 4.85207966e-06
Iter: 627 loss: 4.84857446e-06
Iter: 628 loss: 4.84784823e-06
Iter: 629 loss: 4.84524799e-06
Iter: 630 loss: 4.84010616e-06
Iter: 631 loss: 4.87243642e-06
Iter: 632 loss: 4.83971962e-06
Iter: 633 loss: 4.83597023e-06
Iter: 634 loss: 4.83470467e-06
Iter: 635 loss: 4.83267286e-06
Iter: 636 loss: 4.8281895e-06
Iter: 637 loss: 4.85292503e-06
Iter: 638 loss: 4.82759879e-06
Iter: 639 loss: 4.82316364e-06
Iter: 640 loss: 4.83457461e-06
Iter: 641 loss: 4.821728e-06
Iter: 642 loss: 4.81832376e-06
Iter: 643 loss: 4.8256411e-06
Iter: 644 loss: 4.81694542e-06
Iter: 645 loss: 4.8120346e-06
Iter: 646 loss: 4.82079122e-06
Iter: 647 loss: 4.80988274e-06
Iter: 648 loss: 4.80680683e-06
Iter: 649 loss: 4.82347878e-06
Iter: 650 loss: 4.80634117e-06
Iter: 651 loss: 4.80298695e-06
Iter: 652 loss: 4.84134489e-06
Iter: 653 loss: 4.80303288e-06
Iter: 654 loss: 4.80114613e-06
Iter: 655 loss: 4.79746723e-06
Iter: 656 loss: 4.86901445e-06
Iter: 657 loss: 4.79740174e-06
Iter: 658 loss: 4.79460687e-06
Iter: 659 loss: 4.79510891e-06
Iter: 660 loss: 4.7924359e-06
Iter: 661 loss: 4.78717675e-06
Iter: 662 loss: 4.80062317e-06
Iter: 663 loss: 4.78540778e-06
Iter: 664 loss: 4.78228958e-06
Iter: 665 loss: 4.79064784e-06
Iter: 666 loss: 4.78118091e-06
Iter: 667 loss: 4.7770036e-06
Iter: 668 loss: 4.78630864e-06
Iter: 669 loss: 4.77531466e-06
Iter: 670 loss: 4.77177764e-06
Iter: 671 loss: 4.79743085e-06
Iter: 672 loss: 4.77142839e-06
Iter: 673 loss: 4.76941841e-06
Iter: 674 loss: 4.76671312e-06
Iter: 675 loss: 4.76648802e-06
Iter: 676 loss: 4.76185505e-06
Iter: 677 loss: 4.78791844e-06
Iter: 678 loss: 4.76129571e-06
Iter: 679 loss: 4.7580038e-06
Iter: 680 loss: 4.76134846e-06
Iter: 681 loss: 4.75622164e-06
Iter: 682 loss: 4.75247816e-06
Iter: 683 loss: 4.7666208e-06
Iter: 684 loss: 4.7515382e-06
Iter: 685 loss: 4.74855187e-06
Iter: 686 loss: 4.77843423e-06
Iter: 687 loss: 4.74844819e-06
Iter: 688 loss: 4.74559511e-06
Iter: 689 loss: 4.77320464e-06
Iter: 690 loss: 4.74541321e-06
Iter: 691 loss: 4.74446369e-06
Iter: 692 loss: 4.74197896e-06
Iter: 693 loss: 4.75632896e-06
Iter: 694 loss: 4.74137232e-06
Iter: 695 loss: 4.73729733e-06
Iter: 696 loss: 4.74748822e-06
Iter: 697 loss: 4.73596174e-06
Iter: 698 loss: 4.73310592e-06
Iter: 699 loss: 4.73532691e-06
Iter: 700 loss: 4.73146156e-06
Iter: 701 loss: 4.72731153e-06
Iter: 702 loss: 4.75673869e-06
Iter: 703 loss: 4.72701868e-06
Iter: 704 loss: 4.72476313e-06
Iter: 705 loss: 4.73394584e-06
Iter: 706 loss: 4.72428837e-06
Iter: 707 loss: 4.72149259e-06
Iter: 708 loss: 4.71858039e-06
Iter: 709 loss: 4.71803469e-06
Iter: 710 loss: 4.71516341e-06
Iter: 711 loss: 4.7547619e-06
Iter: 712 loss: 4.71514068e-06
Iter: 713 loss: 4.71260319e-06
Iter: 714 loss: 4.71061867e-06
Iter: 715 loss: 4.7098606e-06
Iter: 716 loss: 4.70648047e-06
Iter: 717 loss: 4.74147419e-06
Iter: 718 loss: 4.70625764e-06
Iter: 719 loss: 4.70387977e-06
Iter: 720 loss: 4.70206669e-06
Iter: 721 loss: 4.7013018e-06
Iter: 722 loss: 4.69978841e-06
Iter: 723 loss: 4.69919e-06
Iter: 724 loss: 4.69677161e-06
Iter: 725 loss: 4.6995674e-06
Iter: 726 loss: 4.695592e-06
Iter: 727 loss: 4.69419774e-06
Iter: 728 loss: 4.69061479e-06
Iter: 729 loss: 4.72551e-06
Iter: 730 loss: 4.69010547e-06
Iter: 731 loss: 4.68599319e-06
Iter: 732 loss: 4.69177394e-06
Iter: 733 loss: 4.68390454e-06
Iter: 734 loss: 4.67920745e-06
Iter: 735 loss: 4.7169442e-06
Iter: 736 loss: 4.67905375e-06
Iter: 737 loss: 4.67560585e-06
Iter: 738 loss: 4.67853351e-06
Iter: 739 loss: 4.67357768e-06
Iter: 740 loss: 4.66901429e-06
Iter: 741 loss: 4.70166287e-06
Iter: 742 loss: 4.66868596e-06
Iter: 743 loss: 4.66602523e-06
Iter: 744 loss: 4.67226801e-06
Iter: 745 loss: 4.66501206e-06
Iter: 746 loss: 4.66174561e-06
Iter: 747 loss: 4.66069559e-06
Iter: 748 loss: 4.6587611e-06
Iter: 749 loss: 4.65506582e-06
Iter: 750 loss: 4.68948e-06
Iter: 751 loss: 4.65498579e-06
Iter: 752 loss: 4.65182529e-06
Iter: 753 loss: 4.64926279e-06
Iter: 754 loss: 4.64835648e-06
Iter: 755 loss: 4.64548521e-06
Iter: 756 loss: 4.64552249e-06
Iter: 757 loss: 4.64417099e-06
Iter: 758 loss: 4.64408549e-06
Iter: 759 loss: 4.64254117e-06
Iter: 760 loss: 4.63846209e-06
Iter: 761 loss: 4.66821712e-06
Iter: 762 loss: 4.63759e-06
Iter: 763 loss: 4.634996e-06
Iter: 764 loss: 4.64996765e-06
Iter: 765 loss: 4.63471588e-06
Iter: 766 loss: 4.63167453e-06
Iter: 767 loss: 4.62778917e-06
Iter: 768 loss: 4.62761727e-06
Iter: 769 loss: 4.62389471e-06
Iter: 770 loss: 4.66187066e-06
Iter: 771 loss: 4.62383423e-06
Iter: 772 loss: 4.62019398e-06
Iter: 773 loss: 4.62119351e-06
Iter: 774 loss: 4.6174564e-06
Iter: 775 loss: 4.61389936e-06
Iter: 776 loss: 4.62434309e-06
Iter: 777 loss: 4.61268246e-06
Iter: 778 loss: 4.60952742e-06
Iter: 779 loss: 4.65393441e-06
Iter: 780 loss: 4.60948513e-06
Iter: 781 loss: 4.60708e-06
Iter: 782 loss: 4.60454157e-06
Iter: 783 loss: 4.60400952e-06
Iter: 784 loss: 4.60046431e-06
Iter: 785 loss: 4.62811477e-06
Iter: 786 loss: 4.6001669e-06
Iter: 787 loss: 4.59750527e-06
Iter: 788 loss: 4.59600415e-06
Iter: 789 loss: 4.5949073e-06
Iter: 790 loss: 4.590961e-06
Iter: 791 loss: 4.63002289e-06
Iter: 792 loss: 4.59086277e-06
Iter: 793 loss: 4.59065177e-06
Iter: 794 loss: 4.58983368e-06
Iter: 795 loss: 4.58861223e-06
Iter: 796 loss: 4.58572322e-06
Iter: 797 loss: 4.60450792e-06
Iter: 798 loss: 4.58496288e-06
Iter: 799 loss: 4.58250452e-06
Iter: 800 loss: 4.59438e-06
Iter: 801 loss: 4.58211525e-06
Iter: 802 loss: 4.58021577e-06
Iter: 803 loss: 4.57952592e-06
Iter: 804 loss: 4.57846e-06
Iter: 805 loss: 4.57465239e-06
Iter: 806 loss: 4.58651e-06
Iter: 807 loss: 4.57357055e-06
Iter: 808 loss: 4.57156e-06
Iter: 809 loss: 4.57761553e-06
Iter: 810 loss: 4.57083206e-06
Iter: 811 loss: 4.56799853e-06
Iter: 812 loss: 4.57056876e-06
Iter: 813 loss: 4.56618454e-06
Iter: 814 loss: 4.56390489e-06
Iter: 815 loss: 4.56781663e-06
Iter: 816 loss: 4.56275302e-06
Iter: 817 loss: 4.55916825e-06
Iter: 818 loss: 4.56474299e-06
Iter: 819 loss: 4.55746431e-06
Iter: 820 loss: 4.55456939e-06
Iter: 821 loss: 4.56441921e-06
Iter: 822 loss: 4.55369536e-06
Iter: 823 loss: 4.54988276e-06
Iter: 824 loss: 4.56501311e-06
Iter: 825 loss: 4.54895189e-06
Iter: 826 loss: 4.54680367e-06
Iter: 827 loss: 4.55263853e-06
Iter: 828 loss: 4.54607198e-06
Iter: 829 loss: 4.54440487e-06
Iter: 830 loss: 4.54441488e-06
Iter: 831 loss: 4.54252267e-06
Iter: 832 loss: 4.54274686e-06
Iter: 833 loss: 4.54100109e-06
Iter: 834 loss: 4.53984148e-06
Iter: 835 loss: 4.5368497e-06
Iter: 836 loss: 4.57159467e-06
Iter: 837 loss: 4.53674602e-06
Iter: 838 loss: 4.53252323e-06
Iter: 839 loss: 4.54988776e-06
Iter: 840 loss: 4.53157736e-06
Iter: 841 loss: 4.52886843e-06
Iter: 842 loss: 4.53387656e-06
Iter: 843 loss: 4.52753193e-06
Iter: 844 loss: 4.52404083e-06
Iter: 845 loss: 4.53376879e-06
Iter: 846 loss: 4.52288759e-06
Iter: 847 loss: 4.51966662e-06
Iter: 848 loss: 4.52958875e-06
Iter: 849 loss: 4.51871529e-06
Iter: 850 loss: 4.51551205e-06
Iter: 851 loss: 4.52285076e-06
Iter: 852 loss: 4.51449614e-06
Iter: 853 loss: 4.51109054e-06
Iter: 854 loss: 4.51777714e-06
Iter: 855 loss: 4.50958942e-06
Iter: 856 loss: 4.50585321e-06
Iter: 857 loss: 4.51984579e-06
Iter: 858 loss: 4.50507878e-06
Iter: 859 loss: 4.50191465e-06
Iter: 860 loss: 4.50396692e-06
Iter: 861 loss: 4.50002972e-06
Iter: 862 loss: 4.49617573e-06
Iter: 863 loss: 4.52805034e-06
Iter: 864 loss: 4.49606432e-06
Iter: 865 loss: 4.49359777e-06
Iter: 866 loss: 4.50232073e-06
Iter: 867 loss: 4.49297295e-06
Iter: 868 loss: 4.49005893e-06
Iter: 869 loss: 4.48729043e-06
Iter: 870 loss: 4.48654828e-06
Iter: 871 loss: 4.49034906e-06
Iter: 872 loss: 4.48480114e-06
Iter: 873 loss: 4.48424e-06
Iter: 874 loss: 4.48235733e-06
Iter: 875 loss: 4.48276205e-06
Iter: 876 loss: 4.48047558e-06
Iter: 877 loss: 4.47629e-06
Iter: 878 loss: 4.48851097e-06
Iter: 879 loss: 4.47513548e-06
Iter: 880 loss: 4.47231832e-06
Iter: 881 loss: 4.48817264e-06
Iter: 882 loss: 4.47202137e-06
Iter: 883 loss: 4.46897593e-06
Iter: 884 loss: 4.46661761e-06
Iter: 885 loss: 4.46573e-06
Iter: 886 loss: 4.46311469e-06
Iter: 887 loss: 4.46302101e-06
Iter: 888 loss: 4.46057038e-06
Iter: 889 loss: 4.45653404e-06
Iter: 890 loss: 4.45648857e-06
Iter: 891 loss: 4.45364276e-06
Iter: 892 loss: 4.45356636e-06
Iter: 893 loss: 4.45134356e-06
Iter: 894 loss: 4.44890338e-06
Iter: 895 loss: 4.44846955e-06
Iter: 896 loss: 4.44532952e-06
Iter: 897 loss: 4.47984257e-06
Iter: 898 loss: 4.44528087e-06
Iter: 899 loss: 4.44268653e-06
Iter: 900 loss: 4.44266425e-06
Iter: 901 loss: 4.44056423e-06
Iter: 902 loss: 4.43775798e-06
Iter: 903 loss: 4.46446847e-06
Iter: 904 loss: 4.43752651e-06
Iter: 905 loss: 4.43558974e-06
Iter: 906 loss: 4.45993555e-06
Iter: 907 loss: 4.43559748e-06
Iter: 908 loss: 4.43367935e-06
Iter: 909 loss: 4.43999625e-06
Iter: 910 loss: 4.43315139e-06
Iter: 911 loss: 4.43206682e-06
Iter: 912 loss: 4.42922328e-06
Iter: 913 loss: 4.46179365e-06
Iter: 914 loss: 4.42905639e-06
Iter: 915 loss: 4.42579858e-06
Iter: 916 loss: 4.43368117e-06
Iter: 917 loss: 4.42455575e-06
Iter: 918 loss: 4.42207329e-06
Iter: 919 loss: 4.42642431e-06
Iter: 920 loss: 4.42105465e-06
Iter: 921 loss: 4.41767043e-06
Iter: 922 loss: 4.42795363e-06
Iter: 923 loss: 4.4165995e-06
Iter: 924 loss: 4.41422344e-06
Iter: 925 loss: 4.42922919e-06
Iter: 926 loss: 4.4139415e-06
Iter: 927 loss: 4.41167776e-06
Iter: 928 loss: 4.41057136e-06
Iter: 929 loss: 4.4093681e-06
Iter: 930 loss: 4.40660369e-06
Iter: 931 loss: 4.43333056e-06
Iter: 932 loss: 4.40648091e-06
Iter: 933 loss: 4.40398708e-06
Iter: 934 loss: 4.40148233e-06
Iter: 935 loss: 4.4010726e-06
Iter: 936 loss: 4.39818905e-06
Iter: 937 loss: 4.43751651e-06
Iter: 938 loss: 4.39817177e-06
Iter: 939 loss: 4.39575615e-06
Iter: 940 loss: 4.39425094e-06
Iter: 941 loss: 4.39329779e-06
Iter: 942 loss: 4.39437099e-06
Iter: 943 loss: 4.39217547e-06
Iter: 944 loss: 4.39095902e-06
Iter: 945 loss: 4.39170208e-06
Iter: 946 loss: 4.39024279e-06
Iter: 947 loss: 4.38897541e-06
Iter: 948 loss: 4.38543657e-06
Iter: 949 loss: 4.40148324e-06
Iter: 950 loss: 4.38435654e-06
Iter: 951 loss: 4.38293728e-06
Iter: 952 loss: 4.38219467e-06
Iter: 953 loss: 4.38059942e-06
Iter: 954 loss: 4.37721428e-06
Iter: 955 loss: 4.44518582e-06
Iter: 956 loss: 4.37709241e-06
Iter: 957 loss: 4.37413473e-06
Iter: 958 loss: 4.4092385e-06
Iter: 959 loss: 4.37396375e-06
Iter: 960 loss: 4.37176914e-06
Iter: 961 loss: 4.37169501e-06
Iter: 962 loss: 4.36995833e-06
Iter: 963 loss: 4.36722394e-06
Iter: 964 loss: 4.38562529e-06
Iter: 965 loss: 4.36688606e-06
Iter: 966 loss: 4.36420623e-06
Iter: 967 loss: 4.36902792e-06
Iter: 968 loss: 4.36299888e-06
Iter: 969 loss: 4.36040409e-06
Iter: 970 loss: 4.37263816e-06
Iter: 971 loss: 4.35993297e-06
Iter: 972 loss: 4.35742504e-06
Iter: 973 loss: 4.36332039e-06
Iter: 974 loss: 4.35642e-06
Iter: 975 loss: 4.35431457e-06
Iter: 976 loss: 4.35903621e-06
Iter: 977 loss: 4.35352695e-06
Iter: 978 loss: 4.35245875e-06
Iter: 979 loss: 4.3519708e-06
Iter: 980 loss: 4.35066295e-06
Iter: 981 loss: 4.34768663e-06
Iter: 982 loss: 4.39513769e-06
Iter: 983 loss: 4.3475784e-06
Iter: 984 loss: 4.34574349e-06
Iter: 985 loss: 4.34795129e-06
Iter: 986 loss: 4.34484809e-06
Iter: 987 loss: 4.34230651e-06
Iter: 988 loss: 4.34650065e-06
Iter: 989 loss: 4.34121421e-06
Iter: 990 loss: 4.33879586e-06
Iter: 991 loss: 4.35482661e-06
Iter: 992 loss: 4.33845207e-06
Iter: 993 loss: 4.3366108e-06
Iter: 994 loss: 4.3355e-06
Iter: 995 loss: 4.33466676e-06
Iter: 996 loss: 4.33210516e-06
Iter: 997 loss: 4.35882157e-06
Iter: 998 loss: 4.33199784e-06
Iter: 999 loss: 4.33032392e-06
Iter: 1000 loss: 4.32821e-06
Iter: 1001 loss: 4.32807064e-06
Iter: 1002 loss: 4.32470142e-06
Iter: 1003 loss: 4.34888079e-06
Iter: 1004 loss: 4.3244936e-06
Iter: 1005 loss: 4.32196384e-06
Iter: 1006 loss: 4.32078468e-06
Iter: 1007 loss: 4.31975059e-06
Iter: 1008 loss: 4.31638273e-06
Iter: 1009 loss: 4.35510265e-06
Iter: 1010 loss: 4.31631315e-06
Iter: 1011 loss: 4.31424405e-06
Iter: 1012 loss: 4.31256467e-06
Iter: 1013 loss: 4.31180797e-06
Iter: 1014 loss: 4.31136459e-06
Iter: 1015 loss: 4.31000399e-06
Iter: 1016 loss: 4.30844966e-06
Iter: 1017 loss: 4.30826367e-06
Iter: 1018 loss: 4.30707496e-06
Iter: 1019 loss: 4.30577074e-06
Iter: 1020 loss: 4.30281443e-06
Iter: 1021 loss: 4.3480677e-06
Iter: 1022 loss: 4.30276532e-06
Iter: 1023 loss: 4.29958254e-06
Iter: 1024 loss: 4.32738716e-06
Iter: 1025 loss: 4.29939882e-06
Iter: 1026 loss: 4.29753163e-06
Iter: 1027 loss: 4.30315322e-06
Iter: 1028 loss: 4.29692045e-06
Iter: 1029 loss: 4.29465035e-06
Iter: 1030 loss: 4.29783677e-06
Iter: 1031 loss: 4.29349893e-06
Iter: 1032 loss: 4.29104784e-06
Iter: 1033 loss: 4.29399415e-06
Iter: 1034 loss: 4.28972908e-06
Iter: 1035 loss: 4.28702151e-06
Iter: 1036 loss: 4.30297541e-06
Iter: 1037 loss: 4.28664316e-06
Iter: 1038 loss: 4.28429757e-06
Iter: 1039 loss: 4.2824513e-06
Iter: 1040 loss: 4.2818865e-06
Iter: 1041 loss: 4.27894383e-06
Iter: 1042 loss: 4.27893929e-06
Iter: 1043 loss: 4.27743498e-06
Iter: 1044 loss: 4.27449049e-06
Iter: 1045 loss: 4.3435607e-06
Iter: 1046 loss: 4.2745487e-06
Iter: 1047 loss: 4.27141e-06
Iter: 1048 loss: 4.31223e-06
Iter: 1049 loss: 4.27142641e-06
Iter: 1050 loss: 4.2697061e-06
Iter: 1051 loss: 4.27826e-06
Iter: 1052 loss: 4.26942597e-06
Iter: 1053 loss: 4.26728e-06
Iter: 1054 loss: 4.28065596e-06
Iter: 1055 loss: 4.26707356e-06
Iter: 1056 loss: 4.26620227e-06
Iter: 1057 loss: 4.2648835e-06
Iter: 1058 loss: 4.26483621e-06
Iter: 1059 loss: 4.26327369e-06
Iter: 1060 loss: 4.26111455e-06
Iter: 1061 loss: 4.26104907e-06
Iter: 1062 loss: 4.25861163e-06
Iter: 1063 loss: 4.29280271e-06
Iter: 1064 loss: 4.25859707e-06
Iter: 1065 loss: 4.25658936e-06
Iter: 1066 loss: 4.25870076e-06
Iter: 1067 loss: 4.25553571e-06
Iter: 1068 loss: 4.25306553e-06
Iter: 1069 loss: 4.26024599e-06
Iter: 1070 loss: 4.25237295e-06
Iter: 1071 loss: 4.25050348e-06
Iter: 1072 loss: 4.25374492e-06
Iter: 1073 loss: 4.24978316e-06
Iter: 1074 loss: 4.24735208e-06
Iter: 1075 loss: 4.25437383e-06
Iter: 1076 loss: 4.24662e-06
Iter: 1077 loss: 4.24489144e-06
Iter: 1078 loss: 4.2472675e-06
Iter: 1079 loss: 4.24409791e-06
Iter: 1080 loss: 4.24138852e-06
Iter: 1081 loss: 4.24207474e-06
Iter: 1082 loss: 4.23929123e-06
Iter: 1083 loss: 4.23675829e-06
Iter: 1084 loss: 4.25234293e-06
Iter: 1085 loss: 4.23646634e-06
Iter: 1086 loss: 4.23620349e-06
Iter: 1087 loss: 4.23558595e-06
Iter: 1088 loss: 4.23469464e-06
Iter: 1089 loss: 4.23310121e-06
Iter: 1090 loss: 4.23307893e-06
Iter: 1091 loss: 4.23188476e-06
Iter: 1092 loss: 4.23056827e-06
Iter: 1093 loss: 4.23027905e-06
Iter: 1094 loss: 4.22810444e-06
Iter: 1095 loss: 4.23783513e-06
Iter: 1096 loss: 4.22771e-06
Iter: 1097 loss: 4.22562698e-06
Iter: 1098 loss: 4.2253796e-06
Iter: 1099 loss: 4.22384392e-06
Iter: 1100 loss: 4.22168205e-06
Iter: 1101 loss: 4.25323515e-06
Iter: 1102 loss: 4.22170797e-06
Iter: 1103 loss: 4.22001631e-06
Iter: 1104 loss: 4.21849109e-06
Iter: 1105 loss: 4.21799e-06
Iter: 1106 loss: 4.21588811e-06
Iter: 1107 loss: 4.21587538e-06
Iter: 1108 loss: 4.21475261e-06
Iter: 1109 loss: 4.21299546e-06
Iter: 1110 loss: 4.2129559e-06
Iter: 1111 loss: 4.2100437e-06
Iter: 1112 loss: 4.21778714e-06
Iter: 1113 loss: 4.2090428e-06
Iter: 1114 loss: 4.20687502e-06
Iter: 1115 loss: 4.20880497e-06
Iter: 1116 loss: 4.20564538e-06
Iter: 1117 loss: 4.20328797e-06
Iter: 1118 loss: 4.23179927e-06
Iter: 1119 loss: 4.20325341e-06
Iter: 1120 loss: 4.20329934e-06
Iter: 1121 loss: 4.20275092e-06
Iter: 1122 loss: 4.20211063e-06
Iter: 1123 loss: 4.20044216e-06
Iter: 1124 loss: 4.20228344e-06
Iter: 1125 loss: 4.19910066e-06
Iter: 1126 loss: 4.19704156e-06
Iter: 1127 loss: 4.20962169e-06
Iter: 1128 loss: 4.19676689e-06
Iter: 1129 loss: 4.19471417e-06
Iter: 1130 loss: 4.19392609e-06
Iter: 1131 loss: 4.19279559e-06
Iter: 1132 loss: 4.19030039e-06
Iter: 1133 loss: 4.22238645e-06
Iter: 1134 loss: 4.19032494e-06
Iter: 1135 loss: 4.18874424e-06
Iter: 1136 loss: 4.18822628e-06
Iter: 1137 loss: 4.1873077e-06
Iter: 1138 loss: 4.18500531e-06
Iter: 1139 loss: 4.20593688e-06
Iter: 1140 loss: 4.18488162e-06
Iter: 1141 loss: 4.18353602e-06
Iter: 1142 loss: 4.18512127e-06
Iter: 1143 loss: 4.18275e-06
Iter: 1144 loss: 4.18070931e-06
Iter: 1145 loss: 4.18574e-06
Iter: 1146 loss: 4.18002674e-06
Iter: 1147 loss: 4.17809679e-06
Iter: 1148 loss: 4.17978299e-06
Iter: 1149 loss: 4.17707724e-06
Iter: 1150 loss: 4.17468164e-06
Iter: 1151 loss: 4.18403943e-06
Iter: 1152 loss: 4.17405818e-06
Iter: 1153 loss: 4.1721064e-06
Iter: 1154 loss: 4.180019e-06
Iter: 1155 loss: 4.17179535e-06
Iter: 1156 loss: 4.17059709e-06
Iter: 1157 loss: 4.17044475e-06
Iter: 1158 loss: 4.16963621e-06
Iter: 1159 loss: 4.16754665e-06
Iter: 1160 loss: 4.18779291e-06
Iter: 1161 loss: 4.16737475e-06
Iter: 1162 loss: 4.16550847e-06
Iter: 1163 loss: 4.164247e-06
Iter: 1164 loss: 4.16355715e-06
Iter: 1165 loss: 4.16119838e-06
Iter: 1166 loss: 4.19371736e-06
Iter: 1167 loss: 4.16115745e-06
Iter: 1168 loss: 4.15980412e-06
Iter: 1169 loss: 4.15773593e-06
Iter: 1170 loss: 4.1576759e-06
Iter: 1171 loss: 4.15512113e-06
Iter: 1172 loss: 4.19016214e-06
Iter: 1173 loss: 4.1551657e-06
Iter: 1174 loss: 4.15369777e-06
Iter: 1175 loss: 4.15765e-06
Iter: 1176 loss: 4.15304885e-06
Iter: 1177 loss: 4.15130398e-06
Iter: 1178 loss: 4.15341492e-06
Iter: 1179 loss: 4.15036311e-06
Iter: 1180 loss: 4.1485564e-06
Iter: 1181 loss: 4.15516388e-06
Iter: 1182 loss: 4.14811075e-06
Iter: 1183 loss: 4.14593887e-06
Iter: 1184 loss: 4.1456924e-06
Iter: 1185 loss: 4.1442313e-06
Iter: 1186 loss: 4.14211354e-06
Iter: 1187 loss: 4.15970317e-06
Iter: 1188 loss: 4.14197075e-06
Iter: 1189 loss: 4.14038095e-06
Iter: 1190 loss: 4.15396789e-06
Iter: 1191 loss: 4.14037822e-06
Iter: 1192 loss: 4.13854e-06
Iter: 1193 loss: 4.14225178e-06
Iter: 1194 loss: 4.13782345e-06
Iter: 1195 loss: 4.13683711e-06
Iter: 1196 loss: 4.13540602e-06
Iter: 1197 loss: 4.13537782e-06
Iter: 1198 loss: 4.13342468e-06
Iter: 1199 loss: 4.13352791e-06
Iter: 1200 loss: 4.13189355e-06
Iter: 1201 loss: 4.12968893e-06
Iter: 1202 loss: 4.13951784e-06
Iter: 1203 loss: 4.12925601e-06
Iter: 1204 loss: 4.12707914e-06
Iter: 1205 loss: 4.13501766e-06
Iter: 1206 loss: 4.1264575e-06
Iter: 1207 loss: 4.12444069e-06
Iter: 1208 loss: 4.1227e-06
Iter: 1209 loss: 4.12222e-06
Iter: 1210 loss: 4.12074496e-06
Iter: 1211 loss: 4.12041754e-06
Iter: 1212 loss: 4.11909332e-06
Iter: 1213 loss: 4.11803831e-06
Iter: 1214 loss: 4.11772953e-06
Iter: 1215 loss: 4.11539622e-06
Iter: 1216 loss: 4.12990175e-06
Iter: 1217 loss: 4.11525889e-06
Iter: 1218 loss: 4.11400924e-06
Iter: 1219 loss: 4.11571045e-06
Iter: 1220 loss: 4.11336896e-06
Iter: 1221 loss: 4.11152e-06
Iter: 1222 loss: 4.11499923e-06
Iter: 1223 loss: 4.11080373e-06
Iter: 1224 loss: 4.1104e-06
Iter: 1225 loss: 4.10990651e-06
Iter: 1226 loss: 4.10931034e-06
Iter: 1227 loss: 4.10786106e-06
Iter: 1228 loss: 4.12111785e-06
Iter: 1229 loss: 4.10776647e-06
Iter: 1230 loss: 4.10610892e-06
Iter: 1231 loss: 4.10584562e-06
Iter: 1232 loss: 4.1046751e-06
Iter: 1233 loss: 4.10267512e-06
Iter: 1234 loss: 4.11276051e-06
Iter: 1235 loss: 4.1023668e-06
Iter: 1236 loss: 4.10038092e-06
Iter: 1237 loss: 4.10160465e-06
Iter: 1238 loss: 4.09907079e-06
Iter: 1239 loss: 4.09701397e-06
Iter: 1240 loss: 4.10800385e-06
Iter: 1241 loss: 4.0967e-06
Iter: 1242 loss: 4.09474524e-06
Iter: 1243 loss: 4.09694076e-06
Iter: 1244 loss: 4.09373843e-06
Iter: 1245 loss: 4.09191762e-06
Iter: 1246 loss: 4.10344182e-06
Iter: 1247 loss: 4.09183122e-06
Iter: 1248 loss: 4.09004588e-06
Iter: 1249 loss: 4.09487848e-06
Iter: 1250 loss: 4.08949109e-06
Iter: 1251 loss: 4.08791084e-06
Iter: 1252 loss: 4.09303493e-06
Iter: 1253 loss: 4.08753249e-06
Iter: 1254 loss: 4.08596861e-06
Iter: 1255 loss: 4.08695496e-06
Iter: 1256 loss: 4.08504138e-06
Iter: 1257 loss: 4.08528513e-06
Iter: 1258 loss: 4.08433652e-06
Iter: 1259 loss: 4.08372534e-06
Iter: 1260 loss: 4.08269443e-06
Iter: 1261 loss: 4.08272354e-06
Iter: 1262 loss: 4.08128926e-06
Iter: 1263 loss: 4.07998505e-06
Iter: 1264 loss: 4.07961898e-06
Iter: 1265 loss: 4.07814878e-06
Iter: 1266 loss: 4.08196865e-06
Iter: 1267 loss: 4.07746e-06
Iter: 1268 loss: 4.07537937e-06
Iter: 1269 loss: 4.08032747e-06
Iter: 1270 loss: 4.07464177e-06
Iter: 1271 loss: 4.07300286e-06
Iter: 1272 loss: 4.07816151e-06
Iter: 1273 loss: 4.07246898e-06
Iter: 1274 loss: 4.07062453e-06
Iter: 1275 loss: 4.07236394e-06
Iter: 1276 loss: 4.06949221e-06
Iter: 1277 loss: 4.06771869e-06
Iter: 1278 loss: 4.07584866e-06
Iter: 1279 loss: 4.06745357e-06
Iter: 1280 loss: 4.06550089e-06
Iter: 1281 loss: 4.0720297e-06
Iter: 1282 loss: 4.06508116e-06
Iter: 1283 loss: 4.06345589e-06
Iter: 1284 loss: 4.07006701e-06
Iter: 1285 loss: 4.06319032e-06
Iter: 1286 loss: 4.06160143e-06
Iter: 1287 loss: 4.06310528e-06
Iter: 1288 loss: 4.06067193e-06
Iter: 1289 loss: 4.05963e-06
Iter: 1290 loss: 4.05960236e-06
Iter: 1291 loss: 4.05852415e-06
Iter: 1292 loss: 4.06158324e-06
Iter: 1293 loss: 4.05822311e-06
Iter: 1294 loss: 4.05731589e-06
Iter: 1295 loss: 4.05582341e-06
Iter: 1296 loss: 4.05587616e-06
Iter: 1297 loss: 4.05448463e-06
Iter: 1298 loss: 4.05947958e-06
Iter: 1299 loss: 4.05412629e-06
Iter: 1300 loss: 4.05267201e-06
Iter: 1301 loss: 4.05205174e-06
Iter: 1302 loss: 4.05127912e-06
Iter: 1303 loss: 4.04945149e-06
Iter: 1304 loss: 4.06297613e-06
Iter: 1305 loss: 4.04932052e-06
Iter: 1306 loss: 4.04755701e-06
Iter: 1307 loss: 4.04857246e-06
Iter: 1308 loss: 4.0464115e-06
Iter: 1309 loss: 4.04473212e-06
Iter: 1310 loss: 4.05086348e-06
Iter: 1311 loss: 4.04434559e-06
Iter: 1312 loss: 4.04253e-06
Iter: 1313 loss: 4.04555112e-06
Iter: 1314 loss: 4.04167622e-06
Iter: 1315 loss: 4.04012e-06
Iter: 1316 loss: 4.0585337e-06
Iter: 1317 loss: 4.04008688e-06
Iter: 1318 loss: 4.03877948e-06
Iter: 1319 loss: 4.03795639e-06
Iter: 1320 loss: 4.03748618e-06
Iter: 1321 loss: 4.03570857e-06
Iter: 1322 loss: 4.05346327e-06
Iter: 1323 loss: 4.03562944e-06
Iter: 1324 loss: 4.03550303e-06
Iter: 1325 loss: 4.0351224e-06
Iter: 1326 loss: 4.0347868e-06
Iter: 1327 loss: 4.03349077e-06
Iter: 1328 loss: 4.03693866e-06
Iter: 1329 loss: 4.03273953e-06
Iter: 1330 loss: 4.03122067e-06
Iter: 1331 loss: 4.0380819e-06
Iter: 1332 loss: 4.03090962e-06
Iter: 1333 loss: 4.0291884e-06
Iter: 1334 loss: 4.0292125e-06
Iter: 1335 loss: 4.02776777e-06
Iter: 1336 loss: 4.02600108e-06
Iter: 1337 loss: 4.03574359e-06
Iter: 1338 loss: 4.02569094e-06
Iter: 1339 loss: 4.02382921e-06
Iter: 1340 loss: 4.0249e-06
Iter: 1341 loss: 4.0225591e-06
Iter: 1342 loss: 4.02100159e-06
Iter: 1343 loss: 4.03452259e-06
Iter: 1344 loss: 4.02088153e-06
Iter: 1345 loss: 4.01942725e-06
Iter: 1346 loss: 4.01961734e-06
Iter: 1347 loss: 4.01829129e-06
Iter: 1348 loss: 4.01648413e-06
Iter: 1349 loss: 4.02607839e-06
Iter: 1350 loss: 4.01620946e-06
Iter: 1351 loss: 4.01458601e-06
Iter: 1352 loss: 4.02122487e-06
Iter: 1353 loss: 4.01421858e-06
Iter: 1354 loss: 4.01300031e-06
Iter: 1355 loss: 4.02147407e-06
Iter: 1356 loss: 4.01282159e-06
Iter: 1357 loss: 4.01210627e-06
Iter: 1358 loss: 4.02052456e-06
Iter: 1359 loss: 4.01212492e-06
Iter: 1360 loss: 4.01125271e-06
Iter: 1361 loss: 4.01042598e-06
Iter: 1362 loss: 4.01018133e-06
Iter: 1363 loss: 4.0092209e-06
Iter: 1364 loss: 4.00922318e-06
Iter: 1365 loss: 4.00844601e-06
Iter: 1366 loss: 4.00720637e-06
Iter: 1367 loss: 4.00649333e-06
Iter: 1368 loss: 4.00595172e-06
Iter: 1369 loss: 4.00419685e-06
Iter: 1370 loss: 4.02513433e-06
Iter: 1371 loss: 4.0042205e-06
Iter: 1372 loss: 4.0031814e-06
Iter: 1373 loss: 4.00258705e-06
Iter: 1374 loss: 4.00218e-06
Iter: 1375 loss: 4.00063e-06
Iter: 1376 loss: 4.01045327e-06
Iter: 1377 loss: 4.00043746e-06
Iter: 1378 loss: 3.99900455e-06
Iter: 1379 loss: 3.99862165e-06
Iter: 1380 loss: 3.99772489e-06
Iter: 1381 loss: 3.99605733e-06
Iter: 1382 loss: 4.00827503e-06
Iter: 1383 loss: 3.99594956e-06
Iter: 1384 loss: 3.9943825e-06
Iter: 1385 loss: 3.9966144e-06
Iter: 1386 loss: 3.99369219e-06
Iter: 1387 loss: 3.9923334e-06
Iter: 1388 loss: 4.00452518e-06
Iter: 1389 loss: 3.99219971e-06
Iter: 1390 loss: 3.9912411e-06
Iter: 1391 loss: 3.99728924e-06
Iter: 1392 loss: 3.99113878e-06
Iter: 1393 loss: 3.99011333e-06
Iter: 1394 loss: 3.99611508e-06
Iter: 1395 loss: 3.99005194e-06
Iter: 1396 loss: 3.98952761e-06
Iter: 1397 loss: 3.9886645e-06
Iter: 1398 loss: 3.98860675e-06
Iter: 1399 loss: 3.98758857e-06
Iter: 1400 loss: 3.98635802e-06
Iter: 1401 loss: 3.98625525e-06
Iter: 1402 loss: 3.98453767e-06
Iter: 1403 loss: 4.00011322e-06
Iter: 1404 loss: 3.98444581e-06
Iter: 1405 loss: 3.98317161e-06
Iter: 1406 loss: 3.98324528e-06
Iter: 1407 loss: 3.98216525e-06
Iter: 1408 loss: 3.98043358e-06
Iter: 1409 loss: 3.99153578e-06
Iter: 1410 loss: 3.98028351e-06
Iter: 1411 loss: 3.97884e-06
Iter: 1412 loss: 3.9786305e-06
Iter: 1413 loss: 3.97770691e-06
Iter: 1414 loss: 3.97592294e-06
Iter: 1415 loss: 3.98985321e-06
Iter: 1416 loss: 3.97571284e-06
Iter: 1417 loss: 3.97445183e-06
Iter: 1418 loss: 3.97463145e-06
Iter: 1419 loss: 3.97350414e-06
Iter: 1420 loss: 3.972e-06
Iter: 1421 loss: 3.98866177e-06
Iter: 1422 loss: 3.97196482e-06
Iter: 1423 loss: 3.9708616e-06
Iter: 1424 loss: 3.97212762e-06
Iter: 1425 loss: 3.97023041e-06
Iter: 1426 loss: 3.96951646e-06
Iter: 1427 loss: 3.96941596e-06
Iter: 1428 loss: 3.9688066e-06
Iter: 1429 loss: 3.96774294e-06
Iter: 1430 loss: 3.98638849e-06
Iter: 1431 loss: 3.96771611e-06
Iter: 1432 loss: 3.96652831e-06
Iter: 1433 loss: 3.96659061e-06
Iter: 1434 loss: 3.96569703e-06
Iter: 1435 loss: 3.96407358e-06
Iter: 1436 loss: 3.97020312e-06
Iter: 1437 loss: 3.96361156e-06
Iter: 1438 loss: 3.96214273e-06
Iter: 1439 loss: 3.96242604e-06
Iter: 1440 loss: 3.96109954e-06
Iter: 1441 loss: 3.95945699e-06
Iter: 1442 loss: 3.97844633e-06
Iter: 1443 loss: 3.95937604e-06
Iter: 1444 loss: 3.95827647e-06
Iter: 1445 loss: 3.95688539e-06
Iter: 1446 loss: 3.95667121e-06
Iter: 1447 loss: 3.95452025e-06
Iter: 1448 loss: 3.97400345e-06
Iter: 1449 loss: 3.95435973e-06
Iter: 1450 loss: 3.95320239e-06
Iter: 1451 loss: 3.95208e-06
Iter: 1452 loss: 3.95174902e-06
Iter: 1453 loss: 3.95011057e-06
Iter: 1454 loss: 3.97234362e-06
Iter: 1455 loss: 3.95009465e-06
Iter: 1456 loss: 3.94895278e-06
Iter: 1457 loss: 3.94915696e-06
Iter: 1458 loss: 3.94798326e-06
Iter: 1459 loss: 3.94810195e-06
Iter: 1460 loss: 3.94744075e-06
Iter: 1461 loss: 3.94669496e-06
Iter: 1462 loss: 3.94619838e-06
Iter: 1463 loss: 3.94608651e-06
Iter: 1464 loss: 3.94508606e-06
Iter: 1465 loss: 3.94475455e-06
Iter: 1466 loss: 3.94424796e-06
Iter: 1467 loss: 3.94317112e-06
Iter: 1468 loss: 3.94696826e-06
Iter: 1469 loss: 3.94279596e-06
Iter: 1470 loss: 3.94161e-06
Iter: 1471 loss: 3.94023391e-06
Iter: 1472 loss: 3.94006111e-06
Iter: 1473 loss: 3.9382785e-06
Iter: 1474 loss: 3.95864345e-06
Iter: 1475 loss: 3.93824075e-06
Iter: 1476 loss: 3.93703294e-06
Iter: 1477 loss: 3.93719802e-06
Iter: 1478 loss: 3.93616e-06
Iter: 1479 loss: 3.93473192e-06
Iter: 1480 loss: 3.94668768e-06
Iter: 1481 loss: 3.93455139e-06
Iter: 1482 loss: 3.93301252e-06
Iter: 1483 loss: 3.93387108e-06
Iter: 1484 loss: 3.93212531e-06
Iter: 1485 loss: 3.93072696e-06
Iter: 1486 loss: 3.94204881e-06
Iter: 1487 loss: 3.93060736e-06
Iter: 1488 loss: 3.9295096e-06
Iter: 1489 loss: 3.92750644e-06
Iter: 1490 loss: 3.92745915e-06
Iter: 1491 loss: 3.92694619e-06
Iter: 1492 loss: 3.92646416e-06
Iter: 1493 loss: 3.9256638e-06
Iter: 1494 loss: 3.93542405e-06
Iter: 1495 loss: 3.92568109e-06
Iter: 1496 loss: 3.92516313e-06
Iter: 1497 loss: 3.92385e-06
Iter: 1498 loss: 3.93418441e-06
Iter: 1499 loss: 3.92359652e-06
Iter: 1500 loss: 3.92221727e-06
Iter: 1501 loss: 3.928551e-06
Iter: 1502 loss: 3.92201e-06
Iter: 1503 loss: 3.92065431e-06
Iter: 1504 loss: 3.9228853e-06
Iter: 1505 loss: 3.92008724e-06
Iter: 1506 loss: 3.91884669e-06
Iter: 1507 loss: 3.92236961e-06
Iter: 1508 loss: 3.91834055e-06
Iter: 1509 loss: 3.91706908e-06
Iter: 1510 loss: 3.91789081e-06
Iter: 1511 loss: 3.91622734e-06
Iter: 1512 loss: 3.91514277e-06
Iter: 1513 loss: 3.92347e-06
Iter: 1514 loss: 3.91503181e-06
Iter: 1515 loss: 3.91375579e-06
Iter: 1516 loss: 3.91326e-06
Iter: 1517 loss: 3.91247931e-06
Iter: 1518 loss: 3.91107369e-06
Iter: 1519 loss: 3.92007678e-06
Iter: 1520 loss: 3.91080175e-06
Iter: 1521 loss: 3.90940659e-06
Iter: 1522 loss: 3.9119227e-06
Iter: 1523 loss: 3.90871264e-06
Iter: 1524 loss: 3.90741661e-06
Iter: 1525 loss: 3.91578442e-06
Iter: 1526 loss: 3.90721925e-06
Iter: 1527 loss: 3.90626701e-06
Iter: 1528 loss: 3.91094545e-06
Iter: 1529 loss: 3.90599826e-06
Iter: 1530 loss: 3.90471359e-06
Iter: 1531 loss: 3.91563435e-06
Iter: 1532 loss: 3.90467085e-06
Iter: 1533 loss: 3.90427886e-06
Iter: 1534 loss: 3.90304285e-06
Iter: 1535 loss: 3.90823516e-06
Iter: 1536 loss: 3.90262312e-06
Iter: 1537 loss: 3.90076684e-06
Iter: 1538 loss: 3.90414152e-06
Iter: 1539 loss: 3.89969864e-06
Iter: 1540 loss: 3.89815159e-06
Iter: 1541 loss: 3.91065714e-06
Iter: 1542 loss: 3.8979806e-06
Iter: 1543 loss: 3.89652723e-06
Iter: 1544 loss: 3.89767865e-06
Iter: 1545 loss: 3.89565776e-06
Iter: 1546 loss: 3.8940716e-06
Iter: 1547 loss: 3.90758623e-06
Iter: 1548 loss: 3.89397701e-06
Iter: 1549 loss: 3.89287743e-06
Iter: 1550 loss: 3.8912226e-06
Iter: 1551 loss: 3.89118895e-06
Iter: 1552 loss: 3.8891676e-06
Iter: 1553 loss: 3.89847628e-06
Iter: 1554 loss: 3.88872786e-06
Iter: 1555 loss: 3.8871126e-06
Iter: 1556 loss: 3.89858451e-06
Iter: 1557 loss: 3.88694843e-06
Iter: 1558 loss: 3.88561875e-06
Iter: 1559 loss: 3.88615445e-06
Iter: 1560 loss: 3.88482295e-06
Iter: 1561 loss: 3.88280205e-06
Iter: 1562 loss: 3.89073466e-06
Iter: 1563 loss: 3.88246372e-06
Iter: 1564 loss: 3.88315402e-06
Iter: 1565 loss: 3.8819303e-06
Iter: 1566 loss: 3.88153057e-06
Iter: 1567 loss: 3.88017861e-06
Iter: 1568 loss: 3.88472108e-06
Iter: 1569 loss: 3.87969203e-06
Iter: 1570 loss: 3.87789805e-06
Iter: 1571 loss: 3.88368153e-06
Iter: 1572 loss: 3.87741511e-06
Iter: 1573 loss: 3.87617729e-06
Iter: 1574 loss: 3.87720047e-06
Iter: 1575 loss: 3.87529553e-06
Iter: 1576 loss: 3.87359296e-06
Iter: 1577 loss: 3.88051239e-06
Iter: 1578 loss: 3.87303407e-06
Iter: 1579 loss: 3.87182126e-06
Iter: 1580 loss: 3.87739965e-06
Iter: 1581 loss: 3.87154341e-06
Iter: 1582 loss: 3.87019554e-06
Iter: 1583 loss: 3.87088676e-06
Iter: 1584 loss: 3.86935744e-06
Iter: 1585 loss: 3.86789225e-06
Iter: 1586 loss: 3.87010823e-06
Iter: 1587 loss: 3.86720603e-06
Iter: 1588 loss: 3.86570355e-06
Iter: 1589 loss: 3.88224407e-06
Iter: 1590 loss: 3.86568126e-06
Iter: 1591 loss: 3.86457077e-06
Iter: 1592 loss: 3.86690454e-06
Iter: 1593 loss: 3.86426746e-06
Iter: 1594 loss: 3.86314696e-06
Iter: 1595 loss: 3.8635e-06
Iter: 1596 loss: 3.86240117e-06
Iter: 1597 loss: 3.8613216e-06
Iter: 1598 loss: 3.86686224e-06
Iter: 1599 loss: 3.86104466e-06
Iter: 1600 loss: 3.8607277e-06
Iter: 1601 loss: 3.86047577e-06
Iter: 1602 loss: 3.86010197e-06
Iter: 1603 loss: 3.85898e-06
Iter: 1604 loss: 3.8647e-06
Iter: 1605 loss: 3.8585822e-06
Iter: 1606 loss: 3.85727435e-06
Iter: 1607 loss: 3.8568437e-06
Iter: 1608 loss: 3.85604426e-06
Iter: 1609 loss: 3.85444764e-06
Iter: 1610 loss: 3.87676027e-06
Iter: 1611 loss: 3.85445855e-06
Iter: 1612 loss: 3.85351905e-06
Iter: 1613 loss: 3.85268731e-06
Iter: 1614 loss: 3.85244766e-06
Iter: 1615 loss: 3.85094972e-06
Iter: 1616 loss: 3.86230658e-06
Iter: 1617 loss: 3.85077237e-06
Iter: 1618 loss: 3.84940085e-06
Iter: 1619 loss: 3.84880468e-06
Iter: 1620 loss: 3.84822488e-06
Iter: 1621 loss: 3.84666146e-06
Iter: 1622 loss: 3.86517559e-06
Iter: 1623 loss: 3.84658142e-06
Iter: 1624 loss: 3.84546365e-06
Iter: 1625 loss: 3.84660871e-06
Iter: 1626 loss: 3.84484292e-06
Iter: 1627 loss: 3.84333725e-06
Iter: 1628 loss: 3.85078783e-06
Iter: 1629 loss: 3.84313262e-06
Iter: 1630 loss: 3.84199757e-06
Iter: 1631 loss: 3.84279883e-06
Iter: 1632 loss: 3.84128907e-06
Iter: 1633 loss: 3.84058103e-06
Iter: 1634 loss: 3.84059513e-06
Iter: 1635 loss: 3.83972201e-06
Iter: 1636 loss: 3.84001078e-06
Iter: 1637 loss: 3.83911174e-06
Iter: 1638 loss: 3.83842962e-06
Iter: 1639 loss: 3.83712268e-06
Iter: 1640 loss: 3.87021555e-06
Iter: 1641 loss: 3.83712904e-06
Iter: 1642 loss: 3.8353819e-06
Iter: 1643 loss: 3.83901e-06
Iter: 1644 loss: 3.83474253e-06
Iter: 1645 loss: 3.83320867e-06
Iter: 1646 loss: 3.83440693e-06
Iter: 1647 loss: 3.83234055e-06
Iter: 1648 loss: 3.83053839e-06
Iter: 1649 loss: 3.84496889e-06
Iter: 1650 loss: 3.83045062e-06
Iter: 1651 loss: 3.8289254e-06
Iter: 1652 loss: 3.82863345e-06
Iter: 1653 loss: 3.82770304e-06
Iter: 1654 loss: 3.82569033e-06
Iter: 1655 loss: 3.84075474e-06
Iter: 1656 loss: 3.82551025e-06
Iter: 1657 loss: 3.82406688e-06
Iter: 1658 loss: 3.82424969e-06
Iter: 1659 loss: 3.82296685e-06
Iter: 1660 loss: 3.82122789e-06
Iter: 1661 loss: 3.83696124e-06
Iter: 1662 loss: 3.82110284e-06
Iter: 1663 loss: 3.81960945e-06
Iter: 1664 loss: 3.82060489e-06
Iter: 1665 loss: 3.81875179e-06
Iter: 1666 loss: 3.81707423e-06
Iter: 1667 loss: 3.83409906e-06
Iter: 1668 loss: 3.81697737e-06
Iter: 1669 loss: 3.81623613e-06
Iter: 1670 loss: 3.81621339e-06
Iter: 1671 loss: 3.81571454e-06
Iter: 1672 loss: 3.8144467e-06
Iter: 1673 loss: 3.81943482e-06
Iter: 1674 loss: 3.81394057e-06
Iter: 1675 loss: 3.81188556e-06
Iter: 1676 loss: 3.81805194e-06
Iter: 1677 loss: 3.81129712e-06
Iter: 1678 loss: 3.80994697e-06
Iter: 1679 loss: 3.81128962e-06
Iter: 1680 loss: 3.80920346e-06
Iter: 1681 loss: 3.80740266e-06
Iter: 1682 loss: 3.81519931e-06
Iter: 1683 loss: 3.80707752e-06
Iter: 1684 loss: 3.80539041e-06
Iter: 1685 loss: 3.80513802e-06
Iter: 1686 loss: 3.80404413e-06
Iter: 1687 loss: 3.80196343e-06
Iter: 1688 loss: 3.81733571e-06
Iter: 1689 loss: 3.80179404e-06
Iter: 1690 loss: 3.80006486e-06
Iter: 1691 loss: 3.80045549e-06
Iter: 1692 loss: 3.79870039e-06
Iter: 1693 loss: 3.79724906e-06
Iter: 1694 loss: 3.79728613e-06
Iter: 1695 loss: 3.79618564e-06
Iter: 1696 loss: 3.79509629e-06
Iter: 1697 loss: 3.79490075e-06
Iter: 1698 loss: 3.79331777e-06
Iter: 1699 loss: 3.81176642e-06
Iter: 1700 loss: 3.79331186e-06
Iter: 1701 loss: 3.79248104e-06
Iter: 1702 loss: 3.79247126e-06
Iter: 1703 loss: 3.79160451e-06
Iter: 1704 loss: 3.79032053e-06
Iter: 1705 loss: 3.79023186e-06
Iter: 1706 loss: 3.78912637e-06
Iter: 1707 loss: 3.78934737e-06
Iter: 1708 loss: 3.78834989e-06
Iter: 1709 loss: 3.78684581e-06
Iter: 1710 loss: 3.7898451e-06
Iter: 1711 loss: 3.78622531e-06
Iter: 1712 loss: 3.78480445e-06
Iter: 1713 loss: 3.79137259e-06
Iter: 1714 loss: 3.7845341e-06
Iter: 1715 loss: 3.78341883e-06
Iter: 1716 loss: 3.78253594e-06
Iter: 1717 loss: 3.7821585e-06
Iter: 1718 loss: 3.78023151e-06
Iter: 1719 loss: 3.79315679e-06
Iter: 1720 loss: 3.77995502e-06
Iter: 1721 loss: 3.77822289e-06
Iter: 1722 loss: 3.77795686e-06
Iter: 1723 loss: 3.77680431e-06
Iter: 1724 loss: 3.77475158e-06
Iter: 1725 loss: 3.79211087e-06
Iter: 1726 loss: 3.77467586e-06
Iter: 1727 loss: 3.77316837e-06
Iter: 1728 loss: 3.77578363e-06
Iter: 1729 loss: 3.77252854e-06
Iter: 1730 loss: 3.77104743e-06
Iter: 1731 loss: 3.77983679e-06
Iter: 1732 loss: 3.77083234e-06
Iter: 1733 loss: 3.76934076e-06
Iter: 1734 loss: 3.77551919e-06
Iter: 1735 loss: 3.76909907e-06
Iter: 1736 loss: 3.76746766e-06
Iter: 1737 loss: 3.77814058e-06
Iter: 1738 loss: 3.76722187e-06
Iter: 1739 loss: 3.76648518e-06
Iter: 1740 loss: 3.76581829e-06
Iter: 1741 loss: 3.76568e-06
Iter: 1742 loss: 3.76449179e-06
Iter: 1743 loss: 3.76337084e-06
Iter: 1744 loss: 3.76311755e-06
Iter: 1745 loss: 3.76131948e-06
Iter: 1746 loss: 3.77357173e-06
Iter: 1747 loss: 3.76122352e-06
Iter: 1748 loss: 3.76003845e-06
Iter: 1749 loss: 3.76037087e-06
Iter: 1750 loss: 3.75924969e-06
Iter: 1751 loss: 3.75778745e-06
Iter: 1752 loss: 3.76828757e-06
Iter: 1753 loss: 3.75763102e-06
Iter: 1754 loss: 3.75634795e-06
Iter: 1755 loss: 3.75446803e-06
Iter: 1756 loss: 3.75439822e-06
Iter: 1757 loss: 3.7520681e-06
Iter: 1758 loss: 3.77385686e-06
Iter: 1759 loss: 3.75199443e-06
Iter: 1760 loss: 3.75032391e-06
Iter: 1761 loss: 3.75128866e-06
Iter: 1762 loss: 3.74924184e-06
Iter: 1763 loss: 3.74754109e-06
Iter: 1764 loss: 3.76143862e-06
Iter: 1765 loss: 3.74751539e-06
Iter: 1766 loss: 3.74612864e-06
Iter: 1767 loss: 3.74762249e-06
Iter: 1768 loss: 3.74529714e-06
Iter: 1769 loss: 3.74481715e-06
Iter: 1770 loss: 3.7443383e-06
Iter: 1771 loss: 3.74376896e-06
Iter: 1772 loss: 3.74245201e-06
Iter: 1773 loss: 3.76324e-06
Iter: 1774 loss: 3.74249248e-06
Iter: 1775 loss: 3.74109368e-06
Iter: 1776 loss: 3.74197384e-06
Iter: 1777 loss: 3.74025035e-06
Iter: 1778 loss: 3.73866487e-06
Iter: 1779 loss: 3.74343e-06
Iter: 1780 loss: 3.73800958e-06
Iter: 1781 loss: 3.73659623e-06
Iter: 1782 loss: 3.73635885e-06
Iter: 1783 loss: 3.73526359e-06
Iter: 1784 loss: 3.733732e-06
Iter: 1785 loss: 3.75906779e-06
Iter: 1786 loss: 3.73371677e-06
Iter: 1787 loss: 3.73249463e-06
Iter: 1788 loss: 3.73082094e-06
Iter: 1789 loss: 3.73079138e-06
Iter: 1790 loss: 3.72877639e-06
Iter: 1791 loss: 3.74727529e-06
Iter: 1792 loss: 3.72870386e-06
Iter: 1793 loss: 3.72698605e-06
Iter: 1794 loss: 3.72645286e-06
Iter: 1795 loss: 3.72549152e-06
Iter: 1796 loss: 3.72314162e-06
Iter: 1797 loss: 3.74606952e-06
Iter: 1798 loss: 3.72300292e-06
Iter: 1799 loss: 3.72154454e-06
Iter: 1800 loss: 3.72019349e-06
Iter: 1801 loss: 3.71976944e-06
Iter: 1802 loss: 3.72118438e-06
Iter: 1803 loss: 3.71907731e-06
Iter: 1804 loss: 3.71848091e-06
Iter: 1805 loss: 3.71721626e-06
Iter: 1806 loss: 3.71721444e-06
Iter: 1807 loss: 3.71584292e-06
Iter: 1808 loss: 3.71501255e-06
Iter: 1809 loss: 3.71453189e-06
Iter: 1810 loss: 3.71277247e-06
Iter: 1811 loss: 3.7280106e-06
Iter: 1812 loss: 3.71272654e-06
Iter: 1813 loss: 3.71160604e-06
Iter: 1814 loss: 3.70980342e-06
Iter: 1815 loss: 3.70982616e-06
Iter: 1816 loss: 3.70791099e-06
Iter: 1817 loss: 3.72783643e-06
Iter: 1818 loss: 3.70772887e-06
Iter: 1819 loss: 3.70632165e-06
Iter: 1820 loss: 3.70638486e-06
Iter: 1821 loss: 3.70510975e-06
Iter: 1822 loss: 3.70326143e-06
Iter: 1823 loss: 3.71763394e-06
Iter: 1824 loss: 3.70324256e-06
Iter: 1825 loss: 3.70159205e-06
Iter: 1826 loss: 3.70145517e-06
Iter: 1827 loss: 3.70022281e-06
Iter: 1828 loss: 3.69844065e-06
Iter: 1829 loss: 3.71188708e-06
Iter: 1830 loss: 3.69821782e-06
Iter: 1831 loss: 3.69661666e-06
Iter: 1832 loss: 3.69609711e-06
Iter: 1833 loss: 3.69511235e-06
Iter: 1834 loss: 3.69454392e-06
Iter: 1835 loss: 3.69415261e-06
Iter: 1836 loss: 3.69309441e-06
Iter: 1837 loss: 3.69625741e-06
Iter: 1838 loss: 3.69282407e-06
Iter: 1839 loss: 3.69195709e-06
Iter: 1840 loss: 3.69048325e-06
Iter: 1841 loss: 3.7235975e-06
Iter: 1842 loss: 3.6905235e-06
Iter: 1843 loss: 3.68889278e-06
Iter: 1844 loss: 3.69770328e-06
Iter: 1845 loss: 3.68874475e-06
Iter: 1846 loss: 3.68721385e-06
Iter: 1847 loss: 3.68688734e-06
Iter: 1848 loss: 3.68606288e-06
Iter: 1849 loss: 3.6841609e-06
Iter: 1850 loss: 3.69850977e-06
Iter: 1851 loss: 3.68398787e-06
Iter: 1852 loss: 3.68280234e-06
Iter: 1853 loss: 3.68229576e-06
Iter: 1854 loss: 3.68167957e-06
Iter: 1855 loss: 3.67989264e-06
Iter: 1856 loss: 3.68929614e-06
Iter: 1857 loss: 3.67960024e-06
Iter: 1858 loss: 3.67806683e-06
Iter: 1859 loss: 3.68189103e-06
Iter: 1860 loss: 3.67732036e-06
Iter: 1861 loss: 3.67583311e-06
Iter: 1862 loss: 3.68374299e-06
Iter: 1863 loss: 3.67552684e-06
Iter: 1864 loss: 3.67410939e-06
Iter: 1865 loss: 3.67388793e-06
Iter: 1866 loss: 3.67290158e-06
Iter: 1867 loss: 3.67107077e-06
Iter: 1868 loss: 3.68050814e-06
Iter: 1869 loss: 3.67069879e-06
Iter: 1870 loss: 3.6688034e-06
Iter: 1871 loss: 3.66984386e-06
Iter: 1872 loss: 3.66758422e-06
Iter: 1873 loss: 3.66648874e-06
Iter: 1874 loss: 3.66647487e-06
Iter: 1875 loss: 3.66528047e-06
Iter: 1876 loss: 3.67505322e-06
Iter: 1877 loss: 3.66516133e-06
Iter: 1878 loss: 3.66453241e-06
Iter: 1879 loss: 3.66281392e-06
Iter: 1880 loss: 3.67972916e-06
Iter: 1881 loss: 3.6626916e-06
Iter: 1882 loss: 3.66123595e-06
Iter: 1883 loss: 3.66928521e-06
Iter: 1884 loss: 3.66101062e-06
Iter: 1885 loss: 3.65951064e-06
Iter: 1886 loss: 3.65998289e-06
Iter: 1887 loss: 3.65840719e-06
Iter: 1888 loss: 3.65674032e-06
Iter: 1889 loss: 3.66705626e-06
Iter: 1890 loss: 3.65651135e-06
Iter: 1891 loss: 3.6552583e-06
Iter: 1892 loss: 3.65448182e-06
Iter: 1893 loss: 3.65403275e-06
Iter: 1894 loss: 3.65204687e-06
Iter: 1895 loss: 3.66538961e-06
Iter: 1896 loss: 3.65180517e-06
Iter: 1897 loss: 3.65021151e-06
Iter: 1898 loss: 3.65016717e-06
Iter: 1899 loss: 3.64873927e-06
Iter: 1900 loss: 3.64692914e-06
Iter: 1901 loss: 3.66465019e-06
Iter: 1902 loss: 3.6467959e-06
Iter: 1903 loss: 3.64523521e-06
Iter: 1904 loss: 3.6468457e-06
Iter: 1905 loss: 3.64433708e-06
Iter: 1906 loss: 3.64260927e-06
Iter: 1907 loss: 3.64774633e-06
Iter: 1908 loss: 3.64211564e-06
Iter: 1909 loss: 3.64052676e-06
Iter: 1910 loss: 3.65445362e-06
Iter: 1911 loss: 3.64044695e-06
Iter: 1912 loss: 3.63875711e-06
Iter: 1913 loss: 3.64733751e-06
Iter: 1914 loss: 3.63848e-06
Iter: 1915 loss: 3.63779122e-06
Iter: 1916 loss: 3.63654362e-06
Iter: 1917 loss: 3.63653953e-06
Iter: 1918 loss: 3.6350616e-06
Iter: 1919 loss: 3.63531399e-06
Iter: 1920 loss: 3.63394156e-06
Iter: 1921 loss: 3.63255754e-06
Iter: 1922 loss: 3.65476967e-06
Iter: 1923 loss: 3.6325082e-06
Iter: 1924 loss: 3.63154641e-06
Iter: 1925 loss: 3.63050799e-06
Iter: 1926 loss: 3.6303386e-06
Iter: 1927 loss: 3.62851688e-06
Iter: 1928 loss: 3.6374322e-06
Iter: 1929 loss: 3.62820288e-06
Iter: 1930 loss: 3.62678816e-06
Iter: 1931 loss: 3.62733363e-06
Iter: 1932 loss: 3.62585251e-06
Iter: 1933 loss: 3.6239212e-06
Iter: 1934 loss: 3.6349702e-06
Iter: 1935 loss: 3.62368792e-06
Iter: 1936 loss: 3.62250375e-06
Iter: 1937 loss: 3.62425931e-06
Iter: 1938 loss: 3.62196306e-06
Iter: 1939 loss: 3.62021e-06
Iter: 1940 loss: 3.62359538e-06
Iter: 1941 loss: 3.61948423e-06
Iter: 1942 loss: 3.6178219e-06
Iter: 1943 loss: 3.62279502e-06
Iter: 1944 loss: 3.61723369e-06
Iter: 1945 loss: 3.61695356e-06
Iter: 1946 loss: 3.61653883e-06
Iter: 1947 loss: 3.61577463e-06
Iter: 1948 loss: 3.61456296e-06
Iter: 1949 loss: 3.61461161e-06
Iter: 1950 loss: 3.61355342e-06
Iter: 1951 loss: 3.6124593e-06
Iter: 1952 loss: 3.61228831e-06
Iter: 1953 loss: 3.61030743e-06
Iter: 1954 loss: 3.61873253e-06
Iter: 1955 loss: 3.60999911e-06
Iter: 1956 loss: 3.60866943e-06
Iter: 1957 loss: 3.61411685e-06
Iter: 1958 loss: 3.60837885e-06
Iter: 1959 loss: 3.60696049e-06
Iter: 1960 loss: 3.60768e-06
Iter: 1961 loss: 3.60600416e-06
Iter: 1962 loss: 3.60447598e-06
Iter: 1963 loss: 3.6068443e-06
Iter: 1964 loss: 3.60378795e-06
Iter: 1965 loss: 3.60165e-06
Iter: 1966 loss: 3.60762806e-06
Iter: 1967 loss: 3.60104241e-06
Iter: 1968 loss: 3.59959972e-06
Iter: 1969 loss: 3.60554986e-06
Iter: 1970 loss: 3.59916726e-06
Iter: 1971 loss: 3.59745377e-06
Iter: 1972 loss: 3.59893556e-06
Iter: 1973 loss: 3.5964074e-06
Iter: 1974 loss: 3.59472483e-06
Iter: 1975 loss: 3.60057e-06
Iter: 1976 loss: 3.5941946e-06
Iter: 1977 loss: 3.59215028e-06
Iter: 1978 loss: 3.59692285e-06
Iter: 1979 loss: 3.59140381e-06
Iter: 1980 loss: 3.59207297e-06
Iter: 1981 loss: 3.59055275e-06
Iter: 1982 loss: 3.59013734e-06
Iter: 1983 loss: 3.58886246e-06
Iter: 1984 loss: 3.59629689e-06
Iter: 1985 loss: 3.5885032e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3
+ date
Mon Oct 26 10:39:26 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi3_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi3_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi3_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi3_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe70411ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7041c6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7041c6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7041c6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7041151e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe704115598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe704115b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f07a6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f07a6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f07d58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f075a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f0759840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f0759d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f071b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f06c6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f06d30d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f06ac268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f0647b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f06779d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f0604f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f063b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f063b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f05ea378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f0597840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f0597378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f0534ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f05787b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f048d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f048d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f04b0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f04f8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f04651e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f0465158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f0476ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f0439598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6f03dd158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.52072095e-05
Iter: 2 loss: 2.14963438e-05
Iter: 3 loss: 4.7917667e-05
Iter: 4 loss: 2.11685074e-05
Iter: 5 loss: 1.98805265e-05
Iter: 6 loss: 1.9812067e-05
Iter: 7 loss: 1.90842275e-05
Iter: 8 loss: 1.72921973e-05
Iter: 9 loss: 3.50922455e-05
Iter: 10 loss: 1.70665407e-05
Iter: 11 loss: 1.53833553e-05
Iter: 12 loss: 3.17504091e-05
Iter: 13 loss: 1.53225665e-05
Iter: 14 loss: 1.443153e-05
Iter: 15 loss: 1.58042494e-05
Iter: 16 loss: 1.40101038e-05
Iter: 17 loss: 1.29453238e-05
Iter: 18 loss: 1.46858556e-05
Iter: 19 loss: 1.24589151e-05
Iter: 20 loss: 1.18160206e-05
Iter: 21 loss: 1.92312818e-05
Iter: 22 loss: 1.18057796e-05
Iter: 23 loss: 1.12574444e-05
Iter: 24 loss: 1.03525517e-05
Iter: 25 loss: 1.03489228e-05
Iter: 26 loss: 9.8068831e-06
Iter: 27 loss: 9.80254208e-06
Iter: 28 loss: 9.33022238e-06
Iter: 29 loss: 8.80834523e-06
Iter: 30 loss: 8.73354111e-06
Iter: 31 loss: 8.41676501e-06
Iter: 32 loss: 8.37598418e-06
Iter: 33 loss: 8.14575105e-06
Iter: 34 loss: 7.7009081e-06
Iter: 35 loss: 1.69243194e-05
Iter: 36 loss: 7.69820144e-06
Iter: 37 loss: 8.32209662e-06
Iter: 38 loss: 7.58078659e-06
Iter: 39 loss: 7.48090861e-06
Iter: 40 loss: 7.3731735e-06
Iter: 41 loss: 7.35604908e-06
Iter: 42 loss: 7.15238275e-06
Iter: 43 loss: 7.10395398e-06
Iter: 44 loss: 6.97395217e-06
Iter: 45 loss: 6.74797275e-06
Iter: 46 loss: 9.14405337e-06
Iter: 47 loss: 6.74238e-06
Iter: 48 loss: 6.58658792e-06
Iter: 49 loss: 6.42924624e-06
Iter: 50 loss: 6.39819609e-06
Iter: 51 loss: 6.17506612e-06
Iter: 52 loss: 8.17375621e-06
Iter: 53 loss: 6.1644896e-06
Iter: 54 loss: 6.03077933e-06
Iter: 55 loss: 6.54452742e-06
Iter: 56 loss: 5.99928762e-06
Iter: 57 loss: 5.87602608e-06
Iter: 58 loss: 6.30728164e-06
Iter: 59 loss: 5.84387544e-06
Iter: 60 loss: 5.7562288e-06
Iter: 61 loss: 5.66147537e-06
Iter: 62 loss: 5.64660513e-06
Iter: 63 loss: 5.49330707e-06
Iter: 64 loss: 6.8463678e-06
Iter: 65 loss: 5.4854554e-06
Iter: 66 loss: 5.38234872e-06
Iter: 67 loss: 5.26031226e-06
Iter: 68 loss: 5.24730967e-06
Iter: 69 loss: 5.20793765e-06
Iter: 70 loss: 5.17306307e-06
Iter: 71 loss: 5.13726081e-06
Iter: 72 loss: 5.13610485e-06
Iter: 73 loss: 5.10513564e-06
Iter: 74 loss: 5.0133167e-06
Iter: 75 loss: 5.33220464e-06
Iter: 76 loss: 4.97141e-06
Iter: 77 loss: 4.89566128e-06
Iter: 78 loss: 5.85167436e-06
Iter: 79 loss: 4.8951124e-06
Iter: 80 loss: 4.8366619e-06
Iter: 81 loss: 4.792043e-06
Iter: 82 loss: 4.77338472e-06
Iter: 83 loss: 4.75189836e-06
Iter: 84 loss: 4.7361973e-06
Iter: 85 loss: 4.70577652e-06
Iter: 86 loss: 4.65069934e-06
Iter: 87 loss: 5.99238456e-06
Iter: 88 loss: 4.65069115e-06
Iter: 89 loss: 4.58541581e-06
Iter: 90 loss: 4.89110425e-06
Iter: 91 loss: 4.57335318e-06
Iter: 92 loss: 4.51819233e-06
Iter: 93 loss: 4.7082417e-06
Iter: 94 loss: 4.50351672e-06
Iter: 95 loss: 4.4485223e-06
Iter: 96 loss: 4.63564265e-06
Iter: 97 loss: 4.43370482e-06
Iter: 98 loss: 4.3932032e-06
Iter: 99 loss: 4.36057553e-06
Iter: 100 loss: 4.34845606e-06
Iter: 101 loss: 4.28937392e-06
Iter: 102 loss: 4.72480315e-06
Iter: 103 loss: 4.28439307e-06
Iter: 104 loss: 4.23519941e-06
Iter: 105 loss: 4.3829109e-06
Iter: 106 loss: 4.22044741e-06
Iter: 107 loss: 4.21072218e-06
Iter: 108 loss: 4.19460866e-06
Iter: 109 loss: 4.18445688e-06
Iter: 110 loss: 4.15345221e-06
Iter: 111 loss: 4.23283882e-06
Iter: 112 loss: 4.13644921e-06
Iter: 113 loss: 4.09508539e-06
Iter: 114 loss: 4.45292426e-06
Iter: 115 loss: 4.09294944e-06
Iter: 116 loss: 4.06648e-06
Iter: 117 loss: 4.03992453e-06
Iter: 118 loss: 4.03453896e-06
Iter: 119 loss: 3.99647e-06
Iter: 120 loss: 4.4767412e-06
Iter: 121 loss: 3.99621149e-06
Iter: 122 loss: 3.9696115e-06
Iter: 123 loss: 3.9442225e-06
Iter: 124 loss: 3.93821119e-06
Iter: 125 loss: 3.92720176e-06
Iter: 126 loss: 3.91684671e-06
Iter: 127 loss: 3.90257946e-06
Iter: 128 loss: 3.88697617e-06
Iter: 129 loss: 3.88447916e-06
Iter: 130 loss: 3.85458407e-06
Iter: 131 loss: 3.92137508e-06
Iter: 132 loss: 3.84317354e-06
Iter: 133 loss: 3.81910877e-06
Iter: 134 loss: 3.80485199e-06
Iter: 135 loss: 3.79486073e-06
Iter: 136 loss: 3.76345724e-06
Iter: 137 loss: 4.13927046e-06
Iter: 138 loss: 3.76290313e-06
Iter: 139 loss: 3.74355477e-06
Iter: 140 loss: 3.82786311e-06
Iter: 141 loss: 3.73956232e-06
Iter: 142 loss: 3.72434943e-06
Iter: 143 loss: 3.72407817e-06
Iter: 144 loss: 3.71623855e-06
Iter: 145 loss: 3.70549242e-06
Iter: 146 loss: 3.70509088e-06
Iter: 147 loss: 3.68816882e-06
Iter: 148 loss: 3.6684371e-06
Iter: 149 loss: 3.66610516e-06
Iter: 150 loss: 3.64570928e-06
Iter: 151 loss: 3.74198294e-06
Iter: 152 loss: 3.64198013e-06
Iter: 153 loss: 3.61863749e-06
Iter: 154 loss: 3.65464e-06
Iter: 155 loss: 3.60765534e-06
Iter: 156 loss: 3.5892881e-06
Iter: 157 loss: 3.67572193e-06
Iter: 158 loss: 3.58588159e-06
Iter: 159 loss: 3.56650617e-06
Iter: 160 loss: 3.61796538e-06
Iter: 161 loss: 3.56010537e-06
Iter: 162 loss: 3.5472e-06
Iter: 163 loss: 3.71227361e-06
Iter: 164 loss: 3.54716803e-06
Iter: 165 loss: 3.53463e-06
Iter: 166 loss: 3.51170615e-06
Iter: 167 loss: 4.0490595e-06
Iter: 168 loss: 3.51176027e-06
Iter: 169 loss: 3.49463062e-06
Iter: 170 loss: 3.6299823e-06
Iter: 171 loss: 3.49348693e-06
Iter: 172 loss: 3.47793502e-06
Iter: 173 loss: 3.48559388e-06
Iter: 174 loss: 3.46750767e-06
Iter: 175 loss: 3.46804177e-06
Iter: 176 loss: 3.46090792e-06
Iter: 177 loss: 3.45371541e-06
Iter: 178 loss: 3.44993305e-06
Iter: 179 loss: 3.44662067e-06
Iter: 180 loss: 3.43698139e-06
Iter: 181 loss: 3.43157217e-06
Iter: 182 loss: 3.42738485e-06
Iter: 183 loss: 3.41537952e-06
Iter: 184 loss: 3.46696697e-06
Iter: 185 loss: 3.41292321e-06
Iter: 186 loss: 3.40099587e-06
Iter: 187 loss: 3.40431416e-06
Iter: 188 loss: 3.39238909e-06
Iter: 189 loss: 3.37958318e-06
Iter: 190 loss: 3.42043518e-06
Iter: 191 loss: 3.37587107e-06
Iter: 192 loss: 3.36398216e-06
Iter: 193 loss: 3.42546446e-06
Iter: 194 loss: 3.36195171e-06
Iter: 195 loss: 3.35272853e-06
Iter: 196 loss: 3.34577089e-06
Iter: 197 loss: 3.34276365e-06
Iter: 198 loss: 3.33167463e-06
Iter: 199 loss: 3.33111461e-06
Iter: 200 loss: 3.32377249e-06
Iter: 201 loss: 3.31773958e-06
Iter: 202 loss: 3.31566048e-06
Iter: 203 loss: 3.30226112e-06
Iter: 204 loss: 3.31578349e-06
Iter: 205 loss: 3.29481622e-06
Iter: 206 loss: 3.28415854e-06
Iter: 207 loss: 3.32817672e-06
Iter: 208 loss: 3.28179885e-06
Iter: 209 loss: 3.2747e-06
Iter: 210 loss: 3.27434282e-06
Iter: 211 loss: 3.26742429e-06
Iter: 212 loss: 3.27642829e-06
Iter: 213 loss: 3.26408963e-06
Iter: 214 loss: 3.25986548e-06
Iter: 215 loss: 3.24844268e-06
Iter: 216 loss: 3.31100046e-06
Iter: 217 loss: 3.24504936e-06
Iter: 218 loss: 3.23701e-06
Iter: 219 loss: 3.23592621e-06
Iter: 220 loss: 3.22945289e-06
Iter: 221 loss: 3.2254502e-06
Iter: 222 loss: 3.22289452e-06
Iter: 223 loss: 3.2131195e-06
Iter: 224 loss: 3.25925794e-06
Iter: 225 loss: 3.21132597e-06
Iter: 226 loss: 3.20320078e-06
Iter: 227 loss: 3.1934087e-06
Iter: 228 loss: 3.19240917e-06
Iter: 229 loss: 3.17987633e-06
Iter: 230 loss: 3.26292047e-06
Iter: 231 loss: 3.17860577e-06
Iter: 232 loss: 3.17010495e-06
Iter: 233 loss: 3.26790041e-06
Iter: 234 loss: 3.17000922e-06
Iter: 235 loss: 3.16379897e-06
Iter: 236 loss: 3.18046636e-06
Iter: 237 loss: 3.16184469e-06
Iter: 238 loss: 3.15353168e-06
Iter: 239 loss: 3.1640825e-06
Iter: 240 loss: 3.14933982e-06
Iter: 241 loss: 3.1431955e-06
Iter: 242 loss: 3.15456509e-06
Iter: 243 loss: 3.14055319e-06
Iter: 244 loss: 3.1351733e-06
Iter: 245 loss: 3.13514556e-06
Iter: 246 loss: 3.12932934e-06
Iter: 247 loss: 3.12499e-06
Iter: 248 loss: 3.12295651e-06
Iter: 249 loss: 3.11860231e-06
Iter: 250 loss: 3.10999985e-06
Iter: 251 loss: 3.27831e-06
Iter: 252 loss: 3.10979431e-06
Iter: 253 loss: 3.10116411e-06
Iter: 254 loss: 3.10113728e-06
Iter: 255 loss: 3.09595703e-06
Iter: 256 loss: 3.09524648e-06
Iter: 257 loss: 3.09160032e-06
Iter: 258 loss: 3.08286e-06
Iter: 259 loss: 3.11651138e-06
Iter: 260 loss: 3.08080439e-06
Iter: 261 loss: 3.07457572e-06
Iter: 262 loss: 3.08675089e-06
Iter: 263 loss: 3.07201958e-06
Iter: 264 loss: 3.0642077e-06
Iter: 265 loss: 3.07073105e-06
Iter: 266 loss: 3.05954381e-06
Iter: 267 loss: 3.0521428e-06
Iter: 268 loss: 3.06990523e-06
Iter: 269 loss: 3.04936748e-06
Iter: 270 loss: 3.04300784e-06
Iter: 271 loss: 3.12290922e-06
Iter: 272 loss: 3.04297714e-06
Iter: 273 loss: 3.03855904e-06
Iter: 274 loss: 3.06303536e-06
Iter: 275 loss: 3.03799607e-06
Iter: 276 loss: 3.033962e-06
Iter: 277 loss: 3.03415936e-06
Iter: 278 loss: 3.0308006e-06
Iter: 279 loss: 3.02835201e-06
Iter: 280 loss: 3.0274914e-06
Iter: 281 loss: 3.02581157e-06
Iter: 282 loss: 3.02066792e-06
Iter: 283 loss: 3.03291313e-06
Iter: 284 loss: 3.01768682e-06
Iter: 285 loss: 3.01138061e-06
Iter: 286 loss: 3.07050573e-06
Iter: 287 loss: 3.01118621e-06
Iter: 288 loss: 3.00609554e-06
Iter: 289 loss: 3.00282068e-06
Iter: 290 loss: 3.00086481e-06
Iter: 291 loss: 2.99460817e-06
Iter: 292 loss: 2.99462545e-06
Iter: 293 loss: 2.99080693e-06
Iter: 294 loss: 2.98944019e-06
Iter: 295 loss: 2.9873695e-06
Iter: 296 loss: 2.98125178e-06
Iter: 297 loss: 3.00976035e-06
Iter: 298 loss: 2.98011332e-06
Iter: 299 loss: 2.97590941e-06
Iter: 300 loss: 2.97171709e-06
Iter: 301 loss: 2.97076031e-06
Iter: 302 loss: 2.96390954e-06
Iter: 303 loss: 3.04514606e-06
Iter: 304 loss: 2.96382154e-06
Iter: 305 loss: 2.95984046e-06
Iter: 306 loss: 2.96349663e-06
Iter: 307 loss: 2.95748714e-06
Iter: 308 loss: 2.95271616e-06
Iter: 309 loss: 3.00863053e-06
Iter: 310 loss: 2.95269319e-06
Iter: 311 loss: 2.95031941e-06
Iter: 312 loss: 2.95034783e-06
Iter: 313 loss: 2.94838e-06
Iter: 314 loss: 2.94514e-06
Iter: 315 loss: 2.94514848e-06
Iter: 316 loss: 2.94171969e-06
Iter: 317 loss: 2.94091387e-06
Iter: 318 loss: 2.93869721e-06
Iter: 319 loss: 2.93401217e-06
Iter: 320 loss: 2.93290122e-06
Iter: 321 loss: 2.92992831e-06
Iter: 322 loss: 2.92432469e-06
Iter: 323 loss: 2.98404939e-06
Iter: 324 loss: 2.92414461e-06
Iter: 325 loss: 2.91933065e-06
Iter: 326 loss: 2.93159883e-06
Iter: 327 loss: 2.91761603e-06
Iter: 328 loss: 2.91334982e-06
Iter: 329 loss: 2.9356147e-06
Iter: 330 loss: 2.9126e-06
Iter: 331 loss: 2.90881508e-06
Iter: 332 loss: 2.90202888e-06
Iter: 333 loss: 2.90205639e-06
Iter: 334 loss: 2.89772379e-06
Iter: 335 loss: 2.8974423e-06
Iter: 336 loss: 2.89391983e-06
Iter: 337 loss: 2.89194827e-06
Iter: 338 loss: 2.8905215e-06
Iter: 339 loss: 2.88654519e-06
Iter: 340 loss: 2.93443918e-06
Iter: 341 loss: 2.88644742e-06
Iter: 342 loss: 2.88352157e-06
Iter: 343 loss: 2.90287335e-06
Iter: 344 loss: 2.88327419e-06
Iter: 345 loss: 2.88004208e-06
Iter: 346 loss: 2.89211857e-06
Iter: 347 loss: 2.87938701e-06
Iter: 348 loss: 2.87674038e-06
Iter: 349 loss: 2.87708758e-06
Iter: 350 loss: 2.87470311e-06
Iter: 351 loss: 2.87251737e-06
Iter: 352 loss: 2.8670861e-06
Iter: 353 loss: 2.92632694e-06
Iter: 354 loss: 2.86650697e-06
Iter: 355 loss: 2.86068e-06
Iter: 356 loss: 2.93675748e-06
Iter: 357 loss: 2.86068e-06
Iter: 358 loss: 2.85677129e-06
Iter: 359 loss: 2.85738361e-06
Iter: 360 loss: 2.85398369e-06
Iter: 361 loss: 2.84994439e-06
Iter: 362 loss: 2.90475509e-06
Iter: 363 loss: 2.84994144e-06
Iter: 364 loss: 2.84636599e-06
Iter: 365 loss: 2.84461748e-06
Iter: 366 loss: 2.84292628e-06
Iter: 367 loss: 2.83849795e-06
Iter: 368 loss: 2.86662271e-06
Iter: 369 loss: 2.83802933e-06
Iter: 370 loss: 2.83384725e-06
Iter: 371 loss: 2.83351574e-06
Iter: 372 loss: 2.83033523e-06
Iter: 373 loss: 2.82572523e-06
Iter: 374 loss: 2.83513737e-06
Iter: 375 loss: 2.82384553e-06
Iter: 376 loss: 2.81844086e-06
Iter: 377 loss: 2.85791702e-06
Iter: 378 loss: 2.8180541e-06
Iter: 379 loss: 2.81661642e-06
Iter: 380 loss: 2.81601797e-06
Iter: 381 loss: 2.81422513e-06
Iter: 382 loss: 2.8120985e-06
Iter: 383 loss: 2.81182747e-06
Iter: 384 loss: 2.80815834e-06
Iter: 385 loss: 2.80801873e-06
Iter: 386 loss: 2.8051536e-06
Iter: 387 loss: 2.80199356e-06
Iter: 388 loss: 2.80946824e-06
Iter: 389 loss: 2.80080349e-06
Iter: 390 loss: 2.79646883e-06
Iter: 391 loss: 2.79358437e-06
Iter: 392 loss: 2.79191863e-06
Iter: 393 loss: 2.78717403e-06
Iter: 394 loss: 2.80361155e-06
Iter: 395 loss: 2.78597554e-06
Iter: 396 loss: 2.78078414e-06
Iter: 397 loss: 2.80748509e-06
Iter: 398 loss: 2.77985146e-06
Iter: 399 loss: 2.77635354e-06
Iter: 400 loss: 2.79768165e-06
Iter: 401 loss: 2.77591926e-06
Iter: 402 loss: 2.77220124e-06
Iter: 403 loss: 2.77280219e-06
Iter: 404 loss: 2.76945229e-06
Iter: 405 loss: 2.76549054e-06
Iter: 406 loss: 2.7662918e-06
Iter: 407 loss: 2.76259107e-06
Iter: 408 loss: 2.75786124e-06
Iter: 409 loss: 2.80900144e-06
Iter: 410 loss: 2.75765478e-06
Iter: 411 loss: 2.75480261e-06
Iter: 412 loss: 2.77406616e-06
Iter: 413 loss: 2.7544952e-06
Iter: 414 loss: 2.75158209e-06
Iter: 415 loss: 2.77906361e-06
Iter: 416 loss: 2.75151569e-06
Iter: 417 loss: 2.75002e-06
Iter: 418 loss: 2.74910713e-06
Iter: 419 loss: 2.74855847e-06
Iter: 420 loss: 2.74596732e-06
Iter: 421 loss: 2.74356307e-06
Iter: 422 loss: 2.74297645e-06
Iter: 423 loss: 2.73906971e-06
Iter: 424 loss: 2.76324545e-06
Iter: 425 loss: 2.73869114e-06
Iter: 426 loss: 2.73613773e-06
Iter: 427 loss: 2.73345745e-06
Iter: 428 loss: 2.73304317e-06
Iter: 429 loss: 2.72906436e-06
Iter: 430 loss: 2.77334857e-06
Iter: 431 loss: 2.7289459e-06
Iter: 432 loss: 2.72603324e-06
Iter: 433 loss: 2.72399438e-06
Iter: 434 loss: 2.72296097e-06
Iter: 435 loss: 2.71863496e-06
Iter: 436 loss: 2.76874289e-06
Iter: 437 loss: 2.718524e-06
Iter: 438 loss: 2.71591557e-06
Iter: 439 loss: 2.71960698e-06
Iter: 440 loss: 2.71475346e-06
Iter: 441 loss: 2.71084377e-06
Iter: 442 loss: 2.71252134e-06
Iter: 443 loss: 2.70817554e-06
Iter: 444 loss: 2.7043418e-06
Iter: 445 loss: 2.7100948e-06
Iter: 446 loss: 2.70253054e-06
Iter: 447 loss: 2.70284863e-06
Iter: 448 loss: 2.70077226e-06
Iter: 449 loss: 2.69918178e-06
Iter: 450 loss: 2.69857401e-06
Iter: 451 loss: 2.69764951e-06
Iter: 452 loss: 2.69604288e-06
Iter: 453 loss: 2.69521342e-06
Iter: 454 loss: 2.69440807e-06
Iter: 455 loss: 2.69147699e-06
Iter: 456 loss: 2.6999287e-06
Iter: 457 loss: 2.69064731e-06
Iter: 458 loss: 2.68818894e-06
Iter: 459 loss: 2.6854259e-06
Iter: 460 loss: 2.68504255e-06
Iter: 461 loss: 2.68204712e-06
Iter: 462 loss: 2.72760599e-06
Iter: 463 loss: 2.68203166e-06
Iter: 464 loss: 2.67956398e-06
Iter: 465 loss: 2.67744667e-06
Iter: 466 loss: 2.67677478e-06
Iter: 467 loss: 2.67385076e-06
Iter: 468 loss: 2.70915871e-06
Iter: 469 loss: 2.67379346e-06
Iter: 470 loss: 2.67119367e-06
Iter: 471 loss: 2.6714024e-06
Iter: 472 loss: 2.66912934e-06
Iter: 473 loss: 2.66603411e-06
Iter: 474 loss: 2.69851262e-06
Iter: 475 loss: 2.66589359e-06
Iter: 476 loss: 2.66392408e-06
Iter: 477 loss: 2.66156076e-06
Iter: 478 loss: 2.66125335e-06
Iter: 479 loss: 2.66005327e-06
Iter: 480 loss: 2.6596615e-06
Iter: 481 loss: 2.65824383e-06
Iter: 482 loss: 2.66647294e-06
Iter: 483 loss: 2.65810922e-06
Iter: 484 loss: 2.65706967e-06
Iter: 485 loss: 2.65410699e-06
Iter: 486 loss: 2.66546272e-06
Iter: 487 loss: 2.65287554e-06
Iter: 488 loss: 2.65053063e-06
Iter: 489 loss: 2.65033782e-06
Iter: 490 loss: 2.64879395e-06
Iter: 491 loss: 2.64690516e-06
Iter: 492 loss: 2.64662026e-06
Iter: 493 loss: 2.64402115e-06
Iter: 494 loss: 2.65791869e-06
Iter: 495 loss: 2.64355094e-06
Iter: 496 loss: 2.64148912e-06
Iter: 497 loss: 2.64073697e-06
Iter: 498 loss: 2.63954e-06
Iter: 499 loss: 2.63660877e-06
Iter: 500 loss: 2.65432436e-06
Iter: 501 loss: 2.63630386e-06
Iter: 502 loss: 2.63329548e-06
Iter: 503 loss: 2.63774223e-06
Iter: 504 loss: 2.63177344e-06
Iter: 505 loss: 2.629552e-06
Iter: 506 loss: 2.64646451e-06
Iter: 507 loss: 2.62937942e-06
Iter: 508 loss: 2.62709045e-06
Iter: 509 loss: 2.62935e-06
Iter: 510 loss: 2.62586036e-06
Iter: 511 loss: 2.62370759e-06
Iter: 512 loss: 2.63110542e-06
Iter: 513 loss: 2.62318963e-06
Iter: 514 loss: 2.62121785e-06
Iter: 515 loss: 2.63619e-06
Iter: 516 loss: 2.62107324e-06
Iter: 517 loss: 2.6185412e-06
Iter: 518 loss: 2.62202e-06
Iter: 519 loss: 2.61724517e-06
Iter: 520 loss: 2.61617083e-06
Iter: 521 loss: 2.61405648e-06
Iter: 522 loss: 2.65458175e-06
Iter: 523 loss: 2.61407126e-06
Iter: 524 loss: 2.61130572e-06
Iter: 525 loss: 2.63030461e-06
Iter: 526 loss: 2.61099763e-06
Iter: 527 loss: 2.60905904e-06
Iter: 528 loss: 2.61577361e-06
Iter: 529 loss: 2.60847332e-06
Iter: 530 loss: 2.60660295e-06
Iter: 531 loss: 2.60351271e-06
Iter: 532 loss: 2.60351248e-06
Iter: 533 loss: 2.60064439e-06
Iter: 534 loss: 2.62828962e-06
Iter: 535 loss: 2.60054048e-06
Iter: 536 loss: 2.59787907e-06
Iter: 537 loss: 2.59990611e-06
Iter: 538 loss: 2.59629815e-06
Iter: 539 loss: 2.59361468e-06
Iter: 540 loss: 2.61343348e-06
Iter: 541 loss: 2.59331455e-06
Iter: 542 loss: 2.59073022e-06
Iter: 543 loss: 2.59398848e-06
Iter: 544 loss: 2.58932982e-06
Iter: 545 loss: 2.58682394e-06
Iter: 546 loss: 2.59816966e-06
Iter: 547 loss: 2.58631917e-06
Iter: 548 loss: 2.5837021e-06
Iter: 549 loss: 2.58813179e-06
Iter: 550 loss: 2.58253863e-06
Iter: 551 loss: 2.58297314e-06
Iter: 552 loss: 2.58162913e-06
Iter: 553 loss: 2.58083605e-06
Iter: 554 loss: 2.57857459e-06
Iter: 555 loss: 2.58222258e-06
Iter: 556 loss: 2.57702663e-06
Iter: 557 loss: 2.57445663e-06
Iter: 558 loss: 2.5997042e-06
Iter: 559 loss: 2.57430565e-06
Iter: 560 loss: 2.57214833e-06
Iter: 561 loss: 2.57564625e-06
Iter: 562 loss: 2.57121519e-06
Iter: 563 loss: 2.5681768e-06
Iter: 564 loss: 2.57303645e-06
Iter: 565 loss: 2.56670501e-06
Iter: 566 loss: 2.56482713e-06
Iter: 567 loss: 2.56750036e-06
Iter: 568 loss: 2.56384715e-06
Iter: 569 loss: 2.56112503e-06
Iter: 570 loss: 2.56468593e-06
Iter: 571 loss: 2.55971736e-06
Iter: 572 loss: 2.55771238e-06
Iter: 573 loss: 2.56492399e-06
Iter: 574 loss: 2.55719578e-06
Iter: 575 loss: 2.55476834e-06
Iter: 576 loss: 2.56279463e-06
Iter: 577 loss: 2.5541292e-06
Iter: 578 loss: 2.55204827e-06
Iter: 579 loss: 2.55606e-06
Iter: 580 loss: 2.55115401e-06
Iter: 581 loss: 2.54824317e-06
Iter: 582 loss: 2.55612917e-06
Iter: 583 loss: 2.54731481e-06
Iter: 584 loss: 2.54659767e-06
Iter: 585 loss: 2.54639804e-06
Iter: 586 loss: 2.54531619e-06
Iter: 587 loss: 2.54444808e-06
Iter: 588 loss: 2.54411543e-06
Iter: 589 loss: 2.54278507e-06
Iter: 590 loss: 2.54234374e-06
Iter: 591 loss: 2.54160614e-06
Iter: 592 loss: 2.53977191e-06
Iter: 593 loss: 2.54153292e-06
Iter: 594 loss: 2.53871212e-06
Iter: 595 loss: 2.5365357e-06
Iter: 596 loss: 2.55771533e-06
Iter: 597 loss: 2.53645157e-06
Iter: 598 loss: 2.53493045e-06
Iter: 599 loss: 2.53555118e-06
Iter: 600 loss: 2.53394137e-06
Iter: 601 loss: 2.53164e-06
Iter: 602 loss: 2.53460848e-06
Iter: 603 loss: 2.53049279e-06
Iter: 604 loss: 2.52855057e-06
Iter: 605 loss: 2.52686664e-06
Iter: 606 loss: 2.52633708e-06
Iter: 607 loss: 2.5243703e-06
Iter: 608 loss: 2.52430618e-06
Iter: 609 loss: 2.52239511e-06
Iter: 610 loss: 2.52007089e-06
Iter: 611 loss: 2.5198342e-06
Iter: 612 loss: 2.51819802e-06
Iter: 613 loss: 2.51790721e-06
Iter: 614 loss: 2.51666324e-06
Iter: 615 loss: 2.5146569e-06
Iter: 616 loss: 2.51461142e-06
Iter: 617 loss: 2.51555639e-06
Iter: 618 loss: 2.5136585e-06
Iter: 619 loss: 2.51286338e-06
Iter: 620 loss: 2.51192455e-06
Iter: 621 loss: 2.51185475e-06
Iter: 622 loss: 2.51062102e-06
Iter: 623 loss: 2.50835183e-06
Iter: 624 loss: 2.5624413e-06
Iter: 625 loss: 2.50830522e-06
Iter: 626 loss: 2.50625e-06
Iter: 627 loss: 2.51376014e-06
Iter: 628 loss: 2.50567928e-06
Iter: 629 loss: 2.50347398e-06
Iter: 630 loss: 2.5075492e-06
Iter: 631 loss: 2.50259677e-06
Iter: 632 loss: 2.50066864e-06
Iter: 633 loss: 2.51948177e-06
Iter: 634 loss: 2.50063772e-06
Iter: 635 loss: 2.49920595e-06
Iter: 636 loss: 2.50000903e-06
Iter: 637 loss: 2.49834511e-06
Iter: 638 loss: 2.49651771e-06
Iter: 639 loss: 2.50218568e-06
Iter: 640 loss: 2.49602294e-06
Iter: 641 loss: 2.49414529e-06
Iter: 642 loss: 2.49303457e-06
Iter: 643 loss: 2.49216737e-06
Iter: 644 loss: 2.49014784e-06
Iter: 645 loss: 2.51055303e-06
Iter: 646 loss: 2.49006553e-06
Iter: 647 loss: 2.48828565e-06
Iter: 648 loss: 2.4868541e-06
Iter: 649 loss: 2.48629249e-06
Iter: 650 loss: 2.48429774e-06
Iter: 651 loss: 2.50052017e-06
Iter: 652 loss: 2.48417609e-06
Iter: 653 loss: 2.48251899e-06
Iter: 654 loss: 2.48255878e-06
Iter: 655 loss: 2.48124479e-06
Iter: 656 loss: 2.48126094e-06
Iter: 657 loss: 2.48036235e-06
Iter: 658 loss: 2.47925345e-06
Iter: 659 loss: 2.47739217e-06
Iter: 660 loss: 2.47744629e-06
Iter: 661 loss: 2.47475e-06
Iter: 662 loss: 2.48302285e-06
Iter: 663 loss: 2.47387948e-06
Iter: 664 loss: 2.47195317e-06
Iter: 665 loss: 2.48005108e-06
Iter: 666 loss: 2.47153457e-06
Iter: 667 loss: 2.46920922e-06
Iter: 668 loss: 2.47032131e-06
Iter: 669 loss: 2.46759782e-06
Iter: 670 loss: 2.46601439e-06
Iter: 671 loss: 2.46591298e-06
Iter: 672 loss: 2.46459672e-06
Iter: 673 loss: 2.46255945e-06
Iter: 674 loss: 2.46257127e-06
Iter: 675 loss: 2.45998444e-06
Iter: 676 loss: 2.47860021e-06
Iter: 677 loss: 2.45980027e-06
Iter: 678 loss: 2.45796582e-06
Iter: 679 loss: 2.45748015e-06
Iter: 680 loss: 2.45644742e-06
Iter: 681 loss: 2.45368938e-06
Iter: 682 loss: 2.46787681e-06
Iter: 683 loss: 2.45324986e-06
Iter: 684 loss: 2.45136948e-06
Iter: 685 loss: 2.45473507e-06
Iter: 686 loss: 2.45046976e-06
Iter: 687 loss: 2.4495007e-06
Iter: 688 loss: 2.44898706e-06
Iter: 689 loss: 2.44828288e-06
Iter: 690 loss: 2.44668854e-06
Iter: 691 loss: 2.47008211e-06
Iter: 692 loss: 2.44660487e-06
Iter: 693 loss: 2.4448259e-06
Iter: 694 loss: 2.44899093e-06
Iter: 695 loss: 2.44413468e-06
Iter: 696 loss: 2.44265675e-06
Iter: 697 loss: 2.44814373e-06
Iter: 698 loss: 2.44231e-06
Iter: 699 loss: 2.44069088e-06
Iter: 700 loss: 2.43984232e-06
Iter: 701 loss: 2.43907834e-06
Iter: 702 loss: 2.43744489e-06
Iter: 703 loss: 2.43742966e-06
Iter: 704 loss: 2.43598879e-06
Iter: 705 loss: 2.43439513e-06
Iter: 706 loss: 2.43403429e-06
Iter: 707 loss: 2.43174145e-06
Iter: 708 loss: 2.4570063e-06
Iter: 709 loss: 2.43167096e-06
Iter: 710 loss: 2.43047498e-06
Iter: 711 loss: 2.42881174e-06
Iter: 712 loss: 2.42872193e-06
Iter: 713 loss: 2.42640408e-06
Iter: 714 loss: 2.44715397e-06
Iter: 715 loss: 2.42637248e-06
Iter: 716 loss: 2.42478291e-06
Iter: 717 loss: 2.42404576e-06
Iter: 718 loss: 2.42328792e-06
Iter: 719 loss: 2.42216447e-06
Iter: 720 loss: 2.42184433e-06
Iter: 721 loss: 2.42066176e-06
Iter: 722 loss: 2.42576812e-06
Iter: 723 loss: 2.42041415e-06
Iter: 724 loss: 2.41986254e-06
Iter: 725 loss: 2.41828729e-06
Iter: 726 loss: 2.42131273e-06
Iter: 727 loss: 2.41722933e-06
Iter: 728 loss: 2.41527664e-06
Iter: 729 loss: 2.41521639e-06
Iter: 730 loss: 2.41401744e-06
Iter: 731 loss: 2.41333237e-06
Iter: 732 loss: 2.41281668e-06
Iter: 733 loss: 2.4108208e-06
Iter: 734 loss: 2.42036685e-06
Iter: 735 loss: 2.41039265e-06
Iter: 736 loss: 2.40909344e-06
Iter: 737 loss: 2.41935868e-06
Iter: 738 loss: 2.40903228e-06
Iter: 739 loss: 2.4077267e-06
Iter: 740 loss: 2.40820282e-06
Iter: 741 loss: 2.40691497e-06
Iter: 742 loss: 2.40553959e-06
Iter: 743 loss: 2.40920076e-06
Iter: 744 loss: 2.40503459e-06
Iter: 745 loss: 2.40363124e-06
Iter: 746 loss: 2.40613781e-06
Iter: 747 loss: 2.4028609e-06
Iter: 748 loss: 2.40144072e-06
Iter: 749 loss: 2.40148756e-06
Iter: 750 loss: 2.40025838e-06
Iter: 751 loss: 2.39870792e-06
Iter: 752 loss: 2.42254782e-06
Iter: 753 loss: 2.3987136e-06
Iter: 754 loss: 2.39823044e-06
Iter: 755 loss: 2.39813653e-06
Iter: 756 loss: 2.39750398e-06
Iter: 757 loss: 2.3956627e-06
Iter: 758 loss: 2.40451254e-06
Iter: 759 loss: 2.39501742e-06
Iter: 760 loss: 2.39355404e-06
Iter: 761 loss: 2.4116548e-06
Iter: 762 loss: 2.39354677e-06
Iter: 763 loss: 2.39246083e-06
Iter: 764 loss: 2.39079122e-06
Iter: 765 loss: 2.39075416e-06
Iter: 766 loss: 2.38986149e-06
Iter: 767 loss: 2.38968687e-06
Iter: 768 loss: 2.3888017e-06
Iter: 769 loss: 2.3866935e-06
Iter: 770 loss: 2.40958457e-06
Iter: 771 loss: 2.38638813e-06
Iter: 772 loss: 2.38533949e-06
Iter: 773 loss: 2.38511075e-06
Iter: 774 loss: 2.38419625e-06
Iter: 775 loss: 2.38491225e-06
Iter: 776 loss: 2.38362827e-06
Iter: 777 loss: 2.38227085e-06
Iter: 778 loss: 2.3825387e-06
Iter: 779 loss: 2.38123766e-06
Iter: 780 loss: 2.37988252e-06
Iter: 781 loss: 2.38603161e-06
Iter: 782 loss: 2.37960921e-06
Iter: 783 loss: 2.37808899e-06
Iter: 784 loss: 2.37925269e-06
Iter: 785 loss: 2.37719587e-06
Iter: 786 loss: 2.37626227e-06
Iter: 787 loss: 2.38734174e-06
Iter: 788 loss: 2.3762243e-06
Iter: 789 loss: 2.37525251e-06
Iter: 790 loss: 2.38126199e-06
Iter: 791 loss: 2.37507402e-06
Iter: 792 loss: 2.3742914e-06
Iter: 793 loss: 2.37256245e-06
Iter: 794 loss: 2.399604e-06
Iter: 795 loss: 2.37250697e-06
Iter: 796 loss: 2.37124686e-06
Iter: 797 loss: 2.37286758e-06
Iter: 798 loss: 2.37052723e-06
Iter: 799 loss: 2.36850883e-06
Iter: 800 loss: 2.37708559e-06
Iter: 801 loss: 2.36808637e-06
Iter: 802 loss: 2.36711253e-06
Iter: 803 loss: 2.36977939e-06
Iter: 804 loss: 2.36677533e-06
Iter: 805 loss: 2.36533879e-06
Iter: 806 loss: 2.36616256e-06
Iter: 807 loss: 2.36445658e-06
Iter: 808 loss: 2.36326855e-06
Iter: 809 loss: 2.37261656e-06
Iter: 810 loss: 2.36313258e-06
Iter: 811 loss: 2.36190681e-06
Iter: 812 loss: 2.36293181e-06
Iter: 813 loss: 2.36123606e-06
Iter: 814 loss: 2.35966968e-06
Iter: 815 loss: 2.36758797e-06
Iter: 816 loss: 2.35942071e-06
Iter: 817 loss: 2.35846164e-06
Iter: 818 loss: 2.35784728e-06
Iter: 819 loss: 2.35746893e-06
Iter: 820 loss: 2.35579159e-06
Iter: 821 loss: 2.36187134e-06
Iter: 822 loss: 2.35539642e-06
Iter: 823 loss: 2.35403195e-06
Iter: 824 loss: 2.36213373e-06
Iter: 825 loss: 2.35389325e-06
Iter: 826 loss: 2.35250059e-06
Iter: 827 loss: 2.36580263e-06
Iter: 828 loss: 2.35240122e-06
Iter: 829 loss: 2.35184461e-06
Iter: 830 loss: 2.35040943e-06
Iter: 831 loss: 2.36429287e-06
Iter: 832 loss: 2.35030348e-06
Iter: 833 loss: 2.34849472e-06
Iter: 834 loss: 2.35371772e-06
Iter: 835 loss: 2.34793492e-06
Iter: 836 loss: 2.34675645e-06
Iter: 837 loss: 2.35105767e-06
Iter: 838 loss: 2.34643676e-06
Iter: 839 loss: 2.34494519e-06
Iter: 840 loss: 2.34914614e-06
Iter: 841 loss: 2.34457593e-06
Iter: 842 loss: 2.3435914e-06
Iter: 843 loss: 2.34704385e-06
Iter: 844 loss: 2.34335562e-06
Iter: 845 loss: 2.34218032e-06
Iter: 846 loss: 2.34161644e-06
Iter: 847 loss: 2.341141e-06
Iter: 848 loss: 2.33997844e-06
Iter: 849 loss: 2.33991977e-06
Iter: 850 loss: 2.33917376e-06
Iter: 851 loss: 2.33833134e-06
Iter: 852 loss: 2.3382197e-06
Iter: 853 loss: 2.33662377e-06
Iter: 854 loss: 2.34122558e-06
Iter: 855 loss: 2.33612377e-06
Iter: 856 loss: 2.33492665e-06
Iter: 857 loss: 2.33562514e-06
Iter: 858 loss: 2.33417768e-06
Iter: 859 loss: 2.33339324e-06
Iter: 860 loss: 2.33323613e-06
Iter: 861 loss: 2.3322782e-06
Iter: 862 loss: 2.33264336e-06
Iter: 863 loss: 2.33156311e-06
Iter: 864 loss: 2.33093897e-06
Iter: 865 loss: 2.32929483e-06
Iter: 866 loss: 2.34049025e-06
Iter: 867 loss: 2.32892648e-06
Iter: 868 loss: 2.32735283e-06
Iter: 869 loss: 2.32736306e-06
Iter: 870 loss: 2.32624757e-06
Iter: 871 loss: 2.32570619e-06
Iter: 872 loss: 2.3252087e-06
Iter: 873 loss: 2.3235616e-06
Iter: 874 loss: 2.34091954e-06
Iter: 875 loss: 2.32353295e-06
Iter: 876 loss: 2.32241769e-06
Iter: 877 loss: 2.32310504e-06
Iter: 878 loss: 2.32173488e-06
Iter: 879 loss: 2.32044749e-06
Iter: 880 loss: 2.32258299e-06
Iter: 881 loss: 2.3198354e-06
Iter: 882 loss: 2.31820354e-06
Iter: 883 loss: 2.32448451e-06
Iter: 884 loss: 2.31785839e-06
Iter: 885 loss: 2.31678951e-06
Iter: 886 loss: 2.3289e-06
Iter: 887 loss: 2.31680701e-06
Iter: 888 loss: 2.31585136e-06
Iter: 889 loss: 2.31469357e-06
Iter: 890 loss: 2.31456352e-06
Iter: 891 loss: 2.31369927e-06
Iter: 892 loss: 2.31367858e-06
Iter: 893 loss: 2.31306763e-06
Iter: 894 loss: 2.32165394e-06
Iter: 895 loss: 2.31306626e-06
Iter: 896 loss: 2.31252716e-06
Iter: 897 loss: 2.31136164e-06
Iter: 898 loss: 2.32286288e-06
Iter: 899 loss: 2.31112017e-06
Iter: 900 loss: 2.31004378e-06
Iter: 901 loss: 2.31497506e-06
Iter: 902 loss: 2.30985916e-06
Iter: 903 loss: 2.30875139e-06
Iter: 904 loss: 2.30786668e-06
Iter: 905 loss: 2.30755541e-06
Iter: 906 loss: 2.30621458e-06
Iter: 907 loss: 2.32744605e-06
Iter: 908 loss: 2.3062164e-06
Iter: 909 loss: 2.30523642e-06
Iter: 910 loss: 2.30433238e-06
Iter: 911 loss: 2.3040534e-06
Iter: 912 loss: 2.30261548e-06
Iter: 913 loss: 2.32188495e-06
Iter: 914 loss: 2.30264868e-06
Iter: 915 loss: 2.30182332e-06
Iter: 916 loss: 2.30199498e-06
Iter: 917 loss: 2.30122396e-06
Iter: 918 loss: 2.29977354e-06
Iter: 919 loss: 2.29989382e-06
Iter: 920 loss: 2.29858551e-06
Iter: 921 loss: 2.29734496e-06
Iter: 922 loss: 2.30606747e-06
Iter: 923 loss: 2.29723105e-06
Iter: 924 loss: 2.29590432e-06
Iter: 925 loss: 2.30044975e-06
Iter: 926 loss: 2.29551324e-06
Iter: 927 loss: 2.29488887e-06
Iter: 928 loss: 2.30315891e-06
Iter: 929 loss: 2.29486477e-06
Iter: 930 loss: 2.29426814e-06
Iter: 931 loss: 2.29615716e-06
Iter: 932 loss: 2.29409625e-06
Iter: 933 loss: 2.29351372e-06
Iter: 934 loss: 2.29214879e-06
Iter: 935 loss: 2.30336627e-06
Iter: 936 loss: 2.29185139e-06
Iter: 937 loss: 2.29052534e-06
Iter: 938 loss: 2.29068473e-06
Iter: 939 loss: 2.28949284e-06
Iter: 940 loss: 2.28767567e-06
Iter: 941 loss: 2.3088021e-06
Iter: 942 loss: 2.28760655e-06
Iter: 943 loss: 2.2864092e-06
Iter: 944 loss: 2.28570616e-06
Iter: 945 loss: 2.28515046e-06
Iter: 946 loss: 2.28374324e-06
Iter: 947 loss: 2.28370914e-06
Iter: 948 loss: 2.28292674e-06
Iter: 949 loss: 2.28263116e-06
Iter: 950 loss: 2.28215868e-06
Iter: 951 loss: 2.28051977e-06
Iter: 952 loss: 2.28465979e-06
Iter: 953 loss: 2.27990085e-06
Iter: 954 loss: 2.27881e-06
Iter: 955 loss: 2.27997e-06
Iter: 956 loss: 2.27821079e-06
Iter: 957 loss: 2.27687633e-06
Iter: 958 loss: 2.28458657e-06
Iter: 959 loss: 2.27672854e-06
Iter: 960 loss: 2.2755205e-06
Iter: 961 loss: 2.27937085e-06
Iter: 962 loss: 2.27515966e-06
Iter: 963 loss: 2.27413966e-06
Iter: 964 loss: 2.2830618e-06
Iter: 965 loss: 2.27411829e-06
Iter: 966 loss: 2.27328201e-06
Iter: 967 loss: 2.27995724e-06
Iter: 968 loss: 2.27328314e-06
Iter: 969 loss: 2.27284954e-06
Iter: 970 loss: 2.27172404e-06
Iter: 971 loss: 2.27750729e-06
Iter: 972 loss: 2.27132159e-06
Iter: 973 loss: 2.26973066e-06
Iter: 974 loss: 2.27680266e-06
Iter: 975 loss: 2.26941597e-06
Iter: 976 loss: 2.26802285e-06
Iter: 977 loss: 2.26654174e-06
Iter: 978 loss: 2.26632687e-06
Iter: 979 loss: 2.26502129e-06
Iter: 980 loss: 2.26496468e-06
Iter: 981 loss: 2.26403699e-06
Iter: 982 loss: 2.26339e-06
Iter: 983 loss: 2.26308202e-06
Iter: 984 loss: 2.26148541e-06
Iter: 985 loss: 2.27833038e-06
Iter: 986 loss: 2.26144243e-06
Iter: 987 loss: 2.26054908e-06
Iter: 988 loss: 2.26029e-06
Iter: 989 loss: 2.25985173e-06
Iter: 990 loss: 2.25816711e-06
Iter: 991 loss: 2.26194697e-06
Iter: 992 loss: 2.2575407e-06
Iter: 993 loss: 2.25653457e-06
Iter: 994 loss: 2.26201428e-06
Iter: 995 loss: 2.25637723e-06
Iter: 996 loss: 2.2552e-06
Iter: 997 loss: 2.25627059e-06
Iter: 998 loss: 2.25446706e-06
Iter: 999 loss: 2.25377698e-06
Iter: 1000 loss: 2.25366648e-06
Iter: 1001 loss: 2.25305803e-06
Iter: 1002 loss: 2.25314125e-06
Iter: 1003 loss: 2.25253871e-06
Iter: 1004 loss: 2.25185227e-06
Iter: 1005 loss: 2.24999053e-06
Iter: 1006 loss: 2.26868565e-06
Iter: 1007 loss: 2.2498125e-06
Iter: 1008 loss: 2.24869882e-06
Iter: 1009 loss: 2.2486829e-06
Iter: 1010 loss: 2.24775886e-06
Iter: 1011 loss: 2.24642645e-06
Iter: 1012 loss: 2.24636983e-06
Iter: 1013 loss: 2.24503219e-06
Iter: 1014 loss: 2.26172142e-06
Iter: 1015 loss: 2.24498e-06
Iter: 1016 loss: 2.24371979e-06
Iter: 1017 loss: 2.24359064e-06
Iter: 1018 loss: 2.24268388e-06
Iter: 1019 loss: 2.24187283e-06
Iter: 1020 loss: 2.24181895e-06
Iter: 1021 loss: 2.24108021e-06
Iter: 1022 loss: 2.23957659e-06
Iter: 1023 loss: 2.26742168e-06
Iter: 1024 loss: 2.23958295e-06
Iter: 1025 loss: 2.23793018e-06
Iter: 1026 loss: 2.25959911e-06
Iter: 1027 loss: 2.23790357e-06
Iter: 1028 loss: 2.23712618e-06
Iter: 1029 loss: 2.23718143e-06
Iter: 1030 loss: 2.23655593e-06
Iter: 1031 loss: 2.23531424e-06
Iter: 1032 loss: 2.24742462e-06
Iter: 1033 loss: 2.23524717e-06
Iter: 1034 loss: 2.23444295e-06
Iter: 1035 loss: 2.24145015e-06
Iter: 1036 loss: 2.23443567e-06
Iter: 1037 loss: 2.23397569e-06
Iter: 1038 loss: 2.23320808e-06
Iter: 1039 loss: 2.23321513e-06
Iter: 1040 loss: 2.23228835e-06
Iter: 1041 loss: 2.23240181e-06
Iter: 1042 loss: 2.23157099e-06
Iter: 1043 loss: 2.2303775e-06
Iter: 1044 loss: 2.23225538e-06
Iter: 1045 loss: 2.22977042e-06
Iter: 1046 loss: 2.22834183e-06
Iter: 1047 loss: 2.23759957e-06
Iter: 1048 loss: 2.22823201e-06
Iter: 1049 loss: 2.22724134e-06
Iter: 1050 loss: 2.22708422e-06
Iter: 1051 loss: 2.22639937e-06
Iter: 1052 loss: 2.22519566e-06
Iter: 1053 loss: 2.23687493e-06
Iter: 1054 loss: 2.22516064e-06
Iter: 1055 loss: 2.2240763e-06
Iter: 1056 loss: 2.22573408e-06
Iter: 1057 loss: 2.22357198e-06
Iter: 1058 loss: 2.22252538e-06
Iter: 1059 loss: 2.22952758e-06
Iter: 1060 loss: 2.22244694e-06
Iter: 1061 loss: 2.22169774e-06
Iter: 1062 loss: 2.22057747e-06
Iter: 1063 loss: 2.22056724e-06
Iter: 1064 loss: 2.21953269e-06
Iter: 1065 loss: 2.21948289e-06
Iter: 1066 loss: 2.21898222e-06
Iter: 1067 loss: 2.21900245e-06
Iter: 1068 loss: 2.21866594e-06
Iter: 1069 loss: 2.21812775e-06
Iter: 1070 loss: 2.23134975e-06
Iter: 1071 loss: 2.21813207e-06
Iter: 1072 loss: 2.2172303e-06
Iter: 1073 loss: 2.21713503e-06
Iter: 1074 loss: 2.2164877e-06
Iter: 1075 loss: 2.21565506e-06
Iter: 1076 loss: 2.2152185e-06
Iter: 1077 loss: 2.21481969e-06
Iter: 1078 loss: 2.21325581e-06
Iter: 1079 loss: 2.22248764e-06
Iter: 1080 loss: 2.21297751e-06
Iter: 1081 loss: 2.21202617e-06
Iter: 1082 loss: 2.21150663e-06
Iter: 1083 loss: 2.21103846e-06
Iter: 1084 loss: 2.20989205e-06
Iter: 1085 loss: 2.20987727e-06
Iter: 1086 loss: 2.20899392e-06
Iter: 1087 loss: 2.20917923e-06
Iter: 1088 loss: 2.20841866e-06
Iter: 1089 loss: 2.20727043e-06
Iter: 1090 loss: 2.21509163e-06
Iter: 1091 loss: 2.207134e-06
Iter: 1092 loss: 2.20619586e-06
Iter: 1093 loss: 2.20574589e-06
Iter: 1094 loss: 2.20535321e-06
Iter: 1095 loss: 2.20420452e-06
Iter: 1096 loss: 2.21243363e-06
Iter: 1097 loss: 2.20410084e-06
Iter: 1098 loss: 2.20345464e-06
Iter: 1099 loss: 2.20343099e-06
Iter: 1100 loss: 2.20278525e-06
Iter: 1101 loss: 2.20272796e-06
Iter: 1102 loss: 2.20226934e-06
Iter: 1103 loss: 2.20163543e-06
Iter: 1104 loss: 2.20365905e-06
Iter: 1105 loss: 2.2014608e-06
Iter: 1106 loss: 2.20095535e-06
Iter: 1107 loss: 2.19964181e-06
Iter: 1108 loss: 2.21508344e-06
Iter: 1109 loss: 2.19955837e-06
Iter: 1110 loss: 2.19801268e-06
Iter: 1111 loss: 2.21101482e-06
Iter: 1112 loss: 2.19788512e-06
Iter: 1113 loss: 2.19701951e-06
Iter: 1114 loss: 2.19734875e-06
Iter: 1115 loss: 2.1964172e-06
Iter: 1116 loss: 2.19517301e-06
Iter: 1117 loss: 2.20143761e-06
Iter: 1118 loss: 2.19494177e-06
Iter: 1119 loss: 2.19410413e-06
Iter: 1120 loss: 2.19283629e-06
Iter: 1121 loss: 2.19282e-06
Iter: 1122 loss: 2.19202684e-06
Iter: 1123 loss: 2.19175604e-06
Iter: 1124 loss: 2.19108688e-06
Iter: 1125 loss: 2.19074172e-06
Iter: 1126 loss: 2.1903993e-06
Iter: 1127 loss: 2.18914579e-06
Iter: 1128 loss: 2.19323e-06
Iter: 1129 loss: 2.18878722e-06
Iter: 1130 loss: 2.18789091e-06
Iter: 1131 loss: 2.19032358e-06
Iter: 1132 loss: 2.18759e-06
Iter: 1133 loss: 2.18661876e-06
Iter: 1134 loss: 2.18660898e-06
Iter: 1135 loss: 2.18614127e-06
Iter: 1136 loss: 2.18570312e-06
Iter: 1137 loss: 2.18556806e-06
Iter: 1138 loss: 2.18486457e-06
Iter: 1139 loss: 2.18518517e-06
Iter: 1140 loss: 2.18441414e-06
Iter: 1141 loss: 2.18340438e-06
Iter: 1142 loss: 2.18408741e-06
Iter: 1143 loss: 2.18275409e-06
Iter: 1144 loss: 2.18159403e-06
Iter: 1145 loss: 2.18206469e-06
Iter: 1146 loss: 2.18084142e-06
Iter: 1147 loss: 2.17975617e-06
Iter: 1148 loss: 2.1893311e-06
Iter: 1149 loss: 2.17974366e-06
Iter: 1150 loss: 2.17858451e-06
Iter: 1151 loss: 2.17702018e-06
Iter: 1152 loss: 2.17693241e-06
Iter: 1153 loss: 2.17600291e-06
Iter: 1154 loss: 2.17586876e-06
Iter: 1155 loss: 2.17503202e-06
Iter: 1156 loss: 2.17422121e-06
Iter: 1157 loss: 2.17397701e-06
Iter: 1158 loss: 2.17302386e-06
Iter: 1159 loss: 2.17305296e-06
Iter: 1160 loss: 2.17227307e-06
Iter: 1161 loss: 2.17084062e-06
Iter: 1162 loss: 2.20331981e-06
Iter: 1163 loss: 2.17089337e-06
Iter: 1164 loss: 2.17148818e-06
Iter: 1165 loss: 2.17022534e-06
Iter: 1166 loss: 2.16968829e-06
Iter: 1167 loss: 2.16965145e-06
Iter: 1168 loss: 2.16930584e-06
Iter: 1169 loss: 2.1686792e-06
Iter: 1170 loss: 2.16763101e-06
Iter: 1171 loss: 2.19200592e-06
Iter: 1172 loss: 2.16760554e-06
Iter: 1173 loss: 2.16628132e-06
Iter: 1174 loss: 2.18439027e-06
Iter: 1175 loss: 2.16625108e-06
Iter: 1176 loss: 2.16565331e-06
Iter: 1177 loss: 2.16529156e-06
Iter: 1178 loss: 2.16500121e-06
Iter: 1179 loss: 2.16379931e-06
Iter: 1180 loss: 2.16467424e-06
Iter: 1181 loss: 2.16301942e-06
Iter: 1182 loss: 2.16186709e-06
Iter: 1183 loss: 2.16438548e-06
Iter: 1184 loss: 2.16135754e-06
Iter: 1185 loss: 2.16021863e-06
Iter: 1186 loss: 2.17001025e-06
Iter: 1187 loss: 2.16020953e-06
Iter: 1188 loss: 2.15931846e-06
Iter: 1189 loss: 2.15930959e-06
Iter: 1190 loss: 2.15860973e-06
Iter: 1191 loss: 2.15736236e-06
Iter: 1192 loss: 2.15998466e-06
Iter: 1193 loss: 2.15686669e-06
Iter: 1194 loss: 2.15599061e-06
Iter: 1195 loss: 2.16530475e-06
Iter: 1196 loss: 2.15598493e-06
Iter: 1197 loss: 2.15537921e-06
Iter: 1198 loss: 2.15555247e-06
Iter: 1199 loss: 2.15486489e-06
Iter: 1200 loss: 2.15429e-06
Iter: 1201 loss: 2.15422324e-06
Iter: 1202 loss: 2.15388172e-06
Iter: 1203 loss: 2.15322871e-06
Iter: 1204 loss: 2.16490139e-06
Iter: 1205 loss: 2.1532187e-06
Iter: 1206 loss: 2.15230534e-06
Iter: 1207 loss: 2.15186401e-06
Iter: 1208 loss: 2.15141517e-06
Iter: 1209 loss: 2.15100795e-06
Iter: 1210 loss: 2.15087721e-06
Iter: 1211 loss: 2.15036562e-06
Iter: 1212 loss: 2.14908096e-06
Iter: 1213 loss: 2.15991849e-06
Iter: 1214 loss: 2.14893635e-06
Iter: 1215 loss: 2.14778152e-06
Iter: 1216 loss: 2.16471176e-06
Iter: 1217 loss: 2.14777583e-06
Iter: 1218 loss: 2.14703e-06
Iter: 1219 loss: 2.14663351e-06
Iter: 1220 loss: 2.14630518e-06
Iter: 1221 loss: 2.14540023e-06
Iter: 1222 loss: 2.14539477e-06
Iter: 1223 loss: 2.14489455e-06
Iter: 1224 loss: 2.14424153e-06
Iter: 1225 loss: 2.14418537e-06
Iter: 1226 loss: 2.14342572e-06
Iter: 1227 loss: 2.14318061e-06
Iter: 1228 loss: 2.14272904e-06
Iter: 1229 loss: 2.14192437e-06
Iter: 1230 loss: 2.14448073e-06
Iter: 1231 loss: 2.1417502e-06
Iter: 1232 loss: 2.14151487e-06
Iter: 1233 loss: 2.14123702e-06
Iter: 1234 loss: 2.14096394e-06
Iter: 1235 loss: 2.14008878e-06
Iter: 1236 loss: 2.14585134e-06
Iter: 1237 loss: 2.13977955e-06
Iter: 1238 loss: 2.13870635e-06
Iter: 1239 loss: 2.14382339e-06
Iter: 1240 loss: 2.13859084e-06
Iter: 1241 loss: 2.13751764e-06
Iter: 1242 loss: 2.13745102e-06
Iter: 1243 loss: 2.13677322e-06
Iter: 1244 loss: 2.13562589e-06
Iter: 1245 loss: 2.13558906e-06
Iter: 1246 loss: 2.1350711e-06
Iter: 1247 loss: 2.13446174e-06
Iter: 1248 loss: 2.13442581e-06
Iter: 1249 loss: 2.1334e-06
Iter: 1250 loss: 2.13618614e-06
Iter: 1251 loss: 2.13308476e-06
Iter: 1252 loss: 2.13226394e-06
Iter: 1253 loss: 2.13326211e-06
Iter: 1254 loss: 2.1318574e-06
Iter: 1255 loss: 2.13068552e-06
Iter: 1256 loss: 2.13568796e-06
Iter: 1257 loss: 2.13039471e-06
Iter: 1258 loss: 2.12959094e-06
Iter: 1259 loss: 2.13605244e-06
Iter: 1260 loss: 2.12952546e-06
Iter: 1261 loss: 2.12878376e-06
Iter: 1262 loss: 2.12759437e-06
Iter: 1263 loss: 2.15843102e-06
Iter: 1264 loss: 2.12757573e-06
Iter: 1265 loss: 2.12701639e-06
Iter: 1266 loss: 2.12689065e-06
Iter: 1267 loss: 2.12620625e-06
Iter: 1268 loss: 2.13003432e-06
Iter: 1269 loss: 2.12610303e-06
Iter: 1270 loss: 2.12563077e-06
Iter: 1271 loss: 2.12446685e-06
Iter: 1272 loss: 2.13766407e-06
Iter: 1273 loss: 2.12437544e-06
Iter: 1274 loss: 2.12334453e-06
Iter: 1275 loss: 2.12897658e-06
Iter: 1276 loss: 2.12326358e-06
Iter: 1277 loss: 2.12238e-06
Iter: 1278 loss: 2.12557e-06
Iter: 1279 loss: 2.12217537e-06
Iter: 1280 loss: 2.12151645e-06
Iter: 1281 loss: 2.12689565e-06
Iter: 1282 loss: 2.12149803e-06
Iter: 1283 loss: 2.12090595e-06
Iter: 1284 loss: 2.12092573e-06
Iter: 1285 loss: 2.12049918e-06
Iter: 1286 loss: 2.11969723e-06
Iter: 1287 loss: 2.11941415e-06
Iter: 1288 loss: 2.11898123e-06
Iter: 1289 loss: 2.11809402e-06
Iter: 1290 loss: 2.12770965e-06
Iter: 1291 loss: 2.11807765e-06
Iter: 1292 loss: 2.11736779e-06
Iter: 1293 loss: 2.11866791e-06
Iter: 1294 loss: 2.11704378e-06
Iter: 1295 loss: 2.1161004e-06
Iter: 1296 loss: 2.11765018e-06
Iter: 1297 loss: 2.1156543e-06
Iter: 1298 loss: 2.11490374e-06
Iter: 1299 loss: 2.11468546e-06
Iter: 1300 loss: 2.11428687e-06
Iter: 1301 loss: 2.11446741e-06
Iter: 1302 loss: 2.11377119e-06
Iter: 1303 loss: 2.11354427e-06
Iter: 1304 loss: 2.11314227e-06
Iter: 1305 loss: 2.11311522e-06
Iter: 1306 loss: 2.11278893e-06
Iter: 1307 loss: 2.11192673e-06
Iter: 1308 loss: 2.12475e-06
Iter: 1309 loss: 2.11188535e-06
Iter: 1310 loss: 2.11116117e-06
Iter: 1311 loss: 2.11113502e-06
Iter: 1312 loss: 2.11061183e-06
Iter: 1313 loss: 2.11103406e-06
Iter: 1314 loss: 2.11029783e-06
Iter: 1315 loss: 2.10957842e-06
Iter: 1316 loss: 2.11354109e-06
Iter: 1317 loss: 2.10944017e-06
Iter: 1318 loss: 2.10906614e-06
Iter: 1319 loss: 2.10944586e-06
Iter: 1320 loss: 2.10882445e-06
Iter: 1321 loss: 2.10823555e-06
Iter: 1322 loss: 2.1083647e-06
Iter: 1323 loss: 2.10780695e-06
Iter: 1324 loss: 2.10716939e-06
Iter: 1325 loss: 2.11391625e-06
Iter: 1326 loss: 2.10713506e-06
Iter: 1327 loss: 2.10661938e-06
Iter: 1328 loss: 2.10816415e-06
Iter: 1329 loss: 2.1064493e-06
Iter: 1330 loss: 2.10596613e-06
Iter: 1331 loss: 2.10570875e-06
Iter: 1332 loss: 2.10552344e-06
Iter: 1333 loss: 2.10542566e-06
Iter: 1334 loss: 2.10526377e-06
Iter: 1335 loss: 2.10501912e-06
Iter: 1336 loss: 2.10469716e-06
Iter: 1337 loss: 2.10469079e-06
Iter: 1338 loss: 2.10428425e-06
Iter: 1339 loss: 2.10353255e-06
Iter: 1340 loss: 2.10353846e-06
Iter: 1341 loss: 2.10299277e-06
Iter: 1342 loss: 2.10300095e-06
Iter: 1343 loss: 2.10258327e-06
Iter: 1344 loss: 2.10206645e-06
Iter: 1345 loss: 2.102e-06
Iter: 1346 loss: 2.10131043e-06
Iter: 1347 loss: 2.10130656e-06
Iter: 1348 loss: 2.10094731e-06
Iter: 1349 loss: 2.10059443e-06
Iter: 1350 loss: 2.10052713e-06
Iter: 1351 loss: 2.09984e-06
Iter: 1352 loss: 2.10112444e-06
Iter: 1353 loss: 2.09951691e-06
Iter: 1354 loss: 2.09912287e-06
Iter: 1355 loss: 2.10219741e-06
Iter: 1356 loss: 2.09906057e-06
Iter: 1357 loss: 2.09855034e-06
Iter: 1358 loss: 2.09938753e-06
Iter: 1359 loss: 2.09833502e-06
Iter: 1360 loss: 2.09790733e-06
Iter: 1361 loss: 2.09867244e-06
Iter: 1362 loss: 2.09772725e-06
Iter: 1363 loss: 2.09745758e-06
Iter: 1364 loss: 2.09745031e-06
Iter: 1365 loss: 2.09717291e-06
Iter: 1366 loss: 2.0970665e-06
Iter: 1367 loss: 2.09691666e-06
Iter: 1368 loss: 2.09657264e-06
Iter: 1369 loss: 2.09621953e-06
Iter: 1370 loss: 2.09615428e-06
Iter: 1371 loss: 2.09568839e-06
Iter: 1372 loss: 2.09678774e-06
Iter: 1373 loss: 2.09550103e-06
Iter: 1374 loss: 2.09482278e-06
Iter: 1375 loss: 2.09567543e-06
Iter: 1376 loss: 2.09447307e-06
Iter: 1377 loss: 2.09416089e-06
Iter: 1378 loss: 2.09413429e-06
Iter: 1379 loss: 2.093901e-06
Iter: 1380 loss: 2.09342124e-06
Iter: 1381 loss: 2.10410235e-06
Iter: 1382 loss: 2.09343261e-06
Iter: 1383 loss: 2.0927182e-06
Iter: 1384 loss: 2.09542895e-06
Iter: 1385 loss: 2.09262e-06
Iter: 1386 loss: 2.09214795e-06
Iter: 1387 loss: 2.0932207e-06
Iter: 1388 loss: 2.09195377e-06
Iter: 1389 loss: 2.09139671e-06
Iter: 1390 loss: 2.09413656e-06
Iter: 1391 loss: 2.09129121e-06
Iter: 1392 loss: 2.09090922e-06
Iter: 1393 loss: 2.09216182e-06
Iter: 1394 loss: 2.09077734e-06
Iter: 1395 loss: 2.09043969e-06
Iter: 1396 loss: 2.09261225e-06
Iter: 1397 loss: 2.09040559e-06
Iter: 1398 loss: 2.09006657e-06
Iter: 1399 loss: 2.09082918e-06
Iter: 1400 loss: 2.08990241e-06
Iter: 1401 loss: 2.08962228e-06
Iter: 1402 loss: 2.08939e-06
Iter: 1403 loss: 2.08932488e-06
Iter: 1404 loss: 2.08890037e-06
Iter: 1405 loss: 2.08934216e-06
Iter: 1406 loss: 2.08862275e-06
Iter: 1407 loss: 2.08809615e-06
Iter: 1408 loss: 2.08867277e-06
Iter: 1409 loss: 2.08784377e-06
Iter: 1410 loss: 2.08739857e-06
Iter: 1411 loss: 2.08737765e-06
Iter: 1412 loss: 2.08702204e-06
Iter: 1413 loss: 2.08678466e-06
Iter: 1414 loss: 2.08663141e-06
Iter: 1415 loss: 2.08592246e-06
Iter: 1416 loss: 2.08719871e-06
Iter: 1417 loss: 2.085598e-06
Iter: 1418 loss: 2.0850789e-06
Iter: 1419 loss: 2.08524852e-06
Iter: 1420 loss: 2.08474012e-06
Iter: 1421 loss: 2.08385904e-06
Iter: 1422 loss: 2.08943e-06
Iter: 1423 loss: 2.08377332e-06
Iter: 1424 loss: 2.08338247e-06
Iter: 1425 loss: 2.08745814e-06
Iter: 1426 loss: 2.08336883e-06
Iter: 1427 loss: 2.083048e-06
Iter: 1428 loss: 2.08370375e-06
Iter: 1429 loss: 2.08298047e-06
Iter: 1430 loss: 2.0825064e-06
Iter: 1431 loss: 2.08432789e-06
Iter: 1432 loss: 2.08243432e-06
Iter: 1433 loss: 2.08216716e-06
Iter: 1434 loss: 2.08205233e-06
Iter: 1435 loss: 2.08193933e-06
Iter: 1436 loss: 2.08156416e-06
Iter: 1437 loss: 2.08134952e-06
Iter: 1438 loss: 2.08124152e-06
Iter: 1439 loss: 2.08059055e-06
Iter: 1440 loss: 2.08221309e-06
Iter: 1441 loss: 2.08046959e-06
Iter: 1442 loss: 2.0797861e-06
Iter: 1443 loss: 2.08201254e-06
Iter: 1444 loss: 2.07960238e-06
Iter: 1445 loss: 2.07902713e-06
Iter: 1446 loss: 2.08439178e-06
Iter: 1447 loss: 2.07903599e-06
Iter: 1448 loss: 2.07859784e-06
Iter: 1449 loss: 2.07824723e-06
Iter: 1450 loss: 2.07812764e-06
Iter: 1451 loss: 2.07750873e-06
Iter: 1452 loss: 2.0798152e-06
Iter: 1453 loss: 2.07737321e-06
Iter: 1454 loss: 2.07666562e-06
Iter: 1455 loss: 2.07691505e-06
Iter: 1456 loss: 2.07622065e-06
Iter: 1457 loss: 2.07588073e-06
Iter: 1458 loss: 2.07578114e-06
Iter: 1459 loss: 2.07549465e-06
Iter: 1460 loss: 2.07579592e-06
Iter: 1461 loss: 2.07537914e-06
Iter: 1462 loss: 2.07491394e-06
Iter: 1463 loss: 2.0775085e-06
Iter: 1464 loss: 2.07484709e-06
Iter: 1465 loss: 2.07454946e-06
Iter: 1466 loss: 2.0743455e-06
Iter: 1467 loss: 2.07424819e-06
Iter: 1468 loss: 2.07391076e-06
Iter: 1469 loss: 2.07347011e-06
Iter: 1470 loss: 2.07342782e-06
Iter: 1471 loss: 2.07280027e-06
Iter: 1472 loss: 2.07801168e-06
Iter: 1473 loss: 2.07275934e-06
Iter: 1474 loss: 2.07228686e-06
Iter: 1475 loss: 2.07271387e-06
Iter: 1476 loss: 2.07203129e-06
Iter: 1477 loss: 2.07156563e-06
Iter: 1478 loss: 2.07708808e-06
Iter: 1479 loss: 2.07155404e-06
Iter: 1480 loss: 2.07122685e-06
Iter: 1481 loss: 2.07135713e-06
Iter: 1482 loss: 2.07101539e-06
Iter: 1483 loss: 2.07054313e-06
Iter: 1484 loss: 2.07006224e-06
Iter: 1485 loss: 2.06994e-06
Iter: 1486 loss: 2.0692371e-06
Iter: 1487 loss: 2.07454787e-06
Iter: 1488 loss: 2.06914319e-06
Iter: 1489 loss: 2.06865093e-06
Iter: 1490 loss: 2.0715006e-06
Iter: 1491 loss: 2.06854838e-06
Iter: 1492 loss: 2.06812683e-06
Iter: 1493 loss: 2.07104813e-06
Iter: 1494 loss: 2.06808409e-06
Iter: 1495 loss: 2.0678217e-06
Iter: 1496 loss: 2.07008088e-06
Iter: 1497 loss: 2.06781351e-06
Iter: 1498 loss: 2.06755408e-06
Iter: 1499 loss: 2.06702271e-06
Iter: 1500 loss: 2.06707591e-06
Iter: 1501 loss: 2.06670802e-06
Iter: 1502 loss: 2.06716072e-06
Iter: 1503 loss: 2.06649474e-06
Iter: 1504 loss: 2.06596519e-06
Iter: 1505 loss: 2.06605773e-06
Iter: 1506 loss: 2.0656139e-06
Iter: 1507 loss: 2.06515233e-06
Iter: 1508 loss: 2.07074936e-06
Iter: 1509 loss: 2.06514324e-06
Iter: 1510 loss: 2.06473328e-06
Iter: 1511 loss: 2.06487789e-06
Iter: 1512 loss: 2.06448431e-06
Iter: 1513 loss: 2.06394134e-06
Iter: 1514 loss: 2.06815548e-06
Iter: 1515 loss: 2.06394316e-06
Iter: 1516 loss: 2.06358663e-06
Iter: 1517 loss: 2.06296e-06
Iter: 1518 loss: 2.07651146e-06
Iter: 1519 loss: 2.06295385e-06
Iter: 1520 loss: 2.06211735e-06
Iter: 1521 loss: 2.06897153e-06
Iter: 1522 loss: 2.06210734e-06
Iter: 1523 loss: 2.06162485e-06
Iter: 1524 loss: 2.06201094e-06
Iter: 1525 loss: 2.06143113e-06
Iter: 1526 loss: 2.0609657e-06
Iter: 1527 loss: 2.06098389e-06
Iter: 1528 loss: 2.06074446e-06
Iter: 1529 loss: 2.06184177e-06
Iter: 1530 loss: 2.06066647e-06
Iter: 1531 loss: 2.06038158e-06
Iter: 1532 loss: 2.060307e-06
Iter: 1533 loss: 2.0601733e-06
Iter: 1534 loss: 2.05981314e-06
Iter: 1535 loss: 2.05980541e-06
Iter: 1536 loss: 2.05953279e-06
Iter: 1537 loss: 2.05910078e-06
Iter: 1538 loss: 2.05933088e-06
Iter: 1539 loss: 2.05881e-06
Iter: 1540 loss: 2.05829383e-06
Iter: 1541 loss: 2.06291907e-06
Iter: 1542 loss: 2.05826973e-06
Iter: 1543 loss: 2.05789252e-06
Iter: 1544 loss: 2.05765e-06
Iter: 1545 loss: 2.05749393e-06
Iter: 1546 loss: 2.05688548e-06
Iter: 1547 loss: 2.06508321e-06
Iter: 1548 loss: 2.05687888e-06
Iter: 1549 loss: 2.0565235e-06
Iter: 1550 loss: 2.05625656e-06
Iter: 1551 loss: 2.05615333e-06
Iter: 1552 loss: 2.05548645e-06
Iter: 1553 loss: 2.05685637e-06
Iter: 1554 loss: 2.05525453e-06
Iter: 1555 loss: 2.05477568e-06
Iter: 1556 loss: 2.05613105e-06
Iter: 1557 loss: 2.05461492e-06
Iter: 1558 loss: 2.05420884e-06
Iter: 1559 loss: 2.05939637e-06
Iter: 1560 loss: 2.0542102e-06
Iter: 1561 loss: 2.05393235e-06
Iter: 1562 loss: 2.05669471e-06
Iter: 1563 loss: 2.05392689e-06
Iter: 1564 loss: 2.05367951e-06
Iter: 1565 loss: 2.05366337e-06
Iter: 1566 loss: 2.0534917e-06
Iter: 1567 loss: 2.05317656e-06
Iter: 1568 loss: 2.05253809e-06
Iter: 1569 loss: 2.05255355e-06
Iter: 1570 loss: 2.05203969e-06
Iter: 1571 loss: 2.05578544e-06
Iter: 1572 loss: 2.05201491e-06
Iter: 1573 loss: 2.05150673e-06
Iter: 1574 loss: 2.05173319e-06
Iter: 1575 loss: 2.05121614e-06
Iter: 1576 loss: 2.05061906e-06
Iter: 1577 loss: 2.05323204e-06
Iter: 1578 loss: 2.05047309e-06
Iter: 1579 loss: 2.04978596e-06
Iter: 1580 loss: 2.05230162e-06
Iter: 1581 loss: 2.049665e-06
Iter: 1582 loss: 2.04908156e-06
Iter: 1583 loss: 2.05172501e-06
Iter: 1584 loss: 2.04901062e-06
Iter: 1585 loss: 2.0486109e-06
Iter: 1586 loss: 2.04809908e-06
Iter: 1587 loss: 2.04802382e-06
Iter: 1588 loss: 2.04730622e-06
Iter: 1589 loss: 2.05324113e-06
Iter: 1590 loss: 2.0472778e-06
Iter: 1591 loss: 2.04682e-06
Iter: 1592 loss: 2.04731987e-06
Iter: 1593 loss: 2.04653793e-06
Iter: 1594 loss: 2.04616299e-06
Iter: 1595 loss: 2.0461016e-06
Iter: 1596 loss: 2.04585422e-06
Iter: 1597 loss: 2.0462046e-06
Iter: 1598 loss: 2.04565094e-06
Iter: 1599 loss: 2.04537355e-06
Iter: 1600 loss: 2.04478283e-06
Iter: 1601 loss: 2.05417814e-06
Iter: 1602 loss: 2.04471939e-06
Iter: 1603 loss: 2.0440425e-06
Iter: 1604 loss: 2.04914454e-06
Iter: 1605 loss: 2.04400681e-06
Iter: 1606 loss: 2.0435134e-06
Iter: 1607 loss: 2.04340358e-06
Iter: 1608 loss: 2.04310277e-06
Iter: 1609 loss: 2.04223397e-06
Iter: 1610 loss: 2.04584057e-06
Iter: 1611 loss: 2.0420498e-06
Iter: 1612 loss: 2.04148091e-06
Iter: 1613 loss: 2.04504e-06
Iter: 1614 loss: 2.04141725e-06
Iter: 1615 loss: 2.0408711e-06
Iter: 1616 loss: 2.04217963e-06
Iter: 1617 loss: 2.04070534e-06
Iter: 1618 loss: 2.04025082e-06
Iter: 1619 loss: 2.04056414e-06
Iter: 1620 loss: 2.03989862e-06
Iter: 1621 loss: 2.03927425e-06
Iter: 1622 loss: 2.04012122e-06
Iter: 1623 loss: 2.03897616e-06
Iter: 1624 loss: 2.0383427e-06
Iter: 1625 loss: 2.04133357e-06
Iter: 1626 loss: 2.03820287e-06
Iter: 1627 loss: 2.03777813e-06
Iter: 1628 loss: 2.03775608e-06
Iter: 1629 loss: 2.03742297e-06
Iter: 1630 loss: 2.0379216e-06
Iter: 1631 loss: 2.03717173e-06
Iter: 1632 loss: 2.03682885e-06
Iter: 1633 loss: 2.03670652e-06
Iter: 1634 loss: 2.03654213e-06
Iter: 1635 loss: 2.03603713e-06
Iter: 1636 loss: 2.03563786e-06
Iter: 1637 loss: 2.03543027e-06
Iter: 1638 loss: 2.03485934e-06
Iter: 1639 loss: 2.03937157e-06
Iter: 1640 loss: 2.03483614e-06
Iter: 1641 loss: 2.03423542e-06
Iter: 1642 loss: 2.03431e-06
Iter: 1643 loss: 2.03371178e-06
Iter: 1644 loss: 2.03327318e-06
Iter: 1645 loss: 2.03326476e-06
Iter: 1646 loss: 2.03279251e-06
Iter: 1647 loss: 2.03268883e-06
Iter: 1648 loss: 2.03241643e-06
Iter: 1649 loss: 2.03168452e-06
Iter: 1650 loss: 2.0338889e-06
Iter: 1651 loss: 2.03158106e-06
Iter: 1652 loss: 2.03106174e-06
Iter: 1653 loss: 2.03085256e-06
Iter: 1654 loss: 2.03063792e-06
Iter: 1655 loss: 2.0299708e-06
Iter: 1656 loss: 2.03525087e-06
Iter: 1657 loss: 2.02992169e-06
Iter: 1658 loss: 2.02952333e-06
Iter: 1659 loss: 2.03457512e-06
Iter: 1660 loss: 2.0295065e-06
Iter: 1661 loss: 2.02906813e-06
Iter: 1662 loss: 2.02963383e-06
Iter: 1663 loss: 2.02886758e-06
Iter: 1664 loss: 2.02842148e-06
Iter: 1665 loss: 2.02867477e-06
Iter: 1666 loss: 2.02822616e-06
Iter: 1667 loss: 2.02770843e-06
Iter: 1668 loss: 2.02703632e-06
Iter: 1669 loss: 2.02701949e-06
Iter: 1670 loss: 2.02636284e-06
Iter: 1671 loss: 2.03462673e-06
Iter: 1672 loss: 2.02634715e-06
Iter: 1673 loss: 2.02576939e-06
Iter: 1674 loss: 2.02535557e-06
Iter: 1675 loss: 2.02513274e-06
Iter: 1676 loss: 2.02452338e-06
Iter: 1677 loss: 2.02449883e-06
Iter: 1678 loss: 2.02406409e-06
Iter: 1679 loss: 2.02461888e-06
Iter: 1680 loss: 2.02378442e-06
Iter: 1681 loss: 2.02314595e-06
Iter: 1682 loss: 2.02523529e-06
Iter: 1683 loss: 2.02291062e-06
Iter: 1684 loss: 2.02254728e-06
Iter: 1685 loss: 2.02209958e-06
Iter: 1686 loss: 2.02200749e-06
Iter: 1687 loss: 2.02117189e-06
Iter: 1688 loss: 2.02543447e-06
Iter: 1689 loss: 2.02099e-06
Iter: 1690 loss: 2.02077285e-06
Iter: 1691 loss: 2.02061074e-06
Iter: 1692 loss: 2.02034698e-06
Iter: 1693 loss: 2.02037677e-06
Iter: 1694 loss: 2.02005253e-06
Iter: 1695 loss: 2.01967487e-06
Iter: 1696 loss: 2.02015963e-06
Iter: 1697 loss: 2.01954231e-06
Iter: 1698 loss: 2.01903572e-06
Iter: 1699 loss: 2.01889816e-06
Iter: 1700 loss: 2.01860325e-06
Iter: 1701 loss: 2.01804596e-06
Iter: 1702 loss: 2.02031811e-06
Iter: 1703 loss: 2.01793728e-06
Iter: 1704 loss: 2.01738658e-06
Iter: 1705 loss: 2.01743296e-06
Iter: 1706 loss: 2.01696957e-06
Iter: 1707 loss: 2.01628291e-06
Iter: 1708 loss: 2.0224295e-06
Iter: 1709 loss: 2.01622743e-06
Iter: 1710 loss: 2.01577973e-06
Iter: 1711 loss: 2.01763532e-06
Iter: 1712 loss: 2.01566e-06
Iter: 1713 loss: 2.01508965e-06
Iter: 1714 loss: 2.01530702e-06
Iter: 1715 loss: 2.01471539e-06
Iter: 1716 loss: 2.01420312e-06
Iter: 1717 loss: 2.01646799e-06
Iter: 1718 loss: 2.01410762e-06
Iter: 1719 loss: 2.01354419e-06
Iter: 1720 loss: 2.01305193e-06
Iter: 1721 loss: 2.01291232e-06
Iter: 1722 loss: 2.01287025e-06
Iter: 1723 loss: 2.0126563e-06
Iter: 1724 loss: 2.0123025e-06
Iter: 1725 loss: 2.01275907e-06
Iter: 1726 loss: 2.01215585e-06
Iter: 1727 loss: 2.01185617e-06
Iter: 1728 loss: 2.01165835e-06
Iter: 1729 loss: 2.01149624e-06
Iter: 1730 loss: 2.01103671e-06
Iter: 1731 loss: 2.01192847e-06
Iter: 1732 loss: 2.0108746e-06
Iter: 1733 loss: 2.01033663e-06
Iter: 1734 loss: 2.0101179e-06
Iter: 1735 loss: 2.0098571e-06
Iter: 1736 loss: 2.00911313e-06
Iter: 1737 loss: 2.01261128e-06
Iter: 1738 loss: 2.00905856e-06
Iter: 1739 loss: 2.00842487e-06
Iter: 1740 loss: 2.01151238e-06
Iter: 1741 loss: 2.00828686e-06
Iter: 1742 loss: 2.00785962e-06
Iter: 1743 loss: 2.00961949e-06
Iter: 1744 loss: 2.00779459e-06
Iter: 1745 loss: 2.00721433e-06
Iter: 1746 loss: 2.00767727e-06
Iter: 1747 loss: 2.00689146e-06
Iter: 1748 loss: 2.00641898e-06
Iter: 1749 loss: 2.00888508e-06
Iter: 1750 loss: 2.00633917e-06
Iter: 1751 loss: 2.00589807e-06
Iter: 1752 loss: 2.00522913e-06
Iter: 1753 loss: 2.00523709e-06
Iter: 1754 loss: 2.00471345e-06
Iter: 1755 loss: 2.00470231e-06
Iter: 1756 loss: 2.00425575e-06
Iter: 1757 loss: 2.00910245e-06
Iter: 1758 loss: 2.00428e-06
Iter: 1759 loss: 2.00403701e-06
Iter: 1760 loss: 2.00351315e-06
Iter: 1761 loss: 2.01131229e-06
Iter: 1762 loss: 2.00345426e-06
Iter: 1763 loss: 2.00283534e-06
Iter: 1764 loss: 2.00690465e-06
Iter: 1765 loss: 2.00282238e-06
Iter: 1766 loss: 2.00243221e-06
Iter: 1767 loss: 2.00187219e-06
Iter: 1768 loss: 2.0018033e-06
Iter: 1769 loss: 2.00114096e-06
Iter: 1770 loss: 2.00755494e-06
Iter: 1771 loss: 2.00106956e-06
Iter: 1772 loss: 2.00063619e-06
Iter: 1773 loss: 2.00098475e-06
Iter: 1774 loss: 2.00034765e-06
Iter: 1775 loss: 1.99976898e-06
Iter: 1776 loss: 2.00463228e-06
Iter: 1777 loss: 1.99975193e-06
Iter: 1778 loss: 1.99932902e-06
Iter: 1779 loss: 1.9995523e-06
Iter: 1780 loss: 1.99898204e-06
Iter: 1781 loss: 1.99836745e-06
Iter: 1782 loss: 2.00052682e-06
Iter: 1783 loss: 1.99823353e-06
Iter: 1784 loss: 1.9977233e-06
Iter: 1785 loss: 1.99725332e-06
Iter: 1786 loss: 1.99716669e-06
Iter: 1787 loss: 1.99651186e-06
Iter: 1788 loss: 1.99649662e-06
Iter: 1789 loss: 1.99627675e-06
Iter: 1790 loss: 1.99627448e-06
Iter: 1791 loss: 1.99607075e-06
Iter: 1792 loss: 1.99553824e-06
Iter: 1793 loss: 2.00087834e-06
Iter: 1794 loss: 1.99547e-06
Iter: 1795 loss: 1.99495412e-06
Iter: 1796 loss: 1.99871374e-06
Iter: 1797 loss: 1.99492115e-06
Iter: 1798 loss: 1.99458646e-06
Iter: 1799 loss: 1.99419674e-06
Iter: 1800 loss: 1.99416013e-06
Iter: 1801 loss: 1.99344186e-06
Iter: 1802 loss: 1.99594592e-06
Iter: 1803 loss: 1.99333613e-06
Iter: 1804 loss: 1.99285364e-06
Iter: 1805 loss: 1.99356646e-06
Iter: 1806 loss: 1.99263536e-06
Iter: 1807 loss: 1.99201077e-06
Iter: 1808 loss: 1.99424062e-06
Iter: 1809 loss: 1.99187571e-06
Iter: 1810 loss: 1.99136707e-06
Iter: 1811 loss: 1.99452916e-06
Iter: 1812 loss: 1.99129954e-06
Iter: 1813 loss: 1.99088799e-06
Iter: 1814 loss: 1.99135502e-06
Iter: 1815 loss: 1.99060969e-06
Iter: 1816 loss: 1.99017677e-06
Iter: 1817 loss: 1.99094529e-06
Iter: 1818 loss: 1.98996872e-06
Iter: 1819 loss: 1.98939756e-06
Iter: 1820 loss: 1.99024475e-06
Iter: 1821 loss: 1.9891695e-06
Iter: 1822 loss: 1.9892384e-06
Iter: 1823 loss: 1.98892735e-06
Iter: 1824 loss: 1.98872544e-06
Iter: 1825 loss: 1.98844305e-06
Iter: 1826 loss: 1.98842122e-06
Iter: 1827 loss: 1.98803923e-06
Iter: 1828 loss: 1.98793464e-06
Iter: 1829 loss: 1.9877225e-06
Iter: 1830 loss: 1.98723842e-06
Iter: 1831 loss: 1.98992029e-06
Iter: 1832 loss: 1.9871286e-06
Iter: 1833 loss: 1.9867457e-06
Iter: 1834 loss: 1.98606949e-06
Iter: 1835 loss: 1.98605267e-06
Iter: 1836 loss: 1.98543148e-06
Iter: 1837 loss: 1.99377564e-06
Iter: 1838 loss: 1.98546354e-06
Iter: 1839 loss: 1.9848892e-06
Iter: 1840 loss: 1.98446469e-06
Iter: 1841 loss: 1.98430143e-06
Iter: 1842 loss: 1.98383827e-06
Iter: 1843 loss: 1.98372754e-06
Iter: 1844 loss: 1.98342013e-06
Iter: 1845 loss: 1.98297266e-06
Iter: 1846 loss: 1.98291377e-06
Iter: 1847 loss: 1.98224143e-06
Iter: 1848 loss: 1.98518319e-06
Iter: 1849 loss: 1.98205703e-06
Iter: 1850 loss: 1.98147973e-06
Iter: 1851 loss: 1.98120051e-06
Iter: 1852 loss: 1.98095836e-06
Iter: 1853 loss: 1.98106363e-06
Iter: 1854 loss: 1.98054431e-06
Iter: 1855 loss: 1.98030148e-06
Iter: 1856 loss: 1.97992813e-06
Iter: 1857 loss: 1.979937e-06
Iter: 1858 loss: 1.9794154e-06
Iter: 1859 loss: 1.97947111e-06
Iter: 1860 loss: 1.97909321e-06
Iter: 1861 loss: 1.97840723e-06
Iter: 1862 loss: 1.98048497e-06
Iter: 1863 loss: 1.97818122e-06
Iter: 1864 loss: 1.97739837e-06
Iter: 1865 loss: 1.97611189e-06
Iter: 1866 loss: 1.97611553e-06
Iter: 1867 loss: 1.97507279e-06
Iter: 1868 loss: 1.97503232e-06
Iter: 1869 loss: 1.97428653e-06
Iter: 1870 loss: 1.97381451e-06
Iter: 1871 loss: 1.97351301e-06
Iter: 1872 loss: 1.97281815e-06
Iter: 1873 loss: 1.97275267e-06
Iter: 1874 loss: 1.97214e-06
Iter: 1875 loss: 1.9718766e-06
Iter: 1876 loss: 1.97151257e-06
Iter: 1877 loss: 1.97068584e-06
Iter: 1878 loss: 1.97534496e-06
Iter: 1879 loss: 1.97055306e-06
Iter: 1880 loss: 1.97004715e-06
Iter: 1881 loss: 1.96997939e-06
Iter: 1882 loss: 1.96964629e-06
Iter: 1883 loss: 1.96928727e-06
Iter: 1884 loss: 1.96919291e-06
Iter: 1885 loss: 1.96884776e-06
Iter: 1886 loss: 1.96898532e-06
Iter: 1887 loss: 1.96867086e-06
Iter: 1888 loss: 1.96830433e-06
Iter: 1889 loss: 1.96796645e-06
Iter: 1890 loss: 1.96791962e-06
Iter: 1891 loss: 1.96736823e-06
Iter: 1892 loss: 1.97214695e-06
Iter: 1893 loss: 1.96731344e-06
Iter: 1894 loss: 1.96683663e-06
Iter: 1895 loss: 1.96662063e-06
Iter: 1896 loss: 1.96637302e-06
Iter: 1897 loss: 1.96585097e-06
Iter: 1898 loss: 1.97064651e-06
Iter: 1899 loss: 1.96574774e-06
Iter: 1900 loss: 1.96537621e-06
Iter: 1901 loss: 1.96502015e-06
Iter: 1902 loss: 1.96488645e-06
Iter: 1903 loss: 1.96422957e-06
Iter: 1904 loss: 1.97103304e-06
Iter: 1905 loss: 1.96416431e-06
Iter: 1906 loss: 1.96372298e-06
Iter: 1907 loss: 1.96546625e-06
Iter: 1908 loss: 1.96361566e-06
Iter: 1909 loss: 1.96301e-06
Iter: 1910 loss: 1.96247038e-06
Iter: 1911 loss: 1.96229576e-06
Iter: 1912 loss: 1.96153974e-06
Iter: 1913 loss: 1.96366227e-06
Iter: 1914 loss: 1.96133988e-06
Iter: 1915 loss: 1.96048609e-06
Iter: 1916 loss: 1.96643259e-06
Iter: 1917 loss: 1.96036535e-06
Iter: 1918 loss: 1.95969187e-06
Iter: 1919 loss: 1.96677411e-06
Iter: 1920 loss: 1.95967255e-06
Iter: 1921 loss: 1.95937719e-06
Iter: 1922 loss: 1.95858161e-06
Iter: 1923 loss: 1.968878e-06
Iter: 1924 loss: 1.95855773e-06
Iter: 1925 loss: 1.95766142e-06
Iter: 1926 loss: 1.96100177e-06
Iter: 1927 loss: 1.95743087e-06
Iter: 1928 loss: 1.95671214e-06
Iter: 1929 loss: 1.95961957e-06
Iter: 1930 loss: 1.95649136e-06
Iter: 1931 loss: 1.95574239e-06
Iter: 1932 loss: 1.95553685e-06
Iter: 1933 loss: 1.95514394e-06
Iter: 1934 loss: 1.95400298e-06
Iter: 1935 loss: 1.9553022e-06
Iter: 1936 loss: 1.9534898e-06
Iter: 1937 loss: 1.9521226e-06
Iter: 1938 loss: 1.96469455e-06
Iter: 1939 loss: 1.95205348e-06
Iter: 1940 loss: 1.95137659e-06
Iter: 1941 loss: 1.95481471e-06
Iter: 1942 loss: 1.95133225e-06
Iter: 1943 loss: 1.95059056e-06
Iter: 1944 loss: 1.95014627e-06
Iter: 1945 loss: 1.94987092e-06
Iter: 1946 loss: 1.94897984e-06
Iter: 1947 loss: 1.95391476e-06
Iter: 1948 loss: 1.94885479e-06
Iter: 1949 loss: 1.94803e-06
Iter: 1950 loss: 1.94763811e-06
Iter: 1951 loss: 1.94729591e-06
Iter: 1952 loss: 1.94749691e-06
Iter: 1953 loss: 1.94676431e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/300_300_300_1
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0
+ date
Mon Oct 26 10:51:21 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 0 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826cf11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826cfbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826da4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826da9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826cd6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826cd6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826cd61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826cd66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826cd6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826beca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826bec730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826b9b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826b7d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826b7da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826b858c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826b7de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826b85b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826aac730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826b479d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826a689d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826a687b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826a48950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18269f4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1826a1ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18269c1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18269c1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18269c1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f182692e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f182695f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1810494d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1810457620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1810453f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18104536a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1810431620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18103cb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1810443620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.5308144e-06
Iter: 2 loss: 6.07599e-06
Iter: 3 loss: 6.04770275e-06
Iter: 4 loss: 5.45920557e-06
Iter: 5 loss: 6.39580276e-06
Iter: 6 loss: 5.18528e-06
Iter: 7 loss: 4.92381e-06
Iter: 8 loss: 4.74401e-06
Iter: 9 loss: 4.64910318e-06
Iter: 10 loss: 4.55820191e-06
Iter: 11 loss: 4.47308958e-06
Iter: 12 loss: 4.37018025e-06
Iter: 13 loss: 4.34228377e-06
Iter: 14 loss: 4.27883697e-06
Iter: 15 loss: 4.18472973e-06
Iter: 16 loss: 4.04480897e-06
Iter: 17 loss: 4.04162529e-06
Iter: 18 loss: 4.08013102e-06
Iter: 19 loss: 3.96612813e-06
Iter: 20 loss: 3.92020411e-06
Iter: 21 loss: 3.77629794e-06
Iter: 22 loss: 4.0065247e-06
Iter: 23 loss: 3.67691382e-06
Iter: 24 loss: 3.50532764e-06
Iter: 25 loss: 5.99631858e-06
Iter: 26 loss: 3.50509868e-06
Iter: 27 loss: 3.37298411e-06
Iter: 28 loss: 3.87093814e-06
Iter: 29 loss: 3.3411925e-06
Iter: 30 loss: 3.28279179e-06
Iter: 31 loss: 3.32171567e-06
Iter: 32 loss: 3.24597613e-06
Iter: 33 loss: 3.20018489e-06
Iter: 34 loss: 3.11000758e-06
Iter: 35 loss: 4.89261765e-06
Iter: 36 loss: 3.10911446e-06
Iter: 37 loss: 3.04274818e-06
Iter: 38 loss: 3.62251831e-06
Iter: 39 loss: 3.03923639e-06
Iter: 40 loss: 2.98256145e-06
Iter: 41 loss: 3.00733245e-06
Iter: 42 loss: 2.94390884e-06
Iter: 43 loss: 2.90515686e-06
Iter: 44 loss: 2.90429443e-06
Iter: 45 loss: 2.85711803e-06
Iter: 46 loss: 2.85683109e-06
Iter: 47 loss: 2.81930602e-06
Iter: 48 loss: 2.75977186e-06
Iter: 49 loss: 2.74886133e-06
Iter: 50 loss: 2.70866076e-06
Iter: 51 loss: 2.66619418e-06
Iter: 52 loss: 3.21071479e-06
Iter: 53 loss: 2.66593315e-06
Iter: 54 loss: 2.60760021e-06
Iter: 55 loss: 2.52983045e-06
Iter: 56 loss: 2.52543214e-06
Iter: 57 loss: 2.46504896e-06
Iter: 58 loss: 2.92746131e-06
Iter: 59 loss: 2.46056698e-06
Iter: 60 loss: 2.396936e-06
Iter: 61 loss: 2.5942245e-06
Iter: 62 loss: 2.37839663e-06
Iter: 63 loss: 2.33828405e-06
Iter: 64 loss: 2.65181097e-06
Iter: 65 loss: 2.33546643e-06
Iter: 66 loss: 2.30478872e-06
Iter: 67 loss: 2.32973707e-06
Iter: 68 loss: 2.28643512e-06
Iter: 69 loss: 2.26777365e-06
Iter: 70 loss: 2.23804864e-06
Iter: 71 loss: 2.2377626e-06
Iter: 72 loss: 2.20211837e-06
Iter: 73 loss: 2.48144079e-06
Iter: 74 loss: 2.19959338e-06
Iter: 75 loss: 2.17793104e-06
Iter: 76 loss: 2.18150808e-06
Iter: 77 loss: 2.16162243e-06
Iter: 78 loss: 2.12514283e-06
Iter: 79 loss: 2.45717092e-06
Iter: 80 loss: 2.12346549e-06
Iter: 81 loss: 2.08747406e-06
Iter: 82 loss: 2.07780658e-06
Iter: 83 loss: 2.05557694e-06
Iter: 84 loss: 2.02217916e-06
Iter: 85 loss: 2.01469743e-06
Iter: 86 loss: 1.99305259e-06
Iter: 87 loss: 1.98897101e-06
Iter: 88 loss: 1.97293e-06
Iter: 89 loss: 1.9579229e-06
Iter: 90 loss: 1.94266295e-06
Iter: 91 loss: 1.93966844e-06
Iter: 92 loss: 1.92245307e-06
Iter: 93 loss: 1.96451924e-06
Iter: 94 loss: 1.91630284e-06
Iter: 95 loss: 1.88945251e-06
Iter: 96 loss: 1.95736266e-06
Iter: 97 loss: 1.88007994e-06
Iter: 98 loss: 1.87382852e-06
Iter: 99 loss: 1.9461952e-06
Iter: 100 loss: 1.87374212e-06
Iter: 101 loss: 1.86627449e-06
Iter: 102 loss: 1.84669409e-06
Iter: 103 loss: 1.9952422e-06
Iter: 104 loss: 1.84277587e-06
Iter: 105 loss: 1.82116355e-06
Iter: 106 loss: 1.90589321e-06
Iter: 107 loss: 1.81618873e-06
Iter: 108 loss: 1.79842641e-06
Iter: 109 loss: 1.78853315e-06
Iter: 110 loss: 1.78076675e-06
Iter: 111 loss: 1.75826403e-06
Iter: 112 loss: 2.10944677e-06
Iter: 113 loss: 1.75823402e-06
Iter: 114 loss: 1.74161266e-06
Iter: 115 loss: 1.90540959e-06
Iter: 116 loss: 1.74105389e-06
Iter: 117 loss: 1.72803652e-06
Iter: 118 loss: 1.70832459e-06
Iter: 119 loss: 1.70799558e-06
Iter: 120 loss: 1.69727696e-06
Iter: 121 loss: 1.84238911e-06
Iter: 122 loss: 1.69723887e-06
Iter: 123 loss: 1.6921822e-06
Iter: 124 loss: 1.69216014e-06
Iter: 125 loss: 1.68758606e-06
Iter: 126 loss: 1.67549069e-06
Iter: 127 loss: 1.76076855e-06
Iter: 128 loss: 1.67280064e-06
Iter: 129 loss: 1.66339146e-06
Iter: 130 loss: 1.67678263e-06
Iter: 131 loss: 1.65883603e-06
Iter: 132 loss: 1.64869346e-06
Iter: 133 loss: 1.64863081e-06
Iter: 134 loss: 1.63748109e-06
Iter: 135 loss: 1.63083666e-06
Iter: 136 loss: 1.62619745e-06
Iter: 137 loss: 1.61650951e-06
Iter: 138 loss: 1.60736545e-06
Iter: 139 loss: 1.60508534e-06
Iter: 140 loss: 1.60261413e-06
Iter: 141 loss: 1.59663102e-06
Iter: 142 loss: 1.59183617e-06
Iter: 143 loss: 1.58281341e-06
Iter: 144 loss: 1.77776099e-06
Iter: 145 loss: 1.5827718e-06
Iter: 146 loss: 1.57155364e-06
Iter: 147 loss: 1.55618568e-06
Iter: 148 loss: 1.55546445e-06
Iter: 149 loss: 1.55271982e-06
Iter: 150 loss: 1.54825739e-06
Iter: 151 loss: 1.54282088e-06
Iter: 152 loss: 1.58355806e-06
Iter: 153 loss: 1.54243196e-06
Iter: 154 loss: 1.53745543e-06
Iter: 155 loss: 1.52670623e-06
Iter: 156 loss: 1.6871702e-06
Iter: 157 loss: 1.52622238e-06
Iter: 158 loss: 1.51961376e-06
Iter: 159 loss: 1.51961058e-06
Iter: 160 loss: 1.51584447e-06
Iter: 161 loss: 1.56995679e-06
Iter: 162 loss: 1.51586664e-06
Iter: 163 loss: 1.51278823e-06
Iter: 164 loss: 1.50302935e-06
Iter: 165 loss: 1.51465019e-06
Iter: 166 loss: 1.49555603e-06
Iter: 167 loss: 1.5039227e-06
Iter: 168 loss: 1.49071877e-06
Iter: 169 loss: 1.48624554e-06
Iter: 170 loss: 1.4752743e-06
Iter: 171 loss: 1.58356681e-06
Iter: 172 loss: 1.47387391e-06
Iter: 173 loss: 1.46580214e-06
Iter: 174 loss: 1.53718599e-06
Iter: 175 loss: 1.46541834e-06
Iter: 176 loss: 1.46218963e-06
Iter: 177 loss: 1.46169373e-06
Iter: 178 loss: 1.45951037e-06
Iter: 179 loss: 1.45281842e-06
Iter: 180 loss: 1.46985076e-06
Iter: 181 loss: 1.4491759e-06
Iter: 182 loss: 1.44422961e-06
Iter: 183 loss: 1.49040329e-06
Iter: 184 loss: 1.4440343e-06
Iter: 185 loss: 1.44027717e-06
Iter: 186 loss: 1.49610389e-06
Iter: 187 loss: 1.44028127e-06
Iter: 188 loss: 1.43624163e-06
Iter: 189 loss: 1.43048305e-06
Iter: 190 loss: 1.43028751e-06
Iter: 191 loss: 1.42532463e-06
Iter: 192 loss: 1.43691022e-06
Iter: 193 loss: 1.42346028e-06
Iter: 194 loss: 1.41834801e-06
Iter: 195 loss: 1.48051697e-06
Iter: 196 loss: 1.41829412e-06
Iter: 197 loss: 1.41345367e-06
Iter: 198 loss: 1.41013982e-06
Iter: 199 loss: 1.40835596e-06
Iter: 200 loss: 1.40313341e-06
Iter: 201 loss: 1.40098302e-06
Iter: 202 loss: 1.39827148e-06
Iter: 203 loss: 1.39129611e-06
Iter: 204 loss: 1.39117697e-06
Iter: 205 loss: 1.38851328e-06
Iter: 206 loss: 1.38567941e-06
Iter: 207 loss: 1.38523296e-06
Iter: 208 loss: 1.38243968e-06
Iter: 209 loss: 1.4010875e-06
Iter: 210 loss: 1.38213613e-06
Iter: 211 loss: 1.37834832e-06
Iter: 212 loss: 1.38048983e-06
Iter: 213 loss: 1.37587733e-06
Iter: 214 loss: 1.37385837e-06
Iter: 215 loss: 1.36960273e-06
Iter: 216 loss: 1.44079866e-06
Iter: 217 loss: 1.36951644e-06
Iter: 218 loss: 1.36338667e-06
Iter: 219 loss: 1.38366909e-06
Iter: 220 loss: 1.3617e-06
Iter: 221 loss: 1.3596059e-06
Iter: 222 loss: 1.358237e-06
Iter: 223 loss: 1.35550181e-06
Iter: 224 loss: 1.34701031e-06
Iter: 225 loss: 1.36030167e-06
Iter: 226 loss: 1.34107154e-06
Iter: 227 loss: 1.34633774e-06
Iter: 228 loss: 1.33842047e-06
Iter: 229 loss: 1.33581739e-06
Iter: 230 loss: 1.33801882e-06
Iter: 231 loss: 1.3343e-06
Iter: 232 loss: 1.33163371e-06
Iter: 233 loss: 1.32651212e-06
Iter: 234 loss: 1.43795694e-06
Iter: 235 loss: 1.32650234e-06
Iter: 236 loss: 1.32859793e-06
Iter: 237 loss: 1.32473735e-06
Iter: 238 loss: 1.32337323e-06
Iter: 239 loss: 1.32229707e-06
Iter: 240 loss: 1.32185414e-06
Iter: 241 loss: 1.31951958e-06
Iter: 242 loss: 1.31554202e-06
Iter: 243 loss: 1.31554054e-06
Iter: 244 loss: 1.3106245e-06
Iter: 245 loss: 1.32160972e-06
Iter: 246 loss: 1.30874764e-06
Iter: 247 loss: 1.30589547e-06
Iter: 248 loss: 1.30522938e-06
Iter: 249 loss: 1.3025001e-06
Iter: 250 loss: 1.29793057e-06
Iter: 251 loss: 1.29791147e-06
Iter: 252 loss: 1.29373802e-06
Iter: 253 loss: 1.28492763e-06
Iter: 254 loss: 1.43318857e-06
Iter: 255 loss: 1.28470424e-06
Iter: 256 loss: 1.30629428e-06
Iter: 257 loss: 1.28257921e-06
Iter: 258 loss: 1.28149009e-06
Iter: 259 loss: 1.27954286e-06
Iter: 260 loss: 1.32518824e-06
Iter: 261 loss: 1.27953717e-06
Iter: 262 loss: 1.27755163e-06
Iter: 263 loss: 1.276072e-06
Iter: 264 loss: 1.2754108e-06
Iter: 265 loss: 1.27501335e-06
Iter: 266 loss: 1.27419719e-06
Iter: 267 loss: 1.27266821e-06
Iter: 268 loss: 1.26855844e-06
Iter: 269 loss: 1.29639716e-06
Iter: 270 loss: 1.26758027e-06
Iter: 271 loss: 1.26587702e-06
Iter: 272 loss: 1.2656684e-06
Iter: 273 loss: 1.26327734e-06
Iter: 274 loss: 1.26050827e-06
Iter: 275 loss: 1.26019063e-06
Iter: 276 loss: 1.25599013e-06
Iter: 277 loss: 1.25483371e-06
Iter: 278 loss: 1.25220367e-06
Iter: 279 loss: 1.24983944e-06
Iter: 280 loss: 1.26642067e-06
Iter: 281 loss: 1.24958513e-06
Iter: 282 loss: 1.24783355e-06
Iter: 283 loss: 1.25277597e-06
Iter: 284 loss: 1.24726716e-06
Iter: 285 loss: 1.24524536e-06
Iter: 286 loss: 1.26520251e-06
Iter: 287 loss: 1.24518283e-06
Iter: 288 loss: 1.24442431e-06
Iter: 289 loss: 1.2422106e-06
Iter: 290 loss: 1.25076917e-06
Iter: 291 loss: 1.24125722e-06
Iter: 292 loss: 1.23813084e-06
Iter: 293 loss: 1.27268777e-06
Iter: 294 loss: 1.23808491e-06
Iter: 295 loss: 1.23554378e-06
Iter: 296 loss: 1.26331565e-06
Iter: 297 loss: 1.23549262e-06
Iter: 298 loss: 1.2340729e-06
Iter: 299 loss: 1.2297611e-06
Iter: 300 loss: 1.24179121e-06
Iter: 301 loss: 1.22749907e-06
Iter: 302 loss: 1.22688834e-06
Iter: 303 loss: 1.2254859e-06
Iter: 304 loss: 1.22347626e-06
Iter: 305 loss: 1.23171458e-06
Iter: 306 loss: 1.22309712e-06
Iter: 307 loss: 1.22168535e-06
Iter: 308 loss: 1.21908033e-06
Iter: 309 loss: 1.27778026e-06
Iter: 310 loss: 1.21906533e-06
Iter: 311 loss: 1.21985727e-06
Iter: 312 loss: 1.21811172e-06
Iter: 313 loss: 1.2174894e-06
Iter: 314 loss: 1.21558833e-06
Iter: 315 loss: 1.22083657e-06
Iter: 316 loss: 1.2145772e-06
Iter: 317 loss: 1.21308096e-06
Iter: 318 loss: 1.21303856e-06
Iter: 319 loss: 1.21170524e-06
Iter: 320 loss: 1.21748883e-06
Iter: 321 loss: 1.21140727e-06
Iter: 322 loss: 1.20980019e-06
Iter: 323 loss: 1.20838172e-06
Iter: 324 loss: 1.20798211e-06
Iter: 325 loss: 1.20618097e-06
Iter: 326 loss: 1.20878349e-06
Iter: 327 loss: 1.20531502e-06
Iter: 328 loss: 1.20289292e-06
Iter: 329 loss: 1.20129619e-06
Iter: 330 loss: 1.20041136e-06
Iter: 331 loss: 1.20127e-06
Iter: 332 loss: 1.19880883e-06
Iter: 333 loss: 1.1980535e-06
Iter: 334 loss: 1.19591039e-06
Iter: 335 loss: 1.20189225e-06
Iter: 336 loss: 1.1947518e-06
Iter: 337 loss: 1.19219612e-06
Iter: 338 loss: 1.22085908e-06
Iter: 339 loss: 1.19217088e-06
Iter: 340 loss: 1.19137212e-06
Iter: 341 loss: 1.19109018e-06
Iter: 342 loss: 1.19056085e-06
Iter: 343 loss: 1.18896855e-06
Iter: 344 loss: 1.19383026e-06
Iter: 345 loss: 1.18818389e-06
Iter: 346 loss: 1.19000515e-06
Iter: 347 loss: 1.18761773e-06
Iter: 348 loss: 1.18724006e-06
Iter: 349 loss: 1.18593312e-06
Iter: 350 loss: 1.18636319e-06
Iter: 351 loss: 1.18470575e-06
Iter: 352 loss: 1.18328308e-06
Iter: 353 loss: 1.18324851e-06
Iter: 354 loss: 1.18185312e-06
Iter: 355 loss: 1.19459469e-06
Iter: 356 loss: 1.18176808e-06
Iter: 357 loss: 1.18080584e-06
Iter: 358 loss: 1.17807292e-06
Iter: 359 loss: 1.19247125e-06
Iter: 360 loss: 1.17720094e-06
Iter: 361 loss: 1.17412822e-06
Iter: 362 loss: 1.19959213e-06
Iter: 363 loss: 1.17392187e-06
Iter: 364 loss: 1.17169816e-06
Iter: 365 loss: 1.18719277e-06
Iter: 366 loss: 1.17153081e-06
Iter: 367 loss: 1.16916181e-06
Iter: 368 loss: 1.18339881e-06
Iter: 369 loss: 1.16887497e-06
Iter: 370 loss: 1.16817807e-06
Iter: 371 loss: 1.16772924e-06
Iter: 372 loss: 1.1674415e-06
Iter: 373 loss: 1.16625017e-06
Iter: 374 loss: 1.16646174e-06
Iter: 375 loss: 1.16535227e-06
Iter: 376 loss: 1.16472108e-06
Iter: 377 loss: 1.16436343e-06
Iter: 378 loss: 1.1638416e-06
Iter: 379 loss: 1.16249873e-06
Iter: 380 loss: 1.17354944e-06
Iter: 381 loss: 1.16220485e-06
Iter: 382 loss: 1.16064814e-06
Iter: 383 loss: 1.1727742e-06
Iter: 384 loss: 1.16052684e-06
Iter: 385 loss: 1.15865237e-06
Iter: 386 loss: 1.16352385e-06
Iter: 387 loss: 1.15801902e-06
Iter: 388 loss: 1.15679154e-06
Iter: 389 loss: 1.15409614e-06
Iter: 390 loss: 1.19524259e-06
Iter: 391 loss: 1.15397643e-06
Iter: 392 loss: 1.15344972e-06
Iter: 393 loss: 1.15253806e-06
Iter: 394 loss: 1.15132536e-06
Iter: 395 loss: 1.15133139e-06
Iter: 396 loss: 1.15039325e-06
Iter: 397 loss: 1.14928014e-06
Iter: 398 loss: 1.14721865e-06
Iter: 399 loss: 1.19060701e-06
Iter: 400 loss: 1.14721934e-06
Iter: 401 loss: 1.14752083e-06
Iter: 402 loss: 1.14630166e-06
Iter: 403 loss: 1.14551858e-06
Iter: 404 loss: 1.14451257e-06
Iter: 405 loss: 1.14446027e-06
Iter: 406 loss: 1.14327906e-06
Iter: 407 loss: 1.14292573e-06
Iter: 408 loss: 1.14220234e-06
Iter: 409 loss: 1.14179738e-06
Iter: 410 loss: 1.14164777e-06
Iter: 411 loss: 1.14098157e-06
Iter: 412 loss: 1.13940382e-06
Iter: 413 loss: 1.15485557e-06
Iter: 414 loss: 1.1391387e-06
Iter: 415 loss: 1.13713918e-06
Iter: 416 loss: 1.14476506e-06
Iter: 417 loss: 1.13664896e-06
Iter: 418 loss: 1.13454905e-06
Iter: 419 loss: 1.15384034e-06
Iter: 420 loss: 1.13445572e-06
Iter: 421 loss: 1.13307533e-06
Iter: 422 loss: 1.13426108e-06
Iter: 423 loss: 1.13225735e-06
Iter: 424 loss: 1.13086412e-06
Iter: 425 loss: 1.13279373e-06
Iter: 426 loss: 1.13016461e-06
Iter: 427 loss: 1.12883993e-06
Iter: 428 loss: 1.12886141e-06
Iter: 429 loss: 1.12827456e-06
Iter: 430 loss: 1.12729617e-06
Iter: 431 loss: 1.12731959e-06
Iter: 432 loss: 1.12605062e-06
Iter: 433 loss: 1.12786847e-06
Iter: 434 loss: 1.12548514e-06
Iter: 435 loss: 1.12434418e-06
Iter: 436 loss: 1.12432008e-06
Iter: 437 loss: 1.12385237e-06
Iter: 438 loss: 1.12295731e-06
Iter: 439 loss: 1.14031877e-06
Iter: 440 loss: 1.12295368e-06
Iter: 441 loss: 1.12170426e-06
Iter: 442 loss: 1.12478347e-06
Iter: 443 loss: 1.1212644e-06
Iter: 444 loss: 1.11915028e-06
Iter: 445 loss: 1.12236694e-06
Iter: 446 loss: 1.1181636e-06
Iter: 447 loss: 1.11672728e-06
Iter: 448 loss: 1.11494023e-06
Iter: 449 loss: 1.11478289e-06
Iter: 450 loss: 1.11415898e-06
Iter: 451 loss: 1.11347754e-06
Iter: 452 loss: 1.11278428e-06
Iter: 453 loss: 1.11254144e-06
Iter: 454 loss: 1.11215672e-06
Iter: 455 loss: 1.11117504e-06
Iter: 456 loss: 1.11177405e-06
Iter: 457 loss: 1.11054283e-06
Iter: 458 loss: 1.10984251e-06
Iter: 459 loss: 1.10982023e-06
Iter: 460 loss: 1.1093598e-06
Iter: 461 loss: 1.10859582e-06
Iter: 462 loss: 1.10861299e-06
Iter: 463 loss: 1.10752956e-06
Iter: 464 loss: 1.10655105e-06
Iter: 465 loss: 1.10634267e-06
Iter: 466 loss: 1.10648079e-06
Iter: 467 loss: 1.10553947e-06
Iter: 468 loss: 1.10501219e-06
Iter: 469 loss: 1.1036924e-06
Iter: 470 loss: 1.11367513e-06
Iter: 471 loss: 1.10341955e-06
Iter: 472 loss: 1.10170299e-06
Iter: 473 loss: 1.11176007e-06
Iter: 474 loss: 1.1014879e-06
Iter: 475 loss: 1.10030396e-06
Iter: 476 loss: 1.10029816e-06
Iter: 477 loss: 1.09964083e-06
Iter: 478 loss: 1.09806876e-06
Iter: 479 loss: 1.11410918e-06
Iter: 480 loss: 1.09782854e-06
Iter: 481 loss: 1.09789517e-06
Iter: 482 loss: 1.0971919e-06
Iter: 483 loss: 1.09675864e-06
Iter: 484 loss: 1.0964834e-06
Iter: 485 loss: 1.096299e-06
Iter: 486 loss: 1.09564985e-06
Iter: 487 loss: 1.09704592e-06
Iter: 488 loss: 1.09536586e-06
Iter: 489 loss: 1.09482448e-06
Iter: 490 loss: 1.10177473e-06
Iter: 491 loss: 1.09480459e-06
Iter: 492 loss: 1.09434654e-06
Iter: 493 loss: 1.09368602e-06
Iter: 494 loss: 1.09368898e-06
Iter: 495 loss: 1.09283701e-06
Iter: 496 loss: 1.09300777e-06
Iter: 497 loss: 1.09222674e-06
Iter: 498 loss: 1.0920445e-06
Iter: 499 loss: 1.09176233e-06
Iter: 500 loss: 1.09136226e-06
Iter: 501 loss: 1.09008215e-06
Iter: 502 loss: 1.09671157e-06
Iter: 503 loss: 1.08967515e-06
Iter: 504 loss: 1.08797667e-06
Iter: 505 loss: 1.09591019e-06
Iter: 506 loss: 1.08764675e-06
Iter: 507 loss: 1.08700215e-06
Iter: 508 loss: 1.08685299e-06
Iter: 509 loss: 1.08629536e-06
Iter: 510 loss: 1.08469112e-06
Iter: 511 loss: 1.09068924e-06
Iter: 512 loss: 1.08402901e-06
Iter: 513 loss: 1.08347524e-06
Iter: 514 loss: 1.08304266e-06
Iter: 515 loss: 1.08222616e-06
Iter: 516 loss: 1.08271922e-06
Iter: 517 loss: 1.08175095e-06
Iter: 518 loss: 1.08094321e-06
Iter: 519 loss: 1.08221843e-06
Iter: 520 loss: 1.080625e-06
Iter: 521 loss: 1.08003155e-06
Iter: 522 loss: 1.08002405e-06
Iter: 523 loss: 1.07960591e-06
Iter: 524 loss: 1.07938581e-06
Iter: 525 loss: 1.07915184e-06
Iter: 526 loss: 1.07862525e-06
Iter: 527 loss: 1.07796836e-06
Iter: 528 loss: 1.07789128e-06
Iter: 529 loss: 1.07740061e-06
Iter: 530 loss: 1.07735104e-06
Iter: 531 loss: 1.07668279e-06
Iter: 532 loss: 1.07541541e-06
Iter: 533 loss: 1.10144242e-06
Iter: 534 loss: 1.0753829e-06
Iter: 535 loss: 1.07417259e-06
Iter: 536 loss: 1.07745655e-06
Iter: 537 loss: 1.07371841e-06
Iter: 538 loss: 1.07307346e-06
Iter: 539 loss: 1.07297569e-06
Iter: 540 loss: 1.07228902e-06
Iter: 541 loss: 1.0705844e-06
Iter: 542 loss: 1.08713834e-06
Iter: 543 loss: 1.07035748e-06
Iter: 544 loss: 1.07000733e-06
Iter: 545 loss: 1.06970674e-06
Iter: 546 loss: 1.06919083e-06
Iter: 547 loss: 1.06967161e-06
Iter: 548 loss: 1.06888365e-06
Iter: 549 loss: 1.06833158e-06
Iter: 550 loss: 1.06832158e-06
Iter: 551 loss: 1.06789685e-06
Iter: 552 loss: 1.06734228e-06
Iter: 553 loss: 1.0673441e-06
Iter: 554 loss: 1.06688901e-06
Iter: 555 loss: 1.06671348e-06
Iter: 556 loss: 1.06649111e-06
Iter: 557 loss: 1.06587845e-06
Iter: 558 loss: 1.06490609e-06
Iter: 559 loss: 1.06487562e-06
Iter: 560 loss: 1.064201e-06
Iter: 561 loss: 1.0641844e-06
Iter: 562 loss: 1.06330663e-06
Iter: 563 loss: 1.0625771e-06
Iter: 564 loss: 1.06233892e-06
Iter: 565 loss: 1.06128061e-06
Iter: 566 loss: 1.06199309e-06
Iter: 567 loss: 1.06060202e-06
Iter: 568 loss: 1.06002108e-06
Iter: 569 loss: 1.05991126e-06
Iter: 570 loss: 1.05927325e-06
Iter: 571 loss: 1.05824415e-06
Iter: 572 loss: 1.05824665e-06
Iter: 573 loss: 1.0576764e-06
Iter: 574 loss: 1.06587026e-06
Iter: 575 loss: 1.05764593e-06
Iter: 576 loss: 1.0570717e-06
Iter: 577 loss: 1.05781078e-06
Iter: 578 loss: 1.05680783e-06
Iter: 579 loss: 1.05615186e-06
Iter: 580 loss: 1.05634035e-06
Iter: 581 loss: 1.05567437e-06
Iter: 582 loss: 1.05525305e-06
Iter: 583 loss: 1.05523111e-06
Iter: 584 loss: 1.05487356e-06
Iter: 585 loss: 1.05464642e-06
Iter: 586 loss: 1.05448862e-06
Iter: 587 loss: 1.05384879e-06
Iter: 588 loss: 1.05298147e-06
Iter: 589 loss: 1.05298454e-06
Iter: 590 loss: 1.05224581e-06
Iter: 591 loss: 1.05223353e-06
Iter: 592 loss: 1.05134802e-06
Iter: 593 loss: 1.05216111e-06
Iter: 594 loss: 1.05085564e-06
Iter: 595 loss: 1.05013578e-06
Iter: 596 loss: 1.04970059e-06
Iter: 597 loss: 1.0493942e-06
Iter: 598 loss: 1.04878006e-06
Iter: 599 loss: 1.04873811e-06
Iter: 600 loss: 1.04805281e-06
Iter: 601 loss: 1.04727087e-06
Iter: 602 loss: 1.04717424e-06
Iter: 603 loss: 1.04650098e-06
Iter: 604 loss: 1.04989329e-06
Iter: 605 loss: 1.04638877e-06
Iter: 606 loss: 1.0456738e-06
Iter: 607 loss: 1.04878927e-06
Iter: 608 loss: 1.04551486e-06
Iter: 609 loss: 1.04501953e-06
Iter: 610 loss: 1.0452884e-06
Iter: 611 loss: 1.04466858e-06
Iter: 612 loss: 1.04430717e-06
Iter: 613 loss: 1.04933611e-06
Iter: 614 loss: 1.04431422e-06
Iter: 615 loss: 1.04394894e-06
Iter: 616 loss: 1.0437401e-06
Iter: 617 loss: 1.04359697e-06
Iter: 618 loss: 1.04300648e-06
Iter: 619 loss: 1.0426071e-06
Iter: 620 loss: 1.0423513e-06
Iter: 621 loss: 1.0417217e-06
Iter: 622 loss: 1.04623e-06
Iter: 623 loss: 1.04167179e-06
Iter: 624 loss: 1.04085518e-06
Iter: 625 loss: 1.04360458e-06
Iter: 626 loss: 1.0406153e-06
Iter: 627 loss: 1.04007836e-06
Iter: 628 loss: 1.03936145e-06
Iter: 629 loss: 1.03931302e-06
Iter: 630 loss: 1.03868024e-06
Iter: 631 loss: 1.0386766e-06
Iter: 632 loss: 1.03798584e-06
Iter: 633 loss: 1.03803143e-06
Iter: 634 loss: 1.03740581e-06
Iter: 635 loss: 1.03693174e-06
Iter: 636 loss: 1.03847083e-06
Iter: 637 loss: 1.03678599e-06
Iter: 638 loss: 1.03621983e-06
Iter: 639 loss: 1.03915772e-06
Iter: 640 loss: 1.03613786e-06
Iter: 641 loss: 1.03571267e-06
Iter: 642 loss: 1.03564423e-06
Iter: 643 loss: 1.03533716e-06
Iter: 644 loss: 1.03492016e-06
Iter: 645 loss: 1.03939726e-06
Iter: 646 loss: 1.03489742e-06
Iter: 647 loss: 1.03454522e-06
Iter: 648 loss: 1.03468e-06
Iter: 649 loss: 1.03427465e-06
Iter: 650 loss: 1.03382877e-06
Iter: 651 loss: 1.03363209e-06
Iter: 652 loss: 1.03335697e-06
Iter: 653 loss: 1.03282855e-06
Iter: 654 loss: 1.0344985e-06
Iter: 655 loss: 1.03266916e-06
Iter: 656 loss: 1.03196749e-06
Iter: 657 loss: 1.0371366e-06
Iter: 658 loss: 1.03188029e-06
Iter: 659 loss: 1.03149e-06
Iter: 660 loss: 1.03075843e-06
Iter: 661 loss: 1.0307491e-06
Iter: 662 loss: 1.03006732e-06
Iter: 663 loss: 1.03662353e-06
Iter: 664 loss: 1.03003958e-06
Iter: 665 loss: 1.02920944e-06
Iter: 666 loss: 1.03060802e-06
Iter: 667 loss: 1.02880472e-06
Iter: 668 loss: 1.02831495e-06
Iter: 669 loss: 1.02876129e-06
Iter: 670 loss: 1.0280578e-06
Iter: 671 loss: 1.02740364e-06
Iter: 672 loss: 1.03209663e-06
Iter: 673 loss: 1.02730723e-06
Iter: 674 loss: 1.02689876e-06
Iter: 675 loss: 1.02714785e-06
Iter: 676 loss: 1.02658055e-06
Iter: 677 loss: 1.0262147e-06
Iter: 678 loss: 1.0289358e-06
Iter: 679 loss: 1.02617844e-06
Iter: 680 loss: 1.02582908e-06
Iter: 681 loss: 1.02592367e-06
Iter: 682 loss: 1.02557647e-06
Iter: 683 loss: 1.02515992e-06
Iter: 684 loss: 1.0252719e-06
Iter: 685 loss: 1.02484114e-06
Iter: 686 loss: 1.02433637e-06
Iter: 687 loss: 1.02446324e-06
Iter: 688 loss: 1.02395677e-06
Iter: 689 loss: 1.02326362e-06
Iter: 690 loss: 1.03377863e-06
Iter: 691 loss: 1.02326544e-06
Iter: 692 loss: 1.02287208e-06
Iter: 693 loss: 1.02204649e-06
Iter: 694 loss: 1.03468915e-06
Iter: 695 loss: 1.02203046e-06
Iter: 696 loss: 1.02114234e-06
Iter: 697 loss: 1.02704564e-06
Iter: 698 loss: 1.02102842e-06
Iter: 699 loss: 1.02002764e-06
Iter: 700 loss: 1.02480931e-06
Iter: 701 loss: 1.01985779e-06
Iter: 702 loss: 1.01940645e-06
Iter: 703 loss: 1.01949774e-06
Iter: 704 loss: 1.01907403e-06
Iter: 705 loss: 1.01852117e-06
Iter: 706 loss: 1.02444255e-06
Iter: 707 loss: 1.01851651e-06
Iter: 708 loss: 1.0181891e-06
Iter: 709 loss: 1.01833155e-06
Iter: 710 loss: 1.0179582e-06
Iter: 711 loss: 1.01762987e-06
Iter: 712 loss: 1.01948967e-06
Iter: 713 loss: 1.0175969e-06
Iter: 714 loss: 1.01721571e-06
Iter: 715 loss: 1.01769149e-06
Iter: 716 loss: 1.01702494e-06
Iter: 717 loss: 1.01661431e-06
Iter: 718 loss: 1.01659884e-06
Iter: 719 loss: 1.01631178e-06
Iter: 720 loss: 1.01578951e-06
Iter: 721 loss: 1.0158127e-06
Iter: 722 loss: 1.01540661e-06
Iter: 723 loss: 1.01486262e-06
Iter: 724 loss: 1.01480737e-06
Iter: 725 loss: 1.01451883e-06
Iter: 726 loss: 1.01378555e-06
Iter: 727 loss: 1.02204763e-06
Iter: 728 loss: 1.01374587e-06
Iter: 729 loss: 1.01295643e-06
Iter: 730 loss: 1.01801925e-06
Iter: 731 loss: 1.01289447e-06
Iter: 732 loss: 1.01215551e-06
Iter: 733 loss: 1.01849264e-06
Iter: 734 loss: 1.01210821e-06
Iter: 735 loss: 1.01179285e-06
Iter: 736 loss: 1.01139813e-06
Iter: 737 loss: 1.01138539e-06
Iter: 738 loss: 1.01082537e-06
Iter: 739 loss: 1.01721821e-06
Iter: 740 loss: 1.01082412e-06
Iter: 741 loss: 1.01046874e-06
Iter: 742 loss: 1.01038609e-06
Iter: 743 loss: 1.0101553e-06
Iter: 744 loss: 1.00969942e-06
Iter: 745 loss: 1.01204125e-06
Iter: 746 loss: 1.00962063e-06
Iter: 747 loss: 1.00911927e-06
Iter: 748 loss: 1.0102367e-06
Iter: 749 loss: 1.00898274e-06
Iter: 750 loss: 1.0085306e-06
Iter: 751 loss: 1.008656e-06
Iter: 752 loss: 1.00820716e-06
Iter: 753 loss: 1.00772775e-06
Iter: 754 loss: 1.00775981e-06
Iter: 755 loss: 1.0073461e-06
Iter: 756 loss: 1.00703346e-06
Iter: 757 loss: 1.0068801e-06
Iter: 758 loss: 1.00665557e-06
Iter: 759 loss: 1.00602983e-06
Iter: 760 loss: 1.01104092e-06
Iter: 761 loss: 1.00592638e-06
Iter: 762 loss: 1.00520538e-06
Iter: 763 loss: 1.00992679e-06
Iter: 764 loss: 1.00510943e-06
Iter: 765 loss: 1.00443719e-06
Iter: 766 loss: 1.01154035e-06
Iter: 767 loss: 1.00442412e-06
Iter: 768 loss: 1.00409466e-06
Iter: 769 loss: 1.00348166e-06
Iter: 770 loss: 1.00348848e-06
Iter: 771 loss: 1.00282637e-06
Iter: 772 loss: 1.00282944e-06
Iter: 773 loss: 1.00242767e-06
Iter: 774 loss: 1.00229931e-06
Iter: 775 loss: 1.0020492e-06
Iter: 776 loss: 1.00150544e-06
Iter: 777 loss: 1.00380566e-06
Iter: 778 loss: 1.00139084e-06
Iter: 779 loss: 1.00084048e-06
Iter: 780 loss: 1.00376224e-06
Iter: 781 loss: 1.00075749e-06
Iter: 782 loss: 1.00041439e-06
Iter: 783 loss: 1.00027808e-06
Iter: 784 loss: 1.00010709e-06
Iter: 785 loss: 9.99617782e-07
Iter: 786 loss: 9.99430085e-07
Iter: 787 loss: 9.99184522e-07
Iter: 788 loss: 9.99046733e-07
Iter: 789 loss: 9.98774e-07
Iter: 790 loss: 9.98558789e-07
Iter: 791 loss: 9.97961706e-07
Iter: 792 loss: 1.00268267e-06
Iter: 793 loss: 9.97854841e-07
Iter: 794 loss: 9.97145889e-07
Iter: 795 loss: 1.0016056e-06
Iter: 796 loss: 9.97073357e-07
Iter: 797 loss: 9.96543349e-07
Iter: 798 loss: 9.96544486e-07
Iter: 799 loss: 9.96251174e-07
Iter: 800 loss: 9.9560441e-07
Iter: 801 loss: 1.00429656e-06
Iter: 802 loss: 9.95550408e-07
Iter: 803 loss: 9.94972766e-07
Iter: 804 loss: 9.94954e-07
Iter: 805 loss: 9.94600327e-07
Iter: 806 loss: 9.94400125e-07
Iter: 807 loss: 9.9420356e-07
Iter: 808 loss: 9.93696062e-07
Iter: 809 loss: 9.95396135e-07
Iter: 810 loss: 9.93552362e-07
Iter: 811 loss: 9.93009735e-07
Iter: 812 loss: 9.96847234e-07
Iter: 813 loss: 9.92997684e-07
Iter: 814 loss: 9.92678451e-07
Iter: 815 loss: 9.92432888e-07
Iter: 816 loss: 9.92315336e-07
Iter: 817 loss: 9.91807724e-07
Iter: 818 loss: 9.9150725e-07
Iter: 819 loss: 9.91303523e-07
Iter: 820 loss: 9.91264756e-07
Iter: 821 loss: 9.90965873e-07
Iter: 822 loss: 9.90721e-07
Iter: 823 loss: 9.90127319e-07
Iter: 824 loss: 9.9665931e-07
Iter: 825 loss: 9.90067178e-07
Iter: 826 loss: 9.89494424e-07
Iter: 827 loss: 9.92200739e-07
Iter: 828 loss: 9.89376e-07
Iter: 829 loss: 9.88906095e-07
Iter: 830 loss: 9.88921101e-07
Iter: 831 loss: 9.88605734e-07
Iter: 832 loss: 9.87921908e-07
Iter: 833 loss: 9.96034828e-07
Iter: 834 loss: 9.87856538e-07
Iter: 835 loss: 9.87378826e-07
Iter: 836 loss: 9.87321528e-07
Iter: 837 loss: 9.86942268e-07
Iter: 838 loss: 9.86518671e-07
Iter: 839 loss: 9.86472401e-07
Iter: 840 loss: 9.85791303e-07
Iter: 841 loss: 9.87649287e-07
Iter: 842 loss: 9.85578e-07
Iter: 843 loss: 9.84921485e-07
Iter: 844 loss: 9.90275339e-07
Iter: 845 loss: 9.84906364e-07
Iter: 846 loss: 9.84531766e-07
Iter: 847 loss: 9.84154e-07
Iter: 848 loss: 9.84086455e-07
Iter: 849 loss: 9.83406153e-07
Iter: 850 loss: 9.83544396e-07
Iter: 851 loss: 9.82965389e-07
Iter: 852 loss: 9.83045425e-07
Iter: 853 loss: 9.8265e-07
Iter: 854 loss: 9.82371148e-07
Iter: 855 loss: 9.81757239e-07
Iter: 856 loss: 9.89027285e-07
Iter: 857 loss: 9.81714265e-07
Iter: 858 loss: 9.81080575e-07
Iter: 859 loss: 9.83458904e-07
Iter: 860 loss: 9.80957452e-07
Iter: 861 loss: 9.80569666e-07
Iter: 862 loss: 9.80537493e-07
Iter: 863 loss: 9.80266577e-07
Iter: 864 loss: 9.79534661e-07
Iter: 865 loss: 9.85988549e-07
Iter: 866 loss: 9.79432571e-07
Iter: 867 loss: 9.79064225e-07
Iter: 868 loss: 9.78976345e-07
Iter: 869 loss: 9.78610615e-07
Iter: 870 loss: 9.78176104e-07
Iter: 871 loss: 9.78099365e-07
Iter: 872 loss: 9.77438e-07
Iter: 873 loss: 9.80174491e-07
Iter: 874 loss: 9.77300715e-07
Iter: 875 loss: 9.76741603e-07
Iter: 876 loss: 9.82316237e-07
Iter: 877 loss: 9.76723413e-07
Iter: 878 loss: 9.76387355e-07
Iter: 879 loss: 9.75947614e-07
Iter: 880 loss: 9.75929311e-07
Iter: 881 loss: 9.75240823e-07
Iter: 882 loss: 9.75140551e-07
Iter: 883 loss: 9.74663067e-07
Iter: 884 loss: 9.74552449e-07
Iter: 885 loss: 9.74324621e-07
Iter: 886 loss: 9.73965598e-07
Iter: 887 loss: 9.73279157e-07
Iter: 888 loss: 9.86824716e-07
Iter: 889 loss: 9.73301667e-07
Iter: 890 loss: 9.72713906e-07
Iter: 891 loss: 9.73959686e-07
Iter: 892 loss: 9.72516204e-07
Iter: 893 loss: 9.72129328e-07
Iter: 894 loss: 9.72117277e-07
Iter: 895 loss: 9.71777695e-07
Iter: 896 loss: 9.71078066e-07
Iter: 897 loss: 9.80786126e-07
Iter: 898 loss: 9.70983137e-07
Iter: 899 loss: 9.70610699e-07
Iter: 900 loss: 9.70567157e-07
Iter: 901 loss: 9.70136853e-07
Iter: 902 loss: 9.69587632e-07
Iter: 903 loss: 9.69572511e-07
Iter: 904 loss: 9.68742938e-07
Iter: 905 loss: 9.70512e-07
Iter: 906 loss: 9.68444624e-07
Iter: 907 loss: 9.67744199e-07
Iter: 908 loss: 9.74012437e-07
Iter: 909 loss: 9.67700771e-07
Iter: 910 loss: 9.6713859e-07
Iter: 911 loss: 9.6632948e-07
Iter: 912 loss: 9.6629617e-07
Iter: 913 loss: 9.65231379e-07
Iter: 914 loss: 9.6792246e-07
Iter: 915 loss: 9.64865876e-07
Iter: 916 loss: 9.64601441e-07
Iter: 917 loss: 9.64494347e-07
Iter: 918 loss: 9.64099854e-07
Iter: 919 loss: 9.63534831e-07
Iter: 920 loss: 9.63521188e-07
Iter: 921 loss: 9.6301585e-07
Iter: 922 loss: 9.63607135e-07
Iter: 923 loss: 9.62747549e-07
Iter: 924 loss: 9.62399213e-07
Iter: 925 loss: 9.62363742e-07
Iter: 926 loss: 9.62012791e-07
Iter: 927 loss: 9.61241199e-07
Iter: 928 loss: 9.70092742e-07
Iter: 929 loss: 9.61170826e-07
Iter: 930 loss: 9.60658e-07
Iter: 931 loss: 9.60645821e-07
Iter: 932 loss: 9.60144916e-07
Iter: 933 loss: 9.5983421e-07
Iter: 934 loss: 9.59627187e-07
Iter: 935 loss: 9.58914825e-07
Iter: 936 loss: 9.61179808e-07
Iter: 937 loss: 9.58708597e-07
Iter: 938 loss: 9.58180408e-07
Iter: 939 loss: 9.60347e-07
Iter: 940 loss: 9.58064675e-07
Iter: 941 loss: 9.57484531e-07
Iter: 942 loss: 9.56883127e-07
Iter: 943 loss: 9.56796384e-07
Iter: 944 loss: 9.5593532e-07
Iter: 945 loss: 9.58777491e-07
Iter: 946 loss: 9.55697715e-07
Iter: 947 loss: 9.55265e-07
Iter: 948 loss: 9.55261e-07
Iter: 949 loss: 9.54757525e-07
Iter: 950 loss: 9.54361667e-07
Iter: 951 loss: 9.54213533e-07
Iter: 952 loss: 9.53667893e-07
Iter: 953 loss: 9.53646065e-07
Iter: 954 loss: 9.53184e-07
Iter: 955 loss: 9.52860717e-07
Iter: 956 loss: 9.52781534e-07
Iter: 957 loss: 9.52389e-07
Iter: 958 loss: 9.51779839e-07
Iter: 959 loss: 9.51764605e-07
Iter: 960 loss: 9.51375114e-07
Iter: 961 loss: 9.51371533e-07
Iter: 962 loss: 9.50985623e-07
Iter: 963 loss: 9.50837773e-07
Iter: 964 loss: 9.50654112e-07
Iter: 965 loss: 9.5014309e-07
Iter: 966 loss: 9.53123958e-07
Iter: 967 loss: 9.50094545e-07
Iter: 968 loss: 9.49738251e-07
Iter: 969 loss: 9.50508593e-07
Iter: 970 loss: 9.49596256e-07
Iter: 971 loss: 9.49136336e-07
Iter: 972 loss: 9.48584329e-07
Iter: 973 loss: 9.48536183e-07
Iter: 974 loss: 9.47745434e-07
Iter: 975 loss: 9.49973241e-07
Iter: 976 loss: 9.47467697e-07
Iter: 977 loss: 9.47003855e-07
Iter: 978 loss: 9.53887138e-07
Iter: 979 loss: 9.47017838e-07
Iter: 980 loss: 9.46498176e-07
Iter: 981 loss: 9.46803198e-07
Iter: 982 loss: 9.46169848e-07
Iter: 983 loss: 9.457857e-07
Iter: 984 loss: 9.455905e-07
Iter: 985 loss: 9.45413092e-07
Iter: 986 loss: 9.45215902e-07
Iter: 987 loss: 9.45134161e-07
Iter: 988 loss: 9.44863132e-07
Iter: 989 loss: 9.44494047e-07
Iter: 990 loss: 9.44493422e-07
Iter: 991 loss: 9.44184194e-07
Iter: 992 loss: 9.46398302e-07
Iter: 993 loss: 9.44139231e-07
Iter: 994 loss: 9.43747182e-07
Iter: 995 loss: 9.43763894e-07
Iter: 996 loss: 9.43450232e-07
Iter: 997 loss: 9.43036866e-07
Iter: 998 loss: 9.45711292e-07
Iter: 999 loss: 9.42996166e-07
Iter: 1000 loss: 9.42630663e-07
Iter: 1001 loss: 9.43029704e-07
Iter: 1002 loss: 9.4241426e-07
Iter: 1003 loss: 9.41877e-07
Iter: 1004 loss: 9.41379653e-07
Iter: 1005 loss: 9.41262044e-07
Iter: 1006 loss: 9.40484824e-07
Iter: 1007 loss: 9.4342596e-07
Iter: 1008 loss: 9.40298605e-07
Iter: 1009 loss: 9.39779397e-07
Iter: 1010 loss: 9.43316479e-07
Iter: 1011 loss: 9.39732843e-07
Iter: 1012 loss: 9.39122287e-07
Iter: 1013 loss: 9.40945768e-07
Iter: 1014 loss: 9.38938399e-07
Iter: 1015 loss: 9.38649805e-07
Iter: 1016 loss: 9.38268613e-07
Iter: 1017 loss: 9.38234052e-07
Iter: 1018 loss: 9.37942332e-07
Iter: 1019 loss: 9.37903565e-07
Iter: 1020 loss: 9.37633558e-07
Iter: 1021 loss: 9.3727931e-07
Iter: 1022 loss: 9.37213485e-07
Iter: 1023 loss: 9.36841104e-07
Iter: 1024 loss: 9.38729386e-07
Iter: 1025 loss: 9.36806941e-07
Iter: 1026 loss: 9.36314677e-07
Iter: 1027 loss: 9.36349693e-07
Iter: 1028 loss: 9.35927631e-07
Iter: 1029 loss: 9.35445428e-07
Iter: 1030 loss: 9.38413905e-07
Iter: 1031 loss: 9.35368689e-07
Iter: 1032 loss: 9.34949639e-07
Iter: 1033 loss: 9.36492597e-07
Iter: 1034 loss: 9.34836351e-07
Iter: 1035 loss: 9.34375521e-07
Iter: 1036 loss: 9.34063451e-07
Iter: 1037 loss: 9.33896104e-07
Iter: 1038 loss: 9.33252181e-07
Iter: 1039 loss: 9.34688899e-07
Iter: 1040 loss: 9.33032879e-07
Iter: 1041 loss: 9.32466492e-07
Iter: 1042 loss: 9.35180253e-07
Iter: 1043 loss: 9.32363037e-07
Iter: 1044 loss: 9.3172855e-07
Iter: 1045 loss: 9.35483797e-07
Iter: 1046 loss: 9.31621571e-07
Iter: 1047 loss: 9.31308307e-07
Iter: 1048 loss: 9.30793306e-07
Iter: 1049 loss: 9.30799729e-07
Iter: 1050 loss: 9.30540182e-07
Iter: 1051 loss: 9.30489819e-07
Iter: 1052 loss: 9.30174451e-07
Iter: 1053 loss: 9.29873806e-07
Iter: 1054 loss: 9.29823614e-07
Iter: 1055 loss: 9.294547e-07
Iter: 1056 loss: 9.31248e-07
Iter: 1057 loss: 9.29341354e-07
Iter: 1058 loss: 9.28888085e-07
Iter: 1059 loss: 9.3004445e-07
Iter: 1060 loss: 9.28692884e-07
Iter: 1061 loss: 9.28364784e-07
Iter: 1062 loss: 9.29273256e-07
Iter: 1063 loss: 9.28239899e-07
Iter: 1064 loss: 9.27876101e-07
Iter: 1065 loss: 9.29179237e-07
Iter: 1066 loss: 9.27805559e-07
Iter: 1067 loss: 9.27408792e-07
Iter: 1068 loss: 9.27223368e-07
Iter: 1069 loss: 9.270492e-07
Iter: 1070 loss: 9.2651544e-07
Iter: 1071 loss: 9.27276346e-07
Iter: 1072 loss: 9.26241739e-07
Iter: 1073 loss: 9.25673646e-07
Iter: 1074 loss: 9.28278e-07
Iter: 1075 loss: 9.25563427e-07
Iter: 1076 loss: 9.25067638e-07
Iter: 1077 loss: 9.31221052e-07
Iter: 1078 loss: 9.2507014e-07
Iter: 1079 loss: 9.24831738e-07
Iter: 1080 loss: 9.24373637e-07
Iter: 1081 loss: 9.32433295e-07
Iter: 1082 loss: 9.24347148e-07
Iter: 1083 loss: 9.24091921e-07
Iter: 1084 loss: 9.24052074e-07
Iter: 1085 loss: 9.23760922e-07
Iter: 1086 loss: 9.23662924e-07
Iter: 1087 loss: 9.23480911e-07
Iter: 1088 loss: 9.23156449e-07
Iter: 1089 loss: 9.2416127e-07
Iter: 1090 loss: 9.23054074e-07
Iter: 1091 loss: 9.22617119e-07
Iter: 1092 loss: 9.23869493e-07
Iter: 1093 loss: 9.22498771e-07
Iter: 1094 loss: 9.22195454e-07
Iter: 1095 loss: 9.22432491e-07
Iter: 1096 loss: 9.21964329e-07
Iter: 1097 loss: 9.21537321e-07
Iter: 1098 loss: 9.23978632e-07
Iter: 1099 loss: 9.21454443e-07
Iter: 1100 loss: 9.21077799e-07
Iter: 1101 loss: 9.20817456e-07
Iter: 1102 loss: 9.20682396e-07
Iter: 1103 loss: 9.20084631e-07
Iter: 1104 loss: 9.20501236e-07
Iter: 1105 loss: 9.19710146e-07
Iter: 1106 loss: 9.190681e-07
Iter: 1107 loss: 9.21403966e-07
Iter: 1108 loss: 9.1888478e-07
Iter: 1109 loss: 9.18346757e-07
Iter: 1110 loss: 9.18329249e-07
Iter: 1111 loss: 9.18099204e-07
Iter: 1112 loss: 9.17524062e-07
Iter: 1113 loss: 9.25042e-07
Iter: 1114 loss: 9.17490695e-07
Iter: 1115 loss: 9.17153216e-07
Iter: 1116 loss: 9.17116267e-07
Iter: 1117 loss: 9.16739396e-07
Iter: 1118 loss: 9.1683836e-07
Iter: 1119 loss: 9.16496219e-07
Iter: 1120 loss: 9.16131739e-07
Iter: 1121 loss: 9.17032708e-07
Iter: 1122 loss: 9.16021463e-07
Iter: 1123 loss: 9.15595933e-07
Iter: 1124 loss: 9.18085e-07
Iter: 1125 loss: 9.15522378e-07
Iter: 1126 loss: 9.15272039e-07
Iter: 1127 loss: 9.15219971e-07
Iter: 1128 loss: 9.15054727e-07
Iter: 1129 loss: 9.14646421e-07
Iter: 1130 loss: 9.18379442e-07
Iter: 1131 loss: 9.1462573e-07
Iter: 1132 loss: 9.14347652e-07
Iter: 1133 loss: 9.1406713e-07
Iter: 1134 loss: 9.13991698e-07
Iter: 1135 loss: 9.13468341e-07
Iter: 1136 loss: 9.13518306e-07
Iter: 1137 loss: 9.13063e-07
Iter: 1138 loss: 9.12372911e-07
Iter: 1139 loss: 9.14269378e-07
Iter: 1140 loss: 9.12156224e-07
Iter: 1141 loss: 9.11683628e-07
Iter: 1142 loss: 9.11683912e-07
Iter: 1143 loss: 9.11389634e-07
Iter: 1144 loss: 9.10794597e-07
Iter: 1145 loss: 9.16273905e-07
Iter: 1146 loss: 9.10696485e-07
Iter: 1147 loss: 9.1034866e-07
Iter: 1148 loss: 9.10325753e-07
Iter: 1149 loss: 9.09905793e-07
Iter: 1150 loss: 9.10753158e-07
Iter: 1151 loss: 9.09789321e-07
Iter: 1152 loss: 9.09537334e-07
Iter: 1153 loss: 9.09710366e-07
Iter: 1154 loss: 9.09392611e-07
Iter: 1155 loss: 9.09055359e-07
Iter: 1156 loss: 9.11691245e-07
Iter: 1157 loss: 9.09041e-07
Iter: 1158 loss: 9.08844186e-07
Iter: 1159 loss: 9.08743175e-07
Iter: 1160 loss: 9.08671495e-07
Iter: 1161 loss: 9.08344077e-07
Iter: 1162 loss: 9.10906e-07
Iter: 1163 loss: 9.08328275e-07
Iter: 1164 loss: 9.08093455e-07
Iter: 1165 loss: 9.07710273e-07
Iter: 1166 loss: 9.07731192e-07
Iter: 1167 loss: 9.07178901e-07
Iter: 1168 loss: 9.07791218e-07
Iter: 1169 loss: 9.06917364e-07
Iter: 1170 loss: 9.06323748e-07
Iter: 1171 loss: 9.07626713e-07
Iter: 1172 loss: 9.06125e-07
Iter: 1173 loss: 9.05884121e-07
Iter: 1174 loss: 9.05784759e-07
Iter: 1175 loss: 9.05608658e-07
Iter: 1176 loss: 9.0514385e-07
Iter: 1177 loss: 9.09655569e-07
Iter: 1178 loss: 9.05096e-07
Iter: 1179 loss: 9.04845592e-07
Iter: 1180 loss: 9.04818876e-07
Iter: 1181 loss: 9.04598608e-07
Iter: 1182 loss: 9.05305967e-07
Iter: 1183 loss: 9.04531646e-07
Iter: 1184 loss: 9.04357535e-07
Iter: 1185 loss: 9.04306717e-07
Iter: 1186 loss: 9.04227e-07
Iter: 1187 loss: 9.04013405e-07
Iter: 1188 loss: 9.04017327e-07
Iter: 1189 loss: 9.03876e-07
Iter: 1190 loss: 9.03607e-07
Iter: 1191 loss: 9.07839819e-07
Iter: 1192 loss: 9.03591172e-07
Iter: 1193 loss: 9.03251816e-07
Iter: 1194 loss: 9.06743594e-07
Iter: 1195 loss: 9.0324e-07
Iter: 1196 loss: 9.02961688e-07
Iter: 1197 loss: 9.02797694e-07
Iter: 1198 loss: 9.02674969e-07
Iter: 1199 loss: 9.02332886e-07
Iter: 1200 loss: 9.02838678e-07
Iter: 1201 loss: 9.0217145e-07
Iter: 1202 loss: 9.01727731e-07
Iter: 1203 loss: 9.02182819e-07
Iter: 1204 loss: 9.01501096e-07
Iter: 1205 loss: 9.01318913e-07
Iter: 1206 loss: 9.01265821e-07
Iter: 1207 loss: 9.01042426e-07
Iter: 1208 loss: 9.00419877e-07
Iter: 1209 loss: 9.05034881e-07
Iter: 1210 loss: 9.0029738e-07
Iter: 1211 loss: 8.99955751e-07
Iter: 1212 loss: 8.99923464e-07
Iter: 1213 loss: 8.99661131e-07
Iter: 1214 loss: 9.00658279e-07
Iter: 1215 loss: 8.99595591e-07
Iter: 1216 loss: 8.99347e-07
Iter: 1217 loss: 8.99017095e-07
Iter: 1218 loss: 8.99002202e-07
Iter: 1219 loss: 8.98770793e-07
Iter: 1220 loss: 8.98742e-07
Iter: 1221 loss: 8.98589406e-07
Iter: 1222 loss: 8.98360213e-07
Iter: 1223 loss: 8.98350322e-07
Iter: 1224 loss: 8.98177404e-07
Iter: 1225 loss: 8.98178882e-07
Iter: 1226 loss: 8.9803666e-07
Iter: 1227 loss: 8.97893869e-07
Iter: 1228 loss: 8.9787e-07
Iter: 1229 loss: 8.97668656e-07
Iter: 1230 loss: 8.97888867e-07
Iter: 1231 loss: 8.97557129e-07
Iter: 1232 loss: 8.9732e-07
Iter: 1233 loss: 8.97981181e-07
Iter: 1234 loss: 8.97232326e-07
Iter: 1235 loss: 8.97071516e-07
Iter: 1236 loss: 8.99172846e-07
Iter: 1237 loss: 8.97070493e-07
Iter: 1238 loss: 8.96854772e-07
Iter: 1239 loss: 8.96392294e-07
Iter: 1240 loss: 9.02165311e-07
Iter: 1241 loss: 8.96362906e-07
Iter: 1242 loss: 8.96179301e-07
Iter: 1243 loss: 8.96178051e-07
Iter: 1244 loss: 8.96034066e-07
Iter: 1245 loss: 8.95903838e-07
Iter: 1246 loss: 8.95882238e-07
Iter: 1247 loss: 8.95622861e-07
Iter: 1248 loss: 8.95666062e-07
Iter: 1249 loss: 8.95424762e-07
Iter: 1250 loss: 8.95253152e-07
Iter: 1251 loss: 8.95260655e-07
Iter: 1252 loss: 8.95123094e-07
Iter: 1253 loss: 8.94883101e-07
Iter: 1254 loss: 8.99181714e-07
Iter: 1255 loss: 8.94852462e-07
Iter: 1256 loss: 8.94697678e-07
Iter: 1257 loss: 8.94692278e-07
Iter: 1258 loss: 8.94550396e-07
Iter: 1259 loss: 8.94405957e-07
Iter: 1260 loss: 8.9438231e-07
Iter: 1261 loss: 8.94120603e-07
Iter: 1262 loss: 8.9439493e-07
Iter: 1263 loss: 8.94007087e-07
Iter: 1264 loss: 8.93738e-07
Iter: 1265 loss: 8.94082859e-07
Iter: 1266 loss: 8.93587753e-07
Iter: 1267 loss: 8.93317235e-07
Iter: 1268 loss: 8.94391e-07
Iter: 1269 loss: 8.93251297e-07
Iter: 1270 loss: 8.92848107e-07
Iter: 1271 loss: 8.93634137e-07
Iter: 1272 loss: 8.9271623e-07
Iter: 1273 loss: 8.92602827e-07
Iter: 1274 loss: 8.92606749e-07
Iter: 1275 loss: 8.92487151e-07
Iter: 1276 loss: 8.92497155e-07
Iter: 1277 loss: 8.92395235e-07
Iter: 1278 loss: 8.92272169e-07
Iter: 1279 loss: 8.92231867e-07
Iter: 1280 loss: 8.92134722e-07
Iter: 1281 loss: 8.91994091e-07
Iter: 1282 loss: 8.9201626e-07
Iter: 1283 loss: 8.9190172e-07
Iter: 1284 loss: 8.91827767e-07
Iter: 1285 loss: 8.91801051e-07
Iter: 1286 loss: 8.91671334e-07
Iter: 1287 loss: 8.92747835e-07
Iter: 1288 loss: 8.91667355e-07
Iter: 1289 loss: 8.91512911e-07
Iter: 1290 loss: 8.91479658e-07
Iter: 1291 loss: 8.91426907e-07
Iter: 1292 loss: 8.91236255e-07
Iter: 1293 loss: 8.91493755e-07
Iter: 1294 loss: 8.9117276e-07
Iter: 1295 loss: 8.90935667e-07
Iter: 1296 loss: 8.90744673e-07
Iter: 1297 loss: 8.90658157e-07
Iter: 1298 loss: 8.90361434e-07
Iter: 1299 loss: 8.9260709e-07
Iter: 1300 loss: 8.90306069e-07
Iter: 1301 loss: 8.89963587e-07
Iter: 1302 loss: 8.91382797e-07
Iter: 1303 loss: 8.89892249e-07
Iter: 1304 loss: 8.89663852e-07
Iter: 1305 loss: 8.89905152e-07
Iter: 1306 loss: 8.89524529e-07
Iter: 1307 loss: 8.8926663e-07
Iter: 1308 loss: 8.91355285e-07
Iter: 1309 loss: 8.892506e-07
Iter: 1310 loss: 8.89030616e-07
Iter: 1311 loss: 8.88675856e-07
Iter: 1312 loss: 8.88683189e-07
Iter: 1313 loss: 8.88402269e-07
Iter: 1314 loss: 8.88383852e-07
Iter: 1315 loss: 8.88244699e-07
Iter: 1316 loss: 8.88373279e-07
Iter: 1317 loss: 8.88127e-07
Iter: 1318 loss: 8.87972647e-07
Iter: 1319 loss: 8.88411e-07
Iter: 1320 loss: 8.87916201e-07
Iter: 1321 loss: 8.87655631e-07
Iter: 1322 loss: 8.87947522e-07
Iter: 1323 loss: 8.87500164e-07
Iter: 1324 loss: 8.87310307e-07
Iter: 1325 loss: 8.87549959e-07
Iter: 1326 loss: 8.87218903e-07
Iter: 1327 loss: 8.86948555e-07
Iter: 1328 loss: 8.86784619e-07
Iter: 1329 loss: 8.86649786e-07
Iter: 1330 loss: 8.86328621e-07
Iter: 1331 loss: 8.89476041e-07
Iter: 1332 loss: 8.86324472e-07
Iter: 1333 loss: 8.86104374e-07
Iter: 1334 loss: 8.88243562e-07
Iter: 1335 loss: 8.8607726e-07
Iter: 1336 loss: 8.85979944e-07
Iter: 1337 loss: 8.85749046e-07
Iter: 1338 loss: 8.85739155e-07
Iter: 1339 loss: 8.85543329e-07
Iter: 1340 loss: 8.85558961e-07
Iter: 1341 loss: 8.85389e-07
Iter: 1342 loss: 8.85112172e-07
Iter: 1343 loss: 8.85096142e-07
Iter: 1344 loss: 8.84892529e-07
Iter: 1345 loss: 8.84898782e-07
Iter: 1346 loss: 8.84724898e-07
Iter: 1347 loss: 8.84832e-07
Iter: 1348 loss: 8.84626e-07
Iter: 1349 loss: 8.84422661e-07
Iter: 1350 loss: 8.84611723e-07
Iter: 1351 loss: 8.84332167e-07
Iter: 1352 loss: 8.83994119e-07
Iter: 1353 loss: 8.84703354e-07
Iter: 1354 loss: 8.83840755e-07
Iter: 1355 loss: 8.83631799e-07
Iter: 1356 loss: 8.83970415e-07
Iter: 1357 loss: 8.83523569e-07
Iter: 1358 loss: 8.83257201e-07
Iter: 1359 loss: 8.83624921e-07
Iter: 1360 loss: 8.83129246e-07
Iter: 1361 loss: 8.82843892e-07
Iter: 1362 loss: 8.84663507e-07
Iter: 1363 loss: 8.82831273e-07
Iter: 1364 loss: 8.82671088e-07
Iter: 1365 loss: 8.84437213e-07
Iter: 1366 loss: 8.82676488e-07
Iter: 1367 loss: 8.82519771e-07
Iter: 1368 loss: 8.8220628e-07
Iter: 1369 loss: 8.85447093e-07
Iter: 1370 loss: 8.8220861e-07
Iter: 1371 loss: 8.81906772e-07
Iter: 1372 loss: 8.81904157e-07
Iter: 1373 loss: 8.81635629e-07
Iter: 1374 loss: 8.8134675e-07
Iter: 1375 loss: 8.81311394e-07
Iter: 1376 loss: 8.80991138e-07
Iter: 1377 loss: 8.83994403e-07
Iter: 1378 loss: 8.80985283e-07
Iter: 1379 loss: 8.80738469e-07
Iter: 1380 loss: 8.80921561e-07
Iter: 1381 loss: 8.80551568e-07
Iter: 1382 loss: 8.80281448e-07
Iter: 1383 loss: 8.81296501e-07
Iter: 1384 loss: 8.80227503e-07
Iter: 1385 loss: 8.79969889e-07
Iter: 1386 loss: 8.81388587e-07
Iter: 1387 loss: 8.79886386e-07
Iter: 1388 loss: 8.79746267e-07
Iter: 1389 loss: 8.79693857e-07
Iter: 1390 loss: 8.79590516e-07
Iter: 1391 loss: 8.79365359e-07
Iter: 1392 loss: 8.79749678e-07
Iter: 1393 loss: 8.7927566e-07
Iter: 1394 loss: 8.78993148e-07
Iter: 1395 loss: 8.79754054e-07
Iter: 1396 loss: 8.78935793e-07
Iter: 1397 loss: 8.78692049e-07
Iter: 1398 loss: 8.81702647e-07
Iter: 1399 loss: 8.7869e-07
Iter: 1400 loss: 8.78533797e-07
Iter: 1401 loss: 8.78220135e-07
Iter: 1402 loss: 8.83333428e-07
Iter: 1403 loss: 8.78179776e-07
Iter: 1404 loss: 8.77847924e-07
Iter: 1405 loss: 8.7786168e-07
Iter: 1406 loss: 8.77553703e-07
Iter: 1407 loss: 8.77608784e-07
Iter: 1408 loss: 8.77354125e-07
Iter: 1409 loss: 8.77139e-07
Iter: 1410 loss: 8.78856213e-07
Iter: 1411 loss: 8.77128855e-07
Iter: 1412 loss: 8.76945819e-07
Iter: 1413 loss: 8.77100604e-07
Iter: 1414 loss: 8.76809054e-07
Iter: 1415 loss: 8.76598733e-07
Iter: 1416 loss: 8.77091679e-07
Iter: 1417 loss: 8.76546892e-07
Iter: 1418 loss: 8.76323952e-07
Iter: 1419 loss: 8.78403398e-07
Iter: 1420 loss: 8.76310082e-07
Iter: 1421 loss: 8.76179456e-07
Iter: 1422 loss: 8.75984369e-07
Iter: 1423 loss: 8.75991873e-07
Iter: 1424 loss: 8.75721184e-07
Iter: 1425 loss: 8.76513241e-07
Iter: 1426 loss: 8.7564672e-07
Iter: 1427 loss: 8.75395e-07
Iter: 1428 loss: 8.76151262e-07
Iter: 1429 loss: 8.75315493e-07
Iter: 1430 loss: 8.75086698e-07
Iter: 1431 loss: 8.77618504e-07
Iter: 1432 loss: 8.75088233e-07
Iter: 1433 loss: 8.74922705e-07
Iter: 1434 loss: 8.7471318e-07
Iter: 1435 loss: 8.74708689e-07
Iter: 1436 loss: 8.7449132e-07
Iter: 1437 loss: 8.77729178e-07
Iter: 1438 loss: 8.7449007e-07
Iter: 1439 loss: 8.74262582e-07
Iter: 1440 loss: 8.74599323e-07
Iter: 1441 loss: 8.74178397e-07
Iter: 1442 loss: 8.74054763e-07
Iter: 1443 loss: 8.74475631e-07
Iter: 1444 loss: 8.74006446e-07
Iter: 1445 loss: 8.73853367e-07
Iter: 1446 loss: 8.73849672e-07
Iter: 1447 loss: 8.73686076e-07
Iter: 1448 loss: 8.73440968e-07
Iter: 1449 loss: 8.73707336e-07
Iter: 1450 loss: 8.73267538e-07
Iter: 1451 loss: 8.73034764e-07
Iter: 1452 loss: 8.73014699e-07
Iter: 1453 loss: 8.72854685e-07
Iter: 1454 loss: 8.72500607e-07
Iter: 1455 loss: 8.79572667e-07
Iter: 1456 loss: 8.72488897e-07
Iter: 1457 loss: 8.71955649e-07
Iter: 1458 loss: 8.7278886e-07
Iter: 1459 loss: 8.71752036e-07
Iter: 1460 loss: 8.71292343e-07
Iter: 1461 loss: 8.72743271e-07
Iter: 1462 loss: 8.71148927e-07
Iter: 1463 loss: 8.70768645e-07
Iter: 1464 loss: 8.76753575e-07
Iter: 1465 loss: 8.70753752e-07
Iter: 1466 loss: 8.70519102e-07
Iter: 1467 loss: 8.70344763e-07
Iter: 1468 loss: 8.70232839e-07
Iter: 1469 loss: 8.70024735e-07
Iter: 1470 loss: 8.73326087e-07
Iter: 1471 loss: 8.70028941e-07
Iter: 1472 loss: 8.69816176e-07
Iter: 1473 loss: 8.70131657e-07
Iter: 1474 loss: 8.69714313e-07
Iter: 1475 loss: 8.69546511e-07
Iter: 1476 loss: 8.6992e-07
Iter: 1477 loss: 8.69489668e-07
Iter: 1478 loss: 8.69268888e-07
Iter: 1479 loss: 8.69410883e-07
Iter: 1480 loss: 8.69126211e-07
Iter: 1481 loss: 8.68822212e-07
Iter: 1482 loss: 8.68935729e-07
Iter: 1483 loss: 8.68632071e-07
Iter: 1484 loss: 8.68335746e-07
Iter: 1485 loss: 8.68343363e-07
Iter: 1486 loss: 8.68132133e-07
Iter: 1487 loss: 8.67846836e-07
Iter: 1488 loss: 8.67845074e-07
Iter: 1489 loss: 8.67472579e-07
Iter: 1490 loss: 8.68589893e-07
Iter: 1491 loss: 8.67328652e-07
Iter: 1492 loss: 8.67042047e-07
Iter: 1493 loss: 8.67658343e-07
Iter: 1494 loss: 8.66922505e-07
Iter: 1495 loss: 8.66680921e-07
Iter: 1496 loss: 8.6668274e-07
Iter: 1497 loss: 8.66481173e-07
Iter: 1498 loss: 8.66312348e-07
Iter: 1499 loss: 8.6629575e-07
Iter: 1500 loss: 8.66060304e-07
Iter: 1501 loss: 8.67950575e-07
Iter: 1502 loss: 8.66053597e-07
Iter: 1503 loss: 8.65855327e-07
Iter: 1504 loss: 8.6625289e-07
Iter: 1505 loss: 8.65733739e-07
Iter: 1506 loss: 8.65572815e-07
Iter: 1507 loss: 8.6584032e-07
Iter: 1508 loss: 8.65470611e-07
Iter: 1509 loss: 8.65220841e-07
Iter: 1510 loss: 8.65569689e-07
Iter: 1511 loss: 8.65084417e-07
Iter: 1512 loss: 8.64829531e-07
Iter: 1513 loss: 8.65199809e-07
Iter: 1514 loss: 8.64661274e-07
Iter: 1515 loss: 8.6449e-07
Iter: 1516 loss: 8.6449603e-07
Iter: 1517 loss: 8.64343406e-07
Iter: 1518 loss: 8.64111655e-07
Iter: 1519 loss: 8.64111712e-07
Iter: 1520 loss: 8.638184e-07
Iter: 1521 loss: 8.64765582e-07
Iter: 1522 loss: 8.63753598e-07
Iter: 1523 loss: 8.63503487e-07
Iter: 1524 loss: 8.63595574e-07
Iter: 1525 loss: 8.63307719e-07
Iter: 1526 loss: 8.63049934e-07
Iter: 1527 loss: 8.63035268e-07
Iter: 1528 loss: 8.62807553e-07
Iter: 1529 loss: 8.62580237e-07
Iter: 1530 loss: 8.62543857e-07
Iter: 1531 loss: 8.62269303e-07
Iter: 1532 loss: 8.64533774e-07
Iter: 1533 loss: 8.62253046e-07
Iter: 1534 loss: 8.61982585e-07
Iter: 1535 loss: 8.62981949e-07
Iter: 1536 loss: 8.61893852e-07
Iter: 1537 loss: 8.61721503e-07
Iter: 1538 loss: 8.61728836e-07
Iter: 1539 loss: 8.61573767e-07
Iter: 1540 loss: 8.61261356e-07
Iter: 1541 loss: 8.62645663e-07
Iter: 1542 loss: 8.6120906e-07
Iter: 1543 loss: 8.61017838e-07
Iter: 1544 loss: 8.61044327e-07
Iter: 1545 loss: 8.60874593e-07
Iter: 1546 loss: 8.60686782e-07
Iter: 1547 loss: 8.60676835e-07
Iter: 1548 loss: 8.60500677e-07
Iter: 1549 loss: 8.60208161e-07
Iter: 1550 loss: 8.60207365e-07
Iter: 1551 loss: 8.59878128e-07
Iter: 1552 loss: 8.6088329e-07
Iter: 1553 loss: 8.59791271e-07
Iter: 1554 loss: 8.59500631e-07
Iter: 1555 loss: 8.59515183e-07
Iter: 1556 loss: 8.5926672e-07
Iter: 1557 loss: 8.58985345e-07
Iter: 1558 loss: 8.58987164e-07
Iter: 1559 loss: 8.5873512e-07
Iter: 1560 loss: 8.58678391e-07
Iter: 1561 loss: 8.58534918e-07
Iter: 1562 loss: 8.58222165e-07
Iter: 1563 loss: 8.59442537e-07
Iter: 1564 loss: 8.58187661e-07
Iter: 1565 loss: 8.57923055e-07
Iter: 1566 loss: 8.58876604e-07
Iter: 1567 loss: 8.57837335e-07
Iter: 1568 loss: 8.57608143e-07
Iter: 1569 loss: 8.57557325e-07
Iter: 1570 loss: 8.57429654e-07
Iter: 1571 loss: 8.57058183e-07
Iter: 1572 loss: 8.59992667e-07
Iter: 1573 loss: 8.57052839e-07
Iter: 1574 loss: 8.56887937e-07
Iter: 1575 loss: 8.56755605e-07
Iter: 1576 loss: 8.56694669e-07
Iter: 1577 loss: 8.56486736e-07
Iter: 1578 loss: 8.56482529e-07
Iter: 1579 loss: 8.56314443e-07
Iter: 1580 loss: 8.56095483e-07
Iter: 1581 loss: 8.56074848e-07
Iter: 1582 loss: 8.55803819e-07
Iter: 1583 loss: 8.56883219e-07
Iter: 1584 loss: 8.55739131e-07
Iter: 1585 loss: 8.55526082e-07
Iter: 1586 loss: 8.55649887e-07
Iter: 1587 loss: 8.55422854e-07
Iter: 1588 loss: 8.55238227e-07
Iter: 1589 loss: 8.57616897e-07
Iter: 1590 loss: 8.55233338e-07
Iter: 1591 loss: 8.55049052e-07
Iter: 1592 loss: 8.54897962e-07
Iter: 1593 loss: 8.54821224e-07
Iter: 1594 loss: 8.54563268e-07
Iter: 1595 loss: 8.5591563e-07
Iter: 1596 loss: 8.54490054e-07
Iter: 1597 loss: 8.54223117e-07
Iter: 1598 loss: 8.55118344e-07
Iter: 1599 loss: 8.54127904e-07
Iter: 1600 loss: 8.53864435e-07
Iter: 1601 loss: 8.53707377e-07
Iter: 1602 loss: 8.53598635e-07
Iter: 1603 loss: 8.53227448e-07
Iter: 1604 loss: 8.5815384e-07
Iter: 1605 loss: 8.53226766e-07
Iter: 1606 loss: 8.53062375e-07
Iter: 1607 loss: 8.52757807e-07
Iter: 1608 loss: 8.5275741e-07
Iter: 1609 loss: 8.52422261e-07
Iter: 1610 loss: 8.56461611e-07
Iter: 1611 loss: 8.52428911e-07
Iter: 1612 loss: 8.52073754e-07
Iter: 1613 loss: 8.52123094e-07
Iter: 1614 loss: 8.51805908e-07
Iter: 1615 loss: 8.51522941e-07
Iter: 1616 loss: 8.52348649e-07
Iter: 1617 loss: 8.51418861e-07
Iter: 1618 loss: 8.51125037e-07
Iter: 1619 loss: 8.51148968e-07
Iter: 1620 loss: 8.50921879e-07
Iter: 1621 loss: 8.50593494e-07
Iter: 1622 loss: 8.54341351e-07
Iter: 1623 loss: 8.50570245e-07
Iter: 1624 loss: 8.50268123e-07
Iter: 1625 loss: 8.50461504e-07
Iter: 1626 loss: 8.5006161e-07
Iter: 1627 loss: 8.49718504e-07
Iter: 1628 loss: 8.50931883e-07
Iter: 1629 loss: 8.4963915e-07
Iter: 1630 loss: 8.49376875e-07
Iter: 1631 loss: 8.50872823e-07
Iter: 1632 loss: 8.49332821e-07
Iter: 1633 loss: 8.49097205e-07
Iter: 1634 loss: 8.49067192e-07
Iter: 1635 loss: 8.48909337e-07
Iter: 1636 loss: 8.48657351e-07
Iter: 1637 loss: 8.48654622e-07
Iter: 1638 loss: 8.48517743e-07
Iter: 1639 loss: 8.48252853e-07
Iter: 1640 loss: 8.52872859e-07
Iter: 1641 loss: 8.4825183e-07
Iter: 1642 loss: 8.47940214e-07
Iter: 1643 loss: 8.5127482e-07
Iter: 1644 loss: 8.47938054e-07
Iter: 1645 loss: 8.47612228e-07
Iter: 1646 loss: 8.47971705e-07
Iter: 1647 loss: 8.47457613e-07
Iter: 1648 loss: 8.47223646e-07
Iter: 1649 loss: 8.47564365e-07
Iter: 1650 loss: 8.47108936e-07
Iter: 1651 loss: 8.46807e-07
Iter: 1652 loss: 8.47224442e-07
Iter: 1653 loss: 8.46674368e-07
Iter: 1654 loss: 8.4645751e-07
Iter: 1655 loss: 8.49210949e-07
Iter: 1656 loss: 8.46446255e-07
Iter: 1657 loss: 8.46212799e-07
Iter: 1658 loss: 8.46539251e-07
Iter: 1659 loss: 8.46104228e-07
Iter: 1660 loss: 8.45906698e-07
Iter: 1661 loss: 8.46625085e-07
Iter: 1662 loss: 8.45856448e-07
Iter: 1663 loss: 8.45652607e-07
Iter: 1664 loss: 8.46486955e-07
Iter: 1665 loss: 8.45621344e-07
Iter: 1666 loss: 8.4545934e-07
Iter: 1667 loss: 8.45394879e-07
Iter: 1668 loss: 8.45314503e-07
Iter: 1669 loss: 8.45102363e-07
Iter: 1670 loss: 8.48206241e-07
Iter: 1671 loss: 8.45112652e-07
Iter: 1672 loss: 8.44936892e-07
Iter: 1673 loss: 8.44591398e-07
Iter: 1674 loss: 8.48131435e-07
Iter: 1675 loss: 8.44537453e-07
Iter: 1676 loss: 8.44094757e-07
Iter: 1677 loss: 8.4912881e-07
Iter: 1678 loss: 8.44130682e-07
Iter: 1679 loss: 8.4370464e-07
Iter: 1680 loss: 8.44461965e-07
Iter: 1681 loss: 8.4352e-07
Iter: 1682 loss: 8.43239491e-07
Iter: 1683 loss: 8.43377848e-07
Iter: 1684 loss: 8.43038094e-07
Iter: 1685 loss: 8.42636155e-07
Iter: 1686 loss: 8.42903148e-07
Iter: 1687 loss: 8.42330337e-07
Iter: 1688 loss: 8.41956535e-07
Iter: 1689 loss: 8.46446255e-07
Iter: 1690 loss: 8.41956194e-07
Iter: 1691 loss: 8.41589042e-07
Iter: 1692 loss: 8.42331701e-07
Iter: 1693 loss: 8.41467113e-07
Iter: 1694 loss: 8.41189e-07
Iter: 1695 loss: 8.41999963e-07
Iter: 1696 loss: 8.41107578e-07
Iter: 1697 loss: 8.40840528e-07
Iter: 1698 loss: 8.42018608e-07
Iter: 1699 loss: 8.40803182e-07
Iter: 1700 loss: 8.40562734e-07
Iter: 1701 loss: 8.40484063e-07
Iter: 1702 loss: 8.40344455e-07
Iter: 1703 loss: 8.40077746e-07
Iter: 1704 loss: 8.44194574e-07
Iter: 1705 loss: 8.40076837e-07
Iter: 1706 loss: 8.39871e-07
Iter: 1707 loss: 8.3939824e-07
Iter: 1708 loss: 8.461368e-07
Iter: 1709 loss: 8.39373399e-07
Iter: 1710 loss: 8.38957192e-07
Iter: 1711 loss: 8.38959636e-07
Iter: 1712 loss: 8.38609481e-07
Iter: 1713 loss: 8.39684731e-07
Iter: 1714 loss: 8.38497044e-07
Iter: 1715 loss: 8.38279391e-07
Iter: 1716 loss: 8.38351e-07
Iter: 1717 loss: 8.38111873e-07
Iter: 1718 loss: 8.37810774e-07
Iter: 1719 loss: 8.37969878e-07
Iter: 1720 loss: 8.37587379e-07
Iter: 1721 loss: 8.37248308e-07
Iter: 1722 loss: 8.41399924e-07
Iter: 1723 loss: 8.37238929e-07
Iter: 1724 loss: 8.37015136e-07
Iter: 1725 loss: 8.3771755e-07
Iter: 1726 loss: 8.36947095e-07
Iter: 1727 loss: 8.36714207e-07
Iter: 1728 loss: 8.36973584e-07
Iter: 1729 loss: 8.36595632e-07
Iter: 1730 loss: 8.36346487e-07
Iter: 1731 loss: 8.37251378e-07
Iter: 1732 loss: 8.36264235e-07
Iter: 1733 loss: 8.35997184e-07
Iter: 1734 loss: 8.35921696e-07
Iter: 1735 loss: 8.35763e-07
Iter: 1736 loss: 8.35440915e-07
Iter: 1737 loss: 8.39953941e-07
Iter: 1738 loss: 8.35437447e-07
Iter: 1739 loss: 8.35173296e-07
Iter: 1740 loss: 8.34737818e-07
Iter: 1741 loss: 8.45774537e-07
Iter: 1742 loss: 8.34733612e-07
Iter: 1743 loss: 8.34381808e-07
Iter: 1744 loss: 8.39345944e-07
Iter: 1745 loss: 8.34376e-07
Iter: 1746 loss: 8.34065759e-07
Iter: 1747 loss: 8.35821766e-07
Iter: 1748 loss: 8.34009654e-07
Iter: 1749 loss: 8.33834406e-07
Iter: 1750 loss: 8.33762897e-07
Iter: 1751 loss: 8.33679337e-07
Iter: 1752 loss: 8.33353511e-07
Iter: 1753 loss: 8.3335982e-07
Iter: 1754 loss: 8.33078673e-07
Iter: 1755 loss: 8.32699641e-07
Iter: 1756 loss: 8.36786057e-07
Iter: 1757 loss: 8.32699e-07
Iter: 1758 loss: 8.3236273e-07
Iter: 1759 loss: 8.33056617e-07
Iter: 1760 loss: 8.32259161e-07
Iter: 1761 loss: 8.3190173e-07
Iter: 1762 loss: 8.32430828e-07
Iter: 1763 loss: 8.31755642e-07
Iter: 1764 loss: 8.31331874e-07
Iter: 1765 loss: 8.32741932e-07
Iter: 1766 loss: 8.31230523e-07
Iter: 1767 loss: 8.30793283e-07
Iter: 1768 loss: 8.31055218e-07
Iter: 1769 loss: 8.30521e-07
Iter: 1770 loss: 8.30211889e-07
Iter: 1771 loss: 8.30210126e-07
Iter: 1772 loss: 8.29988153e-07
Iter: 1773 loss: 8.29605312e-07
Iter: 1774 loss: 8.29612532e-07
Iter: 1775 loss: 8.29271869e-07
Iter: 1776 loss: 8.31448e-07
Iter: 1777 loss: 8.29236e-07
Iter: 1778 loss: 8.28940301e-07
Iter: 1779 loss: 8.31501438e-07
Iter: 1780 loss: 8.28906309e-07
Iter: 1781 loss: 8.28736802e-07
Iter: 1782 loss: 8.2849067e-07
Iter: 1783 loss: 8.28470206e-07
Iter: 1784 loss: 8.28042516e-07
Iter: 1785 loss: 8.2829439e-07
Iter: 1786 loss: 8.2778638e-07
Iter: 1787 loss: 8.27353745e-07
Iter: 1788 loss: 8.31968e-07
Iter: 1789 loss: 8.27353063e-07
Iter: 1790 loss: 8.27006488e-07
Iter: 1791 loss: 8.27911322e-07
Iter: 1792 loss: 8.26902635e-07
Iter: 1793 loss: 8.2653969e-07
Iter: 1794 loss: 8.26945666e-07
Iter: 1795 loss: 8.26385758e-07
Iter: 1796 loss: 8.26009909e-07
Iter: 1797 loss: 8.27960662e-07
Iter: 1798 loss: 8.25945222e-07
Iter: 1799 loss: 8.25648328e-07
Iter: 1800 loss: 8.26156679e-07
Iter: 1801 loss: 8.25492407e-07
Iter: 1802 loss: 8.25287088e-07
Iter: 1803 loss: 8.27641372e-07
Iter: 1804 loss: 8.25298457e-07
Iter: 1805 loss: 8.25054599e-07
Iter: 1806 loss: 8.24830124e-07
Iter: 1807 loss: 8.24803408e-07
Iter: 1808 loss: 8.24527888e-07
Iter: 1809 loss: 8.24751453e-07
Iter: 1810 loss: 8.24382596e-07
Iter: 1811 loss: 8.23995492e-07
Iter: 1812 loss: 8.28192697e-07
Iter: 1813 loss: 8.2397537e-07
Iter: 1814 loss: 8.23715595e-07
Iter: 1815 loss: 8.23489529e-07
Iter: 1816 loss: 8.23398523e-07
Iter: 1817 loss: 8.23039727e-07
Iter: 1818 loss: 8.24425229e-07
Iter: 1819 loss: 8.22937182e-07
Iter: 1820 loss: 8.22681386e-07
Iter: 1821 loss: 8.24550625e-07
Iter: 1822 loss: 8.22687184e-07
Iter: 1823 loss: 8.22497782e-07
Iter: 1824 loss: 8.23009e-07
Iter: 1825 loss: 8.22405525e-07
Iter: 1826 loss: 8.22194e-07
Iter: 1827 loss: 8.22405923e-07
Iter: 1828 loss: 8.22073162e-07
Iter: 1829 loss: 8.21803042e-07
Iter: 1830 loss: 8.23142386e-07
Iter: 1831 loss: 8.2175751e-07
Iter: 1832 loss: 8.21535536e-07
Iter: 1833 loss: 8.21798267e-07
Iter: 1834 loss: 8.21426966e-07
Iter: 1835 loss: 8.21179583e-07
Iter: 1836 loss: 8.2220231e-07
Iter: 1837 loss: 8.21160086e-07
Iter: 1838 loss: 8.20872799e-07
Iter: 1839 loss: 8.20786454e-07
Iter: 1840 loss: 8.20578293e-07
Iter: 1841 loss: 8.20305104e-07
Iter: 1842 loss: 8.20028845e-07
Iter: 1843 loss: 8.19957904e-07
Iter: 1844 loss: 8.19549314e-07
Iter: 1845 loss: 8.195492e-07
Iter: 1846 loss: 8.19318075e-07
Iter: 1847 loss: 8.19348e-07
Iter: 1848 loss: 8.19142542e-07
Iter: 1849 loss: 8.18932392e-07
Iter: 1850 loss: 8.19657942e-07
Iter: 1851 loss: 8.18913236e-07
Iter: 1852 loss: 8.18726903e-07
Iter: 1853 loss: 8.19367756e-07
Iter: 1854 loss: 8.18697913e-07
Iter: 1855 loss: 8.18536307e-07
Iter: 1856 loss: 8.18830927e-07
Iter: 1857 loss: 8.18469744e-07
Iter: 1858 loss: 8.18242711e-07
Iter: 1859 loss: 8.18426884e-07
Iter: 1860 loss: 8.18109129e-07
Iter: 1861 loss: 8.17802174e-07
Iter: 1862 loss: 8.1915357e-07
Iter: 1863 loss: 8.17777504e-07
Iter: 1864 loss: 8.17534328e-07
Iter: 1865 loss: 8.17849241e-07
Iter: 1866 loss: 8.17424279e-07
Iter: 1867 loss: 8.17163595e-07
Iter: 1868 loss: 8.17867146e-07
Iter: 1869 loss: 8.17086288e-07
Iter: 1870 loss: 8.16739316e-07
Iter: 1871 loss: 8.1726705e-07
Iter: 1872 loss: 8.16591466e-07
Iter: 1873 loss: 8.1637063e-07
Iter: 1874 loss: 8.16145359e-07
Iter: 1875 loss: 8.16108297e-07
Iter: 1876 loss: 8.15902808e-07
Iter: 1877 loss: 8.15842554e-07
Iter: 1878 loss: 8.15679186e-07
Iter: 1879 loss: 8.15542137e-07
Iter: 1880 loss: 8.15481712e-07
Iter: 1881 loss: 8.15240924e-07
Iter: 1882 loss: 8.16161389e-07
Iter: 1883 loss: 8.15194596e-07
Iter: 1884 loss: 8.14993484e-07
Iter: 1885 loss: 8.15007e-07
Iter: 1886 loss: 8.14828354e-07
Iter: 1887 loss: 8.14477687e-07
Iter: 1888 loss: 8.16123304e-07
Iter: 1889 loss: 8.1442073e-07
Iter: 1890 loss: 8.14112809e-07
Iter: 1891 loss: 8.14726661e-07
Iter: 1892 loss: 8.13977294e-07
Iter: 1893 loss: 8.13679321e-07
Iter: 1894 loss: 8.15099213e-07
Iter: 1895 loss: 8.13610654e-07
Iter: 1896 loss: 8.13385668e-07
Iter: 1897 loss: 8.14049599e-07
Iter: 1898 loss: 8.13330246e-07
Iter: 1899 loss: 8.13114752e-07
Iter: 1900 loss: 8.13478891e-07
Iter: 1901 loss: 8.13048075e-07
Iter: 1902 loss: 8.12769827e-07
Iter: 1903 loss: 8.13399879e-07
Iter: 1904 loss: 8.1263272e-07
Iter: 1905 loss: 8.12421604e-07
Iter: 1906 loss: 8.12225437e-07
Iter: 1907 loss: 8.12173482e-07
Iter: 1908 loss: 8.11972086e-07
Iter: 1909 loss: 8.11967084e-07
Iter: 1910 loss: 8.11734537e-07
Iter: 1911 loss: 8.11603854e-07
Iter: 1912 loss: 8.11514951e-07
Iter: 1913 loss: 8.11298946e-07
Iter: 1914 loss: 8.12586961e-07
Iter: 1915 loss: 8.11246537e-07
Iter: 1916 loss: 8.11064297e-07
Iter: 1917 loss: 8.11029281e-07
Iter: 1918 loss: 8.10909967e-07
Iter: 1919 loss: 8.10611766e-07
Iter: 1920 loss: 8.13127429e-07
Iter: 1921 loss: 8.10616598e-07
Iter: 1922 loss: 8.10450672e-07
Iter: 1923 loss: 8.1077178e-07
Iter: 1924 loss: 8.10361314e-07
Iter: 1925 loss: 8.10187146e-07
Iter: 1926 loss: 8.10938502e-07
Iter: 1927 loss: 8.10144513e-07
Iter: 1928 loss: 8.10007634e-07
Iter: 1929 loss: 8.10344204e-07
Iter: 1930 loss: 8.09955736e-07
Iter: 1931 loss: 8.09816697e-07
Iter: 1932 loss: 8.09890707e-07
Iter: 1933 loss: 8.09722621e-07
Iter: 1934 loss: 8.09462563e-07
Iter: 1935 loss: 8.10230404e-07
Iter: 1936 loss: 8.09386279e-07
Iter: 1937 loss: 8.09175276e-07
Iter: 1938 loss: 8.09015376e-07
Iter: 1939 loss: 8.08964558e-07
Iter: 1940 loss: 8.08739628e-07
Iter: 1941 loss: 8.0876066e-07
Iter: 1942 loss: 8.08530956e-07
Iter: 1943 loss: 8.08439438e-07
Iter: 1944 loss: 8.08319101e-07
Iter: 1945 loss: 8.0809e-07
Iter: 1946 loss: 8.08955804e-07
Iter: 1947 loss: 8.08046764e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4
+ date
Mon Oct 26 11:18:16 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.4_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.4_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b871cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b8753840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b87531e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b87536a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b8685488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b86a66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b8676c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b861bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b861bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b85ef620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b85ef400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b85ef2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b85a7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b85a7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b8511950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b8511840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b84e2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b848fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b84af730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b8457ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b8443510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71b8451b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f718e7618c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f718e7356a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f718e761268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f718e7599d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f718e6e7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f718e6a6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f718e6a6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f718e66c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f718e66c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7168723158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7168723378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f718e66c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71686f8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71686be8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.70485417e-06
Iter: 2 loss: 4.60219644e-06
Iter: 3 loss: 4.36780192e-06
Iter: 4 loss: 4.11087467e-06
Iter: 5 loss: 3.83834094e-06
Iter: 6 loss: 3.79276662e-06
Iter: 7 loss: 3.61396314e-06
Iter: 8 loss: 5.35667778e-06
Iter: 9 loss: 3.607606e-06
Iter: 10 loss: 3.42201247e-06
Iter: 11 loss: 3.69212739e-06
Iter: 12 loss: 3.3320498e-06
Iter: 13 loss: 3.24720099e-06
Iter: 14 loss: 3.27918406e-06
Iter: 15 loss: 3.18804337e-06
Iter: 16 loss: 3.11465556e-06
Iter: 17 loss: 3.11275107e-06
Iter: 18 loss: 3.0444894e-06
Iter: 19 loss: 2.89486434e-06
Iter: 20 loss: 5.11012104e-06
Iter: 21 loss: 2.88837055e-06
Iter: 22 loss: 2.7573667e-06
Iter: 23 loss: 3.47847345e-06
Iter: 24 loss: 2.73840783e-06
Iter: 25 loss: 2.61548757e-06
Iter: 26 loss: 3.69284862e-06
Iter: 27 loss: 2.6091991e-06
Iter: 28 loss: 2.5126551e-06
Iter: 29 loss: 2.44436615e-06
Iter: 30 loss: 2.41044722e-06
Iter: 31 loss: 2.34928257e-06
Iter: 32 loss: 2.55202895e-06
Iter: 33 loss: 2.33250466e-06
Iter: 34 loss: 2.27930514e-06
Iter: 35 loss: 2.34656227e-06
Iter: 36 loss: 2.25198e-06
Iter: 37 loss: 2.20119637e-06
Iter: 38 loss: 2.24988798e-06
Iter: 39 loss: 2.17232218e-06
Iter: 40 loss: 2.09719246e-06
Iter: 41 loss: 2.55189184e-06
Iter: 42 loss: 2.08800543e-06
Iter: 43 loss: 2.02870933e-06
Iter: 44 loss: 2.02870206e-06
Iter: 45 loss: 1.99602209e-06
Iter: 46 loss: 1.90403512e-06
Iter: 47 loss: 2.37937866e-06
Iter: 48 loss: 1.87446369e-06
Iter: 49 loss: 1.9362783e-06
Iter: 50 loss: 1.8357016e-06
Iter: 51 loss: 1.81249891e-06
Iter: 52 loss: 1.77074185e-06
Iter: 53 loss: 2.78299626e-06
Iter: 54 loss: 1.77074185e-06
Iter: 55 loss: 1.73759111e-06
Iter: 56 loss: 1.8642346e-06
Iter: 57 loss: 1.72972989e-06
Iter: 58 loss: 1.69004716e-06
Iter: 59 loss: 1.97849567e-06
Iter: 60 loss: 1.68675138e-06
Iter: 61 loss: 1.67370763e-06
Iter: 62 loss: 1.64367214e-06
Iter: 63 loss: 2.02554884e-06
Iter: 64 loss: 1.64151697e-06
Iter: 65 loss: 1.59591241e-06
Iter: 66 loss: 1.63081518e-06
Iter: 67 loss: 1.56809915e-06
Iter: 68 loss: 1.55804628e-06
Iter: 69 loss: 1.54551458e-06
Iter: 70 loss: 1.52151063e-06
Iter: 71 loss: 1.48998106e-06
Iter: 72 loss: 1.48802724e-06
Iter: 73 loss: 1.45853301e-06
Iter: 74 loss: 1.51351787e-06
Iter: 75 loss: 1.44600153e-06
Iter: 76 loss: 1.41835494e-06
Iter: 77 loss: 1.42324586e-06
Iter: 78 loss: 1.39764893e-06
Iter: 79 loss: 1.36904953e-06
Iter: 80 loss: 1.36815538e-06
Iter: 81 loss: 1.34671575e-06
Iter: 82 loss: 1.45383797e-06
Iter: 83 loss: 1.3431212e-06
Iter: 84 loss: 1.33169942e-06
Iter: 85 loss: 1.30676165e-06
Iter: 86 loss: 1.68513122e-06
Iter: 87 loss: 1.30576132e-06
Iter: 88 loss: 1.30066496e-06
Iter: 89 loss: 1.29073942e-06
Iter: 90 loss: 1.28402007e-06
Iter: 91 loss: 1.26987311e-06
Iter: 92 loss: 1.50128312e-06
Iter: 93 loss: 1.26951568e-06
Iter: 94 loss: 1.25653605e-06
Iter: 95 loss: 1.36757444e-06
Iter: 96 loss: 1.25580345e-06
Iter: 97 loss: 1.24093503e-06
Iter: 98 loss: 1.2720634e-06
Iter: 99 loss: 1.23501331e-06
Iter: 100 loss: 1.22597385e-06
Iter: 101 loss: 1.20474692e-06
Iter: 102 loss: 1.45727563e-06
Iter: 103 loss: 1.20291952e-06
Iter: 104 loss: 1.17383991e-06
Iter: 105 loss: 1.32406899e-06
Iter: 106 loss: 1.16920603e-06
Iter: 107 loss: 1.16651631e-06
Iter: 108 loss: 1.15819444e-06
Iter: 109 loss: 1.15361036e-06
Iter: 110 loss: 1.14068825e-06
Iter: 111 loss: 1.20422169e-06
Iter: 112 loss: 1.1362763e-06
Iter: 113 loss: 1.1225668e-06
Iter: 114 loss: 1.24735641e-06
Iter: 115 loss: 1.12195528e-06
Iter: 116 loss: 1.1160264e-06
Iter: 117 loss: 1.11567249e-06
Iter: 118 loss: 1.10971041e-06
Iter: 119 loss: 1.0990309e-06
Iter: 120 loss: 1.36019901e-06
Iter: 121 loss: 1.09906523e-06
Iter: 122 loss: 1.08698146e-06
Iter: 123 loss: 1.12404609e-06
Iter: 124 loss: 1.08338168e-06
Iter: 125 loss: 1.07422227e-06
Iter: 126 loss: 1.07419487e-06
Iter: 127 loss: 1.06695893e-06
Iter: 128 loss: 1.05045501e-06
Iter: 129 loss: 1.2621872e-06
Iter: 130 loss: 1.0492779e-06
Iter: 131 loss: 1.03911577e-06
Iter: 132 loss: 1.03897048e-06
Iter: 133 loss: 1.0313704e-06
Iter: 134 loss: 1.09048e-06
Iter: 135 loss: 1.03084312e-06
Iter: 136 loss: 1.02701279e-06
Iter: 137 loss: 1.01578166e-06
Iter: 138 loss: 1.05652271e-06
Iter: 139 loss: 1.01082594e-06
Iter: 140 loss: 1.00175225e-06
Iter: 141 loss: 1.00168222e-06
Iter: 142 loss: 9.96024e-07
Iter: 143 loss: 9.96005383e-07
Iter: 144 loss: 9.91261913e-07
Iter: 145 loss: 9.77744207e-07
Iter: 146 loss: 1.04132641e-06
Iter: 147 loss: 9.72913199e-07
Iter: 148 loss: 9.62622494e-07
Iter: 149 loss: 1.07510687e-06
Iter: 150 loss: 9.62383751e-07
Iter: 151 loss: 9.56093345e-07
Iter: 152 loss: 1.02982096e-06
Iter: 153 loss: 9.5597693e-07
Iter: 154 loss: 9.49430614e-07
Iter: 155 loss: 9.67591859e-07
Iter: 156 loss: 9.47311548e-07
Iter: 157 loss: 9.43778673e-07
Iter: 158 loss: 9.36342758e-07
Iter: 159 loss: 1.06194784e-06
Iter: 160 loss: 9.36191668e-07
Iter: 161 loss: 9.29320549e-07
Iter: 162 loss: 9.28914233e-07
Iter: 163 loss: 9.25195422e-07
Iter: 164 loss: 9.20231059e-07
Iter: 165 loss: 9.19918648e-07
Iter: 166 loss: 9.13909e-07
Iter: 167 loss: 9.31625152e-07
Iter: 168 loss: 9.12092e-07
Iter: 169 loss: 9.03823775e-07
Iter: 170 loss: 9.41856456e-07
Iter: 171 loss: 9.02285251e-07
Iter: 172 loss: 8.97605332e-07
Iter: 173 loss: 8.98816e-07
Iter: 174 loss: 8.94155619e-07
Iter: 175 loss: 8.90509398e-07
Iter: 176 loss: 8.90188858e-07
Iter: 177 loss: 8.87473107e-07
Iter: 178 loss: 8.85405143e-07
Iter: 179 loss: 8.8445222e-07
Iter: 180 loss: 8.82041945e-07
Iter: 181 loss: 8.77144203e-07
Iter: 182 loss: 9.63513e-07
Iter: 183 loss: 8.77055072e-07
Iter: 184 loss: 8.70565884e-07
Iter: 185 loss: 8.65508e-07
Iter: 186 loss: 8.63486719e-07
Iter: 187 loss: 8.5984675e-07
Iter: 188 loss: 8.5898472e-07
Iter: 189 loss: 8.53919573e-07
Iter: 190 loss: 8.60182695e-07
Iter: 191 loss: 8.51277e-07
Iter: 192 loss: 8.46959097e-07
Iter: 193 loss: 8.44578551e-07
Iter: 194 loss: 8.42691122e-07
Iter: 195 loss: 8.44499766e-07
Iter: 196 loss: 8.40841722e-07
Iter: 197 loss: 8.39954112e-07
Iter: 198 loss: 8.37249e-07
Iter: 199 loss: 8.44639715e-07
Iter: 200 loss: 8.35803291e-07
Iter: 201 loss: 8.33184572e-07
Iter: 202 loss: 8.33109766e-07
Iter: 203 loss: 8.30314718e-07
Iter: 204 loss: 8.35217747e-07
Iter: 205 loss: 8.29082751e-07
Iter: 206 loss: 8.26560097e-07
Iter: 207 loss: 8.21000356e-07
Iter: 208 loss: 8.9944308e-07
Iter: 209 loss: 8.20683283e-07
Iter: 210 loss: 8.15006274e-07
Iter: 211 loss: 8.8325362e-07
Iter: 212 loss: 8.1495466e-07
Iter: 213 loss: 8.12579685e-07
Iter: 214 loss: 8.12550923e-07
Iter: 215 loss: 8.10311519e-07
Iter: 216 loss: 8.04698971e-07
Iter: 217 loss: 8.58535373e-07
Iter: 218 loss: 8.03939656e-07
Iter: 219 loss: 7.99396844e-07
Iter: 220 loss: 8.3675684e-07
Iter: 221 loss: 7.9910626e-07
Iter: 222 loss: 7.96758172e-07
Iter: 223 loss: 8.17747946e-07
Iter: 224 loss: 7.96653808e-07
Iter: 225 loss: 7.93453182e-07
Iter: 226 loss: 7.90292688e-07
Iter: 227 loss: 7.89627563e-07
Iter: 228 loss: 7.87097747e-07
Iter: 229 loss: 7.93177833e-07
Iter: 230 loss: 7.8619729e-07
Iter: 231 loss: 7.83585165e-07
Iter: 232 loss: 8.19262937e-07
Iter: 233 loss: 7.83586813e-07
Iter: 234 loss: 7.81831829e-07
Iter: 235 loss: 7.78276558e-07
Iter: 236 loss: 8.4045007e-07
Iter: 237 loss: 7.78213519e-07
Iter: 238 loss: 7.76953584e-07
Iter: 239 loss: 7.7667e-07
Iter: 240 loss: 7.74860609e-07
Iter: 241 loss: 7.71382645e-07
Iter: 242 loss: 8.45251179e-07
Iter: 243 loss: 7.71360533e-07
Iter: 244 loss: 7.67693905e-07
Iter: 245 loss: 7.72231886e-07
Iter: 246 loss: 7.65747586e-07
Iter: 247 loss: 7.62237562e-07
Iter: 248 loss: 7.77385424e-07
Iter: 249 loss: 7.61525598e-07
Iter: 250 loss: 7.57859425e-07
Iter: 251 loss: 7.93833806e-07
Iter: 252 loss: 7.57766202e-07
Iter: 253 loss: 7.5548553e-07
Iter: 254 loss: 7.55083249e-07
Iter: 255 loss: 7.53556378e-07
Iter: 256 loss: 7.51784341e-07
Iter: 257 loss: 7.55142707e-07
Iter: 258 loss: 7.51009168e-07
Iter: 259 loss: 7.49689548e-07
Iter: 260 loss: 7.49644073e-07
Iter: 261 loss: 7.48504817e-07
Iter: 262 loss: 7.46528201e-07
Iter: 263 loss: 7.46539968e-07
Iter: 264 loss: 7.44738543e-07
Iter: 265 loss: 7.501543e-07
Iter: 266 loss: 7.44193e-07
Iter: 267 loss: 7.41126769e-07
Iter: 268 loss: 7.42697182e-07
Iter: 269 loss: 7.39055963e-07
Iter: 270 loss: 7.36806896e-07
Iter: 271 loss: 7.36974869e-07
Iter: 272 loss: 7.35036e-07
Iter: 273 loss: 7.32724743e-07
Iter: 274 loss: 7.32712238e-07
Iter: 275 loss: 7.30552756e-07
Iter: 276 loss: 7.26713665e-07
Iter: 277 loss: 8.22094762e-07
Iter: 278 loss: 7.26705821e-07
Iter: 279 loss: 7.25005464e-07
Iter: 280 loss: 7.3152e-07
Iter: 281 loss: 7.24616768e-07
Iter: 282 loss: 7.23149583e-07
Iter: 283 loss: 7.35966296e-07
Iter: 284 loss: 7.23106609e-07
Iter: 285 loss: 7.21383799e-07
Iter: 286 loss: 7.23204096e-07
Iter: 287 loss: 7.20462708e-07
Iter: 288 loss: 7.19168611e-07
Iter: 289 loss: 7.18005481e-07
Iter: 290 loss: 7.17698072e-07
Iter: 291 loss: 7.152438e-07
Iter: 292 loss: 7.24456356e-07
Iter: 293 loss: 7.14670932e-07
Iter: 294 loss: 7.12613428e-07
Iter: 295 loss: 7.12619e-07
Iter: 296 loss: 7.11389134e-07
Iter: 297 loss: 7.08313337e-07
Iter: 298 loss: 7.35631261e-07
Iter: 299 loss: 7.07820959e-07
Iter: 300 loss: 7.07631102e-07
Iter: 301 loss: 7.06403966e-07
Iter: 302 loss: 7.05255388e-07
Iter: 303 loss: 7.04733282e-07
Iter: 304 loss: 7.04161437e-07
Iter: 305 loss: 7.03189e-07
Iter: 306 loss: 7.03563273e-07
Iter: 307 loss: 7.02509169e-07
Iter: 308 loss: 7.00573196e-07
Iter: 309 loss: 7.08462039e-07
Iter: 310 loss: 7.00170062e-07
Iter: 311 loss: 6.99079237e-07
Iter: 312 loss: 6.98481927e-07
Iter: 313 loss: 6.98030533e-07
Iter: 314 loss: 6.96638779e-07
Iter: 315 loss: 6.99634484e-07
Iter: 316 loss: 6.96079326e-07
Iter: 317 loss: 6.94269033e-07
Iter: 318 loss: 7.08481707e-07
Iter: 319 loss: 6.9413062e-07
Iter: 320 loss: 6.93064237e-07
Iter: 321 loss: 6.92173444e-07
Iter: 322 loss: 6.91881382e-07
Iter: 323 loss: 6.90104457e-07
Iter: 324 loss: 6.90500201e-07
Iter: 325 loss: 6.88813088e-07
Iter: 326 loss: 6.88156774e-07
Iter: 327 loss: 6.87625516e-07
Iter: 328 loss: 6.86768715e-07
Iter: 329 loss: 6.85836312e-07
Iter: 330 loss: 6.85673285e-07
Iter: 331 loss: 6.84263853e-07
Iter: 332 loss: 6.84201439e-07
Iter: 333 loss: 6.83121243e-07
Iter: 334 loss: 6.81224492e-07
Iter: 335 loss: 6.81213351e-07
Iter: 336 loss: 6.80577728e-07
Iter: 337 loss: 6.79507593e-07
Iter: 338 loss: 6.79513505e-07
Iter: 339 loss: 6.78668243e-07
Iter: 340 loss: 6.78619244e-07
Iter: 341 loss: 6.77833839e-07
Iter: 342 loss: 6.76465277e-07
Iter: 343 loss: 7.09558378e-07
Iter: 344 loss: 6.7644811e-07
Iter: 345 loss: 6.75246042e-07
Iter: 346 loss: 6.74744797e-07
Iter: 347 loss: 6.74074329e-07
Iter: 348 loss: 6.72768863e-07
Iter: 349 loss: 6.72681381e-07
Iter: 350 loss: 6.7118458e-07
Iter: 351 loss: 6.71058672e-07
Iter: 352 loss: 6.69948577e-07
Iter: 353 loss: 6.68463258e-07
Iter: 354 loss: 6.65898824e-07
Iter: 355 loss: 6.65904111e-07
Iter: 356 loss: 6.65336074e-07
Iter: 357 loss: 6.64600577e-07
Iter: 358 loss: 6.63724506e-07
Iter: 359 loss: 6.71616363e-07
Iter: 360 loss: 6.63675166e-07
Iter: 361 loss: 6.63138394e-07
Iter: 362 loss: 6.61720151e-07
Iter: 363 loss: 6.7296088e-07
Iter: 364 loss: 6.61481749e-07
Iter: 365 loss: 6.61539957e-07
Iter: 366 loss: 6.60837202e-07
Iter: 367 loss: 6.60317824e-07
Iter: 368 loss: 6.59478587e-07
Iter: 369 loss: 6.59494219e-07
Iter: 370 loss: 6.58412546e-07
Iter: 371 loss: 6.57960925e-07
Iter: 372 loss: 6.57403518e-07
Iter: 373 loss: 6.54847327e-07
Iter: 374 loss: 6.6301385e-07
Iter: 375 loss: 6.54141104e-07
Iter: 376 loss: 6.53038114e-07
Iter: 377 loss: 6.51316441e-07
Iter: 378 loss: 6.51323148e-07
Iter: 379 loss: 6.49106937e-07
Iter: 380 loss: 6.58510373e-07
Iter: 381 loss: 6.48659693e-07
Iter: 382 loss: 6.48443347e-07
Iter: 383 loss: 6.47834895e-07
Iter: 384 loss: 6.47335696e-07
Iter: 385 loss: 6.46024318e-07
Iter: 386 loss: 6.56395855e-07
Iter: 387 loss: 6.4581377e-07
Iter: 388 loss: 6.44464535e-07
Iter: 389 loss: 6.48414641e-07
Iter: 390 loss: 6.44058e-07
Iter: 391 loss: 6.42799421e-07
Iter: 392 loss: 6.4608372e-07
Iter: 393 loss: 6.42326427e-07
Iter: 394 loss: 6.40777557e-07
Iter: 395 loss: 6.60548835e-07
Iter: 396 loss: 6.40778467e-07
Iter: 397 loss: 6.40133578e-07
Iter: 398 loss: 6.38985625e-07
Iter: 399 loss: 6.38998472e-07
Iter: 400 loss: 6.38284462e-07
Iter: 401 loss: 6.3821e-07
Iter: 402 loss: 6.37398557e-07
Iter: 403 loss: 6.35636354e-07
Iter: 404 loss: 6.61036609e-07
Iter: 405 loss: 6.35547394e-07
Iter: 406 loss: 6.34638468e-07
Iter: 407 loss: 6.34619e-07
Iter: 408 loss: 6.3372147e-07
Iter: 409 loss: 6.362269e-07
Iter: 410 loss: 6.33440948e-07
Iter: 411 loss: 6.32611773e-07
Iter: 412 loss: 6.31074499e-07
Iter: 413 loss: 6.65908431e-07
Iter: 414 loss: 6.31054149e-07
Iter: 415 loss: 6.29673877e-07
Iter: 416 loss: 6.37183291e-07
Iter: 417 loss: 6.29432577e-07
Iter: 418 loss: 6.28309635e-07
Iter: 419 loss: 6.43552369e-07
Iter: 420 loss: 6.2831424e-07
Iter: 421 loss: 6.27292422e-07
Iter: 422 loss: 6.27400595e-07
Iter: 423 loss: 6.26512133e-07
Iter: 424 loss: 6.25771918e-07
Iter: 425 loss: 6.25074449e-07
Iter: 426 loss: 6.24906136e-07
Iter: 427 loss: 6.23450319e-07
Iter: 428 loss: 6.26120936e-07
Iter: 429 loss: 6.22837206e-07
Iter: 430 loss: 6.21842617e-07
Iter: 431 loss: 6.36667096e-07
Iter: 432 loss: 6.2185552e-07
Iter: 433 loss: 6.20521291e-07
Iter: 434 loss: 6.20440062e-07
Iter: 435 loss: 6.19455875e-07
Iter: 436 loss: 6.18303488e-07
Iter: 437 loss: 6.20122364e-07
Iter: 438 loss: 6.17743e-07
Iter: 439 loss: 6.16957664e-07
Iter: 440 loss: 6.16918499e-07
Iter: 441 loss: 6.16389798e-07
Iter: 442 loss: 6.15051e-07
Iter: 443 loss: 6.2578107e-07
Iter: 444 loss: 6.14778116e-07
Iter: 445 loss: 6.14511237e-07
Iter: 446 loss: 6.14117198e-07
Iter: 447 loss: 6.1356e-07
Iter: 448 loss: 6.13390171e-07
Iter: 449 loss: 6.13030863e-07
Iter: 450 loss: 6.12304575e-07
Iter: 451 loss: 6.11014e-07
Iter: 452 loss: 6.41773909e-07
Iter: 453 loss: 6.10996722e-07
Iter: 454 loss: 6.10220411e-07
Iter: 455 loss: 6.10157201e-07
Iter: 456 loss: 6.09463e-07
Iter: 457 loss: 6.13689849e-07
Iter: 458 loss: 6.09372705e-07
Iter: 459 loss: 6.0868723e-07
Iter: 460 loss: 6.06994718e-07
Iter: 461 loss: 6.24521249e-07
Iter: 462 loss: 6.06780532e-07
Iter: 463 loss: 6.05194032e-07
Iter: 464 loss: 6.14034036e-07
Iter: 465 loss: 6.04987065e-07
Iter: 466 loss: 6.03633964e-07
Iter: 467 loss: 6.05738e-07
Iter: 468 loss: 6.02984244e-07
Iter: 469 loss: 6.02695764e-07
Iter: 470 loss: 6.02403361e-07
Iter: 471 loss: 6.01783086e-07
Iter: 472 loss: 6.00916906e-07
Iter: 473 loss: 6.00878593e-07
Iter: 474 loss: 5.99946e-07
Iter: 475 loss: 6.0429528e-07
Iter: 476 loss: 5.99779753e-07
Iter: 477 loss: 5.98827853e-07
Iter: 478 loss: 6.0562877e-07
Iter: 479 loss: 5.98766576e-07
Iter: 480 loss: 5.98414e-07
Iter: 481 loss: 5.97524831e-07
Iter: 482 loss: 6.05613309e-07
Iter: 483 loss: 5.97391079e-07
Iter: 484 loss: 5.96714699e-07
Iter: 485 loss: 5.96620339e-07
Iter: 486 loss: 5.96113239e-07
Iter: 487 loss: 5.95331869e-07
Iter: 488 loss: 5.95331755e-07
Iter: 489 loss: 5.94513949e-07
Iter: 490 loss: 5.93412494e-07
Iter: 491 loss: 5.9335008e-07
Iter: 492 loss: 5.91894718e-07
Iter: 493 loss: 6.08473385e-07
Iter: 494 loss: 5.91870617e-07
Iter: 495 loss: 5.91162802e-07
Iter: 496 loss: 5.91137677e-07
Iter: 497 loss: 5.90512116e-07
Iter: 498 loss: 5.89157821e-07
Iter: 499 loss: 6.12516544e-07
Iter: 500 loss: 5.89133094e-07
Iter: 501 loss: 5.88529474e-07
Iter: 502 loss: 5.88502758e-07
Iter: 503 loss: 5.88111902e-07
Iter: 504 loss: 5.90877505e-07
Iter: 505 loss: 5.88079217e-07
Iter: 506 loss: 5.87661361e-07
Iter: 507 loss: 5.86920464e-07
Iter: 508 loss: 5.86930241e-07
Iter: 509 loss: 5.86600095e-07
Iter: 510 loss: 5.86536089e-07
Iter: 511 loss: 5.86187298e-07
Iter: 512 loss: 5.85772739e-07
Iter: 513 loss: 5.85733858e-07
Iter: 514 loss: 5.85041676e-07
Iter: 515 loss: 5.83786345e-07
Iter: 516 loss: 6.1370622e-07
Iter: 517 loss: 5.83788108e-07
Iter: 518 loss: 5.83471206e-07
Iter: 519 loss: 5.82973314e-07
Iter: 520 loss: 5.82589053e-07
Iter: 521 loss: 5.81555582e-07
Iter: 522 loss: 5.87474915e-07
Iter: 523 loss: 5.81270797e-07
Iter: 524 loss: 5.80103347e-07
Iter: 525 loss: 5.86569286e-07
Iter: 526 loss: 5.79921e-07
Iter: 527 loss: 5.79281902e-07
Iter: 528 loss: 5.81553195e-07
Iter: 529 loss: 5.79112e-07
Iter: 530 loss: 5.78510708e-07
Iter: 531 loss: 5.87869295e-07
Iter: 532 loss: 5.78509628e-07
Iter: 533 loss: 5.78073582e-07
Iter: 534 loss: 5.77592573e-07
Iter: 535 loss: 5.77544256e-07
Iter: 536 loss: 5.77053e-07
Iter: 537 loss: 5.8257308e-07
Iter: 538 loss: 5.77037213e-07
Iter: 539 loss: 5.76403465e-07
Iter: 540 loss: 5.75775459e-07
Iter: 541 loss: 5.75652962e-07
Iter: 542 loss: 5.74888759e-07
Iter: 543 loss: 5.75923309e-07
Iter: 544 loss: 5.74521096e-07
Iter: 545 loss: 5.73625e-07
Iter: 546 loss: 5.7701277e-07
Iter: 547 loss: 5.73423108e-07
Iter: 548 loss: 5.72887188e-07
Iter: 549 loss: 5.72861097e-07
Iter: 550 loss: 5.7247388e-07
Iter: 551 loss: 5.71592864e-07
Iter: 552 loss: 5.83054771e-07
Iter: 553 loss: 5.71537214e-07
Iter: 554 loss: 5.71025112e-07
Iter: 555 loss: 5.70981115e-07
Iter: 556 loss: 5.70451448e-07
Iter: 557 loss: 5.70276654e-07
Iter: 558 loss: 5.69956e-07
Iter: 559 loss: 5.69402687e-07
Iter: 560 loss: 5.68927589e-07
Iter: 561 loss: 5.68788323e-07
Iter: 562 loss: 5.67889401e-07
Iter: 563 loss: 5.70955876e-07
Iter: 564 loss: 5.67647078e-07
Iter: 565 loss: 5.66994686e-07
Iter: 566 loss: 5.69429e-07
Iter: 567 loss: 5.66808524e-07
Iter: 568 loss: 5.66150561e-07
Iter: 569 loss: 5.69340614e-07
Iter: 570 loss: 5.66011806e-07
Iter: 571 loss: 5.65208893e-07
Iter: 572 loss: 5.69674e-07
Iter: 573 loss: 5.65113965e-07
Iter: 574 loss: 5.6475983e-07
Iter: 575 loss: 5.6390877e-07
Iter: 576 loss: 5.71711212e-07
Iter: 577 loss: 5.63765525e-07
Iter: 578 loss: 5.62782532e-07
Iter: 579 loss: 5.72313752e-07
Iter: 580 loss: 5.62755076e-07
Iter: 581 loss: 5.62444598e-07
Iter: 582 loss: 5.62347509e-07
Iter: 583 loss: 5.62092964e-07
Iter: 584 loss: 5.61463196e-07
Iter: 585 loss: 5.6629591e-07
Iter: 586 loss: 5.61314039e-07
Iter: 587 loss: 5.61007141e-07
Iter: 588 loss: 5.60934495e-07
Iter: 589 loss: 5.60571493e-07
Iter: 590 loss: 5.60687113e-07
Iter: 591 loss: 5.60318426e-07
Iter: 592 loss: 5.59822467e-07
Iter: 593 loss: 5.58948841e-07
Iter: 594 loss: 5.80577534e-07
Iter: 595 loss: 5.58947704e-07
Iter: 596 loss: 5.58657575e-07
Iter: 597 loss: 5.58457202e-07
Iter: 598 loss: 5.58006434e-07
Iter: 599 loss: 5.57451244e-07
Iter: 600 loss: 5.57391559e-07
Iter: 601 loss: 5.56491898e-07
Iter: 602 loss: 5.55872589e-07
Iter: 603 loss: 5.55583e-07
Iter: 604 loss: 5.54738847e-07
Iter: 605 loss: 5.64349818e-07
Iter: 606 loss: 5.54736971e-07
Iter: 607 loss: 5.5426375e-07
Iter: 608 loss: 5.54248459e-07
Iter: 609 loss: 5.53945824e-07
Iter: 610 loss: 5.53242842e-07
Iter: 611 loss: 5.63794629e-07
Iter: 612 loss: 5.53213908e-07
Iter: 613 loss: 5.52654058e-07
Iter: 614 loss: 5.54291546e-07
Iter: 615 loss: 5.52482447e-07
Iter: 616 loss: 5.52044128e-07
Iter: 617 loss: 5.52032759e-07
Iter: 618 loss: 5.51583753e-07
Iter: 619 loss: 5.50976551e-07
Iter: 620 loss: 5.50942e-07
Iter: 621 loss: 5.50491905e-07
Iter: 622 loss: 5.52506719e-07
Iter: 623 loss: 5.50410675e-07
Iter: 624 loss: 5.49807737e-07
Iter: 625 loss: 5.52150368e-07
Iter: 626 loss: 5.49699052e-07
Iter: 627 loss: 5.49283243e-07
Iter: 628 loss: 5.48687808e-07
Iter: 629 loss: 5.48690537e-07
Iter: 630 loss: 5.48202138e-07
Iter: 631 loss: 5.48212029e-07
Iter: 632 loss: 5.47695322e-07
Iter: 633 loss: 5.47577429e-07
Iter: 634 loss: 5.47257173e-07
Iter: 635 loss: 5.46776562e-07
Iter: 636 loss: 5.47681793e-07
Iter: 637 loss: 5.46553451e-07
Iter: 638 loss: 5.4611877e-07
Iter: 639 loss: 5.46881e-07
Iter: 640 loss: 5.4591942e-07
Iter: 641 loss: 5.4544e-07
Iter: 642 loss: 5.52715051e-07
Iter: 643 loss: 5.45445118e-07
Iter: 644 loss: 5.45212174e-07
Iter: 645 loss: 5.4476584e-07
Iter: 646 loss: 5.54966391e-07
Iter: 647 loss: 5.44770671e-07
Iter: 648 loss: 5.44249929e-07
Iter: 649 loss: 5.43854242e-07
Iter: 650 loss: 5.43645797e-07
Iter: 651 loss: 5.43490671e-07
Iter: 652 loss: 5.43193892e-07
Iter: 653 loss: 5.42779617e-07
Iter: 654 loss: 5.42067369e-07
Iter: 655 loss: 5.60254648e-07
Iter: 656 loss: 5.42060832e-07
Iter: 657 loss: 5.41256611e-07
Iter: 658 loss: 5.43213162e-07
Iter: 659 loss: 5.4101065e-07
Iter: 660 loss: 5.40203473e-07
Iter: 661 loss: 5.51696814e-07
Iter: 662 loss: 5.40213591e-07
Iter: 663 loss: 5.40000883e-07
Iter: 664 loss: 5.39467067e-07
Iter: 665 loss: 5.45551586e-07
Iter: 666 loss: 5.39433586e-07
Iter: 667 loss: 5.39118901e-07
Iter: 668 loss: 5.39023063e-07
Iter: 669 loss: 5.38739357e-07
Iter: 670 loss: 5.38688141e-07
Iter: 671 loss: 5.38478616e-07
Iter: 672 loss: 5.38184281e-07
Iter: 673 loss: 5.38724407e-07
Iter: 674 loss: 5.38067411e-07
Iter: 675 loss: 5.37684286e-07
Iter: 676 loss: 5.39041253e-07
Iter: 677 loss: 5.37580149e-07
Iter: 678 loss: 5.37005e-07
Iter: 679 loss: 5.3728138e-07
Iter: 680 loss: 5.36621769e-07
Iter: 681 loss: 5.36176685e-07
Iter: 682 loss: 5.35384345e-07
Iter: 683 loss: 5.35374056e-07
Iter: 684 loss: 5.34584785e-07
Iter: 685 loss: 5.44850479e-07
Iter: 686 loss: 5.34589503e-07
Iter: 687 loss: 5.34148398e-07
Iter: 688 loss: 5.34148796e-07
Iter: 689 loss: 5.33941659e-07
Iter: 690 loss: 5.33468e-07
Iter: 691 loss: 5.38356062e-07
Iter: 692 loss: 5.33394541e-07
Iter: 693 loss: 5.33229581e-07
Iter: 694 loss: 5.331284e-07
Iter: 695 loss: 5.32894887e-07
Iter: 696 loss: 5.32607146e-07
Iter: 697 loss: 5.32586114e-07
Iter: 698 loss: 5.32252386e-07
Iter: 699 loss: 5.32123124e-07
Iter: 700 loss: 5.31952878e-07
Iter: 701 loss: 5.31340731e-07
Iter: 702 loss: 5.37265691e-07
Iter: 703 loss: 5.31324531e-07
Iter: 704 loss: 5.31068849e-07
Iter: 705 loss: 5.30877628e-07
Iter: 706 loss: 5.3082681e-07
Iter: 707 loss: 5.30358079e-07
Iter: 708 loss: 5.31305886e-07
Iter: 709 loss: 5.30175043e-07
Iter: 710 loss: 5.29652539e-07
Iter: 711 loss: 5.34869855e-07
Iter: 712 loss: 5.2964856e-07
Iter: 713 loss: 5.29412091e-07
Iter: 714 loss: 5.29021065e-07
Iter: 715 loss: 5.29044655e-07
Iter: 716 loss: 5.28467638e-07
Iter: 717 loss: 5.28599173e-07
Iter: 718 loss: 5.28057399e-07
Iter: 719 loss: 5.28024e-07
Iter: 720 loss: 5.27746238e-07
Iter: 721 loss: 5.27524094e-07
Iter: 722 loss: 5.27057182e-07
Iter: 723 loss: 5.34699439e-07
Iter: 724 loss: 5.27036832e-07
Iter: 725 loss: 5.26586462e-07
Iter: 726 loss: 5.29706767e-07
Iter: 727 loss: 5.26542294e-07
Iter: 728 loss: 5.26046563e-07
Iter: 729 loss: 5.28656756e-07
Iter: 730 loss: 5.26006e-07
Iter: 731 loss: 5.25772634e-07
Iter: 732 loss: 5.25325504e-07
Iter: 733 loss: 5.32537911e-07
Iter: 734 loss: 5.25319365e-07
Iter: 735 loss: 5.2507329e-07
Iter: 736 loss: 5.24984671e-07
Iter: 737 loss: 5.24751044e-07
Iter: 738 loss: 5.24303687e-07
Iter: 739 loss: 5.34227183e-07
Iter: 740 loss: 5.24309542e-07
Iter: 741 loss: 5.23821939e-07
Iter: 742 loss: 5.24906568e-07
Iter: 743 loss: 5.23643848e-07
Iter: 744 loss: 5.23063704e-07
Iter: 745 loss: 5.25541566e-07
Iter: 746 loss: 5.2292512e-07
Iter: 747 loss: 5.22399546e-07
Iter: 748 loss: 5.23103154e-07
Iter: 749 loss: 5.22162168e-07
Iter: 750 loss: 5.21760171e-07
Iter: 751 loss: 5.21153424e-07
Iter: 752 loss: 5.21128527e-07
Iter: 753 loss: 5.20924175e-07
Iter: 754 loss: 5.20752053e-07
Iter: 755 loss: 5.20414858e-07
Iter: 756 loss: 5.20763706e-07
Iter: 757 loss: 5.20237e-07
Iter: 758 loss: 5.19946923e-07
Iter: 759 loss: 5.19572438e-07
Iter: 760 loss: 5.19553794e-07
Iter: 761 loss: 5.19106152e-07
Iter: 762 loss: 5.1911843e-07
Iter: 763 loss: 5.18890886e-07
Iter: 764 loss: 5.18427328e-07
Iter: 765 loss: 5.26080726e-07
Iter: 766 loss: 5.18410218e-07
Iter: 767 loss: 5.17923809e-07
Iter: 768 loss: 5.2200869e-07
Iter: 769 loss: 5.17892e-07
Iter: 770 loss: 5.17311946e-07
Iter: 771 loss: 5.18130321e-07
Iter: 772 loss: 5.17035119e-07
Iter: 773 loss: 5.16736918e-07
Iter: 774 loss: 5.1717609e-07
Iter: 775 loss: 5.16594582e-07
Iter: 776 loss: 5.16197304e-07
Iter: 777 loss: 5.18047e-07
Iter: 778 loss: 5.1610084e-07
Iter: 779 loss: 5.15716295e-07
Iter: 780 loss: 5.16077478e-07
Iter: 781 loss: 5.15491934e-07
Iter: 782 loss: 5.15105285e-07
Iter: 783 loss: 5.1496562e-07
Iter: 784 loss: 5.14760075e-07
Iter: 785 loss: 5.14213411e-07
Iter: 786 loss: 5.17638796e-07
Iter: 787 loss: 5.14164e-07
Iter: 788 loss: 5.1365862e-07
Iter: 789 loss: 5.17889248e-07
Iter: 790 loss: 5.13606835e-07
Iter: 791 loss: 5.13386965e-07
Iter: 792 loss: 5.13034195e-07
Iter: 793 loss: 5.13025043e-07
Iter: 794 loss: 5.12720476e-07
Iter: 795 loss: 5.12722124e-07
Iter: 796 loss: 5.12478834e-07
Iter: 797 loss: 5.12154543e-07
Iter: 798 loss: 5.12135955e-07
Iter: 799 loss: 5.11864e-07
Iter: 800 loss: 5.12175e-07
Iter: 801 loss: 5.11719236e-07
Iter: 802 loss: 5.11238227e-07
Iter: 803 loss: 5.13782084e-07
Iter: 804 loss: 5.11175074e-07
Iter: 805 loss: 5.10861184e-07
Iter: 806 loss: 5.1068406e-07
Iter: 807 loss: 5.10565485e-07
Iter: 808 loss: 5.10208e-07
Iter: 809 loss: 5.15051e-07
Iter: 810 loss: 5.10204472e-07
Iter: 811 loss: 5.09848292e-07
Iter: 812 loss: 5.09599488e-07
Iter: 813 loss: 5.09480856e-07
Iter: 814 loss: 5.08926632e-07
Iter: 815 loss: 5.09716358e-07
Iter: 816 loss: 5.08643382e-07
Iter: 817 loss: 5.08231551e-07
Iter: 818 loss: 5.09246092e-07
Iter: 819 loss: 5.08069661e-07
Iter: 820 loss: 5.07679317e-07
Iter: 821 loss: 5.07688469e-07
Iter: 822 loss: 5.07479967e-07
Iter: 823 loss: 5.07193818e-07
Iter: 824 loss: 5.07197456e-07
Iter: 825 loss: 5.06953938e-07
Iter: 826 loss: 5.10736527e-07
Iter: 827 loss: 5.0695752e-07
Iter: 828 loss: 5.06681e-07
Iter: 829 loss: 5.06235267e-07
Iter: 830 loss: 5.06212643e-07
Iter: 831 loss: 5.05787966e-07
Iter: 832 loss: 5.06417905e-07
Iter: 833 loss: 5.05569062e-07
Iter: 834 loss: 5.0517167e-07
Iter: 835 loss: 5.05165303e-07
Iter: 836 loss: 5.04841523e-07
Iter: 837 loss: 5.04096e-07
Iter: 838 loss: 5.12271299e-07
Iter: 839 loss: 5.0402582e-07
Iter: 840 loss: 5.03623141e-07
Iter: 841 loss: 5.03563e-07
Iter: 842 loss: 5.03230126e-07
Iter: 843 loss: 5.03618821e-07
Iter: 844 loss: 5.03046863e-07
Iter: 845 loss: 5.02661294e-07
Iter: 846 loss: 5.02376452e-07
Iter: 847 loss: 5.02260718e-07
Iter: 848 loss: 5.0179608e-07
Iter: 849 loss: 5.03787305e-07
Iter: 850 loss: 5.01690181e-07
Iter: 851 loss: 5.01487079e-07
Iter: 852 loss: 5.01467184e-07
Iter: 853 loss: 5.01233217e-07
Iter: 854 loss: 5.00690248e-07
Iter: 855 loss: 5.05430933e-07
Iter: 856 loss: 5.00586509e-07
Iter: 857 loss: 5.00340434e-07
Iter: 858 loss: 5.00282965e-07
Iter: 859 loss: 4.99986754e-07
Iter: 860 loss: 4.99736643e-07
Iter: 861 loss: 4.99656949e-07
Iter: 862 loss: 4.99159682e-07
Iter: 863 loss: 4.98770305e-07
Iter: 864 loss: 4.98597501e-07
Iter: 865 loss: 4.98547934e-07
Iter: 866 loss: 4.98313625e-07
Iter: 867 loss: 4.98061695e-07
Iter: 868 loss: 4.97546807e-07
Iter: 869 loss: 5.05038e-07
Iter: 870 loss: 4.97533335e-07
Iter: 871 loss: 4.97095812e-07
Iter: 872 loss: 5.01676936e-07
Iter: 873 loss: 4.97074666e-07
Iter: 874 loss: 4.9672019e-07
Iter: 875 loss: 4.98683335e-07
Iter: 876 loss: 4.9665573e-07
Iter: 877 loss: 4.96424605e-07
Iter: 878 loss: 4.96162727e-07
Iter: 879 loss: 4.96116229e-07
Iter: 880 loss: 4.95642269e-07
Iter: 881 loss: 4.96539258e-07
Iter: 882 loss: 4.95432346e-07
Iter: 883 loss: 4.9524283e-07
Iter: 884 loss: 4.95180302e-07
Iter: 885 loss: 4.94951053e-07
Iter: 886 loss: 4.94435767e-07
Iter: 887 loss: 5.03096885e-07
Iter: 888 loss: 4.94416497e-07
Iter: 889 loss: 4.93900757e-07
Iter: 890 loss: 4.96268626e-07
Iter: 891 loss: 4.93791845e-07
Iter: 892 loss: 4.93360176e-07
Iter: 893 loss: 4.99764042e-07
Iter: 894 loss: 4.93361881e-07
Iter: 895 loss: 4.93169637e-07
Iter: 896 loss: 4.92675e-07
Iter: 897 loss: 4.96174266e-07
Iter: 898 loss: 4.92565334e-07
Iter: 899 loss: 4.92294248e-07
Iter: 900 loss: 4.92213928e-07
Iter: 901 loss: 4.91915e-07
Iter: 902 loss: 4.92255e-07
Iter: 903 loss: 4.91723426e-07
Iter: 904 loss: 4.91485366e-07
Iter: 905 loss: 4.91139076e-07
Iter: 906 loss: 4.91135097e-07
Iter: 907 loss: 4.9088635e-07
Iter: 908 loss: 4.90829166e-07
Iter: 909 loss: 4.90632942e-07
Iter: 910 loss: 4.90426942e-07
Iter: 911 loss: 4.90396133e-07
Iter: 912 loss: 4.90074285e-07
Iter: 913 loss: 4.90030629e-07
Iter: 914 loss: 4.89819627e-07
Iter: 915 loss: 4.89519039e-07
Iter: 916 loss: 4.89539218e-07
Iter: 917 loss: 4.89287686e-07
Iter: 918 loss: 4.89671095e-07
Iter: 919 loss: 4.89153081e-07
Iter: 920 loss: 4.88891942e-07
Iter: 921 loss: 4.88379499e-07
Iter: 922 loss: 4.97772e-07
Iter: 923 loss: 4.88377168e-07
Iter: 924 loss: 4.88262458e-07
Iter: 925 loss: 4.88060323e-07
Iter: 926 loss: 4.8790281e-07
Iter: 927 loss: 4.87476143e-07
Iter: 928 loss: 4.90518687e-07
Iter: 929 loss: 4.87352054e-07
Iter: 930 loss: 4.86790213e-07
Iter: 931 loss: 4.88396211e-07
Iter: 932 loss: 4.86620593e-07
Iter: 933 loss: 4.86457111e-07
Iter: 934 loss: 4.86288513e-07
Iter: 935 loss: 4.86150611e-07
Iter: 936 loss: 4.85765213e-07
Iter: 937 loss: 4.87688737e-07
Iter: 938 loss: 4.85598775e-07
Iter: 939 loss: 4.85454848e-07
Iter: 940 loss: 4.85356054e-07
Iter: 941 loss: 4.85141697e-07
Iter: 942 loss: 4.849731e-07
Iter: 943 loss: 4.84913528e-07
Iter: 944 loss: 4.84613906e-07
Iter: 945 loss: 4.84738166e-07
Iter: 946 loss: 4.8441018e-07
Iter: 947 loss: 4.84002e-07
Iter: 948 loss: 4.8646865e-07
Iter: 949 loss: 4.83939061e-07
Iter: 950 loss: 4.83634267e-07
Iter: 951 loss: 4.86179488e-07
Iter: 952 loss: 4.83621704e-07
Iter: 953 loss: 4.83431734e-07
Iter: 954 loss: 4.82884161e-07
Iter: 955 loss: 4.87676175e-07
Iter: 956 loss: 4.82800033e-07
Iter: 957 loss: 4.82627684e-07
Iter: 958 loss: 4.82476651e-07
Iter: 959 loss: 4.82211306e-07
Iter: 960 loss: 4.82062774e-07
Iter: 961 loss: 4.81960569e-07
Iter: 962 loss: 4.81638494e-07
Iter: 963 loss: 4.8112696e-07
Iter: 964 loss: 4.81115649e-07
Iter: 965 loss: 4.81536233e-07
Iter: 966 loss: 4.8093159e-07
Iter: 967 loss: 4.80754693e-07
Iter: 968 loss: 4.80492133e-07
Iter: 969 loss: 4.80480878e-07
Iter: 970 loss: 4.80185349e-07
Iter: 971 loss: 4.8027124e-07
Iter: 972 loss: 4.79936261e-07
Iter: 973 loss: 4.79583832e-07
Iter: 974 loss: 4.79586674e-07
Iter: 975 loss: 4.7941819e-07
Iter: 976 loss: 4.79146e-07
Iter: 977 loss: 4.79138e-07
Iter: 978 loss: 4.78827701e-07
Iter: 979 loss: 4.80146809e-07
Iter: 980 loss: 4.78763525e-07
Iter: 981 loss: 4.78505228e-07
Iter: 982 loss: 4.81266738e-07
Iter: 983 loss: 4.78497725e-07
Iter: 984 loss: 4.78338677e-07
Iter: 985 loss: 4.78215043e-07
Iter: 986 loss: 4.78167408e-07
Iter: 987 loss: 4.77897572e-07
Iter: 988 loss: 4.78265349e-07
Iter: 989 loss: 4.77767e-07
Iter: 990 loss: 4.77399738e-07
Iter: 991 loss: 4.80844847e-07
Iter: 992 loss: 4.77375238e-07
Iter: 993 loss: 4.77239382e-07
Iter: 994 loss: 4.76811351e-07
Iter: 995 loss: 4.79099072e-07
Iter: 996 loss: 4.76682175e-07
Iter: 997 loss: 4.76205116e-07
Iter: 998 loss: 4.82319308e-07
Iter: 999 loss: 4.76208385e-07
Iter: 1000 loss: 4.7594591e-07
Iter: 1001 loss: 4.75924537e-07
Iter: 1002 loss: 4.75801983e-07
Iter: 1003 loss: 4.75439379e-07
Iter: 1004 loss: 4.77180663e-07
Iter: 1005 loss: 4.75333337e-07
Iter: 1006 loss: 4.7524594e-07
Iter: 1007 loss: 4.75087e-07
Iter: 1008 loss: 4.74954106e-07
Iter: 1009 loss: 4.74744098e-07
Iter: 1010 loss: 4.74740574e-07
Iter: 1011 loss: 4.7443757e-07
Iter: 1012 loss: 4.74348894e-07
Iter: 1013 loss: 4.74176915e-07
Iter: 1014 loss: 4.73929049e-07
Iter: 1015 loss: 4.73892698e-07
Iter: 1016 loss: 4.73718217e-07
Iter: 1017 loss: 4.73535636e-07
Iter: 1018 loss: 4.73490218e-07
Iter: 1019 loss: 4.73212879e-07
Iter: 1020 loss: 4.73847422e-07
Iter: 1021 loss: 4.73116359e-07
Iter: 1022 loss: 4.72854197e-07
Iter: 1023 loss: 4.72853e-07
Iter: 1024 loss: 4.72692449e-07
Iter: 1025 loss: 4.72397488e-07
Iter: 1026 loss: 4.78342486e-07
Iter: 1027 loss: 4.72409965e-07
Iter: 1028 loss: 4.7210321e-07
Iter: 1029 loss: 4.72108951e-07
Iter: 1030 loss: 4.71857675e-07
Iter: 1031 loss: 4.72064187e-07
Iter: 1032 loss: 4.71717698e-07
Iter: 1033 loss: 4.71628539e-07
Iter: 1034 loss: 4.71323403e-07
Iter: 1035 loss: 4.73386308e-07
Iter: 1036 loss: 4.71259796e-07
Iter: 1037 loss: 4.70896964e-07
Iter: 1038 loss: 4.72346812e-07
Iter: 1039 loss: 4.70790212e-07
Iter: 1040 loss: 4.70543796e-07
Iter: 1041 loss: 4.70521428e-07
Iter: 1042 loss: 4.70426585e-07
Iter: 1043 loss: 4.70144386e-07
Iter: 1044 loss: 4.71557456e-07
Iter: 1045 loss: 4.70033569e-07
Iter: 1046 loss: 4.69952909e-07
Iter: 1047 loss: 4.6986861e-07
Iter: 1048 loss: 4.69696971e-07
Iter: 1049 loss: 4.69748613e-07
Iter: 1050 loss: 4.69588144e-07
Iter: 1051 loss: 4.69426027e-07
Iter: 1052 loss: 4.69323936e-07
Iter: 1053 loss: 4.69251034e-07
Iter: 1054 loss: 4.69000298e-07
Iter: 1055 loss: 4.72441826e-07
Iter: 1056 loss: 4.69011809e-07
Iter: 1057 loss: 4.68840398e-07
Iter: 1058 loss: 4.68740524e-07
Iter: 1059 loss: 4.68641304e-07
Iter: 1060 loss: 4.68404465e-07
Iter: 1061 loss: 4.67941732e-07
Iter: 1062 loss: 4.78618119e-07
Iter: 1063 loss: 4.67935422e-07
Iter: 1064 loss: 4.67872155e-07
Iter: 1065 loss: 4.67709043e-07
Iter: 1066 loss: 4.67507505e-07
Iter: 1067 loss: 4.67707537e-07
Iter: 1068 loss: 4.67393306e-07
Iter: 1069 loss: 4.67203733e-07
Iter: 1070 loss: 4.66841811e-07
Iter: 1071 loss: 4.74841443e-07
Iter: 1072 loss: 4.66836411e-07
Iter: 1073 loss: 4.66891919e-07
Iter: 1074 loss: 4.66715534e-07
Iter: 1075 loss: 4.66584368e-07
Iter: 1076 loss: 4.66338633e-07
Iter: 1077 loss: 4.71633882e-07
Iter: 1078 loss: 4.66345625e-07
Iter: 1079 loss: 4.66076813e-07
Iter: 1080 loss: 4.65967105e-07
Iter: 1081 loss: 4.65813088e-07
Iter: 1082 loss: 4.6568303e-07
Iter: 1083 loss: 4.65625874e-07
Iter: 1084 loss: 4.65463529e-07
Iter: 1085 loss: 4.65425984e-07
Iter: 1086 loss: 4.65311558e-07
Iter: 1087 loss: 4.65119371e-07
Iter: 1088 loss: 4.6500557e-07
Iter: 1089 loss: 4.64899188e-07
Iter: 1090 loss: 4.64791128e-07
Iter: 1091 loss: 4.64750514e-07
Iter: 1092 loss: 4.6463191e-07
Iter: 1093 loss: 4.64504183e-07
Iter: 1094 loss: 4.64477694e-07
Iter: 1095 loss: 4.64242191e-07
Iter: 1096 loss: 4.63904541e-07
Iter: 1097 loss: 4.63902779e-07
Iter: 1098 loss: 4.63815439e-07
Iter: 1099 loss: 4.63708631e-07
Iter: 1100 loss: 4.63540687e-07
Iter: 1101 loss: 4.63492e-07
Iter: 1102 loss: 4.63390393e-07
Iter: 1103 loss: 4.63187689e-07
Iter: 1104 loss: 4.63128345e-07
Iter: 1105 loss: 4.62986407e-07
Iter: 1106 loss: 4.62976629e-07
Iter: 1107 loss: 4.6289594e-07
Iter: 1108 loss: 4.62810362e-07
Iter: 1109 loss: 4.62591856e-07
Iter: 1110 loss: 4.64278202e-07
Iter: 1111 loss: 4.62544335e-07
Iter: 1112 loss: 4.62267394e-07
Iter: 1113 loss: 4.63258345e-07
Iter: 1114 loss: 4.62194919e-07
Iter: 1115 loss: 4.62056192e-07
Iter: 1116 loss: 4.62037946e-07
Iter: 1117 loss: 4.61899305e-07
Iter: 1118 loss: 4.61597239e-07
Iter: 1119 loss: 4.64784591e-07
Iter: 1120 loss: 4.6155796e-07
Iter: 1121 loss: 4.61171084e-07
Iter: 1122 loss: 4.61967886e-07
Iter: 1123 loss: 4.61009563e-07
Iter: 1124 loss: 4.60864953e-07
Iter: 1125 loss: 4.60844774e-07
Iter: 1126 loss: 4.60709657e-07
Iter: 1127 loss: 4.61063564e-07
Iter: 1128 loss: 4.60672595e-07
Iter: 1129 loss: 4.60529662e-07
Iter: 1130 loss: 4.60366039e-07
Iter: 1131 loss: 4.60358024e-07
Iter: 1132 loss: 4.60190336e-07
Iter: 1133 loss: 4.60706474e-07
Iter: 1134 loss: 4.60155832e-07
Iter: 1135 loss: 4.59939031e-07
Iter: 1136 loss: 4.61030282e-07
Iter: 1137 loss: 4.59921637e-07
Iter: 1138 loss: 4.59766767e-07
Iter: 1139 loss: 4.59529645e-07
Iter: 1140 loss: 4.59533794e-07
Iter: 1141 loss: 4.5938333e-07
Iter: 1142 loss: 4.59366845e-07
Iter: 1143 loss: 4.59221098e-07
Iter: 1144 loss: 4.58911302e-07
Iter: 1145 loss: 4.64397942e-07
Iter: 1146 loss: 4.58914826e-07
Iter: 1147 loss: 4.58740573e-07
Iter: 1148 loss: 4.5873162e-07
Iter: 1149 loss: 4.5858755e-07
Iter: 1150 loss: 4.5896013e-07
Iter: 1151 loss: 4.58541393e-07
Iter: 1152 loss: 4.58403889e-07
Iter: 1153 loss: 4.58194876e-07
Iter: 1154 loss: 4.58184189e-07
Iter: 1155 loss: 4.57975204e-07
Iter: 1156 loss: 4.59449723e-07
Iter: 1157 loss: 4.57951103e-07
Iter: 1158 loss: 4.57818146e-07
Iter: 1159 loss: 4.59395551e-07
Iter: 1160 loss: 4.5781519e-07
Iter: 1161 loss: 4.57673821e-07
Iter: 1162 loss: 4.57377524e-07
Iter: 1163 loss: 4.64048838e-07
Iter: 1164 loss: 4.57385198e-07
Iter: 1165 loss: 4.57090835e-07
Iter: 1166 loss: 4.58339116e-07
Iter: 1167 loss: 4.57029898e-07
Iter: 1168 loss: 4.56876421e-07
Iter: 1169 loss: 4.59381511e-07
Iter: 1170 loss: 4.56883924e-07
Iter: 1171 loss: 4.56714531e-07
Iter: 1172 loss: 4.56474595e-07
Iter: 1173 loss: 4.56467433e-07
Iter: 1174 loss: 4.56269788e-07
Iter: 1175 loss: 4.57608223e-07
Iter: 1176 loss: 4.56243527e-07
Iter: 1177 loss: 4.56087832e-07
Iter: 1178 loss: 4.57759825e-07
Iter: 1179 loss: 4.56076236e-07
Iter: 1180 loss: 4.56004955e-07
Iter: 1181 loss: 4.55754616e-07
Iter: 1182 loss: 4.56658938e-07
Iter: 1183 loss: 4.5562183e-07
Iter: 1184 loss: 4.55644852e-07
Iter: 1185 loss: 4.55450333e-07
Iter: 1186 loss: 4.55340142e-07
Iter: 1187 loss: 4.55171801e-07
Iter: 1188 loss: 4.55175439e-07
Iter: 1189 loss: 4.54958609e-07
Iter: 1190 loss: 4.5489395e-07
Iter: 1191 loss: 4.54767843e-07
Iter: 1192 loss: 4.54629884e-07
Iter: 1193 loss: 4.54619567e-07
Iter: 1194 loss: 4.54492522e-07
Iter: 1195 loss: 4.5520602e-07
Iter: 1196 loss: 4.54481494e-07
Iter: 1197 loss: 4.54354876e-07
Iter: 1198 loss: 4.54095641e-07
Iter: 1199 loss: 4.58222303e-07
Iter: 1200 loss: 4.5408018e-07
Iter: 1201 loss: 4.53869177e-07
Iter: 1202 loss: 4.55308111e-07
Iter: 1203 loss: 4.53842262e-07
Iter: 1204 loss: 4.53625603e-07
Iter: 1205 loss: 4.55416682e-07
Iter: 1206 loss: 4.5361611e-07
Iter: 1207 loss: 4.534e-07
Iter: 1208 loss: 4.52984636e-07
Iter: 1209 loss: 4.6081604e-07
Iter: 1210 loss: 4.52968919e-07
Iter: 1211 loss: 4.52772525e-07
Iter: 1212 loss: 4.5276235e-07
Iter: 1213 loss: 4.52581531e-07
Iter: 1214 loss: 4.52857307e-07
Iter: 1215 loss: 4.52479583e-07
Iter: 1216 loss: 4.5231576e-07
Iter: 1217 loss: 4.52197639e-07
Iter: 1218 loss: 4.52145372e-07
Iter: 1219 loss: 4.52056383e-07
Iter: 1220 loss: 4.52003576e-07
Iter: 1221 loss: 4.51943123e-07
Iter: 1222 loss: 4.51749742e-07
Iter: 1223 loss: 4.5306632e-07
Iter: 1224 loss: 4.51732831e-07
Iter: 1225 loss: 4.51452081e-07
Iter: 1226 loss: 4.51905237e-07
Iter: 1227 loss: 4.51323018e-07
Iter: 1228 loss: 4.51135122e-07
Iter: 1229 loss: 4.51124492e-07
Iter: 1230 loss: 4.50981247e-07
Iter: 1231 loss: 4.50966752e-07
Iter: 1232 loss: 4.50848916e-07
Iter: 1233 loss: 4.50639959e-07
Iter: 1234 loss: 4.50445782e-07
Iter: 1235 loss: 4.50391838e-07
Iter: 1236 loss: 4.50276815e-07
Iter: 1237 loss: 4.50247825e-07
Iter: 1238 loss: 4.5011808e-07
Iter: 1239 loss: 4.50234893e-07
Iter: 1240 loss: 4.5002875e-07
Iter: 1241 loss: 4.49888205e-07
Iter: 1242 loss: 4.49578295e-07
Iter: 1243 loss: 4.54711852e-07
Iter: 1244 loss: 4.49579659e-07
Iter: 1245 loss: 4.49480865e-07
Iter: 1246 loss: 4.49404268e-07
Iter: 1247 loss: 4.49287256e-07
Iter: 1248 loss: 4.49066732e-07
Iter: 1249 loss: 4.53263283e-07
Iter: 1250 loss: 4.49055563e-07
Iter: 1251 loss: 4.48885714e-07
Iter: 1252 loss: 4.48891683e-07
Iter: 1253 loss: 4.48745567e-07
Iter: 1254 loss: 4.48509553e-07
Iter: 1255 loss: 4.48512765e-07
Iter: 1256 loss: 4.48276268e-07
Iter: 1257 loss: 4.48363153e-07
Iter: 1258 loss: 4.48124126e-07
Iter: 1259 loss: 4.47847953e-07
Iter: 1260 loss: 4.50877508e-07
Iter: 1261 loss: 4.47836953e-07
Iter: 1262 loss: 4.47593663e-07
Iter: 1263 loss: 4.48843139e-07
Iter: 1264 loss: 4.47542732e-07
Iter: 1265 loss: 4.47387606e-07
Iter: 1266 loss: 4.47271503e-07
Iter: 1267 loss: 4.47203519e-07
Iter: 1268 loss: 4.46952072e-07
Iter: 1269 loss: 4.46914953e-07
Iter: 1270 loss: 4.46729615e-07
Iter: 1271 loss: 4.46732429e-07
Iter: 1272 loss: 4.46584039e-07
Iter: 1273 loss: 4.46475269e-07
Iter: 1274 loss: 4.46229791e-07
Iter: 1275 loss: 4.49871322e-07
Iter: 1276 loss: 4.46226579e-07
Iter: 1277 loss: 4.45937474e-07
Iter: 1278 loss: 4.46931296e-07
Iter: 1279 loss: 4.45849849e-07
Iter: 1280 loss: 4.45717376e-07
Iter: 1281 loss: 4.45694411e-07
Iter: 1282 loss: 4.45606162e-07
Iter: 1283 loss: 4.45421335e-07
Iter: 1284 loss: 4.47312516e-07
Iter: 1285 loss: 4.45398371e-07
Iter: 1286 loss: 4.45107588e-07
Iter: 1287 loss: 4.48217293e-07
Iter: 1288 loss: 4.45108071e-07
Iter: 1289 loss: 4.44928872e-07
Iter: 1290 loss: 4.44643433e-07
Iter: 1291 loss: 4.44650169e-07
Iter: 1292 loss: 4.443134e-07
Iter: 1293 loss: 4.44276822e-07
Iter: 1294 loss: 4.44026739e-07
Iter: 1295 loss: 4.43863485e-07
Iter: 1296 loss: 4.43803913e-07
Iter: 1297 loss: 4.43591944e-07
Iter: 1298 loss: 4.44120872e-07
Iter: 1299 loss: 4.43530325e-07
Iter: 1300 loss: 4.43377104e-07
Iter: 1301 loss: 4.4321655e-07
Iter: 1302 loss: 4.43195432e-07
Iter: 1303 loss: 4.42923806e-07
Iter: 1304 loss: 4.43915297e-07
Iter: 1305 loss: 4.4288862e-07
Iter: 1306 loss: 4.42797784e-07
Iter: 1307 loss: 4.42760836e-07
Iter: 1308 loss: 4.42682847e-07
Iter: 1309 loss: 4.42468377e-07
Iter: 1310 loss: 4.43828526e-07
Iter: 1311 loss: 4.4238385e-07
Iter: 1312 loss: 4.42143914e-07
Iter: 1313 loss: 4.45001632e-07
Iter: 1314 loss: 4.42143886e-07
Iter: 1315 loss: 4.41954057e-07
Iter: 1316 loss: 4.43838928e-07
Iter: 1317 loss: 4.41954398e-07
Iter: 1318 loss: 4.41844151e-07
Iter: 1319 loss: 4.41616521e-07
Iter: 1320 loss: 4.45830153e-07
Iter: 1321 loss: 4.41610467e-07
Iter: 1322 loss: 4.41391308e-07
Iter: 1323 loss: 4.41402335e-07
Iter: 1324 loss: 4.41303541e-07
Iter: 1325 loss: 4.41118658e-07
Iter: 1326 loss: 4.43811075e-07
Iter: 1327 loss: 4.41106124e-07
Iter: 1328 loss: 4.40849021e-07
Iter: 1329 loss: 4.410077e-07
Iter: 1330 loss: 4.40679173e-07
Iter: 1331 loss: 4.40657345e-07
Iter: 1332 loss: 4.40557528e-07
Iter: 1333 loss: 4.40432302e-07
Iter: 1334 loss: 4.40286811e-07
Iter: 1335 loss: 4.4025839e-07
Iter: 1336 loss: 4.40057761e-07
Iter: 1337 loss: 4.40145925e-07
Iter: 1338 loss: 4.39908689e-07
Iter: 1339 loss: 4.39750409e-07
Iter: 1340 loss: 4.41646705e-07
Iter: 1341 loss: 4.39741655e-07
Iter: 1342 loss: 4.39609664e-07
Iter: 1343 loss: 4.40543232e-07
Iter: 1344 loss: 4.39584653e-07
Iter: 1345 loss: 4.39446922e-07
Iter: 1346 loss: 4.39145424e-07
Iter: 1347 loss: 4.42205248e-07
Iter: 1348 loss: 4.39089035e-07
Iter: 1349 loss: 4.38892187e-07
Iter: 1350 loss: 4.38902248e-07
Iter: 1351 loss: 4.38677375e-07
Iter: 1352 loss: 4.39111034e-07
Iter: 1353 loss: 4.3858239e-07
Iter: 1354 loss: 4.38339384e-07
Iter: 1355 loss: 4.38238544e-07
Iter: 1356 loss: 4.38127444e-07
Iter: 1357 loss: 4.37875542e-07
Iter: 1358 loss: 4.37868408e-07
Iter: 1359 loss: 4.37757649e-07
Iter: 1360 loss: 4.37475137e-07
Iter: 1361 loss: 4.40395809e-07
Iter: 1362 loss: 4.37430884e-07
Iter: 1363 loss: 4.37194245e-07
Iter: 1364 loss: 4.39970734e-07
Iter: 1365 loss: 4.37212066e-07
Iter: 1366 loss: 4.37108639e-07
Iter: 1367 loss: 4.37089795e-07
Iter: 1368 loss: 4.37026642e-07
Iter: 1369 loss: 4.36838093e-07
Iter: 1370 loss: 4.38841369e-07
Iter: 1371 loss: 4.36828515e-07
Iter: 1372 loss: 4.36605831e-07
Iter: 1373 loss: 4.37162612e-07
Iter: 1374 loss: 4.36541342e-07
Iter: 1375 loss: 4.3636561e-07
Iter: 1376 loss: 4.38908557e-07
Iter: 1377 loss: 4.36372602e-07
Iter: 1378 loss: 4.36176293e-07
Iter: 1379 loss: 4.36045696e-07
Iter: 1380 loss: 4.35989023e-07
Iter: 1381 loss: 4.35753606e-07
Iter: 1382 loss: 4.35882185e-07
Iter: 1383 loss: 4.35578414e-07
Iter: 1384 loss: 4.35482207e-07
Iter: 1385 loss: 4.35447049e-07
Iter: 1386 loss: 4.35337313e-07
Iter: 1387 loss: 4.35152458e-07
Iter: 1388 loss: 4.35158711e-07
Iter: 1389 loss: 4.34955211e-07
Iter: 1390 loss: 4.36587015e-07
Iter: 1391 loss: 4.34955041e-07
Iter: 1392 loss: 4.34796448e-07
Iter: 1393 loss: 4.35508809e-07
Iter: 1394 loss: 4.34757879e-07
Iter: 1395 loss: 4.34686172e-07
Iter: 1396 loss: 4.34451806e-07
Iter: 1397 loss: 4.36265481e-07
Iter: 1398 loss: 4.34384106e-07
Iter: 1399 loss: 4.34282384e-07
Iter: 1400 loss: 4.34228582e-07
Iter: 1401 loss: 4.34097956e-07
Iter: 1402 loss: 4.34305122e-07
Iter: 1403 loss: 4.34022866e-07
Iter: 1404 loss: 4.33894968e-07
Iter: 1405 loss: 4.33675e-07
Iter: 1406 loss: 4.38839123e-07
Iter: 1407 loss: 4.33677712e-07
Iter: 1408 loss: 4.33466852e-07
Iter: 1409 loss: 4.36226799e-07
Iter: 1410 loss: 4.33461622e-07
Iter: 1411 loss: 4.33283503e-07
Iter: 1412 loss: 4.33996433e-07
Iter: 1413 loss: 4.33241041e-07
Iter: 1414 loss: 4.33053515e-07
Iter: 1415 loss: 4.32752643e-07
Iter: 1416 loss: 4.3276296e-07
Iter: 1417 loss: 4.32502645e-07
Iter: 1418 loss: 4.35048094e-07
Iter: 1419 loss: 4.32486019e-07
Iter: 1420 loss: 4.32311026e-07
Iter: 1421 loss: 4.34797471e-07
Iter: 1422 loss: 4.32317847e-07
Iter: 1423 loss: 4.32205411e-07
Iter: 1424 loss: 4.3199492e-07
Iter: 1425 loss: 4.35457395e-07
Iter: 1426 loss: 4.31985939e-07
Iter: 1427 loss: 4.3189263e-07
Iter: 1428 loss: 4.31833826e-07
Iter: 1429 loss: 4.31746656e-07
Iter: 1430 loss: 4.31649141e-07
Iter: 1431 loss: 4.31642889e-07
Iter: 1432 loss: 4.31492651e-07
Iter: 1433 loss: 4.31516838e-07
Iter: 1434 loss: 4.31402839e-07
Iter: 1435 loss: 4.31183537e-07
Iter: 1436 loss: 4.3309214e-07
Iter: 1437 loss: 4.31162107e-07
Iter: 1438 loss: 4.31021249e-07
Iter: 1439 loss: 4.31026962e-07
Iter: 1440 loss: 4.30903981e-07
Iter: 1441 loss: 4.30750504e-07
Iter: 1442 loss: 4.30887496e-07
Iter: 1443 loss: 4.30660634e-07
Iter: 1444 loss: 4.30418737e-07
Iter: 1445 loss: 4.31797162e-07
Iter: 1446 loss: 4.30387985e-07
Iter: 1447 loss: 4.301844e-07
Iter: 1448 loss: 4.30358e-07
Iter: 1449 loss: 4.30058549e-07
Iter: 1450 loss: 4.29887479e-07
Iter: 1451 loss: 4.29970783e-07
Iter: 1452 loss: 4.29790646e-07
Iter: 1453 loss: 4.2967693e-07
Iter: 1454 loss: 4.29659394e-07
Iter: 1455 loss: 4.29552472e-07
Iter: 1456 loss: 4.29441883e-07
Iter: 1457 loss: 4.29421561e-07
Iter: 1458 loss: 4.29288946e-07
Iter: 1459 loss: 4.30289e-07
Iter: 1460 loss: 4.29289912e-07
Iter: 1461 loss: 4.29118188e-07
Iter: 1462 loss: 4.29016865e-07
Iter: 1463 loss: 4.28950841e-07
Iter: 1464 loss: 4.28742908e-07
Iter: 1465 loss: 4.28885471e-07
Iter: 1466 loss: 4.28615635e-07
Iter: 1467 loss: 4.28494616e-07
Iter: 1468 loss: 4.28496207e-07
Iter: 1469 loss: 4.28355e-07
Iter: 1470 loss: 4.28181579e-07
Iter: 1471 loss: 4.28169301e-07
Iter: 1472 loss: 4.27942041e-07
Iter: 1473 loss: 4.28549527e-07
Iter: 1474 loss: 4.27847908e-07
Iter: 1475 loss: 4.27711626e-07
Iter: 1476 loss: 4.27698836e-07
Iter: 1477 loss: 4.27607347e-07
Iter: 1478 loss: 4.27548571e-07
Iter: 1479 loss: 4.27505483e-07
Iter: 1480 loss: 4.27320117e-07
Iter: 1481 loss: 4.27237694e-07
Iter: 1482 loss: 4.27170392e-07
Iter: 1483 loss: 4.27017255e-07
Iter: 1484 loss: 4.27007706e-07
Iter: 1485 loss: 4.26885549e-07
Iter: 1486 loss: 4.27023139e-07
Iter: 1487 loss: 4.26796078e-07
Iter: 1488 loss: 4.26667896e-07
Iter: 1489 loss: 4.26758788e-07
Iter: 1490 loss: 4.26574502e-07
Iter: 1491 loss: 4.26472127e-07
Iter: 1492 loss: 4.26461213e-07
Iter: 1493 loss: 4.26393626e-07
Iter: 1494 loss: 4.26194447e-07
Iter: 1495 loss: 4.27648985e-07
Iter: 1496 loss: 4.26162075e-07
Iter: 1497 loss: 4.25988617e-07
Iter: 1498 loss: 4.25986457e-07
Iter: 1499 loss: 4.25832809e-07
Iter: 1500 loss: 4.2629e-07
Iter: 1501 loss: 4.2578705e-07
Iter: 1502 loss: 4.25669e-07
Iter: 1503 loss: 4.25519659e-07
Iter: 1504 loss: 4.25507579e-07
Iter: 1505 loss: 4.25305586e-07
Iter: 1506 loss: 4.27818577e-07
Iter: 1507 loss: 4.25294672e-07
Iter: 1508 loss: 4.25138353e-07
Iter: 1509 loss: 4.25658016e-07
Iter: 1510 loss: 4.25090434e-07
Iter: 1511 loss: 4.24993061e-07
Iter: 1512 loss: 4.25008921e-07
Iter: 1513 loss: 4.24930704e-07
Iter: 1514 loss: 4.24784e-07
Iter: 1515 loss: 4.24761566e-07
Iter: 1516 loss: 4.24667689e-07
Iter: 1517 loss: 4.24590326e-07
Iter: 1518 loss: 4.24542179e-07
Iter: 1519 loss: 4.24499262e-07
Iter: 1520 loss: 4.24331233e-07
Iter: 1521 loss: 4.2595417e-07
Iter: 1522 loss: 4.24304972e-07
Iter: 1523 loss: 4.24175312e-07
Iter: 1524 loss: 4.24180712e-07
Iter: 1525 loss: 4.24061653e-07
Iter: 1526 loss: 4.23996966e-07
Iter: 1527 loss: 4.2393745e-07
Iter: 1528 loss: 4.23810235e-07
Iter: 1529 loss: 4.23745803e-07
Iter: 1530 loss: 4.23672958e-07
Iter: 1531 loss: 4.23483954e-07
Iter: 1532 loss: 4.26278831e-07
Iter: 1533 loss: 4.23483243e-07
Iter: 1534 loss: 4.23357847e-07
Iter: 1535 loss: 4.23256722e-07
Iter: 1536 loss: 4.2322074e-07
Iter: 1537 loss: 4.23046401e-07
Iter: 1538 loss: 4.23677875e-07
Iter: 1539 loss: 4.23012e-07
Iter: 1540 loss: 4.22823462e-07
Iter: 1541 loss: 4.23663437e-07
Iter: 1542 loss: 4.22811269e-07
Iter: 1543 loss: 4.2265458e-07
Iter: 1544 loss: 4.22589807e-07
Iter: 1545 loss: 4.22507156e-07
Iter: 1546 loss: 4.22369936e-07
Iter: 1547 loss: 4.2269221e-07
Iter: 1548 loss: 4.2231818e-07
Iter: 1549 loss: 4.22212821e-07
Iter: 1550 loss: 4.23247144e-07
Iter: 1551 loss: 4.2220725e-07
Iter: 1552 loss: 4.22045275e-07
Iter: 1553 loss: 4.22435136e-07
Iter: 1554 loss: 4.22005371e-07
Iter: 1555 loss: 4.21930565e-07
Iter: 1556 loss: 4.21891627e-07
Iter: 1557 loss: 4.21862694e-07
Iter: 1558 loss: 4.21676674e-07
Iter: 1559 loss: 4.22483197e-07
Iter: 1560 loss: 4.21636855e-07
Iter: 1561 loss: 4.21529336e-07
Iter: 1562 loss: 4.21528796e-07
Iter: 1563 loss: 4.21442166e-07
Iter: 1564 loss: 4.21336836e-07
Iter: 1565 loss: 4.22755761e-07
Iter: 1566 loss: 4.2133928e-07
Iter: 1567 loss: 4.212134e-07
Iter: 1568 loss: 4.20970309e-07
Iter: 1569 loss: 4.25219952e-07
Iter: 1570 loss: 4.20966984e-07
Iter: 1571 loss: 4.20805407e-07
Iter: 1572 loss: 4.23248025e-07
Iter: 1573 loss: 4.20799211e-07
Iter: 1574 loss: 4.20698882e-07
Iter: 1575 loss: 4.21886796e-07
Iter: 1576 loss: 4.20706812e-07
Iter: 1577 loss: 4.20642948e-07
Iter: 1578 loss: 4.20536367e-07
Iter: 1579 loss: 4.20536111e-07
Iter: 1580 loss: 4.20422225e-07
Iter: 1581 loss: 4.20768e-07
Iter: 1582 loss: 4.20382037e-07
Iter: 1583 loss: 4.20292935e-07
Iter: 1584 loss: 4.2116443e-07
Iter: 1585 loss: 4.20287591e-07
Iter: 1586 loss: 4.20180868e-07
Iter: 1587 loss: 4.20303707e-07
Iter: 1588 loss: 4.20127293e-07
Iter: 1589 loss: 4.20004028e-07
Iter: 1590 loss: 4.19824062e-07
Iter: 1591 loss: 4.19814683e-07
Iter: 1592 loss: 4.19782e-07
Iter: 1593 loss: 4.19695937e-07
Iter: 1594 loss: 4.19631306e-07
Iter: 1595 loss: 4.19434201e-07
Iter: 1596 loss: 4.2029518e-07
Iter: 1597 loss: 4.19323953e-07
Iter: 1598 loss: 4.19144527e-07
Iter: 1599 loss: 4.19120738e-07
Iter: 1600 loss: 4.1894819e-07
Iter: 1601 loss: 4.19723904e-07
Iter: 1602 loss: 4.18907462e-07
Iter: 1603 loss: 4.18826403e-07
Iter: 1604 loss: 4.18629952e-07
Iter: 1605 loss: 4.22112748e-07
Iter: 1606 loss: 4.18634897e-07
Iter: 1607 loss: 4.18475707e-07
Iter: 1608 loss: 4.18469e-07
Iter: 1609 loss: 4.18360173e-07
Iter: 1610 loss: 4.18554123e-07
Iter: 1611 loss: 4.18327488e-07
Iter: 1612 loss: 4.18220168e-07
Iter: 1613 loss: 4.18096818e-07
Iter: 1614 loss: 4.18091474e-07
Iter: 1615 loss: 4.17906676e-07
Iter: 1616 loss: 4.18734516e-07
Iter: 1617 loss: 4.1785205e-07
Iter: 1618 loss: 4.17692036e-07
Iter: 1619 loss: 4.20235295e-07
Iter: 1620 loss: 4.17680695e-07
Iter: 1621 loss: 4.17567634e-07
Iter: 1622 loss: 4.17339066e-07
Iter: 1623 loss: 4.21858601e-07
Iter: 1624 loss: 4.17341369e-07
Iter: 1625 loss: 4.17163335e-07
Iter: 1626 loss: 4.17160493e-07
Iter: 1627 loss: 4.16991213e-07
Iter: 1628 loss: 4.17004316e-07
Iter: 1629 loss: 4.16851435e-07
Iter: 1630 loss: 4.16695514e-07
Iter: 1631 loss: 4.1658177e-07
Iter: 1632 loss: 4.16504122e-07
Iter: 1633 loss: 4.16275213e-07
Iter: 1634 loss: 4.16270268e-07
Iter: 1635 loss: 4.16196826e-07
Iter: 1636 loss: 4.1612347e-07
Iter: 1637 loss: 4.16090359e-07
Iter: 1638 loss: 4.15986392e-07
Iter: 1639 loss: 4.16892e-07
Iter: 1640 loss: 4.15977979e-07
Iter: 1641 loss: 4.15852469e-07
Iter: 1642 loss: 4.15749355e-07
Iter: 1643 loss: 4.15747166e-07
Iter: 1644 loss: 4.15618103e-07
Iter: 1645 loss: 4.16303493e-07
Iter: 1646 loss: 4.15615602e-07
Iter: 1647 loss: 4.15489865e-07
Iter: 1648 loss: 4.15297791e-07
Iter: 1649 loss: 4.20221511e-07
Iter: 1650 loss: 4.15293158e-07
Iter: 1651 loss: 4.1519877e-07
Iter: 1652 loss: 4.15171655e-07
Iter: 1653 loss: 4.15039779e-07
Iter: 1654 loss: 4.15079654e-07
Iter: 1655 loss: 4.1492666e-07
Iter: 1656 loss: 4.14750957e-07
Iter: 1657 loss: 4.14678141e-07
Iter: 1658 loss: 4.14581649e-07
Iter: 1659 loss: 4.14408817e-07
Iter: 1660 loss: 4.14383209e-07
Iter: 1661 loss: 4.14304e-07
Iter: 1662 loss: 4.14092142e-07
Iter: 1663 loss: 4.15892941e-07
Iter: 1664 loss: 4.14076794e-07
Iter: 1665 loss: 4.13887648e-07
Iter: 1666 loss: 4.13875284e-07
Iter: 1667 loss: 4.13726e-07
Iter: 1668 loss: 4.13748893e-07
Iter: 1669 loss: 4.13589504e-07
Iter: 1670 loss: 4.13473629e-07
Iter: 1671 loss: 4.14222626e-07
Iter: 1672 loss: 4.13474311e-07
Iter: 1673 loss: 4.13336608e-07
Iter: 1674 loss: 4.13366308e-07
Iter: 1675 loss: 4.13260665e-07
Iter: 1676 loss: 4.13124781e-07
Iter: 1677 loss: 4.13335641e-07
Iter: 1678 loss: 4.13060945e-07
Iter: 1679 loss: 4.12901898e-07
Iter: 1680 loss: 4.12712723e-07
Iter: 1681 loss: 4.12709539e-07
Iter: 1682 loss: 4.12433849e-07
Iter: 1683 loss: 4.14427461e-07
Iter: 1684 loss: 4.12422452e-07
Iter: 1685 loss: 4.12270424e-07
Iter: 1686 loss: 4.12278894e-07
Iter: 1687 loss: 4.12144743e-07
Iter: 1688 loss: 4.11874936e-07
Iter: 1689 loss: 4.16571709e-07
Iter: 1690 loss: 4.11880961e-07
Iter: 1691 loss: 4.11753632e-07
Iter: 1692 loss: 4.11723875e-07
Iter: 1693 loss: 4.11615929e-07
Iter: 1694 loss: 4.11677888e-07
Iter: 1695 loss: 4.11541e-07
Iter: 1696 loss: 4.11434911e-07
Iter: 1697 loss: 4.11401487e-07
Iter: 1698 loss: 4.11339784e-07
Iter: 1699 loss: 4.11234538e-07
Iter: 1700 loss: 4.11229792e-07
Iter: 1701 loss: 4.11189149e-07
Iter: 1702 loss: 4.11142366e-07
Iter: 1703 loss: 4.11138672e-07
Iter: 1704 loss: 4.11014526e-07
Iter: 1705 loss: 4.11461286e-07
Iter: 1706 loss: 4.10986047e-07
Iter: 1707 loss: 4.10850248e-07
Iter: 1708 loss: 4.10855222e-07
Iter: 1709 loss: 4.107618e-07
Iter: 1710 loss: 4.10616451e-07
Iter: 1711 loss: 4.10594396e-07
Iter: 1712 loss: 4.10508164e-07
Iter: 1713 loss: 4.10285e-07
Iter: 1714 loss: 4.09843295e-07
Iter: 1715 loss: 4.17386843e-07
Iter: 1716 loss: 4.09832268e-07
Iter: 1717 loss: 4.09592644e-07
Iter: 1718 loss: 4.09555383e-07
Iter: 1719 loss: 4.09342817e-07
Iter: 1720 loss: 4.10437281e-07
Iter: 1721 loss: 4.09291488e-07
Iter: 1722 loss: 4.09132156e-07
Iter: 1723 loss: 4.08995334e-07
Iter: 1724 loss: 4.08972767e-07
Iter: 1725 loss: 4.08894607e-07
Iter: 1726 loss: 4.08832e-07
Iter: 1727 loss: 4.08787628e-07
Iter: 1728 loss: 4.08645889e-07
Iter: 1729 loss: 4.09813282e-07
Iter: 1730 loss: 4.08633866e-07
Iter: 1731 loss: 4.08522283e-07
Iter: 1732 loss: 4.10471188e-07
Iter: 1733 loss: 4.08517224e-07
Iter: 1734 loss: 4.08394044e-07
Iter: 1735 loss: 4.08384608e-07
Iter: 1736 loss: 4.08278254e-07
Iter: 1737 loss: 4.08168432e-07
Iter: 1738 loss: 4.08562727e-07
Iter: 1739 loss: 4.08148139e-07
Iter: 1740 loss: 4.079821e-07
Iter: 1741 loss: 4.07973289e-07
Iter: 1742 loss: 4.07841526e-07
Iter: 1743 loss: 4.07680432e-07
Iter: 1744 loss: 4.08729761e-07
Iter: 1745 loss: 4.07669177e-07
Iter: 1746 loss: 4.07554808e-07
Iter: 1747 loss: 4.07537755e-07
Iter: 1748 loss: 4.07455843e-07
Iter: 1749 loss: 4.07275706e-07
Iter: 1750 loss: 4.07428928e-07
Iter: 1751 loss: 4.07163611e-07
Iter: 1752 loss: 4.07015477e-07
Iter: 1753 loss: 4.07024515e-07
Iter: 1754 loss: 4.06882492e-07
Iter: 1755 loss: 4.06738366e-07
Iter: 1756 loss: 4.06716481e-07
Iter: 1757 loss: 4.0655155e-07
Iter: 1758 loss: 4.08620622e-07
Iter: 1759 loss: 4.06553397e-07
Iter: 1760 loss: 4.06351177e-07
Iter: 1761 loss: 4.06481291e-07
Iter: 1762 loss: 4.06214468e-07
Iter: 1763 loss: 4.06074662e-07
Iter: 1764 loss: 4.05947389e-07
Iter: 1765 loss: 4.05910782e-07
Iter: 1766 loss: 4.05610535e-07
Iter: 1767 loss: 4.0795598e-07
Iter: 1768 loss: 4.05587457e-07
Iter: 1769 loss: 4.05428295e-07
Iter: 1770 loss: 4.05340842e-07
Iter: 1771 loss: 4.05263506e-07
Iter: 1772 loss: 4.0508661e-07
Iter: 1773 loss: 4.07662526e-07
Iter: 1774 loss: 4.05080726e-07
Iter: 1775 loss: 4.04899538e-07
Iter: 1776 loss: 4.0471906e-07
Iter: 1777 loss: 4.04675916e-07
Iter: 1778 loss: 4.04383343e-07
Iter: 1779 loss: 4.05429034e-07
Iter: 1780 loss: 4.04309276e-07
Iter: 1781 loss: 4.04112171e-07
Iter: 1782 loss: 4.04304586e-07
Iter: 1783 loss: 4.0401963e-07
Iter: 1784 loss: 4.03808315e-07
Iter: 1785 loss: 4.05906519e-07
Iter: 1786 loss: 4.03799589e-07
Iter: 1787 loss: 4.03637074e-07
Iter: 1788 loss: 4.04195134e-07
Iter: 1789 loss: 4.03585432e-07
Iter: 1790 loss: 4.03491697e-07
Iter: 1791 loss: 4.03371757e-07
Iter: 1792 loss: 4.03358968e-07
Iter: 1793 loss: 4.03142252e-07
Iter: 1794 loss: 4.06043512e-07
Iter: 1795 loss: 4.03143645e-07
Iter: 1796 loss: 4.03038484e-07
Iter: 1797 loss: 4.02936081e-07
Iter: 1798 loss: 4.02936024e-07
Iter: 1799 loss: 4.02818728e-07
Iter: 1800 loss: 4.02825606e-07
Iter: 1801 loss: 4.02693956e-07
Iter: 1802 loss: 4.02394221e-07
Iter: 1803 loss: 4.0484656e-07
Iter: 1804 loss: 4.0236506e-07
Iter: 1805 loss: 4.02199078e-07
Iter: 1806 loss: 4.02164062e-07
Iter: 1807 loss: 4.0200797e-07
Iter: 1808 loss: 4.0196565e-07
Iter: 1809 loss: 4.01873763e-07
Iter: 1810 loss: 4.01620582e-07
Iter: 1811 loss: 4.01986e-07
Iter: 1812 loss: 4.01509197e-07
Iter: 1813 loss: 4.01223e-07
Iter: 1814 loss: 4.02374155e-07
Iter: 1815 loss: 4.01146167e-07
Iter: 1816 loss: 4.00951649e-07
Iter: 1817 loss: 4.01216823e-07
Iter: 1818 loss: 4.00843362e-07
Iter: 1819 loss: 4.0052447e-07
Iter: 1820 loss: 4.00795102e-07
Iter: 1821 loss: 4.00339587e-07
Iter: 1822 loss: 4.00032036e-07
Iter: 1823 loss: 4.00669933e-07
Iter: 1824 loss: 3.99925199e-07
Iter: 1825 loss: 3.99767742e-07
Iter: 1826 loss: 3.99769448e-07
Iter: 1827 loss: 3.995778e-07
Iter: 1828 loss: 3.99373334e-07
Iter: 1829 loss: 3.99323426e-07
Iter: 1830 loss: 3.99178333e-07
Iter: 1831 loss: 4.00546753e-07
Iter: 1832 loss: 3.99158125e-07
Iter: 1833 loss: 3.98979353e-07
Iter: 1834 loss: 3.99269254e-07
Iter: 1835 loss: 3.98900283e-07
Iter: 1836 loss: 3.98725348e-07
Iter: 1837 loss: 3.98468643e-07
Iter: 1838 loss: 3.98450425e-07
Iter: 1839 loss: 3.98294191e-07
Iter: 1840 loss: 3.98259431e-07
Iter: 1841 loss: 3.98127554e-07
Iter: 1842 loss: 3.97877386e-07
Iter: 1843 loss: 4.03017509e-07
Iter: 1844 loss: 3.97872554e-07
Iter: 1845 loss: 3.97603145e-07
Iter: 1846 loss: 3.99129362e-07
Iter: 1847 loss: 3.97525298e-07
Iter: 1848 loss: 3.97342887e-07
Iter: 1849 loss: 3.98918985e-07
Iter: 1850 loss: 3.97341324e-07
Iter: 1851 loss: 3.97192792e-07
Iter: 1852 loss: 3.97609483e-07
Iter: 1853 loss: 3.97171959e-07
Iter: 1854 loss: 3.97015413e-07
Iter: 1855 loss: 3.96827687e-07
Iter: 1856 loss: 3.96804239e-07
Iter: 1857 loss: 3.96633538e-07
Iter: 1858 loss: 3.96631151e-07
Iter: 1859 loss: 3.9650979e-07
Iter: 1860 loss: 3.97108408e-07
Iter: 1861 loss: 3.96473609e-07
Iter: 1862 loss: 3.96388486e-07
Iter: 1863 loss: 3.96246776e-07
Iter: 1864 loss: 3.96234583e-07
Iter: 1865 loss: 3.96126296e-07
Iter: 1866 loss: 3.96109073e-07
Iter: 1867 loss: 3.96041969e-07
Iter: 1868 loss: 3.9583017e-07
Iter: 1869 loss: 3.97130066e-07
Iter: 1870 loss: 3.95761248e-07
Iter: 1871 loss: 3.95609817e-07
Iter: 1872 loss: 3.95602285e-07
Iter: 1873 loss: 3.9545489e-07
Iter: 1874 loss: 3.95603053e-07
Iter: 1875 loss: 3.95363628e-07
Iter: 1876 loss: 3.95225129e-07
Iter: 1877 loss: 3.95316079e-07
Iter: 1878 loss: 3.95139068e-07
Iter: 1879 loss: 3.94986e-07
Iter: 1880 loss: 3.95826163e-07
Iter: 1881 loss: 3.94938269e-07
Iter: 1882 loss: 3.9481688e-07
Iter: 1883 loss: 3.95577331e-07
Iter: 1884 loss: 3.94798349e-07
Iter: 1885 loss: 3.9469e-07
Iter: 1886 loss: 3.94731217e-07
Iter: 1887 loss: 3.94622305e-07
Iter: 1888 loss: 3.94496112e-07
Iter: 1889 loss: 3.94445408e-07
Iter: 1890 loss: 3.94371341e-07
Iter: 1891 loss: 3.94293465e-07
Iter: 1892 loss: 3.94260724e-07
Iter: 1893 loss: 3.94188561e-07
Iter: 1894 loss: 3.94026642e-07
Iter: 1895 loss: 3.95924303e-07
Iter: 1896 loss: 3.94024596e-07
Iter: 1897 loss: 3.93880015e-07
Iter: 1898 loss: 3.96009142e-07
Iter: 1899 loss: 3.93863274e-07
Iter: 1900 loss: 3.93721393e-07
Iter: 1901 loss: 3.9360873e-07
Iter: 1902 loss: 3.93545747e-07
Iter: 1903 loss: 3.93370215e-07
Iter: 1904 loss: 3.93171263e-07
Iter: 1905 loss: 3.93119308e-07
Iter: 1906 loss: 3.92925443e-07
Iter: 1907 loss: 3.92884772e-07
Iter: 1908 loss: 3.92787797e-07
Iter: 1909 loss: 3.92563379e-07
Iter: 1910 loss: 3.97062507e-07
Iter: 1911 loss: 3.92567358e-07
Iter: 1912 loss: 3.92312813e-07
Iter: 1913 loss: 3.92317475e-07
Iter: 1914 loss: 3.92106642e-07
Iter: 1915 loss: 3.92040135e-07
Iter: 1916 loss: 3.91975e-07
Iter: 1917 loss: 3.91884839e-07
Iter: 1918 loss: 3.91772119e-07
Iter: 1919 loss: 3.91771437e-07
Iter: 1920 loss: 3.91575554e-07
Iter: 1921 loss: 3.91924857e-07
Iter: 1922 loss: 3.91468291e-07
Iter: 1923 loss: 3.9135324e-07
Iter: 1924 loss: 3.93135508e-07
Iter: 1925 loss: 3.91355059e-07
Iter: 1926 loss: 3.91244271e-07
Iter: 1927 loss: 3.91570296e-07
Iter: 1928 loss: 3.91215451e-07
Iter: 1929 loss: 3.91095881e-07
Iter: 1930 loss: 3.90873765e-07
Iter: 1931 loss: 3.94629751e-07
Iter: 1932 loss: 3.90858958e-07
Iter: 1933 loss: 3.90804985e-07
Iter: 1934 loss: 3.90730122e-07
Iter: 1935 loss: 3.90635961e-07
Iter: 1936 loss: 3.90412026e-07
Iter: 1937 loss: 3.92621303e-07
Iter: 1938 loss: 3.9037127e-07
Iter: 1939 loss: 3.90118942e-07
Iter: 1940 loss: 3.92111247e-07
Iter: 1941 loss: 3.90106578e-07
Iter: 1942 loss: 3.89936673e-07
Iter: 1943 loss: 3.89939942e-07
Iter: 1944 loss: 3.89847315e-07
Iter: 1945 loss: 3.89624233e-07
Iter: 1946 loss: 3.91762086e-07
Iter: 1947 loss: 3.89587399e-07
Iter: 1948 loss: 3.89422752e-07
Iter: 1949 loss: 3.90723471e-07
Iter: 1950 loss: 3.89385264e-07
Iter: 1951 loss: 3.89254808e-07
Iter: 1952 loss: 3.89138535e-07
Iter: 1953 loss: 3.8907308e-07
Iter: 1954 loss: 3.88994465e-07
Iter: 1955 loss: 3.88926082e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8
+ date
Mon Oct 26 11:43:30 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.8_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi0.8_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96d407df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96d4030d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96d401bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96d401bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96d40e8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96cccd8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96cccacf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccc4a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccc477b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccc24840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccc247b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccc24400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccbdc620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccb95510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccb04620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccb048c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccb3f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccaf7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccaef9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96cca8ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccaaa488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96ccaaac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9698590620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9698590c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96985798c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96985606a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96985129d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96984b5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96984b5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f969849c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f969849c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96705590d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f967053c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f967053c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f967050c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96704bd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.65409437e-06
Iter: 2 loss: 4.96523307e-06
Iter: 3 loss: 1.41089531e-05
Iter: 4 loss: 4.9615619e-06
Iter: 5 loss: 4.61578202e-06
Iter: 6 loss: 5.13045507e-06
Iter: 7 loss: 4.45014302e-06
Iter: 8 loss: 4.25493727e-06
Iter: 9 loss: 4.90309139e-06
Iter: 10 loss: 4.20146034e-06
Iter: 11 loss: 3.96639825e-06
Iter: 12 loss: 4.53168559e-06
Iter: 13 loss: 3.88151057e-06
Iter: 14 loss: 3.75342393e-06
Iter: 15 loss: 3.81444397e-06
Iter: 16 loss: 3.66715767e-06
Iter: 17 loss: 3.62103265e-06
Iter: 18 loss: 3.60270769e-06
Iter: 19 loss: 3.54252e-06
Iter: 20 loss: 3.37658798e-06
Iter: 21 loss: 4.34908679e-06
Iter: 22 loss: 3.33001935e-06
Iter: 23 loss: 3.13593682e-06
Iter: 24 loss: 4.20553533e-06
Iter: 25 loss: 3.10790438e-06
Iter: 26 loss: 2.9611856e-06
Iter: 27 loss: 4.41385282e-06
Iter: 28 loss: 2.95615814e-06
Iter: 29 loss: 2.81957773e-06
Iter: 30 loss: 3.09466736e-06
Iter: 31 loss: 2.76397373e-06
Iter: 32 loss: 2.66662096e-06
Iter: 33 loss: 2.63686979e-06
Iter: 34 loss: 2.57897909e-06
Iter: 35 loss: 2.49214e-06
Iter: 36 loss: 2.61479909e-06
Iter: 37 loss: 2.44951889e-06
Iter: 38 loss: 2.37309905e-06
Iter: 39 loss: 3.34776405e-06
Iter: 40 loss: 2.37254858e-06
Iter: 41 loss: 2.32807088e-06
Iter: 42 loss: 2.32797606e-06
Iter: 43 loss: 2.29075567e-06
Iter: 44 loss: 2.20969901e-06
Iter: 45 loss: 3.42278463e-06
Iter: 46 loss: 2.20621268e-06
Iter: 47 loss: 2.18585137e-06
Iter: 48 loss: 2.1669025e-06
Iter: 49 loss: 2.13765043e-06
Iter: 50 loss: 2.06294862e-06
Iter: 51 loss: 2.70501732e-06
Iter: 52 loss: 2.05067181e-06
Iter: 53 loss: 1.98431326e-06
Iter: 54 loss: 2.76606966e-06
Iter: 55 loss: 1.98332759e-06
Iter: 56 loss: 1.92091397e-06
Iter: 57 loss: 2.28907106e-06
Iter: 58 loss: 1.91287381e-06
Iter: 59 loss: 1.8879515e-06
Iter: 60 loss: 1.85458725e-06
Iter: 61 loss: 1.85276986e-06
Iter: 62 loss: 1.83281406e-06
Iter: 63 loss: 1.83048519e-06
Iter: 64 loss: 1.8114622e-06
Iter: 65 loss: 1.81557232e-06
Iter: 66 loss: 1.79743245e-06
Iter: 67 loss: 1.77253946e-06
Iter: 68 loss: 1.71780835e-06
Iter: 69 loss: 2.51519077e-06
Iter: 70 loss: 1.71533384e-06
Iter: 71 loss: 1.65821962e-06
Iter: 72 loss: 2.23054394e-06
Iter: 73 loss: 1.65643178e-06
Iter: 74 loss: 1.60586251e-06
Iter: 75 loss: 1.61777916e-06
Iter: 76 loss: 1.56879264e-06
Iter: 77 loss: 1.53464327e-06
Iter: 78 loss: 1.53265637e-06
Iter: 79 loss: 1.50069548e-06
Iter: 80 loss: 1.62931281e-06
Iter: 81 loss: 1.49362063e-06
Iter: 82 loss: 1.47900869e-06
Iter: 83 loss: 1.47867377e-06
Iter: 84 loss: 1.46724381e-06
Iter: 85 loss: 1.44270371e-06
Iter: 86 loss: 1.59056071e-06
Iter: 87 loss: 1.43964394e-06
Iter: 88 loss: 1.42434135e-06
Iter: 89 loss: 1.41902717e-06
Iter: 90 loss: 1.41034593e-06
Iter: 91 loss: 1.39785209e-06
Iter: 92 loss: 1.39785197e-06
Iter: 93 loss: 1.38328187e-06
Iter: 94 loss: 1.3569329e-06
Iter: 95 loss: 1.99680517e-06
Iter: 96 loss: 1.35694768e-06
Iter: 97 loss: 1.32886953e-06
Iter: 98 loss: 1.32501111e-06
Iter: 99 loss: 1.3052861e-06
Iter: 100 loss: 1.30212493e-06
Iter: 101 loss: 1.29056866e-06
Iter: 102 loss: 1.27608769e-06
Iter: 103 loss: 1.24356325e-06
Iter: 104 loss: 1.69272869e-06
Iter: 105 loss: 1.24185112e-06
Iter: 106 loss: 1.21784296e-06
Iter: 107 loss: 1.27967473e-06
Iter: 108 loss: 1.20965774e-06
Iter: 109 loss: 1.18811624e-06
Iter: 110 loss: 1.29066063e-06
Iter: 111 loss: 1.18419007e-06
Iter: 112 loss: 1.1692407e-06
Iter: 113 loss: 1.2829671e-06
Iter: 114 loss: 1.16812794e-06
Iter: 115 loss: 1.15572425e-06
Iter: 116 loss: 1.29298928e-06
Iter: 117 loss: 1.15541434e-06
Iter: 118 loss: 1.14614841e-06
Iter: 119 loss: 1.13064186e-06
Iter: 120 loss: 1.13056672e-06
Iter: 121 loss: 1.12220812e-06
Iter: 122 loss: 1.12110342e-06
Iter: 123 loss: 1.11352779e-06
Iter: 124 loss: 1.10269991e-06
Iter: 125 loss: 1.10233827e-06
Iter: 126 loss: 1.08695565e-06
Iter: 127 loss: 1.11685426e-06
Iter: 128 loss: 1.08052222e-06
Iter: 129 loss: 1.07470078e-06
Iter: 130 loss: 1.07246331e-06
Iter: 131 loss: 1.06843027e-06
Iter: 132 loss: 1.05827053e-06
Iter: 133 loss: 1.15158491e-06
Iter: 134 loss: 1.05682341e-06
Iter: 135 loss: 1.04421167e-06
Iter: 136 loss: 1.05200672e-06
Iter: 137 loss: 1.03611046e-06
Iter: 138 loss: 1.02739409e-06
Iter: 139 loss: 1.02530566e-06
Iter: 140 loss: 1.01950104e-06
Iter: 141 loss: 1.01139528e-06
Iter: 142 loss: 1.01112721e-06
Iter: 143 loss: 9.99595727e-07
Iter: 144 loss: 9.87092e-07
Iter: 145 loss: 9.85182282e-07
Iter: 146 loss: 9.70869678e-07
Iter: 147 loss: 1.11243e-06
Iter: 148 loss: 9.70367864e-07
Iter: 149 loss: 9.64981e-07
Iter: 150 loss: 9.64534138e-07
Iter: 151 loss: 9.5823e-07
Iter: 152 loss: 9.54419193e-07
Iter: 153 loss: 9.51827815e-07
Iter: 154 loss: 9.4847644e-07
Iter: 155 loss: 9.89186901e-07
Iter: 156 loss: 9.48442903e-07
Iter: 157 loss: 9.45146212e-07
Iter: 158 loss: 9.46216744e-07
Iter: 159 loss: 9.42804832e-07
Iter: 160 loss: 9.38118092e-07
Iter: 161 loss: 9.3356573e-07
Iter: 162 loss: 9.32601949e-07
Iter: 163 loss: 9.29544512e-07
Iter: 164 loss: 9.28390136e-07
Iter: 165 loss: 9.25387724e-07
Iter: 166 loss: 9.17926172e-07
Iter: 167 loss: 9.96431709e-07
Iter: 168 loss: 9.17059594e-07
Iter: 169 loss: 9.07169e-07
Iter: 170 loss: 9.29761541e-07
Iter: 171 loss: 9.03417117e-07
Iter: 172 loss: 8.95720689e-07
Iter: 173 loss: 9.87548219e-07
Iter: 174 loss: 8.95604217e-07
Iter: 175 loss: 8.88303134e-07
Iter: 176 loss: 9.08022855e-07
Iter: 177 loss: 8.85907298e-07
Iter: 178 loss: 8.81447136e-07
Iter: 179 loss: 8.7172748e-07
Iter: 180 loss: 1.01766557e-06
Iter: 181 loss: 8.71325653e-07
Iter: 182 loss: 8.64245862e-07
Iter: 183 loss: 9.14924783e-07
Iter: 184 loss: 8.63595346e-07
Iter: 185 loss: 8.58428677e-07
Iter: 186 loss: 9.40458676e-07
Iter: 187 loss: 8.58431918e-07
Iter: 188 loss: 8.52816072e-07
Iter: 189 loss: 8.57211433e-07
Iter: 190 loss: 8.49403477e-07
Iter: 191 loss: 8.45498676e-07
Iter: 192 loss: 8.52072901e-07
Iter: 193 loss: 8.43720386e-07
Iter: 194 loss: 8.3874761e-07
Iter: 195 loss: 8.68609561e-07
Iter: 196 loss: 8.38132678e-07
Iter: 197 loss: 8.33877e-07
Iter: 198 loss: 8.27054e-07
Iter: 199 loss: 8.27005e-07
Iter: 200 loss: 8.2561894e-07
Iter: 201 loss: 8.23970652e-07
Iter: 202 loss: 8.21130243e-07
Iter: 203 loss: 8.14958696e-07
Iter: 204 loss: 9.06174591e-07
Iter: 205 loss: 8.14651798e-07
Iter: 206 loss: 8.10682423e-07
Iter: 207 loss: 8.33205263e-07
Iter: 208 loss: 8.10122344e-07
Iter: 209 loss: 8.06488629e-07
Iter: 210 loss: 8.18309957e-07
Iter: 211 loss: 8.05523882e-07
Iter: 212 loss: 8.01556098e-07
Iter: 213 loss: 8.32377395e-07
Iter: 214 loss: 8.01257727e-07
Iter: 215 loss: 7.98429824e-07
Iter: 216 loss: 7.94081132e-07
Iter: 217 loss: 7.93969207e-07
Iter: 218 loss: 7.88201135e-07
Iter: 219 loss: 7.81870938e-07
Iter: 220 loss: 7.80916935e-07
Iter: 221 loss: 7.74191562e-07
Iter: 222 loss: 7.7412551e-07
Iter: 223 loss: 7.69969347e-07
Iter: 224 loss: 7.71048576e-07
Iter: 225 loss: 7.66941298e-07
Iter: 226 loss: 7.66385824e-07
Iter: 227 loss: 7.64202468e-07
Iter: 228 loss: 7.6273659e-07
Iter: 229 loss: 7.59277896e-07
Iter: 230 loss: 7.98448411e-07
Iter: 231 loss: 7.5894468e-07
Iter: 232 loss: 7.5512105e-07
Iter: 233 loss: 7.57557473e-07
Iter: 234 loss: 7.52729136e-07
Iter: 235 loss: 7.49934316e-07
Iter: 236 loss: 7.49414198e-07
Iter: 237 loss: 7.474e-07
Iter: 238 loss: 7.43656358e-07
Iter: 239 loss: 8.29347812e-07
Iter: 240 loss: 7.43683245e-07
Iter: 241 loss: 7.40300379e-07
Iter: 242 loss: 7.78151218e-07
Iter: 243 loss: 7.40241717e-07
Iter: 244 loss: 7.36858283e-07
Iter: 245 loss: 7.45276964e-07
Iter: 246 loss: 7.35711069e-07
Iter: 247 loss: 7.33263107e-07
Iter: 248 loss: 7.29764565e-07
Iter: 249 loss: 7.29615749e-07
Iter: 250 loss: 7.27054612e-07
Iter: 251 loss: 7.26917733e-07
Iter: 252 loss: 7.24183565e-07
Iter: 253 loss: 7.22901518e-07
Iter: 254 loss: 7.2158673e-07
Iter: 255 loss: 7.18514627e-07
Iter: 256 loss: 7.27078714e-07
Iter: 257 loss: 7.17510147e-07
Iter: 258 loss: 7.14744715e-07
Iter: 259 loss: 7.11116684e-07
Iter: 260 loss: 7.10899599e-07
Iter: 261 loss: 7.05816376e-07
Iter: 262 loss: 7.37200821e-07
Iter: 263 loss: 7.05203774e-07
Iter: 264 loss: 7.01464955e-07
Iter: 265 loss: 7.35789172e-07
Iter: 266 loss: 7.01281692e-07
Iter: 267 loss: 6.98267741e-07
Iter: 268 loss: 7.30479542e-07
Iter: 269 loss: 6.9820635e-07
Iter: 270 loss: 6.96964037e-07
Iter: 271 loss: 6.9386158e-07
Iter: 272 loss: 7.226393e-07
Iter: 273 loss: 6.93403e-07
Iter: 274 loss: 6.91730747e-07
Iter: 275 loss: 6.91352284e-07
Iter: 276 loss: 6.89496403e-07
Iter: 277 loss: 6.89073318e-07
Iter: 278 loss: 6.87841407e-07
Iter: 279 loss: 6.86231658e-07
Iter: 280 loss: 6.9089208e-07
Iter: 281 loss: 6.85740758e-07
Iter: 282 loss: 6.82971859e-07
Iter: 283 loss: 6.83269491e-07
Iter: 284 loss: 6.80883602e-07
Iter: 285 loss: 6.78275967e-07
Iter: 286 loss: 6.77765229e-07
Iter: 287 loss: 6.76076183e-07
Iter: 288 loss: 6.72977933e-07
Iter: 289 loss: 6.9619955e-07
Iter: 290 loss: 6.72706278e-07
Iter: 291 loss: 6.69690735e-07
Iter: 292 loss: 6.87624606e-07
Iter: 293 loss: 6.69306701e-07
Iter: 294 loss: 6.67399547e-07
Iter: 295 loss: 6.64159e-07
Iter: 296 loss: 6.64142817e-07
Iter: 297 loss: 6.61137506e-07
Iter: 298 loss: 6.77666776e-07
Iter: 299 loss: 6.6066309e-07
Iter: 300 loss: 6.57828252e-07
Iter: 301 loss: 6.64994275e-07
Iter: 302 loss: 6.56834686e-07
Iter: 303 loss: 6.55380518e-07
Iter: 304 loss: 6.7386469e-07
Iter: 305 loss: 6.55382678e-07
Iter: 306 loss: 6.5336792e-07
Iter: 307 loss: 6.50860727e-07
Iter: 308 loss: 6.50672746e-07
Iter: 309 loss: 6.48526623e-07
Iter: 310 loss: 6.5565149e-07
Iter: 311 loss: 6.47925049e-07
Iter: 312 loss: 6.46309104e-07
Iter: 313 loss: 6.64315849e-07
Iter: 314 loss: 6.46282786e-07
Iter: 315 loss: 6.44569809e-07
Iter: 316 loss: 6.42630368e-07
Iter: 317 loss: 6.42397083e-07
Iter: 318 loss: 6.40484416e-07
Iter: 319 loss: 6.59980401e-07
Iter: 320 loss: 6.4042888e-07
Iter: 321 loss: 6.38472329e-07
Iter: 322 loss: 6.41526185e-07
Iter: 323 loss: 6.37584321e-07
Iter: 324 loss: 6.36016466e-07
Iter: 325 loss: 6.32438855e-07
Iter: 326 loss: 6.74974899e-07
Iter: 327 loss: 6.32137755e-07
Iter: 328 loss: 6.31520891e-07
Iter: 329 loss: 6.3038226e-07
Iter: 330 loss: 6.28712769e-07
Iter: 331 loss: 6.2948493e-07
Iter: 332 loss: 6.27572263e-07
Iter: 333 loss: 6.26046813e-07
Iter: 334 loss: 6.24754875e-07
Iter: 335 loss: 6.24353333e-07
Iter: 336 loss: 6.21998197e-07
Iter: 337 loss: 6.31565854e-07
Iter: 338 loss: 6.21509116e-07
Iter: 339 loss: 6.19610091e-07
Iter: 340 loss: 6.22904281e-07
Iter: 341 loss: 6.18770912e-07
Iter: 342 loss: 6.16899058e-07
Iter: 343 loss: 6.42761506e-07
Iter: 344 loss: 6.1690497e-07
Iter: 345 loss: 6.15209785e-07
Iter: 346 loss: 6.21312552e-07
Iter: 347 loss: 6.14771466e-07
Iter: 348 loss: 6.13408304e-07
Iter: 349 loss: 6.10029929e-07
Iter: 350 loss: 6.43636781e-07
Iter: 351 loss: 6.09609572e-07
Iter: 352 loss: 6.08094695e-07
Iter: 353 loss: 6.07969923e-07
Iter: 354 loss: 6.06647518e-07
Iter: 355 loss: 6.18271372e-07
Iter: 356 loss: 6.06587548e-07
Iter: 357 loss: 6.05596313e-07
Iter: 358 loss: 6.03720935e-07
Iter: 359 loss: 6.44842373e-07
Iter: 360 loss: 6.03698197e-07
Iter: 361 loss: 6.02609248e-07
Iter: 362 loss: 6.02430646e-07
Iter: 363 loss: 6.01618581e-07
Iter: 364 loss: 5.99954433e-07
Iter: 365 loss: 6.28322709e-07
Iter: 366 loss: 5.9991919e-07
Iter: 367 loss: 5.97586677e-07
Iter: 368 loss: 5.94817379e-07
Iter: 369 loss: 5.94509743e-07
Iter: 370 loss: 5.93484231e-07
Iter: 371 loss: 5.92818878e-07
Iter: 372 loss: 5.91005801e-07
Iter: 373 loss: 5.92981792e-07
Iter: 374 loss: 5.90008767e-07
Iter: 375 loss: 5.88450916e-07
Iter: 376 loss: 5.88163402e-07
Iter: 377 loss: 5.87132945e-07
Iter: 378 loss: 5.85709131e-07
Iter: 379 loss: 5.94987341e-07
Iter: 380 loss: 5.85556563e-07
Iter: 381 loss: 5.84778491e-07
Iter: 382 loss: 5.90081072e-07
Iter: 383 loss: 5.84682198e-07
Iter: 384 loss: 5.83600126e-07
Iter: 385 loss: 5.84337272e-07
Iter: 386 loss: 5.82911412e-07
Iter: 387 loss: 5.82024e-07
Iter: 388 loss: 5.81575136e-07
Iter: 389 loss: 5.81167285e-07
Iter: 390 loss: 5.79468633e-07
Iter: 391 loss: 5.83158e-07
Iter: 392 loss: 5.78852791e-07
Iter: 393 loss: 5.76783862e-07
Iter: 394 loss: 5.93474908e-07
Iter: 395 loss: 5.76672733e-07
Iter: 396 loss: 5.75430306e-07
Iter: 397 loss: 5.73876491e-07
Iter: 398 loss: 5.73759621e-07
Iter: 399 loss: 5.7227453e-07
Iter: 400 loss: 5.72258841e-07
Iter: 401 loss: 5.71056944e-07
Iter: 402 loss: 5.68798612e-07
Iter: 403 loss: 6.17897399e-07
Iter: 404 loss: 5.68782525e-07
Iter: 405 loss: 5.67618827e-07
Iter: 406 loss: 5.72704607e-07
Iter: 407 loss: 5.6737008e-07
Iter: 408 loss: 5.66522374e-07
Iter: 409 loss: 5.66512256e-07
Iter: 410 loss: 5.65730147e-07
Iter: 411 loss: 5.6491092e-07
Iter: 412 loss: 5.64764719e-07
Iter: 413 loss: 5.63444587e-07
Iter: 414 loss: 5.6178817e-07
Iter: 415 loss: 5.61661295e-07
Iter: 416 loss: 5.59733792e-07
Iter: 417 loss: 5.69469535e-07
Iter: 418 loss: 5.59448e-07
Iter: 419 loss: 5.58519389e-07
Iter: 420 loss: 5.58329702e-07
Iter: 421 loss: 5.57381554e-07
Iter: 422 loss: 5.55610882e-07
Iter: 423 loss: 5.93575578e-07
Iter: 424 loss: 5.55578367e-07
Iter: 425 loss: 5.54238341e-07
Iter: 426 loss: 5.60648573e-07
Iter: 427 loss: 5.54014946e-07
Iter: 428 loss: 5.53136829e-07
Iter: 429 loss: 5.58410534e-07
Iter: 430 loss: 5.53034283e-07
Iter: 431 loss: 5.51864218e-07
Iter: 432 loss: 5.53058157e-07
Iter: 433 loss: 5.51165499e-07
Iter: 434 loss: 5.50169148e-07
Iter: 435 loss: 5.50285506e-07
Iter: 436 loss: 5.49381127e-07
Iter: 437 loss: 5.48050139e-07
Iter: 438 loss: 5.61872241e-07
Iter: 439 loss: 5.48018306e-07
Iter: 440 loss: 5.46909405e-07
Iter: 441 loss: 5.4596768e-07
Iter: 442 loss: 5.45649073e-07
Iter: 443 loss: 5.43931435e-07
Iter: 444 loss: 5.45152318e-07
Iter: 445 loss: 5.4286545e-07
Iter: 446 loss: 5.42270072e-07
Iter: 447 loss: 5.42036e-07
Iter: 448 loss: 5.41239501e-07
Iter: 449 loss: 5.40538338e-07
Iter: 450 loss: 5.40338647e-07
Iter: 451 loss: 5.39614064e-07
Iter: 452 loss: 5.41099325e-07
Iter: 453 loss: 5.39303414e-07
Iter: 454 loss: 5.38507436e-07
Iter: 455 loss: 5.38513859e-07
Iter: 456 loss: 5.37858625e-07
Iter: 457 loss: 5.37548431e-07
Iter: 458 loss: 5.37266885e-07
Iter: 459 loss: 5.36800712e-07
Iter: 460 loss: 5.35513664e-07
Iter: 461 loss: 5.43288934e-07
Iter: 462 loss: 5.35181698e-07
Iter: 463 loss: 5.33353045e-07
Iter: 464 loss: 5.33967352e-07
Iter: 465 loss: 5.32038939e-07
Iter: 466 loss: 5.32813829e-07
Iter: 467 loss: 5.3111944e-07
Iter: 468 loss: 5.3065304e-07
Iter: 469 loss: 5.29463193e-07
Iter: 470 loss: 5.40656e-07
Iter: 471 loss: 5.29298632e-07
Iter: 472 loss: 5.27939733e-07
Iter: 473 loss: 5.38585141e-07
Iter: 474 loss: 5.27839575e-07
Iter: 475 loss: 5.26662461e-07
Iter: 476 loss: 5.33985258e-07
Iter: 477 loss: 5.26514896e-07
Iter: 478 loss: 5.2591929e-07
Iter: 479 loss: 5.24519e-07
Iter: 480 loss: 5.41515135e-07
Iter: 481 loss: 5.24383438e-07
Iter: 482 loss: 5.23389929e-07
Iter: 483 loss: 5.23394078e-07
Iter: 484 loss: 5.22493337e-07
Iter: 485 loss: 5.29887359e-07
Iter: 486 loss: 5.22452524e-07
Iter: 487 loss: 5.2193468e-07
Iter: 488 loss: 5.20763251e-07
Iter: 489 loss: 5.36372625e-07
Iter: 490 loss: 5.20696858e-07
Iter: 491 loss: 5.19266223e-07
Iter: 492 loss: 5.27106863e-07
Iter: 493 loss: 5.19051696e-07
Iter: 494 loss: 5.1834553e-07
Iter: 495 loss: 5.24932e-07
Iter: 496 loss: 5.18334616e-07
Iter: 497 loss: 5.17613444e-07
Iter: 498 loss: 5.2050251e-07
Iter: 499 loss: 5.17436206e-07
Iter: 500 loss: 5.16783e-07
Iter: 501 loss: 5.15582315e-07
Iter: 502 loss: 5.15597321e-07
Iter: 503 loss: 5.14406e-07
Iter: 504 loss: 5.19702439e-07
Iter: 505 loss: 5.14173053e-07
Iter: 506 loss: 5.13363375e-07
Iter: 507 loss: 5.24621214e-07
Iter: 508 loss: 5.13361726e-07
Iter: 509 loss: 5.12404654e-07
Iter: 510 loss: 5.11072869e-07
Iter: 511 loss: 5.11001417e-07
Iter: 512 loss: 5.101395e-07
Iter: 513 loss: 5.10817927e-07
Iter: 514 loss: 5.09620577e-07
Iter: 515 loss: 5.08606945e-07
Iter: 516 loss: 5.08610128e-07
Iter: 517 loss: 5.08170103e-07
Iter: 518 loss: 5.07879065e-07
Iter: 519 loss: 5.07736502e-07
Iter: 520 loss: 5.07167385e-07
Iter: 521 loss: 5.08331766e-07
Iter: 522 loss: 5.06928814e-07
Iter: 523 loss: 5.05917797e-07
Iter: 524 loss: 5.07347e-07
Iter: 525 loss: 5.05428773e-07
Iter: 526 loss: 5.04499042e-07
Iter: 527 loss: 5.03499905e-07
Iter: 528 loss: 5.0337627e-07
Iter: 529 loss: 5.01966156e-07
Iter: 530 loss: 5.03457159e-07
Iter: 531 loss: 5.01197292e-07
Iter: 532 loss: 5.00560304e-07
Iter: 533 loss: 5.00437807e-07
Iter: 534 loss: 4.99662065e-07
Iter: 535 loss: 5.02292e-07
Iter: 536 loss: 4.99483e-07
Iter: 537 loss: 4.98918e-07
Iter: 538 loss: 4.97831e-07
Iter: 539 loss: 5.19663104e-07
Iter: 540 loss: 4.97808173e-07
Iter: 541 loss: 4.96651182e-07
Iter: 542 loss: 5.03603189e-07
Iter: 543 loss: 4.96484404e-07
Iter: 544 loss: 4.9614988e-07
Iter: 545 loss: 4.96024029e-07
Iter: 546 loss: 4.95621805e-07
Iter: 547 loss: 4.94787969e-07
Iter: 548 loss: 5.08629e-07
Iter: 549 loss: 4.94762e-07
Iter: 550 loss: 4.93663e-07
Iter: 551 loss: 4.95342647e-07
Iter: 552 loss: 4.93167136e-07
Iter: 553 loss: 4.92814252e-07
Iter: 554 loss: 4.92724098e-07
Iter: 555 loss: 4.92329264e-07
Iter: 556 loss: 4.91674768e-07
Iter: 557 loss: 4.91652031e-07
Iter: 558 loss: 4.90919433e-07
Iter: 559 loss: 4.91549031e-07
Iter: 560 loss: 4.90496518e-07
Iter: 561 loss: 4.90066668e-07
Iter: 562 loss: 4.90007437e-07
Iter: 563 loss: 4.89513127e-07
Iter: 564 loss: 4.88870569e-07
Iter: 565 loss: 4.88849537e-07
Iter: 566 loss: 4.87955276e-07
Iter: 567 loss: 4.88142462e-07
Iter: 568 loss: 4.87327384e-07
Iter: 569 loss: 4.8635809e-07
Iter: 570 loss: 4.91587855e-07
Iter: 571 loss: 4.86221438e-07
Iter: 572 loss: 4.85172848e-07
Iter: 573 loss: 4.93540085e-07
Iter: 574 loss: 4.85087355e-07
Iter: 575 loss: 4.84694624e-07
Iter: 576 loss: 4.83725e-07
Iter: 577 loss: 4.93424864e-07
Iter: 578 loss: 4.8361261e-07
Iter: 579 loss: 4.82586699e-07
Iter: 580 loss: 4.92671802e-07
Iter: 581 loss: 4.82572545e-07
Iter: 582 loss: 4.81949428e-07
Iter: 583 loss: 4.81924928e-07
Iter: 584 loss: 4.81643212e-07
Iter: 585 loss: 4.81047778e-07
Iter: 586 loss: 4.91658398e-07
Iter: 587 loss: 4.810463e-07
Iter: 588 loss: 4.8022639e-07
Iter: 589 loss: 4.7989721e-07
Iter: 590 loss: 4.79472874e-07
Iter: 591 loss: 4.79109644e-07
Iter: 592 loss: 4.78793481e-07
Iter: 593 loss: 4.78382617e-07
Iter: 594 loss: 4.77425374e-07
Iter: 595 loss: 4.88407636e-07
Iter: 596 loss: 4.77292872e-07
Iter: 597 loss: 4.76370332e-07
Iter: 598 loss: 4.8169511e-07
Iter: 599 loss: 4.76254684e-07
Iter: 600 loss: 4.75652541e-07
Iter: 601 loss: 4.75616531e-07
Iter: 602 loss: 4.75278455e-07
Iter: 603 loss: 4.75298862e-07
Iter: 604 loss: 4.75034881e-07
Iter: 605 loss: 4.74594543e-07
Iter: 606 loss: 4.74181661e-07
Iter: 607 loss: 4.74073687e-07
Iter: 608 loss: 4.7377597e-07
Iter: 609 loss: 4.73682491e-07
Iter: 610 loss: 4.73313037e-07
Iter: 611 loss: 4.72575806e-07
Iter: 612 loss: 4.87087959e-07
Iter: 613 loss: 4.72579785e-07
Iter: 614 loss: 4.71753026e-07
Iter: 615 loss: 4.72701942e-07
Iter: 616 loss: 4.71305754e-07
Iter: 617 loss: 4.70796522e-07
Iter: 618 loss: 4.70715634e-07
Iter: 619 loss: 4.70233203e-07
Iter: 620 loss: 4.69124871e-07
Iter: 621 loss: 4.81725749e-07
Iter: 622 loss: 4.68989754e-07
Iter: 623 loss: 4.67892335e-07
Iter: 624 loss: 4.71457241e-07
Iter: 625 loss: 4.67595527e-07
Iter: 626 loss: 4.6728195e-07
Iter: 627 loss: 4.67195434e-07
Iter: 628 loss: 4.66783575e-07
Iter: 629 loss: 4.66336246e-07
Iter: 630 loss: 4.66240351e-07
Iter: 631 loss: 4.65787423e-07
Iter: 632 loss: 4.67070322e-07
Iter: 633 loss: 4.65644774e-07
Iter: 634 loss: 4.65191647e-07
Iter: 635 loss: 4.67051e-07
Iter: 636 loss: 4.65066961e-07
Iter: 637 loss: 4.64372818e-07
Iter: 638 loss: 4.64007144e-07
Iter: 639 loss: 4.63644653e-07
Iter: 640 loss: 4.63014885e-07
Iter: 641 loss: 4.63086081e-07
Iter: 642 loss: 4.62492096e-07
Iter: 643 loss: 4.62069238e-07
Iter: 644 loss: 4.61962344e-07
Iter: 645 loss: 4.61489151e-07
Iter: 646 loss: 4.60548279e-07
Iter: 647 loss: 4.8046644e-07
Iter: 648 loss: 4.60563484e-07
Iter: 649 loss: 4.59728142e-07
Iter: 650 loss: 4.6420044e-07
Iter: 651 loss: 4.59605644e-07
Iter: 652 loss: 4.59296331e-07
Iter: 653 loss: 4.59267739e-07
Iter: 654 loss: 4.58973602e-07
Iter: 655 loss: 4.58443907e-07
Iter: 656 loss: 4.71092278e-07
Iter: 657 loss: 4.58426882e-07
Iter: 658 loss: 4.57906367e-07
Iter: 659 loss: 4.5902641e-07
Iter: 660 loss: 4.57654551e-07
Iter: 661 loss: 4.57021741e-07
Iter: 662 loss: 4.57015034e-07
Iter: 663 loss: 4.56524845e-07
Iter: 664 loss: 4.55662985e-07
Iter: 665 loss: 4.55665116e-07
Iter: 666 loss: 4.55081022e-07
Iter: 667 loss: 4.54480642e-07
Iter: 668 loss: 4.54380654e-07
Iter: 669 loss: 4.5380196e-07
Iter: 670 loss: 4.54297378e-07
Iter: 671 loss: 4.53467237e-07
Iter: 672 loss: 4.5256246e-07
Iter: 673 loss: 4.60061813e-07
Iter: 674 loss: 4.5252e-07
Iter: 675 loss: 4.52240926e-07
Iter: 676 loss: 4.51831454e-07
Iter: 677 loss: 4.5183782e-07
Iter: 678 loss: 4.51342828e-07
Iter: 679 loss: 4.53107305e-07
Iter: 680 loss: 4.51214532e-07
Iter: 681 loss: 4.50703396e-07
Iter: 682 loss: 4.57064232e-07
Iter: 683 loss: 4.50708711e-07
Iter: 684 loss: 4.50351763e-07
Iter: 685 loss: 4.49641334e-07
Iter: 686 loss: 4.61354205e-07
Iter: 687 loss: 4.49626668e-07
Iter: 688 loss: 4.48852063e-07
Iter: 689 loss: 4.48246425e-07
Iter: 690 loss: 4.47977584e-07
Iter: 691 loss: 4.46821161e-07
Iter: 692 loss: 4.60806831e-07
Iter: 693 loss: 4.46815022e-07
Iter: 694 loss: 4.45976411e-07
Iter: 695 loss: 4.48355593e-07
Iter: 696 loss: 4.45709077e-07
Iter: 697 loss: 4.45281842e-07
Iter: 698 loss: 4.45199305e-07
Iter: 699 loss: 4.44962893e-07
Iter: 700 loss: 4.44539296e-07
Iter: 701 loss: 4.54848049e-07
Iter: 702 loss: 4.44537477e-07
Iter: 703 loss: 4.44040666e-07
Iter: 704 loss: 4.44331818e-07
Iter: 705 loss: 4.43695484e-07
Iter: 706 loss: 4.43434601e-07
Iter: 707 loss: 4.43366957e-07
Iter: 708 loss: 4.43080182e-07
Iter: 709 loss: 4.42764815e-07
Iter: 710 loss: 4.42689156e-07
Iter: 711 loss: 4.42205135e-07
Iter: 712 loss: 4.41124399e-07
Iter: 713 loss: 4.57851229e-07
Iter: 714 loss: 4.41053373e-07
Iter: 715 loss: 4.41219242e-07
Iter: 716 loss: 4.40597773e-07
Iter: 717 loss: 4.40089309e-07
Iter: 718 loss: 4.39234128e-07
Iter: 719 loss: 4.39232849e-07
Iter: 720 loss: 4.38512814e-07
Iter: 721 loss: 4.44323121e-07
Iter: 722 loss: 4.38483198e-07
Iter: 723 loss: 4.37946142e-07
Iter: 724 loss: 4.43606723e-07
Iter: 725 loss: 4.37927184e-07
Iter: 726 loss: 4.37666586e-07
Iter: 727 loss: 4.37164488e-07
Iter: 728 loss: 4.46967e-07
Iter: 729 loss: 4.37169177e-07
Iter: 730 loss: 4.36626181e-07
Iter: 731 loss: 4.37632565e-07
Iter: 732 loss: 4.36396959e-07
Iter: 733 loss: 4.36311353e-07
Iter: 734 loss: 4.36153016e-07
Iter: 735 loss: 4.3592911e-07
Iter: 736 loss: 4.35401205e-07
Iter: 737 loss: 4.41104618e-07
Iter: 738 loss: 4.35368264e-07
Iter: 739 loss: 4.34731447e-07
Iter: 740 loss: 4.36210485e-07
Iter: 741 loss: 4.34469797e-07
Iter: 742 loss: 4.33985548e-07
Iter: 743 loss: 4.39154348e-07
Iter: 744 loss: 4.33976652e-07
Iter: 745 loss: 4.33496979e-07
Iter: 746 loss: 4.34793435e-07
Iter: 747 loss: 4.33349413e-07
Iter: 748 loss: 4.32907683e-07
Iter: 749 loss: 4.32354909e-07
Iter: 750 loss: 4.32334417e-07
Iter: 751 loss: 4.31671879e-07
Iter: 752 loss: 4.35228799e-07
Iter: 753 loss: 4.31560579e-07
Iter: 754 loss: 4.31163755e-07
Iter: 755 loss: 4.36583889e-07
Iter: 756 loss: 4.3115574e-07
Iter: 757 loss: 4.30738339e-07
Iter: 758 loss: 4.30713186e-07
Iter: 759 loss: 4.30384944e-07
Iter: 760 loss: 4.3002251e-07
Iter: 761 loss: 4.30865e-07
Iter: 762 loss: 4.29875485e-07
Iter: 763 loss: 4.29254953e-07
Iter: 764 loss: 4.30529894e-07
Iter: 765 loss: 4.29010043e-07
Iter: 766 loss: 4.28646615e-07
Iter: 767 loss: 4.28015483e-07
Iter: 768 loss: 4.28011163e-07
Iter: 769 loss: 4.27167862e-07
Iter: 770 loss: 4.2821182e-07
Iter: 771 loss: 4.26726245e-07
Iter: 772 loss: 4.26368644e-07
Iter: 773 loss: 4.26317513e-07
Iter: 774 loss: 4.26027555e-07
Iter: 775 loss: 4.29193562e-07
Iter: 776 loss: 4.26017664e-07
Iter: 777 loss: 4.25750613e-07
Iter: 778 loss: 4.25087876e-07
Iter: 779 loss: 4.31122885e-07
Iter: 780 loss: 4.24992948e-07
Iter: 781 loss: 4.24386087e-07
Iter: 782 loss: 4.25499479e-07
Iter: 783 loss: 4.24143423e-07
Iter: 784 loss: 4.23873928e-07
Iter: 785 loss: 4.2377917e-07
Iter: 786 loss: 4.23402327e-07
Iter: 787 loss: 4.22943913e-07
Iter: 788 loss: 4.22924217e-07
Iter: 789 loss: 4.22366156e-07
Iter: 790 loss: 4.23167734e-07
Iter: 791 loss: 4.22098594e-07
Iter: 792 loss: 4.21475391e-07
Iter: 793 loss: 4.23660794e-07
Iter: 794 loss: 4.21322767e-07
Iter: 795 loss: 4.21076322e-07
Iter: 796 loss: 4.2104466e-07
Iter: 797 loss: 4.20783522e-07
Iter: 798 loss: 4.20494302e-07
Iter: 799 loss: 4.20454882e-07
Iter: 800 loss: 4.20014146e-07
Iter: 801 loss: 4.20962635e-07
Iter: 802 loss: 4.19839097e-07
Iter: 803 loss: 4.19256821e-07
Iter: 804 loss: 4.22210803e-07
Iter: 805 loss: 4.19164479e-07
Iter: 806 loss: 4.18835043e-07
Iter: 807 loss: 4.18628474e-07
Iter: 808 loss: 4.1848017e-07
Iter: 809 loss: 4.18158606e-07
Iter: 810 loss: 4.22198525e-07
Iter: 811 loss: 4.1814917e-07
Iter: 812 loss: 4.17766273e-07
Iter: 813 loss: 4.17304477e-07
Iter: 814 loss: 4.17254626e-07
Iter: 815 loss: 4.16929879e-07
Iter: 816 loss: 4.16450973e-07
Iter: 817 loss: 4.16425422e-07
Iter: 818 loss: 4.16085015e-07
Iter: 819 loss: 4.16021891e-07
Iter: 820 loss: 4.15685065e-07
Iter: 821 loss: 4.16244433e-07
Iter: 822 loss: 4.15539489e-07
Iter: 823 loss: 4.15231142e-07
Iter: 824 loss: 4.14553142e-07
Iter: 825 loss: 4.24851123e-07
Iter: 826 loss: 4.14520969e-07
Iter: 827 loss: 4.13823102e-07
Iter: 828 loss: 4.19169567e-07
Iter: 829 loss: 4.13765349e-07
Iter: 830 loss: 4.13344054e-07
Iter: 831 loss: 4.13349824e-07
Iter: 832 loss: 4.12997395e-07
Iter: 833 loss: 4.12904569e-07
Iter: 834 loss: 4.12699535e-07
Iter: 835 loss: 4.12312318e-07
Iter: 836 loss: 4.1349395e-07
Iter: 837 loss: 4.12217048e-07
Iter: 838 loss: 4.11866495e-07
Iter: 839 loss: 4.15737958e-07
Iter: 840 loss: 4.11868569e-07
Iter: 841 loss: 4.11613627e-07
Iter: 842 loss: 4.10928379e-07
Iter: 843 loss: 4.14631586e-07
Iter: 844 loss: 4.10750658e-07
Iter: 845 loss: 4.10615883e-07
Iter: 846 loss: 4.10394193e-07
Iter: 847 loss: 4.10077462e-07
Iter: 848 loss: 4.10223265e-07
Iter: 849 loss: 4.09831841e-07
Iter: 850 loss: 4.09553394e-07
Iter: 851 loss: 4.09255051e-07
Iter: 852 loss: 4.09195025e-07
Iter: 853 loss: 4.08750964e-07
Iter: 854 loss: 4.14089698e-07
Iter: 855 loss: 4.08742466e-07
Iter: 856 loss: 4.08371477e-07
Iter: 857 loss: 4.08884148e-07
Iter: 858 loss: 4.08180711e-07
Iter: 859 loss: 4.07960727e-07
Iter: 860 loss: 4.07527409e-07
Iter: 861 loss: 4.075319e-07
Iter: 862 loss: 4.07075277e-07
Iter: 863 loss: 4.13097439e-07
Iter: 864 loss: 4.07057087e-07
Iter: 865 loss: 4.0671307e-07
Iter: 866 loss: 4.07781272e-07
Iter: 867 loss: 4.06572724e-07
Iter: 868 loss: 4.06207818e-07
Iter: 869 loss: 4.05887704e-07
Iter: 870 loss: 4.05796868e-07
Iter: 871 loss: 4.0529909e-07
Iter: 872 loss: 4.11371e-07
Iter: 873 loss: 4.05278712e-07
Iter: 874 loss: 4.04868956e-07
Iter: 875 loss: 4.05842854e-07
Iter: 876 loss: 4.04715593e-07
Iter: 877 loss: 4.04444563e-07
Iter: 878 loss: 4.04256582e-07
Iter: 879 loss: 4.04164865e-07
Iter: 880 loss: 4.03976202e-07
Iter: 881 loss: 4.03949485e-07
Iter: 882 loss: 4.03723902e-07
Iter: 883 loss: 4.03434399e-07
Iter: 884 loss: 4.03413196e-07
Iter: 885 loss: 4.0313904e-07
Iter: 886 loss: 4.03422462e-07
Iter: 887 loss: 4.02991674e-07
Iter: 888 loss: 4.02586664e-07
Iter: 889 loss: 4.05735648e-07
Iter: 890 loss: 4.02566741e-07
Iter: 891 loss: 4.02294688e-07
Iter: 892 loss: 4.0191847e-07
Iter: 893 loss: 4.01916623e-07
Iter: 894 loss: 4.01438569e-07
Iter: 895 loss: 4.03841454e-07
Iter: 896 loss: 4.0135285e-07
Iter: 897 loss: 4.0093272e-07
Iter: 898 loss: 4.03915379e-07
Iter: 899 loss: 4.00903559e-07
Iter: 900 loss: 4.00540557e-07
Iter: 901 loss: 4.00480559e-07
Iter: 902 loss: 4.00226753e-07
Iter: 903 loss: 3.99889473e-07
Iter: 904 loss: 4.0335442e-07
Iter: 905 loss: 3.99890581e-07
Iter: 906 loss: 3.99629926e-07
Iter: 907 loss: 4.00618575e-07
Iter: 908 loss: 3.99589e-07
Iter: 909 loss: 3.99364666e-07
Iter: 910 loss: 3.99092301e-07
Iter: 911 loss: 3.99054954e-07
Iter: 912 loss: 3.98756185e-07
Iter: 913 loss: 4.00647707e-07
Iter: 914 loss: 3.98705112e-07
Iter: 915 loss: 3.98357457e-07
Iter: 916 loss: 3.99952796e-07
Iter: 917 loss: 3.98289529e-07
Iter: 918 loss: 3.98004204e-07
Iter: 919 loss: 3.97527828e-07
Iter: 920 loss: 3.97512451e-07
Iter: 921 loss: 3.97213597e-07
Iter: 922 loss: 3.97193247e-07
Iter: 923 loss: 3.96833371e-07
Iter: 924 loss: 3.96547932e-07
Iter: 925 loss: 3.96433194e-07
Iter: 926 loss: 3.95981772e-07
Iter: 927 loss: 3.96994494e-07
Iter: 928 loss: 3.95789868e-07
Iter: 929 loss: 3.95513268e-07
Iter: 930 loss: 3.95516e-07
Iter: 931 loss: 3.95290186e-07
Iter: 932 loss: 3.9529408e-07
Iter: 933 loss: 3.9511113e-07
Iter: 934 loss: 3.9486028e-07
Iter: 935 loss: 3.95368829e-07
Iter: 936 loss: 3.94740255e-07
Iter: 937 loss: 3.94455355e-07
Iter: 938 loss: 3.96528606e-07
Iter: 939 loss: 3.94430231e-07
Iter: 940 loss: 3.94202203e-07
Iter: 941 loss: 3.94041962e-07
Iter: 942 loss: 3.93963859e-07
Iter: 943 loss: 3.935927e-07
Iter: 944 loss: 3.9382445e-07
Iter: 945 loss: 3.9335373e-07
Iter: 946 loss: 3.93157706e-07
Iter: 947 loss: 3.93088555e-07
Iter: 948 loss: 3.92918849e-07
Iter: 949 loss: 3.92509719e-07
Iter: 950 loss: 3.97031357e-07
Iter: 951 loss: 3.92490819e-07
Iter: 952 loss: 3.92057927e-07
Iter: 953 loss: 3.9559589e-07
Iter: 954 loss: 3.92047809e-07
Iter: 955 loss: 3.91713229e-07
Iter: 956 loss: 3.94377935e-07
Iter: 957 loss: 3.91693163e-07
Iter: 958 loss: 3.91501544e-07
Iter: 959 loss: 3.91170488e-07
Iter: 960 loss: 3.91178787e-07
Iter: 961 loss: 3.90819366e-07
Iter: 962 loss: 3.94840953e-07
Iter: 963 loss: 3.90821413e-07
Iter: 964 loss: 3.90472906e-07
Iter: 965 loss: 3.91219658e-07
Iter: 966 loss: 3.90359247e-07
Iter: 967 loss: 3.90127809e-07
Iter: 968 loss: 3.90653099e-07
Iter: 969 loss: 3.90028731e-07
Iter: 970 loss: 3.89797265e-07
Iter: 971 loss: 3.91311744e-07
Iter: 972 loss: 3.89765745e-07
Iter: 973 loss: 3.89538e-07
Iter: 974 loss: 3.89427271e-07
Iter: 975 loss: 3.89319951e-07
Iter: 976 loss: 3.89062421e-07
Iter: 977 loss: 3.9005667e-07
Iter: 978 loss: 3.89005464e-07
Iter: 979 loss: 3.88851134e-07
Iter: 980 loss: 3.90721823e-07
Iter: 981 loss: 3.88833428e-07
Iter: 982 loss: 3.8864539e-07
Iter: 983 loss: 3.88325503e-07
Iter: 984 loss: 3.88349463e-07
Iter: 985 loss: 3.87997972e-07
Iter: 986 loss: 3.88693934e-07
Iter: 987 loss: 3.87859075e-07
Iter: 988 loss: 3.87601915e-07
Iter: 989 loss: 3.87599243e-07
Iter: 990 loss: 3.87397591e-07
Iter: 991 loss: 3.86982322e-07
Iter: 992 loss: 3.92885681e-07
Iter: 993 loss: 3.8694526e-07
Iter: 994 loss: 3.86562533e-07
Iter: 995 loss: 3.91297419e-07
Iter: 996 loss: 3.86559577e-07
Iter: 997 loss: 3.8629625e-07
Iter: 998 loss: 3.89356813e-07
Iter: 999 loss: 3.86292214e-07
Iter: 1000 loss: 3.86153374e-07
Iter: 1001 loss: 3.85961926e-07
Iter: 1002 loss: 3.85950642e-07
Iter: 1003 loss: 3.85747398e-07
Iter: 1004 loss: 3.8845053e-07
Iter: 1005 loss: 3.85739554e-07
Iter: 1006 loss: 3.85542165e-07
Iter: 1007 loss: 3.85649e-07
Iter: 1008 loss: 3.85446015e-07
Iter: 1009 loss: 3.8524675e-07
Iter: 1010 loss: 3.85139856e-07
Iter: 1011 loss: 3.85064766e-07
Iter: 1012 loss: 3.84758351e-07
Iter: 1013 loss: 3.87165386e-07
Iter: 1014 loss: 3.8474019e-07
Iter: 1015 loss: 3.84422918e-07
Iter: 1016 loss: 3.85349409e-07
Iter: 1017 loss: 3.84327137e-07
Iter: 1018 loss: 3.8410306e-07
Iter: 1019 loss: 3.8383547e-07
Iter: 1020 loss: 3.83807929e-07
Iter: 1021 loss: 3.83488725e-07
Iter: 1022 loss: 3.88143292e-07
Iter: 1023 loss: 3.83492704e-07
Iter: 1024 loss: 3.83184783e-07
Iter: 1025 loss: 3.82980772e-07
Iter: 1026 loss: 3.82865892e-07
Iter: 1027 loss: 3.82613621e-07
Iter: 1028 loss: 3.84000344e-07
Iter: 1029 loss: 3.82581447e-07
Iter: 1030 loss: 3.82390738e-07
Iter: 1031 loss: 3.84293e-07
Iter: 1032 loss: 3.82386673e-07
Iter: 1033 loss: 3.82255962e-07
Iter: 1034 loss: 3.82085744e-07
Iter: 1035 loss: 3.8203757e-07
Iter: 1036 loss: 3.81847315e-07
Iter: 1037 loss: 3.83774761e-07
Iter: 1038 loss: 3.81845496e-07
Iter: 1039 loss: 3.81642622e-07
Iter: 1040 loss: 3.81649727e-07
Iter: 1041 loss: 3.81507107e-07
Iter: 1042 loss: 3.81209873e-07
Iter: 1043 loss: 3.81112443e-07
Iter: 1044 loss: 3.80958085e-07
Iter: 1045 loss: 3.80620236e-07
Iter: 1046 loss: 3.83271811e-07
Iter: 1047 loss: 3.80595452e-07
Iter: 1048 loss: 3.80324849e-07
Iter: 1049 loss: 3.8275391e-07
Iter: 1050 loss: 3.80312116e-07
Iter: 1051 loss: 3.80129222e-07
Iter: 1052 loss: 3.79763065e-07
Iter: 1053 loss: 3.86881368e-07
Iter: 1054 loss: 3.79757637e-07
Iter: 1055 loss: 3.79561357e-07
Iter: 1056 loss: 3.79549789e-07
Iter: 1057 loss: 3.79351974e-07
Iter: 1058 loss: 3.79640767e-07
Iter: 1059 loss: 3.79275207e-07
Iter: 1060 loss: 3.79109792e-07
Iter: 1061 loss: 3.78906549e-07
Iter: 1062 loss: 3.78889922e-07
Iter: 1063 loss: 3.78702907e-07
Iter: 1064 loss: 3.78674855e-07
Iter: 1065 loss: 3.78527687e-07
Iter: 1066 loss: 3.78403087e-07
Iter: 1067 loss: 3.78363183e-07
Iter: 1068 loss: 3.78123502e-07
Iter: 1069 loss: 3.78731158e-07
Iter: 1070 loss: 3.78040625e-07
Iter: 1071 loss: 3.77718663e-07
Iter: 1072 loss: 3.78827423e-07
Iter: 1073 loss: 3.77615095e-07
Iter: 1074 loss: 3.77381411e-07
Iter: 1075 loss: 3.77270567e-07
Iter: 1076 loss: 3.77149092e-07
Iter: 1077 loss: 3.76832077e-07
Iter: 1078 loss: 3.78184893e-07
Iter: 1079 loss: 3.76783476e-07
Iter: 1080 loss: 3.76583216e-07
Iter: 1081 loss: 3.7656929e-07
Iter: 1082 loss: 3.76405296e-07
Iter: 1083 loss: 3.76119459e-07
Iter: 1084 loss: 3.82640906e-07
Iter: 1085 loss: 3.76107693e-07
Iter: 1086 loss: 3.75847094e-07
Iter: 1087 loss: 3.77672222e-07
Iter: 1088 loss: 3.75818e-07
Iter: 1089 loss: 3.75560489e-07
Iter: 1090 loss: 3.77098615e-07
Iter: 1091 loss: 3.75533205e-07
Iter: 1092 loss: 3.75372281e-07
Iter: 1093 loss: 3.75188847e-07
Iter: 1094 loss: 3.75158237e-07
Iter: 1095 loss: 3.74898661e-07
Iter: 1096 loss: 3.77262552e-07
Iter: 1097 loss: 3.74889453e-07
Iter: 1098 loss: 3.74578121e-07
Iter: 1099 loss: 3.74722362e-07
Iter: 1100 loss: 3.74365186e-07
Iter: 1101 loss: 3.74142331e-07
Iter: 1102 loss: 3.75484206e-07
Iter: 1103 loss: 3.7413642e-07
Iter: 1104 loss: 3.73949149e-07
Iter: 1105 loss: 3.75195555e-07
Iter: 1106 loss: 3.73926781e-07
Iter: 1107 loss: 3.73790954e-07
Iter: 1108 loss: 3.73539365e-07
Iter: 1109 loss: 3.79270233e-07
Iter: 1110 loss: 3.73541724e-07
Iter: 1111 loss: 3.73269415e-07
Iter: 1112 loss: 3.74662761e-07
Iter: 1113 loss: 3.73219706e-07
Iter: 1114 loss: 3.73059e-07
Iter: 1115 loss: 3.73053297e-07
Iter: 1116 loss: 3.72944982e-07
Iter: 1117 loss: 3.72705244e-07
Iter: 1118 loss: 3.7838214e-07
Iter: 1119 loss: 3.72699333e-07
Iter: 1120 loss: 3.72425234e-07
Iter: 1121 loss: 3.72835984e-07
Iter: 1122 loss: 3.72277242e-07
Iter: 1123 loss: 3.72018576e-07
Iter: 1124 loss: 3.72021134e-07
Iter: 1125 loss: 3.71853559e-07
Iter: 1126 loss: 3.71472595e-07
Iter: 1127 loss: 3.7707548e-07
Iter: 1128 loss: 3.71469639e-07
Iter: 1129 loss: 3.71148076e-07
Iter: 1130 loss: 3.75334196e-07
Iter: 1131 loss: 3.71149952e-07
Iter: 1132 loss: 3.70852774e-07
Iter: 1133 loss: 3.726513e-07
Iter: 1134 loss: 3.70832794e-07
Iter: 1135 loss: 3.70682613e-07
Iter: 1136 loss: 3.70659961e-07
Iter: 1137 loss: 3.70553607e-07
Iter: 1138 loss: 3.70374238e-07
Iter: 1139 loss: 3.72549152e-07
Iter: 1140 loss: 3.7036807e-07
Iter: 1141 loss: 3.7024256e-07
Iter: 1142 loss: 3.70133478e-07
Iter: 1143 loss: 3.70093346e-07
Iter: 1144 loss: 3.69906587e-07
Iter: 1145 loss: 3.70068165e-07
Iter: 1146 loss: 3.69785141e-07
Iter: 1147 loss: 3.69584939e-07
Iter: 1148 loss: 3.72142267e-07
Iter: 1149 loss: 3.69585109e-07
Iter: 1150 loss: 3.69380302e-07
Iter: 1151 loss: 3.6954026e-07
Iter: 1152 loss: 3.69255559e-07
Iter: 1153 loss: 3.69085e-07
Iter: 1154 loss: 3.69009712e-07
Iter: 1155 loss: 3.6892277e-07
Iter: 1156 loss: 3.68646454e-07
Iter: 1157 loss: 3.71031433e-07
Iter: 1158 loss: 3.68630765e-07
Iter: 1159 loss: 3.6835263e-07
Iter: 1160 loss: 3.6802345e-07
Iter: 1161 loss: 3.67990168e-07
Iter: 1162 loss: 3.676966e-07
Iter: 1163 loss: 3.69715082e-07
Iter: 1164 loss: 3.67659311e-07
Iter: 1165 loss: 3.67397064e-07
Iter: 1166 loss: 3.69486088e-07
Iter: 1167 loss: 3.67392374e-07
Iter: 1168 loss: 3.6722389e-07
Iter: 1169 loss: 3.67145304e-07
Iter: 1170 loss: 3.67077803e-07
Iter: 1171 loss: 3.66910285e-07
Iter: 1172 loss: 3.6690966e-07
Iter: 1173 loss: 3.66793e-07
Iter: 1174 loss: 3.66629564e-07
Iter: 1175 loss: 3.66611573e-07
Iter: 1176 loss: 3.66392612e-07
Iter: 1177 loss: 3.66549017e-07
Iter: 1178 loss: 3.66255875e-07
Iter: 1179 loss: 3.66035067e-07
Iter: 1180 loss: 3.68603537e-07
Iter: 1181 loss: 3.66034186e-07
Iter: 1182 loss: 3.65813975e-07
Iter: 1183 loss: 3.66400826e-07
Iter: 1184 loss: 3.65737e-07
Iter: 1185 loss: 3.65549738e-07
Iter: 1186 loss: 3.65238861e-07
Iter: 1187 loss: 3.65234882e-07
Iter: 1188 loss: 3.64996339e-07
Iter: 1189 loss: 3.64983862e-07
Iter: 1190 loss: 3.64770017e-07
Iter: 1191 loss: 3.64892628e-07
Iter: 1192 loss: 3.64635071e-07
Iter: 1193 loss: 3.64453285e-07
Iter: 1194 loss: 3.64444816e-07
Iter: 1195 loss: 3.64285654e-07
Iter: 1196 loss: 3.64136554e-07
Iter: 1197 loss: 3.6411555e-07
Iter: 1198 loss: 3.63972049e-07
Iter: 1199 loss: 3.63698405e-07
Iter: 1200 loss: 3.69883253e-07
Iter: 1201 loss: 3.6369056e-07
Iter: 1202 loss: 3.63470122e-07
Iter: 1203 loss: 3.67067287e-07
Iter: 1204 loss: 3.63472338e-07
Iter: 1205 loss: 3.63259829e-07
Iter: 1206 loss: 3.63292088e-07
Iter: 1207 loss: 3.63111e-07
Iter: 1208 loss: 3.62884748e-07
Iter: 1209 loss: 3.62799199e-07
Iter: 1210 loss: 3.62667066e-07
Iter: 1211 loss: 3.62313671e-07
Iter: 1212 loss: 3.6409449e-07
Iter: 1213 loss: 3.62271777e-07
Iter: 1214 loss: 3.62031756e-07
Iter: 1215 loss: 3.62022348e-07
Iter: 1216 loss: 3.61889448e-07
Iter: 1217 loss: 3.6166054e-07
Iter: 1218 loss: 3.61642293e-07
Iter: 1219 loss: 3.61396e-07
Iter: 1220 loss: 3.62185943e-07
Iter: 1221 loss: 3.61309134e-07
Iter: 1222 loss: 3.61011814e-07
Iter: 1223 loss: 3.6324866e-07
Iter: 1224 loss: 3.61002691e-07
Iter: 1225 loss: 3.60839323e-07
Iter: 1226 loss: 3.60704917e-07
Iter: 1227 loss: 3.60680872e-07
Iter: 1228 loss: 3.60451338e-07
Iter: 1229 loss: 3.62541357e-07
Iter: 1230 loss: 3.60418824e-07
Iter: 1231 loss: 3.60165245e-07
Iter: 1232 loss: 3.60354477e-07
Iter: 1233 loss: 3.6000344e-07
Iter: 1234 loss: 3.59829301e-07
Iter: 1235 loss: 3.60954772e-07
Iter: 1236 loss: 3.5980645e-07
Iter: 1237 loss: 3.59623328e-07
Iter: 1238 loss: 3.59750516e-07
Iter: 1239 loss: 3.59474654e-07
Iter: 1240 loss: 3.59264192e-07
Iter: 1241 loss: 3.59152239e-07
Iter: 1242 loss: 3.59029968e-07
Iter: 1243 loss: 3.58698799e-07
Iter: 1244 loss: 3.59784053e-07
Iter: 1245 loss: 3.58604751e-07
Iter: 1246 loss: 3.58321671e-07
Iter: 1247 loss: 3.58332642e-07
Iter: 1248 loss: 3.58118029e-07
Iter: 1249 loss: 3.57871727e-07
Iter: 1250 loss: 3.57854503e-07
Iter: 1251 loss: 3.575839e-07
Iter: 1252 loss: 3.58644485e-07
Iter: 1253 loss: 3.57517479e-07
Iter: 1254 loss: 3.57314548e-07
Iter: 1255 loss: 3.60115052e-07
Iter: 1256 loss: 3.57325888e-07
Iter: 1257 loss: 3.57159394e-07
Iter: 1258 loss: 3.56863211e-07
Iter: 1259 loss: 3.63515539e-07
Iter: 1260 loss: 3.56860085e-07
Iter: 1261 loss: 3.56657466e-07
Iter: 1262 loss: 3.59467435e-07
Iter: 1263 loss: 3.56647405e-07
Iter: 1264 loss: 3.56463318e-07
Iter: 1265 loss: 3.57405384e-07
Iter: 1266 loss: 3.56442541e-07
Iter: 1267 loss: 3.56303588e-07
Iter: 1268 loss: 3.56177821e-07
Iter: 1269 loss: 3.56156477e-07
Iter: 1270 loss: 3.55943087e-07
Iter: 1271 loss: 3.58732365e-07
Iter: 1272 loss: 3.55932116e-07
Iter: 1273 loss: 3.55765735e-07
Iter: 1274 loss: 3.55504568e-07
Iter: 1275 loss: 3.61548928e-07
Iter: 1276 loss: 3.55500589e-07
Iter: 1277 loss: 3.55184795e-07
Iter: 1278 loss: 3.55932571e-07
Iter: 1279 loss: 3.55065254e-07
Iter: 1280 loss: 3.54822077e-07
Iter: 1281 loss: 3.5483373e-07
Iter: 1282 loss: 3.54602321e-07
Iter: 1283 loss: 3.54875397e-07
Iter: 1284 loss: 3.54488e-07
Iter: 1285 loss: 3.54285191e-07
Iter: 1286 loss: 3.54115741e-07
Iter: 1287 loss: 3.54067794e-07
Iter: 1288 loss: 3.53898798e-07
Iter: 1289 loss: 3.53880552e-07
Iter: 1290 loss: 3.53734748e-07
Iter: 1291 loss: 3.53687483e-07
Iter: 1292 loss: 3.53601223e-07
Iter: 1293 loss: 3.53395137e-07
Iter: 1294 loss: 3.53177626e-07
Iter: 1295 loss: 3.53146106e-07
Iter: 1296 loss: 3.53085511e-07
Iter: 1297 loss: 3.52981317e-07
Iter: 1298 loss: 3.52841141e-07
Iter: 1299 loss: 3.52544077e-07
Iter: 1300 loss: 3.56268174e-07
Iter: 1301 loss: 3.52499399e-07
Iter: 1302 loss: 3.52320967e-07
Iter: 1303 loss: 3.52300106e-07
Iter: 1304 loss: 3.52111016e-07
Iter: 1305 loss: 3.52235872e-07
Iter: 1306 loss: 3.51982e-07
Iter: 1307 loss: 3.51781239e-07
Iter: 1308 loss: 3.51583736e-07
Iter: 1309 loss: 3.51514956e-07
Iter: 1310 loss: 3.51356221e-07
Iter: 1311 loss: 3.54344081e-07
Iter: 1312 loss: 3.51342351e-07
Iter: 1313 loss: 3.51184894e-07
Iter: 1314 loss: 3.52133952e-07
Iter: 1315 loss: 3.51178358e-07
Iter: 1316 loss: 3.51052648e-07
Iter: 1317 loss: 3.5088e-07
Iter: 1318 loss: 3.50863957e-07
Iter: 1319 loss: 3.50574226e-07
Iter: 1320 loss: 3.50798416e-07
Iter: 1321 loss: 3.5040614e-07
Iter: 1322 loss: 3.50191328e-07
Iter: 1323 loss: 3.50182546e-07
Iter: 1324 loss: 3.50037396e-07
Iter: 1325 loss: 3.49708642e-07
Iter: 1326 loss: 3.53707037e-07
Iter: 1327 loss: 3.49692925e-07
Iter: 1328 loss: 3.49299285e-07
Iter: 1329 loss: 3.50823143e-07
Iter: 1330 loss: 3.49203958e-07
Iter: 1331 loss: 3.49052243e-07
Iter: 1332 loss: 3.4902996e-07
Iter: 1333 loss: 3.48886459e-07
Iter: 1334 loss: 3.48652122e-07
Iter: 1335 loss: 3.48652947e-07
Iter: 1336 loss: 3.4851189e-07
Iter: 1337 loss: 3.48518796e-07
Iter: 1338 loss: 3.48366598e-07
Iter: 1339 loss: 3.48261267e-07
Iter: 1340 loss: 3.48234e-07
Iter: 1341 loss: 3.48061917e-07
Iter: 1342 loss: 3.48010246e-07
Iter: 1343 loss: 3.47926061e-07
Iter: 1344 loss: 3.47732282e-07
Iter: 1345 loss: 3.50938421e-07
Iter: 1346 loss: 3.47727564e-07
Iter: 1347 loss: 3.47590031e-07
Iter: 1348 loss: 3.47714746e-07
Iter: 1349 loss: 3.47487116e-07
Iter: 1350 loss: 3.47326477e-07
Iter: 1351 loss: 3.47148671e-07
Iter: 1352 loss: 3.47125194e-07
Iter: 1353 loss: 3.46859537e-07
Iter: 1354 loss: 3.48975163e-07
Iter: 1355 loss: 3.46833247e-07
Iter: 1356 loss: 3.46637876e-07
Iter: 1357 loss: 3.48812137e-07
Iter: 1358 loss: 3.46641656e-07
Iter: 1359 loss: 3.46525894e-07
Iter: 1360 loss: 3.46251284e-07
Iter: 1361 loss: 3.49063129e-07
Iter: 1362 loss: 3.46242359e-07
Iter: 1363 loss: 3.45959961e-07
Iter: 1364 loss: 3.49306418e-07
Iter: 1365 loss: 3.45950497e-07
Iter: 1366 loss: 3.4569581e-07
Iter: 1367 loss: 3.47468614e-07
Iter: 1368 loss: 3.45678131e-07
Iter: 1369 loss: 3.45547335e-07
Iter: 1370 loss: 3.45342613e-07
Iter: 1371 loss: 3.453452e-07
Iter: 1372 loss: 3.45161538e-07
Iter: 1373 loss: 3.45172054e-07
Iter: 1374 loss: 3.45041684e-07
Iter: 1375 loss: 3.44867345e-07
Iter: 1376 loss: 3.44849468e-07
Iter: 1377 loss: 3.44638295e-07
Iter: 1378 loss: 3.44860496e-07
Iter: 1379 loss: 3.44512e-07
Iter: 1380 loss: 3.44409557e-07
Iter: 1381 loss: 3.44369084e-07
Iter: 1382 loss: 3.44293483e-07
Iter: 1383 loss: 3.4408572e-07
Iter: 1384 loss: 3.454183e-07
Iter: 1385 loss: 3.44024329e-07
Iter: 1386 loss: 3.4375384e-07
Iter: 1387 loss: 3.45296598e-07
Iter: 1388 loss: 3.43722178e-07
Iter: 1389 loss: 3.43528342e-07
Iter: 1390 loss: 3.45182315e-07
Iter: 1391 loss: 3.4352837e-07
Iter: 1392 loss: 3.43328708e-07
Iter: 1393 loss: 3.43429207e-07
Iter: 1394 loss: 3.43205699e-07
Iter: 1395 loss: 3.43000352e-07
Iter: 1396 loss: 3.428284e-07
Iter: 1397 loss: 3.42779401e-07
Iter: 1398 loss: 3.42699025e-07
Iter: 1399 loss: 3.42616715e-07
Iter: 1400 loss: 3.42517836e-07
Iter: 1401 loss: 3.42424471e-07
Iter: 1402 loss: 3.42382236e-07
Iter: 1403 loss: 3.42211166e-07
Iter: 1404 loss: 3.42127436e-07
Iter: 1405 loss: 3.42051123e-07
Iter: 1406 loss: 3.41861835e-07
Iter: 1407 loss: 3.41835062e-07
Iter: 1408 loss: 3.41723251e-07
Iter: 1409 loss: 3.41438295e-07
Iter: 1410 loss: 3.45280483e-07
Iter: 1411 loss: 3.41429541e-07
Iter: 1412 loss: 3.41128555e-07
Iter: 1413 loss: 3.41194152e-07
Iter: 1414 loss: 3.40904791e-07
Iter: 1415 loss: 3.40979085e-07
Iter: 1416 loss: 3.40764643e-07
Iter: 1417 loss: 3.40631715e-07
Iter: 1418 loss: 3.40388738e-07
Iter: 1419 loss: 3.40397321e-07
Iter: 1420 loss: 3.40180293e-07
Iter: 1421 loss: 3.40918575e-07
Iter: 1422 loss: 3.4010742e-07
Iter: 1423 loss: 3.39984922e-07
Iter: 1424 loss: 3.39992624e-07
Iter: 1425 loss: 3.39874816e-07
Iter: 1426 loss: 3.39654036e-07
Iter: 1427 loss: 3.43441911e-07
Iter: 1428 loss: 3.39637154e-07
Iter: 1429 loss: 3.39354898e-07
Iter: 1430 loss: 3.39924497e-07
Iter: 1431 loss: 3.39251187e-07
Iter: 1432 loss: 3.39038877e-07
Iter: 1433 loss: 3.39045812e-07
Iter: 1434 loss: 3.38851351e-07
Iter: 1435 loss: 3.38925418e-07
Iter: 1436 loss: 3.38709015e-07
Iter: 1437 loss: 3.38473541e-07
Iter: 1438 loss: 3.3825421e-07
Iter: 1439 loss: 3.38207087e-07
Iter: 1440 loss: 3.38170707e-07
Iter: 1441 loss: 3.38056395e-07
Iter: 1442 loss: 3.37931795e-07
Iter: 1443 loss: 3.37655052e-07
Iter: 1444 loss: 3.39704428e-07
Iter: 1445 loss: 3.37603211e-07
Iter: 1446 loss: 3.37305323e-07
Iter: 1447 loss: 3.39223305e-07
Iter: 1448 loss: 3.37280028e-07
Iter: 1449 loss: 3.37146332e-07
Iter: 1450 loss: 3.3712098e-07
Iter: 1451 loss: 3.37026648e-07
Iter: 1452 loss: 3.3687067e-07
Iter: 1453 loss: 3.36864815e-07
Iter: 1454 loss: 3.36702271e-07
Iter: 1455 loss: 3.36904918e-07
Iter: 1456 loss: 3.36614391e-07
Iter: 1457 loss: 3.36417258e-07
Iter: 1458 loss: 3.38987775e-07
Iter: 1459 loss: 3.36417202e-07
Iter: 1460 loss: 3.36295216e-07
Iter: 1461 loss: 3.36047634e-07
Iter: 1462 loss: 3.4129917e-07
Iter: 1463 loss: 3.36039136e-07
Iter: 1464 loss: 3.35746904e-07
Iter: 1465 loss: 3.36539898e-07
Iter: 1466 loss: 3.35640152e-07
Iter: 1467 loss: 3.35492018e-07
Iter: 1468 loss: 3.35472862e-07
Iter: 1469 loss: 3.35318617e-07
Iter: 1470 loss: 3.35194045e-07
Iter: 1471 loss: 3.35134985e-07
Iter: 1472 loss: 3.34976932e-07
Iter: 1473 loss: 3.35515608e-07
Iter: 1474 loss: 3.34937738e-07
Iter: 1475 loss: 3.34788922e-07
Iter: 1476 loss: 3.36402593e-07
Iter: 1477 loss: 3.34796681e-07
Iter: 1478 loss: 3.3466884e-07
Iter: 1479 loss: 3.34459543e-07
Iter: 1480 loss: 3.34456075e-07
Iter: 1481 loss: 3.3422404e-07
Iter: 1482 loss: 3.34581983e-07
Iter: 1483 loss: 3.34113565e-07
Iter: 1484 loss: 3.33985412e-07
Iter: 1485 loss: 3.33967108e-07
Iter: 1486 loss: 3.33808259e-07
Iter: 1487 loss: 3.33445342e-07
Iter: 1488 loss: 3.36935386e-07
Iter: 1489 loss: 3.3340018e-07
Iter: 1490 loss: 3.33108375e-07
Iter: 1491 loss: 3.34739e-07
Iter: 1492 loss: 3.33077935e-07
Iter: 1493 loss: 3.32892e-07
Iter: 1494 loss: 3.35303781e-07
Iter: 1495 loss: 3.32873242e-07
Iter: 1496 loss: 3.32699983e-07
Iter: 1497 loss: 3.33049229e-07
Iter: 1498 loss: 3.32611364e-07
Iter: 1499 loss: 3.32515526e-07
Iter: 1500 loss: 3.3235608e-07
Iter: 1501 loss: 3.32343689e-07
Iter: 1502 loss: 3.32216189e-07
Iter: 1503 loss: 3.32203456e-07
Iter: 1504 loss: 3.32046312e-07
Iter: 1505 loss: 3.32033267e-07
Iter: 1506 loss: 3.31913384e-07
Iter: 1507 loss: 3.31768092e-07
Iter: 1508 loss: 3.31729382e-07
Iter: 1509 loss: 3.31639626e-07
Iter: 1510 loss: 3.31449598e-07
Iter: 1511 loss: 3.34357566e-07
Iter: 1512 loss: 3.31443744e-07
Iter: 1513 loss: 3.31321928e-07
Iter: 1514 loss: 3.31072329e-07
Iter: 1515 loss: 3.35909874e-07
Iter: 1516 loss: 3.31064314e-07
Iter: 1517 loss: 3.30752897e-07
Iter: 1518 loss: 3.31115274e-07
Iter: 1519 loss: 3.30579326e-07
Iter: 1520 loss: 3.30395125e-07
Iter: 1521 loss: 3.30383102e-07
Iter: 1522 loss: 3.30214505e-07
Iter: 1523 loss: 3.30912258e-07
Iter: 1524 loss: 3.30183838e-07
Iter: 1525 loss: 3.30073249e-07
Iter: 1526 loss: 3.29952513e-07
Iter: 1527 loss: 3.29935062e-07
Iter: 1528 loss: 3.29712606e-07
Iter: 1529 loss: 3.29916247e-07
Iter: 1530 loss: 3.29582093e-07
Iter: 1531 loss: 3.29404713e-07
Iter: 1532 loss: 3.31332558e-07
Iter: 1533 loss: 3.29404173e-07
Iter: 1534 loss: 3.29194506e-07
Iter: 1535 loss: 3.29628392e-07
Iter: 1536 loss: 3.29093552e-07
Iter: 1537 loss: 3.28932884e-07
Iter: 1538 loss: 3.28725946e-07
Iter: 1539 loss: 3.28718755e-07
Iter: 1540 loss: 3.28529666e-07
Iter: 1541 loss: 3.28529211e-07
Iter: 1542 loss: 3.28330941e-07
Iter: 1543 loss: 3.28479899e-07
Iter: 1544 loss: 3.28218107e-07
Iter: 1545 loss: 3.28037203e-07
Iter: 1546 loss: 3.28166038e-07
Iter: 1547 loss: 3.27940825e-07
Iter: 1548 loss: 3.27776576e-07
Iter: 1549 loss: 3.27785358e-07
Iter: 1550 loss: 3.27704697e-07
Iter: 1551 loss: 3.27483207e-07
Iter: 1552 loss: 3.29663521e-07
Iter: 1553 loss: 3.27450437e-07
Iter: 1554 loss: 3.27315718e-07
Iter: 1555 loss: 3.27310147e-07
Iter: 1556 loss: 3.27121256e-07
Iter: 1557 loss: 3.27095506e-07
Iter: 1558 loss: 3.26964539e-07
Iter: 1559 loss: 3.26748477e-07
Iter: 1560 loss: 3.26641043e-07
Iter: 1561 loss: 3.26557483e-07
Iter: 1562 loss: 3.26310214e-07
Iter: 1563 loss: 3.2749557e-07
Iter: 1564 loss: 3.26251836e-07
Iter: 1565 loss: 3.26041544e-07
Iter: 1566 loss: 3.29080308e-07
Iter: 1567 loss: 3.2604629e-07
Iter: 1568 loss: 3.25845633e-07
Iter: 1569 loss: 3.25969069e-07
Iter: 1570 loss: 3.25731207e-07
Iter: 1571 loss: 3.25627212e-07
Iter: 1572 loss: 3.25837306e-07
Iter: 1573 loss: 3.25571477e-07
Iter: 1574 loss: 3.254533e-07
Iter: 1575 loss: 3.27016778e-07
Iter: 1576 loss: 3.25448354e-07
Iter: 1577 loss: 3.25338505e-07
Iter: 1578 loss: 3.25292319e-07
Iter: 1579 loss: 3.25254746e-07
Iter: 1580 loss: 3.25146146e-07
Iter: 1581 loss: 3.25517249e-07
Iter: 1582 loss: 3.25106555e-07
Iter: 1583 loss: 3.24952111e-07
Iter: 1584 loss: 3.25173374e-07
Iter: 1585 loss: 3.24875742e-07
Iter: 1586 loss: 3.24713653e-07
Iter: 1587 loss: 3.2444882e-07
Iter: 1588 loss: 3.24444983e-07
Iter: 1589 loss: 3.24429152e-07
Iter: 1590 loss: 3.24293893e-07
Iter: 1591 loss: 3.24203086e-07
Iter: 1592 loss: 3.23911536e-07
Iter: 1593 loss: 3.27793344e-07
Iter: 1594 loss: 3.23885956e-07
Iter: 1595 loss: 3.23626779e-07
Iter: 1596 loss: 3.24094259e-07
Iter: 1597 loss: 3.23496863e-07
Iter: 1598 loss: 3.23275174e-07
Iter: 1599 loss: 3.25253552e-07
Iter: 1600 loss: 3.23255449e-07
Iter: 1601 loss: 3.23152022e-07
Iter: 1602 loss: 3.2314864e-07
Iter: 1603 loss: 3.23036033e-07
Iter: 1604 loss: 3.22841345e-07
Iter: 1605 loss: 3.25781286e-07
Iter: 1606 loss: 3.22817016e-07
Iter: 1607 loss: 3.22688066e-07
Iter: 1608 loss: 3.22690568e-07
Iter: 1609 loss: 3.22562727e-07
Iter: 1610 loss: 3.22494543e-07
Iter: 1611 loss: 3.22426e-07
Iter: 1612 loss: 3.22241078e-07
Iter: 1613 loss: 3.22844812e-07
Iter: 1614 loss: 3.22192619e-07
Iter: 1615 loss: 3.22023823e-07
Iter: 1616 loss: 3.23354783e-07
Iter: 1617 loss: 3.22015069e-07
Iter: 1618 loss: 3.2187387e-07
Iter: 1619 loss: 3.21650333e-07
Iter: 1620 loss: 3.21642631e-07
Iter: 1621 loss: 3.21436e-07
Iter: 1622 loss: 3.22285359e-07
Iter: 1623 loss: 3.21384931e-07
Iter: 1624 loss: 3.212092e-07
Iter: 1625 loss: 3.23604752e-07
Iter: 1626 loss: 3.21206187e-07
Iter: 1627 loss: 3.21086418e-07
Iter: 1628 loss: 3.20825109e-07
Iter: 1629 loss: 3.2471803e-07
Iter: 1630 loss: 3.20809e-07
Iter: 1631 loss: 3.20570138e-07
Iter: 1632 loss: 3.21536788e-07
Iter: 1633 loss: 3.20524975e-07
Iter: 1634 loss: 3.20316275e-07
Iter: 1635 loss: 3.20885505e-07
Iter: 1636 loss: 3.20225126e-07
Iter: 1637 loss: 3.19967171e-07
Iter: 1638 loss: 3.22362496e-07
Iter: 1639 loss: 3.19946395e-07
Iter: 1640 loss: 3.19844446e-07
Iter: 1641 loss: 3.19702593e-07
Iter: 1642 loss: 3.19678804e-07
Iter: 1643 loss: 3.19572109e-07
Iter: 1644 loss: 3.19548889e-07
Iter: 1645 loss: 3.19479426e-07
Iter: 1646 loss: 3.19342348e-07
Iter: 1647 loss: 3.2260985e-07
Iter: 1648 loss: 3.19352125e-07
Iter: 1649 loss: 3.19201149e-07
Iter: 1650 loss: 3.20717049e-07
Iter: 1651 loss: 3.19198591e-07
Iter: 1652 loss: 3.19057676e-07
Iter: 1653 loss: 3.19038463e-07
Iter: 1654 loss: 3.18965732e-07
Iter: 1655 loss: 3.18748022e-07
Iter: 1656 loss: 3.19007654e-07
Iter: 1657 loss: 3.18641241e-07
Iter: 1658 loss: 3.18533836e-07
Iter: 1659 loss: 3.18506e-07
Iter: 1660 loss: 3.18405029e-07
Iter: 1661 loss: 3.18328318e-07
Iter: 1662 loss: 3.18311379e-07
Iter: 1663 loss: 3.18128855e-07
Iter: 1664 loss: 3.17934649e-07
Iter: 1665 loss: 3.17914782e-07
Iter: 1666 loss: 3.17636648e-07
Iter: 1667 loss: 3.18291e-07
Iter: 1668 loss: 3.17522392e-07
Iter: 1669 loss: 3.17417175e-07
Iter: 1670 loss: 3.17373861e-07
Iter: 1671 loss: 3.17206286e-07
Iter: 1672 loss: 3.16908711e-07
Iter: 1673 loss: 3.23065308e-07
Iter: 1674 loss: 3.16906608e-07
Iter: 1675 loss: 3.16721582e-07
Iter: 1676 loss: 3.16728489e-07
Iter: 1677 loss: 3.16532294e-07
Iter: 1678 loss: 3.16707911e-07
Iter: 1679 loss: 3.16408517e-07
Iter: 1680 loss: 3.16278772e-07
Iter: 1681 loss: 3.16752846e-07
Iter: 1682 loss: 3.16256774e-07
Iter: 1683 loss: 3.16124726e-07
Iter: 1684 loss: 3.16864941e-07
Iter: 1685 loss: 3.16093548e-07
Iter: 1686 loss: 3.16003025e-07
Iter: 1687 loss: 3.15912132e-07
Iter: 1688 loss: 3.1590514e-07
Iter: 1689 loss: 3.15757489e-07
Iter: 1690 loss: 3.16424945e-07
Iter: 1691 loss: 3.15742909e-07
Iter: 1692 loss: 3.15591194e-07
Iter: 1693 loss: 3.16610453e-07
Iter: 1694 loss: 3.15568229e-07
Iter: 1695 loss: 3.15464035e-07
Iter: 1696 loss: 3.15266107e-07
Iter: 1697 loss: 3.19849676e-07
Iter: 1698 loss: 3.15261076e-07
Iter: 1699 loss: 3.15044019e-07
Iter: 1700 loss: 3.15460397e-07
Iter: 1701 loss: 3.14950057e-07
Iter: 1702 loss: 3.14714384e-07
Iter: 1703 loss: 3.15301122e-07
Iter: 1704 loss: 3.1464694e-07
Iter: 1705 loss: 3.14418e-07
Iter: 1706 loss: 3.14420873e-07
Iter: 1707 loss: 3.14308295e-07
Iter: 1708 loss: 3.14077568e-07
Iter: 1709 loss: 3.17237379e-07
Iter: 1710 loss: 3.14058042e-07
Iter: 1711 loss: 3.13965927e-07
Iter: 1712 loss: 3.13908e-07
Iter: 1713 loss: 3.13811711e-07
Iter: 1714 loss: 3.13684978e-07
Iter: 1715 loss: 3.13684581e-07
Iter: 1716 loss: 3.13562e-07
Iter: 1717 loss: 3.15116893e-07
Iter: 1718 loss: 3.13554921e-07
Iter: 1719 loss: 3.13449334e-07
Iter: 1720 loss: 3.1331416e-07
Iter: 1721 loss: 3.13301712e-07
Iter: 1722 loss: 3.13110746e-07
Iter: 1723 loss: 3.13410112e-07
Iter: 1724 loss: 3.1302983e-07
Iter: 1725 loss: 3.13003767e-07
Iter: 1726 loss: 3.12944e-07
Iter: 1727 loss: 3.12864984e-07
Iter: 1728 loss: 3.12685501e-07
Iter: 1729 loss: 3.15069485e-07
Iter: 1730 loss: 3.12665e-07
Iter: 1731 loss: 3.12471315e-07
Iter: 1732 loss: 3.12788757e-07
Iter: 1733 loss: 3.12347964e-07
Iter: 1734 loss: 3.12138724e-07
Iter: 1735 loss: 3.12610837e-07
Iter: 1736 loss: 3.12056272e-07
Iter: 1737 loss: 3.11934969e-07
Iter: 1738 loss: 3.11924282e-07
Iter: 1739 loss: 3.11808236e-07
Iter: 1740 loss: 3.11580095e-07
Iter: 1741 loss: 3.1565375e-07
Iter: 1742 loss: 3.11569607e-07
Iter: 1743 loss: 3.11472121e-07
Iter: 1744 loss: 3.114572e-07
Iter: 1745 loss: 3.11328051e-07
Iter: 1746 loss: 3.11280246e-07
Iter: 1747 loss: 3.11214251e-07
Iter: 1748 loss: 3.11062593e-07
Iter: 1749 loss: 3.11086211e-07
Iter: 1750 loss: 3.10944529e-07
Iter: 1751 loss: 3.10759731e-07
Iter: 1752 loss: 3.10762744e-07
Iter: 1753 loss: 3.1066304e-07
Iter: 1754 loss: 3.10416e-07
Iter: 1755 loss: 3.12784891e-07
Iter: 1756 loss: 3.10380386e-07
Iter: 1757 loss: 3.10136471e-07
Iter: 1758 loss: 3.12350835e-07
Iter: 1759 loss: 3.10130019e-07
Iter: 1760 loss: 3.09982454e-07
Iter: 1761 loss: 3.09977338e-07
Iter: 1762 loss: 3.09886047e-07
Iter: 1763 loss: 3.09779864e-07
Iter: 1764 loss: 3.0976139e-07
Iter: 1765 loss: 3.09614592e-07
Iter: 1766 loss: 3.09607174e-07
Iter: 1767 loss: 3.09485074e-07
Iter: 1768 loss: 3.09323809e-07
Iter: 1769 loss: 3.11221328e-07
Iter: 1770 loss: 3.09318523e-07
Iter: 1771 loss: 3.0915686e-07
Iter: 1772 loss: 3.09705598e-07
Iter: 1773 loss: 3.09111499e-07
Iter: 1774 loss: 3.09030696e-07
Iter: 1775 loss: 3.08936848e-07
Iter: 1776 loss: 3.08895096e-07
Iter: 1777 loss: 3.08720217e-07
Iter: 1778 loss: 3.10789176e-07
Iter: 1779 loss: 3.08719962e-07
Iter: 1780 loss: 3.08582088e-07
Iter: 1781 loss: 3.08373444e-07
Iter: 1782 loss: 3.08375377e-07
Iter: 1783 loss: 3.0823719e-07
Iter: 1784 loss: 3.08240146e-07
Iter: 1785 loss: 3.08087181e-07
Iter: 1786 loss: 3.07821381e-07
Iter: 1787 loss: 3.07797961e-07
Iter: 1788 loss: 3.07599123e-07
Iter: 1789 loss: 3.07972385e-07
Iter: 1790 loss: 3.07488591e-07
Iter: 1791 loss: 3.07377491e-07
Iter: 1792 loss: 3.07384568e-07
Iter: 1793 loss: 3.07256926e-07
Iter: 1794 loss: 3.0718968e-07
Iter: 1795 loss: 3.07147275e-07
Iter: 1796 loss: 3.06994792e-07
Iter: 1797 loss: 3.07179448e-07
Iter: 1798 loss: 3.06914274e-07
Iter: 1799 loss: 3.06780635e-07
Iter: 1800 loss: 3.0671174e-07
Iter: 1801 loss: 3.06635798e-07
Iter: 1802 loss: 3.06521571e-07
Iter: 1803 loss: 3.06514295e-07
Iter: 1804 loss: 3.06378382e-07
Iter: 1805 loss: 3.06442757e-07
Iter: 1806 loss: 3.06280697e-07
Iter: 1807 loss: 3.06130858e-07
Iter: 1808 loss: 3.0621726e-07
Iter: 1809 loss: 3.0603718e-07
Iter: 1810 loss: 3.05945832e-07
Iter: 1811 loss: 3.05940063e-07
Iter: 1812 loss: 3.05857583e-07
Iter: 1813 loss: 3.05700496e-07
Iter: 1814 loss: 3.09004093e-07
Iter: 1815 loss: 3.0570061e-07
Iter: 1816 loss: 3.05569358e-07
Iter: 1817 loss: 3.0698618e-07
Iter: 1818 loss: 3.05557421e-07
Iter: 1819 loss: 3.05412527e-07
Iter: 1820 loss: 3.05335959e-07
Iter: 1821 loss: 3.05267235e-07
Iter: 1822 loss: 3.0512075e-07
Iter: 1823 loss: 3.05167276e-07
Iter: 1824 loss: 3.05003084e-07
Iter: 1825 loss: 3.04897014e-07
Iter: 1826 loss: 3.04870099e-07
Iter: 1827 loss: 3.04781167e-07
Iter: 1828 loss: 3.04567664e-07
Iter: 1829 loss: 3.09147651e-07
Iter: 1830 loss: 3.04564196e-07
Iter: 1831 loss: 3.04354103e-07
Iter: 1832 loss: 3.04827438e-07
Iter: 1833 loss: 3.0427114e-07
Iter: 1834 loss: 3.04098876e-07
Iter: 1835 loss: 3.04296123e-07
Iter: 1836 loss: 3.040048e-07
Iter: 1837 loss: 3.03853056e-07
Iter: 1838 loss: 3.05285539e-07
Iter: 1839 loss: 3.03833872e-07
Iter: 1840 loss: 3.03724619e-07
Iter: 1841 loss: 3.0505214e-07
Iter: 1842 loss: 3.03722118e-07
Iter: 1843 loss: 3.03638103e-07
Iter: 1844 loss: 3.03516288e-07
Iter: 1845 loss: 3.03504066e-07
Iter: 1846 loss: 3.03417238e-07
Iter: 1847 loss: 3.03420194e-07
Iter: 1848 loss: 3.03299345e-07
Iter: 1849 loss: 3.03069726e-07
Iter: 1850 loss: 3.07001301e-07
Iter: 1851 loss: 3.03058243e-07
Iter: 1852 loss: 3.02889902e-07
Iter: 1853 loss: 3.041593e-07
Iter: 1854 loss: 3.02868216e-07
Iter: 1855 loss: 3.02721077e-07
Iter: 1856 loss: 3.03822617e-07
Iter: 1857 loss: 3.02709509e-07
Iter: 1858 loss: 3.02593747e-07
Iter: 1859 loss: 3.02360604e-07
Iter: 1860 loss: 3.06762246e-07
Iter: 1861 loss: 3.02366544e-07
Iter: 1862 loss: 3.02149374e-07
Iter: 1863 loss: 3.03651689e-07
Iter: 1864 loss: 3.02130616e-07
Iter: 1865 loss: 3.02020737e-07
Iter: 1866 loss: 3.02018293e-07
Iter: 1867 loss: 3.01939565e-07
Iter: 1868 loss: 3.01753403e-07
Iter: 1869 loss: 3.04589634e-07
Iter: 1870 loss: 3.01746127e-07
Iter: 1871 loss: 3.01599755e-07
Iter: 1872 loss: 3.0264323e-07
Iter: 1873 loss: 3.01580911e-07
Iter: 1874 loss: 3.0146208e-07
Iter: 1875 loss: 3.01764373e-07
Iter: 1876 loss: 3.0140103e-07
Iter: 1877 loss: 3.01302293e-07
Iter: 1878 loss: 3.02610601e-07
Iter: 1879 loss: 3.01298826e-07
Iter: 1880 loss: 3.01232035e-07
Iter: 1881 loss: 3.01203443e-07
Iter: 1882 loss: 3.01154955e-07
Iter: 1883 loss: 3.01071793e-07
Iter: 1884 loss: 3.0207687e-07
Iter: 1885 loss: 3.01072589e-07
Iter: 1886 loss: 3.00996589e-07
Iter: 1887 loss: 3.00876422e-07
Iter: 1888 loss: 3.00871449e-07
Iter: 1889 loss: 3.00723229e-07
Iter: 1890 loss: 3.00659849e-07
Iter: 1891 loss: 3.00596895e-07
Iter: 1892 loss: 3.00612641e-07
Iter: 1893 loss: 3.00523169e-07
Iter: 1894 loss: 3.0046138e-07
Iter: 1895 loss: 3.00265697e-07
Iter: 1896 loss: 3.009431e-07
Iter: 1897 loss: 3.0015957e-07
Iter: 1898 loss: 2.99988244e-07
Iter: 1899 loss: 3.01481805e-07
Iter: 1900 loss: 2.99961584e-07
Iter: 1901 loss: 2.99819789e-07
Iter: 1902 loss: 3.00417724e-07
Iter: 1903 loss: 2.99786393e-07
Iter: 1904 loss: 2.99717982e-07
Iter: 1905 loss: 2.99700162e-07
Iter: 1906 loss: 2.99628766e-07
Iter: 1907 loss: 2.99507633e-07
Iter: 1908 loss: 3.02252886e-07
Iter: 1909 loss: 2.99501636e-07
Iter: 1910 loss: 2.99386954e-07
Iter: 1911 loss: 3.00310802e-07
Iter: 1912 loss: 2.99381838e-07
Iter: 1913 loss: 2.99268436e-07
Iter: 1914 loss: 2.9951741e-07
Iter: 1915 loss: 2.99219835e-07
Iter: 1916 loss: 2.99122917e-07
Iter: 1917 loss: 2.99062265e-07
Iter: 1918 loss: 2.99026851e-07
Iter: 1919 loss: 2.98869679e-07
Iter: 1920 loss: 2.99996373e-07
Iter: 1921 loss: 2.98846231e-07
Iter: 1922 loss: 2.98687894e-07
Iter: 1923 loss: 2.99527358e-07
Iter: 1924 loss: 2.98680504e-07
Iter: 1925 loss: 2.98603197e-07
Iter: 1926 loss: 2.98437101e-07
Iter: 1927 loss: 3.01316589e-07
Iter: 1928 loss: 2.98436248e-07
Iter: 1929 loss: 2.9835121e-07
Iter: 1930 loss: 2.98340922e-07
Iter: 1931 loss: 2.98249887e-07
Iter: 1932 loss: 2.98204014e-07
Iter: 1933 loss: 2.98166412e-07
Iter: 1934 loss: 2.98047212e-07
Iter: 1935 loss: 2.9804778e-07
Iter: 1936 loss: 2.97947935e-07
Iter: 1937 loss: 2.97797186e-07
Iter: 1938 loss: 2.97788745e-07
Iter: 1939 loss: 2.97665508e-07
Iter: 1940 loss: 2.97582204e-07
Iter: 1941 loss: 2.9755256e-07
Iter: 1942 loss: 2.97430944e-07
Iter: 1943 loss: 2.97315893e-07
Iter: 1944 loss: 2.97293838e-07
Iter: 1945 loss: 2.97152411e-07
Iter: 1946 loss: 2.97247055e-07
Iter: 1947 loss: 2.97059756e-07
Iter: 1948 loss: 2.96964487e-07
Iter: 1949 loss: 2.96960877e-07
Iter: 1950 loss: 2.96869672e-07
Iter: 1951 loss: 2.96800749e-07
Iter: 1952 loss: 2.96788983e-07
Iter: 1953 loss: 2.96713665e-07
Iter: 1954 loss: 2.9724643e-07
Iter: 1955 loss: 2.9670673e-07
Iter: 1956 loss: 2.96626695e-07
Iter: 1957 loss: 2.96888061e-07
Iter: 1958 loss: 2.96609244e-07
Iter: 1959 loss: 2.96545522e-07
Iter: 1960 loss: 2.96401197e-07
Iter: 1961 loss: 2.98372555e-07
Iter: 1962 loss: 2.96395967e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2
+ date
Mon Oct 26 12:10:59 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.2_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.2_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f42c2158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f427d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f427dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f43607b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f4360268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f4209ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f4209840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f4198598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f41980d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f41faa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f4134950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f41358c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f4135a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f40c5d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f40c5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f40a8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f40a8158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29f40a89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e064e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e0667158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e0667730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e062bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e05ec510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e06066a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e0606620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e059c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e0553620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e0534620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e053eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e04ca8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e04d1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e048c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e048cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e0466510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e047db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29e04282f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 8.19197703e-06
Iter: 2 loss: 6.39308746e-06
Iter: 3 loss: 6.3419825e-06
Iter: 4 loss: 5.72840827e-06
Iter: 5 loss: 7.09963933e-06
Iter: 6 loss: 5.49563038e-06
Iter: 7 loss: 5.1975544e-06
Iter: 8 loss: 6.06973117e-06
Iter: 9 loss: 5.1056777e-06
Iter: 10 loss: 4.75627439e-06
Iter: 11 loss: 5.81550921e-06
Iter: 12 loss: 4.65175162e-06
Iter: 13 loss: 4.51608139e-06
Iter: 14 loss: 5.65473783e-06
Iter: 15 loss: 4.50812149e-06
Iter: 16 loss: 4.39039241e-06
Iter: 17 loss: 4.47379e-06
Iter: 18 loss: 4.31729586e-06
Iter: 19 loss: 4.22318908e-06
Iter: 20 loss: 4.13113912e-06
Iter: 21 loss: 4.11098608e-06
Iter: 22 loss: 3.93348455e-06
Iter: 23 loss: 5.27375369e-06
Iter: 24 loss: 3.91973026e-06
Iter: 25 loss: 3.76939533e-06
Iter: 26 loss: 4.31166973e-06
Iter: 27 loss: 3.73144871e-06
Iter: 28 loss: 3.60597846e-06
Iter: 29 loss: 3.40516885e-06
Iter: 30 loss: 3.40359406e-06
Iter: 31 loss: 3.18864659e-06
Iter: 32 loss: 3.75561922e-06
Iter: 33 loss: 3.11654912e-06
Iter: 34 loss: 2.97541828e-06
Iter: 35 loss: 2.97514543e-06
Iter: 36 loss: 2.89807213e-06
Iter: 37 loss: 2.85441092e-06
Iter: 38 loss: 2.82123938e-06
Iter: 39 loss: 2.7903659e-06
Iter: 40 loss: 2.76632818e-06
Iter: 41 loss: 2.71384988e-06
Iter: 42 loss: 2.73544492e-06
Iter: 43 loss: 2.67776932e-06
Iter: 44 loss: 2.63629317e-06
Iter: 45 loss: 2.78591938e-06
Iter: 46 loss: 2.62574713e-06
Iter: 47 loss: 2.5636582e-06
Iter: 48 loss: 2.54374845e-06
Iter: 49 loss: 2.50737799e-06
Iter: 50 loss: 2.45507181e-06
Iter: 51 loss: 2.94418714e-06
Iter: 52 loss: 2.45286265e-06
Iter: 53 loss: 2.40538679e-06
Iter: 54 loss: 2.44060425e-06
Iter: 55 loss: 2.37624317e-06
Iter: 56 loss: 2.31561489e-06
Iter: 57 loss: 2.22089511e-06
Iter: 58 loss: 2.21976052e-06
Iter: 59 loss: 2.21193409e-06
Iter: 60 loss: 2.18516129e-06
Iter: 61 loss: 2.15195246e-06
Iter: 62 loss: 2.09622908e-06
Iter: 63 loss: 2.09605901e-06
Iter: 64 loss: 2.04095431e-06
Iter: 65 loss: 2.26954671e-06
Iter: 66 loss: 2.02904084e-06
Iter: 67 loss: 1.98105727e-06
Iter: 68 loss: 2.05292031e-06
Iter: 69 loss: 1.95811936e-06
Iter: 70 loss: 1.90181208e-06
Iter: 71 loss: 1.91211029e-06
Iter: 72 loss: 1.8597791e-06
Iter: 73 loss: 1.78908658e-06
Iter: 74 loss: 2.43782256e-06
Iter: 75 loss: 1.78600362e-06
Iter: 76 loss: 1.75408968e-06
Iter: 77 loss: 1.75364232e-06
Iter: 78 loss: 1.72304635e-06
Iter: 79 loss: 1.73607623e-06
Iter: 80 loss: 1.70219266e-06
Iter: 81 loss: 1.68054521e-06
Iter: 82 loss: 1.77560992e-06
Iter: 83 loss: 1.67615121e-06
Iter: 84 loss: 1.64690857e-06
Iter: 85 loss: 1.6620827e-06
Iter: 86 loss: 1.62754145e-06
Iter: 87 loss: 1.61417483e-06
Iter: 88 loss: 1.71753686e-06
Iter: 89 loss: 1.61327921e-06
Iter: 90 loss: 1.59919e-06
Iter: 91 loss: 1.60104128e-06
Iter: 92 loss: 1.58849934e-06
Iter: 93 loss: 1.57053751e-06
Iter: 94 loss: 1.583933e-06
Iter: 95 loss: 1.55962221e-06
Iter: 96 loss: 1.54129316e-06
Iter: 97 loss: 1.52993971e-06
Iter: 98 loss: 1.52266057e-06
Iter: 99 loss: 1.48872266e-06
Iter: 100 loss: 1.89878222e-06
Iter: 101 loss: 1.48832714e-06
Iter: 102 loss: 1.47428182e-06
Iter: 103 loss: 1.44913156e-06
Iter: 104 loss: 2.07188668e-06
Iter: 105 loss: 1.44916271e-06
Iter: 106 loss: 1.41719192e-06
Iter: 107 loss: 1.4165e-06
Iter: 108 loss: 1.39147096e-06
Iter: 109 loss: 1.36840583e-06
Iter: 110 loss: 1.36669007e-06
Iter: 111 loss: 1.34995366e-06
Iter: 112 loss: 1.38110067e-06
Iter: 113 loss: 1.34287848e-06
Iter: 114 loss: 1.33199376e-06
Iter: 115 loss: 1.33070898e-06
Iter: 116 loss: 1.32453215e-06
Iter: 117 loss: 1.31047989e-06
Iter: 118 loss: 1.49820289e-06
Iter: 119 loss: 1.30964474e-06
Iter: 120 loss: 1.29599971e-06
Iter: 121 loss: 1.29594059e-06
Iter: 122 loss: 1.28602414e-06
Iter: 123 loss: 1.27732676e-06
Iter: 124 loss: 1.27478449e-06
Iter: 125 loss: 1.25946337e-06
Iter: 126 loss: 1.28024635e-06
Iter: 127 loss: 1.25184897e-06
Iter: 128 loss: 1.23214477e-06
Iter: 129 loss: 1.38219957e-06
Iter: 130 loss: 1.23063091e-06
Iter: 131 loss: 1.22009226e-06
Iter: 132 loss: 1.20812615e-06
Iter: 133 loss: 1.20653158e-06
Iter: 134 loss: 1.19457638e-06
Iter: 135 loss: 1.26958412e-06
Iter: 136 loss: 1.19315e-06
Iter: 137 loss: 1.1823239e-06
Iter: 138 loss: 1.28124918e-06
Iter: 139 loss: 1.18176308e-06
Iter: 140 loss: 1.17423747e-06
Iter: 141 loss: 1.1666807e-06
Iter: 142 loss: 1.16519675e-06
Iter: 143 loss: 1.15156649e-06
Iter: 144 loss: 1.15939e-06
Iter: 145 loss: 1.14278282e-06
Iter: 146 loss: 1.12686689e-06
Iter: 147 loss: 1.11932081e-06
Iter: 148 loss: 1.1115186e-06
Iter: 149 loss: 1.09600717e-06
Iter: 150 loss: 1.09563098e-06
Iter: 151 loss: 1.0846087e-06
Iter: 152 loss: 1.08459949e-06
Iter: 153 loss: 1.07889537e-06
Iter: 154 loss: 1.0707397e-06
Iter: 155 loss: 1.07047401e-06
Iter: 156 loss: 1.06169705e-06
Iter: 157 loss: 1.16686624e-06
Iter: 158 loss: 1.06159882e-06
Iter: 159 loss: 1.05325944e-06
Iter: 160 loss: 1.05440381e-06
Iter: 161 loss: 1.04699188e-06
Iter: 162 loss: 1.04213791e-06
Iter: 163 loss: 1.0493402e-06
Iter: 164 loss: 1.03978505e-06
Iter: 165 loss: 1.03366449e-06
Iter: 166 loss: 1.07562551e-06
Iter: 167 loss: 1.03310231e-06
Iter: 168 loss: 1.02721867e-06
Iter: 169 loss: 1.0242577e-06
Iter: 170 loss: 1.02152956e-06
Iter: 171 loss: 1.01381318e-06
Iter: 172 loss: 1.02595129e-06
Iter: 173 loss: 1.01015712e-06
Iter: 174 loss: 1.00430304e-06
Iter: 175 loss: 1.06925233e-06
Iter: 176 loss: 1.00421175e-06
Iter: 177 loss: 9.97058351e-07
Iter: 178 loss: 9.86096e-07
Iter: 179 loss: 9.85915563e-07
Iter: 180 loss: 9.76993874e-07
Iter: 181 loss: 1.02871661e-06
Iter: 182 loss: 9.75780154e-07
Iter: 183 loss: 9.67759888e-07
Iter: 184 loss: 9.6341148e-07
Iter: 185 loss: 9.59764066e-07
Iter: 186 loss: 9.49924413e-07
Iter: 187 loss: 1.00988109e-06
Iter: 188 loss: 9.48687841e-07
Iter: 189 loss: 9.41349754e-07
Iter: 190 loss: 9.41205e-07
Iter: 191 loss: 9.37483378e-07
Iter: 192 loss: 9.32202738e-07
Iter: 193 loss: 9.32012483e-07
Iter: 194 loss: 9.26256462e-07
Iter: 195 loss: 9.76997285e-07
Iter: 196 loss: 9.25986399e-07
Iter: 197 loss: 9.19860554e-07
Iter: 198 loss: 9.24741e-07
Iter: 199 loss: 9.16143449e-07
Iter: 200 loss: 9.12681344e-07
Iter: 201 loss: 9.06721e-07
Iter: 202 loss: 9.0671972e-07
Iter: 203 loss: 9.03163141e-07
Iter: 204 loss: 9.02192141e-07
Iter: 205 loss: 8.98423195e-07
Iter: 206 loss: 8.91990908e-07
Iter: 207 loss: 8.91979482e-07
Iter: 208 loss: 8.85264967e-07
Iter: 209 loss: 8.91671277e-07
Iter: 210 loss: 8.81426388e-07
Iter: 211 loss: 8.72175633e-07
Iter: 212 loss: 9.3050221e-07
Iter: 213 loss: 8.71122893e-07
Iter: 214 loss: 8.67047959e-07
Iter: 215 loss: 8.66893743e-07
Iter: 216 loss: 8.64832487e-07
Iter: 217 loss: 8.59001489e-07
Iter: 218 loss: 8.85522184e-07
Iter: 219 loss: 8.568569e-07
Iter: 220 loss: 8.50779657e-07
Iter: 221 loss: 8.50780452e-07
Iter: 222 loss: 8.47889396e-07
Iter: 223 loss: 8.82443487e-07
Iter: 224 loss: 8.47845513e-07
Iter: 225 loss: 8.44428314e-07
Iter: 226 loss: 8.40547727e-07
Iter: 227 loss: 8.4005012e-07
Iter: 228 loss: 8.35585183e-07
Iter: 229 loss: 8.35376738e-07
Iter: 230 loss: 8.319671e-07
Iter: 231 loss: 8.2874692e-07
Iter: 232 loss: 8.28721e-07
Iter: 233 loss: 8.24955e-07
Iter: 234 loss: 8.26748646e-07
Iter: 235 loss: 8.2244253e-07
Iter: 236 loss: 8.18622425e-07
Iter: 237 loss: 8.16714646e-07
Iter: 238 loss: 8.14924135e-07
Iter: 239 loss: 8.09901053e-07
Iter: 240 loss: 8.28374596e-07
Iter: 241 loss: 8.08627192e-07
Iter: 242 loss: 8.06743458e-07
Iter: 243 loss: 8.06520347e-07
Iter: 244 loss: 8.04101489e-07
Iter: 245 loss: 7.97936309e-07
Iter: 246 loss: 8.45657837e-07
Iter: 247 loss: 7.96704967e-07
Iter: 248 loss: 7.92358833e-07
Iter: 249 loss: 8.51995651e-07
Iter: 250 loss: 7.92355593e-07
Iter: 251 loss: 7.88947546e-07
Iter: 252 loss: 7.86776695e-07
Iter: 253 loss: 7.85481802e-07
Iter: 254 loss: 7.81216272e-07
Iter: 255 loss: 7.8116949e-07
Iter: 256 loss: 7.77112518e-07
Iter: 257 loss: 7.71083933e-07
Iter: 258 loss: 7.70918689e-07
Iter: 259 loss: 7.66874109e-07
Iter: 260 loss: 8.30709723e-07
Iter: 261 loss: 7.66891787e-07
Iter: 262 loss: 7.63283822e-07
Iter: 263 loss: 7.80954906e-07
Iter: 264 loss: 7.62598802e-07
Iter: 265 loss: 7.60035903e-07
Iter: 266 loss: 7.58997089e-07
Iter: 267 loss: 7.57574583e-07
Iter: 268 loss: 7.55136853e-07
Iter: 269 loss: 7.62706804e-07
Iter: 270 loss: 7.54419432e-07
Iter: 271 loss: 7.51227333e-07
Iter: 272 loss: 7.69482256e-07
Iter: 273 loss: 7.50805498e-07
Iter: 274 loss: 7.49256742e-07
Iter: 275 loss: 7.4576127e-07
Iter: 276 loss: 7.98490305e-07
Iter: 277 loss: 7.4564457e-07
Iter: 278 loss: 7.41815313e-07
Iter: 279 loss: 7.55650717e-07
Iter: 280 loss: 7.4089462e-07
Iter: 281 loss: 7.37039954e-07
Iter: 282 loss: 7.8021958e-07
Iter: 283 loss: 7.36983452e-07
Iter: 284 loss: 7.33978482e-07
Iter: 285 loss: 7.38170684e-07
Iter: 286 loss: 7.32494925e-07
Iter: 287 loss: 7.30056399e-07
Iter: 288 loss: 7.24110691e-07
Iter: 289 loss: 7.88666625e-07
Iter: 290 loss: 7.23524749e-07
Iter: 291 loss: 7.19034745e-07
Iter: 292 loss: 7.18853528e-07
Iter: 293 loss: 7.16007548e-07
Iter: 294 loss: 7.50341371e-07
Iter: 295 loss: 7.15949909e-07
Iter: 296 loss: 7.13527584e-07
Iter: 297 loss: 7.12461031e-07
Iter: 298 loss: 7.11206837e-07
Iter: 299 loss: 7.09233348e-07
Iter: 300 loss: 7.09185542e-07
Iter: 301 loss: 7.07200343e-07
Iter: 302 loss: 7.06361902e-07
Iter: 303 loss: 7.05352e-07
Iter: 304 loss: 7.03329761e-07
Iter: 305 loss: 7.00856162e-07
Iter: 306 loss: 7.00609348e-07
Iter: 307 loss: 6.99509826e-07
Iter: 308 loss: 6.98731242e-07
Iter: 309 loss: 6.97186863e-07
Iter: 310 loss: 6.9470741e-07
Iter: 311 loss: 6.94688367e-07
Iter: 312 loss: 6.91684363e-07
Iter: 313 loss: 6.96024244e-07
Iter: 314 loss: 6.90254808e-07
Iter: 315 loss: 6.87640124e-07
Iter: 316 loss: 6.93017e-07
Iter: 317 loss: 6.86609894e-07
Iter: 318 loss: 6.83886e-07
Iter: 319 loss: 7.14082887e-07
Iter: 320 loss: 6.83839062e-07
Iter: 321 loss: 6.81315839e-07
Iter: 322 loss: 6.8387709e-07
Iter: 323 loss: 6.79921072e-07
Iter: 324 loss: 6.77392734e-07
Iter: 325 loss: 6.76255468e-07
Iter: 326 loss: 6.74979049e-07
Iter: 327 loss: 6.71577254e-07
Iter: 328 loss: 6.74435512e-07
Iter: 329 loss: 6.69604219e-07
Iter: 330 loss: 6.65285e-07
Iter: 331 loss: 6.74799765e-07
Iter: 332 loss: 6.63591948e-07
Iter: 333 loss: 6.64841764e-07
Iter: 334 loss: 6.62089803e-07
Iter: 335 loss: 6.61044055e-07
Iter: 336 loss: 6.61584409e-07
Iter: 337 loss: 6.60362161e-07
Iter: 338 loss: 6.58911176e-07
Iter: 339 loss: 6.59672821e-07
Iter: 340 loss: 6.57964904e-07
Iter: 341 loss: 6.55270242e-07
Iter: 342 loss: 6.5417953e-07
Iter: 343 loss: 6.52707172e-07
Iter: 344 loss: 6.51128403e-07
Iter: 345 loss: 6.56899374e-07
Iter: 346 loss: 6.50709e-07
Iter: 347 loss: 6.48639457e-07
Iter: 348 loss: 6.6171782e-07
Iter: 349 loss: 6.48402306e-07
Iter: 350 loss: 6.47260947e-07
Iter: 351 loss: 6.46607873e-07
Iter: 352 loss: 6.46157332e-07
Iter: 353 loss: 6.44522174e-07
Iter: 354 loss: 6.44630745e-07
Iter: 355 loss: 6.43258602e-07
Iter: 356 loss: 6.40645681e-07
Iter: 357 loss: 6.45195144e-07
Iter: 358 loss: 6.39472887e-07
Iter: 359 loss: 6.3746e-07
Iter: 360 loss: 6.37463e-07
Iter: 361 loss: 6.35758568e-07
Iter: 362 loss: 6.39394159e-07
Iter: 363 loss: 6.35081847e-07
Iter: 364 loss: 6.33239551e-07
Iter: 365 loss: 6.29838837e-07
Iter: 366 loss: 7.13220174e-07
Iter: 367 loss: 6.29834119e-07
Iter: 368 loss: 6.26565338e-07
Iter: 369 loss: 6.39985501e-07
Iter: 370 loss: 6.25843086e-07
Iter: 371 loss: 6.24721451e-07
Iter: 372 loss: 6.24385393e-07
Iter: 373 loss: 6.23110168e-07
Iter: 374 loss: 6.25929374e-07
Iter: 375 loss: 6.22589255e-07
Iter: 376 loss: 6.21364393e-07
Iter: 377 loss: 6.18938316e-07
Iter: 378 loss: 6.6504532e-07
Iter: 379 loss: 6.18894e-07
Iter: 380 loss: 6.19197579e-07
Iter: 381 loss: 6.18099079e-07
Iter: 382 loss: 6.17441458e-07
Iter: 383 loss: 6.15377587e-07
Iter: 384 loss: 6.21110757e-07
Iter: 385 loss: 6.14314672e-07
Iter: 386 loss: 6.13583268e-07
Iter: 387 loss: 6.1263205e-07
Iter: 388 loss: 6.1183971e-07
Iter: 389 loss: 6.10245479e-07
Iter: 390 loss: 6.37749849e-07
Iter: 391 loss: 6.10195343e-07
Iter: 392 loss: 6.07737945e-07
Iter: 393 loss: 6.06251945e-07
Iter: 394 loss: 6.05253547e-07
Iter: 395 loss: 6.02983278e-07
Iter: 396 loss: 6.02913644e-07
Iter: 397 loss: 6.00982e-07
Iter: 398 loss: 6.10185339e-07
Iter: 399 loss: 6.00589829e-07
Iter: 400 loss: 5.99463647e-07
Iter: 401 loss: 5.96899781e-07
Iter: 402 loss: 6.37513494e-07
Iter: 403 loss: 5.96832706e-07
Iter: 404 loss: 5.9446e-07
Iter: 405 loss: 6.17101932e-07
Iter: 406 loss: 5.94388496e-07
Iter: 407 loss: 5.93263e-07
Iter: 408 loss: 5.93272034e-07
Iter: 409 loss: 5.92229696e-07
Iter: 410 loss: 5.92774768e-07
Iter: 411 loss: 5.9157793e-07
Iter: 412 loss: 5.90585898e-07
Iter: 413 loss: 5.88822672e-07
Iter: 414 loss: 5.88826197e-07
Iter: 415 loss: 5.87726618e-07
Iter: 416 loss: 5.87513057e-07
Iter: 417 loss: 5.86548197e-07
Iter: 418 loss: 5.8650744e-07
Iter: 419 loss: 5.85766202e-07
Iter: 420 loss: 5.84557256e-07
Iter: 421 loss: 5.91346179e-07
Iter: 422 loss: 5.84418899e-07
Iter: 423 loss: 5.8314788e-07
Iter: 424 loss: 5.83780889e-07
Iter: 425 loss: 5.82265329e-07
Iter: 426 loss: 5.8103e-07
Iter: 427 loss: 5.80347319e-07
Iter: 428 loss: 5.79776099e-07
Iter: 429 loss: 5.78081199e-07
Iter: 430 loss: 5.84258885e-07
Iter: 431 loss: 5.77641458e-07
Iter: 432 loss: 5.76057516e-07
Iter: 433 loss: 5.99593875e-07
Iter: 434 loss: 5.76049274e-07
Iter: 435 loss: 5.75194576e-07
Iter: 436 loss: 5.74918886e-07
Iter: 437 loss: 5.74397745e-07
Iter: 438 loss: 5.73027535e-07
Iter: 439 loss: 5.70121642e-07
Iter: 440 loss: 6.22765413e-07
Iter: 441 loss: 5.70082875e-07
Iter: 442 loss: 5.70220664e-07
Iter: 443 loss: 5.68938049e-07
Iter: 444 loss: 5.6770773e-07
Iter: 445 loss: 5.6894396e-07
Iter: 446 loss: 5.67038171e-07
Iter: 447 loss: 5.66146866e-07
Iter: 448 loss: 5.6504814e-07
Iter: 449 loss: 5.6492695e-07
Iter: 450 loss: 5.63174353e-07
Iter: 451 loss: 5.75059914e-07
Iter: 452 loss: 5.62983303e-07
Iter: 453 loss: 5.61693128e-07
Iter: 454 loss: 5.76342472e-07
Iter: 455 loss: 5.61660386e-07
Iter: 456 loss: 5.61140325e-07
Iter: 457 loss: 5.61170225e-07
Iter: 458 loss: 5.60754188e-07
Iter: 459 loss: 5.59703722e-07
Iter: 460 loss: 5.60627655e-07
Iter: 461 loss: 5.59110674e-07
Iter: 462 loss: 5.58258307e-07
Iter: 463 loss: 5.57969884e-07
Iter: 464 loss: 5.57475346e-07
Iter: 465 loss: 5.55821714e-07
Iter: 466 loss: 5.56004863e-07
Iter: 467 loss: 5.54549104e-07
Iter: 468 loss: 5.52939241e-07
Iter: 469 loss: 5.52915367e-07
Iter: 470 loss: 5.51440849e-07
Iter: 471 loss: 5.53544112e-07
Iter: 472 loss: 5.50712116e-07
Iter: 473 loss: 5.49501124e-07
Iter: 474 loss: 5.48490902e-07
Iter: 475 loss: 5.48146772e-07
Iter: 476 loss: 5.46608248e-07
Iter: 477 loss: 5.67031e-07
Iter: 478 loss: 5.46569709e-07
Iter: 479 loss: 5.45643218e-07
Iter: 480 loss: 5.60516185e-07
Iter: 481 loss: 5.45636851e-07
Iter: 482 loss: 5.45215585e-07
Iter: 483 loss: 5.4404137e-07
Iter: 484 loss: 5.52351821e-07
Iter: 485 loss: 5.43798933e-07
Iter: 486 loss: 5.42651378e-07
Iter: 487 loss: 5.52075335e-07
Iter: 488 loss: 5.42586349e-07
Iter: 489 loss: 5.41465056e-07
Iter: 490 loss: 5.49180356e-07
Iter: 491 loss: 5.41384e-07
Iter: 492 loss: 5.40518386e-07
Iter: 493 loss: 5.40029077e-07
Iter: 494 loss: 5.3969319e-07
Iter: 495 loss: 5.38495499e-07
Iter: 496 loss: 5.49417905e-07
Iter: 497 loss: 5.38464519e-07
Iter: 498 loss: 5.37492895e-07
Iter: 499 loss: 5.36275365e-07
Iter: 500 loss: 5.36174184e-07
Iter: 501 loss: 5.34724904e-07
Iter: 502 loss: 5.36838684e-07
Iter: 503 loss: 5.33982302e-07
Iter: 504 loss: 5.33009e-07
Iter: 505 loss: 5.33033472e-07
Iter: 506 loss: 5.32026945e-07
Iter: 507 loss: 5.33643288e-07
Iter: 508 loss: 5.3155668e-07
Iter: 509 loss: 5.30639568e-07
Iter: 510 loss: 5.31887849e-07
Iter: 511 loss: 5.30222451e-07
Iter: 512 loss: 5.29274928e-07
Iter: 513 loss: 5.29360364e-07
Iter: 514 loss: 5.28538862e-07
Iter: 515 loss: 5.27167288e-07
Iter: 516 loss: 5.4663542e-07
Iter: 517 loss: 5.27187353e-07
Iter: 518 loss: 5.26631425e-07
Iter: 519 loss: 5.25424639e-07
Iter: 520 loss: 5.43037345e-07
Iter: 521 loss: 5.25369103e-07
Iter: 522 loss: 5.23658798e-07
Iter: 523 loss: 5.27306725e-07
Iter: 524 loss: 5.22989637e-07
Iter: 525 loss: 5.22673e-07
Iter: 526 loss: 5.22379878e-07
Iter: 527 loss: 5.21828895e-07
Iter: 528 loss: 5.20494666e-07
Iter: 529 loss: 5.36521611e-07
Iter: 530 loss: 5.20424351e-07
Iter: 531 loss: 5.19435844e-07
Iter: 532 loss: 5.19427658e-07
Iter: 533 loss: 5.18637876e-07
Iter: 534 loss: 5.2039104e-07
Iter: 535 loss: 5.18339789e-07
Iter: 536 loss: 5.17812623e-07
Iter: 537 loss: 5.1791676e-07
Iter: 538 loss: 5.17409262e-07
Iter: 539 loss: 5.16620958e-07
Iter: 540 loss: 5.16165755e-07
Iter: 541 loss: 5.15848797e-07
Iter: 542 loss: 5.14896954e-07
Iter: 543 loss: 5.14862222e-07
Iter: 544 loss: 5.13828411e-07
Iter: 545 loss: 5.1298548e-07
Iter: 546 loss: 5.12667e-07
Iter: 547 loss: 5.11275175e-07
Iter: 548 loss: 5.13285272e-07
Iter: 549 loss: 5.10612722e-07
Iter: 550 loss: 5.09713061e-07
Iter: 551 loss: 5.09661731e-07
Iter: 552 loss: 5.08739049e-07
Iter: 553 loss: 5.08523158e-07
Iter: 554 loss: 5.07993661e-07
Iter: 555 loss: 5.07243385e-07
Iter: 556 loss: 5.06453773e-07
Iter: 557 loss: 5.06297738e-07
Iter: 558 loss: 5.05342314e-07
Iter: 559 loss: 5.15950887e-07
Iter: 560 loss: 5.05339528e-07
Iter: 561 loss: 5.04653372e-07
Iter: 562 loss: 5.04687307e-07
Iter: 563 loss: 5.04106254e-07
Iter: 564 loss: 5.03162141e-07
Iter: 565 loss: 5.10856523e-07
Iter: 566 loss: 5.0307807e-07
Iter: 567 loss: 5.01911302e-07
Iter: 568 loss: 5.03128263e-07
Iter: 569 loss: 5.01270108e-07
Iter: 570 loss: 5.00505621e-07
Iter: 571 loss: 5.05321168e-07
Iter: 572 loss: 5.00408078e-07
Iter: 573 loss: 4.99672751e-07
Iter: 574 loss: 5.00573719e-07
Iter: 575 loss: 4.99274392e-07
Iter: 576 loss: 4.98396e-07
Iter: 577 loss: 4.97759515e-07
Iter: 578 loss: 4.97463077e-07
Iter: 579 loss: 4.96304153e-07
Iter: 580 loss: 4.99688554e-07
Iter: 581 loss: 4.95935e-07
Iter: 582 loss: 4.953705e-07
Iter: 583 loss: 4.95279778e-07
Iter: 584 loss: 4.94762162e-07
Iter: 585 loss: 4.93267521e-07
Iter: 586 loss: 5.00371698e-07
Iter: 587 loss: 4.92788445e-07
Iter: 588 loss: 4.91997639e-07
Iter: 589 loss: 4.9182654e-07
Iter: 590 loss: 4.9133422e-07
Iter: 591 loss: 4.98797e-07
Iter: 592 loss: 4.91329956e-07
Iter: 593 loss: 4.90887942e-07
Iter: 594 loss: 4.89493175e-07
Iter: 595 loss: 4.91869514e-07
Iter: 596 loss: 4.88588626e-07
Iter: 597 loss: 4.87306579e-07
Iter: 598 loss: 5.0583958e-07
Iter: 599 loss: 4.87314765e-07
Iter: 600 loss: 4.86463364e-07
Iter: 601 loss: 4.89810532e-07
Iter: 602 loss: 4.86277031e-07
Iter: 603 loss: 4.85291594e-07
Iter: 604 loss: 4.92343588e-07
Iter: 605 loss: 4.85245096e-07
Iter: 606 loss: 4.84445e-07
Iter: 607 loss: 4.84775569e-07
Iter: 608 loss: 4.8391405e-07
Iter: 609 loss: 4.83416e-07
Iter: 610 loss: 4.87331249e-07
Iter: 611 loss: 4.83389385e-07
Iter: 612 loss: 4.82778546e-07
Iter: 613 loss: 4.81896564e-07
Iter: 614 loss: 4.81868256e-07
Iter: 615 loss: 4.8077726e-07
Iter: 616 loss: 4.82919916e-07
Iter: 617 loss: 4.80354799e-07
Iter: 618 loss: 4.79221853e-07
Iter: 619 loss: 4.82986309e-07
Iter: 620 loss: 4.78922743e-07
Iter: 621 loss: 4.78203788e-07
Iter: 622 loss: 4.78204811e-07
Iter: 623 loss: 4.77641095e-07
Iter: 624 loss: 4.76393069e-07
Iter: 625 loss: 4.90480375e-07
Iter: 626 loss: 4.76255423e-07
Iter: 627 loss: 4.75836089e-07
Iter: 628 loss: 4.75613717e-07
Iter: 629 loss: 4.75045283e-07
Iter: 630 loss: 4.76201819e-07
Iter: 631 loss: 4.74814e-07
Iter: 632 loss: 4.74384763e-07
Iter: 633 loss: 4.733445e-07
Iter: 634 loss: 4.87557543e-07
Iter: 635 loss: 4.7330326e-07
Iter: 636 loss: 4.7205998e-07
Iter: 637 loss: 4.77763251e-07
Iter: 638 loss: 4.7180373e-07
Iter: 639 loss: 4.7158747e-07
Iter: 640 loss: 4.71388432e-07
Iter: 641 loss: 4.7099644e-07
Iter: 642 loss: 4.70203304e-07
Iter: 643 loss: 4.85532382e-07
Iter: 644 loss: 4.70193044e-07
Iter: 645 loss: 4.6958e-07
Iter: 646 loss: 4.76090236e-07
Iter: 647 loss: 4.69580641e-07
Iter: 648 loss: 4.69093749e-07
Iter: 649 loss: 4.7052049e-07
Iter: 650 loss: 4.68952237e-07
Iter: 651 loss: 4.68401311e-07
Iter: 652 loss: 4.67666268e-07
Iter: 653 loss: 4.67631224e-07
Iter: 654 loss: 4.66617e-07
Iter: 655 loss: 4.67908109e-07
Iter: 656 loss: 4.66097e-07
Iter: 657 loss: 4.6538517e-07
Iter: 658 loss: 4.65374626e-07
Iter: 659 loss: 4.6466036e-07
Iter: 660 loss: 4.64808522e-07
Iter: 661 loss: 4.64148684e-07
Iter: 662 loss: 4.63210256e-07
Iter: 663 loss: 4.63120216e-07
Iter: 664 loss: 4.62413311e-07
Iter: 665 loss: 4.62259e-07
Iter: 666 loss: 4.61981671e-07
Iter: 667 loss: 4.61563502e-07
Iter: 668 loss: 4.60742456e-07
Iter: 669 loss: 4.76418e-07
Iter: 670 loss: 4.60747202e-07
Iter: 671 loss: 4.59989025e-07
Iter: 672 loss: 4.59782029e-07
Iter: 673 loss: 4.59336803e-07
Iter: 674 loss: 4.58550176e-07
Iter: 675 loss: 4.68192383e-07
Iter: 676 loss: 4.58538807e-07
Iter: 677 loss: 4.57939791e-07
Iter: 678 loss: 4.57749394e-07
Iter: 679 loss: 4.57429536e-07
Iter: 680 loss: 4.57371982e-07
Iter: 681 loss: 4.5704428e-07
Iter: 682 loss: 4.56722802e-07
Iter: 683 loss: 4.55959935e-07
Iter: 684 loss: 4.6427607e-07
Iter: 685 loss: 4.55911788e-07
Iter: 686 loss: 4.54992829e-07
Iter: 687 loss: 4.60500218e-07
Iter: 688 loss: 4.54865926e-07
Iter: 689 loss: 4.54034875e-07
Iter: 690 loss: 4.57856459e-07
Iter: 691 loss: 4.53862583e-07
Iter: 692 loss: 4.53135442e-07
Iter: 693 loss: 4.52515508e-07
Iter: 694 loss: 4.5235322e-07
Iter: 695 loss: 4.51500796e-07
Iter: 696 loss: 4.56340899e-07
Iter: 697 loss: 4.51364713e-07
Iter: 698 loss: 4.50810916e-07
Iter: 699 loss: 4.57482656e-07
Iter: 700 loss: 4.50799035e-07
Iter: 701 loss: 4.50275422e-07
Iter: 702 loss: 4.50883647e-07
Iter: 703 loss: 4.50028438e-07
Iter: 704 loss: 4.49566727e-07
Iter: 705 loss: 4.52725772e-07
Iter: 706 loss: 4.49526198e-07
Iter: 707 loss: 4.49083416e-07
Iter: 708 loss: 4.4892704e-07
Iter: 709 loss: 4.48656834e-07
Iter: 710 loss: 4.48112189e-07
Iter: 711 loss: 4.47249874e-07
Iter: 712 loss: 4.47252859e-07
Iter: 713 loss: 4.46329068e-07
Iter: 714 loss: 4.51479906e-07
Iter: 715 loss: 4.46208929e-07
Iter: 716 loss: 4.45325639e-07
Iter: 717 loss: 4.49671177e-07
Iter: 718 loss: 4.45186174e-07
Iter: 719 loss: 4.44893402e-07
Iter: 720 loss: 4.44790857e-07
Iter: 721 loss: 4.44645934e-07
Iter: 722 loss: 4.44145428e-07
Iter: 723 loss: 4.4490838e-07
Iter: 724 loss: 4.43833358e-07
Iter: 725 loss: 4.43354594e-07
Iter: 726 loss: 4.43327e-07
Iter: 727 loss: 4.42858948e-07
Iter: 728 loss: 4.43544195e-07
Iter: 729 loss: 4.42608439e-07
Iter: 730 loss: 4.42050037e-07
Iter: 731 loss: 4.41821129e-07
Iter: 732 loss: 4.41555898e-07
Iter: 733 loss: 4.40989481e-07
Iter: 734 loss: 4.40982433e-07
Iter: 735 loss: 4.40425481e-07
Iter: 736 loss: 4.40768076e-07
Iter: 737 loss: 4.40067311e-07
Iter: 738 loss: 4.39460791e-07
Iter: 739 loss: 4.4215372e-07
Iter: 740 loss: 4.39342557e-07
Iter: 741 loss: 4.3862795e-07
Iter: 742 loss: 4.39052883e-07
Iter: 743 loss: 4.38161123e-07
Iter: 744 loss: 4.37513904e-07
Iter: 745 loss: 4.38704717e-07
Iter: 746 loss: 4.37261178e-07
Iter: 747 loss: 4.36738048e-07
Iter: 748 loss: 4.36499363e-07
Iter: 749 loss: 4.36250332e-07
Iter: 750 loss: 4.35518928e-07
Iter: 751 loss: 4.39933217e-07
Iter: 752 loss: 4.35436846e-07
Iter: 753 loss: 4.35154476e-07
Iter: 754 loss: 4.35074213e-07
Iter: 755 loss: 4.34845731e-07
Iter: 756 loss: 4.34566459e-07
Iter: 757 loss: 4.34536588e-07
Iter: 758 loss: 4.34111598e-07
Iter: 759 loss: 4.33424589e-07
Iter: 760 loss: 4.33413817e-07
Iter: 761 loss: 4.32866159e-07
Iter: 762 loss: 4.3283751e-07
Iter: 763 loss: 4.32222919e-07
Iter: 764 loss: 4.32308525e-07
Iter: 765 loss: 4.31786816e-07
Iter: 766 loss: 4.31073431e-07
Iter: 767 loss: 4.31297309e-07
Iter: 768 loss: 4.30562693e-07
Iter: 769 loss: 4.30095525e-07
Iter: 770 loss: 4.29993236e-07
Iter: 771 loss: 4.29621764e-07
Iter: 772 loss: 4.29328452e-07
Iter: 773 loss: 4.29164e-07
Iter: 774 loss: 4.28746915e-07
Iter: 775 loss: 4.32746e-07
Iter: 776 loss: 4.28734268e-07
Iter: 777 loss: 4.2828708e-07
Iter: 778 loss: 4.27455291e-07
Iter: 779 loss: 4.27451653e-07
Iter: 780 loss: 4.26779422e-07
Iter: 781 loss: 4.28974971e-07
Iter: 782 loss: 4.26614974e-07
Iter: 783 loss: 4.26070386e-07
Iter: 784 loss: 4.29852378e-07
Iter: 785 loss: 4.26009649e-07
Iter: 786 loss: 4.25681208e-07
Iter: 787 loss: 4.30351292e-07
Iter: 788 loss: 4.25689905e-07
Iter: 789 loss: 4.25358962e-07
Iter: 790 loss: 4.24687727e-07
Iter: 791 loss: 4.38360473e-07
Iter: 792 loss: 4.24690143e-07
Iter: 793 loss: 4.24165e-07
Iter: 794 loss: 4.26375067e-07
Iter: 795 loss: 4.24091411e-07
Iter: 796 loss: 4.23516042e-07
Iter: 797 loss: 4.23210281e-07
Iter: 798 loss: 4.22971937e-07
Iter: 799 loss: 4.2244028e-07
Iter: 800 loss: 4.294956e-07
Iter: 801 loss: 4.22416235e-07
Iter: 802 loss: 4.21890064e-07
Iter: 803 loss: 4.24311054e-07
Iter: 804 loss: 4.21793033e-07
Iter: 805 loss: 4.2133783e-07
Iter: 806 loss: 4.21060548e-07
Iter: 807 loss: 4.20887318e-07
Iter: 808 loss: 4.20576185e-07
Iter: 809 loss: 4.20501e-07
Iter: 810 loss: 4.20258175e-07
Iter: 811 loss: 4.19760568e-07
Iter: 812 loss: 4.27703981e-07
Iter: 813 loss: 4.19713047e-07
Iter: 814 loss: 4.19079527e-07
Iter: 815 loss: 4.19997463e-07
Iter: 816 loss: 4.1874145e-07
Iter: 817 loss: 4.180323e-07
Iter: 818 loss: 4.28475659e-07
Iter: 819 loss: 4.1802906e-07
Iter: 820 loss: 4.17687062e-07
Iter: 821 loss: 4.17135851e-07
Iter: 822 loss: 4.17126e-07
Iter: 823 loss: 4.16478855e-07
Iter: 824 loss: 4.19202081e-07
Iter: 825 loss: 4.16316368e-07
Iter: 826 loss: 4.15911074e-07
Iter: 827 loss: 4.15921249e-07
Iter: 828 loss: 4.15531929e-07
Iter: 829 loss: 4.15803129e-07
Iter: 830 loss: 4.15274542e-07
Iter: 831 loss: 4.15006753e-07
Iter: 832 loss: 4.14796034e-07
Iter: 833 loss: 4.14680017e-07
Iter: 834 loss: 4.14115675e-07
Iter: 835 loss: 4.14849126e-07
Iter: 836 loss: 4.13859425e-07
Iter: 837 loss: 4.13314893e-07
Iter: 838 loss: 4.18742303e-07
Iter: 839 loss: 4.13295396e-07
Iter: 840 loss: 4.12822374e-07
Iter: 841 loss: 4.15138601e-07
Iter: 842 loss: 4.12749046e-07
Iter: 843 loss: 4.12442034e-07
Iter: 844 loss: 4.13380235e-07
Iter: 845 loss: 4.12371719e-07
Iter: 846 loss: 4.11961935e-07
Iter: 847 loss: 4.11511849e-07
Iter: 848 loss: 4.11433717e-07
Iter: 849 loss: 4.10914538e-07
Iter: 850 loss: 4.10854796e-07
Iter: 851 loss: 4.10438304e-07
Iter: 852 loss: 4.09756268e-07
Iter: 853 loss: 4.18571972e-07
Iter: 854 loss: 4.09757547e-07
Iter: 855 loss: 4.09173452e-07
Iter: 856 loss: 4.10531641e-07
Iter: 857 loss: 4.08957419e-07
Iter: 858 loss: 4.08525977e-07
Iter: 859 loss: 4.08025528e-07
Iter: 860 loss: 4.07980451e-07
Iter: 861 loss: 4.07734234e-07
Iter: 862 loss: 4.07679352e-07
Iter: 863 loss: 4.07311859e-07
Iter: 864 loss: 4.07195159e-07
Iter: 865 loss: 4.07003938e-07
Iter: 866 loss: 4.06618e-07
Iter: 867 loss: 4.0659387e-07
Iter: 868 loss: 4.06290269e-07
Iter: 869 loss: 4.05754292e-07
Iter: 870 loss: 4.0645557e-07
Iter: 871 loss: 4.05465812e-07
Iter: 872 loss: 4.05012543e-07
Iter: 873 loss: 4.11464896e-07
Iter: 874 loss: 4.0500737e-07
Iter: 875 loss: 4.04602417e-07
Iter: 876 loss: 4.0640677e-07
Iter: 877 loss: 4.04533353e-07
Iter: 878 loss: 4.0419124e-07
Iter: 879 loss: 4.04411708e-07
Iter: 880 loss: 4.03961536e-07
Iter: 881 loss: 4.03453e-07
Iter: 882 loss: 4.05610052e-07
Iter: 883 loss: 4.03360048e-07
Iter: 884 loss: 4.03037745e-07
Iter: 885 loss: 4.03239653e-07
Iter: 886 loss: 4.02832086e-07
Iter: 887 loss: 4.02560175e-07
Iter: 888 loss: 4.03099449e-07
Iter: 889 loss: 4.0244214e-07
Iter: 890 loss: 4.01982106e-07
Iter: 891 loss: 4.03055708e-07
Iter: 892 loss: 4.01781733e-07
Iter: 893 loss: 4.01460227e-07
Iter: 894 loss: 4.01190562e-07
Iter: 895 loss: 4.0108057e-07
Iter: 896 loss: 4.00616671e-07
Iter: 897 loss: 4.06535406e-07
Iter: 898 loss: 4.00590181e-07
Iter: 899 loss: 4.00122076e-07
Iter: 900 loss: 4.01298308e-07
Iter: 901 loss: 3.99957315e-07
Iter: 902 loss: 3.99627368e-07
Iter: 903 loss: 3.99048e-07
Iter: 904 loss: 3.99054045e-07
Iter: 905 loss: 3.98356804e-07
Iter: 906 loss: 3.99232448e-07
Iter: 907 loss: 3.97995905e-07
Iter: 908 loss: 3.97625627e-07
Iter: 909 loss: 3.97570318e-07
Iter: 910 loss: 3.97233123e-07
Iter: 911 loss: 3.99052851e-07
Iter: 912 loss: 3.97176905e-07
Iter: 913 loss: 3.96970336e-07
Iter: 914 loss: 3.97315432e-07
Iter: 915 loss: 3.96852101e-07
Iter: 916 loss: 3.9657985e-07
Iter: 917 loss: 3.97452823e-07
Iter: 918 loss: 3.9651141e-07
Iter: 919 loss: 3.96256326e-07
Iter: 920 loss: 3.95692041e-07
Iter: 921 loss: 4.03548228e-07
Iter: 922 loss: 3.95648584e-07
Iter: 923 loss: 3.95471261e-07
Iter: 924 loss: 3.95346234e-07
Iter: 925 loss: 3.95046072e-07
Iter: 926 loss: 3.94401582e-07
Iter: 927 loss: 4.05005494e-07
Iter: 928 loss: 3.94390952e-07
Iter: 929 loss: 3.93694904e-07
Iter: 930 loss: 3.96936741e-07
Iter: 931 loss: 3.93590909e-07
Iter: 932 loss: 3.93154068e-07
Iter: 933 loss: 3.93155e-07
Iter: 934 loss: 3.92831765e-07
Iter: 935 loss: 3.92305225e-07
Iter: 936 loss: 3.92273165e-07
Iter: 937 loss: 3.91692794e-07
Iter: 938 loss: 3.92565937e-07
Iter: 939 loss: 3.91410111e-07
Iter: 940 loss: 3.90943057e-07
Iter: 941 loss: 3.94228437e-07
Iter: 942 loss: 3.90924924e-07
Iter: 943 loss: 3.90634284e-07
Iter: 944 loss: 3.93057064e-07
Iter: 945 loss: 3.90624507e-07
Iter: 946 loss: 3.90289756e-07
Iter: 947 loss: 3.91103015e-07
Iter: 948 loss: 3.90176751e-07
Iter: 949 loss: 3.89958359e-07
Iter: 950 loss: 3.90448292e-07
Iter: 951 loss: 3.8987551e-07
Iter: 952 loss: 3.89545562e-07
Iter: 953 loss: 3.89425026e-07
Iter: 954 loss: 3.89221412e-07
Iter: 955 loss: 3.88819046e-07
Iter: 956 loss: 3.88465025e-07
Iter: 957 loss: 3.88376975e-07
Iter: 958 loss: 3.8802915e-07
Iter: 959 loss: 3.87942265e-07
Iter: 960 loss: 3.87709e-07
Iter: 961 loss: 3.87716454e-07
Iter: 962 loss: 3.87516536e-07
Iter: 963 loss: 3.8721862e-07
Iter: 964 loss: 3.87497778e-07
Iter: 965 loss: 3.87044622e-07
Iter: 966 loss: 3.86675651e-07
Iter: 967 loss: 3.90208925e-07
Iter: 968 loss: 3.86655415e-07
Iter: 969 loss: 3.86428894e-07
Iter: 970 loss: 3.86002455e-07
Iter: 971 loss: 3.95150664e-07
Iter: 972 loss: 3.85989381e-07
Iter: 973 loss: 3.85502e-07
Iter: 974 loss: 3.85761922e-07
Iter: 975 loss: 3.8519471e-07
Iter: 976 loss: 3.84646228e-07
Iter: 977 loss: 3.91691486e-07
Iter: 978 loss: 3.84620733e-07
Iter: 979 loss: 3.8436383e-07
Iter: 980 loss: 3.84366842e-07
Iter: 981 loss: 3.84102407e-07
Iter: 982 loss: 3.83628446e-07
Iter: 983 loss: 3.93221626e-07
Iter: 984 loss: 3.83634358e-07
Iter: 985 loss: 3.83120408e-07
Iter: 986 loss: 3.88784599e-07
Iter: 987 loss: 3.83103412e-07
Iter: 988 loss: 3.82707213e-07
Iter: 989 loss: 3.82354216e-07
Iter: 990 loss: 3.82254086e-07
Iter: 991 loss: 3.81824151e-07
Iter: 992 loss: 3.84143505e-07
Iter: 993 loss: 3.81786919e-07
Iter: 994 loss: 3.81519e-07
Iter: 995 loss: 3.85509821e-07
Iter: 996 loss: 3.81513246e-07
Iter: 997 loss: 3.8127385e-07
Iter: 998 loss: 3.80930459e-07
Iter: 999 loss: 3.80927105e-07
Iter: 1000 loss: 3.80651443e-07
Iter: 1001 loss: 3.80654313e-07
Iter: 1002 loss: 3.80384677e-07
Iter: 1003 loss: 3.80759303e-07
Iter: 1004 loss: 3.8026613e-07
Iter: 1005 loss: 3.80061692e-07
Iter: 1006 loss: 3.79549817e-07
Iter: 1007 loss: 3.83600536e-07
Iter: 1008 loss: 3.79427661e-07
Iter: 1009 loss: 3.78851951e-07
Iter: 1010 loss: 3.86388365e-07
Iter: 1011 loss: 3.78830151e-07
Iter: 1012 loss: 3.78476898e-07
Iter: 1013 loss: 3.81055116e-07
Iter: 1014 loss: 3.78439552e-07
Iter: 1015 loss: 3.77995377e-07
Iter: 1016 loss: 3.78226872e-07
Iter: 1017 loss: 3.77681033e-07
Iter: 1018 loss: 3.77340768e-07
Iter: 1019 loss: 3.77877228e-07
Iter: 1020 loss: 3.77165293e-07
Iter: 1021 loss: 3.7677836e-07
Iter: 1022 loss: 3.7988292e-07
Iter: 1023 loss: 3.76748119e-07
Iter: 1024 loss: 3.76522735e-07
Iter: 1025 loss: 3.76383525e-07
Iter: 1026 loss: 3.76297379e-07
Iter: 1027 loss: 3.75945433e-07
Iter: 1028 loss: 3.7649005e-07
Iter: 1029 loss: 3.7576703e-07
Iter: 1030 loss: 3.75534341e-07
Iter: 1031 loss: 3.75540225e-07
Iter: 1032 loss: 3.75317882e-07
Iter: 1033 loss: 3.74875867e-07
Iter: 1034 loss: 3.82991345e-07
Iter: 1035 loss: 3.74877089e-07
Iter: 1036 loss: 3.74494704e-07
Iter: 1037 loss: 3.79263867e-07
Iter: 1038 loss: 3.74486746e-07
Iter: 1039 loss: 3.74186072e-07
Iter: 1040 loss: 3.75699358e-07
Iter: 1041 loss: 3.7412326e-07
Iter: 1042 loss: 3.73892817e-07
Iter: 1043 loss: 3.73524614e-07
Iter: 1044 loss: 3.73506282e-07
Iter: 1045 loss: 3.73071799e-07
Iter: 1046 loss: 3.72761576e-07
Iter: 1047 loss: 3.72580217e-07
Iter: 1048 loss: 3.72047339e-07
Iter: 1049 loss: 3.77233391e-07
Iter: 1050 loss: 3.7203796e-07
Iter: 1051 loss: 3.7162846e-07
Iter: 1052 loss: 3.72693734e-07
Iter: 1053 loss: 3.71463528e-07
Iter: 1054 loss: 3.7122328e-07
Iter: 1055 loss: 3.71231806e-07
Iter: 1056 loss: 3.70945088e-07
Iter: 1057 loss: 3.70990051e-07
Iter: 1058 loss: 3.70743862e-07
Iter: 1059 loss: 3.70486248e-07
Iter: 1060 loss: 3.70132682e-07
Iter: 1061 loss: 3.70122052e-07
Iter: 1062 loss: 3.69856167e-07
Iter: 1063 loss: 3.69843235e-07
Iter: 1064 loss: 3.69615037e-07
Iter: 1065 loss: 3.69310158e-07
Iter: 1066 loss: 3.69304871e-07
Iter: 1067 loss: 3.68862203e-07
Iter: 1068 loss: 3.70054863e-07
Iter: 1069 loss: 3.68729445e-07
Iter: 1070 loss: 3.68449889e-07
Iter: 1071 loss: 3.68446194e-07
Iter: 1072 loss: 3.68212e-07
Iter: 1073 loss: 3.67876737e-07
Iter: 1074 loss: 3.67862299e-07
Iter: 1075 loss: 3.67685914e-07
Iter: 1076 loss: 3.67666701e-07
Iter: 1077 loss: 3.67493612e-07
Iter: 1078 loss: 3.67173243e-07
Iter: 1079 loss: 3.67179467e-07
Iter: 1080 loss: 3.6680774e-07
Iter: 1081 loss: 3.67629838e-07
Iter: 1082 loss: 3.66674783e-07
Iter: 1083 loss: 3.66336621e-07
Iter: 1084 loss: 3.66427e-07
Iter: 1085 loss: 3.66103649e-07
Iter: 1086 loss: 3.65822132e-07
Iter: 1087 loss: 3.65823837e-07
Iter: 1088 loss: 3.6551512e-07
Iter: 1089 loss: 3.66477821e-07
Iter: 1090 loss: 3.65428406e-07
Iter: 1091 loss: 3.65241704e-07
Iter: 1092 loss: 3.64895556e-07
Iter: 1093 loss: 3.72770842e-07
Iter: 1094 loss: 3.64880748e-07
Iter: 1095 loss: 3.64402638e-07
Iter: 1096 loss: 3.66700448e-07
Iter: 1097 loss: 3.6434173e-07
Iter: 1098 loss: 3.63925e-07
Iter: 1099 loss: 3.68399071e-07
Iter: 1100 loss: 3.63912591e-07
Iter: 1101 loss: 3.63759426e-07
Iter: 1102 loss: 3.63351262e-07
Iter: 1103 loss: 3.68067674e-07
Iter: 1104 loss: 3.63335687e-07
Iter: 1105 loss: 3.63303968e-07
Iter: 1106 loss: 3.631352e-07
Iter: 1107 loss: 3.62972685e-07
Iter: 1108 loss: 3.62723426e-07
Iter: 1109 loss: 3.62737381e-07
Iter: 1110 loss: 3.62421332e-07
Iter: 1111 loss: 3.65182814e-07
Iter: 1112 loss: 3.62403654e-07
Iter: 1113 loss: 3.62138337e-07
Iter: 1114 loss: 3.62285732e-07
Iter: 1115 loss: 3.61960758e-07
Iter: 1116 loss: 3.61695072e-07
Iter: 1117 loss: 3.61457808e-07
Iter: 1118 loss: 3.61394711e-07
Iter: 1119 loss: 3.6093698e-07
Iter: 1120 loss: 3.61959593e-07
Iter: 1121 loss: 3.60757923e-07
Iter: 1122 loss: 3.60535893e-07
Iter: 1123 loss: 3.60497637e-07
Iter: 1124 loss: 3.60253097e-07
Iter: 1125 loss: 3.60436e-07
Iter: 1126 loss: 3.60094532e-07
Iter: 1127 loss: 3.59843767e-07
Iter: 1128 loss: 3.59606815e-07
Iter: 1129 loss: 3.59564041e-07
Iter: 1130 loss: 3.59209196e-07
Iter: 1131 loss: 3.61568084e-07
Iter: 1132 loss: 3.5913456e-07
Iter: 1133 loss: 3.587935e-07
Iter: 1134 loss: 3.60136738e-07
Iter: 1135 loss: 3.58689874e-07
Iter: 1136 loss: 3.58493367e-07
Iter: 1137 loss: 3.58411057e-07
Iter: 1138 loss: 3.58317521e-07
Iter: 1139 loss: 3.58028245e-07
Iter: 1140 loss: 3.61191894e-07
Iter: 1141 loss: 3.58019179e-07
Iter: 1142 loss: 3.57751247e-07
Iter: 1143 loss: 3.57810336e-07
Iter: 1144 loss: 3.57554171e-07
Iter: 1145 loss: 3.57340582e-07
Iter: 1146 loss: 3.58327554e-07
Iter: 1147 loss: 3.57293771e-07
Iter: 1148 loss: 3.57015949e-07
Iter: 1149 loss: 3.56888e-07
Iter: 1150 loss: 3.56734034e-07
Iter: 1151 loss: 3.5646508e-07
Iter: 1152 loss: 3.56489466e-07
Iter: 1153 loss: 3.5621116e-07
Iter: 1154 loss: 3.55839461e-07
Iter: 1155 loss: 3.57578443e-07
Iter: 1156 loss: 3.55775228e-07
Iter: 1157 loss: 3.5555729e-07
Iter: 1158 loss: 3.55562e-07
Iter: 1159 loss: 3.5533202e-07
Iter: 1160 loss: 3.55018301e-07
Iter: 1161 loss: 3.54985275e-07
Iter: 1162 loss: 3.54720328e-07
Iter: 1163 loss: 3.55012219e-07
Iter: 1164 loss: 3.54588963e-07
Iter: 1165 loss: 3.54351357e-07
Iter: 1166 loss: 3.54353574e-07
Iter: 1167 loss: 3.54117958e-07
Iter: 1168 loss: 3.53665683e-07
Iter: 1169 loss: 3.63345e-07
Iter: 1170 loss: 3.53678416e-07
Iter: 1171 loss: 3.53275652e-07
Iter: 1172 loss: 3.56747535e-07
Iter: 1173 loss: 3.53264454e-07
Iter: 1174 loss: 3.5297748e-07
Iter: 1175 loss: 3.56172649e-07
Iter: 1176 loss: 3.5297569e-07
Iter: 1177 loss: 3.52753119e-07
Iter: 1178 loss: 3.52410609e-07
Iter: 1179 loss: 3.52414219e-07
Iter: 1180 loss: 3.52213391e-07
Iter: 1181 loss: 3.52193894e-07
Iter: 1182 loss: 3.52011511e-07
Iter: 1183 loss: 3.51806364e-07
Iter: 1184 loss: 3.51796132e-07
Iter: 1185 loss: 3.51481248e-07
Iter: 1186 loss: 3.51665335e-07
Iter: 1187 loss: 3.5128221e-07
Iter: 1188 loss: 3.50928673e-07
Iter: 1189 loss: 3.52646509e-07
Iter: 1190 loss: 3.50884818e-07
Iter: 1191 loss: 3.50825218e-07
Iter: 1192 loss: 3.50765049e-07
Iter: 1193 loss: 3.50660628e-07
Iter: 1194 loss: 3.50415235e-07
Iter: 1195 loss: 3.53473752e-07
Iter: 1196 loss: 3.50404321e-07
Iter: 1197 loss: 3.50116665e-07
Iter: 1198 loss: 3.50915684e-07
Iter: 1199 loss: 3.50048595e-07
Iter: 1200 loss: 3.49768186e-07
Iter: 1201 loss: 3.51808126e-07
Iter: 1202 loss: 3.49755084e-07
Iter: 1203 loss: 3.49522622e-07
Iter: 1204 loss: 3.49221779e-07
Iter: 1205 loss: 3.49186934e-07
Iter: 1206 loss: 3.4886304e-07
Iter: 1207 loss: 3.5144285e-07
Iter: 1208 loss: 3.48859373e-07
Iter: 1209 loss: 3.48493472e-07
Iter: 1210 loss: 3.49233972e-07
Iter: 1211 loss: 3.48316803e-07
Iter: 1212 loss: 3.48071552e-07
Iter: 1213 loss: 3.48495121e-07
Iter: 1214 loss: 3.47942688e-07
Iter: 1215 loss: 3.47666912e-07
Iter: 1216 loss: 3.49662628e-07
Iter: 1217 loss: 3.47624308e-07
Iter: 1218 loss: 3.47412197e-07
Iter: 1219 loss: 3.47438856e-07
Iter: 1220 loss: 3.47243201e-07
Iter: 1221 loss: 3.47028958e-07
Iter: 1222 loss: 3.46799879e-07
Iter: 1223 loss: 3.46764182e-07
Iter: 1224 loss: 3.46550138e-07
Iter: 1225 loss: 3.46531522e-07
Iter: 1226 loss: 3.46349054e-07
Iter: 1227 loss: 3.47700905e-07
Iter: 1228 loss: 3.46348372e-07
Iter: 1229 loss: 3.4619967e-07
Iter: 1230 loss: 3.45890498e-07
Iter: 1231 loss: 3.49677379e-07
Iter: 1232 loss: 3.45851674e-07
Iter: 1233 loss: 3.4554543e-07
Iter: 1234 loss: 3.48293668e-07
Iter: 1235 loss: 3.45537558e-07
Iter: 1236 loss: 3.45206701e-07
Iter: 1237 loss: 3.46099682e-07
Iter: 1238 loss: 3.45096623e-07
Iter: 1239 loss: 3.44864134e-07
Iter: 1240 loss: 3.44699856e-07
Iter: 1241 loss: 3.44638806e-07
Iter: 1242 loss: 3.44405464e-07
Iter: 1243 loss: 3.44394437e-07
Iter: 1244 loss: 3.44185594e-07
Iter: 1245 loss: 3.43912575e-07
Iter: 1246 loss: 3.43890918e-07
Iter: 1247 loss: 3.43507168e-07
Iter: 1248 loss: 3.44668365e-07
Iter: 1249 loss: 3.43402633e-07
Iter: 1250 loss: 3.43071463e-07
Iter: 1251 loss: 3.47035098e-07
Iter: 1252 loss: 3.4308411e-07
Iter: 1253 loss: 3.42897351e-07
Iter: 1254 loss: 3.4253685e-07
Iter: 1255 loss: 3.47670863e-07
Iter: 1256 loss: 3.42511e-07
Iter: 1257 loss: 3.4219272e-07
Iter: 1258 loss: 3.44664784e-07
Iter: 1259 loss: 3.42167709e-07
Iter: 1260 loss: 3.41856861e-07
Iter: 1261 loss: 3.42353303e-07
Iter: 1262 loss: 3.41713161e-07
Iter: 1263 loss: 3.41695909e-07
Iter: 1264 loss: 3.41571308e-07
Iter: 1265 loss: 3.41471946e-07
Iter: 1266 loss: 3.41198813e-07
Iter: 1267 loss: 3.42881606e-07
Iter: 1268 loss: 3.41136825e-07
Iter: 1269 loss: 3.40813358e-07
Iter: 1270 loss: 3.41671694e-07
Iter: 1271 loss: 3.40700666e-07
Iter: 1272 loss: 3.40449276e-07
Iter: 1273 loss: 3.44261593e-07
Iter: 1274 loss: 3.40436401e-07
Iter: 1275 loss: 3.40236284e-07
Iter: 1276 loss: 3.40697682e-07
Iter: 1277 loss: 3.40156134e-07
Iter: 1278 loss: 3.39952237e-07
Iter: 1279 loss: 3.3968476e-07
Iter: 1280 loss: 3.39660289e-07
Iter: 1281 loss: 3.39606402e-07
Iter: 1282 loss: 3.39523694e-07
Iter: 1283 loss: 3.39423252e-07
Iter: 1284 loss: 3.39131645e-07
Iter: 1285 loss: 3.41705487e-07
Iter: 1286 loss: 3.3911283e-07
Iter: 1287 loss: 3.38809969e-07
Iter: 1288 loss: 3.40403034e-07
Iter: 1289 loss: 3.38776942e-07
Iter: 1290 loss: 3.38519385e-07
Iter: 1291 loss: 3.40523655e-07
Iter: 1292 loss: 3.38502417e-07
Iter: 1293 loss: 3.38266261e-07
Iter: 1294 loss: 3.37915054e-07
Iter: 1295 loss: 3.37912809e-07
Iter: 1296 loss: 3.37576836e-07
Iter: 1297 loss: 3.37754841e-07
Iter: 1298 loss: 3.37355914e-07
Iter: 1299 loss: 3.37253198e-07
Iter: 1300 loss: 3.37167279e-07
Iter: 1301 loss: 3.36977678e-07
Iter: 1302 loss: 3.37284916e-07
Iter: 1303 loss: 3.3687968e-07
Iter: 1304 loss: 3.36729329e-07
Iter: 1305 loss: 3.3658074e-07
Iter: 1306 loss: 3.36531798e-07
Iter: 1307 loss: 3.3627893e-07
Iter: 1308 loss: 3.36522419e-07
Iter: 1309 loss: 3.36139578e-07
Iter: 1310 loss: 3.35927723e-07
Iter: 1311 loss: 3.35921015e-07
Iter: 1312 loss: 3.35742811e-07
Iter: 1313 loss: 3.35556763e-07
Iter: 1314 loss: 3.35514642e-07
Iter: 1315 loss: 3.35227355e-07
Iter: 1316 loss: 3.35634809e-07
Iter: 1317 loss: 3.35087236e-07
Iter: 1318 loss: 3.34891297e-07
Iter: 1319 loss: 3.34887801e-07
Iter: 1320 loss: 3.34684e-07
Iter: 1321 loss: 3.34364472e-07
Iter: 1322 loss: 3.34360607e-07
Iter: 1323 loss: 3.34063941e-07
Iter: 1324 loss: 3.35335812e-07
Iter: 1325 loss: 3.34011304e-07
Iter: 1326 loss: 3.33756589e-07
Iter: 1327 loss: 3.36407311e-07
Iter: 1328 loss: 3.33757271e-07
Iter: 1329 loss: 3.33624143e-07
Iter: 1330 loss: 3.33670101e-07
Iter: 1331 loss: 3.33507387e-07
Iter: 1332 loss: 3.33343337e-07
Iter: 1333 loss: 3.33079015e-07
Iter: 1334 loss: 3.33067447e-07
Iter: 1335 loss: 3.33086234e-07
Iter: 1336 loss: 3.32909565e-07
Iter: 1337 loss: 3.32776239e-07
Iter: 1338 loss: 3.32512627e-07
Iter: 1339 loss: 3.37782069e-07
Iter: 1340 loss: 3.32500946e-07
Iter: 1341 loss: 3.32264506e-07
Iter: 1342 loss: 3.32273373e-07
Iter: 1343 loss: 3.32084937e-07
Iter: 1344 loss: 3.31774118e-07
Iter: 1345 loss: 3.33787227e-07
Iter: 1346 loss: 3.31733133e-07
Iter: 1347 loss: 3.31468158e-07
Iter: 1348 loss: 3.33210153e-07
Iter: 1349 loss: 3.31425326e-07
Iter: 1350 loss: 3.31150062e-07
Iter: 1351 loss: 3.31532306e-07
Iter: 1352 loss: 3.31018498e-07
Iter: 1353 loss: 3.30846831e-07
Iter: 1354 loss: 3.31041178e-07
Iter: 1355 loss: 3.30756393e-07
Iter: 1356 loss: 3.30532629e-07
Iter: 1357 loss: 3.31453e-07
Iter: 1358 loss: 3.30455521e-07
Iter: 1359 loss: 3.30155558e-07
Iter: 1360 loss: 3.30896796e-07
Iter: 1361 loss: 3.30040706e-07
Iter: 1362 loss: 3.29896267e-07
Iter: 1363 loss: 3.30046475e-07
Iter: 1364 loss: 3.29787269e-07
Iter: 1365 loss: 3.29544378e-07
Iter: 1366 loss: 3.30780381e-07
Iter: 1367 loss: 3.29509191e-07
Iter: 1368 loss: 3.29287587e-07
Iter: 1369 loss: 3.28965e-07
Iter: 1370 loss: 3.28961733e-07
Iter: 1371 loss: 3.28614448e-07
Iter: 1372 loss: 3.30027603e-07
Iter: 1373 loss: 3.28525061e-07
Iter: 1374 loss: 3.2843684e-07
Iter: 1375 loss: 3.28379031e-07
Iter: 1376 loss: 3.28241242e-07
Iter: 1377 loss: 3.27962084e-07
Iter: 1378 loss: 3.32215194e-07
Iter: 1379 loss: 3.27937073e-07
Iter: 1380 loss: 3.27727037e-07
Iter: 1381 loss: 3.28123264e-07
Iter: 1382 loss: 3.27605392e-07
Iter: 1383 loss: 3.27475448e-07
Iter: 1384 loss: 3.27455666e-07
Iter: 1385 loss: 3.27332202e-07
Iter: 1386 loss: 3.27459389e-07
Iter: 1387 loss: 3.27268424e-07
Iter: 1388 loss: 3.27103493e-07
Iter: 1389 loss: 3.26943677e-07
Iter: 1390 loss: 3.26894963e-07
Iter: 1391 loss: 3.2679452e-07
Iter: 1392 loss: 3.26775591e-07
Iter: 1393 loss: 3.2664866e-07
Iter: 1394 loss: 3.26423333e-07
Iter: 1395 loss: 3.26430325e-07
Iter: 1396 loss: 3.26100349e-07
Iter: 1397 loss: 3.26683875e-07
Iter: 1398 loss: 3.2596455e-07
Iter: 1399 loss: 3.25702388e-07
Iter: 1400 loss: 3.2970587e-07
Iter: 1401 loss: 3.25702842e-07
Iter: 1402 loss: 3.25542487e-07
Iter: 1403 loss: 3.25321082e-07
Iter: 1404 loss: 3.25321139e-07
Iter: 1405 loss: 3.25081032e-07
Iter: 1406 loss: 3.25886162e-07
Iter: 1407 loss: 3.25018732e-07
Iter: 1408 loss: 3.24854909e-07
Iter: 1409 loss: 3.24855733e-07
Iter: 1410 loss: 3.24747191e-07
Iter: 1411 loss: 3.24557277e-07
Iter: 1412 loss: 3.24566543e-07
Iter: 1413 loss: 3.2436111e-07
Iter: 1414 loss: 3.24215137e-07
Iter: 1415 loss: 3.24154541e-07
Iter: 1416 loss: 3.23872e-07
Iter: 1417 loss: 3.26555323e-07
Iter: 1418 loss: 3.23873621e-07
Iter: 1419 loss: 3.23657e-07
Iter: 1420 loss: 3.25703581e-07
Iter: 1421 loss: 3.23630161e-07
Iter: 1422 loss: 3.23507294e-07
Iter: 1423 loss: 3.23250902e-07
Iter: 1424 loss: 3.23250674e-07
Iter: 1425 loss: 3.23032594e-07
Iter: 1426 loss: 3.2303285e-07
Iter: 1427 loss: 3.22848763e-07
Iter: 1428 loss: 3.23051069e-07
Iter: 1429 loss: 3.22747837e-07
Iter: 1430 loss: 3.22608798e-07
Iter: 1431 loss: 3.22526e-07
Iter: 1432 loss: 3.22446681e-07
Iter: 1433 loss: 3.22140238e-07
Iter: 1434 loss: 3.23557572e-07
Iter: 1435 loss: 3.22071e-07
Iter: 1436 loss: 3.21889956e-07
Iter: 1437 loss: 3.22024903e-07
Iter: 1438 loss: 3.21782522e-07
Iter: 1439 loss: 3.21594655e-07
Iter: 1440 loss: 3.22800076e-07
Iter: 1441 loss: 3.21589937e-07
Iter: 1442 loss: 3.21425858e-07
Iter: 1443 loss: 3.22597828e-07
Iter: 1444 loss: 3.21412017e-07
Iter: 1445 loss: 3.21282073e-07
Iter: 1446 loss: 3.21017978e-07
Iter: 1447 loss: 3.24463599e-07
Iter: 1448 loss: 3.20988022e-07
Iter: 1449 loss: 3.20711877e-07
Iter: 1450 loss: 3.21286109e-07
Iter: 1451 loss: 3.20561469e-07
Iter: 1452 loss: 3.20285608e-07
Iter: 1453 loss: 3.23284496e-07
Iter: 1454 loss: 3.2029152e-07
Iter: 1455 loss: 3.20108228e-07
Iter: 1456 loss: 3.21946487e-07
Iter: 1457 loss: 3.20094927e-07
Iter: 1458 loss: 3.19946395e-07
Iter: 1459 loss: 3.19808208e-07
Iter: 1460 loss: 3.19784647e-07
Iter: 1461 loss: 3.19662149e-07
Iter: 1462 loss: 3.19673063e-07
Iter: 1463 loss: 3.19537207e-07
Iter: 1464 loss: 3.19208596e-07
Iter: 1465 loss: 3.21640243e-07
Iter: 1466 loss: 3.19134358e-07
Iter: 1467 loss: 3.18938305e-07
Iter: 1468 loss: 3.18930745e-07
Iter: 1469 loss: 3.18761778e-07
Iter: 1470 loss: 3.19760289e-07
Iter: 1471 loss: 3.18746629e-07
Iter: 1472 loss: 3.18592072e-07
Iter: 1473 loss: 3.18350516e-07
Iter: 1474 loss: 3.18338834e-07
Iter: 1475 loss: 3.18231429e-07
Iter: 1476 loss: 3.1822097e-07
Iter: 1477 loss: 3.18061979e-07
Iter: 1478 loss: 3.17925185e-07
Iter: 1479 loss: 3.17899719e-07
Iter: 1480 loss: 3.1769315e-07
Iter: 1481 loss: 3.17408137e-07
Iter: 1482 loss: 3.17395e-07
Iter: 1483 loss: 3.17094191e-07
Iter: 1484 loss: 3.20321135e-07
Iter: 1485 loss: 3.17085949e-07
Iter: 1486 loss: 3.16858859e-07
Iter: 1487 loss: 3.16954925e-07
Iter: 1488 loss: 3.16688215e-07
Iter: 1489 loss: 3.16527036e-07
Iter: 1490 loss: 3.16503787e-07
Iter: 1491 loss: 3.16418607e-07
Iter: 1492 loss: 3.16304835e-07
Iter: 1493 loss: 3.16299776e-07
Iter: 1494 loss: 3.16136e-07
Iter: 1495 loss: 3.17958524e-07
Iter: 1496 loss: 3.16135697e-07
Iter: 1497 loss: 3.16054752e-07
Iter: 1498 loss: 3.15932425e-07
Iter: 1499 loss: 3.15943282e-07
Iter: 1500 loss: 3.15747826e-07
Iter: 1501 loss: 3.16384359e-07
Iter: 1502 loss: 3.15720285e-07
Iter: 1503 loss: 3.15505446e-07
Iter: 1504 loss: 3.15948284e-07
Iter: 1505 loss: 3.15434363e-07
Iter: 1506 loss: 3.15286371e-07
Iter: 1507 loss: 3.15109446e-07
Iter: 1508 loss: 3.15102199e-07
Iter: 1509 loss: 3.14950341e-07
Iter: 1510 loss: 3.14918736e-07
Iter: 1511 loss: 3.14796608e-07
Iter: 1512 loss: 3.14540586e-07
Iter: 1513 loss: 3.1958416e-07
Iter: 1514 loss: 3.14556303e-07
Iter: 1515 loss: 3.14310796e-07
Iter: 1516 loss: 3.1494011e-07
Iter: 1517 loss: 3.14243721e-07
Iter: 1518 loss: 3.14025726e-07
Iter: 1519 loss: 3.14499033e-07
Iter: 1520 loss: 3.13945947e-07
Iter: 1521 loss: 3.13779537e-07
Iter: 1522 loss: 3.13777093e-07
Iter: 1523 loss: 3.13626572e-07
Iter: 1524 loss: 3.1363345e-07
Iter: 1525 loss: 3.13513198e-07
Iter: 1526 loss: 3.13330645e-07
Iter: 1527 loss: 3.14707e-07
Iter: 1528 loss: 3.13321408e-07
Iter: 1529 loss: 3.13180152e-07
Iter: 1530 loss: 3.13576209e-07
Iter: 1531 loss: 3.13153862e-07
Iter: 1532 loss: 3.13066067e-07
Iter: 1533 loss: 3.12902046e-07
Iter: 1534 loss: 3.12904632e-07
Iter: 1535 loss: 3.12687916e-07
Iter: 1536 loss: 3.15478871e-07
Iter: 1537 loss: 3.12696869e-07
Iter: 1538 loss: 3.12572922e-07
Iter: 1539 loss: 3.12414926e-07
Iter: 1540 loss: 3.12394434e-07
Iter: 1541 loss: 3.12187183e-07
Iter: 1542 loss: 3.12784664e-07
Iter: 1543 loss: 3.12110615e-07
Iter: 1544 loss: 3.11911663e-07
Iter: 1545 loss: 3.11915528e-07
Iter: 1546 loss: 3.11812926e-07
Iter: 1547 loss: 3.11539736e-07
Iter: 1548 loss: 3.14721319e-07
Iter: 1549 loss: 3.11522911e-07
Iter: 1550 loss: 3.11264387e-07
Iter: 1551 loss: 3.11382e-07
Iter: 1552 loss: 3.11070664e-07
Iter: 1553 loss: 3.10790881e-07
Iter: 1554 loss: 3.13626884e-07
Iter: 1555 loss: 3.10785879e-07
Iter: 1556 loss: 3.10552082e-07
Iter: 1557 loss: 3.12383719e-07
Iter: 1558 loss: 3.10559614e-07
Iter: 1559 loss: 3.10358473e-07
Iter: 1560 loss: 3.10674238e-07
Iter: 1561 loss: 3.10292251e-07
Iter: 1562 loss: 3.10120441e-07
Iter: 1563 loss: 3.10816404e-07
Iter: 1564 loss: 3.10082385e-07
Iter: 1565 loss: 3.09883546e-07
Iter: 1566 loss: 3.09733394e-07
Iter: 1567 loss: 3.09651938e-07
Iter: 1568 loss: 3.09477429e-07
Iter: 1569 loss: 3.10366204e-07
Iter: 1570 loss: 3.09464184e-07
Iter: 1571 loss: 3.09277596e-07
Iter: 1572 loss: 3.10020255e-07
Iter: 1573 loss: 3.09241699e-07
Iter: 1574 loss: 3.09096833e-07
Iter: 1575 loss: 3.09126278e-07
Iter: 1576 loss: 3.08990053e-07
Iter: 1577 loss: 3.08824895e-07
Iter: 1578 loss: 3.09115876e-07
Iter: 1579 loss: 3.08778851e-07
Iter: 1580 loss: 3.08690375e-07
Iter: 1581 loss: 3.08674032e-07
Iter: 1582 loss: 3.08587801e-07
Iter: 1583 loss: 3.08386e-07
Iter: 1584 loss: 3.0955735e-07
Iter: 1585 loss: 3.08314611e-07
Iter: 1586 loss: 3.08133167e-07
Iter: 1587 loss: 3.09005941e-07
Iter: 1588 loss: 3.08092e-07
Iter: 1589 loss: 3.07824564e-07
Iter: 1590 loss: 3.07655853e-07
Iter: 1591 loss: 3.07573202e-07
Iter: 1592 loss: 3.07469804e-07
Iter: 1593 loss: 3.07348841e-07
Iter: 1594 loss: 3.07214577e-07
Iter: 1595 loss: 3.06933828e-07
Iter: 1596 loss: 3.06934453e-07
Iter: 1597 loss: 3.0678558e-07
Iter: 1598 loss: 3.06779498e-07
Iter: 1599 loss: 3.0660695e-07
Iter: 1600 loss: 3.06484935e-07
Iter: 1601 loss: 3.06436391e-07
Iter: 1602 loss: 3.06235165e-07
Iter: 1603 loss: 3.07764253e-07
Iter: 1604 loss: 3.06196512e-07
Iter: 1605 loss: 3.06095728e-07
Iter: 1606 loss: 3.06296499e-07
Iter: 1607 loss: 3.06013163e-07
Iter: 1608 loss: 3.05917126e-07
Iter: 1609 loss: 3.05773938e-07
Iter: 1610 loss: 3.05776837e-07
Iter: 1611 loss: 3.05618585e-07
Iter: 1612 loss: 3.06473851e-07
Iter: 1613 loss: 3.05600395e-07
Iter: 1614 loss: 3.05437396e-07
Iter: 1615 loss: 3.06341065e-07
Iter: 1616 loss: 3.05430376e-07
Iter: 1617 loss: 3.05308106e-07
Iter: 1618 loss: 3.05190099e-07
Iter: 1619 loss: 3.0514741e-07
Iter: 1620 loss: 3.0496966e-07
Iter: 1621 loss: 3.04885077e-07
Iter: 1622 loss: 3.04792195e-07
Iter: 1623 loss: 3.04511531e-07
Iter: 1624 loss: 3.05688616e-07
Iter: 1625 loss: 3.0446941e-07
Iter: 1626 loss: 3.04310674e-07
Iter: 1627 loss: 3.04297373e-07
Iter: 1628 loss: 3.04181356e-07
Iter: 1629 loss: 3.04325852e-07
Iter: 1630 loss: 3.04094755e-07
Iter: 1631 loss: 3.0396194e-07
Iter: 1632 loss: 3.04314938e-07
Iter: 1633 loss: 3.03913509e-07
Iter: 1634 loss: 3.03757332e-07
Iter: 1635 loss: 3.04243486e-07
Iter: 1636 loss: 3.03687813e-07
Iter: 1637 loss: 3.03591264e-07
Iter: 1638 loss: 3.03478402e-07
Iter: 1639 loss: 3.03454215e-07
Iter: 1640 loss: 3.03251056e-07
Iter: 1641 loss: 3.06018052e-07
Iter: 1642 loss: 3.03242956e-07
Iter: 1643 loss: 3.03144304e-07
Iter: 1644 loss: 3.02898428e-07
Iter: 1645 loss: 3.05132119e-07
Iter: 1646 loss: 3.02877055e-07
Iter: 1647 loss: 3.02914941e-07
Iter: 1648 loss: 3.027547e-07
Iter: 1649 loss: 3.02660851e-07
Iter: 1650 loss: 3.0260378e-07
Iter: 1651 loss: 3.02554525e-07
Iter: 1652 loss: 3.02428958e-07
Iter: 1653 loss: 3.02320245e-07
Iter: 1654 loss: 3.02278494e-07
Iter: 1655 loss: 3.02064393e-07
Iter: 1656 loss: 3.02561347e-07
Iter: 1657 loss: 3.01970516e-07
Iter: 1658 loss: 3.01813344e-07
Iter: 1659 loss: 3.03996131e-07
Iter: 1660 loss: 3.01812861e-07
Iter: 1661 loss: 3.01665807e-07
Iter: 1662 loss: 3.02244132e-07
Iter: 1663 loss: 3.01635197e-07
Iter: 1664 loss: 3.01474529e-07
Iter: 1665 loss: 3.01483681e-07
Iter: 1666 loss: 3.01326e-07
Iter: 1667 loss: 3.01180307e-07
Iter: 1668 loss: 3.03026923e-07
Iter: 1669 loss: 3.01176328e-07
Iter: 1670 loss: 3.01072816e-07
Iter: 1671 loss: 3.00799059e-07
Iter: 1672 loss: 3.03635431e-07
Iter: 1673 loss: 3.00781124e-07
Iter: 1674 loss: 3.00440263e-07
Iter: 1675 loss: 3.01595747e-07
Iter: 1676 loss: 3.00361819e-07
Iter: 1677 loss: 3.00205386e-07
Iter: 1678 loss: 3.001864e-07
Iter: 1679 loss: 3.0003514e-07
Iter: 1680 loss: 3.00138765e-07
Iter: 1681 loss: 2.99962835e-07
Iter: 1682 loss: 2.99812712e-07
Iter: 1683 loss: 2.99805663e-07
Iter: 1684 loss: 2.99693795e-07
Iter: 1685 loss: 2.99615522e-07
Iter: 1686 loss: 2.99582211e-07
Iter: 1687 loss: 2.99518319e-07
Iter: 1688 loss: 2.99384226e-07
Iter: 1689 loss: 3.01316959e-07
Iter: 1690 loss: 2.99339433e-07
Iter: 1691 loss: 2.99204146e-07
Iter: 1692 loss: 2.99654062e-07
Iter: 1693 loss: 2.9913673e-07
Iter: 1694 loss: 2.99004654e-07
Iter: 1695 loss: 3.00220194e-07
Iter: 1696 loss: 2.99007979e-07
Iter: 1697 loss: 2.98861323e-07
Iter: 1698 loss: 2.99307942e-07
Iter: 1699 loss: 2.98830884e-07
Iter: 1700 loss: 2.98719328e-07
Iter: 1701 loss: 2.99161115e-07
Iter: 1702 loss: 2.98697955e-07
Iter: 1703 loss: 2.98588759e-07
Iter: 1704 loss: 2.98367837e-07
Iter: 1705 loss: 3.03895234e-07
Iter: 1706 loss: 2.98363432e-07
Iter: 1707 loss: 2.98107437e-07
Iter: 1708 loss: 2.98631079e-07
Iter: 1709 loss: 2.98012338e-07
Iter: 1710 loss: 2.97802274e-07
Iter: 1711 loss: 2.99583e-07
Iter: 1712 loss: 2.97781554e-07
Iter: 1713 loss: 2.97632141e-07
Iter: 1714 loss: 2.98696023e-07
Iter: 1715 loss: 2.9761361e-07
Iter: 1716 loss: 2.97486679e-07
Iter: 1717 loss: 2.97316831e-07
Iter: 1718 loss: 2.97311544e-07
Iter: 1719 loss: 2.97152468e-07
Iter: 1720 loss: 2.97145107e-07
Iter: 1721 loss: 2.97026077e-07
Iter: 1722 loss: 2.97328882e-07
Iter: 1723 loss: 2.96969688e-07
Iter: 1724 loss: 2.96873623e-07
Iter: 1725 loss: 2.96754877e-07
Iter: 1726 loss: 2.96737511e-07
Iter: 1727 loss: 2.96593583e-07
Iter: 1728 loss: 2.97420883e-07
Iter: 1729 loss: 2.96573489e-07
Iter: 1730 loss: 2.96465515e-07
Iter: 1731 loss: 2.96463895e-07
Iter: 1732 loss: 2.96387839e-07
Iter: 1733 loss: 2.96262954e-07
Iter: 1734 loss: 2.96260282e-07
Iter: 1735 loss: 2.96058715e-07
Iter: 1736 loss: 2.9693075e-07
Iter: 1737 loss: 2.96025632e-07
Iter: 1738 loss: 2.95916493e-07
Iter: 1739 loss: 2.95789135e-07
Iter: 1740 loss: 2.95784844e-07
Iter: 1741 loss: 2.95552383e-07
Iter: 1742 loss: 2.96139604e-07
Iter: 1743 loss: 2.95473569e-07
Iter: 1744 loss: 2.95274788e-07
Iter: 1745 loss: 2.97566373e-07
Iter: 1746 loss: 2.95268762e-07
Iter: 1747 loss: 2.95115228e-07
Iter: 1748 loss: 2.9532552e-07
Iter: 1749 loss: 2.9504605e-07
Iter: 1750 loss: 2.94896722e-07
Iter: 1751 loss: 2.95017827e-07
Iter: 1752 loss: 2.94831182e-07
Iter: 1753 loss: 2.94639563e-07
Iter: 1754 loss: 2.96563883e-07
Iter: 1755 loss: 2.94641893e-07
Iter: 1756 loss: 2.94541849e-07
Iter: 1757 loss: 2.94364554e-07
Iter: 1758 loss: 2.94358898e-07
Iter: 1759 loss: 2.94151107e-07
Iter: 1760 loss: 2.94512802e-07
Iter: 1761 loss: 2.94087727e-07
Iter: 1762 loss: 2.93942378e-07
Iter: 1763 loss: 2.9395045e-07
Iter: 1764 loss: 2.93802714e-07
Iter: 1765 loss: 2.93731972e-07
Iter: 1766 loss: 2.93671633e-07
Iter: 1767 loss: 2.9352924e-07
Iter: 1768 loss: 2.95419937e-07
Iter: 1769 loss: 2.93526256e-07
Iter: 1770 loss: 2.93420385e-07
Iter: 1771 loss: 2.93249e-07
Iter: 1772 loss: 2.93238315e-07
Iter: 1773 loss: 2.93049e-07
Iter: 1774 loss: 2.93573464e-07
Iter: 1775 loss: 2.93010714e-07
Iter: 1776 loss: 2.92857692e-07
Iter: 1777 loss: 2.9418382e-07
Iter: 1778 loss: 2.92852206e-07
Iter: 1779 loss: 2.92753e-07
Iter: 1780 loss: 2.93493486e-07
Iter: 1781 loss: 2.92736786e-07
Iter: 1782 loss: 2.92644984e-07
Iter: 1783 loss: 2.92491393e-07
Iter: 1784 loss: 2.95846377e-07
Iter: 1785 loss: 2.92512254e-07
Iter: 1786 loss: 2.92409482e-07
Iter: 1787 loss: 2.92379468e-07
Iter: 1788 loss: 2.92305259e-07
Iter: 1789 loss: 2.92190578e-07
Iter: 1790 loss: 2.92190975e-07
Iter: 1791 loss: 2.92005467e-07
Iter: 1792 loss: 2.918066e-07
Iter: 1793 loss: 2.91782101e-07
Iter: 1794 loss: 2.91552368e-07
Iter: 1795 loss: 2.94631e-07
Iter: 1796 loss: 2.91543131e-07
Iter: 1797 loss: 2.91345032e-07
Iter: 1798 loss: 2.920101e-07
Iter: 1799 loss: 2.91276194e-07
Iter: 1800 loss: 2.91147956e-07
Iter: 1801 loss: 2.91766e-07
Iter: 1802 loss: 2.91111064e-07
Iter: 1803 loss: 2.90952215e-07
Iter: 1804 loss: 2.91258885e-07
Iter: 1805 loss: 2.90887726e-07
Iter: 1806 loss: 2.90795356e-07
Iter: 1807 loss: 2.90768185e-07
Iter: 1808 loss: 2.90735159e-07
Iter: 1809 loss: 2.90577589e-07
Iter: 1810 loss: 2.90721459e-07
Iter: 1811 loss: 2.905104e-07
Iter: 1812 loss: 2.90355388e-07
Iter: 1813 loss: 2.92593541e-07
Iter: 1814 loss: 2.90360617e-07
Iter: 1815 loss: 2.90226438e-07
Iter: 1816 loss: 2.90122045e-07
Iter: 1817 loss: 2.90086462e-07
Iter: 1818 loss: 2.89973286e-07
Iter: 1819 loss: 2.91109416e-07
Iter: 1820 loss: 2.8996655e-07
Iter: 1821 loss: 2.89812533e-07
Iter: 1822 loss: 2.89954727e-07
Iter: 1823 loss: 2.89719196e-07
Iter: 1824 loss: 2.89557192e-07
Iter: 1825 loss: 2.89559779e-07
Iter: 1826 loss: 2.89440749e-07
Iter: 1827 loss: 2.89291506e-07
Iter: 1828 loss: 2.89674517e-07
Iter: 1829 loss: 2.89234606e-07
Iter: 1830 loss: 2.89100313e-07
Iter: 1831 loss: 2.89114439e-07
Iter: 1832 loss: 2.89004134e-07
Iter: 1833 loss: 2.88982676e-07
Iter: 1834 loss: 2.8891796e-07
Iter: 1835 loss: 2.8880811e-07
Iter: 1836 loss: 2.8998258e-07
Iter: 1837 loss: 2.88811151e-07
Iter: 1838 loss: 2.88732849e-07
Iter: 1839 loss: 2.88645708e-07
Iter: 1840 loss: 2.88646277e-07
Iter: 1841 loss: 2.8846739e-07
Iter: 1842 loss: 2.88482852e-07
Iter: 1843 loss: 2.88339663e-07
Iter: 1844 loss: 2.88211595e-07
Iter: 1845 loss: 2.88217535e-07
Iter: 1846 loss: 2.88086028e-07
Iter: 1847 loss: 2.88245275e-07
Iter: 1848 loss: 2.88028389e-07
Iter: 1849 loss: 2.87896341e-07
Iter: 1850 loss: 2.87965065e-07
Iter: 1851 loss: 2.87816022e-07
Iter: 1852 loss: 2.8770441e-07
Iter: 1853 loss: 2.87691478e-07
Iter: 1854 loss: 2.8762426e-07
Iter: 1855 loss: 2.87513899e-07
Iter: 1856 loss: 2.87522852e-07
Iter: 1857 loss: 2.87392623e-07
Iter: 1858 loss: 2.87422097e-07
Iter: 1859 loss: 2.87272684e-07
Iter: 1860 loss: 2.87167893e-07
Iter: 1861 loss: 2.87160134e-07
Iter: 1862 loss: 2.87060288e-07
Iter: 1863 loss: 2.8700174e-07
Iter: 1864 loss: 2.86957061e-07
Iter: 1865 loss: 2.86811655e-07
Iter: 1866 loss: 2.87759178e-07
Iter: 1867 loss: 2.86805403e-07
Iter: 1868 loss: 2.866594e-07
Iter: 1869 loss: 2.86712122e-07
Iter: 1870 loss: 2.86553e-07
Iter: 1871 loss: 2.86390787e-07
Iter: 1872 loss: 2.86302225e-07
Iter: 1873 loss: 2.86226026e-07
Iter: 1874 loss: 2.86043758e-07
Iter: 1875 loss: 2.87629518e-07
Iter: 1876 loss: 2.86047907e-07
Iter: 1877 loss: 2.85891247e-07
Iter: 1878 loss: 2.86897148e-07
Iter: 1879 loss: 2.85865298e-07
Iter: 1880 loss: 2.85753913e-07
Iter: 1881 loss: 2.85749223e-07
Iter: 1882 loss: 2.8565114e-07
Iter: 1883 loss: 2.85585713e-07
Iter: 1884 loss: 2.85578665e-07
Iter: 1885 loss: 2.8550275e-07
Iter: 1886 loss: 2.85420271e-07
Iter: 1887 loss: 2.85409499e-07
Iter: 1888 loss: 2.85299e-07
Iter: 1889 loss: 2.85241782e-07
Iter: 1890 loss: 2.85211797e-07
Iter: 1891 loss: 2.85092881e-07
Iter: 1892 loss: 2.8508623e-07
Iter: 1893 loss: 2.84994371e-07
Iter: 1894 loss: 2.85225269e-07
Iter: 1895 loss: 2.84962653e-07
Iter: 1896 loss: 2.84876251e-07
Iter: 1897 loss: 2.84900779e-07
Iter: 1898 loss: 2.84796e-07
Iter: 1899 loss: 2.84641146e-07
Iter: 1900 loss: 2.85129232e-07
Iter: 1901 loss: 2.84600674e-07
Iter: 1902 loss: 2.84459873e-07
Iter: 1903 loss: 2.84376569e-07
Iter: 1904 loss: 2.84321118e-07
Iter: 1905 loss: 2.84136149e-07
Iter: 1906 loss: 2.84522969e-07
Iter: 1907 loss: 2.84074275e-07
Iter: 1908 loss: 2.83899425e-07
Iter: 1909 loss: 2.86460136e-07
Iter: 1910 loss: 2.83905138e-07
Iter: 1911 loss: 2.83777581e-07
Iter: 1912 loss: 2.83832236e-07
Iter: 1913 loss: 2.83684926e-07
Iter: 1914 loss: 2.8359176e-07
Iter: 1915 loss: 2.84379837e-07
Iter: 1916 loss: 2.83577776e-07
Iter: 1917 loss: 2.83454028e-07
Iter: 1918 loss: 2.83599206e-07
Iter: 1919 loss: 2.83389056e-07
Iter: 1920 loss: 2.8329444e-07
Iter: 1921 loss: 2.83183908e-07
Iter: 1922 loss: 2.83160205e-07
Iter: 1923 loss: 2.83053055e-07
Iter: 1924 loss: 2.83053851e-07
Iter: 1925 loss: 2.82957728e-07
Iter: 1926 loss: 2.83292451e-07
Iter: 1927 loss: 2.82950822e-07
Iter: 1928 loss: 2.82855524e-07
Iter: 1929 loss: 2.8284029e-07
Iter: 1930 loss: 2.82776568e-07
Iter: 1931 loss: 2.82644635e-07
Iter: 1932 loss: 2.83568284e-07
Iter: 1933 loss: 2.82646567e-07
Iter: 1934 loss: 2.82559427e-07
Iter: 1935 loss: 2.82506818e-07
Iter: 1936 loss: 2.82485075e-07
Iter: 1937 loss: 2.82362606e-07
Iter: 1938 loss: 2.82405495e-07
Iter: 1939 loss: 2.82279757e-07
Iter: 1940 loss: 2.8213924e-07
Iter: 1941 loss: 2.82149614e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6
+ date
Mon Oct 26 12:36:20 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.6_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.6_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.6_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.6 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi1.6_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9b399d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9ad9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9ad9d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9b9ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9b90510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9a53730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9a27f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa99b4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa99c6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa99a5d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa995ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa994ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa99357b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9935d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa98bca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa98bc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa98a5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9867a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9830730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa97fcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa97fb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa97ed598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9774840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9768950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa97687b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa97452f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9721b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa96d0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa96d08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9678730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa9698950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa5512268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa5512378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa54e8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa54cbae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa54820d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.21510593e-05
Iter: 2 loss: 1.14575814e-05
Iter: 3 loss: 9.97023926e-06
Iter: 4 loss: 9.48242268e-06
Iter: 5 loss: 1.0051077e-05
Iter: 6 loss: 9.22262825e-06
Iter: 7 loss: 8.74789475e-06
Iter: 8 loss: 8.66354185e-06
Iter: 9 loss: 8.34132243e-06
Iter: 10 loss: 7.74153705e-06
Iter: 11 loss: 9.00537088e-06
Iter: 12 loss: 7.50423942e-06
Iter: 13 loss: 7.20824573e-06
Iter: 14 loss: 1.00076604e-05
Iter: 15 loss: 7.19644049e-06
Iter: 16 loss: 6.94390656e-06
Iter: 17 loss: 7.96746917e-06
Iter: 18 loss: 6.88791897e-06
Iter: 19 loss: 6.69663586e-06
Iter: 20 loss: 6.79735513e-06
Iter: 21 loss: 6.5700774e-06
Iter: 22 loss: 6.20523588e-06
Iter: 23 loss: 6.3791e-06
Iter: 24 loss: 5.95952042e-06
Iter: 25 loss: 5.67404e-06
Iter: 26 loss: 5.5247956e-06
Iter: 27 loss: 5.39389703e-06
Iter: 28 loss: 5.05075423e-06
Iter: 29 loss: 8.3641371e-06
Iter: 30 loss: 5.03807769e-06
Iter: 31 loss: 4.76162495e-06
Iter: 32 loss: 4.76446712e-06
Iter: 33 loss: 4.54193e-06
Iter: 34 loss: 4.31291346e-06
Iter: 35 loss: 6.40314e-06
Iter: 36 loss: 4.30261844e-06
Iter: 37 loss: 4.32101842e-06
Iter: 38 loss: 4.24084919e-06
Iter: 39 loss: 4.19376556e-06
Iter: 40 loss: 4.12555164e-06
Iter: 41 loss: 4.12351937e-06
Iter: 42 loss: 4.0248774e-06
Iter: 43 loss: 4.17276533e-06
Iter: 44 loss: 3.97781423e-06
Iter: 45 loss: 3.86414195e-06
Iter: 46 loss: 4.3183054e-06
Iter: 47 loss: 3.83841052e-06
Iter: 48 loss: 3.77057e-06
Iter: 49 loss: 3.65110827e-06
Iter: 50 loss: 3.65111646e-06
Iter: 51 loss: 3.50112714e-06
Iter: 52 loss: 3.50120354e-06
Iter: 53 loss: 3.43160696e-06
Iter: 54 loss: 3.49352399e-06
Iter: 55 loss: 3.39110215e-06
Iter: 56 loss: 3.2956325e-06
Iter: 57 loss: 3.55816655e-06
Iter: 58 loss: 3.26475447e-06
Iter: 59 loss: 3.179337e-06
Iter: 60 loss: 3.30654871e-06
Iter: 61 loss: 3.13844771e-06
Iter: 62 loss: 3.06542211e-06
Iter: 63 loss: 2.95589462e-06
Iter: 64 loss: 2.95350469e-06
Iter: 65 loss: 2.87400394e-06
Iter: 66 loss: 2.87288321e-06
Iter: 67 loss: 2.81549683e-06
Iter: 68 loss: 2.91176e-06
Iter: 69 loss: 2.78952803e-06
Iter: 70 loss: 2.72534453e-06
Iter: 71 loss: 3.00315469e-06
Iter: 72 loss: 2.71211729e-06
Iter: 73 loss: 2.64011987e-06
Iter: 74 loss: 3.34887568e-06
Iter: 75 loss: 2.63764923e-06
Iter: 76 loss: 2.61631317e-06
Iter: 77 loss: 2.57514557e-06
Iter: 78 loss: 3.42736735e-06
Iter: 79 loss: 2.57487545e-06
Iter: 80 loss: 2.51392612e-06
Iter: 81 loss: 2.79957476e-06
Iter: 82 loss: 2.50273615e-06
Iter: 83 loss: 2.44578359e-06
Iter: 84 loss: 2.45335673e-06
Iter: 85 loss: 2.40258419e-06
Iter: 86 loss: 2.35139441e-06
Iter: 87 loss: 2.86447403e-06
Iter: 88 loss: 2.34974732e-06
Iter: 89 loss: 2.31663125e-06
Iter: 90 loss: 2.60327533e-06
Iter: 91 loss: 2.31484569e-06
Iter: 92 loss: 2.290682e-06
Iter: 93 loss: 2.24103201e-06
Iter: 94 loss: 3.13156261e-06
Iter: 95 loss: 2.24009273e-06
Iter: 96 loss: 2.20000152e-06
Iter: 97 loss: 2.1982919e-06
Iter: 98 loss: 2.18232117e-06
Iter: 99 loss: 2.14172155e-06
Iter: 100 loss: 2.49208529e-06
Iter: 101 loss: 2.13508179e-06
Iter: 102 loss: 2.09398513e-06
Iter: 103 loss: 2.62108e-06
Iter: 104 loss: 2.09368136e-06
Iter: 105 loss: 2.0600653e-06
Iter: 106 loss: 2.01951434e-06
Iter: 107 loss: 2.01546e-06
Iter: 108 loss: 1.96878591e-06
Iter: 109 loss: 2.32292064e-06
Iter: 110 loss: 1.96521569e-06
Iter: 111 loss: 1.93550227e-06
Iter: 112 loss: 1.93544747e-06
Iter: 113 loss: 1.90511923e-06
Iter: 114 loss: 2.00472823e-06
Iter: 115 loss: 1.8967952e-06
Iter: 116 loss: 1.87645492e-06
Iter: 117 loss: 1.84055625e-06
Iter: 118 loss: 1.84049907e-06
Iter: 119 loss: 1.82482825e-06
Iter: 120 loss: 1.82452072e-06
Iter: 121 loss: 1.80674624e-06
Iter: 122 loss: 1.80058601e-06
Iter: 123 loss: 1.79054359e-06
Iter: 124 loss: 1.76882861e-06
Iter: 125 loss: 1.91970116e-06
Iter: 126 loss: 1.76674394e-06
Iter: 127 loss: 1.75111677e-06
Iter: 128 loss: 1.84489011e-06
Iter: 129 loss: 1.74919865e-06
Iter: 130 loss: 1.73474587e-06
Iter: 131 loss: 1.70440148e-06
Iter: 132 loss: 2.20665606e-06
Iter: 133 loss: 1.70349495e-06
Iter: 134 loss: 1.68075576e-06
Iter: 135 loss: 1.98170346e-06
Iter: 136 loss: 1.68069232e-06
Iter: 137 loss: 1.65811377e-06
Iter: 138 loss: 1.69814143e-06
Iter: 139 loss: 1.64832579e-06
Iter: 140 loss: 1.62801928e-06
Iter: 141 loss: 1.60763454e-06
Iter: 142 loss: 1.60350646e-06
Iter: 143 loss: 1.58181399e-06
Iter: 144 loss: 1.91264871e-06
Iter: 145 loss: 1.58179944e-06
Iter: 146 loss: 1.56572014e-06
Iter: 147 loss: 1.55648058e-06
Iter: 148 loss: 1.54961492e-06
Iter: 149 loss: 1.55859573e-06
Iter: 150 loss: 1.54220504e-06
Iter: 151 loss: 1.53482347e-06
Iter: 152 loss: 1.51712823e-06
Iter: 153 loss: 1.70895009e-06
Iter: 154 loss: 1.51514666e-06
Iter: 155 loss: 1.50028075e-06
Iter: 156 loss: 1.5186472e-06
Iter: 157 loss: 1.49251241e-06
Iter: 158 loss: 1.47254229e-06
Iter: 159 loss: 1.52860571e-06
Iter: 160 loss: 1.46615639e-06
Iter: 161 loss: 1.45539605e-06
Iter: 162 loss: 1.45482954e-06
Iter: 163 loss: 1.44403873e-06
Iter: 164 loss: 1.42595468e-06
Iter: 165 loss: 1.4259133e-06
Iter: 166 loss: 1.4175971e-06
Iter: 167 loss: 1.41572059e-06
Iter: 168 loss: 1.40875932e-06
Iter: 169 loss: 1.3956078e-06
Iter: 170 loss: 1.6785591e-06
Iter: 171 loss: 1.39556369e-06
Iter: 172 loss: 1.38046789e-06
Iter: 173 loss: 1.47128185e-06
Iter: 174 loss: 1.37855704e-06
Iter: 175 loss: 1.36891322e-06
Iter: 176 loss: 1.45208401e-06
Iter: 177 loss: 1.3684097e-06
Iter: 178 loss: 1.35970618e-06
Iter: 179 loss: 1.33995673e-06
Iter: 180 loss: 1.60931563e-06
Iter: 181 loss: 1.33883236e-06
Iter: 182 loss: 1.32351636e-06
Iter: 183 loss: 1.41059627e-06
Iter: 184 loss: 1.32138746e-06
Iter: 185 loss: 1.30620811e-06
Iter: 186 loss: 1.36205324e-06
Iter: 187 loss: 1.30251067e-06
Iter: 188 loss: 1.29343346e-06
Iter: 189 loss: 1.29320415e-06
Iter: 190 loss: 1.28343754e-06
Iter: 191 loss: 1.28698935e-06
Iter: 192 loss: 1.27651333e-06
Iter: 193 loss: 1.26981854e-06
Iter: 194 loss: 1.25442671e-06
Iter: 195 loss: 1.45479669e-06
Iter: 196 loss: 1.25339614e-06
Iter: 197 loss: 1.23929533e-06
Iter: 198 loss: 1.40006955e-06
Iter: 199 loss: 1.23902021e-06
Iter: 200 loss: 1.22959932e-06
Iter: 201 loss: 1.3075487e-06
Iter: 202 loss: 1.22898894e-06
Iter: 203 loss: 1.22103938e-06
Iter: 204 loss: 1.27362864e-06
Iter: 205 loss: 1.22018173e-06
Iter: 206 loss: 1.21488529e-06
Iter: 207 loss: 1.20665527e-06
Iter: 208 loss: 1.20647724e-06
Iter: 209 loss: 1.19960816e-06
Iter: 210 loss: 1.19951756e-06
Iter: 211 loss: 1.19372248e-06
Iter: 212 loss: 1.1831562e-06
Iter: 213 loss: 1.42986244e-06
Iter: 214 loss: 1.18314495e-06
Iter: 215 loss: 1.1743889e-06
Iter: 216 loss: 1.2322887e-06
Iter: 217 loss: 1.17343689e-06
Iter: 218 loss: 1.1648151e-06
Iter: 219 loss: 1.15826799e-06
Iter: 220 loss: 1.1554049e-06
Iter: 221 loss: 1.15342846e-06
Iter: 222 loss: 1.14919681e-06
Iter: 223 loss: 1.14547208e-06
Iter: 224 loss: 1.13561805e-06
Iter: 225 loss: 1.20139794e-06
Iter: 226 loss: 1.13323813e-06
Iter: 227 loss: 1.13445662e-06
Iter: 228 loss: 1.12740304e-06
Iter: 229 loss: 1.12431621e-06
Iter: 230 loss: 1.12207022e-06
Iter: 231 loss: 1.12107932e-06
Iter: 232 loss: 1.11662916e-06
Iter: 233 loss: 1.11142811e-06
Iter: 234 loss: 1.1108516e-06
Iter: 235 loss: 1.10372594e-06
Iter: 236 loss: 1.12941e-06
Iter: 237 loss: 1.10193992e-06
Iter: 238 loss: 1.09620055e-06
Iter: 239 loss: 1.0877618e-06
Iter: 240 loss: 1.08755398e-06
Iter: 241 loss: 1.07953542e-06
Iter: 242 loss: 1.1456226e-06
Iter: 243 loss: 1.07905612e-06
Iter: 244 loss: 1.0719192e-06
Iter: 245 loss: 1.15269359e-06
Iter: 246 loss: 1.07181677e-06
Iter: 247 loss: 1.0684837e-06
Iter: 248 loss: 1.06393202e-06
Iter: 249 loss: 1.06369862e-06
Iter: 250 loss: 1.05641357e-06
Iter: 251 loss: 1.07098515e-06
Iter: 252 loss: 1.0534277e-06
Iter: 253 loss: 1.04742185e-06
Iter: 254 loss: 1.06522634e-06
Iter: 255 loss: 1.04551339e-06
Iter: 256 loss: 1.03970069e-06
Iter: 257 loss: 1.11280497e-06
Iter: 258 loss: 1.03965272e-06
Iter: 259 loss: 1.03561354e-06
Iter: 260 loss: 1.04122523e-06
Iter: 261 loss: 1.03371133e-06
Iter: 262 loss: 1.02948502e-06
Iter: 263 loss: 1.08026711e-06
Iter: 264 loss: 1.02941146e-06
Iter: 265 loss: 1.02657953e-06
Iter: 266 loss: 1.02046556e-06
Iter: 267 loss: 1.11809891e-06
Iter: 268 loss: 1.02028775e-06
Iter: 269 loss: 1.01436353e-06
Iter: 270 loss: 1.0736635e-06
Iter: 271 loss: 1.01424348e-06
Iter: 272 loss: 1.00877924e-06
Iter: 273 loss: 1.01919397e-06
Iter: 274 loss: 1.0065155e-06
Iter: 275 loss: 1.00309956e-06
Iter: 276 loss: 9.97306529e-07
Iter: 277 loss: 9.9728959e-07
Iter: 278 loss: 9.90457579e-07
Iter: 279 loss: 1.05925778e-06
Iter: 280 loss: 9.90244e-07
Iter: 281 loss: 9.8605733e-07
Iter: 282 loss: 1.01994124e-06
Iter: 283 loss: 9.85822453e-07
Iter: 284 loss: 9.81502581e-07
Iter: 285 loss: 9.7685313e-07
Iter: 286 loss: 9.76056754e-07
Iter: 287 loss: 9.71153e-07
Iter: 288 loss: 9.71809641e-07
Iter: 289 loss: 9.67432e-07
Iter: 290 loss: 9.60378429e-07
Iter: 291 loss: 1.01615217e-06
Iter: 292 loss: 9.59923227e-07
Iter: 293 loss: 9.5506266e-07
Iter: 294 loss: 9.81946414e-07
Iter: 295 loss: 9.54365646e-07
Iter: 296 loss: 9.49643322e-07
Iter: 297 loss: 9.77897798e-07
Iter: 298 loss: 9.49002185e-07
Iter: 299 loss: 9.46009209e-07
Iter: 300 loss: 9.72910584e-07
Iter: 301 loss: 9.45885915e-07
Iter: 302 loss: 9.43582847e-07
Iter: 303 loss: 9.37423124e-07
Iter: 304 loss: 9.80005098e-07
Iter: 305 loss: 9.36022843e-07
Iter: 306 loss: 9.34612501e-07
Iter: 307 loss: 9.33508886e-07
Iter: 308 loss: 9.30790918e-07
Iter: 309 loss: 9.2581422e-07
Iter: 310 loss: 1.04567778e-06
Iter: 311 loss: 9.25863333e-07
Iter: 312 loss: 9.21074843e-07
Iter: 313 loss: 9.50223352e-07
Iter: 314 loss: 9.2039636e-07
Iter: 315 loss: 9.17331533e-07
Iter: 316 loss: 9.12053395e-07
Iter: 317 loss: 9.12056862e-07
Iter: 318 loss: 9.09581217e-07
Iter: 319 loss: 9.08543939e-07
Iter: 320 loss: 9.05464162e-07
Iter: 321 loss: 9.09493735e-07
Iter: 322 loss: 9.03893124e-07
Iter: 323 loss: 9.00916461e-07
Iter: 324 loss: 8.99868e-07
Iter: 325 loss: 8.98209919e-07
Iter: 326 loss: 8.93446042e-07
Iter: 327 loss: 9.02658087e-07
Iter: 328 loss: 8.91455386e-07
Iter: 329 loss: 8.87576334e-07
Iter: 330 loss: 9.14024838e-07
Iter: 331 loss: 8.87170302e-07
Iter: 332 loss: 8.83249811e-07
Iter: 333 loss: 9.1364825e-07
Iter: 334 loss: 8.82936e-07
Iter: 335 loss: 8.80763366e-07
Iter: 336 loss: 8.89242529e-07
Iter: 337 loss: 8.80186519e-07
Iter: 338 loss: 8.7809542e-07
Iter: 339 loss: 8.75745e-07
Iter: 340 loss: 8.7541622e-07
Iter: 341 loss: 8.72290059e-07
Iter: 342 loss: 8.81541837e-07
Iter: 343 loss: 8.71313659e-07
Iter: 344 loss: 8.6879038e-07
Iter: 345 loss: 9.02761485e-07
Iter: 346 loss: 8.68761276e-07
Iter: 347 loss: 8.66423761e-07
Iter: 348 loss: 8.61661306e-07
Iter: 349 loss: 9.49729326e-07
Iter: 350 loss: 8.61568708e-07
Iter: 351 loss: 8.56237534e-07
Iter: 352 loss: 8.52652249e-07
Iter: 353 loss: 8.50721733e-07
Iter: 354 loss: 8.46945113e-07
Iter: 355 loss: 8.46526234e-07
Iter: 356 loss: 8.43096814e-07
Iter: 357 loss: 8.38308e-07
Iter: 358 loss: 8.38065887e-07
Iter: 359 loss: 8.34533694e-07
Iter: 360 loss: 8.34517778e-07
Iter: 361 loss: 8.31112231e-07
Iter: 362 loss: 8.50697802e-07
Iter: 363 loss: 8.30674253e-07
Iter: 364 loss: 8.28263524e-07
Iter: 365 loss: 8.27813153e-07
Iter: 366 loss: 8.26193343e-07
Iter: 367 loss: 8.22690481e-07
Iter: 368 loss: 8.32685373e-07
Iter: 369 loss: 8.21621086e-07
Iter: 370 loss: 8.19108379e-07
Iter: 371 loss: 8.18914032e-07
Iter: 372 loss: 8.17842931e-07
Iter: 373 loss: 8.14433861e-07
Iter: 374 loss: 8.27599195e-07
Iter: 375 loss: 8.13048e-07
Iter: 376 loss: 8.09043115e-07
Iter: 377 loss: 8.19252477e-07
Iter: 378 loss: 8.07629533e-07
Iter: 379 loss: 8.05000752e-07
Iter: 380 loss: 8.0465054e-07
Iter: 381 loss: 8.02678471e-07
Iter: 382 loss: 8.04193405e-07
Iter: 383 loss: 8.01485612e-07
Iter: 384 loss: 7.99645704e-07
Iter: 385 loss: 8.01266765e-07
Iter: 386 loss: 7.98551923e-07
Iter: 387 loss: 7.96054564e-07
Iter: 388 loss: 8.09233e-07
Iter: 389 loss: 7.95726066e-07
Iter: 390 loss: 7.93517302e-07
Iter: 391 loss: 7.90617037e-07
Iter: 392 loss: 7.90435706e-07
Iter: 393 loss: 7.87286183e-07
Iter: 394 loss: 7.87932493e-07
Iter: 395 loss: 7.84893246e-07
Iter: 396 loss: 7.80467474e-07
Iter: 397 loss: 8.3799614e-07
Iter: 398 loss: 7.80488335e-07
Iter: 399 loss: 7.77278046e-07
Iter: 400 loss: 7.94039352e-07
Iter: 401 loss: 7.76801528e-07
Iter: 402 loss: 7.74371642e-07
Iter: 403 loss: 7.72693e-07
Iter: 404 loss: 7.71796351e-07
Iter: 405 loss: 7.73012857e-07
Iter: 406 loss: 7.70717918e-07
Iter: 407 loss: 7.70007489e-07
Iter: 408 loss: 7.67815948e-07
Iter: 409 loss: 7.73377906e-07
Iter: 410 loss: 7.6651861e-07
Iter: 411 loss: 7.63227717e-07
Iter: 412 loss: 7.7945549e-07
Iter: 413 loss: 7.62729371e-07
Iter: 414 loss: 7.60498551e-07
Iter: 415 loss: 7.64255788e-07
Iter: 416 loss: 7.59521527e-07
Iter: 417 loss: 7.5822868e-07
Iter: 418 loss: 7.58002955e-07
Iter: 419 loss: 7.56824761e-07
Iter: 420 loss: 7.54521466e-07
Iter: 421 loss: 8.00946964e-07
Iter: 422 loss: 7.54496284e-07
Iter: 423 loss: 7.52027404e-07
Iter: 424 loss: 7.8081132e-07
Iter: 425 loss: 7.52001142e-07
Iter: 426 loss: 7.50500647e-07
Iter: 427 loss: 7.49179435e-07
Iter: 428 loss: 7.48800232e-07
Iter: 429 loss: 7.45538955e-07
Iter: 430 loss: 7.45301122e-07
Iter: 431 loss: 7.42857367e-07
Iter: 432 loss: 7.39827669e-07
Iter: 433 loss: 7.56613133e-07
Iter: 434 loss: 7.39398e-07
Iter: 435 loss: 7.36492098e-07
Iter: 436 loss: 7.63459525e-07
Iter: 437 loss: 7.36360732e-07
Iter: 438 loss: 7.34647e-07
Iter: 439 loss: 7.33585296e-07
Iter: 440 loss: 7.32910507e-07
Iter: 441 loss: 7.31834803e-07
Iter: 442 loss: 7.31634145e-07
Iter: 443 loss: 7.30355396e-07
Iter: 444 loss: 7.29063572e-07
Iter: 445 loss: 7.28828809e-07
Iter: 446 loss: 7.26978669e-07
Iter: 447 loss: 7.26031544e-07
Iter: 448 loss: 7.25178211e-07
Iter: 449 loss: 7.23198752e-07
Iter: 450 loss: 7.2615461e-07
Iter: 451 loss: 7.22221159e-07
Iter: 452 loss: 7.20519e-07
Iter: 453 loss: 7.20494313e-07
Iter: 454 loss: 7.19217951e-07
Iter: 455 loss: 7.20678486e-07
Iter: 456 loss: 7.18567094e-07
Iter: 457 loss: 7.17284934e-07
Iter: 458 loss: 7.14583621e-07
Iter: 459 loss: 7.61119225e-07
Iter: 460 loss: 7.1451575e-07
Iter: 461 loss: 7.12708e-07
Iter: 462 loss: 7.12422889e-07
Iter: 463 loss: 7.11064899e-07
Iter: 464 loss: 7.08321863e-07
Iter: 465 loss: 7.60778846e-07
Iter: 466 loss: 7.08239554e-07
Iter: 467 loss: 7.05369587e-07
Iter: 468 loss: 7.23387586e-07
Iter: 469 loss: 7.05077241e-07
Iter: 470 loss: 7.02842726e-07
Iter: 471 loss: 7.20233288e-07
Iter: 472 loss: 7.02658838e-07
Iter: 473 loss: 7.00984174e-07
Iter: 474 loss: 7.10285576e-07
Iter: 475 loss: 7.00779083e-07
Iter: 476 loss: 6.9995275e-07
Iter: 477 loss: 7.04187528e-07
Iter: 478 loss: 6.99823318e-07
Iter: 479 loss: 6.98494887e-07
Iter: 480 loss: 6.95950916e-07
Iter: 481 loss: 7.46718626e-07
Iter: 482 loss: 6.95965582e-07
Iter: 483 loss: 6.94080313e-07
Iter: 484 loss: 6.96687039e-07
Iter: 485 loss: 6.93179459e-07
Iter: 486 loss: 6.90894e-07
Iter: 487 loss: 6.96945506e-07
Iter: 488 loss: 6.90075581e-07
Iter: 489 loss: 6.88479417e-07
Iter: 490 loss: 7.02649061e-07
Iter: 491 loss: 6.88389378e-07
Iter: 492 loss: 6.8632869e-07
Iter: 493 loss: 6.86298961e-07
Iter: 494 loss: 6.84672614e-07
Iter: 495 loss: 6.82745281e-07
Iter: 496 loss: 6.84408747e-07
Iter: 497 loss: 6.81560209e-07
Iter: 498 loss: 6.79938e-07
Iter: 499 loss: 6.99989698e-07
Iter: 500 loss: 6.79945344e-07
Iter: 501 loss: 6.78539777e-07
Iter: 502 loss: 6.79243556e-07
Iter: 503 loss: 6.77597541e-07
Iter: 504 loss: 6.75700221e-07
Iter: 505 loss: 6.775835e-07
Iter: 506 loss: 6.74623493e-07
Iter: 507 loss: 6.73244074e-07
Iter: 508 loss: 6.78143408e-07
Iter: 509 loss: 6.72889712e-07
Iter: 510 loss: 6.71027806e-07
Iter: 511 loss: 6.8007887e-07
Iter: 512 loss: 6.70764166e-07
Iter: 513 loss: 6.69586939e-07
Iter: 514 loss: 6.7609318e-07
Iter: 515 loss: 6.69395149e-07
Iter: 516 loss: 6.6797611e-07
Iter: 517 loss: 6.68732127e-07
Iter: 518 loss: 6.67035806e-07
Iter: 519 loss: 6.65627567e-07
Iter: 520 loss: 6.64726144e-07
Iter: 521 loss: 6.64168454e-07
Iter: 522 loss: 6.62283583e-07
Iter: 523 loss: 6.61690876e-07
Iter: 524 loss: 6.6055793e-07
Iter: 525 loss: 6.59180159e-07
Iter: 526 loss: 6.59049761e-07
Iter: 527 loss: 6.5751658e-07
Iter: 528 loss: 6.61672971e-07
Iter: 529 loss: 6.56992086e-07
Iter: 530 loss: 6.55920076e-07
Iter: 531 loss: 6.56997088e-07
Iter: 532 loss: 6.55340216e-07
Iter: 533 loss: 6.53917596e-07
Iter: 534 loss: 6.51734695e-07
Iter: 535 loss: 6.51674782e-07
Iter: 536 loss: 6.50302241e-07
Iter: 537 loss: 6.49893934e-07
Iter: 538 loss: 6.49239041e-07
Iter: 539 loss: 6.47548632e-07
Iter: 540 loss: 6.6552866e-07
Iter: 541 loss: 6.47334332e-07
Iter: 542 loss: 6.45270745e-07
Iter: 543 loss: 6.61089189e-07
Iter: 544 loss: 6.45098282e-07
Iter: 545 loss: 6.43524231e-07
Iter: 546 loss: 6.5643269e-07
Iter: 547 loss: 6.43394969e-07
Iter: 548 loss: 6.41869349e-07
Iter: 549 loss: 6.44398654e-07
Iter: 550 loss: 6.41140559e-07
Iter: 551 loss: 6.39696054e-07
Iter: 552 loss: 6.54103701e-07
Iter: 553 loss: 6.39658879e-07
Iter: 554 loss: 6.39038205e-07
Iter: 555 loss: 6.37364053e-07
Iter: 556 loss: 6.5264976e-07
Iter: 557 loss: 6.3716891e-07
Iter: 558 loss: 6.35481911e-07
Iter: 559 loss: 6.4057258e-07
Iter: 560 loss: 6.34912794e-07
Iter: 561 loss: 6.33622335e-07
Iter: 562 loss: 6.33602042e-07
Iter: 563 loss: 6.32564138e-07
Iter: 564 loss: 6.34858e-07
Iter: 565 loss: 6.32148158e-07
Iter: 566 loss: 6.31283797e-07
Iter: 567 loss: 6.29597935e-07
Iter: 568 loss: 6.65342668e-07
Iter: 569 loss: 6.29604529e-07
Iter: 570 loss: 6.27981308e-07
Iter: 571 loss: 6.48163166e-07
Iter: 572 loss: 6.27960731e-07
Iter: 573 loss: 6.26525321e-07
Iter: 574 loss: 6.31360649e-07
Iter: 575 loss: 6.2616482e-07
Iter: 576 loss: 6.25016639e-07
Iter: 577 loss: 6.25403118e-07
Iter: 578 loss: 6.24178597e-07
Iter: 579 loss: 6.22640414e-07
Iter: 580 loss: 6.24948143e-07
Iter: 581 loss: 6.21906793e-07
Iter: 582 loss: 6.21324e-07
Iter: 583 loss: 6.21101037e-07
Iter: 584 loss: 6.20322226e-07
Iter: 585 loss: 6.19619698e-07
Iter: 586 loss: 6.19442e-07
Iter: 587 loss: 6.17928322e-07
Iter: 588 loss: 6.24044162e-07
Iter: 589 loss: 6.17565206e-07
Iter: 590 loss: 6.16989269e-07
Iter: 591 loss: 6.16124794e-07
Iter: 592 loss: 6.16107172e-07
Iter: 593 loss: 6.14721728e-07
Iter: 594 loss: 6.17566911e-07
Iter: 595 loss: 6.14155965e-07
Iter: 596 loss: 6.1318741e-07
Iter: 597 loss: 6.28338626e-07
Iter: 598 loss: 6.13195652e-07
Iter: 599 loss: 6.12049575e-07
Iter: 600 loss: 6.11158555e-07
Iter: 601 loss: 6.10813061e-07
Iter: 602 loss: 6.09536414e-07
Iter: 603 loss: 6.08172456e-07
Iter: 604 loss: 6.07927689e-07
Iter: 605 loss: 6.06208118e-07
Iter: 606 loss: 6.06183676e-07
Iter: 607 loss: 6.05027367e-07
Iter: 608 loss: 6.09382369e-07
Iter: 609 loss: 6.04728655e-07
Iter: 610 loss: 6.03520618e-07
Iter: 611 loss: 6.02745899e-07
Iter: 612 loss: 6.02267619e-07
Iter: 613 loss: 6.00775593e-07
Iter: 614 loss: 6.02616126e-07
Iter: 615 loss: 5.99939256e-07
Iter: 616 loss: 5.9895342e-07
Iter: 617 loss: 5.98752763e-07
Iter: 618 loss: 5.97982819e-07
Iter: 619 loss: 5.99488715e-07
Iter: 620 loss: 5.97660858e-07
Iter: 621 loss: 5.97112773e-07
Iter: 622 loss: 5.9918176e-07
Iter: 623 loss: 5.96986752e-07
Iter: 624 loss: 5.96396376e-07
Iter: 625 loss: 5.94921403e-07
Iter: 626 loss: 6.07634149e-07
Iter: 627 loss: 5.94688458e-07
Iter: 628 loss: 5.93035679e-07
Iter: 629 loss: 5.98894246e-07
Iter: 630 loss: 5.92617653e-07
Iter: 631 loss: 5.91341859e-07
Iter: 632 loss: 6.07959123e-07
Iter: 633 loss: 5.9132708e-07
Iter: 634 loss: 5.90158606e-07
Iter: 635 loss: 5.95693223e-07
Iter: 636 loss: 5.89928277e-07
Iter: 637 loss: 5.8907051e-07
Iter: 638 loss: 5.88437729e-07
Iter: 639 loss: 5.881937e-07
Iter: 640 loss: 5.86769829e-07
Iter: 641 loss: 5.86198212e-07
Iter: 642 loss: 5.85495172e-07
Iter: 643 loss: 5.8520078e-07
Iter: 644 loss: 5.84660597e-07
Iter: 645 loss: 5.83945507e-07
Iter: 646 loss: 5.82506914e-07
Iter: 647 loss: 6.04884463e-07
Iter: 648 loss: 5.82434893e-07
Iter: 649 loss: 5.80824349e-07
Iter: 650 loss: 5.92710137e-07
Iter: 651 loss: 5.80670189e-07
Iter: 652 loss: 5.79999323e-07
Iter: 653 loss: 5.79999551e-07
Iter: 654 loss: 5.7923171e-07
Iter: 655 loss: 5.79527523e-07
Iter: 656 loss: 5.78743652e-07
Iter: 657 loss: 5.78156801e-07
Iter: 658 loss: 5.79761718e-07
Iter: 659 loss: 5.77948754e-07
Iter: 660 loss: 5.77090645e-07
Iter: 661 loss: 5.75998456e-07
Iter: 662 loss: 5.75950821e-07
Iter: 663 loss: 5.74733292e-07
Iter: 664 loss: 5.76259481e-07
Iter: 665 loss: 5.74127455e-07
Iter: 666 loss: 5.72479564e-07
Iter: 667 loss: 5.76338209e-07
Iter: 668 loss: 5.71803184e-07
Iter: 669 loss: 5.71176486e-07
Iter: 670 loss: 5.71061264e-07
Iter: 671 loss: 5.70207476e-07
Iter: 672 loss: 5.68570215e-07
Iter: 673 loss: 6.02407681e-07
Iter: 674 loss: 5.68557482e-07
Iter: 675 loss: 5.67583811e-07
Iter: 676 loss: 5.76047285e-07
Iter: 677 loss: 5.67517077e-07
Iter: 678 loss: 5.66682786e-07
Iter: 679 loss: 5.66732353e-07
Iter: 680 loss: 5.66041365e-07
Iter: 681 loss: 5.65101629e-07
Iter: 682 loss: 5.65077585e-07
Iter: 683 loss: 5.64478341e-07
Iter: 684 loss: 5.63588628e-07
Iter: 685 loss: 5.63573053e-07
Iter: 686 loss: 5.63321464e-07
Iter: 687 loss: 5.62968353e-07
Iter: 688 loss: 5.6253964e-07
Iter: 689 loss: 5.61663853e-07
Iter: 690 loss: 5.81252834e-07
Iter: 691 loss: 5.61678462e-07
Iter: 692 loss: 5.60578655e-07
Iter: 693 loss: 5.6722547e-07
Iter: 694 loss: 5.60468322e-07
Iter: 695 loss: 5.59779323e-07
Iter: 696 loss: 5.59058549e-07
Iter: 697 loss: 5.58944237e-07
Iter: 698 loss: 5.57513886e-07
Iter: 699 loss: 5.59992372e-07
Iter: 700 loss: 5.56934651e-07
Iter: 701 loss: 5.55747249e-07
Iter: 702 loss: 5.57050157e-07
Iter: 703 loss: 5.55107704e-07
Iter: 704 loss: 5.54485325e-07
Iter: 705 loss: 5.54353278e-07
Iter: 706 loss: 5.53588e-07
Iter: 707 loss: 5.53654331e-07
Iter: 708 loss: 5.53051223e-07
Iter: 709 loss: 5.52326355e-07
Iter: 710 loss: 5.52616086e-07
Iter: 711 loss: 5.51815e-07
Iter: 712 loss: 5.50650839e-07
Iter: 713 loss: 5.48997718e-07
Iter: 714 loss: 5.48923367e-07
Iter: 715 loss: 5.49326501e-07
Iter: 716 loss: 5.48147682e-07
Iter: 717 loss: 5.47668378e-07
Iter: 718 loss: 5.47132345e-07
Iter: 719 loss: 5.47037757e-07
Iter: 720 loss: 5.46339209e-07
Iter: 721 loss: 5.46340118e-07
Iter: 722 loss: 5.45897933e-07
Iter: 723 loss: 5.45425678e-07
Iter: 724 loss: 5.45354737e-07
Iter: 725 loss: 5.44672957e-07
Iter: 726 loss: 5.45983312e-07
Iter: 727 loss: 5.4441e-07
Iter: 728 loss: 5.43480894e-07
Iter: 729 loss: 5.45653847e-07
Iter: 730 loss: 5.43159899e-07
Iter: 731 loss: 5.42593455e-07
Iter: 732 loss: 5.41442205e-07
Iter: 733 loss: 5.62237858e-07
Iter: 734 loss: 5.41420491e-07
Iter: 735 loss: 5.4013e-07
Iter: 736 loss: 5.56112809e-07
Iter: 737 loss: 5.40143901e-07
Iter: 738 loss: 5.39302846e-07
Iter: 739 loss: 5.40688234e-07
Iter: 740 loss: 5.38882318e-07
Iter: 741 loss: 5.38049221e-07
Iter: 742 loss: 5.38060647e-07
Iter: 743 loss: 5.37541439e-07
Iter: 744 loss: 5.36118705e-07
Iter: 745 loss: 5.43944395e-07
Iter: 746 loss: 5.35676179e-07
Iter: 747 loss: 5.34806304e-07
Iter: 748 loss: 5.47173499e-07
Iter: 749 loss: 5.34803e-07
Iter: 750 loss: 5.34079902e-07
Iter: 751 loss: 5.33989692e-07
Iter: 752 loss: 5.33464117e-07
Iter: 753 loss: 5.32605e-07
Iter: 754 loss: 5.40039707e-07
Iter: 755 loss: 5.32588274e-07
Iter: 756 loss: 5.31974649e-07
Iter: 757 loss: 5.32015179e-07
Iter: 758 loss: 5.31467037e-07
Iter: 759 loss: 5.30791226e-07
Iter: 760 loss: 5.30971704e-07
Iter: 761 loss: 5.30271393e-07
Iter: 762 loss: 5.29602232e-07
Iter: 763 loss: 5.33064167e-07
Iter: 764 loss: 5.29437216e-07
Iter: 765 loss: 5.28662895e-07
Iter: 766 loss: 5.30352395e-07
Iter: 767 loss: 5.28383396e-07
Iter: 768 loss: 5.27648353e-07
Iter: 769 loss: 5.34883043e-07
Iter: 770 loss: 5.27613565e-07
Iter: 771 loss: 5.27094357e-07
Iter: 772 loss: 5.25873531e-07
Iter: 773 loss: 5.37852372e-07
Iter: 774 loss: 5.25731366e-07
Iter: 775 loss: 5.24391e-07
Iter: 776 loss: 5.35131676e-07
Iter: 777 loss: 5.24309655e-07
Iter: 778 loss: 5.23314952e-07
Iter: 779 loss: 5.28505666e-07
Iter: 780 loss: 5.23119468e-07
Iter: 781 loss: 5.22597816e-07
Iter: 782 loss: 5.22610321e-07
Iter: 783 loss: 5.2222083e-07
Iter: 784 loss: 5.21228344e-07
Iter: 785 loss: 5.28055693e-07
Iter: 786 loss: 5.21019274e-07
Iter: 787 loss: 5.20341928e-07
Iter: 788 loss: 5.203309e-07
Iter: 789 loss: 5.19585114e-07
Iter: 790 loss: 5.23874689e-07
Iter: 791 loss: 5.19486889e-07
Iter: 792 loss: 5.1903254e-07
Iter: 793 loss: 5.19645141e-07
Iter: 794 loss: 5.1883228e-07
Iter: 795 loss: 5.18407091e-07
Iter: 796 loss: 5.19217679e-07
Iter: 797 loss: 5.18231275e-07
Iter: 798 loss: 5.1759605e-07
Iter: 799 loss: 5.16435762e-07
Iter: 800 loss: 5.44627596e-07
Iter: 801 loss: 5.16459295e-07
Iter: 802 loss: 5.15591921e-07
Iter: 803 loss: 5.15588908e-07
Iter: 804 loss: 5.14802537e-07
Iter: 805 loss: 5.16172861e-07
Iter: 806 loss: 5.1446375e-07
Iter: 807 loss: 5.13878831e-07
Iter: 808 loss: 5.13670784e-07
Iter: 809 loss: 5.13327223e-07
Iter: 810 loss: 5.12483666e-07
Iter: 811 loss: 5.15600391e-07
Iter: 812 loss: 5.1226607e-07
Iter: 813 loss: 5.11593726e-07
Iter: 814 loss: 5.14205567e-07
Iter: 815 loss: 5.11414783e-07
Iter: 816 loss: 5.10672407e-07
Iter: 817 loss: 5.17847752e-07
Iter: 818 loss: 5.10632503e-07
Iter: 819 loss: 5.10280529e-07
Iter: 820 loss: 5.09619895e-07
Iter: 821 loss: 5.24315567e-07
Iter: 822 loss: 5.09627625e-07
Iter: 823 loss: 5.09239044e-07
Iter: 824 loss: 5.09123765e-07
Iter: 825 loss: 5.08753942e-07
Iter: 826 loss: 5.08417543e-07
Iter: 827 loss: 5.08319317e-07
Iter: 828 loss: 5.07734399e-07
Iter: 829 loss: 5.083632e-07
Iter: 830 loss: 5.07386062e-07
Iter: 831 loss: 5.0649e-07
Iter: 832 loss: 5.08293851e-07
Iter: 833 loss: 5.06213382e-07
Iter: 834 loss: 5.05490391e-07
Iter: 835 loss: 5.04629043e-07
Iter: 836 loss: 5.04526383e-07
Iter: 837 loss: 5.04243644e-07
Iter: 838 loss: 5.03921456e-07
Iter: 839 loss: 5.03546175e-07
Iter: 840 loss: 5.0362496e-07
Iter: 841 loss: 5.03255251e-07
Iter: 842 loss: 5.02697389e-07
Iter: 843 loss: 5.02589273e-07
Iter: 844 loss: 5.02224452e-07
Iter: 845 loss: 5.01575528e-07
Iter: 846 loss: 5.02248213e-07
Iter: 847 loss: 5.01192517e-07
Iter: 848 loss: 5.01139311e-07
Iter: 849 loss: 5.00840088e-07
Iter: 850 loss: 5.00554506e-07
Iter: 851 loss: 4.99961629e-07
Iter: 852 loss: 5.11745498e-07
Iter: 853 loss: 4.9996703e-07
Iter: 854 loss: 4.99262569e-07
Iter: 855 loss: 5.02893499e-07
Iter: 856 loss: 4.99143482e-07
Iter: 857 loss: 4.98507575e-07
Iter: 858 loss: 5.03888259e-07
Iter: 859 loss: 4.98456245e-07
Iter: 860 loss: 4.98038389e-07
Iter: 861 loss: 4.97370763e-07
Iter: 862 loss: 4.97346775e-07
Iter: 863 loss: 4.96767143e-07
Iter: 864 loss: 4.9676089e-07
Iter: 865 loss: 4.9635139e-07
Iter: 866 loss: 4.95513291e-07
Iter: 867 loss: 5.11562803e-07
Iter: 868 loss: 4.95483164e-07
Iter: 869 loss: 4.9481e-07
Iter: 870 loss: 5.03542083e-07
Iter: 871 loss: 4.94804965e-07
Iter: 872 loss: 4.94191511e-07
Iter: 873 loss: 4.96634584e-07
Iter: 874 loss: 4.94023084e-07
Iter: 875 loss: 4.93583912e-07
Iter: 876 loss: 4.93584821e-07
Iter: 877 loss: 4.93201924e-07
Iter: 878 loss: 4.92540835e-07
Iter: 879 loss: 4.92036406e-07
Iter: 880 loss: 4.91805963e-07
Iter: 881 loss: 4.91240485e-07
Iter: 882 loss: 4.91220874e-07
Iter: 883 loss: 4.90703712e-07
Iter: 884 loss: 4.93378252e-07
Iter: 885 loss: 4.90565583e-07
Iter: 886 loss: 4.90196726e-07
Iter: 887 loss: 4.90233333e-07
Iter: 888 loss: 4.89883e-07
Iter: 889 loss: 4.89478737e-07
Iter: 890 loss: 4.89489651e-07
Iter: 891 loss: 4.8910573e-07
Iter: 892 loss: 4.88293381e-07
Iter: 893 loss: 5.00621468e-07
Iter: 894 loss: 4.88267233e-07
Iter: 895 loss: 4.87810325e-07
Iter: 896 loss: 4.87809643e-07
Iter: 897 loss: 4.87396107e-07
Iter: 898 loss: 4.87164471e-07
Iter: 899 loss: 4.86981151e-07
Iter: 900 loss: 4.86501222e-07
Iter: 901 loss: 4.87499733e-07
Iter: 902 loss: 4.86314718e-07
Iter: 903 loss: 4.85691203e-07
Iter: 904 loss: 4.88660476e-07
Iter: 905 loss: 4.85579903e-07
Iter: 906 loss: 4.84986685e-07
Iter: 907 loss: 4.86199951e-07
Iter: 908 loss: 4.84749705e-07
Iter: 909 loss: 4.84201e-07
Iter: 910 loss: 4.83659e-07
Iter: 911 loss: 4.83544113e-07
Iter: 912 loss: 4.8275075e-07
Iter: 913 loss: 4.85957e-07
Iter: 914 loss: 4.82580845e-07
Iter: 915 loss: 4.82101939e-07
Iter: 916 loss: 4.82084261e-07
Iter: 917 loss: 4.81731945e-07
Iter: 918 loss: 4.81273105e-07
Iter: 919 loss: 4.8123195e-07
Iter: 920 loss: 4.80788401e-07
Iter: 921 loss: 4.80793517e-07
Iter: 922 loss: 4.8033894e-07
Iter: 923 loss: 4.80222525e-07
Iter: 924 loss: 4.79923472e-07
Iter: 925 loss: 4.79497146e-07
Iter: 926 loss: 4.79291089e-07
Iter: 927 loss: 4.790719e-07
Iter: 928 loss: 4.7844e-07
Iter: 929 loss: 4.86073191e-07
Iter: 930 loss: 4.78418201e-07
Iter: 931 loss: 4.78065886e-07
Iter: 932 loss: 4.77570779e-07
Iter: 933 loss: 4.77542471e-07
Iter: 934 loss: 4.76837499e-07
Iter: 935 loss: 4.81555958e-07
Iter: 936 loss: 4.76763034e-07
Iter: 937 loss: 4.76253547e-07
Iter: 938 loss: 4.79279436e-07
Iter: 939 loss: 4.76173085e-07
Iter: 940 loss: 4.75738545e-07
Iter: 941 loss: 4.75061142e-07
Iter: 942 loss: 4.75025672e-07
Iter: 943 loss: 4.74372655e-07
Iter: 944 loss: 4.75332911e-07
Iter: 945 loss: 4.74032106e-07
Iter: 946 loss: 4.73648555e-07
Iter: 947 loss: 4.7354672e-07
Iter: 948 loss: 4.73195314e-07
Iter: 949 loss: 4.73531543e-07
Iter: 950 loss: 4.72982038e-07
Iter: 951 loss: 4.7260886e-07
Iter: 952 loss: 4.7368124e-07
Iter: 953 loss: 4.72493724e-07
Iter: 954 loss: 4.72046963e-07
Iter: 955 loss: 4.74037137e-07
Iter: 956 loss: 4.71956866e-07
Iter: 957 loss: 4.71688566e-07
Iter: 958 loss: 4.71128317e-07
Iter: 959 loss: 4.80527262e-07
Iter: 960 loss: 4.71145398e-07
Iter: 961 loss: 4.70656232e-07
Iter: 962 loss: 4.70633097e-07
Iter: 963 loss: 4.70296385e-07
Iter: 964 loss: 4.69769816e-07
Iter: 965 loss: 4.69769759e-07
Iter: 966 loss: 4.69291592e-07
Iter: 967 loss: 4.76072131e-07
Iter: 968 loss: 4.69285595e-07
Iter: 969 loss: 4.68968921e-07
Iter: 970 loss: 4.70028766e-07
Iter: 971 loss: 4.68893091e-07
Iter: 972 loss: 4.68478788e-07
Iter: 973 loss: 4.67852146e-07
Iter: 974 loss: 4.67847258e-07
Iter: 975 loss: 4.6711267e-07
Iter: 976 loss: 4.67487325e-07
Iter: 977 loss: 4.66635612e-07
Iter: 978 loss: 4.66176516e-07
Iter: 979 loss: 4.66102279e-07
Iter: 980 loss: 4.65625533e-07
Iter: 981 loss: 4.66840561e-07
Iter: 982 loss: 4.65443577e-07
Iter: 983 loss: 4.65043854e-07
Iter: 984 loss: 4.65676493e-07
Iter: 985 loss: 4.64806931e-07
Iter: 986 loss: 4.64486533e-07
Iter: 987 loss: 4.69985338e-07
Iter: 988 loss: 4.64486305e-07
Iter: 989 loss: 4.64243044e-07
Iter: 990 loss: 4.63686746e-07
Iter: 991 loss: 4.68095607e-07
Iter: 992 loss: 4.63577521e-07
Iter: 993 loss: 4.63148353e-07
Iter: 994 loss: 4.6314031e-07
Iter: 995 loss: 4.62786261e-07
Iter: 996 loss: 4.62393359e-07
Iter: 997 loss: 4.62309345e-07
Iter: 998 loss: 4.61791643e-07
Iter: 999 loss: 4.64597e-07
Iter: 1000 loss: 4.61698335e-07
Iter: 1001 loss: 4.61253819e-07
Iter: 1002 loss: 4.63877598e-07
Iter: 1003 loss: 4.61186374e-07
Iter: 1004 loss: 4.6074021e-07
Iter: 1005 loss: 4.60705621e-07
Iter: 1006 loss: 4.6034674e-07
Iter: 1007 loss: 4.59835206e-07
Iter: 1008 loss: 4.59264101e-07
Iter: 1009 loss: 4.59196542e-07
Iter: 1010 loss: 4.58514478e-07
Iter: 1011 loss: 4.68264091e-07
Iter: 1012 loss: 4.5851769e-07
Iter: 1013 loss: 4.58095599e-07
Iter: 1014 loss: 4.6416784e-07
Iter: 1015 loss: 4.58086049e-07
Iter: 1016 loss: 4.57787053e-07
Iter: 1017 loss: 4.57889087e-07
Iter: 1018 loss: 4.57573549e-07
Iter: 1019 loss: 4.57311842e-07
Iter: 1020 loss: 4.57302349e-07
Iter: 1021 loss: 4.57071195e-07
Iter: 1022 loss: 4.56418121e-07
Iter: 1023 loss: 4.6087149e-07
Iter: 1024 loss: 4.5629929e-07
Iter: 1025 loss: 4.55762404e-07
Iter: 1026 loss: 4.6323791e-07
Iter: 1027 loss: 4.55742111e-07
Iter: 1028 loss: 4.55304928e-07
Iter: 1029 loss: 4.55814529e-07
Iter: 1030 loss: 4.55037508e-07
Iter: 1031 loss: 4.54553742e-07
Iter: 1032 loss: 4.55797903e-07
Iter: 1033 loss: 4.54410724e-07
Iter: 1034 loss: 4.5402291e-07
Iter: 1035 loss: 4.56293776e-07
Iter: 1036 loss: 4.53951515e-07
Iter: 1037 loss: 4.53559608e-07
Iter: 1038 loss: 4.5435246e-07
Iter: 1039 loss: 4.53387315e-07
Iter: 1040 loss: 4.53101876e-07
Iter: 1041 loss: 4.52725857e-07
Iter: 1042 loss: 4.52699538e-07
Iter: 1043 loss: 4.52182292e-07
Iter: 1044 loss: 4.55316354e-07
Iter: 1045 loss: 4.52132127e-07
Iter: 1046 loss: 4.51678972e-07
Iter: 1047 loss: 4.54872321e-07
Iter: 1048 loss: 4.51644951e-07
Iter: 1049 loss: 4.51142739e-07
Iter: 1050 loss: 4.52034e-07
Iter: 1051 loss: 4.50926791e-07
Iter: 1052 loss: 4.50634815e-07
Iter: 1053 loss: 4.52262952e-07
Iter: 1054 loss: 4.50571349e-07
Iter: 1055 loss: 4.50103698e-07
Iter: 1056 loss: 4.50098923e-07
Iter: 1057 loss: 4.49728958e-07
Iter: 1058 loss: 4.49418167e-07
Iter: 1059 loss: 4.49333612e-07
Iter: 1060 loss: 4.49166066e-07
Iter: 1061 loss: 4.48635234e-07
Iter: 1062 loss: 4.52919409e-07
Iter: 1063 loss: 4.48576543e-07
Iter: 1064 loss: 4.48270583e-07
Iter: 1065 loss: 4.48415108e-07
Iter: 1066 loss: 4.48082858e-07
Iter: 1067 loss: 4.47682311e-07
Iter: 1068 loss: 4.48252e-07
Iter: 1069 loss: 4.47431688e-07
Iter: 1070 loss: 4.46903641e-07
Iter: 1071 loss: 4.51758154e-07
Iter: 1072 loss: 4.46894632e-07
Iter: 1073 loss: 4.46574688e-07
Iter: 1074 loss: 4.45928663e-07
Iter: 1075 loss: 4.57539954e-07
Iter: 1076 loss: 4.45907375e-07
Iter: 1077 loss: 4.45246485e-07
Iter: 1078 loss: 4.50587521e-07
Iter: 1079 loss: 4.45209537e-07
Iter: 1080 loss: 4.44680097e-07
Iter: 1081 loss: 4.45539115e-07
Iter: 1082 loss: 4.44410489e-07
Iter: 1083 loss: 4.43833727e-07
Iter: 1084 loss: 4.52983386e-07
Iter: 1085 loss: 4.43810734e-07
Iter: 1086 loss: 4.4353547e-07
Iter: 1087 loss: 4.4331091e-07
Iter: 1088 loss: 4.43242129e-07
Iter: 1089 loss: 4.42720903e-07
Iter: 1090 loss: 4.48335356e-07
Iter: 1091 loss: 4.42712064e-07
Iter: 1092 loss: 4.42520616e-07
Iter: 1093 loss: 4.42110377e-07
Iter: 1094 loss: 4.48017317e-07
Iter: 1095 loss: 4.42072832e-07
Iter: 1096 loss: 4.41669442e-07
Iter: 1097 loss: 4.46729302e-07
Iter: 1098 loss: 4.41663076e-07
Iter: 1099 loss: 4.41269975e-07
Iter: 1100 loss: 4.42136866e-07
Iter: 1101 loss: 4.41133864e-07
Iter: 1102 loss: 4.40777427e-07
Iter: 1103 loss: 4.40674626e-07
Iter: 1104 loss: 4.40453618e-07
Iter: 1105 loss: 4.40169629e-07
Iter: 1106 loss: 4.40141832e-07
Iter: 1107 loss: 4.39893597e-07
Iter: 1108 loss: 4.39296855e-07
Iter: 1109 loss: 4.46533562e-07
Iter: 1110 loss: 4.3919988e-07
Iter: 1111 loss: 4.38617e-07
Iter: 1112 loss: 4.4163e-07
Iter: 1113 loss: 4.38528e-07
Iter: 1114 loss: 4.37948273e-07
Iter: 1115 loss: 4.4006191e-07
Iter: 1116 loss: 4.37849621e-07
Iter: 1117 loss: 4.37530559e-07
Iter: 1118 loss: 4.37517542e-07
Iter: 1119 loss: 4.37226078e-07
Iter: 1120 loss: 4.36740663e-07
Iter: 1121 loss: 4.36739583e-07
Iter: 1122 loss: 4.36520679e-07
Iter: 1123 loss: 4.36432657e-07
Iter: 1124 loss: 4.36259768e-07
Iter: 1125 loss: 4.35922402e-07
Iter: 1126 loss: 4.40472661e-07
Iter: 1127 loss: 4.35889604e-07
Iter: 1128 loss: 4.35479762e-07
Iter: 1129 loss: 4.38652364e-07
Iter: 1130 loss: 4.35448129e-07
Iter: 1131 loss: 4.35165362e-07
Iter: 1132 loss: 4.35965802e-07
Iter: 1133 loss: 4.35064351e-07
Iter: 1134 loss: 4.34711154e-07
Iter: 1135 loss: 4.34781498e-07
Iter: 1136 loss: 4.34460816e-07
Iter: 1137 loss: 4.34114554e-07
Iter: 1138 loss: 4.38756757e-07
Iter: 1139 loss: 4.34117112e-07
Iter: 1140 loss: 4.3380274e-07
Iter: 1141 loss: 4.33304535e-07
Iter: 1142 loss: 4.33302205e-07
Iter: 1143 loss: 4.32718963e-07
Iter: 1144 loss: 4.3347984e-07
Iter: 1145 loss: 4.3244836e-07
Iter: 1146 loss: 4.31859121e-07
Iter: 1147 loss: 4.36001699e-07
Iter: 1148 loss: 4.31777607e-07
Iter: 1149 loss: 4.31452946e-07
Iter: 1150 loss: 4.35933089e-07
Iter: 1151 loss: 4.31439247e-07
Iter: 1152 loss: 4.31052854e-07
Iter: 1153 loss: 4.30976513e-07
Iter: 1154 loss: 4.3075e-07
Iter: 1155 loss: 4.30551182e-07
Iter: 1156 loss: 4.30539217e-07
Iter: 1157 loss: 4.30317186e-07
Iter: 1158 loss: 4.29859142e-07
Iter: 1159 loss: 4.3521851e-07
Iter: 1160 loss: 4.29847631e-07
Iter: 1161 loss: 4.29435943e-07
Iter: 1162 loss: 4.33141622e-07
Iter: 1163 loss: 4.29413177e-07
Iter: 1164 loss: 4.29119382e-07
Iter: 1165 loss: 4.29795563e-07
Iter: 1166 loss: 4.29012601e-07
Iter: 1167 loss: 4.28608359e-07
Iter: 1168 loss: 4.28516614e-07
Iter: 1169 loss: 4.28268265e-07
Iter: 1170 loss: 4.27838273e-07
Iter: 1171 loss: 4.3106121e-07
Iter: 1172 loss: 4.27786e-07
Iter: 1173 loss: 4.27276404e-07
Iter: 1174 loss: 4.27569546e-07
Iter: 1175 loss: 4.26954472e-07
Iter: 1176 loss: 4.26450356e-07
Iter: 1177 loss: 4.26584506e-07
Iter: 1178 loss: 4.26102361e-07
Iter: 1179 loss: 4.25497063e-07
Iter: 1180 loss: 4.28786564e-07
Iter: 1181 loss: 4.25395569e-07
Iter: 1182 loss: 4.25001588e-07
Iter: 1183 loss: 4.27858879e-07
Iter: 1184 loss: 4.24958159e-07
Iter: 1185 loss: 4.24511654e-07
Iter: 1186 loss: 4.25827636e-07
Iter: 1187 loss: 4.24354027e-07
Iter: 1188 loss: 4.24134413e-07
Iter: 1189 loss: 4.26236596e-07
Iter: 1190 loss: 4.24101756e-07
Iter: 1191 loss: 4.23869267e-07
Iter: 1192 loss: 4.23508226e-07
Iter: 1193 loss: 4.23483925e-07
Iter: 1194 loss: 4.23074823e-07
Iter: 1195 loss: 4.24162579e-07
Iter: 1196 loss: 4.22947039e-07
Iter: 1197 loss: 4.22562948e-07
Iter: 1198 loss: 4.23751032e-07
Iter: 1199 loss: 4.22447584e-07
Iter: 1200 loss: 4.21933976e-07
Iter: 1201 loss: 4.22225753e-07
Iter: 1202 loss: 4.21598941e-07
Iter: 1203 loss: 4.21125947e-07
Iter: 1204 loss: 4.22679534e-07
Iter: 1205 loss: 4.20977301e-07
Iter: 1206 loss: 4.20422879e-07
Iter: 1207 loss: 4.23287588e-07
Iter: 1208 loss: 4.20335795e-07
Iter: 1209 loss: 4.19986634e-07
Iter: 1210 loss: 4.19698324e-07
Iter: 1211 loss: 4.19611979e-07
Iter: 1212 loss: 4.1910144e-07
Iter: 1213 loss: 4.22123748e-07
Iter: 1214 loss: 4.19026208e-07
Iter: 1215 loss: 4.18710215e-07
Iter: 1216 loss: 4.19764774e-07
Iter: 1217 loss: 4.18593515e-07
Iter: 1218 loss: 4.18239523e-07
Iter: 1219 loss: 4.22113715e-07
Iter: 1220 loss: 4.18242593e-07
Iter: 1221 loss: 4.18052878e-07
Iter: 1222 loss: 4.18179241e-07
Iter: 1223 loss: 4.17950503e-07
Iter: 1224 loss: 4.1763505e-07
Iter: 1225 loss: 4.17556976e-07
Iter: 1226 loss: 4.17318233e-07
Iter: 1227 loss: 4.1698479e-07
Iter: 1228 loss: 4.17121015e-07
Iter: 1229 loss: 4.16745877e-07
Iter: 1230 loss: 4.1629815e-07
Iter: 1231 loss: 4.18115974e-07
Iter: 1232 loss: 4.16199441e-07
Iter: 1233 loss: 4.15742875e-07
Iter: 1234 loss: 4.17098533e-07
Iter: 1235 loss: 4.15622054e-07
Iter: 1236 loss: 4.15258e-07
Iter: 1237 loss: 4.15184815e-07
Iter: 1238 loss: 4.14962756e-07
Iter: 1239 loss: 4.14487346e-07
Iter: 1240 loss: 4.21538459e-07
Iter: 1241 loss: 4.14490927e-07
Iter: 1242 loss: 4.14243942e-07
Iter: 1243 loss: 4.13879633e-07
Iter: 1244 loss: 4.13839388e-07
Iter: 1245 loss: 4.13314325e-07
Iter: 1246 loss: 4.15038954e-07
Iter: 1247 loss: 4.1318998e-07
Iter: 1248 loss: 4.12753309e-07
Iter: 1249 loss: 4.13630801e-07
Iter: 1250 loss: 4.12571524e-07
Iter: 1251 loss: 4.12249534e-07
Iter: 1252 loss: 4.12209943e-07
Iter: 1253 loss: 4.12016931e-07
Iter: 1254 loss: 4.11918222e-07
Iter: 1255 loss: 4.11836595e-07
Iter: 1256 loss: 4.11505482e-07
Iter: 1257 loss: 4.1386744e-07
Iter: 1258 loss: 4.11456313e-07
Iter: 1259 loss: 4.11212397e-07
Iter: 1260 loss: 4.10883928e-07
Iter: 1261 loss: 4.10888049e-07
Iter: 1262 loss: 4.1051257e-07
Iter: 1263 loss: 4.10963651e-07
Iter: 1264 loss: 4.10278915e-07
Iter: 1265 loss: 4.09812969e-07
Iter: 1266 loss: 4.14782619e-07
Iter: 1267 loss: 4.09812031e-07
Iter: 1268 loss: 4.0953509e-07
Iter: 1269 loss: 4.09177119e-07
Iter: 1270 loss: 4.09142416e-07
Iter: 1271 loss: 4.08865e-07
Iter: 1272 loss: 4.08840947e-07
Iter: 1273 loss: 4.08614e-07
Iter: 1274 loss: 4.08251537e-07
Iter: 1275 loss: 4.08245739e-07
Iter: 1276 loss: 4.07831294e-07
Iter: 1277 loss: 4.09442634e-07
Iter: 1278 loss: 4.07738924e-07
Iter: 1279 loss: 4.0736137e-07
Iter: 1280 loss: 4.0773449e-07
Iter: 1281 loss: 4.07166169e-07
Iter: 1282 loss: 4.07000641e-07
Iter: 1283 loss: 4.06908214e-07
Iter: 1284 loss: 4.06703748e-07
Iter: 1285 loss: 4.06523355e-07
Iter: 1286 loss: 4.06448e-07
Iter: 1287 loss: 4.06152395e-07
Iter: 1288 loss: 4.09898917e-07
Iter: 1289 loss: 4.06159359e-07
Iter: 1290 loss: 4.0595711e-07
Iter: 1291 loss: 4.05669368e-07
Iter: 1292 loss: 4.05666555e-07
Iter: 1293 loss: 4.05253502e-07
Iter: 1294 loss: 4.05307617e-07
Iter: 1295 loss: 4.04949e-07
Iter: 1296 loss: 4.04733612e-07
Iter: 1297 loss: 4.0463334e-07
Iter: 1298 loss: 4.04454624e-07
Iter: 1299 loss: 4.04014827e-07
Iter: 1300 loss: 4.12803956e-07
Iter: 1301 loss: 4.04024718e-07
Iter: 1302 loss: 4.03702018e-07
Iter: 1303 loss: 4.03700056e-07
Iter: 1304 loss: 4.03466885e-07
Iter: 1305 loss: 4.03558943e-07
Iter: 1306 loss: 4.03297634e-07
Iter: 1307 loss: 4.03010063e-07
Iter: 1308 loss: 4.03251079e-07
Iter: 1309 loss: 4.0282498e-07
Iter: 1310 loss: 4.02439952e-07
Iter: 1311 loss: 4.02618213e-07
Iter: 1312 loss: 4.0213979e-07
Iter: 1313 loss: 4.01912246e-07
Iter: 1314 loss: 4.01874217e-07
Iter: 1315 loss: 4.01604723e-07
Iter: 1316 loss: 4.01574141e-07
Iter: 1317 loss: 4.01389173e-07
Iter: 1318 loss: 4.01070537e-07
Iter: 1319 loss: 4.03911656e-07
Iter: 1320 loss: 4.01037767e-07
Iter: 1321 loss: 4.00742749e-07
Iter: 1322 loss: 4.00752384e-07
Iter: 1323 loss: 4.00493207e-07
Iter: 1324 loss: 4.00220557e-07
Iter: 1325 loss: 3.99737758e-07
Iter: 1326 loss: 3.99735711e-07
Iter: 1327 loss: 3.99453967e-07
Iter: 1328 loss: 3.99425858e-07
Iter: 1329 loss: 3.9915443e-07
Iter: 1330 loss: 3.98975203e-07
Iter: 1331 loss: 3.98828263e-07
Iter: 1332 loss: 3.98411544e-07
Iter: 1333 loss: 3.99959703e-07
Iter: 1334 loss: 3.98313745e-07
Iter: 1335 loss: 3.97989169e-07
Iter: 1336 loss: 4.00330975e-07
Iter: 1337 loss: 3.97936105e-07
Iter: 1338 loss: 3.97646033e-07
Iter: 1339 loss: 3.97421161e-07
Iter: 1340 loss: 3.97342262e-07
Iter: 1341 loss: 3.9689121e-07
Iter: 1342 loss: 3.9694703e-07
Iter: 1343 loss: 3.9655481e-07
Iter: 1344 loss: 3.96251608e-07
Iter: 1345 loss: 3.96240551e-07
Iter: 1346 loss: 3.95925099e-07
Iter: 1347 loss: 3.96735231e-07
Iter: 1348 loss: 3.95811583e-07
Iter: 1349 loss: 3.95575512e-07
Iter: 1350 loss: 3.9656112e-07
Iter: 1351 loss: 3.95512529e-07
Iter: 1352 loss: 3.95237777e-07
Iter: 1353 loss: 3.95923792e-07
Iter: 1354 loss: 3.95141e-07
Iter: 1355 loss: 3.94921244e-07
Iter: 1356 loss: 3.94536272e-07
Iter: 1357 loss: 4.03305052e-07
Iter: 1358 loss: 3.94536698e-07
Iter: 1359 loss: 3.94180745e-07
Iter: 1360 loss: 3.96938702e-07
Iter: 1361 loss: 3.94150248e-07
Iter: 1362 loss: 3.93770733e-07
Iter: 1363 loss: 3.95332108e-07
Iter: 1364 loss: 3.93684275e-07
Iter: 1365 loss: 3.93403241e-07
Iter: 1366 loss: 3.93729749e-07
Iter: 1367 loss: 3.93262894e-07
Iter: 1368 loss: 3.92982429e-07
Iter: 1369 loss: 3.94388906e-07
Iter: 1370 loss: 3.92951677e-07
Iter: 1371 loss: 3.92576879e-07
Iter: 1372 loss: 3.92831026e-07
Iter: 1373 loss: 3.92373664e-07
Iter: 1374 loss: 3.92046616e-07
Iter: 1375 loss: 3.91706408e-07
Iter: 1376 loss: 3.9164695e-07
Iter: 1377 loss: 3.91184869e-07
Iter: 1378 loss: 3.95667371e-07
Iter: 1379 loss: 3.91172932e-07
Iter: 1380 loss: 3.90802541e-07
Iter: 1381 loss: 3.95700596e-07
Iter: 1382 loss: 3.90800949e-07
Iter: 1383 loss: 3.90561496e-07
Iter: 1384 loss: 3.90639343e-07
Iter: 1385 loss: 3.9034984e-07
Iter: 1386 loss: 3.90087848e-07
Iter: 1387 loss: 3.92973163e-07
Iter: 1388 loss: 3.90084324e-07
Iter: 1389 loss: 3.89883866e-07
Iter: 1390 loss: 3.89447223e-07
Iter: 1391 loss: 3.95073016e-07
Iter: 1392 loss: 3.89418688e-07
Iter: 1393 loss: 3.89046363e-07
Iter: 1394 loss: 3.91116203e-07
Iter: 1395 loss: 3.88999041e-07
Iter: 1396 loss: 3.8873992e-07
Iter: 1397 loss: 3.92805731e-07
Iter: 1398 loss: 3.8872804e-07
Iter: 1399 loss: 3.88521e-07
Iter: 1400 loss: 3.8835617e-07
Iter: 1401 loss: 3.88297792e-07
Iter: 1402 loss: 3.87944254e-07
Iter: 1403 loss: 3.89497814e-07
Iter: 1404 loss: 3.87889912e-07
Iter: 1405 loss: 3.8757247e-07
Iter: 1406 loss: 3.88967635e-07
Iter: 1407 loss: 3.87472312e-07
Iter: 1408 loss: 3.87171355e-07
Iter: 1409 loss: 3.86616762e-07
Iter: 1410 loss: 3.97106419e-07
Iter: 1411 loss: 3.86614374e-07
Iter: 1412 loss: 3.86122395e-07
Iter: 1413 loss: 3.91631545e-07
Iter: 1414 loss: 3.86105569e-07
Iter: 1415 loss: 3.85816719e-07
Iter: 1416 loss: 3.85822887e-07
Iter: 1417 loss: 3.85550265e-07
Iter: 1418 loss: 3.85789974e-07
Iter: 1419 loss: 3.85443229e-07
Iter: 1420 loss: 3.85227651e-07
Iter: 1421 loss: 3.86820716e-07
Iter: 1422 loss: 3.85205141e-07
Iter: 1423 loss: 3.85006047e-07
Iter: 1424 loss: 3.84570171e-07
Iter: 1425 loss: 3.93188941e-07
Iter: 1426 loss: 3.84559769e-07
Iter: 1427 loss: 3.84250257e-07
Iter: 1428 loss: 3.84584496e-07
Iter: 1429 loss: 3.84050225e-07
Iter: 1430 loss: 3.83756799e-07
Iter: 1431 loss: 3.83749523e-07
Iter: 1432 loss: 3.83491937e-07
Iter: 1433 loss: 3.8332314e-07
Iter: 1434 loss: 3.83243531e-07
Iter: 1435 loss: 3.82840938e-07
Iter: 1436 loss: 3.84233317e-07
Iter: 1437 loss: 3.82751409e-07
Iter: 1438 loss: 3.82440334e-07
Iter: 1439 loss: 3.84699661e-07
Iter: 1440 loss: 3.82400231e-07
Iter: 1441 loss: 3.82139831e-07
Iter: 1442 loss: 3.81672692e-07
Iter: 1443 loss: 3.81674397e-07
Iter: 1444 loss: 3.81266062e-07
Iter: 1445 loss: 3.83233584e-07
Iter: 1446 loss: 3.81166842e-07
Iter: 1447 loss: 3.80853237e-07
Iter: 1448 loss: 3.85224723e-07
Iter: 1449 loss: 3.80839538e-07
Iter: 1450 loss: 3.80590507e-07
Iter: 1451 loss: 3.82378147e-07
Iter: 1452 loss: 3.80582236e-07
Iter: 1453 loss: 3.80435438e-07
Iter: 1454 loss: 3.80597072e-07
Iter: 1455 loss: 3.80376605e-07
Iter: 1456 loss: 3.80134395e-07
Iter: 1457 loss: 3.80034521e-07
Iter: 1458 loss: 3.79919726e-07
Iter: 1459 loss: 3.79664357e-07
Iter: 1460 loss: 3.79298143e-07
Iter: 1461 loss: 3.79267647e-07
Iter: 1462 loss: 3.78958816e-07
Iter: 1463 loss: 3.78963136e-07
Iter: 1464 loss: 3.78678521e-07
Iter: 1465 loss: 3.79159161e-07
Iter: 1466 loss: 3.78533827e-07
Iter: 1467 loss: 3.78233551e-07
Iter: 1468 loss: 3.78527488e-07
Iter: 1469 loss: 3.78079221e-07
Iter: 1470 loss: 3.77791423e-07
Iter: 1471 loss: 3.80732217e-07
Iter: 1472 loss: 3.77766696e-07
Iter: 1473 loss: 3.77500726e-07
Iter: 1474 loss: 3.77374761e-07
Iter: 1475 loss: 3.77247773e-07
Iter: 1476 loss: 3.76964465e-07
Iter: 1477 loss: 3.77186666e-07
Iter: 1478 loss: 3.767652e-07
Iter: 1479 loss: 3.76395406e-07
Iter: 1480 loss: 3.78603033e-07
Iter: 1481 loss: 3.76327648e-07
Iter: 1482 loss: 3.76138985e-07
Iter: 1483 loss: 3.7611639e-07
Iter: 1484 loss: 3.75995398e-07
Iter: 1485 loss: 3.75849879e-07
Iter: 1486 loss: 3.75828904e-07
Iter: 1487 loss: 3.75579305e-07
Iter: 1488 loss: 3.77340768e-07
Iter: 1489 loss: 3.75570949e-07
Iter: 1490 loss: 3.75429e-07
Iter: 1491 loss: 3.75103e-07
Iter: 1492 loss: 3.77891354e-07
Iter: 1493 loss: 3.75059244e-07
Iter: 1494 loss: 3.74696583e-07
Iter: 1495 loss: 3.77611713e-07
Iter: 1496 loss: 3.74668502e-07
Iter: 1497 loss: 3.74364475e-07
Iter: 1498 loss: 3.75425032e-07
Iter: 1499 loss: 3.74272702e-07
Iter: 1500 loss: 3.73959125e-07
Iter: 1501 loss: 3.74817091e-07
Iter: 1502 loss: 3.73817727e-07
Iter: 1503 loss: 3.73606895e-07
Iter: 1504 loss: 3.73845666e-07
Iter: 1505 loss: 3.73460551e-07
Iter: 1506 loss: 3.73102125e-07
Iter: 1507 loss: 3.75418381e-07
Iter: 1508 loss: 3.73053751e-07
Iter: 1509 loss: 3.72888792e-07
Iter: 1510 loss: 3.72688e-07
Iter: 1511 loss: 3.72643171e-07
Iter: 1512 loss: 3.72289435e-07
Iter: 1513 loss: 3.73148e-07
Iter: 1514 loss: 3.72196212e-07
Iter: 1515 loss: 3.71976313e-07
Iter: 1516 loss: 3.719733e-07
Iter: 1517 loss: 3.71757068e-07
Iter: 1518 loss: 3.72495094e-07
Iter: 1519 loss: 3.71671064e-07
Iter: 1520 loss: 3.71525402e-07
Iter: 1521 loss: 3.72002148e-07
Iter: 1522 loss: 3.71504285e-07
Iter: 1523 loss: 3.71322756e-07
Iter: 1524 loss: 3.71177549e-07
Iter: 1525 loss: 3.71101805e-07
Iter: 1526 loss: 3.70869e-07
Iter: 1527 loss: 3.71064573e-07
Iter: 1528 loss: 3.70686678e-07
Iter: 1529 loss: 3.70428666e-07
Iter: 1530 loss: 3.70547468e-07
Iter: 1531 loss: 3.70233636e-07
Iter: 1532 loss: 3.70014675e-07
Iter: 1533 loss: 3.69980455e-07
Iter: 1534 loss: 3.69834481e-07
Iter: 1535 loss: 3.6947921e-07
Iter: 1536 loss: 3.74250078e-07
Iter: 1537 loss: 3.69461674e-07
Iter: 1538 loss: 3.69128117e-07
Iter: 1539 loss: 3.74111721e-07
Iter: 1540 loss: 3.69118283e-07
Iter: 1541 loss: 3.68886e-07
Iter: 1542 loss: 3.69753593e-07
Iter: 1543 loss: 3.68817723e-07
Iter: 1544 loss: 3.68566816e-07
Iter: 1545 loss: 3.68364425e-07
Iter: 1546 loss: 3.68279871e-07
Iter: 1547 loss: 3.67885832e-07
Iter: 1548 loss: 3.6863608e-07
Iter: 1549 loss: 3.67723317e-07
Iter: 1550 loss: 3.67575979e-07
Iter: 1551 loss: 3.67511575e-07
Iter: 1552 loss: 3.6734815e-07
Iter: 1553 loss: 3.67236737e-07
Iter: 1554 loss: 3.67169378e-07
Iter: 1555 loss: 3.66900565e-07
Iter: 1556 loss: 3.68358712e-07
Iter: 1557 loss: 3.66871035e-07
Iter: 1558 loss: 3.66672054e-07
Iter: 1559 loss: 3.66534778e-07
Iter: 1560 loss: 3.66474154e-07
Iter: 1561 loss: 3.66211196e-07
Iter: 1562 loss: 3.66065734e-07
Iter: 1563 loss: 3.6595614e-07
Iter: 1564 loss: 3.65737577e-07
Iter: 1565 loss: 3.65708729e-07
Iter: 1566 loss: 3.65512108e-07
Iter: 1567 loss: 3.65169967e-07
Iter: 1568 loss: 3.65177669e-07
Iter: 1569 loss: 3.64796676e-07
Iter: 1570 loss: 3.67073767e-07
Iter: 1571 loss: 3.64759046e-07
Iter: 1572 loss: 3.64488585e-07
Iter: 1573 loss: 3.66249537e-07
Iter: 1574 loss: 3.64457151e-07
Iter: 1575 loss: 3.64198854e-07
Iter: 1576 loss: 3.64476591e-07
Iter: 1577 loss: 3.64051346e-07
Iter: 1578 loss: 3.63765594e-07
Iter: 1579 loss: 3.63508462e-07
Iter: 1580 loss: 3.63423766e-07
Iter: 1581 loss: 3.6333978e-07
Iter: 1582 loss: 3.63200456e-07
Iter: 1583 loss: 3.63027652e-07
Iter: 1584 loss: 3.63007587e-07
Iter: 1585 loss: 3.62879234e-07
Iter: 1586 loss: 3.62699041e-07
Iter: 1587 loss: 3.63750303e-07
Iter: 1588 loss: 3.62657033e-07
Iter: 1589 loss: 3.62473088e-07
Iter: 1590 loss: 3.6230162e-07
Iter: 1591 loss: 3.62226785e-07
Iter: 1592 loss: 3.61934326e-07
Iter: 1593 loss: 3.61695925e-07
Iter: 1594 loss: 3.61630242e-07
Iter: 1595 loss: 3.61286709e-07
Iter: 1596 loss: 3.65961398e-07
Iter: 1597 loss: 3.61286482e-07
Iter: 1598 loss: 3.60972734e-07
Iter: 1599 loss: 3.617738e-07
Iter: 1600 loss: 3.60851459e-07
Iter: 1601 loss: 3.6056943e-07
Iter: 1602 loss: 3.60809e-07
Iter: 1603 loss: 3.60384036e-07
Iter: 1604 loss: 3.60059033e-07
Iter: 1605 loss: 3.60588331e-07
Iter: 1606 loss: 3.59924684e-07
Iter: 1607 loss: 3.59550882e-07
Iter: 1608 loss: 3.62620369e-07
Iter: 1609 loss: 3.59511375e-07
Iter: 1610 loss: 3.59317482e-07
Iter: 1611 loss: 3.590917e-07
Iter: 1612 loss: 3.5906919e-07
Iter: 1613 loss: 3.58893402e-07
Iter: 1614 loss: 3.58874615e-07
Iter: 1615 loss: 3.58704369e-07
Iter: 1616 loss: 3.59006378e-07
Iter: 1617 loss: 3.58636385e-07
Iter: 1618 loss: 3.58492883e-07
Iter: 1619 loss: 3.58633201e-07
Iter: 1620 loss: 3.58406652e-07
Iter: 1621 loss: 3.58188117e-07
Iter: 1622 loss: 3.5891776e-07
Iter: 1623 loss: 3.58148839e-07
Iter: 1624 loss: 3.58001898e-07
Iter: 1625 loss: 3.57742749e-07
Iter: 1626 loss: 3.63687548e-07
Iter: 1627 loss: 3.57728766e-07
Iter: 1628 loss: 3.57430338e-07
Iter: 1629 loss: 3.59093406e-07
Iter: 1630 loss: 3.57420447e-07
Iter: 1631 loss: 3.57164197e-07
Iter: 1632 loss: 3.59270928e-07
Iter: 1633 loss: 3.57144188e-07
Iter: 1634 loss: 3.56923636e-07
Iter: 1635 loss: 3.56947112e-07
Iter: 1636 loss: 3.56759386e-07
Iter: 1637 loss: 3.56471674e-07
Iter: 1638 loss: 3.56551112e-07
Iter: 1639 loss: 3.56254503e-07
Iter: 1640 loss: 3.55987879e-07
Iter: 1641 loss: 3.55966591e-07
Iter: 1642 loss: 3.55808254e-07
Iter: 1643 loss: 3.55472338e-07
Iter: 1644 loss: 3.62825176e-07
Iter: 1645 loss: 3.55468558e-07
Iter: 1646 loss: 3.55198324e-07
Iter: 1647 loss: 3.58564591e-07
Iter: 1648 loss: 3.55181271e-07
Iter: 1649 loss: 3.54965096e-07
Iter: 1650 loss: 3.57277258e-07
Iter: 1651 loss: 3.54930791e-07
Iter: 1652 loss: 3.54767252e-07
Iter: 1653 loss: 3.54691622e-07
Iter: 1654 loss: 3.54622188e-07
Iter: 1655 loss: 3.54413032e-07
Iter: 1656 loss: 3.56505154e-07
Iter: 1657 loss: 3.54397741e-07
Iter: 1658 loss: 3.54275784e-07
Iter: 1659 loss: 3.54015754e-07
Iter: 1660 loss: 3.57699662e-07
Iter: 1661 loss: 3.5398898e-07
Iter: 1662 loss: 3.53707208e-07
Iter: 1663 loss: 3.54456091e-07
Iter: 1664 loss: 3.536191e-07
Iter: 1665 loss: 3.53386525e-07
Iter: 1666 loss: 3.53375725e-07
Iter: 1667 loss: 3.53194679e-07
Iter: 1668 loss: 3.53252574e-07
Iter: 1669 loss: 3.53049359e-07
Iter: 1670 loss: 3.52782308e-07
Iter: 1671 loss: 3.52937377e-07
Iter: 1672 loss: 3.52640598e-07
Iter: 1673 loss: 3.52437866e-07
Iter: 1674 loss: 3.55644318e-07
Iter: 1675 loss: 3.52431243e-07
Iter: 1676 loss: 3.52218052e-07
Iter: 1677 loss: 3.51858347e-07
Iter: 1678 loss: 3.51852862e-07
Iter: 1679 loss: 3.51525415e-07
Iter: 1680 loss: 3.52565337e-07
Iter: 1681 loss: 3.51444896e-07
Iter: 1682 loss: 3.51286701e-07
Iter: 1683 loss: 3.51252254e-07
Iter: 1684 loss: 3.51118842e-07
Iter: 1685 loss: 3.51014933e-07
Iter: 1686 loss: 3.50959169e-07
Iter: 1687 loss: 3.50785854e-07
Iter: 1688 loss: 3.52007646e-07
Iter: 1689 loss: 3.50769085e-07
Iter: 1690 loss: 3.50606143e-07
Iter: 1691 loss: 3.50395567e-07
Iter: 1692 loss: 3.50386927e-07
Iter: 1693 loss: 3.50174e-07
Iter: 1694 loss: 3.50017729e-07
Iter: 1695 loss: 3.49918e-07
Iter: 1696 loss: 3.49700201e-07
Iter: 1697 loss: 3.49694e-07
Iter: 1698 loss: 3.49489255e-07
Iter: 1699 loss: 3.49784983e-07
Iter: 1700 loss: 3.49394668e-07
Iter: 1701 loss: 3.49178265e-07
Iter: 1702 loss: 3.49358118e-07
Iter: 1703 loss: 3.49048378e-07
Iter: 1704 loss: 3.48839933e-07
Iter: 1705 loss: 3.49627271e-07
Iter: 1706 loss: 3.48788944e-07
Iter: 1707 loss: 3.48515698e-07
Iter: 1708 loss: 3.49865019e-07
Iter: 1709 loss: 3.48468745e-07
Iter: 1710 loss: 3.48320668e-07
Iter: 1711 loss: 3.48115321e-07
Iter: 1712 loss: 3.48096648e-07
Iter: 1713 loss: 3.47957638e-07
Iter: 1714 loss: 3.47941977e-07
Iter: 1715 loss: 3.47772925e-07
Iter: 1716 loss: 3.4784469e-07
Iter: 1717 loss: 3.47678338e-07
Iter: 1718 loss: 3.47497746e-07
Iter: 1719 loss: 3.47761159e-07
Iter: 1720 loss: 3.47429534e-07
Iter: 1721 loss: 3.47175387e-07
Iter: 1722 loss: 3.47705338e-07
Iter: 1723 loss: 3.47104219e-07
Iter: 1724 loss: 3.469641e-07
Iter: 1725 loss: 3.46689205e-07
Iter: 1726 loss: 3.52372354e-07
Iter: 1727 loss: 3.46669509e-07
Iter: 1728 loss: 3.46406068e-07
Iter: 1729 loss: 3.47998139e-07
Iter: 1730 loss: 3.46342745e-07
Iter: 1731 loss: 3.46070948e-07
Iter: 1732 loss: 3.49449465e-07
Iter: 1733 loss: 3.4607271e-07
Iter: 1734 loss: 3.45897604e-07
Iter: 1735 loss: 3.46044317e-07
Iter: 1736 loss: 3.4581916e-07
Iter: 1737 loss: 3.45627825e-07
Iter: 1738 loss: 3.45618673e-07
Iter: 1739 loss: 3.4547935e-07
Iter: 1740 loss: 3.45267694e-07
Iter: 1741 loss: 3.4526505e-07
Iter: 1742 loss: 3.45125443e-07
Iter: 1743 loss: 3.44878856e-07
Iter: 1744 loss: 3.44883858e-07
Iter: 1745 loss: 3.44783359e-07
Iter: 1746 loss: 3.4477452e-07
Iter: 1747 loss: 3.44639631e-07
Iter: 1748 loss: 3.44875019e-07
Iter: 1749 loss: 3.44603848e-07
Iter: 1750 loss: 3.4448658e-07
Iter: 1751 loss: 3.44439968e-07
Iter: 1752 loss: 3.44371e-07
Iter: 1753 loss: 3.44163482e-07
Iter: 1754 loss: 3.45332e-07
Iter: 1755 loss: 3.44113573e-07
Iter: 1756 loss: 3.43979053e-07
Iter: 1757 loss: 3.43612129e-07
Iter: 1758 loss: 3.48362732e-07
Iter: 1759 loss: 3.4358834e-07
Iter: 1760 loss: 3.43299178e-07
Iter: 1761 loss: 3.45034834e-07
Iter: 1762 loss: 3.43228123e-07
Iter: 1763 loss: 3.42972271e-07
Iter: 1764 loss: 3.45166825e-07
Iter: 1765 loss: 3.42973692e-07
Iter: 1766 loss: 3.42755584e-07
Iter: 1767 loss: 3.43618922e-07
Iter: 1768 loss: 3.42692488e-07
Iter: 1769 loss: 3.42499476e-07
Iter: 1770 loss: 3.42315758e-07
Iter: 1771 loss: 3.4227898e-07
Iter: 1772 loss: 3.42096939e-07
Iter: 1773 loss: 3.44717193e-07
Iter: 1774 loss: 3.42087446e-07
Iter: 1775 loss: 3.41917598e-07
Iter: 1776 loss: 3.42210797e-07
Iter: 1777 loss: 3.41848249e-07
Iter: 1778 loss: 3.41707221e-07
Iter: 1779 loss: 3.42255504e-07
Iter: 1780 loss: 3.41675218e-07
Iter: 1781 loss: 3.41520945e-07
Iter: 1782 loss: 3.42711587e-07
Iter: 1783 loss: 3.41516483e-07
Iter: 1784 loss: 3.41401091e-07
Iter: 1785 loss: 3.41265604e-07
Iter: 1786 loss: 3.41251877e-07
Iter: 1787 loss: 3.41102e-07
Iter: 1788 loss: 3.43256119e-07
Iter: 1789 loss: 3.41105135e-07
Iter: 1790 loss: 3.40963084e-07
Iter: 1791 loss: 3.40718486e-07
Iter: 1792 loss: 3.46046448e-07
Iter: 1793 loss: 3.40716383e-07
Iter: 1794 loss: 3.4047909e-07
Iter: 1795 loss: 3.41580403e-07
Iter: 1796 loss: 3.40445865e-07
Iter: 1797 loss: 3.40220083e-07
Iter: 1798 loss: 3.40447e-07
Iter: 1799 loss: 3.40096221e-07
Iter: 1800 loss: 3.39953658e-07
Iter: 1801 loss: 3.39950532e-07
Iter: 1802 loss: 3.39812544e-07
Iter: 1803 loss: 3.39632351e-07
Iter: 1804 loss: 3.39622432e-07
Iter: 1805 loss: 3.39428311e-07
Iter: 1806 loss: 3.39956557e-07
Iter: 1807 loss: 3.3935919e-07
Iter: 1808 loss: 3.39189228e-07
Iter: 1809 loss: 3.39181184e-07
Iter: 1810 loss: 3.39071676e-07
Iter: 1811 loss: 3.39023188e-07
Iter: 1812 loss: 3.38947871e-07
Iter: 1813 loss: 3.38835122e-07
Iter: 1814 loss: 3.38844757e-07
Iter: 1815 loss: 3.38730899e-07
Iter: 1816 loss: 3.38594646e-07
Iter: 1817 loss: 3.3857566e-07
Iter: 1818 loss: 3.38450832e-07
Iter: 1819 loss: 3.39959485e-07
Iter: 1820 loss: 3.38440714e-07
Iter: 1821 loss: 3.3833777e-07
Iter: 1822 loss: 3.38179348e-07
Iter: 1823 loss: 3.38175369e-07
Iter: 1824 loss: 3.37969198e-07
Iter: 1825 loss: 3.38326174e-07
Iter: 1826 loss: 3.37903657e-07
Iter: 1827 loss: 3.37711015e-07
Iter: 1828 loss: 3.37945153e-07
Iter: 1829 loss: 3.37599744e-07
Iter: 1830 loss: 3.37457379e-07
Iter: 1831 loss: 3.37440554e-07
Iter: 1832 loss: 3.37333773e-07
Iter: 1833 loss: 3.37359808e-07
Iter: 1834 loss: 3.37243193e-07
Iter: 1835 loss: 3.37116091e-07
Iter: 1836 loss: 3.37055781e-07
Iter: 1837 loss: 3.36983135e-07
Iter: 1838 loss: 3.3681053e-07
Iter: 1839 loss: 3.3926014e-07
Iter: 1840 loss: 3.36818175e-07
Iter: 1841 loss: 3.36663732e-07
Iter: 1842 loss: 3.36718699e-07
Iter: 1843 loss: 3.36546464e-07
Iter: 1844 loss: 3.36405265e-07
Iter: 1845 loss: 3.37103188e-07
Iter: 1846 loss: 3.36347028e-07
Iter: 1847 loss: 3.3616206e-07
Iter: 1848 loss: 3.36693205e-07
Iter: 1849 loss: 3.36077733e-07
Iter: 1850 loss: 3.35989284e-07
Iter: 1851 loss: 3.36180562e-07
Iter: 1852 loss: 3.35962682e-07
Iter: 1853 loss: 3.35791356e-07
Iter: 1854 loss: 3.35887506e-07
Iter: 1855 loss: 3.35675139e-07
Iter: 1856 loss: 3.3551396e-07
Iter: 1857 loss: 3.35660417e-07
Iter: 1858 loss: 3.35435629e-07
Iter: 1859 loss: 3.35237075e-07
Iter: 1860 loss: 3.35246312e-07
Iter: 1861 loss: 3.35068364e-07
Iter: 1862 loss: 3.34920657e-07
Iter: 1863 loss: 3.34913409e-07
Iter: 1864 loss: 3.34741202e-07
Iter: 1865 loss: 3.34907838e-07
Iter: 1866 loss: 3.34668243e-07
Iter: 1867 loss: 3.34511924e-07
Iter: 1868 loss: 3.3463607e-07
Iter: 1869 loss: 3.34383657e-07
Iter: 1870 loss: 3.34240781e-07
Iter: 1871 loss: 3.34825671e-07
Iter: 1872 loss: 3.34176434e-07
Iter: 1873 loss: 3.33989135e-07
Iter: 1874 loss: 3.35557417e-07
Iter: 1875 loss: 3.33996411e-07
Iter: 1876 loss: 3.33874198e-07
Iter: 1877 loss: 3.33720976e-07
Iter: 1878 loss: 3.33704691e-07
Iter: 1879 loss: 3.33566902e-07
Iter: 1880 loss: 3.33556841e-07
Iter: 1881 loss: 3.3348681e-07
Iter: 1882 loss: 3.33336232e-07
Iter: 1883 loss: 3.34682511e-07
Iter: 1884 loss: 3.33310197e-07
Iter: 1885 loss: 3.33202905e-07
Iter: 1886 loss: 3.33183806e-07
Iter: 1887 loss: 3.33085666e-07
Iter: 1888 loss: 3.33017226e-07
Iter: 1889 loss: 3.3298e-07
Iter: 1890 loss: 3.32840614e-07
Iter: 1891 loss: 3.32687875e-07
Iter: 1892 loss: 3.32659738e-07
Iter: 1893 loss: 3.32374555e-07
Iter: 1894 loss: 3.32492135e-07
Iter: 1895 loss: 3.32202035e-07
Iter: 1896 loss: 3.32023035e-07
Iter: 1897 loss: 3.3199683e-07
Iter: 1898 loss: 3.3187132e-07
Iter: 1899 loss: 3.32230115e-07
Iter: 1900 loss: 3.31822719e-07
Iter: 1901 loss: 3.31683964e-07
Iter: 1902 loss: 3.31464406e-07
Iter: 1903 loss: 3.31471568e-07
Iter: 1904 loss: 3.31333e-07
Iter: 1905 loss: 3.3132369e-07
Iter: 1906 loss: 3.31186016e-07
Iter: 1907 loss: 3.30999114e-07
Iter: 1908 loss: 3.30984108e-07
Iter: 1909 loss: 3.30900633e-07
Iter: 1910 loss: 3.30869909e-07
Iter: 1911 loss: 3.3079661e-07
Iter: 1912 loss: 3.306248e-07
Iter: 1913 loss: 3.33344531e-07
Iter: 1914 loss: 3.30613403e-07
Iter: 1915 loss: 3.30449666e-07
Iter: 1916 loss: 3.31059255e-07
Iter: 1917 loss: 3.30383102e-07
Iter: 1918 loss: 3.30255631e-07
Iter: 1919 loss: 3.31619276e-07
Iter: 1920 loss: 3.30248099e-07
Iter: 1921 loss: 3.30132252e-07
Iter: 1922 loss: 3.29992901e-07
Iter: 1923 loss: 3.29976302e-07
Iter: 1924 loss: 3.2975754e-07
Iter: 1925 loss: 3.29661901e-07
Iter: 1926 loss: 3.29529e-07
Iter: 1927 loss: 3.29309131e-07
Iter: 1928 loss: 3.32109522e-07
Iter: 1929 loss: 3.29306943e-07
Iter: 1930 loss: 3.29148406e-07
Iter: 1931 loss: 3.29854e-07
Iter: 1932 loss: 3.29132547e-07
Iter: 1933 loss: 3.2898248e-07
Iter: 1934 loss: 3.29765385e-07
Iter: 1935 loss: 3.28959345e-07
Iter: 1936 loss: 3.28832726e-07
Iter: 1937 loss: 3.28622576e-07
Iter: 1938 loss: 3.32440663e-07
Iter: 1939 loss: 3.28615215e-07
Iter: 1940 loss: 3.28517302e-07
Iter: 1941 loss: 3.28478905e-07
Iter: 1942 loss: 3.28368429e-07
Iter: 1943 loss: 3.28454803e-07
Iter: 1944 loss: 3.28306e-07
Iter: 1945 loss: 3.28155437e-07
Iter: 1946 loss: 3.29245182e-07
Iter: 1947 loss: 3.28151884e-07
Iter: 1948 loss: 3.28049055e-07
Iter: 1949 loss: 3.27953586e-07
Iter: 1950 loss: 3.27928888e-07
Iter: 1951 loss: 3.27780839e-07
Iter: 1952 loss: 3.27881139e-07
Iter: 1953 loss: 3.2767673e-07
Iter: 1954 loss: 3.27511856e-07
Iter: 1955 loss: 3.29372654e-07
Iter: 1956 loss: 3.27487783e-07
Iter: 1957 loss: 3.27390865e-07
Iter: 1958 loss: 3.2714911e-07
Iter: 1959 loss: 3.30803175e-07
Iter: 1960 loss: 3.27142686e-07
Iter: 1961 loss: 3.26885981e-07
Iter: 1962 loss: 3.27926074e-07
Iter: 1963 loss: 3.26826353e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2
+ date
Mon Oct 26 13:02:00 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241e98f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241e98488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241e4e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241ee1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241f23488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241f23c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241dbbbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241d4fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241db5158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241d1fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241cdb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241cbd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241cbd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241cad620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241c467b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241c46d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241c2f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241beb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241bebbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241b7bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241b8b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241b6f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241b176a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241b227b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241b22730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241ad36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241a94840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241a5d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241a5d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32419fabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3241a10950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32419dd510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32419ddea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32419c6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f321753db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f321753d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 8.30938516e-05
Iter: 2 loss: 6.91588502e-05
Iter: 3 loss: 6.89458684e-05
Iter: 4 loss: 6.12253934e-05
Iter: 5 loss: 0.000124349841
Iter: 6 loss: 6.07215115e-05
Iter: 7 loss: 5.64324801e-05
Iter: 8 loss: 4.8862581e-05
Iter: 9 loss: 4.88633596e-05
Iter: 10 loss: 4.1755964e-05
Iter: 11 loss: 0.000112422116
Iter: 12 loss: 4.15165341e-05
Iter: 13 loss: 3.73532384e-05
Iter: 14 loss: 4.66820202e-05
Iter: 15 loss: 3.57741301e-05
Iter: 16 loss: 3.13933706e-05
Iter: 17 loss: 4.05185601e-05
Iter: 18 loss: 2.96484814e-05
Iter: 19 loss: 2.70082382e-05
Iter: 20 loss: 3.12195298e-05
Iter: 21 loss: 2.57830034e-05
Iter: 22 loss: 2.27168366e-05
Iter: 23 loss: 3.9985116e-05
Iter: 24 loss: 2.22896306e-05
Iter: 25 loss: 2.06695404e-05
Iter: 26 loss: 3.15546858e-05
Iter: 27 loss: 2.05073356e-05
Iter: 28 loss: 1.93829765e-05
Iter: 29 loss: 1.97325135e-05
Iter: 30 loss: 1.85798854e-05
Iter: 31 loss: 1.71950705e-05
Iter: 32 loss: 1.8443312e-05
Iter: 33 loss: 1.63901695e-05
Iter: 34 loss: 1.50846517e-05
Iter: 35 loss: 3.30115581e-05
Iter: 36 loss: 1.5079735e-05
Iter: 37 loss: 1.43953757e-05
Iter: 38 loss: 1.36909057e-05
Iter: 39 loss: 1.35607279e-05
Iter: 40 loss: 1.34016154e-05
Iter: 41 loss: 1.286515e-05
Iter: 42 loss: 1.26389787e-05
Iter: 43 loss: 1.22208367e-05
Iter: 44 loss: 2.18833229e-05
Iter: 45 loss: 1.22203492e-05
Iter: 46 loss: 1.17185518e-05
Iter: 47 loss: 1.28441188e-05
Iter: 48 loss: 1.15286566e-05
Iter: 49 loss: 1.09441144e-05
Iter: 50 loss: 1.37771758e-05
Iter: 51 loss: 1.08414424e-05
Iter: 52 loss: 1.05245181e-05
Iter: 53 loss: 1.06341595e-05
Iter: 54 loss: 1.03007296e-05
Iter: 55 loss: 9.77980562e-06
Iter: 56 loss: 1.11490299e-05
Iter: 57 loss: 9.60381294e-06
Iter: 58 loss: 9.29847738e-06
Iter: 59 loss: 9.71704321e-06
Iter: 60 loss: 9.14645079e-06
Iter: 61 loss: 8.73960744e-06
Iter: 62 loss: 1.09755874e-05
Iter: 63 loss: 8.6808468e-06
Iter: 64 loss: 8.44291e-06
Iter: 65 loss: 8.74801117e-06
Iter: 66 loss: 8.32117439e-06
Iter: 67 loss: 8.05686796e-06
Iter: 68 loss: 8.87414353e-06
Iter: 69 loss: 7.97899884e-06
Iter: 70 loss: 7.71584382e-06
Iter: 71 loss: 7.6404649e-06
Iter: 72 loss: 7.48110551e-06
Iter: 73 loss: 7.24529446e-06
Iter: 74 loss: 7.2437233e-06
Iter: 75 loss: 7.10180302e-06
Iter: 76 loss: 7.09840151e-06
Iter: 77 loss: 7.00476539e-06
Iter: 78 loss: 6.79590357e-06
Iter: 79 loss: 9.74228533e-06
Iter: 80 loss: 6.78486322e-06
Iter: 81 loss: 6.63466972e-06
Iter: 82 loss: 6.67309178e-06
Iter: 83 loss: 6.52578183e-06
Iter: 84 loss: 6.2341237e-06
Iter: 85 loss: 7.75157696e-06
Iter: 86 loss: 6.18801278e-06
Iter: 87 loss: 6.08268829e-06
Iter: 88 loss: 6.48711284e-06
Iter: 89 loss: 6.05793048e-06
Iter: 90 loss: 5.94414087e-06
Iter: 91 loss: 5.86535407e-06
Iter: 92 loss: 5.82432222e-06
Iter: 93 loss: 5.68196629e-06
Iter: 94 loss: 7.4188647e-06
Iter: 95 loss: 5.68012501e-06
Iter: 96 loss: 5.59723776e-06
Iter: 97 loss: 5.50257118e-06
Iter: 98 loss: 5.49050674e-06
Iter: 99 loss: 5.35974777e-06
Iter: 100 loss: 7.19475247e-06
Iter: 101 loss: 5.35925801e-06
Iter: 102 loss: 5.2702685e-06
Iter: 103 loss: 5.09868551e-06
Iter: 104 loss: 8.67257859e-06
Iter: 105 loss: 5.0977751e-06
Iter: 106 loss: 4.94701726e-06
Iter: 107 loss: 6.56559951e-06
Iter: 108 loss: 4.94354481e-06
Iter: 109 loss: 4.82451833e-06
Iter: 110 loss: 5.07835557e-06
Iter: 111 loss: 4.7776075e-06
Iter: 112 loss: 4.65725589e-06
Iter: 113 loss: 4.71081648e-06
Iter: 114 loss: 4.575395e-06
Iter: 115 loss: 4.54420751e-06
Iter: 116 loss: 4.50925791e-06
Iter: 117 loss: 4.44411035e-06
Iter: 118 loss: 4.65618723e-06
Iter: 119 loss: 4.42584405e-06
Iter: 120 loss: 4.37342305e-06
Iter: 121 loss: 4.26141196e-06
Iter: 122 loss: 6.02600221e-06
Iter: 123 loss: 4.25734197e-06
Iter: 124 loss: 4.21899813e-06
Iter: 125 loss: 4.21313689e-06
Iter: 126 loss: 4.16667217e-06
Iter: 127 loss: 4.08577034e-06
Iter: 128 loss: 4.08580672e-06
Iter: 129 loss: 4.03089052e-06
Iter: 130 loss: 4.03081413e-06
Iter: 131 loss: 3.99433702e-06
Iter: 132 loss: 3.97575877e-06
Iter: 133 loss: 3.9587394e-06
Iter: 134 loss: 3.8810872e-06
Iter: 135 loss: 3.9920892e-06
Iter: 136 loss: 3.84346322e-06
Iter: 137 loss: 3.7944194e-06
Iter: 138 loss: 3.91959429e-06
Iter: 139 loss: 3.77740707e-06
Iter: 140 loss: 3.70728344e-06
Iter: 141 loss: 3.8895123e-06
Iter: 142 loss: 3.68314386e-06
Iter: 143 loss: 3.63346066e-06
Iter: 144 loss: 3.71934e-06
Iter: 145 loss: 3.61134812e-06
Iter: 146 loss: 3.55186307e-06
Iter: 147 loss: 3.74900651e-06
Iter: 148 loss: 3.53564724e-06
Iter: 149 loss: 3.49247944e-06
Iter: 150 loss: 3.4666773e-06
Iter: 151 loss: 3.44893442e-06
Iter: 152 loss: 3.44667251e-06
Iter: 153 loss: 3.41131636e-06
Iter: 154 loss: 3.39875669e-06
Iter: 155 loss: 3.36137123e-06
Iter: 156 loss: 3.48694084e-06
Iter: 157 loss: 3.34403057e-06
Iter: 158 loss: 3.30468288e-06
Iter: 159 loss: 3.6396234e-06
Iter: 160 loss: 3.30255398e-06
Iter: 161 loss: 3.25859219e-06
Iter: 162 loss: 3.3764361e-06
Iter: 163 loss: 3.24405e-06
Iter: 164 loss: 3.20270419e-06
Iter: 165 loss: 3.30554576e-06
Iter: 166 loss: 3.18818e-06
Iter: 167 loss: 3.15716488e-06
Iter: 168 loss: 3.10649921e-06
Iter: 169 loss: 3.10620862e-06
Iter: 170 loss: 3.05562344e-06
Iter: 171 loss: 3.05453841e-06
Iter: 172 loss: 3.02602552e-06
Iter: 173 loss: 3.00135207e-06
Iter: 174 loss: 2.99373323e-06
Iter: 175 loss: 2.95759583e-06
Iter: 176 loss: 3.31542742e-06
Iter: 177 loss: 2.95624204e-06
Iter: 178 loss: 2.93095127e-06
Iter: 179 loss: 2.95045e-06
Iter: 180 loss: 2.91552487e-06
Iter: 181 loss: 2.87726698e-06
Iter: 182 loss: 2.97643874e-06
Iter: 183 loss: 2.86418617e-06
Iter: 184 loss: 2.83959184e-06
Iter: 185 loss: 2.83334225e-06
Iter: 186 loss: 2.81779558e-06
Iter: 187 loss: 2.78945618e-06
Iter: 188 loss: 2.78691346e-06
Iter: 189 loss: 2.77163463e-06
Iter: 190 loss: 2.74959939e-06
Iter: 191 loss: 2.74915624e-06
Iter: 192 loss: 2.7263352e-06
Iter: 193 loss: 2.70851547e-06
Iter: 194 loss: 2.70152213e-06
Iter: 195 loss: 2.66747975e-06
Iter: 196 loss: 2.82570841e-06
Iter: 197 loss: 2.66103552e-06
Iter: 198 loss: 2.64134223e-06
Iter: 199 loss: 2.64005052e-06
Iter: 200 loss: 2.62944445e-06
Iter: 201 loss: 2.62420235e-06
Iter: 202 loss: 2.61927653e-06
Iter: 203 loss: 2.59903391e-06
Iter: 204 loss: 2.56038288e-06
Iter: 205 loss: 3.37883807e-06
Iter: 206 loss: 2.56021099e-06
Iter: 207 loss: 2.53055441e-06
Iter: 208 loss: 2.84041471e-06
Iter: 209 loss: 2.52968266e-06
Iter: 210 loss: 2.50237395e-06
Iter: 211 loss: 2.63165612e-06
Iter: 212 loss: 2.49745858e-06
Iter: 213 loss: 2.48137121e-06
Iter: 214 loss: 2.60885281e-06
Iter: 215 loss: 2.48025071e-06
Iter: 216 loss: 2.46461059e-06
Iter: 217 loss: 2.448852e-06
Iter: 218 loss: 2.44585954e-06
Iter: 219 loss: 2.42358283e-06
Iter: 220 loss: 2.47843582e-06
Iter: 221 loss: 2.415773e-06
Iter: 222 loss: 2.39972769e-06
Iter: 223 loss: 2.39938072e-06
Iter: 224 loss: 2.38369034e-06
Iter: 225 loss: 2.44961961e-06
Iter: 226 loss: 2.38033681e-06
Iter: 227 loss: 2.37078439e-06
Iter: 228 loss: 2.35277298e-06
Iter: 229 loss: 2.75166258e-06
Iter: 230 loss: 2.35257312e-06
Iter: 231 loss: 2.33244441e-06
Iter: 232 loss: 2.45935439e-06
Iter: 233 loss: 2.33024593e-06
Iter: 234 loss: 2.3158168e-06
Iter: 235 loss: 2.30188425e-06
Iter: 236 loss: 2.29879788e-06
Iter: 237 loss: 2.28496629e-06
Iter: 238 loss: 2.2846009e-06
Iter: 239 loss: 2.26910811e-06
Iter: 240 loss: 2.26405223e-06
Iter: 241 loss: 2.25535268e-06
Iter: 242 loss: 2.23513644e-06
Iter: 243 loss: 2.33107471e-06
Iter: 244 loss: 2.23155644e-06
Iter: 245 loss: 2.22118683e-06
Iter: 246 loss: 2.19733965e-06
Iter: 247 loss: 2.50862195e-06
Iter: 248 loss: 2.1957203e-06
Iter: 249 loss: 2.18752075e-06
Iter: 250 loss: 2.18263654e-06
Iter: 251 loss: 2.17214e-06
Iter: 252 loss: 2.16209355e-06
Iter: 253 loss: 2.15975911e-06
Iter: 254 loss: 2.14628471e-06
Iter: 255 loss: 2.3309758e-06
Iter: 256 loss: 2.14622287e-06
Iter: 257 loss: 2.13295789e-06
Iter: 258 loss: 2.12595683e-06
Iter: 259 loss: 2.12000305e-06
Iter: 260 loss: 2.11597217e-06
Iter: 261 loss: 2.10939015e-06
Iter: 262 loss: 2.10520466e-06
Iter: 263 loss: 2.09648465e-06
Iter: 264 loss: 2.26366046e-06
Iter: 265 loss: 2.09651e-06
Iter: 266 loss: 2.08525807e-06
Iter: 267 loss: 2.06586856e-06
Iter: 268 loss: 2.06596314e-06
Iter: 269 loss: 2.05746778e-06
Iter: 270 loss: 2.05584979e-06
Iter: 271 loss: 2.04548269e-06
Iter: 272 loss: 2.02884098e-06
Iter: 273 loss: 2.02864203e-06
Iter: 274 loss: 2.01367061e-06
Iter: 275 loss: 2.01362127e-06
Iter: 276 loss: 2.00321097e-06
Iter: 277 loss: 2.04174489e-06
Iter: 278 loss: 2.00069599e-06
Iter: 279 loss: 1.98891621e-06
Iter: 280 loss: 1.98268685e-06
Iter: 281 loss: 1.97736244e-06
Iter: 282 loss: 1.96471819e-06
Iter: 283 loss: 1.97747431e-06
Iter: 284 loss: 1.95771554e-06
Iter: 285 loss: 1.94480663e-06
Iter: 286 loss: 2.03836748e-06
Iter: 287 loss: 1.94391214e-06
Iter: 288 loss: 1.93289247e-06
Iter: 289 loss: 1.94264453e-06
Iter: 290 loss: 1.92660627e-06
Iter: 291 loss: 1.91550453e-06
Iter: 292 loss: 1.936334e-06
Iter: 293 loss: 1.9106983e-06
Iter: 294 loss: 1.89687262e-06
Iter: 295 loss: 2.02654383e-06
Iter: 296 loss: 1.89635318e-06
Iter: 297 loss: 1.88951969e-06
Iter: 298 loss: 1.89684022e-06
Iter: 299 loss: 1.88571676e-06
Iter: 300 loss: 1.87623732e-06
Iter: 301 loss: 1.88616866e-06
Iter: 302 loss: 1.87093724e-06
Iter: 303 loss: 1.87468731e-06
Iter: 304 loss: 1.86724026e-06
Iter: 305 loss: 1.86454929e-06
Iter: 306 loss: 1.85616182e-06
Iter: 307 loss: 1.86838884e-06
Iter: 308 loss: 1.84983242e-06
Iter: 309 loss: 1.83972065e-06
Iter: 310 loss: 1.97752365e-06
Iter: 311 loss: 1.83964642e-06
Iter: 312 loss: 1.83297539e-06
Iter: 313 loss: 1.84614873e-06
Iter: 314 loss: 1.83018574e-06
Iter: 315 loss: 1.81999508e-06
Iter: 316 loss: 1.82845929e-06
Iter: 317 loss: 1.81394091e-06
Iter: 318 loss: 1.80560392e-06
Iter: 319 loss: 1.88314436e-06
Iter: 320 loss: 1.80518487e-06
Iter: 321 loss: 1.79949961e-06
Iter: 322 loss: 1.79508925e-06
Iter: 323 loss: 1.79346125e-06
Iter: 324 loss: 1.78510322e-06
Iter: 325 loss: 1.78738742e-06
Iter: 326 loss: 1.77905486e-06
Iter: 327 loss: 1.76853928e-06
Iter: 328 loss: 1.87034152e-06
Iter: 329 loss: 1.76821709e-06
Iter: 330 loss: 1.76029994e-06
Iter: 331 loss: 1.74862623e-06
Iter: 332 loss: 1.7482721e-06
Iter: 333 loss: 1.73789829e-06
Iter: 334 loss: 1.90427113e-06
Iter: 335 loss: 1.7378336e-06
Iter: 336 loss: 1.72847524e-06
Iter: 337 loss: 1.74053775e-06
Iter: 338 loss: 1.72375223e-06
Iter: 339 loss: 1.72702084e-06
Iter: 340 loss: 1.72035459e-06
Iter: 341 loss: 1.71751697e-06
Iter: 342 loss: 1.71050203e-06
Iter: 343 loss: 1.77641391e-06
Iter: 344 loss: 1.70966428e-06
Iter: 345 loss: 1.69968871e-06
Iter: 346 loss: 1.7265055e-06
Iter: 347 loss: 1.69628777e-06
Iter: 348 loss: 1.6912104e-06
Iter: 349 loss: 1.68213114e-06
Iter: 350 loss: 1.90446599e-06
Iter: 351 loss: 1.6821084e-06
Iter: 352 loss: 1.67884389e-06
Iter: 353 loss: 1.67624091e-06
Iter: 354 loss: 1.67200574e-06
Iter: 355 loss: 1.67658732e-06
Iter: 356 loss: 1.66974655e-06
Iter: 357 loss: 1.66450536e-06
Iter: 358 loss: 1.66162499e-06
Iter: 359 loss: 1.65931601e-06
Iter: 360 loss: 1.65132883e-06
Iter: 361 loss: 1.65531947e-06
Iter: 362 loss: 1.6459104e-06
Iter: 363 loss: 1.63767368e-06
Iter: 364 loss: 1.7103041e-06
Iter: 365 loss: 1.63713594e-06
Iter: 366 loss: 1.62925744e-06
Iter: 367 loss: 1.62946537e-06
Iter: 368 loss: 1.62300307e-06
Iter: 369 loss: 1.61960497e-06
Iter: 370 loss: 1.61883781e-06
Iter: 371 loss: 1.61536821e-06
Iter: 372 loss: 1.60795889e-06
Iter: 373 loss: 1.72648879e-06
Iter: 374 loss: 1.60760374e-06
Iter: 375 loss: 1.61204071e-06
Iter: 376 loss: 1.6050401e-06
Iter: 377 loss: 1.60255615e-06
Iter: 378 loss: 1.59912724e-06
Iter: 379 loss: 1.59902743e-06
Iter: 380 loss: 1.59459182e-06
Iter: 381 loss: 1.58799503e-06
Iter: 382 loss: 1.58778346e-06
Iter: 383 loss: 1.58132775e-06
Iter: 384 loss: 1.601142e-06
Iter: 385 loss: 1.57931458e-06
Iter: 386 loss: 1.57229351e-06
Iter: 387 loss: 1.64223684e-06
Iter: 388 loss: 1.57198406e-06
Iter: 389 loss: 1.56858766e-06
Iter: 390 loss: 1.57287855e-06
Iter: 391 loss: 1.56679539e-06
Iter: 392 loss: 1.56210399e-06
Iter: 393 loss: 1.56927581e-06
Iter: 394 loss: 1.55997304e-06
Iter: 395 loss: 1.55461e-06
Iter: 396 loss: 1.60081709e-06
Iter: 397 loss: 1.55433372e-06
Iter: 398 loss: 1.55076043e-06
Iter: 399 loss: 1.54270447e-06
Iter: 400 loss: 1.65101096e-06
Iter: 401 loss: 1.54217355e-06
Iter: 402 loss: 1.53390101e-06
Iter: 403 loss: 1.57425302e-06
Iter: 404 loss: 1.53251165e-06
Iter: 405 loss: 1.52617406e-06
Iter: 406 loss: 1.5795963e-06
Iter: 407 loss: 1.52574307e-06
Iter: 408 loss: 1.52081441e-06
Iter: 409 loss: 1.52885048e-06
Iter: 410 loss: 1.51849395e-06
Iter: 411 loss: 1.51571794e-06
Iter: 412 loss: 1.5149576e-06
Iter: 413 loss: 1.51271172e-06
Iter: 414 loss: 1.51369022e-06
Iter: 415 loss: 1.51115e-06
Iter: 416 loss: 1.50896494e-06
Iter: 417 loss: 1.50287349e-06
Iter: 418 loss: 1.53757946e-06
Iter: 419 loss: 1.50113806e-06
Iter: 420 loss: 1.49421226e-06
Iter: 421 loss: 1.60109653e-06
Iter: 422 loss: 1.4942982e-06
Iter: 423 loss: 1.49067478e-06
Iter: 424 loss: 1.48582012e-06
Iter: 425 loss: 1.48553659e-06
Iter: 426 loss: 1.48114964e-06
Iter: 427 loss: 1.48077243e-06
Iter: 428 loss: 1.47818832e-06
Iter: 429 loss: 1.47728724e-06
Iter: 430 loss: 1.47576554e-06
Iter: 431 loss: 1.47116202e-06
Iter: 432 loss: 1.50026472e-06
Iter: 433 loss: 1.4706194e-06
Iter: 434 loss: 1.46748471e-06
Iter: 435 loss: 1.47027731e-06
Iter: 436 loss: 1.4655526e-06
Iter: 437 loss: 1.46109869e-06
Iter: 438 loss: 1.45729678e-06
Iter: 439 loss: 1.4561133e-06
Iter: 440 loss: 1.44873138e-06
Iter: 441 loss: 1.45995182e-06
Iter: 442 loss: 1.4452371e-06
Iter: 443 loss: 1.43997147e-06
Iter: 444 loss: 1.44000114e-06
Iter: 445 loss: 1.436199e-06
Iter: 446 loss: 1.43898274e-06
Iter: 447 loss: 1.43384671e-06
Iter: 448 loss: 1.43576472e-06
Iter: 449 loss: 1.4324047e-06
Iter: 450 loss: 1.43113471e-06
Iter: 451 loss: 1.42693091e-06
Iter: 452 loss: 1.43046213e-06
Iter: 453 loss: 1.42333431e-06
Iter: 454 loss: 1.41916712e-06
Iter: 455 loss: 1.4190764e-06
Iter: 456 loss: 1.41621865e-06
Iter: 457 loss: 1.41486089e-06
Iter: 458 loss: 1.41348085e-06
Iter: 459 loss: 1.40855263e-06
Iter: 460 loss: 1.42661565e-06
Iter: 461 loss: 1.4072898e-06
Iter: 462 loss: 1.40376585e-06
Iter: 463 loss: 1.4217544e-06
Iter: 464 loss: 1.40316638e-06
Iter: 465 loss: 1.39980864e-06
Iter: 466 loss: 1.42258614e-06
Iter: 467 loss: 1.39966664e-06
Iter: 468 loss: 1.39708561e-06
Iter: 469 loss: 1.39480505e-06
Iter: 470 loss: 1.3941609e-06
Iter: 471 loss: 1.38834901e-06
Iter: 472 loss: 1.40139139e-06
Iter: 473 loss: 1.38620021e-06
Iter: 474 loss: 1.38305109e-06
Iter: 475 loss: 1.38126711e-06
Iter: 476 loss: 1.37985103e-06
Iter: 477 loss: 1.37479879e-06
Iter: 478 loss: 1.40635098e-06
Iter: 479 loss: 1.3742374e-06
Iter: 480 loss: 1.37053053e-06
Iter: 481 loss: 1.36939309e-06
Iter: 482 loss: 1.36717199e-06
Iter: 483 loss: 1.36368385e-06
Iter: 484 loss: 1.36372751e-06
Iter: 485 loss: 1.36077188e-06
Iter: 486 loss: 1.39611643e-06
Iter: 487 loss: 1.36072845e-06
Iter: 488 loss: 1.35857272e-06
Iter: 489 loss: 1.35319328e-06
Iter: 490 loss: 1.40260443e-06
Iter: 491 loss: 1.35244773e-06
Iter: 492 loss: 1.34747324e-06
Iter: 493 loss: 1.35698576e-06
Iter: 494 loss: 1.34535128e-06
Iter: 495 loss: 1.34167499e-06
Iter: 496 loss: 1.3764394e-06
Iter: 497 loss: 1.34148763e-06
Iter: 498 loss: 1.33796777e-06
Iter: 499 loss: 1.33867252e-06
Iter: 500 loss: 1.33517142e-06
Iter: 501 loss: 1.33192157e-06
Iter: 502 loss: 1.33197045e-06
Iter: 503 loss: 1.32941898e-06
Iter: 504 loss: 1.33003925e-06
Iter: 505 loss: 1.32765672e-06
Iter: 506 loss: 1.32364278e-06
Iter: 507 loss: 1.33943729e-06
Iter: 508 loss: 1.3228231e-06
Iter: 509 loss: 1.32048137e-06
Iter: 510 loss: 1.31552338e-06
Iter: 511 loss: 1.38775863e-06
Iter: 512 loss: 1.31525201e-06
Iter: 513 loss: 1.31168213e-06
Iter: 514 loss: 1.31148681e-06
Iter: 515 loss: 1.30910712e-06
Iter: 516 loss: 1.31167599e-06
Iter: 517 loss: 1.30765056e-06
Iter: 518 loss: 1.30418368e-06
Iter: 519 loss: 1.31477407e-06
Iter: 520 loss: 1.3031613e-06
Iter: 521 loss: 1.29964701e-06
Iter: 522 loss: 1.30350372e-06
Iter: 523 loss: 1.29778482e-06
Iter: 524 loss: 1.29697082e-06
Iter: 525 loss: 1.29561954e-06
Iter: 526 loss: 1.29484636e-06
Iter: 527 loss: 1.29237901e-06
Iter: 528 loss: 1.29615182e-06
Iter: 529 loss: 1.29067632e-06
Iter: 530 loss: 1.28545491e-06
Iter: 531 loss: 1.29696787e-06
Iter: 532 loss: 1.2835585e-06
Iter: 533 loss: 1.2802841e-06
Iter: 534 loss: 1.28760826e-06
Iter: 535 loss: 1.27893645e-06
Iter: 536 loss: 1.27541603e-06
Iter: 537 loss: 1.3015499e-06
Iter: 538 loss: 1.27520264e-06
Iter: 539 loss: 1.2722121e-06
Iter: 540 loss: 1.28809177e-06
Iter: 541 loss: 1.27172223e-06
Iter: 542 loss: 1.2686487e-06
Iter: 543 loss: 1.27132682e-06
Iter: 544 loss: 1.26667271e-06
Iter: 545 loss: 1.26443501e-06
Iter: 546 loss: 1.27279509e-06
Iter: 547 loss: 1.26375926e-06
Iter: 548 loss: 1.26103453e-06
Iter: 549 loss: 1.26314467e-06
Iter: 550 loss: 1.25933309e-06
Iter: 551 loss: 1.25675467e-06
Iter: 552 loss: 1.25962742e-06
Iter: 553 loss: 1.25529573e-06
Iter: 554 loss: 1.25239012e-06
Iter: 555 loss: 1.28743773e-06
Iter: 556 loss: 1.25232714e-06
Iter: 557 loss: 1.25067868e-06
Iter: 558 loss: 1.26050895e-06
Iter: 559 loss: 1.25043903e-06
Iter: 560 loss: 1.24835242e-06
Iter: 561 loss: 1.25123711e-06
Iter: 562 loss: 1.24733594e-06
Iter: 563 loss: 1.24524558e-06
Iter: 564 loss: 1.24373025e-06
Iter: 565 loss: 1.24314238e-06
Iter: 566 loss: 1.24063604e-06
Iter: 567 loss: 1.23688426e-06
Iter: 568 loss: 1.23679115e-06
Iter: 569 loss: 1.23276254e-06
Iter: 570 loss: 1.23284531e-06
Iter: 571 loss: 1.2306316e-06
Iter: 572 loss: 1.22849428e-06
Iter: 573 loss: 1.22800827e-06
Iter: 574 loss: 1.22543872e-06
Iter: 575 loss: 1.22521237e-06
Iter: 576 loss: 1.22356914e-06
Iter: 577 loss: 1.2204473e-06
Iter: 578 loss: 1.29576802e-06
Iter: 579 loss: 1.22043195e-06
Iter: 580 loss: 1.21673281e-06
Iter: 581 loss: 1.2392353e-06
Iter: 582 loss: 1.21632468e-06
Iter: 583 loss: 1.21401058e-06
Iter: 584 loss: 1.22086067e-06
Iter: 585 loss: 1.21325661e-06
Iter: 586 loss: 1.21045059e-06
Iter: 587 loss: 1.2144078e-06
Iter: 588 loss: 1.20909749e-06
Iter: 589 loss: 1.20726554e-06
Iter: 590 loss: 1.22470112e-06
Iter: 591 loss: 1.207178e-06
Iter: 592 loss: 1.20541256e-06
Iter: 593 loss: 1.21762992e-06
Iter: 594 loss: 1.20529091e-06
Iter: 595 loss: 1.20415689e-06
Iter: 596 loss: 1.20356822e-06
Iter: 597 loss: 1.20305788e-06
Iter: 598 loss: 1.20113123e-06
Iter: 599 loss: 1.19892377e-06
Iter: 600 loss: 1.19873425e-06
Iter: 601 loss: 1.19586559e-06
Iter: 602 loss: 1.21626067e-06
Iter: 603 loss: 1.19563151e-06
Iter: 604 loss: 1.19337312e-06
Iter: 605 loss: 1.18977891e-06
Iter: 606 loss: 1.18977596e-06
Iter: 607 loss: 1.1872894e-06
Iter: 608 loss: 1.18722392e-06
Iter: 609 loss: 1.18517028e-06
Iter: 610 loss: 1.19306969e-06
Iter: 611 loss: 1.184692e-06
Iter: 612 loss: 1.18261278e-06
Iter: 613 loss: 1.19123956e-06
Iter: 614 loss: 1.18214371e-06
Iter: 615 loss: 1.18074979e-06
Iter: 616 loss: 1.1774946e-06
Iter: 617 loss: 1.22089727e-06
Iter: 618 loss: 1.17722254e-06
Iter: 619 loss: 1.17377022e-06
Iter: 620 loss: 1.22243307e-06
Iter: 621 loss: 1.17380887e-06
Iter: 622 loss: 1.17176319e-06
Iter: 623 loss: 1.1699268e-06
Iter: 624 loss: 1.1694292e-06
Iter: 625 loss: 1.16683714e-06
Iter: 626 loss: 1.1866548e-06
Iter: 627 loss: 1.16663523e-06
Iter: 628 loss: 1.1650543e-06
Iter: 629 loss: 1.16494357e-06
Iter: 630 loss: 1.16393994e-06
Iter: 631 loss: 1.16213573e-06
Iter: 632 loss: 1.20276889e-06
Iter: 633 loss: 1.16210435e-06
Iter: 634 loss: 1.16027036e-06
Iter: 635 loss: 1.16071953e-06
Iter: 636 loss: 1.15892135e-06
Iter: 637 loss: 1.15652165e-06
Iter: 638 loss: 1.17708282e-06
Iter: 639 loss: 1.15638682e-06
Iter: 640 loss: 1.15456089e-06
Iter: 641 loss: 1.15187049e-06
Iter: 642 loss: 1.1518639e-06
Iter: 643 loss: 1.14938825e-06
Iter: 644 loss: 1.17966522e-06
Iter: 645 loss: 1.14938075e-06
Iter: 646 loss: 1.14705654e-06
Iter: 647 loss: 1.14528757e-06
Iter: 648 loss: 1.14453519e-06
Iter: 649 loss: 1.14361342e-06
Iter: 650 loss: 1.14269847e-06
Iter: 651 loss: 1.14163254e-06
Iter: 652 loss: 1.13980786e-06
Iter: 653 loss: 1.13983594e-06
Iter: 654 loss: 1.13705823e-06
Iter: 655 loss: 1.14813406e-06
Iter: 656 loss: 1.13640544e-06
Iter: 657 loss: 1.13491751e-06
Iter: 658 loss: 1.13656597e-06
Iter: 659 loss: 1.13408976e-06
Iter: 660 loss: 1.13210808e-06
Iter: 661 loss: 1.14123441e-06
Iter: 662 loss: 1.13183273e-06
Iter: 663 loss: 1.13082911e-06
Iter: 664 loss: 1.13060241e-06
Iter: 665 loss: 1.13012e-06
Iter: 666 loss: 1.12866837e-06
Iter: 667 loss: 1.13056785e-06
Iter: 668 loss: 1.12744578e-06
Iter: 669 loss: 1.12499208e-06
Iter: 670 loss: 1.14816316e-06
Iter: 671 loss: 1.12491659e-06
Iter: 672 loss: 1.12306429e-06
Iter: 673 loss: 1.12596535e-06
Iter: 674 loss: 1.12223893e-06
Iter: 675 loss: 1.11977465e-06
Iter: 676 loss: 1.1261784e-06
Iter: 677 loss: 1.11898453e-06
Iter: 678 loss: 1.11710267e-06
Iter: 679 loss: 1.11602321e-06
Iter: 680 loss: 1.11526481e-06
Iter: 681 loss: 1.11294582e-06
Iter: 682 loss: 1.13311364e-06
Iter: 683 loss: 1.11282907e-06
Iter: 684 loss: 1.11108625e-06
Iter: 685 loss: 1.12943633e-06
Iter: 686 loss: 1.11100928e-06
Iter: 687 loss: 1.10979113e-06
Iter: 688 loss: 1.11241297e-06
Iter: 689 loss: 1.10930341e-06
Iter: 690 loss: 1.10808094e-06
Iter: 691 loss: 1.10637302e-06
Iter: 692 loss: 1.10622682e-06
Iter: 693 loss: 1.10375026e-06
Iter: 694 loss: 1.12273119e-06
Iter: 695 loss: 1.10347355e-06
Iter: 696 loss: 1.10249448e-06
Iter: 697 loss: 1.1064601e-06
Iter: 698 loss: 1.10236647e-06
Iter: 699 loss: 1.10088126e-06
Iter: 700 loss: 1.10483461e-06
Iter: 701 loss: 1.10040196e-06
Iter: 702 loss: 1.09956102e-06
Iter: 703 loss: 1.09855705e-06
Iter: 704 loss: 1.09840494e-06
Iter: 705 loss: 1.09686221e-06
Iter: 706 loss: 1.09477378e-06
Iter: 707 loss: 1.094656e-06
Iter: 708 loss: 1.09280847e-06
Iter: 709 loss: 1.09277107e-06
Iter: 710 loss: 1.09077564e-06
Iter: 711 loss: 1.09044026e-06
Iter: 712 loss: 1.08906465e-06
Iter: 713 loss: 1.08678387e-06
Iter: 714 loss: 1.11150007e-06
Iter: 715 loss: 1.08671713e-06
Iter: 716 loss: 1.08565814e-06
Iter: 717 loss: 1.08380766e-06
Iter: 718 loss: 1.08381187e-06
Iter: 719 loss: 1.08240738e-06
Iter: 720 loss: 1.08234985e-06
Iter: 721 loss: 1.08094093e-06
Iter: 722 loss: 1.08021709e-06
Iter: 723 loss: 1.07955577e-06
Iter: 724 loss: 1.07745018e-06
Iter: 725 loss: 1.08000279e-06
Iter: 726 loss: 1.07642973e-06
Iter: 727 loss: 1.07493656e-06
Iter: 728 loss: 1.07932533e-06
Iter: 729 loss: 1.07448022e-06
Iter: 730 loss: 1.07298308e-06
Iter: 731 loss: 1.08636232e-06
Iter: 732 loss: 1.07295023e-06
Iter: 733 loss: 1.07172491e-06
Iter: 734 loss: 1.07914957e-06
Iter: 735 loss: 1.07155165e-06
Iter: 736 loss: 1.07097583e-06
Iter: 737 loss: 1.06940274e-06
Iter: 738 loss: 1.08434028e-06
Iter: 739 loss: 1.0691532e-06
Iter: 740 loss: 1.06724679e-06
Iter: 741 loss: 1.07186213e-06
Iter: 742 loss: 1.06670564e-06
Iter: 743 loss: 1.06427092e-06
Iter: 744 loss: 1.06766697e-06
Iter: 745 loss: 1.06310665e-06
Iter: 746 loss: 1.06178823e-06
Iter: 747 loss: 1.06173e-06
Iter: 748 loss: 1.06054097e-06
Iter: 749 loss: 1.06118227e-06
Iter: 750 loss: 1.05973368e-06
Iter: 751 loss: 1.05846425e-06
Iter: 752 loss: 1.06221614e-06
Iter: 753 loss: 1.05809704e-06
Iter: 754 loss: 1.05655704e-06
Iter: 755 loss: 1.05808749e-06
Iter: 756 loss: 1.05564015e-06
Iter: 757 loss: 1.05393201e-06
Iter: 758 loss: 1.07739334e-06
Iter: 759 loss: 1.05392485e-06
Iter: 760 loss: 1.05302411e-06
Iter: 761 loss: 1.05094853e-06
Iter: 762 loss: 1.07488825e-06
Iter: 763 loss: 1.05079357e-06
Iter: 764 loss: 1.04837898e-06
Iter: 765 loss: 1.07347944e-06
Iter: 766 loss: 1.04830349e-06
Iter: 767 loss: 1.04759488e-06
Iter: 768 loss: 1.04760795e-06
Iter: 769 loss: 1.04687081e-06
Iter: 770 loss: 1.04529977e-06
Iter: 771 loss: 1.06561743e-06
Iter: 772 loss: 1.04517073e-06
Iter: 773 loss: 1.04380763e-06
Iter: 774 loss: 1.04777962e-06
Iter: 775 loss: 1.04333299e-06
Iter: 776 loss: 1.04182936e-06
Iter: 777 loss: 1.04227979e-06
Iter: 778 loss: 1.04073342e-06
Iter: 779 loss: 1.03901482e-06
Iter: 780 loss: 1.05347112e-06
Iter: 781 loss: 1.03895695e-06
Iter: 782 loss: 1.03745674e-06
Iter: 783 loss: 1.03739512e-06
Iter: 784 loss: 1.03626166e-06
Iter: 785 loss: 1.03454863e-06
Iter: 786 loss: 1.03463924e-06
Iter: 787 loss: 1.0332642e-06
Iter: 788 loss: 1.03211505e-06
Iter: 789 loss: 1.03201432e-06
Iter: 790 loss: 1.03097477e-06
Iter: 791 loss: 1.03082687e-06
Iter: 792 loss: 1.03013213e-06
Iter: 793 loss: 1.02811828e-06
Iter: 794 loss: 1.0324809e-06
Iter: 795 loss: 1.02737204e-06
Iter: 796 loss: 1.02612319e-06
Iter: 797 loss: 1.0418612e-06
Iter: 798 loss: 1.02615058e-06
Iter: 799 loss: 1.02515537e-06
Iter: 800 loss: 1.02340368e-06
Iter: 801 loss: 1.02344666e-06
Iter: 802 loss: 1.0229495e-06
Iter: 803 loss: 1.02221225e-06
Iter: 804 loss: 1.02167928e-06
Iter: 805 loss: 1.02033505e-06
Iter: 806 loss: 1.03648358e-06
Iter: 807 loss: 1.02022159e-06
Iter: 808 loss: 1.01854766e-06
Iter: 809 loss: 1.01802618e-06
Iter: 810 loss: 1.01710543e-06
Iter: 811 loss: 1.01538512e-06
Iter: 812 loss: 1.02806621e-06
Iter: 813 loss: 1.01522232e-06
Iter: 814 loss: 1.01368335e-06
Iter: 815 loss: 1.0212583e-06
Iter: 816 loss: 1.01342368e-06
Iter: 817 loss: 1.01225692e-06
Iter: 818 loss: 1.01081287e-06
Iter: 819 loss: 1.01065802e-06
Iter: 820 loss: 1.00868306e-06
Iter: 821 loss: 1.02454851e-06
Iter: 822 loss: 1.00860075e-06
Iter: 823 loss: 1.00691227e-06
Iter: 824 loss: 1.00868931e-06
Iter: 825 loss: 1.00597947e-06
Iter: 826 loss: 1.00511625e-06
Iter: 827 loss: 1.00496754e-06
Iter: 828 loss: 1.00422153e-06
Iter: 829 loss: 1.00306193e-06
Iter: 830 loss: 1.0030933e-06
Iter: 831 loss: 1.00144507e-06
Iter: 832 loss: 1.01389537e-06
Iter: 833 loss: 1.00131956e-06
Iter: 834 loss: 1.00060288e-06
Iter: 835 loss: 1.00598868e-06
Iter: 836 loss: 1.00052034e-06
Iter: 837 loss: 9.99585609e-07
Iter: 838 loss: 9.98496262e-07
Iter: 839 loss: 9.98400765e-07
Iter: 840 loss: 9.96851e-07
Iter: 841 loss: 9.97218e-07
Iter: 842 loss: 9.95749588e-07
Iter: 843 loss: 9.94467428e-07
Iter: 844 loss: 9.94476409e-07
Iter: 845 loss: 9.93456524e-07
Iter: 846 loss: 9.92193e-07
Iter: 847 loss: 9.92151286e-07
Iter: 848 loss: 9.91105e-07
Iter: 849 loss: 9.91256798e-07
Iter: 850 loss: 9.90393687e-07
Iter: 851 loss: 9.88843226e-07
Iter: 852 loss: 9.92731e-07
Iter: 853 loss: 9.88348347e-07
Iter: 854 loss: 9.86900659e-07
Iter: 855 loss: 9.87081421e-07
Iter: 856 loss: 9.85724455e-07
Iter: 857 loss: 9.84243343e-07
Iter: 858 loss: 1.00162356e-06
Iter: 859 loss: 9.84289727e-07
Iter: 860 loss: 9.83220161e-07
Iter: 861 loss: 9.91676416e-07
Iter: 862 loss: 9.83114774e-07
Iter: 863 loss: 9.82430379e-07
Iter: 864 loss: 9.8476221e-07
Iter: 865 loss: 9.82229722e-07
Iter: 866 loss: 9.81535777e-07
Iter: 867 loss: 9.82664801e-07
Iter: 868 loss: 9.8119267e-07
Iter: 869 loss: 9.80181426e-07
Iter: 870 loss: 9.84096459e-07
Iter: 871 loss: 9.79943934e-07
Iter: 872 loss: 9.78932917e-07
Iter: 873 loss: 9.79203264e-07
Iter: 874 loss: 9.7818679e-07
Iter: 875 loss: 9.77467607e-07
Iter: 876 loss: 9.75600415e-07
Iter: 877 loss: 9.91473485e-07
Iter: 878 loss: 9.75326202e-07
Iter: 879 loss: 9.73489705e-07
Iter: 880 loss: 9.73495389e-07
Iter: 881 loss: 9.72298e-07
Iter: 882 loss: 9.72036673e-07
Iter: 883 loss: 9.71300324e-07
Iter: 884 loss: 9.6996564e-07
Iter: 885 loss: 9.87577209e-07
Iter: 886 loss: 9.69952453e-07
Iter: 887 loss: 9.68927679e-07
Iter: 888 loss: 9.67441792e-07
Iter: 889 loss: 9.67424171e-07
Iter: 890 loss: 9.65433173e-07
Iter: 891 loss: 9.76352567e-07
Iter: 892 loss: 9.6518329e-07
Iter: 893 loss: 9.6370411e-07
Iter: 894 loss: 9.72669909e-07
Iter: 895 loss: 9.63509365e-07
Iter: 896 loss: 9.62364311e-07
Iter: 897 loss: 9.72125e-07
Iter: 898 loss: 9.62210265e-07
Iter: 899 loss: 9.61383876e-07
Iter: 900 loss: 9.63193315e-07
Iter: 901 loss: 9.61117621e-07
Iter: 902 loss: 9.60335228e-07
Iter: 903 loss: 9.66793777e-07
Iter: 904 loss: 9.60220291e-07
Iter: 905 loss: 9.59696422e-07
Iter: 906 loss: 9.60550324e-07
Iter: 907 loss: 9.59490762e-07
Iter: 908 loss: 9.58874e-07
Iter: 909 loss: 9.57413249e-07
Iter: 910 loss: 9.6839517e-07
Iter: 911 loss: 9.57085e-07
Iter: 912 loss: 9.55564929e-07
Iter: 913 loss: 9.7634e-07
Iter: 914 loss: 9.55545488e-07
Iter: 915 loss: 9.54666802e-07
Iter: 916 loss: 9.53249639e-07
Iter: 917 loss: 9.5318245e-07
Iter: 918 loss: 9.5198078e-07
Iter: 919 loss: 9.51897164e-07
Iter: 920 loss: 9.50855053e-07
Iter: 921 loss: 9.51615789e-07
Iter: 922 loss: 9.50160768e-07
Iter: 923 loss: 9.48987804e-07
Iter: 924 loss: 9.51900347e-07
Iter: 925 loss: 9.48532545e-07
Iter: 926 loss: 9.47203148e-07
Iter: 927 loss: 9.49036178e-07
Iter: 928 loss: 9.46534442e-07
Iter: 929 loss: 9.45501938e-07
Iter: 930 loss: 9.58035571e-07
Iter: 931 loss: 9.45489887e-07
Iter: 932 loss: 9.44550948e-07
Iter: 933 loss: 9.46298883e-07
Iter: 934 loss: 9.44122121e-07
Iter: 935 loss: 9.43670898e-07
Iter: 936 loss: 9.4363611e-07
Iter: 937 loss: 9.43169084e-07
Iter: 938 loss: 9.42799261e-07
Iter: 939 loss: 9.42636802e-07
Iter: 940 loss: 9.41775511e-07
Iter: 941 loss: 9.41175927e-07
Iter: 942 loss: 9.40846462e-07
Iter: 943 loss: 9.39763424e-07
Iter: 944 loss: 9.4507152e-07
Iter: 945 loss: 9.39593292e-07
Iter: 946 loss: 9.38591882e-07
Iter: 947 loss: 9.36928927e-07
Iter: 948 loss: 9.36971446e-07
Iter: 949 loss: 9.35738512e-07
Iter: 950 loss: 9.35724586e-07
Iter: 951 loss: 9.34774846e-07
Iter: 952 loss: 9.35767048e-07
Iter: 953 loss: 9.34294064e-07
Iter: 954 loss: 9.33246724e-07
Iter: 955 loss: 9.39562938e-07
Iter: 956 loss: 9.33189256e-07
Iter: 957 loss: 9.32203704e-07
Iter: 958 loss: 9.33348531e-07
Iter: 959 loss: 9.31739919e-07
Iter: 960 loss: 9.30862029e-07
Iter: 961 loss: 9.32172838e-07
Iter: 962 loss: 9.30418764e-07
Iter: 963 loss: 9.29343571e-07
Iter: 964 loss: 9.40845382e-07
Iter: 965 loss: 9.29319185e-07
Iter: 966 loss: 9.28432144e-07
Iter: 967 loss: 9.31161139e-07
Iter: 968 loss: 9.28225631e-07
Iter: 969 loss: 9.27472115e-07
Iter: 970 loss: 9.3402042e-07
Iter: 971 loss: 9.27474389e-07
Iter: 972 loss: 9.26863208e-07
Iter: 973 loss: 9.25776419e-07
Iter: 974 loss: 9.25796542e-07
Iter: 975 loss: 9.24988854e-07
Iter: 976 loss: 9.2998232e-07
Iter: 977 loss: 9.2489347e-07
Iter: 978 loss: 9.24079359e-07
Iter: 979 loss: 9.22804247e-07
Iter: 980 loss: 9.2279754e-07
Iter: 981 loss: 9.21556364e-07
Iter: 982 loss: 9.27245708e-07
Iter: 983 loss: 9.21310686e-07
Iter: 984 loss: 9.20188768e-07
Iter: 985 loss: 9.25160293e-07
Iter: 986 loss: 9.20003117e-07
Iter: 987 loss: 9.18841408e-07
Iter: 988 loss: 9.19601689e-07
Iter: 989 loss: 9.18170826e-07
Iter: 990 loss: 9.16864906e-07
Iter: 991 loss: 9.30536658e-07
Iter: 992 loss: 9.16821079e-07
Iter: 993 loss: 9.16038e-07
Iter: 994 loss: 9.15232135e-07
Iter: 995 loss: 9.15055239e-07
Iter: 996 loss: 9.13865733e-07
Iter: 997 loss: 9.29943894e-07
Iter: 998 loss: 9.13848737e-07
Iter: 999 loss: 9.1316366e-07
Iter: 1000 loss: 9.18591354e-07
Iter: 1001 loss: 9.13075041e-07
Iter: 1002 loss: 9.12603241e-07
Iter: 1003 loss: 9.15090538e-07
Iter: 1004 loss: 9.12502344e-07
Iter: 1005 loss: 9.11837446e-07
Iter: 1006 loss: 9.11825111e-07
Iter: 1007 loss: 9.11317386e-07
Iter: 1008 loss: 9.10738095e-07
Iter: 1009 loss: 9.1018e-07
Iter: 1010 loss: 9.10077233e-07
Iter: 1011 loss: 9.08969639e-07
Iter: 1012 loss: 9.15665396e-07
Iter: 1013 loss: 9.08836455e-07
Iter: 1014 loss: 9.08147172e-07
Iter: 1015 loss: 9.08062248e-07
Iter: 1016 loss: 9.07553783e-07
Iter: 1017 loss: 9.06559308e-07
Iter: 1018 loss: 9.09661424e-07
Iter: 1019 loss: 9.06331536e-07
Iter: 1020 loss: 9.05288402e-07
Iter: 1021 loss: 9.05022148e-07
Iter: 1022 loss: 9.04425633e-07
Iter: 1023 loss: 9.03644491e-07
Iter: 1024 loss: 9.03591797e-07
Iter: 1025 loss: 9.02874206e-07
Iter: 1026 loss: 9.01924409e-07
Iter: 1027 loss: 9.01900535e-07
Iter: 1028 loss: 9.00654e-07
Iter: 1029 loss: 9.09446783e-07
Iter: 1030 loss: 9.00528903e-07
Iter: 1031 loss: 8.99657039e-07
Iter: 1032 loss: 9.03128807e-07
Iter: 1033 loss: 8.99441943e-07
Iter: 1034 loss: 8.98576275e-07
Iter: 1035 loss: 9.06105242e-07
Iter: 1036 loss: 8.9854052e-07
Iter: 1037 loss: 8.97875e-07
Iter: 1038 loss: 9.02353861e-07
Iter: 1039 loss: 8.97770519e-07
Iter: 1040 loss: 8.97415475e-07
Iter: 1041 loss: 8.96386439e-07
Iter: 1042 loss: 9.06991033e-07
Iter: 1043 loss: 8.96302424e-07
Iter: 1044 loss: 8.95514631e-07
Iter: 1045 loss: 9.04792557e-07
Iter: 1046 loss: 8.95512528e-07
Iter: 1047 loss: 8.94801e-07
Iter: 1048 loss: 8.94404479e-07
Iter: 1049 loss: 8.94143454e-07
Iter: 1050 loss: 8.93064396e-07
Iter: 1051 loss: 8.97683208e-07
Iter: 1052 loss: 8.9285453e-07
Iter: 1053 loss: 8.92056789e-07
Iter: 1054 loss: 8.92348112e-07
Iter: 1055 loss: 8.91443506e-07
Iter: 1056 loss: 8.90604269e-07
Iter: 1057 loss: 8.98657561e-07
Iter: 1058 loss: 8.90595174e-07
Iter: 1059 loss: 8.89826538e-07
Iter: 1060 loss: 8.90101489e-07
Iter: 1061 loss: 8.89327623e-07
Iter: 1062 loss: 8.88426428e-07
Iter: 1063 loss: 8.94308755e-07
Iter: 1064 loss: 8.88254363e-07
Iter: 1065 loss: 8.87531087e-07
Iter: 1066 loss: 8.87053716e-07
Iter: 1067 loss: 8.86716634e-07
Iter: 1068 loss: 8.86103066e-07
Iter: 1069 loss: 8.8601962e-07
Iter: 1070 loss: 8.85457098e-07
Iter: 1071 loss: 8.87348733e-07
Iter: 1072 loss: 8.85234613e-07
Iter: 1073 loss: 8.84598194e-07
Iter: 1074 loss: 8.85076815e-07
Iter: 1075 loss: 8.84119572e-07
Iter: 1076 loss: 8.83615485e-07
Iter: 1077 loss: 8.83488951e-07
Iter: 1078 loss: 8.83254415e-07
Iter: 1079 loss: 8.82369363e-07
Iter: 1080 loss: 8.82869926e-07
Iter: 1081 loss: 8.8182793e-07
Iter: 1082 loss: 8.8097039e-07
Iter: 1083 loss: 8.89735475e-07
Iter: 1084 loss: 8.80923722e-07
Iter: 1085 loss: 8.80229322e-07
Iter: 1086 loss: 8.79508946e-07
Iter: 1087 loss: 8.79373943e-07
Iter: 1088 loss: 8.7831279e-07
Iter: 1089 loss: 8.80401899e-07
Iter: 1090 loss: 8.77884759e-07
Iter: 1091 loss: 8.76929619e-07
Iter: 1092 loss: 8.86935254e-07
Iter: 1093 loss: 8.76905517e-07
Iter: 1094 loss: 8.76243917e-07
Iter: 1095 loss: 8.77949105e-07
Iter: 1096 loss: 8.75942135e-07
Iter: 1097 loss: 8.75197031e-07
Iter: 1098 loss: 8.77450702e-07
Iter: 1099 loss: 8.74922137e-07
Iter: 1100 loss: 8.74329487e-07
Iter: 1101 loss: 8.7654405e-07
Iter: 1102 loss: 8.74116949e-07
Iter: 1103 loss: 8.73434715e-07
Iter: 1104 loss: 8.79411118e-07
Iter: 1105 loss: 8.73443867e-07
Iter: 1106 loss: 8.72976e-07
Iter: 1107 loss: 8.74761781e-07
Iter: 1108 loss: 8.72846329e-07
Iter: 1109 loss: 8.72457122e-07
Iter: 1110 loss: 8.71798704e-07
Iter: 1111 loss: 8.71811551e-07
Iter: 1112 loss: 8.70960321e-07
Iter: 1113 loss: 8.72163e-07
Iter: 1114 loss: 8.70618919e-07
Iter: 1115 loss: 8.69871e-07
Iter: 1116 loss: 8.73286808e-07
Iter: 1117 loss: 8.69740063e-07
Iter: 1118 loss: 8.68887923e-07
Iter: 1119 loss: 8.70267229e-07
Iter: 1120 loss: 8.68557265e-07
Iter: 1121 loss: 8.67816425e-07
Iter: 1122 loss: 8.6809348e-07
Iter: 1123 loss: 8.6727448e-07
Iter: 1124 loss: 8.66402843e-07
Iter: 1125 loss: 8.73020554e-07
Iter: 1126 loss: 8.66315418e-07
Iter: 1127 loss: 8.65616471e-07
Iter: 1128 loss: 8.6526785e-07
Iter: 1129 loss: 8.65006768e-07
Iter: 1130 loss: 8.64150479e-07
Iter: 1131 loss: 8.64189815e-07
Iter: 1132 loss: 8.63568118e-07
Iter: 1133 loss: 8.63199489e-07
Iter: 1134 loss: 8.62985246e-07
Iter: 1135 loss: 8.62579782e-07
Iter: 1136 loss: 8.62531749e-07
Iter: 1137 loss: 8.62050683e-07
Iter: 1138 loss: 8.62369347e-07
Iter: 1139 loss: 8.61786532e-07
Iter: 1140 loss: 8.61198032e-07
Iter: 1141 loss: 8.6142677e-07
Iter: 1142 loss: 8.60773923e-07
Iter: 1143 loss: 8.60041496e-07
Iter: 1144 loss: 8.60345949e-07
Iter: 1145 loss: 8.59600959e-07
Iter: 1146 loss: 8.58764793e-07
Iter: 1147 loss: 8.59379213e-07
Iter: 1148 loss: 8.58308e-07
Iter: 1149 loss: 8.57570967e-07
Iter: 1150 loss: 8.68210748e-07
Iter: 1151 loss: 8.57582393e-07
Iter: 1152 loss: 8.57074e-07
Iter: 1153 loss: 8.56516749e-07
Iter: 1154 loss: 8.5638419e-07
Iter: 1155 loss: 8.55487656e-07
Iter: 1156 loss: 8.60659952e-07
Iter: 1157 loss: 8.55470773e-07
Iter: 1158 loss: 8.54782854e-07
Iter: 1159 loss: 8.54480277e-07
Iter: 1160 loss: 8.54138761e-07
Iter: 1161 loss: 8.53379674e-07
Iter: 1162 loss: 8.60957414e-07
Iter: 1163 loss: 8.53383426e-07
Iter: 1164 loss: 8.52626727e-07
Iter: 1165 loss: 8.54340271e-07
Iter: 1166 loss: 8.52366668e-07
Iter: 1167 loss: 8.51723939e-07
Iter: 1168 loss: 8.56519705e-07
Iter: 1169 loss: 8.51672894e-07
Iter: 1170 loss: 8.51227298e-07
Iter: 1171 loss: 8.56287784e-07
Iter: 1172 loss: 8.51214736e-07
Iter: 1173 loss: 8.50841332e-07
Iter: 1174 loss: 8.50543756e-07
Iter: 1175 loss: 8.5039477e-07
Iter: 1176 loss: 8.49814342e-07
Iter: 1177 loss: 8.51967968e-07
Iter: 1178 loss: 8.49684966e-07
Iter: 1179 loss: 8.49162e-07
Iter: 1180 loss: 8.48078514e-07
Iter: 1181 loss: 8.66754e-07
Iter: 1182 loss: 8.48047875e-07
Iter: 1183 loss: 8.47137585e-07
Iter: 1184 loss: 8.5878429e-07
Iter: 1185 loss: 8.47131673e-07
Iter: 1186 loss: 8.46267255e-07
Iter: 1187 loss: 8.46977116e-07
Iter: 1188 loss: 8.45690238e-07
Iter: 1189 loss: 8.44873966e-07
Iter: 1190 loss: 8.50015908e-07
Iter: 1191 loss: 8.44853048e-07
Iter: 1192 loss: 8.4413557e-07
Iter: 1193 loss: 8.44118233e-07
Iter: 1194 loss: 8.43630687e-07
Iter: 1195 loss: 8.42832151e-07
Iter: 1196 loss: 8.44351916e-07
Iter: 1197 loss: 8.42430268e-07
Iter: 1198 loss: 8.41651399e-07
Iter: 1199 loss: 8.49759374e-07
Iter: 1200 loss: 8.41611381e-07
Iter: 1201 loss: 8.41066935e-07
Iter: 1202 loss: 8.42776899e-07
Iter: 1203 loss: 8.40868609e-07
Iter: 1204 loss: 8.40413691e-07
Iter: 1205 loss: 8.46784e-07
Iter: 1206 loss: 8.40392318e-07
Iter: 1207 loss: 8.3990426e-07
Iter: 1208 loss: 8.40334053e-07
Iter: 1209 loss: 8.39618622e-07
Iter: 1210 loss: 8.39168933e-07
Iter: 1211 loss: 8.39780455e-07
Iter: 1212 loss: 8.38951337e-07
Iter: 1213 loss: 8.38318215e-07
Iter: 1214 loss: 8.38255175e-07
Iter: 1215 loss: 8.37820039e-07
Iter: 1216 loss: 8.37095e-07
Iter: 1217 loss: 8.37815492e-07
Iter: 1218 loss: 8.36765366e-07
Iter: 1219 loss: 8.35779474e-07
Iter: 1220 loss: 8.38563778e-07
Iter: 1221 loss: 8.35471212e-07
Iter: 1222 loss: 8.34744583e-07
Iter: 1223 loss: 8.41876727e-07
Iter: 1224 loss: 8.34712068e-07
Iter: 1225 loss: 8.34131697e-07
Iter: 1226 loss: 8.3385271e-07
Iter: 1227 loss: 8.33627155e-07
Iter: 1228 loss: 8.32802e-07
Iter: 1229 loss: 8.33439572e-07
Iter: 1230 loss: 8.32336866e-07
Iter: 1231 loss: 8.31373825e-07
Iter: 1232 loss: 8.38416554e-07
Iter: 1233 loss: 8.31320961e-07
Iter: 1234 loss: 8.3062281e-07
Iter: 1235 loss: 8.31929128e-07
Iter: 1236 loss: 8.3039231e-07
Iter: 1237 loss: 8.29732812e-07
Iter: 1238 loss: 8.29762257e-07
Iter: 1239 loss: 8.29350824e-07
Iter: 1240 loss: 8.33246872e-07
Iter: 1241 loss: 8.29339172e-07
Iter: 1242 loss: 8.29093835e-07
Iter: 1243 loss: 8.28559678e-07
Iter: 1244 loss: 8.3956229e-07
Iter: 1245 loss: 8.28575708e-07
Iter: 1246 loss: 8.2784743e-07
Iter: 1247 loss: 8.30229965e-07
Iter: 1248 loss: 8.27701285e-07
Iter: 1249 loss: 8.27035876e-07
Iter: 1250 loss: 8.26398264e-07
Iter: 1251 loss: 8.2624e-07
Iter: 1252 loss: 8.25208303e-07
Iter: 1253 loss: 8.32081582e-07
Iter: 1254 loss: 8.25126619e-07
Iter: 1255 loss: 8.24521805e-07
Iter: 1256 loss: 8.24959159e-07
Iter: 1257 loss: 8.24138056e-07
Iter: 1258 loss: 8.23237883e-07
Iter: 1259 loss: 8.27708504e-07
Iter: 1260 loss: 8.2305985e-07
Iter: 1261 loss: 8.22414677e-07
Iter: 1262 loss: 8.22730044e-07
Iter: 1263 loss: 8.21925482e-07
Iter: 1264 loss: 8.21288779e-07
Iter: 1265 loss: 8.25656628e-07
Iter: 1266 loss: 8.21209937e-07
Iter: 1267 loss: 8.20642583e-07
Iter: 1268 loss: 8.20255e-07
Iter: 1269 loss: 8.20054424e-07
Iter: 1270 loss: 8.19425395e-07
Iter: 1271 loss: 8.19454e-07
Iter: 1272 loss: 8.18946774e-07
Iter: 1273 loss: 8.23096798e-07
Iter: 1274 loss: 8.18874128e-07
Iter: 1275 loss: 8.18434046e-07
Iter: 1276 loss: 8.18491685e-07
Iter: 1277 loss: 8.18149545e-07
Iter: 1278 loss: 8.1763676e-07
Iter: 1279 loss: 8.1806445e-07
Iter: 1280 loss: 8.17366185e-07
Iter: 1281 loss: 8.16779163e-07
Iter: 1282 loss: 8.18268177e-07
Iter: 1283 loss: 8.16594707e-07
Iter: 1284 loss: 8.16114095e-07
Iter: 1285 loss: 8.16435715e-07
Iter: 1286 loss: 8.15809472e-07
Iter: 1287 loss: 8.15145711e-07
Iter: 1288 loss: 8.15156056e-07
Iter: 1289 loss: 8.14626901e-07
Iter: 1290 loss: 8.14107807e-07
Iter: 1291 loss: 8.14073701e-07
Iter: 1292 loss: 8.13636461e-07
Iter: 1293 loss: 8.13575355e-07
Iter: 1294 loss: 8.13293809e-07
Iter: 1295 loss: 8.12668418e-07
Iter: 1296 loss: 8.14144698e-07
Iter: 1297 loss: 8.12531425e-07
Iter: 1298 loss: 8.11797236e-07
Iter: 1299 loss: 8.12426379e-07
Iter: 1300 loss: 8.11378413e-07
Iter: 1301 loss: 8.10637175e-07
Iter: 1302 loss: 8.11682696e-07
Iter: 1303 loss: 8.10303732e-07
Iter: 1304 loss: 8.09784467e-07
Iter: 1305 loss: 8.09713e-07
Iter: 1306 loss: 8.09211258e-07
Iter: 1307 loss: 8.12101234e-07
Iter: 1308 loss: 8.09145149e-07
Iter: 1309 loss: 8.08795e-07
Iter: 1310 loss: 8.08515495e-07
Iter: 1311 loss: 8.08420623e-07
Iter: 1312 loss: 8.07921879e-07
Iter: 1313 loss: 8.0920347e-07
Iter: 1314 loss: 8.0773674e-07
Iter: 1315 loss: 8.07218555e-07
Iter: 1316 loss: 8.08340644e-07
Iter: 1317 loss: 8.07033416e-07
Iter: 1318 loss: 8.06446053e-07
Iter: 1319 loss: 8.06341347e-07
Iter: 1320 loss: 8.05924e-07
Iter: 1321 loss: 8.05360173e-07
Iter: 1322 loss: 8.07753e-07
Iter: 1323 loss: 8.05248874e-07
Iter: 1324 loss: 8.04614274e-07
Iter: 1325 loss: 8.06883577e-07
Iter: 1326 loss: 8.04460171e-07
Iter: 1327 loss: 8.03901457e-07
Iter: 1328 loss: 8.05605907e-07
Iter: 1329 loss: 8.03746786e-07
Iter: 1330 loss: 8.03131059e-07
Iter: 1331 loss: 8.03758667e-07
Iter: 1332 loss: 8.02788179e-07
Iter: 1333 loss: 8.02194e-07
Iter: 1334 loss: 8.01862029e-07
Iter: 1335 loss: 8.01570138e-07
Iter: 1336 loss: 8.00999487e-07
Iter: 1337 loss: 8.00964358e-07
Iter: 1338 loss: 8.00680425e-07
Iter: 1339 loss: 8.00717658e-07
Iter: 1340 loss: 8.00411215e-07
Iter: 1341 loss: 8.00144903e-07
Iter: 1342 loss: 8.00100565e-07
Iter: 1343 loss: 7.99743532e-07
Iter: 1344 loss: 8.00939517e-07
Iter: 1345 loss: 7.99626264e-07
Iter: 1346 loss: 7.99235863e-07
Iter: 1347 loss: 7.99264342e-07
Iter: 1348 loss: 7.98917313e-07
Iter: 1349 loss: 7.98377755e-07
Iter: 1350 loss: 8.00170596e-07
Iter: 1351 loss: 7.98151063e-07
Iter: 1352 loss: 7.97654252e-07
Iter: 1353 loss: 7.9736634e-07
Iter: 1354 loss: 7.97227187e-07
Iter: 1355 loss: 7.9648521e-07
Iter: 1356 loss: 8.015e-07
Iter: 1357 loss: 7.96409324e-07
Iter: 1358 loss: 7.95970664e-07
Iter: 1359 loss: 7.97221674e-07
Iter: 1360 loss: 7.957496e-07
Iter: 1361 loss: 7.95246763e-07
Iter: 1362 loss: 7.9714232e-07
Iter: 1363 loss: 7.9511176e-07
Iter: 1364 loss: 7.94560151e-07
Iter: 1365 loss: 7.94124219e-07
Iter: 1366 loss: 7.93975119e-07
Iter: 1367 loss: 7.9332267e-07
Iter: 1368 loss: 8.00572423e-07
Iter: 1369 loss: 7.93371214e-07
Iter: 1370 loss: 7.92926357e-07
Iter: 1371 loss: 7.9740056e-07
Iter: 1372 loss: 7.92896628e-07
Iter: 1373 loss: 7.92413289e-07
Iter: 1374 loss: 7.92813239e-07
Iter: 1375 loss: 7.92138735e-07
Iter: 1376 loss: 7.9174265e-07
Iter: 1377 loss: 7.92548e-07
Iter: 1378 loss: 7.91595426e-07
Iter: 1379 loss: 7.9117234e-07
Iter: 1380 loss: 7.91151535e-07
Iter: 1381 loss: 7.90802972e-07
Iter: 1382 loss: 7.90303943e-07
Iter: 1383 loss: 7.93364563e-07
Iter: 1384 loss: 7.90284048e-07
Iter: 1385 loss: 7.89870228e-07
Iter: 1386 loss: 7.89361195e-07
Iter: 1387 loss: 7.89247508e-07
Iter: 1388 loss: 7.88516047e-07
Iter: 1389 loss: 7.93915774e-07
Iter: 1390 loss: 7.88507919e-07
Iter: 1391 loss: 7.88035891e-07
Iter: 1392 loss: 7.88224497e-07
Iter: 1393 loss: 7.87762815e-07
Iter: 1394 loss: 7.87070235e-07
Iter: 1395 loss: 7.9138249e-07
Iter: 1396 loss: 7.86977e-07
Iter: 1397 loss: 7.86413239e-07
Iter: 1398 loss: 7.8620883e-07
Iter: 1399 loss: 7.85953148e-07
Iter: 1400 loss: 7.8520327e-07
Iter: 1401 loss: 7.89299065e-07
Iter: 1402 loss: 7.85142902e-07
Iter: 1403 loss: 7.84480108e-07
Iter: 1404 loss: 7.84874032e-07
Iter: 1405 loss: 7.841208e-07
Iter: 1406 loss: 7.83823168e-07
Iter: 1407 loss: 7.83635528e-07
Iter: 1408 loss: 7.8347e-07
Iter: 1409 loss: 7.83042083e-07
Iter: 1410 loss: 7.89000467e-07
Iter: 1411 loss: 7.83027531e-07
Iter: 1412 loss: 7.82558e-07
Iter: 1413 loss: 7.85315194e-07
Iter: 1414 loss: 7.82481379e-07
Iter: 1415 loss: 7.82109964e-07
Iter: 1416 loss: 7.82580457e-07
Iter: 1417 loss: 7.81864969e-07
Iter: 1418 loss: 7.8139152e-07
Iter: 1419 loss: 7.81878839e-07
Iter: 1420 loss: 7.81131462e-07
Iter: 1421 loss: 7.80609696e-07
Iter: 1422 loss: 7.85016596e-07
Iter: 1423 loss: 7.80578603e-07
Iter: 1424 loss: 7.80181608e-07
Iter: 1425 loss: 7.79416951e-07
Iter: 1426 loss: 7.95162578e-07
Iter: 1427 loss: 7.79435481e-07
Iter: 1428 loss: 7.78701065e-07
Iter: 1429 loss: 7.89435376e-07
Iter: 1430 loss: 7.78716526e-07
Iter: 1431 loss: 7.78131835e-07
Iter: 1432 loss: 7.79292691e-07
Iter: 1433 loss: 7.77930381e-07
Iter: 1434 loss: 7.77441755e-07
Iter: 1435 loss: 7.78963226e-07
Iter: 1436 loss: 7.77328125e-07
Iter: 1437 loss: 7.76736101e-07
Iter: 1438 loss: 7.76507477e-07
Iter: 1439 loss: 7.76170964e-07
Iter: 1440 loss: 7.76492186e-07
Iter: 1441 loss: 7.7594359e-07
Iter: 1442 loss: 7.75676426e-07
Iter: 1443 loss: 7.75258343e-07
Iter: 1444 loss: 7.75210765e-07
Iter: 1445 loss: 7.74755335e-07
Iter: 1446 loss: 7.75247088e-07
Iter: 1447 loss: 7.74518753e-07
Iter: 1448 loss: 7.73933607e-07
Iter: 1449 loss: 7.77856258e-07
Iter: 1450 loss: 7.73929116e-07
Iter: 1451 loss: 7.73380634e-07
Iter: 1452 loss: 7.73281045e-07
Iter: 1453 loss: 7.72945441e-07
Iter: 1454 loss: 7.72474152e-07
Iter: 1455 loss: 7.7534537e-07
Iter: 1456 loss: 7.72488363e-07
Iter: 1457 loss: 7.7199951e-07
Iter: 1458 loss: 7.71713303e-07
Iter: 1459 loss: 7.71573e-07
Iter: 1460 loss: 7.70892541e-07
Iter: 1461 loss: 7.73124611e-07
Iter: 1462 loss: 7.70756799e-07
Iter: 1463 loss: 7.69998394e-07
Iter: 1464 loss: 7.7151708e-07
Iter: 1465 loss: 7.69781536e-07
Iter: 1466 loss: 7.69138524e-07
Iter: 1467 loss: 7.71871612e-07
Iter: 1468 loss: 7.68971688e-07
Iter: 1469 loss: 7.68457767e-07
Iter: 1470 loss: 7.68664165e-07
Iter: 1471 loss: 7.68066684e-07
Iter: 1472 loss: 7.67905362e-07
Iter: 1473 loss: 7.67769279e-07
Iter: 1474 loss: 7.67508e-07
Iter: 1475 loss: 7.67075278e-07
Iter: 1476 loss: 7.67011386e-07
Iter: 1477 loss: 7.66566586e-07
Iter: 1478 loss: 7.67048618e-07
Iter: 1479 loss: 7.66310052e-07
Iter: 1480 loss: 7.65770437e-07
Iter: 1481 loss: 7.68753239e-07
Iter: 1482 loss: 7.65697678e-07
Iter: 1483 loss: 7.65243897e-07
Iter: 1484 loss: 7.65852803e-07
Iter: 1485 loss: 7.64995207e-07
Iter: 1486 loss: 7.64542733e-07
Iter: 1487 loss: 7.64918127e-07
Iter: 1488 loss: 7.64306094e-07
Iter: 1489 loss: 7.63577248e-07
Iter: 1490 loss: 7.6553647e-07
Iter: 1491 loss: 7.6345566e-07
Iter: 1492 loss: 7.62875061e-07
Iter: 1493 loss: 7.63926607e-07
Iter: 1494 loss: 7.62652576e-07
Iter: 1495 loss: 7.62011837e-07
Iter: 1496 loss: 7.64564e-07
Iter: 1497 loss: 7.61882916e-07
Iter: 1498 loss: 7.61364845e-07
Iter: 1499 loss: 7.63318e-07
Iter: 1500 loss: 7.6118755e-07
Iter: 1501 loss: 7.60714613e-07
Iter: 1502 loss: 7.60726664e-07
Iter: 1503 loss: 7.60231728e-07
Iter: 1504 loss: 7.59813e-07
Iter: 1505 loss: 7.6712e-07
Iter: 1506 loss: 7.59804607e-07
Iter: 1507 loss: 7.59319846e-07
Iter: 1508 loss: 7.60775038e-07
Iter: 1509 loss: 7.59148122e-07
Iter: 1510 loss: 7.58751639e-07
Iter: 1511 loss: 7.58502154e-07
Iter: 1512 loss: 7.58448095e-07
Iter: 1513 loss: 7.57881537e-07
Iter: 1514 loss: 7.59840191e-07
Iter: 1515 loss: 7.57761256e-07
Iter: 1516 loss: 7.57198222e-07
Iter: 1517 loss: 7.59296256e-07
Iter: 1518 loss: 7.57131e-07
Iter: 1519 loss: 7.56704537e-07
Iter: 1520 loss: 7.56660597e-07
Iter: 1521 loss: 7.56357224e-07
Iter: 1522 loss: 7.55782139e-07
Iter: 1523 loss: 7.59080478e-07
Iter: 1524 loss: 7.55770202e-07
Iter: 1525 loss: 7.55282144e-07
Iter: 1526 loss: 7.55348367e-07
Iter: 1527 loss: 7.54955124e-07
Iter: 1528 loss: 7.54263056e-07
Iter: 1529 loss: 7.55947838e-07
Iter: 1530 loss: 7.53985205e-07
Iter: 1531 loss: 7.53382e-07
Iter: 1532 loss: 7.56516329e-07
Iter: 1533 loss: 7.53306153e-07
Iter: 1534 loss: 7.52675874e-07
Iter: 1535 loss: 7.53292397e-07
Iter: 1536 loss: 7.5231037e-07
Iter: 1537 loss: 7.51685207e-07
Iter: 1538 loss: 7.53066047e-07
Iter: 1539 loss: 7.5143339e-07
Iter: 1540 loss: 7.50964318e-07
Iter: 1541 loss: 7.50916797e-07
Iter: 1542 loss: 7.50588185e-07
Iter: 1543 loss: 7.50024924e-07
Iter: 1544 loss: 7.64211848e-07
Iter: 1545 loss: 7.50024242e-07
Iter: 1546 loss: 7.49411242e-07
Iter: 1547 loss: 7.49951425e-07
Iter: 1548 loss: 7.49072342e-07
Iter: 1549 loss: 7.48393745e-07
Iter: 1550 loss: 7.56100917e-07
Iter: 1551 loss: 7.48407956e-07
Iter: 1552 loss: 7.48017214e-07
Iter: 1553 loss: 7.48236857e-07
Iter: 1554 loss: 7.47738397e-07
Iter: 1555 loss: 7.47303488e-07
Iter: 1556 loss: 7.48147613e-07
Iter: 1557 loss: 7.47109766e-07
Iter: 1558 loss: 7.46608862e-07
Iter: 1559 loss: 7.47880108e-07
Iter: 1560 loss: 7.46390583e-07
Iter: 1561 loss: 7.45878253e-07
Iter: 1562 loss: 7.46921444e-07
Iter: 1563 loss: 7.45617626e-07
Iter: 1564 loss: 7.45104387e-07
Iter: 1565 loss: 7.46957312e-07
Iter: 1566 loss: 7.44937e-07
Iter: 1567 loss: 7.444159e-07
Iter: 1568 loss: 7.45202499e-07
Iter: 1569 loss: 7.44123327e-07
Iter: 1570 loss: 7.43437852e-07
Iter: 1571 loss: 7.43626e-07
Iter: 1572 loss: 7.42917223e-07
Iter: 1573 loss: 7.42892269e-07
Iter: 1574 loss: 7.42524776e-07
Iter: 1575 loss: 7.42242833e-07
Iter: 1576 loss: 7.42018528e-07
Iter: 1577 loss: 7.41888584e-07
Iter: 1578 loss: 7.41538031e-07
Iter: 1579 loss: 7.40978521e-07
Iter: 1580 loss: 7.40915482e-07
Iter: 1581 loss: 7.40425776e-07
Iter: 1582 loss: 7.40406165e-07
Iter: 1583 loss: 7.39970858e-07
Iter: 1584 loss: 7.39797088e-07
Iter: 1585 loss: 7.39564427e-07
Iter: 1586 loss: 7.3901424e-07
Iter: 1587 loss: 7.41001031e-07
Iter: 1588 loss: 7.38812105e-07
Iter: 1589 loss: 7.38253e-07
Iter: 1590 loss: 7.39265545e-07
Iter: 1591 loss: 7.38034146e-07
Iter: 1592 loss: 7.3744809e-07
Iter: 1593 loss: 7.40096425e-07
Iter: 1594 loss: 7.3732366e-07
Iter: 1595 loss: 7.36883749e-07
Iter: 1596 loss: 7.37372716e-07
Iter: 1597 loss: 7.36604875e-07
Iter: 1598 loss: 7.3599108e-07
Iter: 1599 loss: 7.39071879e-07
Iter: 1600 loss: 7.35953e-07
Iter: 1601 loss: 7.35427534e-07
Iter: 1602 loss: 7.35261096e-07
Iter: 1603 loss: 7.34996206e-07
Iter: 1604 loss: 7.34782361e-07
Iter: 1605 loss: 7.34618538e-07
Iter: 1606 loss: 7.34281912e-07
Iter: 1607 loss: 7.34399293e-07
Iter: 1608 loss: 7.34068806e-07
Iter: 1609 loss: 7.33718252e-07
Iter: 1610 loss: 7.33129582e-07
Iter: 1611 loss: 7.33148e-07
Iter: 1612 loss: 7.32708827e-07
Iter: 1613 loss: 7.39331199e-07
Iter: 1614 loss: 7.32678643e-07
Iter: 1615 loss: 7.32246e-07
Iter: 1616 loss: 7.32449848e-07
Iter: 1617 loss: 7.31968612e-07
Iter: 1618 loss: 7.3148658e-07
Iter: 1619 loss: 7.32728267e-07
Iter: 1620 loss: 7.31397449e-07
Iter: 1621 loss: 7.30809063e-07
Iter: 1622 loss: 7.31324917e-07
Iter: 1623 loss: 7.30588397e-07
Iter: 1624 loss: 7.29974033e-07
Iter: 1625 loss: 7.32779881e-07
Iter: 1626 loss: 7.29843237e-07
Iter: 1627 loss: 7.29357907e-07
Iter: 1628 loss: 7.29854378e-07
Iter: 1629 loss: 7.29121098e-07
Iter: 1630 loss: 7.28602686e-07
Iter: 1631 loss: 7.32277442e-07
Iter: 1632 loss: 7.28588475e-07
Iter: 1633 loss: 7.28224222e-07
Iter: 1634 loss: 7.28336374e-07
Iter: 1635 loss: 7.27893791e-07
Iter: 1636 loss: 7.27591e-07
Iter: 1637 loss: 7.27544716e-07
Iter: 1638 loss: 7.27258e-07
Iter: 1639 loss: 7.28363659e-07
Iter: 1640 loss: 7.2717603e-07
Iter: 1641 loss: 7.26937344e-07
Iter: 1642 loss: 7.26522558e-07
Iter: 1643 loss: 7.36450943e-07
Iter: 1644 loss: 7.26515395e-07
Iter: 1645 loss: 7.2608e-07
Iter: 1646 loss: 7.27971951e-07
Iter: 1647 loss: 7.26006647e-07
Iter: 1648 loss: 7.2552109e-07
Iter: 1649 loss: 7.27483496e-07
Iter: 1650 loss: 7.25390464e-07
Iter: 1651 loss: 7.2500859e-07
Iter: 1652 loss: 7.25388702e-07
Iter: 1653 loss: 7.24799918e-07
Iter: 1654 loss: 7.2437274e-07
Iter: 1655 loss: 7.24748134e-07
Iter: 1656 loss: 7.2409091e-07
Iter: 1657 loss: 7.23558287e-07
Iter: 1658 loss: 7.2673032e-07
Iter: 1659 loss: 7.23497351e-07
Iter: 1660 loss: 7.23037374e-07
Iter: 1661 loss: 7.23319715e-07
Iter: 1662 loss: 7.22779419e-07
Iter: 1663 loss: 7.22268965e-07
Iter: 1664 loss: 7.25153541e-07
Iter: 1665 loss: 7.22172786e-07
Iter: 1666 loss: 7.21726565e-07
Iter: 1667 loss: 7.22258164e-07
Iter: 1668 loss: 7.21443143e-07
Iter: 1669 loss: 7.20912453e-07
Iter: 1670 loss: 7.24273491e-07
Iter: 1671 loss: 7.20903415e-07
Iter: 1672 loss: 7.2050932e-07
Iter: 1673 loss: 7.20513526e-07
Iter: 1674 loss: 7.20297294e-07
Iter: 1675 loss: 7.19798606e-07
Iter: 1676 loss: 7.23557264e-07
Iter: 1677 loss: 7.19739376e-07
Iter: 1678 loss: 7.19248646e-07
Iter: 1679 loss: 7.20963897e-07
Iter: 1680 loss: 7.19095169e-07
Iter: 1681 loss: 7.18620811e-07
Iter: 1682 loss: 7.24292477e-07
Iter: 1683 loss: 7.18641672e-07
Iter: 1684 loss: 7.18289e-07
Iter: 1685 loss: 7.18517413e-07
Iter: 1686 loss: 7.18092e-07
Iter: 1687 loss: 7.17695798e-07
Iter: 1688 loss: 7.17542889e-07
Iter: 1689 loss: 7.17363207e-07
Iter: 1690 loss: 7.16807108e-07
Iter: 1691 loss: 7.21275228e-07
Iter: 1692 loss: 7.16792556e-07
Iter: 1693 loss: 7.16390559e-07
Iter: 1694 loss: 7.16467071e-07
Iter: 1695 loss: 7.16082809e-07
Iter: 1696 loss: 7.15610781e-07
Iter: 1697 loss: 7.18361775e-07
Iter: 1698 loss: 7.15566216e-07
Iter: 1699 loss: 7.15128067e-07
Iter: 1700 loss: 7.15689339e-07
Iter: 1701 loss: 7.14905354e-07
Iter: 1702 loss: 7.144e-07
Iter: 1703 loss: 7.16957402e-07
Iter: 1704 loss: 7.14368468e-07
Iter: 1705 loss: 7.14163093e-07
Iter: 1706 loss: 7.14152122e-07
Iter: 1707 loss: 7.13969598e-07
Iter: 1708 loss: 7.13544068e-07
Iter: 1709 loss: 7.19127343e-07
Iter: 1710 loss: 7.13512577e-07
Iter: 1711 loss: 7.13052032e-07
Iter: 1712 loss: 7.13126497e-07
Iter: 1713 loss: 7.12769634e-07
Iter: 1714 loss: 7.12270094e-07
Iter: 1715 loss: 7.19433842e-07
Iter: 1716 loss: 7.12288852e-07
Iter: 1717 loss: 7.11903681e-07
Iter: 1718 loss: 7.12724784e-07
Iter: 1719 loss: 7.11717803e-07
Iter: 1720 loss: 7.11365374e-07
Iter: 1721 loss: 7.11435121e-07
Iter: 1722 loss: 7.11104803e-07
Iter: 1723 loss: 7.10705e-07
Iter: 1724 loss: 7.13110239e-07
Iter: 1725 loss: 7.1070707e-07
Iter: 1726 loss: 7.10344921e-07
Iter: 1727 loss: 7.10738959e-07
Iter: 1728 loss: 7.10181553e-07
Iter: 1729 loss: 7.09740561e-07
Iter: 1730 loss: 7.10887093e-07
Iter: 1731 loss: 7.09657e-07
Iter: 1732 loss: 7.09199867e-07
Iter: 1733 loss: 7.09639892e-07
Iter: 1734 loss: 7.08916048e-07
Iter: 1735 loss: 7.08377911e-07
Iter: 1736 loss: 7.12162432e-07
Iter: 1737 loss: 7.08324137e-07
Iter: 1738 loss: 7.08079938e-07
Iter: 1739 loss: 7.08101652e-07
Iter: 1740 loss: 7.07838922e-07
Iter: 1741 loss: 7.07452841e-07
Iter: 1742 loss: 7.07464096e-07
Iter: 1743 loss: 7.07072502e-07
Iter: 1744 loss: 7.06885771e-07
Iter: 1745 loss: 7.06712228e-07
Iter: 1746 loss: 7.06163291e-07
Iter: 1747 loss: 7.0894e-07
Iter: 1748 loss: 7.06059893e-07
Iter: 1749 loss: 7.05649313e-07
Iter: 1750 loss: 7.10305471e-07
Iter: 1751 loss: 7.05662615e-07
Iter: 1752 loss: 7.05364869e-07
Iter: 1753 loss: 7.05036598e-07
Iter: 1754 loss: 7.05007778e-07
Iter: 1755 loss: 7.04544732e-07
Iter: 1756 loss: 7.05351567e-07
Iter: 1757 loss: 7.04330716e-07
Iter: 1758 loss: 7.03846467e-07
Iter: 1759 loss: 7.07490074e-07
Iter: 1760 loss: 7.03803721e-07
Iter: 1761 loss: 7.03482385e-07
Iter: 1762 loss: 7.04085096e-07
Iter: 1763 loss: 7.0335966e-07
Iter: 1764 loss: 7.02949421e-07
Iter: 1765 loss: 7.03153205e-07
Iter: 1766 loss: 7.02655029e-07
Iter: 1767 loss: 7.02174248e-07
Iter: 1768 loss: 7.04956221e-07
Iter: 1769 loss: 7.02150317e-07
Iter: 1770 loss: 7.01727572e-07
Iter: 1771 loss: 7.05194054e-07
Iter: 1772 loss: 7.01699378e-07
Iter: 1773 loss: 7.012639e-07
Iter: 1774 loss: 7.02641046e-07
Iter: 1775 loss: 7.01207796e-07
Iter: 1776 loss: 7.00942621e-07
Iter: 1777 loss: 7.00492762e-07
Iter: 1778 loss: 7.10432516e-07
Iter: 1779 loss: 7.00521923e-07
Iter: 1780 loss: 6.99994587e-07
Iter: 1781 loss: 7.01301474e-07
Iter: 1782 loss: 6.99832e-07
Iter: 1783 loss: 6.9947987e-07
Iter: 1784 loss: 7.03839589e-07
Iter: 1785 loss: 6.99482484e-07
Iter: 1786 loss: 6.99149325e-07
Iter: 1787 loss: 6.99449515e-07
Iter: 1788 loss: 6.98928091e-07
Iter: 1789 loss: 6.98593e-07
Iter: 1790 loss: 6.98572364e-07
Iter: 1791 loss: 6.98323731e-07
Iter: 1792 loss: 6.97868643e-07
Iter: 1793 loss: 7.00358328e-07
Iter: 1794 loss: 6.97818109e-07
Iter: 1795 loss: 6.97483188e-07
Iter: 1796 loss: 6.98578674e-07
Iter: 1797 loss: 6.97325504e-07
Iter: 1798 loss: 6.96917084e-07
Iter: 1799 loss: 6.97087444e-07
Iter: 1800 loss: 6.96579264e-07
Iter: 1801 loss: 6.9615362e-07
Iter: 1802 loss: 6.98217946e-07
Iter: 1803 loss: 6.9603351e-07
Iter: 1804 loss: 6.95659764e-07
Iter: 1805 loss: 6.98747954e-07
Iter: 1806 loss: 6.95625488e-07
Iter: 1807 loss: 6.95434551e-07
Iter: 1808 loss: 6.98844701e-07
Iter: 1809 loss: 6.95395499e-07
Iter: 1810 loss: 6.95184156e-07
Iter: 1811 loss: 6.94815355e-07
Iter: 1812 loss: 7.02599095e-07
Iter: 1813 loss: 6.94844744e-07
Iter: 1814 loss: 6.94370897e-07
Iter: 1815 loss: 6.94675634e-07
Iter: 1816 loss: 6.94093615e-07
Iter: 1817 loss: 6.93727e-07
Iter: 1818 loss: 6.9488982e-07
Iter: 1819 loss: 6.93577249e-07
Iter: 1820 loss: 6.93237723e-07
Iter: 1821 loss: 6.93271431e-07
Iter: 1822 loss: 6.93012566e-07
Iter: 1823 loss: 6.9270942e-07
Iter: 1824 loss: 6.9267935e-07
Iter: 1825 loss: 6.92261494e-07
Iter: 1826 loss: 6.92623587e-07
Iter: 1827 loss: 6.92048957e-07
Iter: 1828 loss: 6.915526e-07
Iter: 1829 loss: 6.93548884e-07
Iter: 1830 loss: 6.91423907e-07
Iter: 1831 loss: 6.90955062e-07
Iter: 1832 loss: 6.93510344e-07
Iter: 1833 loss: 6.90881279e-07
Iter: 1834 loss: 6.90537149e-07
Iter: 1835 loss: 6.90359229e-07
Iter: 1836 loss: 6.90151921e-07
Iter: 1837 loss: 6.89734406e-07
Iter: 1838 loss: 6.9441063e-07
Iter: 1839 loss: 6.89700471e-07
Iter: 1840 loss: 6.89520846e-07
Iter: 1841 loss: 6.89562e-07
Iter: 1842 loss: 6.89338776e-07
Iter: 1843 loss: 6.89106287e-07
Iter: 1844 loss: 6.89107708e-07
Iter: 1845 loss: 6.88771706e-07
Iter: 1846 loss: 6.8908264e-07
Iter: 1847 loss: 6.88639204e-07
Iter: 1848 loss: 6.88392674e-07
Iter: 1849 loss: 6.88380965e-07
Iter: 1850 loss: 6.88124828e-07
Iter: 1851 loss: 6.87817305e-07
Iter: 1852 loss: 6.91656396e-07
Iter: 1853 loss: 6.87813156e-07
Iter: 1854 loss: 6.87520867e-07
Iter: 1855 loss: 6.8872e-07
Iter: 1856 loss: 6.87466809e-07
Iter: 1857 loss: 6.87183615e-07
Iter: 1858 loss: 6.87028717e-07
Iter: 1859 loss: 6.86930093e-07
Iter: 1860 loss: 6.86564135e-07
Iter: 1861 loss: 6.87029569e-07
Iter: 1862 loss: 6.86359442e-07
Iter: 1863 loss: 6.86005706e-07
Iter: 1864 loss: 6.88559453e-07
Iter: 1865 loss: 6.85946134e-07
Iter: 1866 loss: 6.85599559e-07
Iter: 1867 loss: 6.86759051e-07
Iter: 1868 loss: 6.8550537e-07
Iter: 1869 loss: 6.8520626e-07
Iter: 1870 loss: 6.85830969e-07
Iter: 1871 loss: 6.85104112e-07
Iter: 1872 loss: 6.8488464e-07
Iter: 1873 loss: 6.87509e-07
Iter: 1874 loss: 6.84926817e-07
Iter: 1875 loss: 6.84578367e-07
Iter: 1876 loss: 6.84860595e-07
Iter: 1877 loss: 6.84411589e-07
Iter: 1878 loss: 6.84228667e-07
Iter: 1879 loss: 6.84204451e-07
Iter: 1880 loss: 6.84021643e-07
Iter: 1881 loss: 6.83743963e-07
Iter: 1882 loss: 6.83896587e-07
Iter: 1883 loss: 6.83542225e-07
Iter: 1884 loss: 6.83103394e-07
Iter: 1885 loss: 6.84312113e-07
Iter: 1886 loss: 6.83039673e-07
Iter: 1887 loss: 6.82705377e-07
Iter: 1888 loss: 6.84339511e-07
Iter: 1889 loss: 6.82635914e-07
Iter: 1890 loss: 6.82260634e-07
Iter: 1891 loss: 6.83562405e-07
Iter: 1892 loss: 6.82170707e-07
Iter: 1893 loss: 6.81920767e-07
Iter: 1894 loss: 6.81597498e-07
Iter: 1895 loss: 6.81529286e-07
Iter: 1896 loss: 6.81145e-07
Iter: 1897 loss: 6.8316956e-07
Iter: 1898 loss: 6.81088409e-07
Iter: 1899 loss: 6.80705341e-07
Iter: 1900 loss: 6.82522796e-07
Iter: 1901 loss: 6.80656854e-07
Iter: 1902 loss: 6.80312382e-07
Iter: 1903 loss: 6.81341589e-07
Iter: 1904 loss: 6.80193693e-07
Iter: 1905 loss: 6.79881964e-07
Iter: 1906 loss: 6.81025426e-07
Iter: 1907 loss: 6.79764412e-07
Iter: 1908 loss: 6.79551306e-07
Iter: 1909 loss: 6.79538743e-07
Iter: 1910 loss: 6.79474397e-07
Iter: 1911 loss: 6.79201435e-07
Iter: 1912 loss: 6.83150802e-07
Iter: 1913 loss: 6.79193363e-07
Iter: 1914 loss: 6.78889705e-07
Iter: 1915 loss: 6.79326718e-07
Iter: 1916 loss: 6.78747142e-07
Iter: 1917 loss: 6.78352308e-07
Iter: 1918 loss: 6.79309892e-07
Iter: 1919 loss: 6.78219465e-07
Iter: 1920 loss: 6.77912965e-07
Iter: 1921 loss: 6.79244295e-07
Iter: 1922 loss: 6.77811386e-07
Iter: 1923 loss: 6.77557409e-07
Iter: 1924 loss: 6.81194365e-07
Iter: 1925 loss: 6.77539617e-07
Iter: 1926 loss: 6.77321339e-07
Iter: 1927 loss: 6.76964873e-07
Iter: 1928 loss: 6.83243e-07
Iter: 1929 loss: 6.76942648e-07
Iter: 1930 loss: 6.76485229e-07
Iter: 1931 loss: 6.78331617e-07
Iter: 1932 loss: 6.76412185e-07
Iter: 1933 loss: 6.76025252e-07
Iter: 1934 loss: 6.76764103e-07
Iter: 1935 loss: 6.75832439e-07
Iter: 1936 loss: 6.7534188e-07
Iter: 1937 loss: 6.77525861e-07
Iter: 1938 loss: 6.75242632e-07
Iter: 1939 loss: 6.74821536e-07
Iter: 1940 loss: 6.76304751e-07
Iter: 1941 loss: 6.74696309e-07
Iter: 1942 loss: 6.7449821e-07
Iter: 1943 loss: 6.7449389e-07
Iter: 1944 loss: 6.74312446e-07
Iter: 1945 loss: 6.73967179e-07
Iter: 1946 loss: 6.81113647e-07
Iter: 1947 loss: 6.73954048e-07
Iter: 1948 loss: 6.73580757e-07
Iter: 1949 loss: 6.73693535e-07
Iter: 1950 loss: 6.73317459e-07
Iter: 1951 loss: 6.72893634e-07
Iter: 1952 loss: 6.74687499e-07
Iter: 1953 loss: 6.72759597e-07
Iter: 1954 loss: 6.72373858e-07
Iter: 1955 loss: 6.72798137e-07
Iter: 1956 loss: 6.72153249e-07
Iter: 1957 loss: 6.7177757e-07
Iter: 1958 loss: 6.75868591e-07
Iter: 1959 loss: 6.71735506e-07
Iter: 1960 loss: 6.71438556e-07
Iter: 1961 loss: 6.71678094e-07
Iter: 1962 loss: 6.71240286e-07
Iter: 1963 loss: 6.70919235e-07
Iter: 1964 loss: 6.7112444e-07
Iter: 1965 loss: 6.70679469e-07
Iter: 1966 loss: 6.70225575e-07
Iter: 1967 loss: 6.70480915e-07
Iter: 1968 loss: 6.69987571e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4
+ date
Mon Oct 26 13:28:24 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.4_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.4_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a21b20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a21089d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a2108c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a2217730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a2217c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a22179d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a20c56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a20697b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a207f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a202e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1ff69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1fe6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1fe6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1fc32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1fc3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1fc3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1f2b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1ed2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1e99950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1ebaea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1ec7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1e7b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a1e7b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8089928840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8089927620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8089955510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f808997a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f808990d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f808990d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f808990d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8064118ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f806413f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f806413f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8064164620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f806418bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80640e2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.38733077e-05
Iter: 2 loss: 0.00310486183
Iter: 3 loss: 4.01651414e-05
Iter: 4 loss: 3.87693908e-05
Iter: 5 loss: 3.77705073e-05
Iter: 6 loss: 3.72776631e-05
Iter: 7 loss: 3.47093483e-05
Iter: 8 loss: 3.57947865e-05
Iter: 9 loss: 3.29522554e-05
Iter: 10 loss: 3.24656139e-05
Iter: 11 loss: 3.08633389e-05
Iter: 12 loss: 3.11042677e-05
Iter: 13 loss: 2.92596815e-05
Iter: 14 loss: 2.64049449e-05
Iter: 15 loss: 3.59677506e-05
Iter: 16 loss: 2.56233e-05
Iter: 17 loss: 2.39588244e-05
Iter: 18 loss: 3.77805191e-05
Iter: 19 loss: 2.38558187e-05
Iter: 20 loss: 2.29531324e-05
Iter: 21 loss: 2.32641178e-05
Iter: 22 loss: 2.23158168e-05
Iter: 23 loss: 2.14789889e-05
Iter: 24 loss: 2.87444636e-05
Iter: 25 loss: 2.14347492e-05
Iter: 26 loss: 2.08667443e-05
Iter: 27 loss: 2.16172157e-05
Iter: 28 loss: 2.05788765e-05
Iter: 29 loss: 2.00738268e-05
Iter: 30 loss: 2.21610244e-05
Iter: 31 loss: 1.99648275e-05
Iter: 32 loss: 1.96160381e-05
Iter: 33 loss: 2.07099638e-05
Iter: 34 loss: 1.95153134e-05
Iter: 35 loss: 1.92426087e-05
Iter: 36 loss: 1.92669249e-05
Iter: 37 loss: 1.90317514e-05
Iter: 38 loss: 1.87451224e-05
Iter: 39 loss: 2.0698255e-05
Iter: 40 loss: 1.87162696e-05
Iter: 41 loss: 1.89782259e-05
Iter: 42 loss: 1.86267898e-05
Iter: 43 loss: 1.85927565e-05
Iter: 44 loss: 1.84759883e-05
Iter: 45 loss: 1.84267774e-05
Iter: 46 loss: 1.83390694e-05
Iter: 47 loss: 1.81357755e-05
Iter: 48 loss: 1.85049867e-05
Iter: 49 loss: 1.8047569e-05
Iter: 50 loss: 1.77923976e-05
Iter: 51 loss: 1.82838594e-05
Iter: 52 loss: 1.76854046e-05
Iter: 53 loss: 1.75214391e-05
Iter: 54 loss: 1.75160712e-05
Iter: 55 loss: 1.74201086e-05
Iter: 56 loss: 1.7377879e-05
Iter: 57 loss: 1.73285353e-05
Iter: 58 loss: 1.7184484e-05
Iter: 59 loss: 1.72769214e-05
Iter: 60 loss: 1.70933818e-05
Iter: 61 loss: 1.6956772e-05
Iter: 62 loss: 1.85209847e-05
Iter: 63 loss: 1.69544837e-05
Iter: 64 loss: 1.68686929e-05
Iter: 65 loss: 1.6853679e-05
Iter: 66 loss: 1.6794831e-05
Iter: 67 loss: 1.66841255e-05
Iter: 68 loss: 1.69565574e-05
Iter: 69 loss: 1.66443133e-05
Iter: 70 loss: 1.65345518e-05
Iter: 71 loss: 1.71050451e-05
Iter: 72 loss: 1.65171623e-05
Iter: 73 loss: 1.69619816e-05
Iter: 74 loss: 1.65041019e-05
Iter: 75 loss: 1.64944e-05
Iter: 76 loss: 1.64621542e-05
Iter: 77 loss: 1.64966659e-05
Iter: 78 loss: 1.64376761e-05
Iter: 79 loss: 1.63984405e-05
Iter: 80 loss: 1.63009536e-05
Iter: 81 loss: 1.71908359e-05
Iter: 82 loss: 1.62866927e-05
Iter: 83 loss: 1.61508797e-05
Iter: 84 loss: 1.80051684e-05
Iter: 85 loss: 1.61502103e-05
Iter: 86 loss: 1.60959862e-05
Iter: 87 loss: 1.60954332e-05
Iter: 88 loss: 1.60471754e-05
Iter: 89 loss: 1.60721793e-05
Iter: 90 loss: 1.60153977e-05
Iter: 91 loss: 1.59529445e-05
Iter: 92 loss: 1.59676383e-05
Iter: 93 loss: 1.59076371e-05
Iter: 94 loss: 1.58300136e-05
Iter: 95 loss: 1.63560326e-05
Iter: 96 loss: 1.58222792e-05
Iter: 97 loss: 1.5767e-05
Iter: 98 loss: 1.5906684e-05
Iter: 99 loss: 1.57478244e-05
Iter: 100 loss: 1.56792084e-05
Iter: 101 loss: 1.57665963e-05
Iter: 102 loss: 1.56446804e-05
Iter: 103 loss: 1.55974158e-05
Iter: 104 loss: 1.61008738e-05
Iter: 105 loss: 1.55957205e-05
Iter: 106 loss: 1.55826892e-05
Iter: 107 loss: 1.5706366e-05
Iter: 108 loss: 1.55820599e-05
Iter: 109 loss: 1.55604212e-05
Iter: 110 loss: 1.55160033e-05
Iter: 111 loss: 1.63026834e-05
Iter: 112 loss: 1.55153575e-05
Iter: 113 loss: 1.54874742e-05
Iter: 114 loss: 1.54226655e-05
Iter: 115 loss: 1.62321303e-05
Iter: 116 loss: 1.54177324e-05
Iter: 117 loss: 1.53773763e-05
Iter: 118 loss: 1.54678473e-05
Iter: 119 loss: 1.53623841e-05
Iter: 120 loss: 1.5313246e-05
Iter: 121 loss: 1.55133421e-05
Iter: 122 loss: 1.53025e-05
Iter: 123 loss: 1.52570483e-05
Iter: 124 loss: 1.53127185e-05
Iter: 125 loss: 1.52327784e-05
Iter: 126 loss: 1.51737322e-05
Iter: 127 loss: 1.55697635e-05
Iter: 128 loss: 1.51675722e-05
Iter: 129 loss: 1.51371914e-05
Iter: 130 loss: 1.52041985e-05
Iter: 131 loss: 1.51254881e-05
Iter: 132 loss: 1.50885926e-05
Iter: 133 loss: 1.51127797e-05
Iter: 134 loss: 1.50650212e-05
Iter: 135 loss: 1.50176047e-05
Iter: 136 loss: 1.51239301e-05
Iter: 137 loss: 1.49999378e-05
Iter: 138 loss: 1.4964422e-05
Iter: 139 loss: 1.52937719e-05
Iter: 140 loss: 1.49628095e-05
Iter: 141 loss: 1.4938948e-05
Iter: 142 loss: 1.49921707e-05
Iter: 143 loss: 1.49299067e-05
Iter: 144 loss: 1.49537555e-05
Iter: 145 loss: 1.49196776e-05
Iter: 146 loss: 1.49157313e-05
Iter: 147 loss: 1.49024909e-05
Iter: 148 loss: 1.48574427e-05
Iter: 149 loss: 1.51627582e-05
Iter: 150 loss: 1.4846366e-05
Iter: 151 loss: 1.47993223e-05
Iter: 152 loss: 1.54349436e-05
Iter: 153 loss: 1.47993396e-05
Iter: 154 loss: 1.47731971e-05
Iter: 155 loss: 1.47543e-05
Iter: 156 loss: 1.47448627e-05
Iter: 157 loss: 1.47023247e-05
Iter: 158 loss: 1.50097185e-05
Iter: 159 loss: 1.46988041e-05
Iter: 160 loss: 1.46738575e-05
Iter: 161 loss: 1.46815528e-05
Iter: 162 loss: 1.4655071e-05
Iter: 163 loss: 1.4628301e-05
Iter: 164 loss: 1.47876763e-05
Iter: 165 loss: 1.46242619e-05
Iter: 166 loss: 1.46012972e-05
Iter: 167 loss: 1.46241136e-05
Iter: 168 loss: 1.45882386e-05
Iter: 169 loss: 1.45632839e-05
Iter: 170 loss: 1.45895492e-05
Iter: 171 loss: 1.45492932e-05
Iter: 172 loss: 1.45271106e-05
Iter: 173 loss: 1.45512195e-05
Iter: 174 loss: 1.45145896e-05
Iter: 175 loss: 1.44851483e-05
Iter: 176 loss: 1.44528585e-05
Iter: 177 loss: 1.44482456e-05
Iter: 178 loss: 1.44242258e-05
Iter: 179 loss: 1.44227834e-05
Iter: 180 loss: 1.44177266e-05
Iter: 181 loss: 1.44103724e-05
Iter: 182 loss: 1.44082387e-05
Iter: 183 loss: 1.4399302e-05
Iter: 184 loss: 1.4372391e-05
Iter: 185 loss: 1.46882394e-05
Iter: 186 loss: 1.43702409e-05
Iter: 187 loss: 1.43356465e-05
Iter: 188 loss: 1.44433689e-05
Iter: 189 loss: 1.43258712e-05
Iter: 190 loss: 1.42935796e-05
Iter: 191 loss: 1.44292317e-05
Iter: 192 loss: 1.42868066e-05
Iter: 193 loss: 1.42625358e-05
Iter: 194 loss: 1.43872958e-05
Iter: 195 loss: 1.4258665e-05
Iter: 196 loss: 1.42374256e-05
Iter: 197 loss: 1.42537283e-05
Iter: 198 loss: 1.42245608e-05
Iter: 199 loss: 1.41962319e-05
Iter: 200 loss: 1.42891031e-05
Iter: 201 loss: 1.41884084e-05
Iter: 202 loss: 1.41611508e-05
Iter: 203 loss: 1.42267891e-05
Iter: 204 loss: 1.41511482e-05
Iter: 205 loss: 1.41265327e-05
Iter: 206 loss: 1.42834951e-05
Iter: 207 loss: 1.41238388e-05
Iter: 208 loss: 1.41033634e-05
Iter: 209 loss: 1.41046912e-05
Iter: 210 loss: 1.40873362e-05
Iter: 211 loss: 1.40578677e-05
Iter: 212 loss: 1.41929786e-05
Iter: 213 loss: 1.4051885e-05
Iter: 214 loss: 1.41508435e-05
Iter: 215 loss: 1.4048579e-05
Iter: 216 loss: 1.40466127e-05
Iter: 217 loss: 1.40382008e-05
Iter: 218 loss: 1.40225675e-05
Iter: 219 loss: 1.40223055e-05
Iter: 220 loss: 1.40057173e-05
Iter: 221 loss: 1.40241846e-05
Iter: 222 loss: 1.39967178e-05
Iter: 223 loss: 1.39764743e-05
Iter: 224 loss: 1.40244347e-05
Iter: 225 loss: 1.39681815e-05
Iter: 226 loss: 1.39467429e-05
Iter: 227 loss: 1.40202246e-05
Iter: 228 loss: 1.39408603e-05
Iter: 229 loss: 1.39213853e-05
Iter: 230 loss: 1.39569329e-05
Iter: 231 loss: 1.3912656e-05
Iter: 232 loss: 1.38923488e-05
Iter: 233 loss: 1.39269632e-05
Iter: 234 loss: 1.38834812e-05
Iter: 235 loss: 1.38564365e-05
Iter: 236 loss: 1.39553504e-05
Iter: 237 loss: 1.3849598e-05
Iter: 238 loss: 1.38343312e-05
Iter: 239 loss: 1.39215681e-05
Iter: 240 loss: 1.38321302e-05
Iter: 241 loss: 1.38149362e-05
Iter: 242 loss: 1.38185742e-05
Iter: 243 loss: 1.3801804e-05
Iter: 244 loss: 1.37783245e-05
Iter: 245 loss: 1.38699161e-05
Iter: 246 loss: 1.37726e-05
Iter: 247 loss: 1.38070081e-05
Iter: 248 loss: 1.37691604e-05
Iter: 249 loss: 1.3765316e-05
Iter: 250 loss: 1.37545376e-05
Iter: 251 loss: 1.38187224e-05
Iter: 252 loss: 1.37513398e-05
Iter: 253 loss: 1.37443349e-05
Iter: 254 loss: 1.37255192e-05
Iter: 255 loss: 1.38524665e-05
Iter: 256 loss: 1.37207207e-05
Iter: 257 loss: 1.36944409e-05
Iter: 258 loss: 1.37871111e-05
Iter: 259 loss: 1.36878207e-05
Iter: 260 loss: 1.36622748e-05
Iter: 261 loss: 1.38341038e-05
Iter: 262 loss: 1.36595409e-05
Iter: 263 loss: 1.36445069e-05
Iter: 264 loss: 1.36863864e-05
Iter: 265 loss: 1.36399212e-05
Iter: 266 loss: 1.36218932e-05
Iter: 267 loss: 1.36166955e-05
Iter: 268 loss: 1.36066101e-05
Iter: 269 loss: 1.35827268e-05
Iter: 270 loss: 1.37673687e-05
Iter: 271 loss: 1.3580905e-05
Iter: 272 loss: 1.35623977e-05
Iter: 273 loss: 1.35993432e-05
Iter: 274 loss: 1.35542341e-05
Iter: 275 loss: 1.35376613e-05
Iter: 276 loss: 1.36352237e-05
Iter: 277 loss: 1.3535765e-05
Iter: 278 loss: 1.35193859e-05
Iter: 279 loss: 1.3523153e-05
Iter: 280 loss: 1.35075106e-05
Iter: 281 loss: 1.35105947e-05
Iter: 282 loss: 1.35015498e-05
Iter: 283 loss: 1.34927523e-05
Iter: 284 loss: 1.34899437e-05
Iter: 285 loss: 1.34846e-05
Iter: 286 loss: 1.34796064e-05
Iter: 287 loss: 1.3462768e-05
Iter: 288 loss: 1.34872098e-05
Iter: 289 loss: 1.34509264e-05
Iter: 290 loss: 1.3431325e-05
Iter: 291 loss: 1.35694008e-05
Iter: 292 loss: 1.34296206e-05
Iter: 293 loss: 1.34135871e-05
Iter: 294 loss: 1.35314704e-05
Iter: 295 loss: 1.34117799e-05
Iter: 296 loss: 1.33985441e-05
Iter: 297 loss: 1.34194497e-05
Iter: 298 loss: 1.33923904e-05
Iter: 299 loss: 1.33783751e-05
Iter: 300 loss: 1.33776075e-05
Iter: 301 loss: 1.33672856e-05
Iter: 302 loss: 1.33482281e-05
Iter: 303 loss: 1.348982e-05
Iter: 304 loss: 1.33464164e-05
Iter: 305 loss: 1.3330804e-05
Iter: 306 loss: 1.33618887e-05
Iter: 307 loss: 1.33243875e-05
Iter: 308 loss: 1.33093981e-05
Iter: 309 loss: 1.33456124e-05
Iter: 310 loss: 1.33042213e-05
Iter: 311 loss: 1.32867517e-05
Iter: 312 loss: 1.33729163e-05
Iter: 313 loss: 1.32835175e-05
Iter: 314 loss: 1.32800469e-05
Iter: 315 loss: 1.32773166e-05
Iter: 316 loss: 1.32697678e-05
Iter: 317 loss: 1.32841933e-05
Iter: 318 loss: 1.32663463e-05
Iter: 319 loss: 1.32623245e-05
Iter: 320 loss: 1.32509749e-05
Iter: 321 loss: 1.33027752e-05
Iter: 322 loss: 1.32469959e-05
Iter: 323 loss: 1.32353643e-05
Iter: 324 loss: 1.32214936e-05
Iter: 325 loss: 1.3220264e-05
Iter: 326 loss: 1.31993484e-05
Iter: 327 loss: 1.33420926e-05
Iter: 328 loss: 1.3197242e-05
Iter: 329 loss: 1.31818197e-05
Iter: 330 loss: 1.32491132e-05
Iter: 331 loss: 1.31782117e-05
Iter: 332 loss: 1.31638963e-05
Iter: 333 loss: 1.31854931e-05
Iter: 334 loss: 1.31564066e-05
Iter: 335 loss: 1.31386896e-05
Iter: 336 loss: 1.31930292e-05
Iter: 337 loss: 1.31335692e-05
Iter: 338 loss: 1.3118326e-05
Iter: 339 loss: 1.31863371e-05
Iter: 340 loss: 1.31153211e-05
Iter: 341 loss: 1.31032411e-05
Iter: 342 loss: 1.31253109e-05
Iter: 343 loss: 1.30981025e-05
Iter: 344 loss: 1.30827284e-05
Iter: 345 loss: 1.30935023e-05
Iter: 346 loss: 1.30728022e-05
Iter: 347 loss: 1.30732142e-05
Iter: 348 loss: 1.30647195e-05
Iter: 349 loss: 1.30571934e-05
Iter: 350 loss: 1.31231391e-05
Iter: 351 loss: 1.3056806e-05
Iter: 352 loss: 1.30538619e-05
Iter: 353 loss: 1.30452536e-05
Iter: 354 loss: 1.30525668e-05
Iter: 355 loss: 1.30377139e-05
Iter: 356 loss: 1.30234221e-05
Iter: 357 loss: 1.3022609e-05
Iter: 358 loss: 1.30113385e-05
Iter: 359 loss: 1.29940781e-05
Iter: 360 loss: 1.30572544e-05
Iter: 361 loss: 1.29902483e-05
Iter: 362 loss: 1.29728287e-05
Iter: 363 loss: 1.30141707e-05
Iter: 364 loss: 1.29667606e-05
Iter: 365 loss: 1.29524142e-05
Iter: 366 loss: 1.3059901e-05
Iter: 367 loss: 1.29512828e-05
Iter: 368 loss: 1.29401451e-05
Iter: 369 loss: 1.293477e-05
Iter: 370 loss: 1.29294567e-05
Iter: 371 loss: 1.29135515e-05
Iter: 372 loss: 1.30536018e-05
Iter: 373 loss: 1.29123055e-05
Iter: 374 loss: 1.28986594e-05
Iter: 375 loss: 1.29163864e-05
Iter: 376 loss: 1.28915526e-05
Iter: 377 loss: 1.28753036e-05
Iter: 378 loss: 1.2870998e-05
Iter: 379 loss: 1.28606016e-05
Iter: 380 loss: 1.28720621e-05
Iter: 381 loss: 1.28540451e-05
Iter: 382 loss: 1.28488118e-05
Iter: 383 loss: 1.28488264e-05
Iter: 384 loss: 1.284686e-05
Iter: 385 loss: 1.28400943e-05
Iter: 386 loss: 1.28318188e-05
Iter: 387 loss: 1.28295778e-05
Iter: 388 loss: 1.2815467e-05
Iter: 389 loss: 1.28644233e-05
Iter: 390 loss: 1.28120646e-05
Iter: 391 loss: 1.28003876e-05
Iter: 392 loss: 1.27912372e-05
Iter: 393 loss: 1.27878147e-05
Iter: 394 loss: 1.27709245e-05
Iter: 395 loss: 1.2933906e-05
Iter: 396 loss: 1.27703224e-05
Iter: 397 loss: 1.27595322e-05
Iter: 398 loss: 1.27792782e-05
Iter: 399 loss: 1.27549756e-05
Iter: 400 loss: 1.27379562e-05
Iter: 401 loss: 1.27162e-05
Iter: 402 loss: 1.27145022e-05
Iter: 403 loss: 1.27010589e-05
Iter: 404 loss: 1.26994037e-05
Iter: 405 loss: 1.26888808e-05
Iter: 406 loss: 1.27016665e-05
Iter: 407 loss: 1.2683251e-05
Iter: 408 loss: 1.26694194e-05
Iter: 409 loss: 1.26738505e-05
Iter: 410 loss: 1.26596688e-05
Iter: 411 loss: 1.26462455e-05
Iter: 412 loss: 1.28430993e-05
Iter: 413 loss: 1.26461018e-05
Iter: 414 loss: 1.26486066e-05
Iter: 415 loss: 1.26410569e-05
Iter: 416 loss: 1.26390105e-05
Iter: 417 loss: 1.2631268e-05
Iter: 418 loss: 1.26324358e-05
Iter: 419 loss: 1.26237173e-05
Iter: 420 loss: 1.26143e-05
Iter: 421 loss: 1.26230643e-05
Iter: 422 loss: 1.26086743e-05
Iter: 423 loss: 1.25956103e-05
Iter: 424 loss: 1.25815313e-05
Iter: 425 loss: 1.2579575e-05
Iter: 426 loss: 1.25654451e-05
Iter: 427 loss: 1.25654633e-05
Iter: 428 loss: 1.25546485e-05
Iter: 429 loss: 1.25545348e-05
Iter: 430 loss: 1.25461966e-05
Iter: 431 loss: 1.25298529e-05
Iter: 432 loss: 1.25652623e-05
Iter: 433 loss: 1.25242204e-05
Iter: 434 loss: 1.2509694e-05
Iter: 435 loss: 1.25844699e-05
Iter: 436 loss: 1.25075567e-05
Iter: 437 loss: 1.24949074e-05
Iter: 438 loss: 1.25461311e-05
Iter: 439 loss: 1.24922317e-05
Iter: 440 loss: 1.24810158e-05
Iter: 441 loss: 1.2489174e-05
Iter: 442 loss: 1.24741546e-05
Iter: 443 loss: 1.24564976e-05
Iter: 444 loss: 1.24814014e-05
Iter: 445 loss: 1.24479075e-05
Iter: 446 loss: 1.24838789e-05
Iter: 447 loss: 1.24448688e-05
Iter: 448 loss: 1.244132e-05
Iter: 449 loss: 1.24319986e-05
Iter: 450 loss: 1.25085598e-05
Iter: 451 loss: 1.2429733e-05
Iter: 452 loss: 1.24241105e-05
Iter: 453 loss: 1.24145008e-05
Iter: 454 loss: 1.24143016e-05
Iter: 455 loss: 1.24015569e-05
Iter: 456 loss: 1.24386988e-05
Iter: 457 loss: 1.23978098e-05
Iter: 458 loss: 1.23874815e-05
Iter: 459 loss: 1.2381799e-05
Iter: 460 loss: 1.23770651e-05
Iter: 461 loss: 1.23633017e-05
Iter: 462 loss: 1.24884755e-05
Iter: 463 loss: 1.23627924e-05
Iter: 464 loss: 1.23514856e-05
Iter: 465 loss: 1.23480713e-05
Iter: 466 loss: 1.23414056e-05
Iter: 467 loss: 1.23251684e-05
Iter: 468 loss: 1.23704585e-05
Iter: 469 loss: 1.23196842e-05
Iter: 470 loss: 1.23089094e-05
Iter: 471 loss: 1.23436093e-05
Iter: 472 loss: 1.230609e-05
Iter: 473 loss: 1.22936035e-05
Iter: 474 loss: 1.22856618e-05
Iter: 475 loss: 1.22805959e-05
Iter: 476 loss: 1.22666333e-05
Iter: 477 loss: 1.22665106e-05
Iter: 478 loss: 1.22581141e-05
Iter: 479 loss: 1.22530073e-05
Iter: 480 loss: 1.22495312e-05
Iter: 481 loss: 1.22829788e-05
Iter: 482 loss: 1.22458059e-05
Iter: 483 loss: 1.22449137e-05
Iter: 484 loss: 1.22409947e-05
Iter: 485 loss: 1.22320089e-05
Iter: 486 loss: 1.22323954e-05
Iter: 487 loss: 1.22213296e-05
Iter: 488 loss: 1.22289166e-05
Iter: 489 loss: 1.22148904e-05
Iter: 490 loss: 1.22057063e-05
Iter: 491 loss: 1.22058345e-05
Iter: 492 loss: 1.21986141e-05
Iter: 493 loss: 1.21898047e-05
Iter: 494 loss: 1.21890025e-05
Iter: 495 loss: 1.21730845e-05
Iter: 496 loss: 1.21883986e-05
Iter: 497 loss: 1.21638968e-05
Iter: 498 loss: 1.21508874e-05
Iter: 499 loss: 1.23215978e-05
Iter: 500 loss: 1.21510529e-05
Iter: 501 loss: 1.21415032e-05
Iter: 502 loss: 1.21353241e-05
Iter: 503 loss: 1.21316443e-05
Iter: 504 loss: 1.2117167e-05
Iter: 505 loss: 1.21549383e-05
Iter: 506 loss: 1.21120338e-05
Iter: 507 loss: 1.20976883e-05
Iter: 508 loss: 1.21867506e-05
Iter: 509 loss: 1.20959185e-05
Iter: 510 loss: 1.20862896e-05
Iter: 511 loss: 1.2105962e-05
Iter: 512 loss: 1.20820441e-05
Iter: 513 loss: 1.20697841e-05
Iter: 514 loss: 1.20889681e-05
Iter: 515 loss: 1.20641935e-05
Iter: 516 loss: 1.20862151e-05
Iter: 517 loss: 1.20595887e-05
Iter: 518 loss: 1.20585119e-05
Iter: 519 loss: 1.20540262e-05
Iter: 520 loss: 1.20446675e-05
Iter: 521 loss: 1.22838319e-05
Iter: 522 loss: 1.20444038e-05
Iter: 523 loss: 1.20338409e-05
Iter: 524 loss: 1.20485383e-05
Iter: 525 loss: 1.2028403e-05
Iter: 526 loss: 1.20191389e-05
Iter: 527 loss: 1.20817331e-05
Iter: 528 loss: 1.20178956e-05
Iter: 529 loss: 1.20083987e-05
Iter: 530 loss: 1.201143e-05
Iter: 531 loss: 1.20012228e-05
Iter: 532 loss: 1.19890265e-05
Iter: 533 loss: 1.20115019e-05
Iter: 534 loss: 1.19832184e-05
Iter: 535 loss: 1.19719189e-05
Iter: 536 loss: 1.20079967e-05
Iter: 537 loss: 1.19687247e-05
Iter: 538 loss: 1.19549622e-05
Iter: 539 loss: 1.19833521e-05
Iter: 540 loss: 1.19493106e-05
Iter: 541 loss: 1.19407514e-05
Iter: 542 loss: 1.2021399e-05
Iter: 543 loss: 1.19404649e-05
Iter: 544 loss: 1.19329861e-05
Iter: 545 loss: 1.19317119e-05
Iter: 546 loss: 1.19264632e-05
Iter: 547 loss: 1.19155357e-05
Iter: 548 loss: 1.19575543e-05
Iter: 549 loss: 1.19131018e-05
Iter: 550 loss: 1.19492652e-05
Iter: 551 loss: 1.19110027e-05
Iter: 552 loss: 1.19091774e-05
Iter: 553 loss: 1.1903654e-05
Iter: 554 loss: 1.18960679e-05
Iter: 555 loss: 1.18944645e-05
Iter: 556 loss: 1.18859734e-05
Iter: 557 loss: 1.18947301e-05
Iter: 558 loss: 1.18810322e-05
Iter: 559 loss: 1.18729149e-05
Iter: 560 loss: 1.18913358e-05
Iter: 561 loss: 1.18698717e-05
Iter: 562 loss: 1.18584721e-05
Iter: 563 loss: 1.18738371e-05
Iter: 564 loss: 1.18531498e-05
Iter: 565 loss: 1.18398984e-05
Iter: 566 loss: 1.19001197e-05
Iter: 567 loss: 1.18373791e-05
Iter: 568 loss: 1.18296557e-05
Iter: 569 loss: 1.18284897e-05
Iter: 570 loss: 1.18233538e-05
Iter: 571 loss: 1.18108519e-05
Iter: 572 loss: 1.18787484e-05
Iter: 573 loss: 1.18089711e-05
Iter: 574 loss: 1.18004973e-05
Iter: 575 loss: 1.1841712e-05
Iter: 576 loss: 1.17992968e-05
Iter: 577 loss: 1.17910386e-05
Iter: 578 loss: 1.17897071e-05
Iter: 579 loss: 1.17841555e-05
Iter: 580 loss: 1.17734098e-05
Iter: 581 loss: 1.18191319e-05
Iter: 582 loss: 1.17711352e-05
Iter: 583 loss: 1.17917707e-05
Iter: 584 loss: 1.1768996e-05
Iter: 585 loss: 1.17665186e-05
Iter: 586 loss: 1.17596619e-05
Iter: 587 loss: 1.18051421e-05
Iter: 588 loss: 1.17580212e-05
Iter: 589 loss: 1.17544278e-05
Iter: 590 loss: 1.1745793e-05
Iter: 591 loss: 1.18609678e-05
Iter: 592 loss: 1.17450509e-05
Iter: 593 loss: 1.17327318e-05
Iter: 594 loss: 1.1772695e-05
Iter: 595 loss: 1.17293912e-05
Iter: 596 loss: 1.17180243e-05
Iter: 597 loss: 1.17610762e-05
Iter: 598 loss: 1.17152886e-05
Iter: 599 loss: 1.170423e-05
Iter: 600 loss: 1.17517829e-05
Iter: 601 loss: 1.17022755e-05
Iter: 602 loss: 1.16934643e-05
Iter: 603 loss: 1.1711857e-05
Iter: 604 loss: 1.16901574e-05
Iter: 605 loss: 1.16802839e-05
Iter: 606 loss: 1.16662604e-05
Iter: 607 loss: 1.16652463e-05
Iter: 608 loss: 1.16515312e-05
Iter: 609 loss: 1.16513229e-05
Iter: 610 loss: 1.16402716e-05
Iter: 611 loss: 1.1641243e-05
Iter: 612 loss: 1.16316269e-05
Iter: 613 loss: 1.16162264e-05
Iter: 614 loss: 1.16726569e-05
Iter: 615 loss: 1.16128458e-05
Iter: 616 loss: 1.16046631e-05
Iter: 617 loss: 1.16045703e-05
Iter: 618 loss: 1.15937701e-05
Iter: 619 loss: 1.1620803e-05
Iter: 620 loss: 1.15898156e-05
Iter: 621 loss: 1.15859957e-05
Iter: 622 loss: 1.15744551e-05
Iter: 623 loss: 1.1616351e-05
Iter: 624 loss: 1.1569573e-05
Iter: 625 loss: 1.15589837e-05
Iter: 626 loss: 1.15534494e-05
Iter: 627 loss: 1.15484363e-05
Iter: 628 loss: 1.15324274e-05
Iter: 629 loss: 1.17071231e-05
Iter: 630 loss: 1.15320563e-05
Iter: 631 loss: 1.15227385e-05
Iter: 632 loss: 1.15794701e-05
Iter: 633 loss: 1.15216972e-05
Iter: 634 loss: 1.15121929e-05
Iter: 635 loss: 1.15100293e-05
Iter: 636 loss: 1.15037919e-05
Iter: 637 loss: 1.14889335e-05
Iter: 638 loss: 1.15238345e-05
Iter: 639 loss: 1.14835539e-05
Iter: 640 loss: 1.14723243e-05
Iter: 641 loss: 1.15337898e-05
Iter: 642 loss: 1.14703489e-05
Iter: 643 loss: 1.1460409e-05
Iter: 644 loss: 1.14736586e-05
Iter: 645 loss: 1.14551594e-05
Iter: 646 loss: 1.14442246e-05
Iter: 647 loss: 1.14984787e-05
Iter: 648 loss: 1.14423583e-05
Iter: 649 loss: 1.14333598e-05
Iter: 650 loss: 1.14520089e-05
Iter: 651 loss: 1.14291961e-05
Iter: 652 loss: 1.14303548e-05
Iter: 653 loss: 1.14237209e-05
Iter: 654 loss: 1.14222803e-05
Iter: 655 loss: 1.14179766e-05
Iter: 656 loss: 1.1409692e-05
Iter: 657 loss: 1.140941e-05
Iter: 658 loss: 1.1398025e-05
Iter: 659 loss: 1.14064551e-05
Iter: 660 loss: 1.13906644e-05
Iter: 661 loss: 1.13791775e-05
Iter: 662 loss: 1.1412385e-05
Iter: 663 loss: 1.13752722e-05
Iter: 664 loss: 1.13631586e-05
Iter: 665 loss: 1.14148188e-05
Iter: 666 loss: 1.13604419e-05
Iter: 667 loss: 1.13493124e-05
Iter: 668 loss: 1.1413249e-05
Iter: 669 loss: 1.13480583e-05
Iter: 670 loss: 1.13413116e-05
Iter: 671 loss: 1.13612377e-05
Iter: 672 loss: 1.13394226e-05
Iter: 673 loss: 1.13323777e-05
Iter: 674 loss: 1.13292599e-05
Iter: 675 loss: 1.1325963e-05
Iter: 676 loss: 1.1316e-05
Iter: 677 loss: 1.13667484e-05
Iter: 678 loss: 1.13142833e-05
Iter: 679 loss: 1.13054812e-05
Iter: 680 loss: 1.13198357e-05
Iter: 681 loss: 1.13009683e-05
Iter: 682 loss: 1.12914622e-05
Iter: 683 loss: 1.13392689e-05
Iter: 684 loss: 1.12897733e-05
Iter: 685 loss: 1.13088654e-05
Iter: 686 loss: 1.12879206e-05
Iter: 687 loss: 1.12867374e-05
Iter: 688 loss: 1.12833095e-05
Iter: 689 loss: 1.12735161e-05
Iter: 690 loss: 1.14484465e-05
Iter: 691 loss: 1.1273326e-05
Iter: 692 loss: 1.12634343e-05
Iter: 693 loss: 1.13117931e-05
Iter: 694 loss: 1.12615126e-05
Iter: 695 loss: 1.12530333e-05
Iter: 696 loss: 1.12414618e-05
Iter: 697 loss: 1.12406487e-05
Iter: 698 loss: 1.12331336e-05
Iter: 699 loss: 1.12323305e-05
Iter: 700 loss: 1.12265134e-05
Iter: 701 loss: 1.12281541e-05
Iter: 702 loss: 1.12225134e-05
Iter: 703 loss: 1.12137895e-05
Iter: 704 loss: 1.12396019e-05
Iter: 705 loss: 1.12112302e-05
Iter: 706 loss: 1.12021862e-05
Iter: 707 loss: 1.12010257e-05
Iter: 708 loss: 1.11945637e-05
Iter: 709 loss: 1.11834397e-05
Iter: 710 loss: 1.12056878e-05
Iter: 711 loss: 1.11791915e-05
Iter: 712 loss: 1.11710688e-05
Iter: 713 loss: 1.12079888e-05
Iter: 714 loss: 1.11696809e-05
Iter: 715 loss: 1.11612144e-05
Iter: 716 loss: 1.11690188e-05
Iter: 717 loss: 1.11568625e-05
Iter: 718 loss: 1.12085536e-05
Iter: 719 loss: 1.11555928e-05
Iter: 720 loss: 1.11541522e-05
Iter: 721 loss: 1.11495901e-05
Iter: 722 loss: 1.11540294e-05
Iter: 723 loss: 1.11457884e-05
Iter: 724 loss: 1.11394347e-05
Iter: 725 loss: 1.11372974e-05
Iter: 726 loss: 1.11328909e-05
Iter: 727 loss: 1.11254358e-05
Iter: 728 loss: 1.11366071e-05
Iter: 729 loss: 1.11215759e-05
Iter: 730 loss: 1.11106401e-05
Iter: 731 loss: 1.11235522e-05
Iter: 732 loss: 1.11047102e-05
Iter: 733 loss: 1.10949713e-05
Iter: 734 loss: 1.11514782e-05
Iter: 735 loss: 1.10941455e-05
Iter: 736 loss: 1.10856781e-05
Iter: 737 loss: 1.11044028e-05
Iter: 738 loss: 1.10823757e-05
Iter: 739 loss: 1.10715864e-05
Iter: 740 loss: 1.10700421e-05
Iter: 741 loss: 1.10623114e-05
Iter: 742 loss: 1.10543378e-05
Iter: 743 loss: 1.10543742e-05
Iter: 744 loss: 1.10480842e-05
Iter: 745 loss: 1.10554429e-05
Iter: 746 loss: 1.10451792e-05
Iter: 747 loss: 1.1036087e-05
Iter: 748 loss: 1.10322089e-05
Iter: 749 loss: 1.10276505e-05
Iter: 750 loss: 1.10208839e-05
Iter: 751 loss: 1.10203182e-05
Iter: 752 loss: 1.10106512e-05
Iter: 753 loss: 1.10482324e-05
Iter: 754 loss: 1.10085912e-05
Iter: 755 loss: 1.10067103e-05
Iter: 756 loss: 1.10007541e-05
Iter: 757 loss: 1.1014361e-05
Iter: 758 loss: 1.0997006e-05
Iter: 759 loss: 1.09872326e-05
Iter: 760 loss: 1.09869316e-05
Iter: 761 loss: 1.09791399e-05
Iter: 762 loss: 1.09692855e-05
Iter: 763 loss: 1.10977435e-05
Iter: 764 loss: 1.09690909e-05
Iter: 765 loss: 1.09622861e-05
Iter: 766 loss: 1.09559414e-05
Iter: 767 loss: 1.09541888e-05
Iter: 768 loss: 1.09429402e-05
Iter: 769 loss: 1.09918592e-05
Iter: 770 loss: 1.09406665e-05
Iter: 771 loss: 1.09279927e-05
Iter: 772 loss: 1.09543962e-05
Iter: 773 loss: 1.09227421e-05
Iter: 774 loss: 1.09127386e-05
Iter: 775 loss: 1.09894436e-05
Iter: 776 loss: 1.09119819e-05
Iter: 777 loss: 1.09040284e-05
Iter: 778 loss: 1.09385419e-05
Iter: 779 loss: 1.09023476e-05
Iter: 780 loss: 1.08938666e-05
Iter: 781 loss: 1.08995109e-05
Iter: 782 loss: 1.08883787e-05
Iter: 783 loss: 1.0879512e-05
Iter: 784 loss: 1.09448674e-05
Iter: 785 loss: 1.08786717e-05
Iter: 786 loss: 1.08775876e-05
Iter: 787 loss: 1.08741533e-05
Iter: 788 loss: 1.08729018e-05
Iter: 789 loss: 1.0868891e-05
Iter: 790 loss: 1.08599961e-05
Iter: 791 loss: 1.10767114e-05
Iter: 792 loss: 1.08600179e-05
Iter: 793 loss: 1.08512277e-05
Iter: 794 loss: 1.09094408e-05
Iter: 795 loss: 1.08501663e-05
Iter: 796 loss: 1.08425029e-05
Iter: 797 loss: 1.0838472e-05
Iter: 798 loss: 1.08346112e-05
Iter: 799 loss: 1.08206696e-05
Iter: 800 loss: 1.09223947e-05
Iter: 801 loss: 1.08193526e-05
Iter: 802 loss: 1.08122949e-05
Iter: 803 loss: 1.08166605e-05
Iter: 804 loss: 1.08079548e-05
Iter: 805 loss: 1.07975957e-05
Iter: 806 loss: 1.08301556e-05
Iter: 807 loss: 1.07943342e-05
Iter: 808 loss: 1.07848473e-05
Iter: 809 loss: 1.08343647e-05
Iter: 810 loss: 1.07837986e-05
Iter: 811 loss: 1.07760243e-05
Iter: 812 loss: 1.07792366e-05
Iter: 813 loss: 1.07702317e-05
Iter: 814 loss: 1.07590377e-05
Iter: 815 loss: 1.07897049e-05
Iter: 816 loss: 1.07549267e-05
Iter: 817 loss: 1.07447249e-05
Iter: 818 loss: 1.08226513e-05
Iter: 819 loss: 1.07438209e-05
Iter: 820 loss: 1.07358601e-05
Iter: 821 loss: 1.07578308e-05
Iter: 822 loss: 1.07331134e-05
Iter: 823 loss: 1.07299757e-05
Iter: 824 loss: 1.07296419e-05
Iter: 825 loss: 1.07238884e-05
Iter: 826 loss: 1.07165251e-05
Iter: 827 loss: 1.07160122e-05
Iter: 828 loss: 1.0713532e-05
Iter: 829 loss: 1.07075e-05
Iter: 830 loss: 1.0767646e-05
Iter: 831 loss: 1.07068354e-05
Iter: 832 loss: 1.0699252e-05
Iter: 833 loss: 1.07041578e-05
Iter: 834 loss: 1.06946873e-05
Iter: 835 loss: 1.06844063e-05
Iter: 836 loss: 1.0724958e-05
Iter: 837 loss: 1.06823572e-05
Iter: 838 loss: 1.067415e-05
Iter: 839 loss: 1.07125788e-05
Iter: 840 loss: 1.06727966e-05
Iter: 841 loss: 1.06659882e-05
Iter: 842 loss: 1.06643402e-05
Iter: 843 loss: 1.06594589e-05
Iter: 844 loss: 1.0651409e-05
Iter: 845 loss: 1.07211927e-05
Iter: 846 loss: 1.06509742e-05
Iter: 847 loss: 1.06436537e-05
Iter: 848 loss: 1.06449443e-05
Iter: 849 loss: 1.06383268e-05
Iter: 850 loss: 1.0630315e-05
Iter: 851 loss: 1.06789576e-05
Iter: 852 loss: 1.06289899e-05
Iter: 853 loss: 1.06216376e-05
Iter: 854 loss: 1.06372418e-05
Iter: 855 loss: 1.0618588e-05
Iter: 856 loss: 1.06115094e-05
Iter: 857 loss: 1.06707048e-05
Iter: 858 loss: 1.06108055e-05
Iter: 859 loss: 1.06203188e-05
Iter: 860 loss: 1.06093985e-05
Iter: 861 loss: 1.06085654e-05
Iter: 862 loss: 1.06062071e-05
Iter: 863 loss: 1.05988529e-05
Iter: 864 loss: 1.06697353e-05
Iter: 865 loss: 1.05981071e-05
Iter: 866 loss: 1.05912859e-05
Iter: 867 loss: 1.06419384e-05
Iter: 868 loss: 1.0590973e-05
Iter: 869 loss: 1.05858207e-05
Iter: 870 loss: 1.05860008e-05
Iter: 871 loss: 1.05817689e-05
Iter: 872 loss: 1.05728523e-05
Iter: 873 loss: 1.05877662e-05
Iter: 874 loss: 1.05686822e-05
Iter: 875 loss: 1.05620047e-05
Iter: 876 loss: 1.06517145e-05
Iter: 877 loss: 1.05618819e-05
Iter: 878 loss: 1.05566878e-05
Iter: 879 loss: 1.05475119e-05
Iter: 880 loss: 1.05476374e-05
Iter: 881 loss: 1.05346717e-05
Iter: 882 loss: 1.06248908e-05
Iter: 883 loss: 1.05334e-05
Iter: 884 loss: 1.05265499e-05
Iter: 885 loss: 1.05403215e-05
Iter: 886 loss: 1.05237423e-05
Iter: 887 loss: 1.05144181e-05
Iter: 888 loss: 1.05211147e-05
Iter: 889 loss: 1.05086092e-05
Iter: 890 loss: 1.04972969e-05
Iter: 891 loss: 1.05518475e-05
Iter: 892 loss: 1.04951987e-05
Iter: 893 loss: 1.05222298e-05
Iter: 894 loss: 1.04938117e-05
Iter: 895 loss: 1.04924284e-05
Iter: 896 loss: 1.04885785e-05
Iter: 897 loss: 1.04990613e-05
Iter: 898 loss: 1.04868905e-05
Iter: 899 loss: 1.04828296e-05
Iter: 900 loss: 1.04739956e-05
Iter: 901 loss: 1.0585406e-05
Iter: 902 loss: 1.04732162e-05
Iter: 903 loss: 1.04610453e-05
Iter: 904 loss: 1.04983174e-05
Iter: 905 loss: 1.04571236e-05
Iter: 906 loss: 1.04479441e-05
Iter: 907 loss: 1.05044364e-05
Iter: 908 loss: 1.04469291e-05
Iter: 909 loss: 1.04396468e-05
Iter: 910 loss: 1.04374449e-05
Iter: 911 loss: 1.04326991e-05
Iter: 912 loss: 1.04197616e-05
Iter: 913 loss: 1.04885075e-05
Iter: 914 loss: 1.04176852e-05
Iter: 915 loss: 1.04105166e-05
Iter: 916 loss: 1.04323954e-05
Iter: 917 loss: 1.04085584e-05
Iter: 918 loss: 1.03996917e-05
Iter: 919 loss: 1.04107421e-05
Iter: 920 loss: 1.03950442e-05
Iter: 921 loss: 1.03873444e-05
Iter: 922 loss: 1.04460669e-05
Iter: 923 loss: 1.03865959e-05
Iter: 924 loss: 1.03806433e-05
Iter: 925 loss: 1.03794746e-05
Iter: 926 loss: 1.03758248e-05
Iter: 927 loss: 1.03984457e-05
Iter: 928 loss: 1.03743805e-05
Iter: 929 loss: 1.0372938e-05
Iter: 930 loss: 1.03684706e-05
Iter: 931 loss: 1.03970415e-05
Iter: 932 loss: 1.03675156e-05
Iter: 933 loss: 1.03645107e-05
Iter: 934 loss: 1.03601251e-05
Iter: 935 loss: 1.03597849e-05
Iter: 936 loss: 1.0352438e-05
Iter: 937 loss: 1.03511657e-05
Iter: 938 loss: 1.03456132e-05
Iter: 939 loss: 1.03363836e-05
Iter: 940 loss: 1.04025703e-05
Iter: 941 loss: 1.03354305e-05
Iter: 942 loss: 1.03284738e-05
Iter: 943 loss: 1.03251032e-05
Iter: 944 loss: 1.03219218e-05
Iter: 945 loss: 1.03108741e-05
Iter: 946 loss: 1.03552502e-05
Iter: 947 loss: 1.03084521e-05
Iter: 948 loss: 1.02992453e-05
Iter: 949 loss: 1.03560178e-05
Iter: 950 loss: 1.02977265e-05
Iter: 951 loss: 1.02907543e-05
Iter: 952 loss: 1.02956992e-05
Iter: 953 loss: 1.02868235e-05
Iter: 954 loss: 1.02753083e-05
Iter: 955 loss: 1.0290325e-05
Iter: 956 loss: 1.02696586e-05
Iter: 957 loss: 1.0261545e-05
Iter: 958 loss: 1.03790026e-05
Iter: 959 loss: 1.02614267e-05
Iter: 960 loss: 1.02579934e-05
Iter: 961 loss: 1.03001748e-05
Iter: 962 loss: 1.02577796e-05
Iter: 963 loss: 1.02532777e-05
Iter: 964 loss: 1.02772601e-05
Iter: 965 loss: 1.02524e-05
Iter: 966 loss: 1.02510676e-05
Iter: 967 loss: 1.02457216e-05
Iter: 968 loss: 1.02356535e-05
Iter: 969 loss: 1.02354625e-05
Iter: 970 loss: 1.02276035e-05
Iter: 971 loss: 1.02267595e-05
Iter: 972 loss: 1.02219801e-05
Iter: 973 loss: 1.02367867e-05
Iter: 974 loss: 1.02203776e-05
Iter: 975 loss: 1.02145332e-05
Iter: 976 loss: 1.02206886e-05
Iter: 977 loss: 1.02110698e-05
Iter: 978 loss: 1.02025988e-05
Iter: 979 loss: 1.01981077e-05
Iter: 980 loss: 1.01943933e-05
Iter: 981 loss: 1.01842252e-05
Iter: 982 loss: 1.03352932e-05
Iter: 983 loss: 1.01842734e-05
Iter: 984 loss: 1.01752594e-05
Iter: 985 loss: 1.01920032e-05
Iter: 986 loss: 1.01713313e-05
Iter: 987 loss: 1.01631786e-05
Iter: 988 loss: 1.01877522e-05
Iter: 989 loss: 1.01602272e-05
Iter: 990 loss: 1.01509031e-05
Iter: 991 loss: 1.0200336e-05
Iter: 992 loss: 1.01495316e-05
Iter: 993 loss: 1.01469577e-05
Iter: 994 loss: 1.01459791e-05
Iter: 995 loss: 1.01433334e-05
Iter: 996 loss: 1.01432133e-05
Iter: 997 loss: 1.01421083e-05
Iter: 998 loss: 1.01388987e-05
Iter: 999 loss: 1.01353071e-05
Iter: 1000 loss: 1.01342957e-05
Iter: 1001 loss: 1.01271726e-05
Iter: 1002 loss: 1.01345886e-05
Iter: 1003 loss: 1.01232936e-05
Iter: 1004 loss: 1.01123178e-05
Iter: 1005 loss: 1.01391943e-05
Iter: 1006 loss: 1.01086553e-05
Iter: 1007 loss: 1.01006171e-05
Iter: 1008 loss: 1.01802252e-05
Iter: 1009 loss: 1.01001388e-05
Iter: 1010 loss: 1.00949064e-05
Iter: 1011 loss: 1.0097232e-05
Iter: 1012 loss: 1.00911211e-05
Iter: 1013 loss: 1.0082269e-05
Iter: 1014 loss: 1.00835405e-05
Iter: 1015 loss: 1.00761399e-05
Iter: 1016 loss: 1.00654179e-05
Iter: 1017 loss: 1.01406749e-05
Iter: 1018 loss: 1.00642619e-05
Iter: 1019 loss: 1.00562083e-05
Iter: 1020 loss: 1.0064472e-05
Iter: 1021 loss: 1.00517118e-05
Iter: 1022 loss: 1.00426114e-05
Iter: 1023 loss: 1.00788493e-05
Iter: 1024 loss: 1.0040545e-05
Iter: 1025 loss: 1.00352154e-05
Iter: 1026 loss: 1.01099231e-05
Iter: 1027 loss: 1.00352863e-05
Iter: 1028 loss: 1.00373672e-05
Iter: 1029 loss: 1.00334264e-05
Iter: 1030 loss: 1.00326133e-05
Iter: 1031 loss: 1.00298694e-05
Iter: 1032 loss: 1.00265934e-05
Iter: 1033 loss: 1.00259285e-05
Iter: 1034 loss: 1.00197412e-05
Iter: 1035 loss: 1.00250563e-05
Iter: 1036 loss: 1.00161678e-05
Iter: 1037 loss: 1.0009876e-05
Iter: 1038 loss: 1.00292655e-05
Iter: 1039 loss: 1.00077486e-05
Iter: 1040 loss: 1.00002526e-05
Iter: 1041 loss: 1.00229117e-05
Iter: 1042 loss: 9.99800432e-06
Iter: 1043 loss: 9.99081385e-06
Iter: 1044 loss: 1.00195539e-05
Iter: 1045 loss: 9.9889412e-06
Iter: 1046 loss: 9.98391806e-06
Iter: 1047 loss: 9.99877921e-06
Iter: 1048 loss: 9.98215455e-06
Iter: 1049 loss: 9.97690859e-06
Iter: 1050 loss: 9.97373e-06
Iter: 1051 loss: 9.97151892e-06
Iter: 1052 loss: 9.96246126e-06
Iter: 1053 loss: 1.00195375e-05
Iter: 1054 loss: 9.96156177e-06
Iter: 1055 loss: 9.95502614e-06
Iter: 1056 loss: 9.96143899e-06
Iter: 1057 loss: 9.95121445e-06
Iter: 1058 loss: 9.94235e-06
Iter: 1059 loss: 9.95152186e-06
Iter: 1060 loss: 9.93709e-06
Iter: 1061 loss: 9.98365613e-06
Iter: 1062 loss: 9.9346189e-06
Iter: 1063 loss: 9.93284e-06
Iter: 1064 loss: 9.92693276e-06
Iter: 1065 loss: 9.92484638e-06
Iter: 1066 loss: 9.92029527e-06
Iter: 1067 loss: 9.91584147e-06
Iter: 1068 loss: 9.92616e-06
Iter: 1069 loss: 9.91418165e-06
Iter: 1070 loss: 9.9086792e-06
Iter: 1071 loss: 9.90388071e-06
Iter: 1072 loss: 9.90261287e-06
Iter: 1073 loss: 9.89302316e-06
Iter: 1074 loss: 9.9384315e-06
Iter: 1075 loss: 9.89129e-06
Iter: 1076 loss: 9.88348165e-06
Iter: 1077 loss: 9.91819252e-06
Iter: 1078 loss: 9.88215e-06
Iter: 1079 loss: 9.87511703e-06
Iter: 1080 loss: 9.8861e-06
Iter: 1081 loss: 9.87162e-06
Iter: 1082 loss: 9.86360101e-06
Iter: 1083 loss: 9.88801548e-06
Iter: 1084 loss: 9.86143823e-06
Iter: 1085 loss: 9.85559745e-06
Iter: 1086 loss: 9.86768282e-06
Iter: 1087 loss: 9.85294446e-06
Iter: 1088 loss: 9.8459368e-06
Iter: 1089 loss: 9.8632072e-06
Iter: 1090 loss: 9.84322105e-06
Iter: 1091 loss: 9.83565951e-06
Iter: 1092 loss: 9.84668077e-06
Iter: 1093 loss: 9.83170867e-06
Iter: 1094 loss: 9.8321907e-06
Iter: 1095 loss: 9.82894835e-06
Iter: 1096 loss: 9.82450638e-06
Iter: 1097 loss: 9.82281654e-06
Iter: 1098 loss: 9.82081929e-06
Iter: 1099 loss: 9.81906123e-06
Iter: 1100 loss: 9.81387257e-06
Iter: 1101 loss: 9.83807e-06
Iter: 1102 loss: 9.81192352e-06
Iter: 1103 loss: 9.80555e-06
Iter: 1104 loss: 9.80567529e-06
Iter: 1105 loss: 9.80032473e-06
Iter: 1106 loss: 9.78907337e-06
Iter: 1107 loss: 9.81471658e-06
Iter: 1108 loss: 9.78501157e-06
Iter: 1109 loss: 9.77622221e-06
Iter: 1110 loss: 9.77616583e-06
Iter: 1111 loss: 9.77010859e-06
Iter: 1112 loss: 9.78069238e-06
Iter: 1113 loss: 9.76724186e-06
Iter: 1114 loss: 9.75944386e-06
Iter: 1115 loss: 9.77022319e-06
Iter: 1116 loss: 9.7552529e-06
Iter: 1117 loss: 9.748107e-06
Iter: 1118 loss: 9.76947922e-06
Iter: 1119 loss: 9.7457978e-06
Iter: 1120 loss: 9.73840451e-06
Iter: 1121 loss: 9.74148679e-06
Iter: 1122 loss: 9.73319402e-06
Iter: 1123 loss: 9.72280577e-06
Iter: 1124 loss: 9.73615715e-06
Iter: 1125 loss: 9.71770896e-06
Iter: 1126 loss: 9.7236607e-06
Iter: 1127 loss: 9.71521331e-06
Iter: 1128 loss: 9.71281e-06
Iter: 1129 loss: 9.72670568e-06
Iter: 1130 loss: 9.7123866e-06
Iter: 1131 loss: 9.71142072e-06
Iter: 1132 loss: 9.70798374e-06
Iter: 1133 loss: 9.70599922e-06
Iter: 1134 loss: 9.70358451e-06
Iter: 1135 loss: 9.69576831e-06
Iter: 1136 loss: 9.69666144e-06
Iter: 1137 loss: 9.68972563e-06
Iter: 1138 loss: 9.68081713e-06
Iter: 1139 loss: 9.75110834e-06
Iter: 1140 loss: 9.68014e-06
Iter: 1141 loss: 9.67359665e-06
Iter: 1142 loss: 9.67766482e-06
Iter: 1143 loss: 9.66946209e-06
Iter: 1144 loss: 9.66021344e-06
Iter: 1145 loss: 9.69738e-06
Iter: 1146 loss: 9.65790787e-06
Iter: 1147 loss: 9.65052823e-06
Iter: 1148 loss: 9.67129e-06
Iter: 1149 loss: 9.64816e-06
Iter: 1150 loss: 9.64225637e-06
Iter: 1151 loss: 9.66298467e-06
Iter: 1152 loss: 9.6406e-06
Iter: 1153 loss: 9.63384082e-06
Iter: 1154 loss: 9.63981e-06
Iter: 1155 loss: 9.62982449e-06
Iter: 1156 loss: 9.6237718e-06
Iter: 1157 loss: 9.63482307e-06
Iter: 1158 loss: 9.62145077e-06
Iter: 1159 loss: 9.61188e-06
Iter: 1160 loss: 9.60564648e-06
Iter: 1161 loss: 9.60225861e-06
Iter: 1162 loss: 9.70734072e-06
Iter: 1163 loss: 9.60096622e-06
Iter: 1164 loss: 9.60027501e-06
Iter: 1165 loss: 9.59735189e-06
Iter: 1166 loss: 9.59203226e-06
Iter: 1167 loss: 9.59170393e-06
Iter: 1168 loss: 9.58514102e-06
Iter: 1169 loss: 9.59498266e-06
Iter: 1170 loss: 9.58187e-06
Iter: 1171 loss: 9.57405791e-06
Iter: 1172 loss: 9.57832e-06
Iter: 1173 loss: 9.56925214e-06
Iter: 1174 loss: 9.560541e-06
Iter: 1175 loss: 9.64504216e-06
Iter: 1176 loss: 9.56041731e-06
Iter: 1177 loss: 9.55486939e-06
Iter: 1178 loss: 9.54830284e-06
Iter: 1179 loss: 9.5474461e-06
Iter: 1180 loss: 9.53698e-06
Iter: 1181 loss: 9.60603757e-06
Iter: 1182 loss: 9.53595554e-06
Iter: 1183 loss: 9.52945447e-06
Iter: 1184 loss: 9.5749765e-06
Iter: 1185 loss: 9.52879782e-06
Iter: 1186 loss: 9.52227583e-06
Iter: 1187 loss: 9.52211121e-06
Iter: 1188 loss: 9.51691436e-06
Iter: 1189 loss: 9.50729827e-06
Iter: 1190 loss: 9.55244832e-06
Iter: 1191 loss: 9.50550566e-06
Iter: 1192 loss: 9.49711284e-06
Iter: 1193 loss: 9.50265803e-06
Iter: 1194 loss: 9.49171863e-06
Iter: 1195 loss: 9.48272827e-06
Iter: 1196 loss: 9.55829273e-06
Iter: 1197 loss: 9.48222532e-06
Iter: 1198 loss: 9.50582216e-06
Iter: 1199 loss: 9.48134584e-06
Iter: 1200 loss: 9.48075649e-06
Iter: 1201 loss: 9.47868466e-06
Iter: 1202 loss: 9.47423632e-06
Iter: 1203 loss: 9.57842894e-06
Iter: 1204 loss: 9.473717e-06
Iter: 1205 loss: 9.46739783e-06
Iter: 1206 loss: 9.465939e-06
Iter: 1207 loss: 9.46163254e-06
Iter: 1208 loss: 9.45338434e-06
Iter: 1209 loss: 9.47668923e-06
Iter: 1210 loss: 9.45062675e-06
Iter: 1211 loss: 9.44307612e-06
Iter: 1212 loss: 9.4812749e-06
Iter: 1213 loss: 9.44164822e-06
Iter: 1214 loss: 9.43384748e-06
Iter: 1215 loss: 9.43428859e-06
Iter: 1216 loss: 9.42778752e-06
Iter: 1217 loss: 9.41861163e-06
Iter: 1218 loss: 9.47978151e-06
Iter: 1219 loss: 9.41726921e-06
Iter: 1220 loss: 9.41239432e-06
Iter: 1221 loss: 9.44732528e-06
Iter: 1222 loss: 9.41198414e-06
Iter: 1223 loss: 9.40625341e-06
Iter: 1224 loss: 9.39985694e-06
Iter: 1225 loss: 9.39882739e-06
Iter: 1226 loss: 9.39235451e-06
Iter: 1227 loss: 9.39215624e-06
Iter: 1228 loss: 9.38840822e-06
Iter: 1229 loss: 9.38222729e-06
Iter: 1230 loss: 9.38225276e-06
Iter: 1231 loss: 9.39218808e-06
Iter: 1232 loss: 9.38018457e-06
Iter: 1233 loss: 9.37756522e-06
Iter: 1234 loss: 9.37167715e-06
Iter: 1235 loss: 9.45720694e-06
Iter: 1236 loss: 9.37150253e-06
Iter: 1237 loss: 9.36903052e-06
Iter: 1238 loss: 9.36750166e-06
Iter: 1239 loss: 9.36631477e-06
Iter: 1240 loss: 9.36222841e-06
Iter: 1241 loss: 9.36003562e-06
Iter: 1242 loss: 9.35857679e-06
Iter: 1243 loss: 9.34985474e-06
Iter: 1244 loss: 9.35808e-06
Iter: 1245 loss: 9.34484888e-06
Iter: 1246 loss: 9.33589763e-06
Iter: 1247 loss: 9.39563e-06
Iter: 1248 loss: 9.33496813e-06
Iter: 1249 loss: 9.33008778e-06
Iter: 1250 loss: 9.3471017e-06
Iter: 1251 loss: 9.32864896e-06
Iter: 1252 loss: 9.32322655e-06
Iter: 1253 loss: 9.32356306e-06
Iter: 1254 loss: 9.31872728e-06
Iter: 1255 loss: 9.31034447e-06
Iter: 1256 loss: 9.35133539e-06
Iter: 1257 loss: 9.30897932e-06
Iter: 1258 loss: 9.30446367e-06
Iter: 1259 loss: 9.34322361e-06
Iter: 1260 loss: 9.30433634e-06
Iter: 1261 loss: 9.29968428e-06
Iter: 1262 loss: 9.29685484e-06
Iter: 1263 loss: 9.29523048e-06
Iter: 1264 loss: 9.30234455e-06
Iter: 1265 loss: 9.29371708e-06
Iter: 1266 loss: 9.29172529e-06
Iter: 1267 loss: 9.29158e-06
Iter: 1268 loss: 9.29027e-06
Iter: 1269 loss: 9.28901773e-06
Iter: 1270 loss: 9.28598183e-06
Iter: 1271 loss: 9.29029284e-06
Iter: 1272 loss: 9.28392546e-06
Iter: 1273 loss: 9.27699875e-06
Iter: 1274 loss: 9.29012276e-06
Iter: 1275 loss: 9.27426117e-06
Iter: 1276 loss: 9.26941357e-06
Iter: 1277 loss: 9.28398185e-06
Iter: 1278 loss: 9.26801295e-06
Iter: 1279 loss: 9.26164466e-06
Iter: 1280 loss: 9.26614939e-06
Iter: 1281 loss: 9.25755e-06
Iter: 1282 loss: 9.25127461e-06
Iter: 1283 loss: 9.29398084e-06
Iter: 1284 loss: 9.25067161e-06
Iter: 1285 loss: 9.24518736e-06
Iter: 1286 loss: 9.23622065e-06
Iter: 1287 loss: 9.23616517e-06
Iter: 1288 loss: 9.22780055e-06
Iter: 1289 loss: 9.227796e-06
Iter: 1290 loss: 9.22378604e-06
Iter: 1291 loss: 9.22406161e-06
Iter: 1292 loss: 9.22090385e-06
Iter: 1293 loss: 9.2128048e-06
Iter: 1294 loss: 9.23122843e-06
Iter: 1295 loss: 9.20981802e-06
Iter: 1296 loss: 9.21174797e-06
Iter: 1297 loss: 9.20794264e-06
Iter: 1298 loss: 9.20557432e-06
Iter: 1299 loss: 9.21223273e-06
Iter: 1300 loss: 9.20501407e-06
Iter: 1301 loss: 9.20376078e-06
Iter: 1302 loss: 9.20073944e-06
Iter: 1303 loss: 9.21073843e-06
Iter: 1304 loss: 9.19914601e-06
Iter: 1305 loss: 9.19357e-06
Iter: 1306 loss: 9.19315062e-06
Iter: 1307 loss: 9.1895281e-06
Iter: 1308 loss: 9.18218393e-06
Iter: 1309 loss: 9.21445098e-06
Iter: 1310 loss: 9.18078513e-06
Iter: 1311 loss: 9.17531543e-06
Iter: 1312 loss: 9.19469767e-06
Iter: 1313 loss: 9.17386114e-06
Iter: 1314 loss: 9.16760837e-06
Iter: 1315 loss: 9.16950194e-06
Iter: 1316 loss: 9.16330282e-06
Iter: 1317 loss: 9.15377314e-06
Iter: 1318 loss: 9.17022e-06
Iter: 1319 loss: 9.14968859e-06
Iter: 1320 loss: 9.14284828e-06
Iter: 1321 loss: 9.19773811e-06
Iter: 1322 loss: 9.14231714e-06
Iter: 1323 loss: 9.13597341e-06
Iter: 1324 loss: 9.13345e-06
Iter: 1325 loss: 9.12978794e-06
Iter: 1326 loss: 9.12275573e-06
Iter: 1327 loss: 9.12281394e-06
Iter: 1328 loss: 9.12003907e-06
Iter: 1329 loss: 9.12000542e-06
Iter: 1330 loss: 9.11743e-06
Iter: 1331 loss: 9.13628355e-06
Iter: 1332 loss: 9.11714505e-06
Iter: 1333 loss: 9.1156362e-06
Iter: 1334 loss: 9.11127609e-06
Iter: 1335 loss: 9.110061e-06
Iter: 1336 loss: 9.10623e-06
Iter: 1337 loss: 9.09953451e-06
Iter: 1338 loss: 9.1279e-06
Iter: 1339 loss: 9.09803e-06
Iter: 1340 loss: 9.09109258e-06
Iter: 1341 loss: 9.08384391e-06
Iter: 1342 loss: 9.08238871e-06
Iter: 1343 loss: 9.07349749e-06
Iter: 1344 loss: 9.19727245e-06
Iter: 1345 loss: 9.07350477e-06
Iter: 1346 loss: 9.06792775e-06
Iter: 1347 loss: 9.07213507e-06
Iter: 1348 loss: 9.06487e-06
Iter: 1349 loss: 9.05735e-06
Iter: 1350 loss: 9.0684116e-06
Iter: 1351 loss: 9.05364504e-06
Iter: 1352 loss: 9.04837907e-06
Iter: 1353 loss: 9.10949257e-06
Iter: 1354 loss: 9.04835724e-06
Iter: 1355 loss: 9.04431181e-06
Iter: 1356 loss: 9.0378071e-06
Iter: 1357 loss: 9.03740147e-06
Iter: 1358 loss: 9.02955617e-06
Iter: 1359 loss: 9.12397445e-06
Iter: 1360 loss: 9.02914508e-06
Iter: 1361 loss: 9.02519059e-06
Iter: 1362 loss: 9.0637559e-06
Iter: 1363 loss: 9.02513057e-06
Iter: 1364 loss: 9.02492502e-06
Iter: 1365 loss: 9.0230169e-06
Iter: 1366 loss: 9.02246848e-06
Iter: 1367 loss: 9.02028205e-06
Iter: 1368 loss: 9.02014381e-06
Iter: 1369 loss: 9.0178919e-06
Iter: 1370 loss: 9.01417661e-06
Iter: 1371 loss: 9.00819759e-06
Iter: 1372 loss: 9.00834857e-06
Iter: 1373 loss: 9.00025952e-06
Iter: 1374 loss: 9.03459e-06
Iter: 1375 loss: 8.99856241e-06
Iter: 1376 loss: 8.99356564e-06
Iter: 1377 loss: 9.00970917e-06
Iter: 1378 loss: 8.992115e-06
Iter: 1379 loss: 8.98772214e-06
Iter: 1380 loss: 8.98417238e-06
Iter: 1381 loss: 8.98298276e-06
Iter: 1382 loss: 8.97477548e-06
Iter: 1383 loss: 9.01363637e-06
Iter: 1384 loss: 8.97337486e-06
Iter: 1385 loss: 8.96783149e-06
Iter: 1386 loss: 8.96750407e-06
Iter: 1387 loss: 8.96366237e-06
Iter: 1388 loss: 8.95520679e-06
Iter: 1389 loss: 8.99169936e-06
Iter: 1390 loss: 8.95358789e-06
Iter: 1391 loss: 8.94865934e-06
Iter: 1392 loss: 8.96503843e-06
Iter: 1393 loss: 8.94747791e-06
Iter: 1394 loss: 8.9437e-06
Iter: 1395 loss: 8.95835728e-06
Iter: 1396 loss: 8.94261666e-06
Iter: 1397 loss: 8.94542e-06
Iter: 1398 loss: 8.94137702e-06
Iter: 1399 loss: 8.94063305e-06
Iter: 1400 loss: 8.93801916e-06
Iter: 1401 loss: 8.9450532e-06
Iter: 1402 loss: 8.93665765e-06
Iter: 1403 loss: 8.93370088e-06
Iter: 1404 loss: 8.92870867e-06
Iter: 1405 loss: 8.92868775e-06
Iter: 1406 loss: 8.92217395e-06
Iter: 1407 loss: 8.94652294e-06
Iter: 1408 loss: 8.92031494e-06
Iter: 1409 loss: 8.91396121e-06
Iter: 1410 loss: 8.91939635e-06
Iter: 1411 loss: 8.91068885e-06
Iter: 1412 loss: 8.90326919e-06
Iter: 1413 loss: 8.94133154e-06
Iter: 1414 loss: 8.90225783e-06
Iter: 1415 loss: 8.89642433e-06
Iter: 1416 loss: 8.90048432e-06
Iter: 1417 loss: 8.89274088e-06
Iter: 1418 loss: 8.88711747e-06
Iter: 1419 loss: 8.9329e-06
Iter: 1420 loss: 8.88667e-06
Iter: 1421 loss: 8.88302202e-06
Iter: 1422 loss: 8.89393596e-06
Iter: 1423 loss: 8.88183422e-06
Iter: 1424 loss: 8.87679562e-06
Iter: 1425 loss: 8.87184251e-06
Iter: 1426 loss: 8.87089482e-06
Iter: 1427 loss: 8.87138413e-06
Iter: 1428 loss: 8.8679335e-06
Iter: 1429 loss: 8.8693123e-06
Iter: 1430 loss: 8.86767066e-06
Iter: 1431 loss: 8.86705948e-06
Iter: 1432 loss: 8.86558155e-06
Iter: 1433 loss: 8.86738599e-06
Iter: 1434 loss: 8.86454927e-06
Iter: 1435 loss: 8.86160433e-06
Iter: 1436 loss: 8.85510326e-06
Iter: 1437 loss: 8.93595552e-06
Iter: 1438 loss: 8.85502595e-06
Iter: 1439 loss: 8.84899055e-06
Iter: 1440 loss: 8.90852061e-06
Iter: 1441 loss: 8.84864494e-06
Iter: 1442 loss: 8.84530527e-06
Iter: 1443 loss: 8.8464185e-06
Iter: 1444 loss: 8.84268684e-06
Iter: 1445 loss: 8.83725534e-06
Iter: 1446 loss: 8.84579458e-06
Iter: 1447 loss: 8.83486427e-06
Iter: 1448 loss: 8.82935637e-06
Iter: 1449 loss: 8.85889494e-06
Iter: 1450 loss: 8.82862514e-06
Iter: 1451 loss: 8.82526183e-06
Iter: 1452 loss: 8.83052053e-06
Iter: 1453 loss: 8.82369568e-06
Iter: 1454 loss: 8.81877622e-06
Iter: 1455 loss: 8.81793494e-06
Iter: 1456 loss: 8.81459891e-06
Iter: 1457 loss: 8.80843254e-06
Iter: 1458 loss: 8.84984e-06
Iter: 1459 loss: 8.80765583e-06
Iter: 1460 loss: 8.80403877e-06
Iter: 1461 loss: 8.80802509e-06
Iter: 1462 loss: 8.80184143e-06
Iter: 1463 loss: 8.80369589e-06
Iter: 1464 loss: 8.80026619e-06
Iter: 1465 loss: 8.79784329e-06
Iter: 1466 loss: 8.79479376e-06
Iter: 1467 loss: 8.79432e-06
Iter: 1468 loss: 8.79244908e-06
Iter: 1469 loss: 8.7884564e-06
Iter: 1470 loss: 8.7885328e-06
Iter: 1471 loss: 8.78527226e-06
Iter: 1472 loss: 8.79295658e-06
Iter: 1473 loss: 8.78409264e-06
Iter: 1474 loss: 8.7798e-06
Iter: 1475 loss: 8.77748607e-06
Iter: 1476 loss: 8.77558341e-06
Iter: 1477 loss: 8.76991362e-06
Iter: 1478 loss: 8.80470179e-06
Iter: 1479 loss: 8.7691742e-06
Iter: 1480 loss: 8.76409831e-06
Iter: 1481 loss: 8.7658791e-06
Iter: 1482 loss: 8.7605531e-06
Iter: 1483 loss: 8.75451e-06
Iter: 1484 loss: 8.77292678e-06
Iter: 1485 loss: 8.75255319e-06
Iter: 1486 loss: 8.74708348e-06
Iter: 1487 loss: 8.76144804e-06
Iter: 1488 loss: 8.74511898e-06
Iter: 1489 loss: 8.73900444e-06
Iter: 1490 loss: 8.75683054e-06
Iter: 1491 loss: 8.73728095e-06
Iter: 1492 loss: 8.73305362e-06
Iter: 1493 loss: 8.75146907e-06
Iter: 1494 loss: 8.73223689e-06
Iter: 1495 loss: 8.73315548e-06
Iter: 1496 loss: 8.73103818e-06
Iter: 1497 loss: 8.72929741e-06
Iter: 1498 loss: 8.72861438e-06
Iter: 1499 loss: 8.72788678e-06
Iter: 1500 loss: 8.72637611e-06
Iter: 1501 loss: 8.72306e-06
Iter: 1502 loss: 8.77025104e-06
Iter: 1503 loss: 8.7227927e-06
Iter: 1504 loss: 8.71918e-06
Iter: 1505 loss: 8.7147564e-06
Iter: 1506 loss: 8.71445718e-06
Iter: 1507 loss: 8.70748408e-06
Iter: 1508 loss: 8.75457317e-06
Iter: 1509 loss: 8.70695e-06
Iter: 1510 loss: 8.70230724e-06
Iter: 1511 loss: 8.70112399e-06
Iter: 1512 loss: 8.69819723e-06
Iter: 1513 loss: 8.69139694e-06
Iter: 1514 loss: 8.74702528e-06
Iter: 1515 loss: 8.69108862e-06
Iter: 1516 loss: 8.68816e-06
Iter: 1517 loss: 8.6962591e-06
Iter: 1518 loss: 8.68727329e-06
Iter: 1519 loss: 8.68370262e-06
Iter: 1520 loss: 8.67870585e-06
Iter: 1521 loss: 8.67864674e-06
Iter: 1522 loss: 8.67378276e-06
Iter: 1523 loss: 8.67370363e-06
Iter: 1524 loss: 8.66987466e-06
Iter: 1525 loss: 8.66283335e-06
Iter: 1526 loss: 8.81496908e-06
Iter: 1527 loss: 8.66277696e-06
Iter: 1528 loss: 8.65929906e-06
Iter: 1529 loss: 8.65827406e-06
Iter: 1530 loss: 8.66059236e-06
Iter: 1531 loss: 8.65706e-06
Iter: 1532 loss: 8.65660877e-06
Iter: 1533 loss: 8.65501534e-06
Iter: 1534 loss: 8.65078891e-06
Iter: 1535 loss: 8.71623342e-06
Iter: 1536 loss: 8.6506634e-06
Iter: 1537 loss: 8.64487629e-06
Iter: 1538 loss: 8.66429764e-06
Iter: 1539 loss: 8.64320282e-06
Iter: 1540 loss: 8.63860714e-06
Iter: 1541 loss: 8.64837693e-06
Iter: 1542 loss: 8.63672813e-06
Iter: 1543 loss: 8.6327691e-06
Iter: 1544 loss: 8.65085531e-06
Iter: 1545 loss: 8.63151e-06
Iter: 1546 loss: 8.62775414e-06
Iter: 1547 loss: 8.6282e-06
Iter: 1548 loss: 8.62503839e-06
Iter: 1549 loss: 8.61904482e-06
Iter: 1550 loss: 8.64064714e-06
Iter: 1551 loss: 8.61753e-06
Iter: 1552 loss: 8.61393164e-06
Iter: 1553 loss: 8.62396064e-06
Iter: 1554 loss: 8.61241915e-06
Iter: 1555 loss: 8.60790169e-06
Iter: 1556 loss: 8.61122498e-06
Iter: 1557 loss: 8.60455748e-06
Iter: 1558 loss: 8.59993634e-06
Iter: 1559 loss: 8.62974139e-06
Iter: 1560 loss: 8.5991478e-06
Iter: 1561 loss: 8.59467764e-06
Iter: 1562 loss: 8.59075863e-06
Iter: 1563 loss: 8.58944804e-06
Iter: 1564 loss: 8.60503224e-06
Iter: 1565 loss: 8.58808198e-06
Iter: 1566 loss: 8.58578e-06
Iter: 1567 loss: 8.58157e-06
Iter: 1568 loss: 8.66606933e-06
Iter: 1569 loss: 8.58122348e-06
Iter: 1570 loss: 8.57960367e-06
Iter: 1571 loss: 8.57630403e-06
Iter: 1572 loss: 8.62420529e-06
Iter: 1573 loss: 8.57596933e-06
Iter: 1574 loss: 8.57063e-06
Iter: 1575 loss: 8.58294e-06
Iter: 1576 loss: 8.56871338e-06
Iter: 1577 loss: 8.56428505e-06
Iter: 1578 loss: 8.58893327e-06
Iter: 1579 loss: 8.56331826e-06
Iter: 1580 loss: 8.55940561e-06
Iter: 1581 loss: 8.5550364e-06
Iter: 1582 loss: 8.55427e-06
Iter: 1583 loss: 8.54776408e-06
Iter: 1584 loss: 8.59941065e-06
Iter: 1585 loss: 8.54706e-06
Iter: 1586 loss: 8.54219434e-06
Iter: 1587 loss: 8.55012422e-06
Iter: 1588 loss: 8.53985603e-06
Iter: 1589 loss: 8.5345182e-06
Iter: 1590 loss: 8.54233076e-06
Iter: 1591 loss: 8.53197889e-06
Iter: 1592 loss: 8.52565609e-06
Iter: 1593 loss: 8.54679729e-06
Iter: 1594 loss: 8.52397807e-06
Iter: 1595 loss: 8.51885852e-06
Iter: 1596 loss: 8.53413621e-06
Iter: 1597 loss: 8.51703589e-06
Iter: 1598 loss: 8.51397363e-06
Iter: 1599 loss: 8.51389632e-06
Iter: 1600 loss: 8.51028835e-06
Iter: 1601 loss: 8.53742e-06
Iter: 1602 loss: 8.51008372e-06
Iter: 1603 loss: 8.50898141e-06
Iter: 1604 loss: 8.50546e-06
Iter: 1605 loss: 8.50628567e-06
Iter: 1606 loss: 8.50203469e-06
Iter: 1607 loss: 8.49806656e-06
Iter: 1608 loss: 8.51345703e-06
Iter: 1609 loss: 8.49678327e-06
Iter: 1610 loss: 8.49119715e-06
Iter: 1611 loss: 8.4885487e-06
Iter: 1612 loss: 8.48558557e-06
Iter: 1613 loss: 8.47755291e-06
Iter: 1614 loss: 8.53226084e-06
Iter: 1615 loss: 8.47681713e-06
Iter: 1616 loss: 8.47205411e-06
Iter: 1617 loss: 8.48975105e-06
Iter: 1618 loss: 8.47102456e-06
Iter: 1619 loss: 8.46667e-06
Iter: 1620 loss: 8.46571857e-06
Iter: 1621 loss: 8.46267631e-06
Iter: 1622 loss: 8.45830527e-06
Iter: 1623 loss: 8.52703124e-06
Iter: 1624 loss: 8.45830618e-06
Iter: 1625 loss: 8.4554049e-06
Iter: 1626 loss: 8.45308568e-06
Iter: 1627 loss: 8.45219438e-06
Iter: 1628 loss: 8.44705573e-06
Iter: 1629 loss: 8.46146941e-06
Iter: 1630 loss: 8.44529313e-06
Iter: 1631 loss: 8.43979797e-06
Iter: 1632 loss: 8.46208241e-06
Iter: 1633 loss: 8.43839734e-06
Iter: 1634 loss: 8.4489393e-06
Iter: 1635 loss: 8.43684302e-06
Iter: 1636 loss: 8.43611e-06
Iter: 1637 loss: 8.43468479e-06
Iter: 1638 loss: 8.43316775e-06
Iter: 1639 loss: 8.43208181e-06
Iter: 1640 loss: 8.42864938e-06
Iter: 1641 loss: 8.42517511e-06
Iter: 1642 loss: 8.42444388e-06
Iter: 1643 loss: 8.41820292e-06
Iter: 1644 loss: 8.46228158e-06
Iter: 1645 loss: 8.4175108e-06
Iter: 1646 loss: 8.41355e-06
Iter: 1647 loss: 8.41514884e-06
Iter: 1648 loss: 8.41052952e-06
Iter: 1649 loss: 8.40348e-06
Iter: 1650 loss: 8.40102621e-06
Iter: 1651 loss: 8.39757377e-06
Iter: 1652 loss: 8.3913983e-06
Iter: 1653 loss: 8.39171662e-06
Iter: 1654 loss: 8.38748565e-06
Iter: 1655 loss: 8.38574124e-06
Iter: 1656 loss: 8.38326832e-06
Iter: 1657 loss: 8.37760126e-06
Iter: 1658 loss: 8.42089412e-06
Iter: 1659 loss: 8.3772893e-06
Iter: 1660 loss: 8.37257e-06
Iter: 1661 loss: 8.3673076e-06
Iter: 1662 loss: 8.36655727e-06
Iter: 1663 loss: 8.35861647e-06
Iter: 1664 loss: 8.41117e-06
Iter: 1665 loss: 8.35758874e-06
Iter: 1666 loss: 8.35397077e-06
Iter: 1667 loss: 8.35378341e-06
Iter: 1668 loss: 8.34963066e-06
Iter: 1669 loss: 8.37686457e-06
Iter: 1670 loss: 8.34893854e-06
Iter: 1671 loss: 8.3477571e-06
Iter: 1672 loss: 8.34431376e-06
Iter: 1673 loss: 8.34369894e-06
Iter: 1674 loss: 8.34039383e-06
Iter: 1675 loss: 8.33337435e-06
Iter: 1676 loss: 8.33525155e-06
Iter: 1677 loss: 8.32813566e-06
Iter: 1678 loss: 8.32498e-06
Iter: 1679 loss: 8.32390833e-06
Iter: 1680 loss: 8.32091155e-06
Iter: 1681 loss: 8.31627403e-06
Iter: 1682 loss: 8.31604666e-06
Iter: 1683 loss: 8.30923818e-06
Iter: 1684 loss: 8.32926162e-06
Iter: 1685 loss: 8.30702356e-06
Iter: 1686 loss: 8.30155295e-06
Iter: 1687 loss: 8.32699152e-06
Iter: 1688 loss: 8.30057797e-06
Iter: 1689 loss: 8.29546116e-06
Iter: 1690 loss: 8.28780503e-06
Iter: 1691 loss: 8.28728116e-06
Iter: 1692 loss: 8.27763597e-06
Iter: 1693 loss: 8.36931758e-06
Iter: 1694 loss: 8.27716758e-06
Iter: 1695 loss: 8.26995e-06
Iter: 1696 loss: 8.28424527e-06
Iter: 1697 loss: 8.26690666e-06
Iter: 1698 loss: 8.26161158e-06
Iter: 1699 loss: 8.31174111e-06
Iter: 1700 loss: 8.2611823e-06
Iter: 1701 loss: 8.26786891e-06
Iter: 1702 loss: 8.25969801e-06
Iter: 1703 loss: 8.25923416e-06
Iter: 1704 loss: 8.25624466e-06
Iter: 1705 loss: 8.24777544e-06
Iter: 1706 loss: 8.36513209e-06
Iter: 1707 loss: 8.24741437e-06
Iter: 1708 loss: 8.24129074e-06
Iter: 1709 loss: 8.29827e-06
Iter: 1710 loss: 8.24100789e-06
Iter: 1711 loss: 8.23630398e-06
Iter: 1712 loss: 8.23989467e-06
Iter: 1713 loss: 8.23345727e-06
Iter: 1714 loss: 8.22832e-06
Iter: 1715 loss: 8.26394535e-06
Iter: 1716 loss: 8.22768288e-06
Iter: 1717 loss: 8.22313177e-06
Iter: 1718 loss: 8.22314087e-06
Iter: 1719 loss: 8.21975482e-06
Iter: 1720 loss: 8.21368121e-06
Iter: 1721 loss: 8.22697621e-06
Iter: 1722 loss: 8.21158847e-06
Iter: 1723 loss: 8.20670812e-06
Iter: 1724 loss: 8.21456342e-06
Iter: 1725 loss: 8.20433343e-06
Iter: 1726 loss: 8.19832439e-06
Iter: 1727 loss: 8.19290835e-06
Iter: 1728 loss: 8.19117668e-06
Iter: 1729 loss: 8.18539229e-06
Iter: 1730 loss: 8.2556071e-06
Iter: 1731 loss: 8.18529497e-06
Iter: 1732 loss: 8.18171611e-06
Iter: 1733 loss: 8.17696855e-06
Iter: 1734 loss: 8.17654382e-06
Iter: 1735 loss: 8.19216257e-06
Iter: 1736 loss: 8.17523232e-06
Iter: 1737 loss: 8.17433e-06
Iter: 1738 loss: 8.17116415e-06
Iter: 1739 loss: 8.17249384e-06
Iter: 1740 loss: 8.16829925e-06
Iter: 1741 loss: 8.1636008e-06
Iter: 1742 loss: 8.16452939e-06
Iter: 1743 loss: 8.16012835e-06
Iter: 1744 loss: 8.15495332e-06
Iter: 1745 loss: 8.17969885e-06
Iter: 1746 loss: 8.15412204e-06
Iter: 1747 loss: 8.14866689e-06
Iter: 1748 loss: 8.15319072e-06
Iter: 1749 loss: 8.14527084e-06
Iter: 1750 loss: 8.13798397e-06
Iter: 1751 loss: 8.16262218e-06
Iter: 1752 loss: 8.13572842e-06
Iter: 1753 loss: 8.13035058e-06
Iter: 1754 loss: 8.15292424e-06
Iter: 1755 loss: 8.12917278e-06
Iter: 1756 loss: 8.12438e-06
Iter: 1757 loss: 8.12017424e-06
Iter: 1758 loss: 8.11915379e-06
Iter: 1759 loss: 8.11033e-06
Iter: 1760 loss: 8.15481053e-06
Iter: 1761 loss: 8.10894926e-06
Iter: 1762 loss: 8.10328129e-06
Iter: 1763 loss: 8.11955215e-06
Iter: 1764 loss: 8.10146412e-06
Iter: 1765 loss: 8.09440553e-06
Iter: 1766 loss: 8.09584253e-06
Iter: 1767 loss: 8.08932145e-06
Iter: 1768 loss: 8.12265171e-06
Iter: 1769 loss: 8.08786535e-06
Iter: 1770 loss: 8.08669847e-06
Iter: 1771 loss: 8.08211098e-06
Iter: 1772 loss: 8.08478489e-06
Iter: 1773 loss: 8.07831384e-06
Iter: 1774 loss: 8.07177275e-06
Iter: 1775 loss: 8.07529432e-06
Iter: 1776 loss: 8.06749085e-06
Iter: 1777 loss: 8.06137723e-06
Iter: 1778 loss: 8.06753269e-06
Iter: 1779 loss: 8.05761374e-06
Iter: 1780 loss: 8.05002401e-06
Iter: 1781 loss: 8.09982e-06
Iter: 1782 loss: 8.04892079e-06
Iter: 1783 loss: 8.04355113e-06
Iter: 1784 loss: 8.0476475e-06
Iter: 1785 loss: 8.04049e-06
Iter: 1786 loss: 8.03364492e-06
Iter: 1787 loss: 8.09057929e-06
Iter: 1788 loss: 8.03288094e-06
Iter: 1789 loss: 8.02891e-06
Iter: 1790 loss: 8.02786872e-06
Iter: 1791 loss: 8.02530121e-06
Iter: 1792 loss: 8.01857e-06
Iter: 1793 loss: 8.02817158e-06
Iter: 1794 loss: 8.01514943e-06
Iter: 1795 loss: 8.00902671e-06
Iter: 1796 loss: 8.05837499e-06
Iter: 1797 loss: 8.0086138e-06
Iter: 1798 loss: 8.00554881e-06
Iter: 1799 loss: 8.00616181e-06
Iter: 1800 loss: 8.00291e-06
Iter: 1801 loss: 8.00081762e-06
Iter: 1802 loss: 8.00018e-06
Iter: 1803 loss: 7.9963047e-06
Iter: 1804 loss: 7.99129e-06
Iter: 1805 loss: 7.99081772e-06
Iter: 1806 loss: 7.98898327e-06
Iter: 1807 loss: 7.98566816e-06
Iter: 1808 loss: 7.98577184e-06
Iter: 1809 loss: 7.982444e-06
Iter: 1810 loss: 7.97912526e-06
Iter: 1811 loss: 7.97817302e-06
Iter: 1812 loss: 7.97151552e-06
Iter: 1813 loss: 8.00617545e-06
Iter: 1814 loss: 7.97068424e-06
Iter: 1815 loss: 7.96611585e-06
Iter: 1816 loss: 7.98796464e-06
Iter: 1817 loss: 7.96532e-06
Iter: 1818 loss: 7.96214772e-06
Iter: 1819 loss: 7.96707627e-06
Iter: 1820 loss: 7.96053882e-06
Iter: 1821 loss: 7.95548294e-06
Iter: 1822 loss: 7.95988126e-06
Iter: 1823 loss: 7.95262167e-06
Iter: 1824 loss: 7.94821062e-06
Iter: 1825 loss: 7.9667534e-06
Iter: 1826 loss: 7.94715197e-06
Iter: 1827 loss: 7.94253174e-06
Iter: 1828 loss: 7.95156302e-06
Iter: 1829 loss: 7.94088555e-06
Iter: 1830 loss: 7.93554682e-06
Iter: 1831 loss: 7.93633353e-06
Iter: 1832 loss: 7.9313786e-06
Iter: 1833 loss: 7.92551327e-06
Iter: 1834 loss: 7.98493693e-06
Iter: 1835 loss: 7.9255351e-06
Iter: 1836 loss: 7.92127139e-06
Iter: 1837 loss: 7.92304127e-06
Iter: 1838 loss: 7.91805815e-06
Iter: 1839 loss: 7.91592629e-06
Iter: 1840 loss: 7.91422917e-06
Iter: 1841 loss: 7.91356888e-06
Iter: 1842 loss: 7.91091952e-06
Iter: 1843 loss: 7.90589183e-06
Iter: 1844 loss: 7.90561899e-06
Iter: 1845 loss: 7.89854312e-06
Iter: 1846 loss: 7.91088496e-06
Iter: 1847 loss: 7.895419e-06
Iter: 1848 loss: 7.88963553e-06
Iter: 1849 loss: 7.92723313e-06
Iter: 1850 loss: 7.88920624e-06
Iter: 1851 loss: 7.88448506e-06
Iter: 1852 loss: 7.892535e-06
Iter: 1853 loss: 7.88226316e-06
Iter: 1854 loss: 7.87615863e-06
Iter: 1855 loss: 7.8722378e-06
Iter: 1856 loss: 7.86974124e-06
Iter: 1857 loss: 7.86511737e-06
Iter: 1858 loss: 7.86488272e-06
Iter: 1859 loss: 7.86109376e-06
Iter: 1860 loss: 7.86425426e-06
Iter: 1861 loss: 7.85916109e-06
Iter: 1862 loss: 7.85401426e-06
Iter: 1863 loss: 7.86374767e-06
Iter: 1864 loss: 7.8519879e-06
Iter: 1865 loss: 7.84628537e-06
Iter: 1866 loss: 7.85859902e-06
Iter: 1867 loss: 7.84403619e-06
Iter: 1868 loss: 7.83978066e-06
Iter: 1869 loss: 7.87920908e-06
Iter: 1870 loss: 7.839506e-06
Iter: 1871 loss: 7.8461e-06
Iter: 1872 loss: 7.83852374e-06
Iter: 1873 loss: 7.83804717e-06
Iter: 1874 loss: 7.83661744e-06
Iter: 1875 loss: 7.83119685e-06
Iter: 1876 loss: 7.84772328e-06
Iter: 1877 loss: 7.82845655e-06
Iter: 1878 loss: 7.81962535e-06
Iter: 1879 loss: 7.87273711e-06
Iter: 1880 loss: 7.81857489e-06
Iter: 1881 loss: 7.81321432e-06
Iter: 1882 loss: 7.83581e-06
Iter: 1883 loss: 7.8121966e-06
Iter: 1884 loss: 7.80728078e-06
Iter: 1885 loss: 7.80727623e-06
Iter: 1886 loss: 7.80356277e-06
Iter: 1887 loss: 7.79581296e-06
Iter: 1888 loss: 7.82565257e-06
Iter: 1889 loss: 7.79387483e-06
Iter: 1890 loss: 7.7886707e-06
Iter: 1891 loss: 7.80844221e-06
Iter: 1892 loss: 7.7874447e-06
Iter: 1893 loss: 7.78227e-06
Iter: 1894 loss: 7.80019764e-06
Iter: 1895 loss: 7.78106732e-06
Iter: 1896 loss: 7.77619789e-06
Iter: 1897 loss: 7.77242167e-06
Iter: 1898 loss: 7.77101741e-06
Iter: 1899 loss: 7.76467186e-06
Iter: 1900 loss: 7.86748751e-06
Iter: 1901 loss: 7.76455727e-06
Iter: 1902 loss: 7.76078377e-06
Iter: 1903 loss: 7.7624145e-06
Iter: 1904 loss: 7.75835451e-06
Iter: 1905 loss: 7.75335775e-06
Iter: 1906 loss: 7.76235902e-06
Iter: 1907 loss: 7.75144e-06
Iter: 1908 loss: 7.74807e-06
Iter: 1909 loss: 7.74700493e-06
Iter: 1910 loss: 7.74634282e-06
Iter: 1911 loss: 7.74364889e-06
Iter: 1912 loss: 7.74012096e-06
Iter: 1913 loss: 7.73941247e-06
Iter: 1914 loss: 7.73595366e-06
Iter: 1915 loss: 7.74544787e-06
Iter: 1916 loss: 7.7348368e-06
Iter: 1917 loss: 7.73080501e-06
Iter: 1918 loss: 7.73103602e-06
Iter: 1919 loss: 7.72775456e-06
Iter: 1920 loss: 7.7214354e-06
Iter: 1921 loss: 7.76301385e-06
Iter: 1922 loss: 7.72094336e-06
Iter: 1923 loss: 7.71730447e-06
Iter: 1924 loss: 7.72766361e-06
Iter: 1925 loss: 7.71616305e-06
Iter: 1926 loss: 7.71198e-06
Iter: 1927 loss: 7.7097593e-06
Iter: 1928 loss: 7.70731822e-06
Iter: 1929 loss: 7.70112729e-06
Iter: 1930 loss: 7.77588866e-06
Iter: 1931 loss: 7.70089628e-06
Iter: 1932 loss: 7.69818689e-06
Iter: 1933 loss: 7.70318093e-06
Iter: 1934 loss: 7.69698272e-06
Iter: 1935 loss: 7.69305916e-06
Iter: 1936 loss: 7.69096914e-06
Iter: 1937 loss: 7.68926111e-06
Iter: 1938 loss: 7.6811848e-06
Iter: 1939 loss: 7.70707e-06
Iter: 1940 loss: 7.67918937e-06
Iter: 1941 loss: 7.67555e-06
Iter: 1942 loss: 7.71121813e-06
Iter: 1943 loss: 7.67534e-06
Iter: 1944 loss: 7.67425e-06
Iter: 1945 loss: 7.67338497e-06
Iter: 1946 loss: 7.67288657e-06
Iter: 1947 loss: 7.6709639e-06
Iter: 1948 loss: 7.66619632e-06
Iter: 1949 loss: 7.74432738e-06
Iter: 1950 loss: 7.66600169e-06
Iter: 1951 loss: 7.66050744e-06
Iter: 1952 loss: 7.66803259e-06
Iter: 1953 loss: 7.65749701e-06
Iter: 1954 loss: 7.6526012e-06
Iter: 1955 loss: 7.69659437e-06
Iter: 1956 loss: 7.65222285e-06
Iter: 1957 loss: 7.64816286e-06
Iter: 1958 loss: 7.6485976e-06
Iter: 1959 loss: 7.644996e-06
Iter: 1960 loss: 7.63891876e-06
Iter: 1961 loss: 7.64472497e-06
Iter: 1962 loss: 7.63507342e-06
Iter: 1963 loss: 7.6282895e-06
Iter: 1964 loss: 7.66720677e-06
Iter: 1965 loss: 7.62750551e-06
Iter: 1966 loss: 7.62218542e-06
Iter: 1967 loss: 7.65866e-06
Iter: 1968 loss: 7.62160107e-06
Iter: 1969 loss: 7.6179067e-06
Iter: 1970 loss: 7.6248175e-06
Iter: 1971 loss: 7.61597812e-06
Iter: 1972 loss: 7.61074125e-06
Iter: 1973 loss: 7.61788715e-06
Iter: 1974 loss: 7.608121e-06
Iter: 1975 loss: 7.60290914e-06
Iter: 1976 loss: 7.62824902e-06
Iter: 1977 loss: 7.60188777e-06
Iter: 1978 loss: 7.59964951e-06
Iter: 1979 loss: 7.59963359e-06
Iter: 1980 loss: 7.59659906e-06
Iter: 1981 loss: 7.60120884e-06
Iter: 1982 loss: 7.59517707e-06
Iter: 1983 loss: 7.5941457e-06
Iter: 1984 loss: 7.59030036e-06
Iter: 1985 loss: 7.59397562e-06
Iter: 1986 loss: 7.58735587e-06
Iter: 1987 loss: 7.58143915e-06
Iter: 1988 loss: 7.60342027e-06
Iter: 1989 loss: 7.58025817e-06
Iter: 1990 loss: 7.57592261e-06
Iter: 1991 loss: 7.58686292e-06
Iter: 1992 loss: 7.57450562e-06
Iter: 1993 loss: 7.56901954e-06
Iter: 1994 loss: 7.5750313e-06
Iter: 1995 loss: 7.56606096e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8
+ date
Mon Oct 26 13:54:23 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.8_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi2.8_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97441f3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f974422da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9744203d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9744203ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9744170268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97441629d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f974413f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97440ecae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97440eab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97440a49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97440a4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f974407ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9744079620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f974401fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f973d671620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f973d6718c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f973d62d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f973d5cca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f973d5fc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f973d5a1e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f973d5c46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f973d59cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f973d50b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f970a836840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f970a837378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f970a837c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f970a8839d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f970a807730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f970a807400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f970a7b8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f970a7d48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f970a774158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f970a7a2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96e40362f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96e40227b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f96dc260268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 38598.2617
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Traceback (most recent call last):
  File "biholoNN_train.py", line 169, in <module>
    results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func, initial_position=init_params, max_iterations=max_epochs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 287, in minimize
    parallel_iterations=parallel_iterations)[0]
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2499, in while_loop_v2
    return_same_structure=True)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2735, in while_loop
    loop_vars = body(*loop_vars)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 260, in _body
    max_line_search_iterations)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 156, in line_search_step
    max_iterations=max_iterations)  # No search needed for these.
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 259, in hager_zhang
    threshold_use_approximate_wolfe_condition)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 609, in _prepare_args
    val_initial = value_and_gradients_function(initial_step_size)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 251, in _restricted_func
    objective_value, gradient = value_and_gradients_function(pt)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
	 [[Sum_1/_34]]
  (1) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_16876]

Function call stack:
f -> f

++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3
+ date
Mon Oct 26 13:57:23 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi3_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi3_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi3_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 3 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-2_phi3_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdddff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdddfea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bddcd378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdc91510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdd47950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bddcd268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdc0ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdb6c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdb708c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdb31b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdb09950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdb1df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdb1a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdb1a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bda81bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bdb1abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bda5c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bda81d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bda249d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bd9cdf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bd9ac488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bd9acc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bd9a6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bd9ac1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b7008598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b7008b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b6fc07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b6ede730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b6ede2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b6f09ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b6f098c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b6e8a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b6ec0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b6e8a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b6e7d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8b6e28620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.0236622784
Iter: 2 loss: 0.0218585953
Iter: 3 loss: 0.0192067772
Iter: 4 loss: 0.0191168487
Iter: 5 loss: 0.0181047861
Iter: 6 loss: 0.0173432902
Iter: 7 loss: 0.01595347
Iter: 8 loss: 0.017009981
Iter: 9 loss: 0.0151630957
Iter: 10 loss: 0.0141086075
Iter: 11 loss: 0.0176991262
Iter: 12 loss: 0.013851556
Iter: 13 loss: 0.0130365891
Iter: 14 loss: 0.0143520683
Iter: 15 loss: 0.0126714231
Iter: 16 loss: 0.0120109655
Iter: 17 loss: 0.0144366175
Iter: 18 loss: 0.011849435
Iter: 19 loss: 0.0114622284
Iter: 20 loss: 0.0116185537
Iter: 21 loss: 0.0111963581
Iter: 22 loss: 0.0107370336
Iter: 23 loss: 0.0114569198
Iter: 24 loss: 0.0105201164
Iter: 25 loss: 0.0101934075
Iter: 26 loss: 0.0100581739
Iter: 27 loss: 0.00988513324
Iter: 28 loss: 0.0095379781
Iter: 29 loss: 0.00974839553
Iter: 30 loss: 0.00930527
Iter: 31 loss: 0.00898614898
Iter: 32 loss: 0.00891512632
Iter: 33 loss: 0.00871072244
Iter: 34 loss: 0.00833899
Iter: 35 loss: 0.00964885391
Iter: 36 loss: 0.00822733715
Iter: 37 loss: 0.00802119076
Iter: 38 loss: 0.00798791461
Iter: 39 loss: 0.00784968864
Iter: 40 loss: 0.00762094231
Iter: 41 loss: 0.00880831
Iter: 42 loss: 0.00757952454
Iter: 43 loss: 0.0074167
Iter: 44 loss: 0.00784348696
Iter: 45 loss: 0.00736065349
Iter: 46 loss: 0.00716777891
Iter: 47 loss: 0.00709111243
Iter: 48 loss: 0.00699100131
Iter: 49 loss: 0.00673743244
Iter: 50 loss: 0.00876752753
Iter: 51 loss: 0.00671522412
Iter: 52 loss: 0.00648062536
Iter: 53 loss: 0.00768648507
Iter: 54 loss: 0.00644422462
Iter: 55 loss: 0.00628478173
Iter: 56 loss: 0.00673198793
Iter: 57 loss: 0.0062229
Iter: 58 loss: 0.00600275444
Iter: 59 loss: 0.00794614758
Iter: 60 loss: 0.00599164143
Iter: 61 loss: 0.00581374113
Iter: 62 loss: 0.00651866524
Iter: 63 loss: 0.00577348
Iter: 64 loss: 0.00562381931
Iter: 65 loss: 0.00565679884
Iter: 66 loss: 0.00551195
Iter: 67 loss: 0.00535923522
Iter: 68 loss: 0.00549104484
Iter: 69 loss: 0.00526764756
Iter: 70 loss: 0.00512756687
Iter: 71 loss: 0.00583530357
Iter: 72 loss: 0.00509581
Iter: 73 loss: 0.00491607655
Iter: 74 loss: 0.00577511173
Iter: 75 loss: 0.00488382578
Iter: 76 loss: 0.00473958068
Iter: 77 loss: 0.00596892461
Iter: 78 loss: 0.00473153871
Iter: 79 loss: 0.00461841095
Iter: 80 loss: 0.00527920481
Iter: 81 loss: 0.00460260827
Iter: 82 loss: 0.00450923946
Iter: 83 loss: 0.00505984109
Iter: 84 loss: 0.00449748524
Iter: 85 loss: 0.00442911685
Iter: 86 loss: 0.00448311213
Iter: 87 loss: 0.00438696425
Iter: 88 loss: 0.00429206528
Iter: 89 loss: 0.00442166068
Iter: 90 loss: 0.00424525747
Iter: 91 loss: 0.0041320622
Iter: 92 loss: 0.00461635552
Iter: 93 loss: 0.00410948321
Iter: 94 loss: 0.00399787165
Iter: 95 loss: 0.00488484185
Iter: 96 loss: 0.00398884807
Iter: 97 loss: 0.00393591309
Iter: 98 loss: 0.00393038429
Iter: 99 loss: 0.00387232192
Iter: 100 loss: 0.00396428816
Iter: 101 loss: 0.00384585862
Iter: 102 loss: 0.00376450503
Iter: 103 loss: 0.00407194439
Iter: 104 loss: 0.00374419149
Iter: 105 loss: 0.00365990354
Iter: 106 loss: 0.00400580838
Iter: 107 loss: 0.00364191947
Iter: 108 loss: 0.00356825721
Iter: 109 loss: 0.00388585124
Iter: 110 loss: 0.00355328363
Iter: 111 loss: 0.00350341201
Iter: 112 loss: 0.00357020972
Iter: 113 loss: 0.00347719248
Iter: 114 loss: 0.0034153522
Iter: 115 loss: 0.00355560239
Iter: 116 loss: 0.00339269126
Iter: 117 loss: 0.00333158299
Iter: 118 loss: 0.00355313183
Iter: 119 loss: 0.003316093
Iter: 120 loss: 0.00324716908
Iter: 121 loss: 0.00345573435
Iter: 122 loss: 0.00322658289
Iter: 123 loss: 0.0031569642
Iter: 124 loss: 0.00352134
Iter: 125 loss: 0.00314624375
Iter: 126 loss: 0.00308995275
Iter: 127 loss: 0.00322794355
Iter: 128 loss: 0.00307042105
Iter: 129 loss: 0.00302036596
Iter: 130 loss: 0.00326810428
Iter: 131 loss: 0.00301061594
Iter: 132 loss: 0.00297144731
Iter: 133 loss: 0.00309945317
Iter: 134 loss: 0.0029609527
Iter: 135 loss: 0.00292232772
Iter: 136 loss: 0.00298027601
Iter: 137 loss: 0.00290397322
Iter: 138 loss: 0.00286783162
Iter: 139 loss: 0.00296451245
Iter: 140 loss: 0.00285481382
Iter: 141 loss: 0.00281900587
Iter: 142 loss: 0.00285602454
Iter: 143 loss: 0.00279923528
Iter: 144 loss: 0.00274887634
Iter: 145 loss: 0.0028741823
Iter: 146 loss: 0.00273043476
Iter: 147 loss: 0.00269453926
Iter: 148 loss: 0.00287528709
Iter: 149 loss: 0.00268872338
Iter: 150 loss: 0.00265367725
Iter: 151 loss: 0.00282410881
Iter: 152 loss: 0.00264758104
Iter: 153 loss: 0.00260962686
Iter: 154 loss: 0.00277879788
Iter: 155 loss: 0.00260177115
Iter: 156 loss: 0.00256757019
Iter: 157 loss: 0.00279125641
Iter: 158 loss: 0.00256387098
Iter: 159 loss: 0.00253160158
Iter: 160 loss: 0.00255147694
Iter: 161 loss: 0.00251085102
Iter: 162 loss: 0.00247354526
Iter: 163 loss: 0.00269353809
Iter: 164 loss: 0.00246757222
Iter: 165 loss: 0.00244018761
Iter: 166 loss: 0.00249029
Iter: 167 loss: 0.00242858054
Iter: 168 loss: 0.00239406573
Iter: 169 loss: 0.00247481023
Iter: 170 loss: 0.00238142116
Iter: 171 loss: 0.00234988052
Iter: 172 loss: 0.00250911782
Iter: 173 loss: 0.0023446274
Iter: 174 loss: 0.00231545419
Iter: 175 loss: 0.00237818388
Iter: 176 loss: 0.00230387948
Iter: 177 loss: 0.00227293652
Iter: 178 loss: 0.0023324443
Iter: 179 loss: 0.00226001767
Iter: 180 loss: 0.00223170454
Iter: 181 loss: 0.00233991724
Iter: 182 loss: 0.00222366769
Iter: 183 loss: 0.00218804437
Iter: 184 loss: 0.00239151181
Iter: 185 loss: 0.00218329695
Iter: 186 loss: 0.00214994187
Iter: 187 loss: 0.00242118305
Iter: 188 loss: 0.00214756187
Iter: 189 loss: 0.00211301167
Iter: 190 loss: 0.00222921697
Iter: 191 loss: 0.00210337527
Iter: 192 loss: 0.00207366073
Iter: 193 loss: 0.00216468237
Iter: 194 loss: 0.00206477381
Iter: 195 loss: 0.00204158947
Iter: 196 loss: 0.00243685069
Iter: 197 loss: 0.00204159413
Iter: 198 loss: 0.00202510599
Iter: 199 loss: 0.00203947816
Iter: 200 loss: 0.00201539276
Iter: 201 loss: 0.00199506292
Iter: 202 loss: 0.00201547984
Iter: 203 loss: 0.00198348705
Iter: 204 loss: 0.00195873273
Iter: 205 loss: 0.00206428184
Iter: 206 loss: 0.00195351569
Iter: 207 loss: 0.00193523872
Iter: 208 loss: 0.00208999123
Iter: 209 loss: 0.00193417
Iter: 210 loss: 0.00191892392
Iter: 211 loss: 0.00190986064
Iter: 212 loss: 0.00190340681
Iter: 213 loss: 0.0018820751
Iter: 214 loss: 0.00193538843
Iter: 215 loss: 0.00187447551
Iter: 216 loss: 0.00184967392
Iter: 217 loss: 0.00185195473
Iter: 218 loss: 0.00183045829
Iter: 219 loss: 0.00180102396
Iter: 220 loss: 0.00200624391
Iter: 221 loss: 0.00179831299
Iter: 222 loss: 0.00177058508
Iter: 223 loss: 0.00185966061
Iter: 224 loss: 0.00176281843
Iter: 225 loss: 0.0017431482
Iter: 226 loss: 0.00184158
Iter: 227 loss: 0.0017395739
Iter: 228 loss: 0.00172656321
Iter: 229 loss: 0.00174840319
Iter: 230 loss: 0.00172069925
Iter: 231 loss: 0.00170529634
Iter: 232 loss: 0.00175303919
Iter: 233 loss: 0.00170078874
Iter: 234 loss: 0.00168577489
Iter: 235 loss: 0.00169209379
Iter: 236 loss: 0.00167552009
Iter: 237 loss: 0.00165559293
Iter: 238 loss: 0.00181286037
Iter: 239 loss: 0.00165422796
Iter: 240 loss: 0.00163979013
Iter: 241 loss: 0.00176405022
Iter: 242 loss: 0.0016389722
Iter: 243 loss: 0.00162579294
Iter: 244 loss: 0.00162841217
Iter: 245 loss: 0.00161593966
Iter: 246 loss: 0.00159967795
Iter: 247 loss: 0.0016895989
Iter: 248 loss: 0.00159734581
Iter: 249 loss: 0.00158128864
Iter: 250 loss: 0.0016147031
Iter: 251 loss: 0.00157481572
Iter: 252 loss: 0.00156161271
Iter: 253 loss: 0.00157057738
Iter: 254 loss: 0.00155302696
Iter: 255 loss: 0.00153744803
Iter: 256 loss: 0.0015639835
Iter: 257 loss: 0.00153056136
Iter: 258 loss: 0.00151492783
Iter: 259 loss: 0.00156238838
Iter: 260 loss: 0.00151022268
Iter: 261 loss: 0.00149428775
Iter: 262 loss: 0.00157878501
Iter: 263 loss: 0.00149193476
Iter: 264 loss: 0.00147945969
Iter: 265 loss: 0.00152743352
Iter: 266 loss: 0.00147652696
Iter: 267 loss: 0.00146565028
Iter: 268 loss: 0.00148355239
Iter: 269 loss: 0.0014606514
Iter: 270 loss: 0.00144805294
Iter: 271 loss: 0.00145254191
Iter: 272 loss: 0.00143918674
Iter: 273 loss: 0.00142451108
Iter: 274 loss: 0.00142486603
Iter: 275 loss: 0.00141282543
Iter: 276 loss: 0.0013967941
Iter: 277 loss: 0.0013967721
Iter: 278 loss: 0.00138505292
Iter: 279 loss: 0.00147425791
Iter: 280 loss: 0.00138415408
Iter: 281 loss: 0.00137542887
Iter: 282 loss: 0.00137089333
Iter: 283 loss: 0.00136688026
Iter: 284 loss: 0.00135663722
Iter: 285 loss: 0.00145370141
Iter: 286 loss: 0.00135617564
Iter: 287 loss: 0.00134542584
Iter: 288 loss: 0.00136080594
Iter: 289 loss: 0.0013401038
Iter: 290 loss: 0.00132753025
Iter: 291 loss: 0.0013442412
Iter: 292 loss: 0.00132116547
Iter: 293 loss: 0.00130986469
Iter: 294 loss: 0.00134793762
Iter: 295 loss: 0.00130676071
Iter: 296 loss: 0.00129489915
Iter: 297 loss: 0.0013361536
Iter: 298 loss: 0.00129184546
Iter: 299 loss: 0.00128055515
Iter: 300 loss: 0.00131154573
Iter: 301 loss: 0.00127690029
Iter: 302 loss: 0.00126455
Iter: 303 loss: 0.0012850943
Iter: 304 loss: 0.00125897303
Iter: 305 loss: 0.0012465741
Iter: 306 loss: 0.00125955511
Iter: 307 loss: 0.00123969512
Iter: 308 loss: 0.00122630631
Iter: 309 loss: 0.00127493427
Iter: 310 loss: 0.00122289581
Iter: 311 loss: 0.00121338223
Iter: 312 loss: 0.00121337944
Iter: 313 loss: 0.0012048746
Iter: 314 loss: 0.00120584737
Iter: 315 loss: 0.00119834812
Iter: 316 loss: 0.00118925958
Iter: 317 loss: 0.00122052233
Iter: 318 loss: 0.00118684769
Iter: 319 loss: 0.0011778262
Iter: 320 loss: 0.0012294089
Iter: 321 loss: 0.00117650861
Iter: 322 loss: 0.00116776978
Iter: 323 loss: 0.00119545148
Iter: 324 loss: 0.00116519351
Iter: 325 loss: 0.00115788856
Iter: 326 loss: 0.00117604237
Iter: 327 loss: 0.00115531625
Iter: 328 loss: 0.00114810304
Iter: 329 loss: 0.00116187695
Iter: 330 loss: 0.00114507671
Iter: 331 loss: 0.00113656453
Iter: 332 loss: 0.00115916459
Iter: 333 loss: 0.00113372714
Iter: 334 loss: 0.00112409191
Iter: 335 loss: 0.00112818321
Iter: 336 loss: 0.00111745577
Iter: 337 loss: 0.00110423833
Iter: 338 loss: 0.00116336811
Iter: 339 loss: 0.00110160967
Iter: 340 loss: 0.00109255896
Iter: 341 loss: 0.0011022212
Iter: 342 loss: 0.00108760537
Iter: 343 loss: 0.00107737049
Iter: 344 loss: 0.00114305806
Iter: 345 loss: 0.00107624673
Iter: 346 loss: 0.00106850546
Iter: 347 loss: 0.00113077811
Iter: 348 loss: 0.00106798555
Iter: 349 loss: 0.00106160459
Iter: 350 loss: 0.00105749373
Iter: 351 loss: 0.00105501479
Iter: 352 loss: 0.00104895758
Iter: 353 loss: 0.00104894489
Iter: 354 loss: 0.00104388176
Iter: 355 loss: 0.00104767131
Iter: 356 loss: 0.00104077149
Iter: 357 loss: 0.00103383104
Iter: 358 loss: 0.00105276448
Iter: 359 loss: 0.00103154464
Iter: 360 loss: 0.00102403131
Iter: 361 loss: 0.0010453146
Iter: 362 loss: 0.0010216604
Iter: 363 loss: 0.0010135246
Iter: 364 loss: 0.00102966523
Iter: 365 loss: 0.00101015181
Iter: 366 loss: 0.00100255595
Iter: 367 loss: 0.00104183797
Iter: 368 loss: 0.00100131857
Iter: 369 loss: 0.000994531903
Iter: 370 loss: 0.00100877648
Iter: 371 loss: 0.000991857727
Iter: 372 loss: 0.000985227292
Iter: 373 loss: 0.000994571717
Iter: 374 loss: 0.000981957768
Iter: 375 loss: 0.000974080176
Iter: 376 loss: 0.000989814405
Iter: 377 loss: 0.00097083376
Iter: 378 loss: 0.000965018524
Iter: 379 loss: 0.00104718679
Iter: 380 loss: 0.000965008803
Iter: 381 loss: 0.000959228084
Iter: 382 loss: 0.000956038944
Iter: 383 loss: 0.000953486422
Iter: 384 loss: 0.000946346205
Iter: 385 loss: 0.00100315781
Iter: 386 loss: 0.00094584038
Iter: 387 loss: 0.000940375263
Iter: 388 loss: 0.000967502245
Iter: 389 loss: 0.000939437479
Iter: 390 loss: 0.000934199896
Iter: 391 loss: 0.000931691786
Iter: 392 loss: 0.00092914945
Iter: 393 loss: 0.000922955747
Iter: 394 loss: 0.000972025504
Iter: 395 loss: 0.000922527455
Iter: 396 loss: 0.000917642377
Iter: 397 loss: 0.000927356596
Iter: 398 loss: 0.00091564958
Iter: 399 loss: 0.000909643
Iter: 400 loss: 0.000917721365
Iter: 401 loss: 0.000906619593
Iter: 402 loss: 0.000900694169
Iter: 403 loss: 0.000921764062
Iter: 404 loss: 0.000899149862
Iter: 405 loss: 0.000892125827
Iter: 406 loss: 0.000894749071
Iter: 407 loss: 0.000887213391
Iter: 408 loss: 0.00088028016
Iter: 409 loss: 0.000919410144
Iter: 410 loss: 0.000879318221
Iter: 411 loss: 0.000873537094
Iter: 412 loss: 0.000887048198
Iter: 413 loss: 0.000871413737
Iter: 414 loss: 0.000866093207
Iter: 415 loss: 0.000925008266
Iter: 416 loss: 0.00086599862
Iter: 417 loss: 0.000861861743
Iter: 418 loss: 0.000858923246
Iter: 419 loss: 0.00085746625
Iter: 420 loss: 0.000851370278
Iter: 421 loss: 0.000937993405
Iter: 422 loss: 0.000851347
Iter: 423 loss: 0.000847367919
Iter: 424 loss: 0.000850225217
Iter: 425 loss: 0.000844894559
Iter: 426 loss: 0.000839771703
Iter: 427 loss: 0.000839667395
Iter: 428 loss: 0.000835642335
Iter: 429 loss: 0.000828953809
Iter: 430 loss: 0.000872856821
Iter: 431 loss: 0.000828245422
Iter: 432 loss: 0.000822908536
Iter: 433 loss: 0.000835837331
Iter: 434 loss: 0.000821006543
Iter: 435 loss: 0.000815145089
Iter: 436 loss: 0.000823909882
Iter: 437 loss: 0.000812330924
Iter: 438 loss: 0.000806226919
Iter: 439 loss: 0.00083482
Iter: 440 loss: 0.000805117073
Iter: 441 loss: 0.000799854286
Iter: 442 loss: 0.000803366886
Iter: 443 loss: 0.000796551
Iter: 444 loss: 0.000790300895
Iter: 445 loss: 0.000808762154
Iter: 446 loss: 0.000788373174
Iter: 447 loss: 0.000782739837
Iter: 448 loss: 0.000829929253
Iter: 449 loss: 0.000782396295
Iter: 450 loss: 0.0007776
Iter: 451 loss: 0.000794277934
Iter: 452 loss: 0.000776342873
Iter: 453 loss: 0.000772570842
Iter: 454 loss: 0.00077830645
Iter: 455 loss: 0.000770786311
Iter: 456 loss: 0.000766116544
Iter: 457 loss: 0.000812743674
Iter: 458 loss: 0.00076595787
Iter: 459 loss: 0.000763135264
Iter: 460 loss: 0.000760465045
Iter: 461 loss: 0.000759810384
Iter: 462 loss: 0.000754346722
Iter: 463 loss: 0.000766159443
Iter: 464 loss: 0.000752235414
Iter: 465 loss: 0.00074659707
Iter: 466 loss: 0.000763155695
Iter: 467 loss: 0.000744843448
Iter: 468 loss: 0.000739073439
Iter: 469 loss: 0.000750401872
Iter: 470 loss: 0.000736704969
Iter: 471 loss: 0.00073086773
Iter: 472 loss: 0.000762379961
Iter: 473 loss: 0.000730007247
Iter: 474 loss: 0.000725443
Iter: 475 loss: 0.000734014553
Iter: 476 loss: 0.000723497942
Iter: 477 loss: 0.0007186078
Iter: 478 loss: 0.000726508326
Iter: 479 loss: 0.000716354582
Iter: 480 loss: 0.000711565313
Iter: 481 loss: 0.000722621451
Iter: 482 loss: 0.000709791668
Iter: 483 loss: 0.000704544364
Iter: 484 loss: 0.000751648447
Iter: 485 loss: 0.000704296
Iter: 486 loss: 0.00070041
Iter: 487 loss: 0.000702728401
Iter: 488 loss: 0.000697903335
Iter: 489 loss: 0.000693503884
Iter: 490 loss: 0.000716054521
Iter: 491 loss: 0.00069278368
Iter: 492 loss: 0.000687948719
Iter: 493 loss: 0.000713742105
Iter: 494 loss: 0.000687190157
Iter: 495 loss: 0.000684752362
Iter: 496 loss: 0.00068331114
Iter: 497 loss: 0.000682290411
Iter: 498 loss: 0.000677011092
Iter: 499 loss: 0.000692091358
Iter: 500 loss: 0.000675373594
Iter: 501 loss: 0.000670897309
Iter: 502 loss: 0.000697153446
Iter: 503 loss: 0.000670312
Iter: 504 loss: 0.000666665437
Iter: 505 loss: 0.000667887623
Iter: 506 loss: 0.000664087944
Iter: 507 loss: 0.000659379701
Iter: 508 loss: 0.000690581859
Iter: 509 loss: 0.000658899429
Iter: 510 loss: 0.00065541768
Iter: 511 loss: 0.000659496756
Iter: 512 loss: 0.000653565163
Iter: 513 loss: 0.000649523106
Iter: 514 loss: 0.000655518845
Iter: 515 loss: 0.00064758386
Iter: 516 loss: 0.00064344256
Iter: 517 loss: 0.000653596944
Iter: 518 loss: 0.000641966355
Iter: 519 loss: 0.000636953744
Iter: 520 loss: 0.000672106398
Iter: 521 loss: 0.000636497105
Iter: 522 loss: 0.000633028918
Iter: 523 loss: 0.000637021731
Iter: 524 loss: 0.000631177216
Iter: 525 loss: 0.00062792626
Iter: 526 loss: 0.000653084
Iter: 527 loss: 0.000627685222
Iter: 528 loss: 0.000624273322
Iter: 529 loss: 0.000631576288
Iter: 530 loss: 0.000622924708
Iter: 531 loss: 0.000620098435
Iter: 532 loss: 0.000618132763
Iter: 533 loss: 0.000617115642
Iter: 534 loss: 0.000611692958
Iter: 535 loss: 0.000642315
Iter: 536 loss: 0.000610957504
Iter: 537 loss: 0.000606755377
Iter: 538 loss: 0.000624159817
Iter: 539 loss: 0.000605838257
Iter: 540 loss: 0.000602016225
Iter: 541 loss: 0.000602309767
Iter: 542 loss: 0.000599050254
Iter: 543 loss: 0.000594823214
Iter: 544 loss: 0.000647156849
Iter: 545 loss: 0.000594784506
Iter: 546 loss: 0.00059174042
Iter: 547 loss: 0.000592506
Iter: 548 loss: 0.000589511357
Iter: 549 loss: 0.000585769536
Iter: 550 loss: 0.000592230819
Iter: 551 loss: 0.000584105728
Iter: 552 loss: 0.000580408261
Iter: 553 loss: 0.000598081388
Iter: 554 loss: 0.000579750398
Iter: 555 loss: 0.000576056
Iter: 556 loss: 0.000596172933
Iter: 557 loss: 0.000575515209
Iter: 558 loss: 0.000572681893
Iter: 559 loss: 0.00057281449
Iter: 560 loss: 0.000570454111
Iter: 561 loss: 0.0005677148
Iter: 562 loss: 0.000567694078
Iter: 563 loss: 0.000565302558
Iter: 564 loss: 0.000563048
Iter: 565 loss: 0.00056249334
Iter: 566 loss: 0.000558976142
Iter: 567 loss: 0.000564250862
Iter: 568 loss: 0.000557290739
Iter: 569 loss: 0.000553544669
Iter: 570 loss: 0.000574354257
Iter: 571 loss: 0.00055301527
Iter: 572 loss: 0.000549342192
Iter: 573 loss: 0.000561568944
Iter: 574 loss: 0.000548323093
Iter: 575 loss: 0.00054537
Iter: 576 loss: 0.000550178171
Iter: 577 loss: 0.000544008624
Iter: 578 loss: 0.000540474546
Iter: 579 loss: 0.000550191
Iter: 580 loss: 0.000539332686
Iter: 581 loss: 0.00053568196
Iter: 582 loss: 0.000555164
Iter: 583 loss: 0.000535126252
Iter: 584 loss: 0.000532491831
Iter: 585 loss: 0.000530243618
Iter: 586 loss: 0.000529523415
Iter: 587 loss: 0.000525789568
Iter: 588 loss: 0.000565090857
Iter: 589 loss: 0.000525691954
Iter: 590 loss: 0.000522804854
Iter: 591 loss: 0.000535838364
Iter: 592 loss: 0.000522246934
Iter: 593 loss: 0.000519522349
Iter: 594 loss: 0.000518940738
Iter: 595 loss: 0.000517158769
Iter: 596 loss: 0.000514975807
Iter: 597 loss: 0.000514881802
Iter: 598 loss: 0.000512903789
Iter: 599 loss: 0.000510913669
Iter: 600 loss: 0.000510514132
Iter: 601 loss: 0.00050725881
Iter: 602 loss: 0.000511451042
Iter: 603 loss: 0.000505593489
Iter: 604 loss: 0.00050190225
Iter: 605 loss: 0.000508915109
Iter: 606 loss: 0.000500354159
Iter: 607 loss: 0.000497004192
Iter: 608 loss: 0.000536131382
Iter: 609 loss: 0.000496952
Iter: 610 loss: 0.0004943487
Iter: 611 loss: 0.000497474
Iter: 612 loss: 0.000492976629
Iter: 613 loss: 0.000489652564
Iter: 614 loss: 0.000498231268
Iter: 615 loss: 0.00048850954
Iter: 616 loss: 0.000485603727
Iter: 617 loss: 0.000497121131
Iter: 618 loss: 0.000484945311
Iter: 619 loss: 0.000482189207
Iter: 620 loss: 0.000488635851
Iter: 621 loss: 0.000481172174
Iter: 622 loss: 0.000478401576
Iter: 623 loss: 0.000482165546
Iter: 624 loss: 0.00047700468
Iter: 625 loss: 0.000474002445
Iter: 626 loss: 0.000492581632
Iter: 627 loss: 0.000473639375
Iter: 628 loss: 0.000470966217
Iter: 629 loss: 0.000481381954
Iter: 630 loss: 0.000470336527
Iter: 631 loss: 0.000467954407
Iter: 632 loss: 0.000472120184
Iter: 633 loss: 0.000466890866
Iter: 634 loss: 0.000463744043
Iter: 635 loss: 0.000482811709
Iter: 636 loss: 0.000463347533
Iter: 637 loss: 0.000461547519
Iter: 638 loss: 0.000460450305
Iter: 639 loss: 0.000459715724
Iter: 640 loss: 0.00045696157
Iter: 641 loss: 0.000463204138
Iter: 642 loss: 0.000455927569
Iter: 643 loss: 0.000453264045
Iter: 644 loss: 0.000456838636
Iter: 645 loss: 0.000451917702
Iter: 646 loss: 0.000448574981
Iter: 647 loss: 0.000474784378
Iter: 648 loss: 0.000448340288
Iter: 649 loss: 0.000445948681
Iter: 650 loss: 0.000451808446
Iter: 651 loss: 0.000445096171
Iter: 652 loss: 0.000442516233
Iter: 653 loss: 0.000444606849
Iter: 654 loss: 0.000440965727
Iter: 655 loss: 0.000438141695
Iter: 656 loss: 0.000458248542
Iter: 657 loss: 0.000437892944
Iter: 658 loss: 0.000435806054
Iter: 659 loss: 0.000436392234
Iter: 660 loss: 0.00043429283
Iter: 661 loss: 0.000431914435
Iter: 662 loss: 0.000456766575
Iter: 663 loss: 0.000431853638
Iter: 664 loss: 0.000429989072
Iter: 665 loss: 0.000435260677
Iter: 666 loss: 0.000429386069
Iter: 667 loss: 0.00042756676
Iter: 668 loss: 0.000436105882
Iter: 669 loss: 0.000427224557
Iter: 670 loss: 0.000425134116
Iter: 671 loss: 0.000424709811
Iter: 672 loss: 0.000423327496
Iter: 673 loss: 0.000421059725
Iter: 674 loss: 0.000421421893
Iter: 675 loss: 0.000419344637
Iter: 676 loss: 0.00041620541
Iter: 677 loss: 0.000426942221
Iter: 678 loss: 0.000415368704
Iter: 679 loss: 0.000412880065
Iter: 680 loss: 0.000425362203
Iter: 681 loss: 0.000412448426
Iter: 682 loss: 0.000410395034
Iter: 683 loss: 0.000424493221
Iter: 684 loss: 0.000410200708
Iter: 685 loss: 0.000408318418
Iter: 686 loss: 0.000409827277
Iter: 687 loss: 0.000407179818
Iter: 688 loss: 0.000405126542
Iter: 689 loss: 0.000414428301
Iter: 690 loss: 0.000404732942
Iter: 691 loss: 0.000402918726
Iter: 692 loss: 0.000404219667
Iter: 693 loss: 0.000401799422
Iter: 694 loss: 0.000399375189
Iter: 695 loss: 0.000407960091
Iter: 696 loss: 0.000398749311
Iter: 697 loss: 0.000396903779
Iter: 698 loss: 0.000410632929
Iter: 699 loss: 0.000396760297
Iter: 700 loss: 0.000394972973
Iter: 701 loss: 0.000397725555
Iter: 702 loss: 0.000394133298
Iter: 703 loss: 0.000392230664
Iter: 704 loss: 0.000406943902
Iter: 705 loss: 0.000392096641
Iter: 706 loss: 0.000390835718
Iter: 707 loss: 0.000389080145
Iter: 708 loss: 0.000389006571
Iter: 709 loss: 0.000386684726
Iter: 710 loss: 0.000392640446
Iter: 711 loss: 0.000385884283
Iter: 712 loss: 0.000383916893
Iter: 713 loss: 0.000386078085
Iter: 714 loss: 0.000382840051
Iter: 715 loss: 0.000380453828
Iter: 716 loss: 0.00040031891
Iter: 717 loss: 0.000380303041
Iter: 718 loss: 0.000378573022
Iter: 719 loss: 0.000385619409
Iter: 720 loss: 0.000378197525
Iter: 721 loss: 0.000376418349
Iter: 722 loss: 0.000378051656
Iter: 723 loss: 0.000375391101
Iter: 724 loss: 0.000373470131
Iter: 725 loss: 0.000380894053
Iter: 726 loss: 0.000373017654
Iter: 727 loss: 0.000371000497
Iter: 728 loss: 0.000373112707
Iter: 729 loss: 0.000369885529
Iter: 730 loss: 0.000367833301
Iter: 731 loss: 0.00038013357
Iter: 732 loss: 0.000367570843
Iter: 733 loss: 0.000365861342
Iter: 734 loss: 0.000371877861
Iter: 735 loss: 0.000365413202
Iter: 736 loss: 0.000363742613
Iter: 737 loss: 0.000375146396
Iter: 738 loss: 0.000363572879
Iter: 739 loss: 0.000362194231
Iter: 740 loss: 0.000363695261
Iter: 741 loss: 0.000361437094
Iter: 742 loss: 0.000359804515
Iter: 743 loss: 0.000359063968
Iter: 744 loss: 0.000358245859
Iter: 745 loss: 0.000356362551
Iter: 746 loss: 0.000361595943
Iter: 747 loss: 0.000355750613
Iter: 748 loss: 0.000353657058
Iter: 749 loss: 0.000357625249
Iter: 750 loss: 0.000352771051
Iter: 751 loss: 0.000350669958
Iter: 752 loss: 0.000362921215
Iter: 753 loss: 0.00035038538
Iter: 754 loss: 0.000348420406
Iter: 755 loss: 0.000357225887
Iter: 756 loss: 0.000348034606
Iter: 757 loss: 0.00034664874
Iter: 758 loss: 0.000348899688
Iter: 759 loss: 0.000346012181
Iter: 760 loss: 0.000344273052
Iter: 761 loss: 0.000348377274
Iter: 762 loss: 0.000343630149
Iter: 763 loss: 0.000342009414
Iter: 764 loss: 0.000347079127
Iter: 765 loss: 0.00034153735
Iter: 766 loss: 0.000339734426
Iter: 767 loss: 0.000341744861
Iter: 768 loss: 0.00033875613
Iter: 769 loss: 0.000336953497
Iter: 770 loss: 0.000355820579
Iter: 771 loss: 0.000336904603
Iter: 772 loss: 0.000335804099
Iter: 773 loss: 0.000345468114
Iter: 774 loss: 0.000335744
Iter: 775 loss: 0.000334748242
Iter: 776 loss: 0.000333244796
Iter: 777 loss: 0.000333216653
Iter: 778 loss: 0.000331375806
Iter: 779 loss: 0.000338913087
Iter: 780 loss: 0.00033097394
Iter: 781 loss: 0.000329386792
Iter: 782 loss: 0.000328797905
Iter: 783 loss: 0.000327924616
Iter: 784 loss: 0.000325846777
Iter: 785 loss: 0.000337693142
Iter: 786 loss: 0.000325567031
Iter: 787 loss: 0.000323871907
Iter: 788 loss: 0.000328751747
Iter: 789 loss: 0.000323338812
Iter: 790 loss: 0.000321776257
Iter: 791 loss: 0.000336497673
Iter: 792 loss: 0.000321717584
Iter: 793 loss: 0.000320467021
Iter: 794 loss: 0.000320815976
Iter: 795 loss: 0.000319566811
Iter: 796 loss: 0.000317790109
Iter: 797 loss: 0.000320561812
Iter: 798 loss: 0.000316955789
Iter: 799 loss: 0.00031528197
Iter: 800 loss: 0.00032839
Iter: 801 loss: 0.000315170764
Iter: 802 loss: 0.000313777709
Iter: 803 loss: 0.000318836712
Iter: 804 loss: 0.000313425728
Iter: 805 loss: 0.000312365039
Iter: 806 loss: 0.000318179256
Iter: 807 loss: 0.000312208664
Iter: 808 loss: 0.000311171956
Iter: 809 loss: 0.000313880184
Iter: 810 loss: 0.000310820731
Iter: 811 loss: 0.000309797644
Iter: 812 loss: 0.000310910749
Iter: 813 loss: 0.000309241092
Iter: 814 loss: 0.000307985756
Iter: 815 loss: 0.000307545532
Iter: 816 loss: 0.000306837086
Iter: 817 loss: 0.000305224385
Iter: 818 loss: 0.000313386612
Iter: 819 loss: 0.000304959685
Iter: 820 loss: 0.000303557172
Iter: 821 loss: 0.000302832224
Iter: 822 loss: 0.000302183907
Iter: 823 loss: 0.000300255342
Iter: 824 loss: 0.0003188384
Iter: 825 loss: 0.000300181942
Iter: 826 loss: 0.000298591
Iter: 827 loss: 0.000299731531
Iter: 828 loss: 0.000297606137
Iter: 829 loss: 0.000296142913
Iter: 830 loss: 0.000296139857
Iter: 831 loss: 0.000295269128
Iter: 832 loss: 0.000293655845
Iter: 833 loss: 0.000330305047
Iter: 834 loss: 0.000293655088
Iter: 835 loss: 0.000291932432
Iter: 836 loss: 0.000307646318
Iter: 837 loss: 0.000291856
Iter: 838 loss: 0.000290651951
Iter: 839 loss: 0.000296294282
Iter: 840 loss: 0.000290426804
Iter: 841 loss: 0.000288983225
Iter: 842 loss: 0.00029358649
Iter: 843 loss: 0.000288568845
Iter: 844 loss: 0.000287470582
Iter: 845 loss: 0.000292612589
Iter: 846 loss: 0.00028727125
Iter: 847 loss: 0.00028640512
Iter: 848 loss: 0.000287251896
Iter: 849 loss: 0.000285921618
Iter: 850 loss: 0.000284792855
Iter: 851 loss: 0.000284861278
Iter: 852 loss: 0.0002839074
Iter: 853 loss: 0.0002825509
Iter: 854 loss: 0.000285661139
Iter: 855 loss: 0.000282039051
Iter: 856 loss: 0.000280494685
Iter: 857 loss: 0.000290066848
Iter: 858 loss: 0.000280309381
Iter: 859 loss: 0.000279196305
Iter: 860 loss: 0.000279111293
Iter: 861 loss: 0.000278280058
Iter: 862 loss: 0.000276689854
Iter: 863 loss: 0.000280317268
Iter: 864 loss: 0.000276088889
Iter: 865 loss: 0.000274464401
Iter: 866 loss: 0.000286532799
Iter: 867 loss: 0.000274332211
Iter: 868 loss: 0.000273207785
Iter: 869 loss: 0.000278873515
Iter: 870 loss: 0.000273023092
Iter: 871 loss: 0.000271994504
Iter: 872 loss: 0.000272579084
Iter: 873 loss: 0.000271322264
Iter: 874 loss: 0.000270405493
Iter: 875 loss: 0.000282526365
Iter: 876 loss: 0.00027039909
Iter: 877 loss: 0.000269540324
Iter: 878 loss: 0.000270542689
Iter: 879 loss: 0.000269088137
Iter: 880 loss: 0.000268171716
Iter: 881 loss: 0.000268638076
Iter: 882 loss: 0.000267566415
Iter: 883 loss: 0.000266331714
Iter: 884 loss: 0.000270963152
Iter: 885 loss: 0.000266029558
Iter: 886 loss: 0.000265081908
Iter: 887 loss: 0.00026448039
Iter: 888 loss: 0.000264105212
Iter: 889 loss: 0.000262783782
Iter: 890 loss: 0.000270642631
Iter: 891 loss: 0.000262621674
Iter: 892 loss: 0.00026151893
Iter: 893 loss: 0.000266197749
Iter: 894 loss: 0.000261286303
Iter: 895 loss: 0.000260131084
Iter: 896 loss: 0.000260282075
Iter: 897 loss: 0.00025925276
Iter: 898 loss: 0.000257946143
Iter: 899 loss: 0.000259350636
Iter: 900 loss: 0.00025723569
Iter: 901 loss: 0.000255789957
Iter: 902 loss: 0.00027361195
Iter: 903 loss: 0.000255771854
Iter: 904 loss: 0.000254948798
Iter: 905 loss: 0.000257117383
Iter: 906 loss: 0.000254673534
Iter: 907 loss: 0.000253650476
Iter: 908 loss: 0.00025554272
Iter: 909 loss: 0.000253212696
Iter: 910 loss: 0.000252522237
Iter: 911 loss: 0.000252516154
Iter: 912 loss: 0.000252015219
Iter: 913 loss: 0.000251077232
Iter: 914 loss: 0.000271881348
Iter: 915 loss: 0.000251072721
Iter: 916 loss: 0.000249952194
Iter: 917 loss: 0.000256944972
Iter: 918 loss: 0.000249819772
Iter: 919 loss: 0.00024891994
Iter: 920 loss: 0.000249466946
Iter: 921 loss: 0.000248337223
Iter: 922 loss: 0.000247276039
Iter: 923 loss: 0.000248559169
Iter: 924 loss: 0.000246717769
Iter: 925 loss: 0.000245566247
Iter: 926 loss: 0.000252501806
Iter: 927 loss: 0.000245419622
Iter: 928 loss: 0.000244359166
Iter: 929 loss: 0.00024752313
Iter: 930 loss: 0.000244036404
Iter: 931 loss: 0.00024295591
Iter: 932 loss: 0.0002431912
Iter: 933 loss: 0.000242159862
Iter: 934 loss: 0.000240912574
Iter: 935 loss: 0.000248701515
Iter: 936 loss: 0.000240764886
Iter: 937 loss: 0.000239683359
Iter: 938 loss: 0.000239205314
Iter: 939 loss: 0.000238658395
Iter: 940 loss: 0.000237952743
Iter: 941 loss: 0.000237829256
Iter: 942 loss: 0.000237118846
Iter: 943 loss: 0.000238549401
Iter: 944 loss: 0.000236829757
Iter: 945 loss: 0.000235866435
Iter: 946 loss: 0.000237147047
Iter: 947 loss: 0.000235379674
Iter: 948 loss: 0.000234540639
Iter: 949 loss: 0.000236602442
Iter: 950 loss: 0.000234246399
Iter: 951 loss: 0.000233512532
Iter: 952 loss: 0.000234263978
Iter: 953 loss: 0.000233105224
Iter: 954 loss: 0.000231967264
Iter: 955 loss: 0.00023383912
Iter: 956 loss: 0.000231446684
Iter: 957 loss: 0.000230587873
Iter: 958 loss: 0.000230188365
Iter: 959 loss: 0.000229760859
Iter: 960 loss: 0.000228513236
Iter: 961 loss: 0.000240688008
Iter: 962 loss: 0.000228468911
Iter: 963 loss: 0.000227536075
Iter: 964 loss: 0.000229538651
Iter: 965 loss: 0.000227167504
Iter: 966 loss: 0.000226072254
Iter: 967 loss: 0.000228389341
Iter: 968 loss: 0.000225642085
Iter: 969 loss: 0.000224615127
Iter: 970 loss: 0.000226607954
Iter: 971 loss: 0.000224183372
Iter: 972 loss: 0.000223084324
Iter: 973 loss: 0.000225863041
Iter: 974 loss: 0.000222706367
Iter: 975 loss: 0.000222012473
Iter: 976 loss: 0.000232573293
Iter: 977 loss: 0.000222012095
Iter: 978 loss: 0.00022130624
Iter: 979 loss: 0.000223086885
Iter: 980 loss: 0.000221061229
Iter: 981 loss: 0.000220342496
Iter: 982 loss: 0.000221013557
Iter: 983 loss: 0.000219934329
Iter: 984 loss: 0.000219197944
Iter: 985 loss: 0.000220503556
Iter: 986 loss: 0.000218874542
Iter: 987 loss: 0.00021801627
Iter: 988 loss: 0.00021934582
Iter: 989 loss: 0.000217610912
Iter: 990 loss: 0.000216670363
Iter: 991 loss: 0.000220928632
Iter: 992 loss: 0.00021649104
Iter: 993 loss: 0.000215670952
Iter: 994 loss: 0.000216425949
Iter: 995 loss: 0.000215197826
Iter: 996 loss: 0.000214196945
Iter: 997 loss: 0.000216222
Iter: 998 loss: 0.000213787644
Iter: 999 loss: 0.000212842337
Iter: 1000 loss: 0.000214262414
Iter: 1001 loss: 0.000212386629
Iter: 1002 loss: 0.000211188701
Iter: 1003 loss: 0.0002135057
Iter: 1004 loss: 0.00021068989
Iter: 1005 loss: 0.000209745587
Iter: 1006 loss: 0.000221745373
Iter: 1007 loss: 0.00020973722
Iter: 1008 loss: 0.00020893391
Iter: 1009 loss: 0.000209635531
Iter: 1010 loss: 0.000208454439
Iter: 1011 loss: 0.000207641628
Iter: 1012 loss: 0.000212064857
Iter: 1013 loss: 0.000207520206
Iter: 1014 loss: 0.000206956203
Iter: 1015 loss: 0.000206947851
Iter: 1016 loss: 0.000206473458
Iter: 1017 loss: 0.000205404649
Iter: 1018 loss: 0.000220092159
Iter: 1019 loss: 0.000205345437
Iter: 1020 loss: 0.000204502721
Iter: 1021 loss: 0.000210491446
Iter: 1022 loss: 0.000204429583
Iter: 1023 loss: 0.0002036019
Iter: 1024 loss: 0.000204412776
Iter: 1025 loss: 0.000203134201
Iter: 1026 loss: 0.000202190364
Iter: 1027 loss: 0.000206262834
Iter: 1028 loss: 0.000201992094
Iter: 1029 loss: 0.000201246032
Iter: 1030 loss: 0.000201521849
Iter: 1031 loss: 0.000200724055
Iter: 1032 loss: 0.000199755435
Iter: 1033 loss: 0.000207150253
Iter: 1034 loss: 0.000199679707
Iter: 1035 loss: 0.000198979411
Iter: 1036 loss: 0.000198313137
Iter: 1037 loss: 0.000198151713
Iter: 1038 loss: 0.000197075307
Iter: 1039 loss: 0.000206576224
Iter: 1040 loss: 0.000197022076
Iter: 1041 loss: 0.000196230132
Iter: 1042 loss: 0.000199722737
Iter: 1043 loss: 0.000196073728
Iter: 1044 loss: 0.000195296045
Iter: 1045 loss: 0.000197202229
Iter: 1046 loss: 0.000195013941
Iter: 1047 loss: 0.000194448934
Iter: 1048 loss: 0.000201836447
Iter: 1049 loss: 0.000194447653
Iter: 1050 loss: 0.000193861226
Iter: 1051 loss: 0.00019452619
Iter: 1052 loss: 0.000193547021
Iter: 1053 loss: 0.000193042782
Iter: 1054 loss: 0.000192333231
Iter: 1055 loss: 0.000192303545
Iter: 1056 loss: 0.000191376515
Iter: 1057 loss: 0.000195414264
Iter: 1058 loss: 0.000191192827
Iter: 1059 loss: 0.000190308871
Iter: 1060 loss: 0.000195771398
Iter: 1061 loss: 0.000190205799
Iter: 1062 loss: 0.000189492508
Iter: 1063 loss: 0.000190267761
Iter: 1064 loss: 0.000189103419
Iter: 1065 loss: 0.00018831926
Iter: 1066 loss: 0.00018993394
Iter: 1067 loss: 0.000188007602
Iter: 1068 loss: 0.000187126832
Iter: 1069 loss: 0.000192133317
Iter: 1070 loss: 0.000187000274
Iter: 1071 loss: 0.000186371879
Iter: 1072 loss: 0.000186796198
Iter: 1073 loss: 0.000185978512
Iter: 1074 loss: 0.000185098397
Iter: 1075 loss: 0.000186035293
Iter: 1076 loss: 0.000184614793
Iter: 1077 loss: 0.000183817654
Iter: 1078 loss: 0.000189622777
Iter: 1079 loss: 0.000183753684
Iter: 1080 loss: 0.000183000768
Iter: 1081 loss: 0.000184830264
Iter: 1082 loss: 0.000182724587
Iter: 1083 loss: 0.000182395743
Iter: 1084 loss: 0.000182341333
Iter: 1085 loss: 0.000181995711
Iter: 1086 loss: 0.000181689218
Iter: 1087 loss: 0.000181598763
Iter: 1088 loss: 0.000180993491
Iter: 1089 loss: 0.000180279312
Iter: 1090 loss: 0.0001801982
Iter: 1091 loss: 0.000179378811
Iter: 1092 loss: 0.000182636752
Iter: 1093 loss: 0.000179193128
Iter: 1094 loss: 0.000178353424
Iter: 1095 loss: 0.000182619755
Iter: 1096 loss: 0.000178213988
Iter: 1097 loss: 0.000177511567
Iter: 1098 loss: 0.000180516
Iter: 1099 loss: 0.000177367052
Iter: 1100 loss: 0.000176785936
Iter: 1101 loss: 0.000176526955
Iter: 1102 loss: 0.000176232832
Iter: 1103 loss: 0.000175465015
Iter: 1104 loss: 0.000181476877
Iter: 1105 loss: 0.000175409019
Iter: 1106 loss: 0.000174744884
Iter: 1107 loss: 0.00017590748
Iter: 1108 loss: 0.000174456451
Iter: 1109 loss: 0.000173716486
Iter: 1110 loss: 0.00017393734
Iter: 1111 loss: 0.0001731831
Iter: 1112 loss: 0.000172381086
Iter: 1113 loss: 0.000177790731
Iter: 1114 loss: 0.000172303917
Iter: 1115 loss: 0.000171654581
Iter: 1116 loss: 0.000175366426
Iter: 1117 loss: 0.000171566207
Iter: 1118 loss: 0.000170956308
Iter: 1119 loss: 0.000176417496
Iter: 1120 loss: 0.000170927466
Iter: 1121 loss: 0.000170526444
Iter: 1122 loss: 0.000170390762
Iter: 1123 loss: 0.000170160682
Iter: 1124 loss: 0.00016957974
Iter: 1125 loss: 0.000168886909
Iter: 1126 loss: 0.000168815546
Iter: 1127 loss: 0.000168031576
Iter: 1128 loss: 0.000176556743
Iter: 1129 loss: 0.000168013765
Iter: 1130 loss: 0.000167419144
Iter: 1131 loss: 0.000168378305
Iter: 1132 loss: 0.000167146121
Iter: 1133 loss: 0.000166473124
Iter: 1134 loss: 0.000170416984
Iter: 1135 loss: 0.000166383936
Iter: 1136 loss: 0.000165854202
Iter: 1137 loss: 0.000165752252
Iter: 1138 loss: 0.000165397971
Iter: 1139 loss: 0.000164598576
Iter: 1140 loss: 0.000168182582
Iter: 1141 loss: 0.000164441386
Iter: 1142 loss: 0.000163853503
Iter: 1143 loss: 0.000166984188
Iter: 1144 loss: 0.000163761899
Iter: 1145 loss: 0.000163186633
Iter: 1146 loss: 0.000163268327
Iter: 1147 loss: 0.000162746874
Iter: 1148 loss: 0.000162174911
Iter: 1149 loss: 0.000166485406
Iter: 1150 loss: 0.000162133307
Iter: 1151 loss: 0.000161770527
Iter: 1152 loss: 0.000161770411
Iter: 1153 loss: 0.000161441654
Iter: 1154 loss: 0.000161186443
Iter: 1155 loss: 0.00016108845
Iter: 1156 loss: 0.000160642434
Iter: 1157 loss: 0.00016115277
Iter: 1158 loss: 0.0001604055
Iter: 1159 loss: 0.00015986072
Iter: 1160 loss: 0.000159582167
Iter: 1161 loss: 0.000159331772
Iter: 1162 loss: 0.000158589304
Iter: 1163 loss: 0.000164222874
Iter: 1164 loss: 0.000158530747
Iter: 1165 loss: 0.000158044539
Iter: 1166 loss: 0.000160563577
Iter: 1167 loss: 0.000157965711
Iter: 1168 loss: 0.000157437
Iter: 1169 loss: 0.000157655071
Iter: 1170 loss: 0.000157072238
Iter: 1171 loss: 0.00015646001
Iter: 1172 loss: 0.0001591935
Iter: 1173 loss: 0.000156342037
Iter: 1174 loss: 0.000155818125
Iter: 1175 loss: 0.000156727489
Iter: 1176 loss: 0.000155580259
Iter: 1177 loss: 0.00015499095
Iter: 1178 loss: 0.000158026203
Iter: 1179 loss: 0.00015489987
Iter: 1180 loss: 0.000154470385
Iter: 1181 loss: 0.000155110567
Iter: 1182 loss: 0.000154267647
Iter: 1183 loss: 0.000153829489
Iter: 1184 loss: 0.000157436545
Iter: 1185 loss: 0.00015380187
Iter: 1186 loss: 0.000153310248
Iter: 1187 loss: 0.000154581212
Iter: 1188 loss: 0.000153145331
Iter: 1189 loss: 0.000152820532
Iter: 1190 loss: 0.00015274396
Iter: 1191 loss: 0.000152535504
Iter: 1192 loss: 0.000151995278
Iter: 1193 loss: 0.00015207252
Iter: 1194 loss: 0.000151586079
Iter: 1195 loss: 0.000151077111
Iter: 1196 loss: 0.000152485503
Iter: 1197 loss: 0.000150914624
Iter: 1198 loss: 0.000150310312
Iter: 1199 loss: 0.000151893415
Iter: 1200 loss: 0.000150106425
Iter: 1201 loss: 0.000149554748
Iter: 1202 loss: 0.000153472356
Iter: 1203 loss: 0.000149507876
Iter: 1204 loss: 0.000149051804
Iter: 1205 loss: 0.000149292435
Iter: 1206 loss: 0.000148749066
Iter: 1207 loss: 0.000148174338
Iter: 1208 loss: 0.00014971412
Iter: 1209 loss: 0.000147983039
Iter: 1210 loss: 0.00014746809
Iter: 1211 loss: 0.000150231455
Iter: 1212 loss: 0.000147387036
Iter: 1213 loss: 0.000146905892
Iter: 1214 loss: 0.000147515166
Iter: 1215 loss: 0.00014665554
Iter: 1216 loss: 0.00014618873
Iter: 1217 loss: 0.000148457562
Iter: 1218 loss: 0.000146103746
Iter: 1219 loss: 0.000145777798
Iter: 1220 loss: 0.00014577304
Iter: 1221 loss: 0.000145584883
Iter: 1222 loss: 0.00014520729
Iter: 1223 loss: 0.000152027947
Iter: 1224 loss: 0.000145201629
Iter: 1225 loss: 0.000144742953
Iter: 1226 loss: 0.000146345279
Iter: 1227 loss: 0.000144628531
Iter: 1228 loss: 0.000144222402
Iter: 1229 loss: 0.000144154212
Iter: 1230 loss: 0.000143873971
Iter: 1231 loss: 0.000143271012
Iter: 1232 loss: 0.000146340637
Iter: 1233 loss: 0.000143169455
Iter: 1234 loss: 0.000142723555
Iter: 1235 loss: 0.000143744241
Iter: 1236 loss: 0.000142558391
Iter: 1237 loss: 0.00014199916
Iter: 1238 loss: 0.000144246311
Iter: 1239 loss: 0.000141878671
Iter: 1240 loss: 0.000141465236
Iter: 1241 loss: 0.000141758414
Iter: 1242 loss: 0.000141213779
Iter: 1243 loss: 0.000140640826
Iter: 1244 loss: 0.000142026111
Iter: 1245 loss: 0.000140437594
Iter: 1246 loss: 0.000139885029
Iter: 1247 loss: 0.000143187048
Iter: 1248 loss: 0.000139812779
Iter: 1249 loss: 0.000139396201
Iter: 1250 loss: 0.000139757161
Iter: 1251 loss: 0.0001391572
Iter: 1252 loss: 0.000138947595
Iter: 1253 loss: 0.000138886055
Iter: 1254 loss: 0.00013865839
Iter: 1255 loss: 0.000138335614
Iter: 1256 loss: 0.000138324627
Iter: 1257 loss: 0.000137970113
Iter: 1258 loss: 0.000138451607
Iter: 1259 loss: 0.000137789058
Iter: 1260 loss: 0.000137359966
Iter: 1261 loss: 0.00013811051
Iter: 1262 loss: 0.000137169176
Iter: 1263 loss: 0.000136725837
Iter: 1264 loss: 0.000137749274
Iter: 1265 loss: 0.000136558083
Iter: 1266 loss: 0.000136066985
Iter: 1267 loss: 0.000136506802
Iter: 1268 loss: 0.000135777926
Iter: 1269 loss: 0.000135290364
Iter: 1270 loss: 0.00013989085
Iter: 1271 loss: 0.000135268463
Iter: 1272 loss: 0.000134864385
Iter: 1273 loss: 0.000135836715
Iter: 1274 loss: 0.000134715461
Iter: 1275 loss: 0.000134278103
Iter: 1276 loss: 0.000134394344
Iter: 1277 loss: 0.000133960333
Iter: 1278 loss: 0.000133506721
Iter: 1279 loss: 0.00013769287
Iter: 1280 loss: 0.00013348824
Iter: 1281 loss: 0.000133063921
Iter: 1282 loss: 0.000133485577
Iter: 1283 loss: 0.000132824789
Iter: 1284 loss: 0.000132526242
Iter: 1285 loss: 0.00013698524
Iter: 1286 loss: 0.000132525427
Iter: 1287 loss: 0.000132207904
Iter: 1288 loss: 0.000132970774
Iter: 1289 loss: 0.00013209357
Iter: 1290 loss: 0.000131840061
Iter: 1291 loss: 0.000131465058
Iter: 1292 loss: 0.000131457695
Iter: 1293 loss: 0.000131077715
Iter: 1294 loss: 0.000134037895
Iter: 1295 loss: 0.000131050547
Iter: 1296 loss: 0.000130731612
Iter: 1297 loss: 0.000130534667
Iter: 1298 loss: 0.000130403074
Iter: 1299 loss: 0.000129921478
Iter: 1300 loss: 0.000132473637
Iter: 1301 loss: 0.000129849708
Iter: 1302 loss: 0.000129457665
Iter: 1303 loss: 0.000129969791
Iter: 1304 loss: 0.00012925826
Iter: 1305 loss: 0.00012879631
Iter: 1306 loss: 0.000131707086
Iter: 1307 loss: 0.000128740838
Iter: 1308 loss: 0.000128358603
Iter: 1309 loss: 0.000129169362
Iter: 1310 loss: 0.000128210755
Iter: 1311 loss: 0.000127821026
Iter: 1312 loss: 0.000128236163
Iter: 1313 loss: 0.000127605832
Iter: 1314 loss: 0.000127178559
Iter: 1315 loss: 0.000130045
Iter: 1316 loss: 0.000127133811
Iter: 1317 loss: 0.000126770639
Iter: 1318 loss: 0.000127887906
Iter: 1319 loss: 0.000126665123
Iter: 1320 loss: 0.000126400439
Iter: 1321 loss: 0.000126399682
Iter: 1322 loss: 0.000126146726
Iter: 1323 loss: 0.000125772567
Iter: 1324 loss: 0.000125762614
Iter: 1325 loss: 0.000125480612
Iter: 1326 loss: 0.000125532111
Iter: 1327 loss: 0.000125265695
Iter: 1328 loss: 0.000124804443
Iter: 1329 loss: 0.000126650499
Iter: 1330 loss: 0.000124701299
Iter: 1331 loss: 0.000124381142
Iter: 1332 loss: 0.000124877741
Iter: 1333 loss: 0.000124234735
Iter: 1334 loss: 0.000123838632
Iter: 1335 loss: 0.000124393337
Iter: 1336 loss: 0.000123641425
Iter: 1337 loss: 0.000123303893
Iter: 1338 loss: 0.00012573952
Iter: 1339 loss: 0.000123273348
Iter: 1340 loss: 0.000122953934
Iter: 1341 loss: 0.000123378355
Iter: 1342 loss: 0.000122789148
Iter: 1343 loss: 0.000122416401
Iter: 1344 loss: 0.000123700811
Iter: 1345 loss: 0.000122317113
Iter: 1346 loss: 0.000121988065
Iter: 1347 loss: 0.000122112426
Iter: 1348 loss: 0.000121758159
Iter: 1349 loss: 0.000121373065
Iter: 1350 loss: 0.000125135644
Iter: 1351 loss: 0.000121359248
Iter: 1352 loss: 0.000121093188
Iter: 1353 loss: 0.000122793906
Iter: 1354 loss: 0.000121063291
Iter: 1355 loss: 0.000120815545
Iter: 1356 loss: 0.000122389349
Iter: 1357 loss: 0.00012078794
Iter: 1358 loss: 0.000120638222
Iter: 1359 loss: 0.000120311473
Iter: 1360 loss: 0.000125484745
Iter: 1361 loss: 0.000120300763
Iter: 1362 loss: 0.000119958408
Iter: 1363 loss: 0.000121619516
Iter: 1364 loss: 0.000119896387
Iter: 1365 loss: 0.000119584132
Iter: 1366 loss: 0.000120081109
Iter: 1367 loss: 0.000119434626
Iter: 1368 loss: 0.000119064891
Iter: 1369 loss: 0.000120621917
Iter: 1370 loss: 0.000118981858
Iter: 1371 loss: 0.000118702905
Iter: 1372 loss: 0.000118596676
Iter: 1373 loss: 0.000118443895
Iter: 1374 loss: 0.000118101685
Iter: 1375 loss: 0.000122379599
Iter: 1376 loss: 0.000118098731
Iter: 1377 loss: 0.000117843854
Iter: 1378 loss: 0.000118141645
Iter: 1379 loss: 0.000117705909
Iter: 1380 loss: 0.000117385971
Iter: 1381 loss: 0.000118217729
Iter: 1382 loss: 0.000117275718
Iter: 1383 loss: 0.000116993782
Iter: 1384 loss: 0.000117562158
Iter: 1385 loss: 0.000116881805
Iter: 1386 loss: 0.000116563358
Iter: 1387 loss: 0.000118498298
Iter: 1388 loss: 0.000116518713
Iter: 1389 loss: 0.000116313713
Iter: 1390 loss: 0.000116313313
Iter: 1391 loss: 0.000116164367
Iter: 1392 loss: 0.00011603957
Iter: 1393 loss: 0.000115995914
Iter: 1394 loss: 0.000115757117
Iter: 1395 loss: 0.000115517694
Iter: 1396 loss: 0.00011546928
Iter: 1397 loss: 0.000115160648
Iter: 1398 loss: 0.000116727919
Iter: 1399 loss: 0.000115113035
Iter: 1400 loss: 0.000114788665
Iter: 1401 loss: 0.000115462
Iter: 1402 loss: 0.00011465975
Iter: 1403 loss: 0.000114337032
Iter: 1404 loss: 0.00011517774
Iter: 1405 loss: 0.000114230468
Iter: 1406 loss: 0.000113939408
Iter: 1407 loss: 0.000114578987
Iter: 1408 loss: 0.000113825576
Iter: 1409 loss: 0.000113491464
Iter: 1410 loss: 0.000114203445
Iter: 1411 loss: 0.000113357113
Iter: 1412 loss: 0.000113084199
Iter: 1413 loss: 0.000115559138
Iter: 1414 loss: 0.000113071743
Iter: 1415 loss: 0.000112815978
Iter: 1416 loss: 0.000112717185
Iter: 1417 loss: 0.000112579357
Iter: 1418 loss: 0.000112265858
Iter: 1419 loss: 0.000114939685
Iter: 1420 loss: 0.000112249414
Iter: 1421 loss: 0.000112062065
Iter: 1422 loss: 0.00011434855
Iter: 1423 loss: 0.000112058937
Iter: 1424 loss: 0.000111860267
Iter: 1425 loss: 0.000111925023
Iter: 1426 loss: 0.000111721252
Iter: 1427 loss: 0.000111488087
Iter: 1428 loss: 0.000111629939
Iter: 1429 loss: 0.000111340385
Iter: 1430 loss: 0.000111113739
Iter: 1431 loss: 0.000111045141
Iter: 1432 loss: 0.000110912639
Iter: 1433 loss: 0.000110586319
Iter: 1434 loss: 0.000112553542
Iter: 1435 loss: 0.000110544745
Iter: 1436 loss: 0.000110308079
Iter: 1437 loss: 0.000110864028
Iter: 1438 loss: 0.000110221619
Iter: 1439 loss: 0.000109914079
Iter: 1440 loss: 0.000110171488
Iter: 1441 loss: 0.000109728746
Iter: 1442 loss: 0.000109425338
Iter: 1443 loss: 0.000110536566
Iter: 1444 loss: 0.000109346649
Iter: 1445 loss: 0.000109040877
Iter: 1446 loss: 0.000109706016
Iter: 1447 loss: 0.000108923618
Iter: 1448 loss: 0.000108628941
Iter: 1449 loss: 0.000110313311
Iter: 1450 loss: 0.000108590131
Iter: 1451 loss: 0.000108336244
Iter: 1452 loss: 0.00010904776
Iter: 1453 loss: 0.000108249522
Iter: 1454 loss: 0.000108017339
Iter: 1455 loss: 0.000109010929
Iter: 1456 loss: 0.000107970562
Iter: 1457 loss: 0.000107753454
Iter: 1458 loss: 0.000110262219
Iter: 1459 loss: 0.000107746848
Iter: 1460 loss: 0.000107627449
Iter: 1461 loss: 0.000107543252
Iter: 1462 loss: 0.000107500789
Iter: 1463 loss: 0.000107286556
Iter: 1464 loss: 0.000107101368
Iter: 1465 loss: 0.000107042171
Iter: 1466 loss: 0.000106767911
Iter: 1467 loss: 0.000108149907
Iter: 1468 loss: 0.0001067216
Iter: 1469 loss: 0.000106429812
Iter: 1470 loss: 0.000106456486
Iter: 1471 loss: 0.000106205087
Iter: 1472 loss: 0.000105937266
Iter: 1473 loss: 0.00010982345
Iter: 1474 loss: 0.000105935571
Iter: 1475 loss: 0.000105727973
Iter: 1476 loss: 0.000105513114
Iter: 1477 loss: 0.000105474428
Iter: 1478 loss: 0.000105133295
Iter: 1479 loss: 0.000107231048
Iter: 1480 loss: 0.000105091545
Iter: 1481 loss: 0.000104827202
Iter: 1482 loss: 0.00010568228
Iter: 1483 loss: 0.000104755163
Iter: 1484 loss: 0.000104462873
Iter: 1485 loss: 0.000105341795
Iter: 1486 loss: 0.000104375271
Iter: 1487 loss: 0.000104132028
Iter: 1488 loss: 0.000105627085
Iter: 1489 loss: 0.000104099687
Iter: 1490 loss: 0.000103948609
Iter: 1491 loss: 0.000103950442
Iter: 1492 loss: 0.000103826489
Iter: 1493 loss: 0.000103668164
Iter: 1494 loss: 0.000103657745
Iter: 1495 loss: 0.00010345687
Iter: 1496 loss: 0.00010397131
Iter: 1497 loss: 0.000103390412
Iter: 1498 loss: 0.000103222177
Iter: 1499 loss: 0.00010330946
Iter: 1500 loss: 0.000103114457
Iter: 1501 loss: 0.000102886261
Iter: 1502 loss: 0.000103085607
Iter: 1503 loss: 0.000102750448
Iter: 1504 loss: 0.000102517202
Iter: 1505 loss: 0.00010399193
Iter: 1506 loss: 0.000102489474
Iter: 1507 loss: 0.000102243161
Iter: 1508 loss: 0.000102455946
Iter: 1509 loss: 0.000102095059
Iter: 1510 loss: 0.000101860336
Iter: 1511 loss: 0.000102806196
Iter: 1512 loss: 0.000101803817
Iter: 1513 loss: 0.000101561876
Iter: 1514 loss: 0.000101842103
Iter: 1515 loss: 0.000101427286
Iter: 1516 loss: 0.000101201644
Iter: 1517 loss: 0.000103072256
Iter: 1518 loss: 0.000101187943
Iter: 1519 loss: 0.000100994563
Iter: 1520 loss: 0.000101503501
Iter: 1521 loss: 0.000100931364
Iter: 1522 loss: 0.000100793841
Iter: 1523 loss: 0.000100794772
Iter: 1524 loss: 0.000100668432
Iter: 1525 loss: 0.000100764344
Iter: 1526 loss: 0.000100595593
Iter: 1527 loss: 0.000100475663
Iter: 1528 loss: 0.000100456899
Iter: 1529 loss: 0.000100377598
Iter: 1530 loss: 0.000100167425
Iter: 1531 loss: 0.000100196485
Iter: 1532 loss: 0.000100010962
Iter: 1533 loss: 9.97896786e-05
Iter: 1534 loss: 0.000100609628
Iter: 1535 loss: 9.97372772e-05
Iter: 1536 loss: 9.9523786e-05
Iter: 1537 loss: 9.98197793e-05
Iter: 1538 loss: 9.94194561e-05
Iter: 1539 loss: 9.9192679e-05
Iter: 1540 loss: 0.000100395555
Iter: 1541 loss: 9.91553825e-05
Iter: 1542 loss: 9.89531e-05
Iter: 1543 loss: 9.92625137e-05
Iter: 1544 loss: 9.88549e-05
Iter: 1545 loss: 9.86177474e-05
Iter: 1546 loss: 9.91467241e-05
Iter: 1547 loss: 9.85277584e-05
Iter: 1548 loss: 9.83288264e-05
Iter: 1549 loss: 9.89032051e-05
Iter: 1550 loss: 9.82635102e-05
Iter: 1551 loss: 9.80562691e-05
Iter: 1552 loss: 9.94239963e-05
Iter: 1553 loss: 9.80357872e-05
Iter: 1554 loss: 9.78969692e-05
Iter: 1555 loss: 9.91555717e-05
Iter: 1556 loss: 9.78921453e-05
Iter: 1557 loss: 9.77705422e-05
Iter: 1558 loss: 9.82962883e-05
Iter: 1559 loss: 9.77472373e-05
Iter: 1560 loss: 9.76446609e-05
Iter: 1561 loss: 9.74855502e-05
Iter: 1562 loss: 9.74843933e-05
Iter: 1563 loss: 9.7285767e-05
Iter: 1564 loss: 9.82218189e-05
Iter: 1565 loss: 9.72506314e-05
Iter: 1566 loss: 9.70849505e-05
Iter: 1567 loss: 9.7035474e-05
Iter: 1568 loss: 9.69312896e-05
Iter: 1569 loss: 9.66910884e-05
Iter: 1570 loss: 9.76125812e-05
Iter: 1571 loss: 9.66363295e-05
Iter: 1572 loss: 9.64486535e-05
Iter: 1573 loss: 9.70215333e-05
Iter: 1574 loss: 9.63923521e-05
Iter: 1575 loss: 9.6182077e-05
Iter: 1576 loss: 9.68700188e-05
Iter: 1577 loss: 9.61228798e-05
Iter: 1578 loss: 9.59588579e-05
Iter: 1579 loss: 9.64147039e-05
Iter: 1580 loss: 9.59084646e-05
Iter: 1581 loss: 9.57296579e-05
Iter: 1582 loss: 9.58009623e-05
Iter: 1583 loss: 9.56054864e-05
Iter: 1584 loss: 9.54106217e-05
Iter: 1585 loss: 9.70422552e-05
Iter: 1586 loss: 9.5400821e-05
Iter: 1587 loss: 9.52666887e-05
Iter: 1588 loss: 9.65359795e-05
Iter: 1589 loss: 9.52597329e-05
Iter: 1590 loss: 9.51433758e-05
Iter: 1591 loss: 9.5823576e-05
Iter: 1592 loss: 9.51318361e-05
Iter: 1593 loss: 9.50327667e-05
Iter: 1594 loss: 9.49349924e-05
Iter: 1595 loss: 9.49136156e-05
Iter: 1596 loss: 9.47761582e-05
Iter: 1597 loss: 9.50717076e-05
Iter: 1598 loss: 9.47211083e-05
Iter: 1599 loss: 9.45534848e-05
Iter: 1600 loss: 9.46700311e-05
Iter: 1601 loss: 9.44484054e-05
Iter: 1602 loss: 9.42671904e-05
Iter: 1603 loss: 9.48284069e-05
Iter: 1604 loss: 9.42158076e-05
Iter: 1605 loss: 9.4052928e-05
Iter: 1606 loss: 9.41272083e-05
Iter: 1607 loss: 9.39433667e-05
Iter: 1608 loss: 9.37124e-05
Iter: 1609 loss: 9.52424598e-05
Iter: 1610 loss: 9.36890865e-05
Iter: 1611 loss: 9.35230346e-05
Iter: 1612 loss: 9.3767565e-05
Iter: 1613 loss: 9.34410564e-05
Iter: 1614 loss: 9.32223775e-05
Iter: 1615 loss: 9.3486582e-05
Iter: 1616 loss: 9.31068644e-05
Iter: 1617 loss: 9.29227899e-05
Iter: 1618 loss: 9.40614918e-05
Iter: 1619 loss: 9.28975787e-05
Iter: 1620 loss: 9.27389046e-05
Iter: 1621 loss: 9.36959041e-05
Iter: 1622 loss: 9.27214714e-05
Iter: 1623 loss: 9.26007124e-05
Iter: 1624 loss: 9.41621838e-05
Iter: 1625 loss: 9.25999266e-05
Iter: 1626 loss: 9.25147324e-05
Iter: 1627 loss: 9.24926426e-05
Iter: 1628 loss: 9.243963e-05
Iter: 1629 loss: 9.23265252e-05
Iter: 1630 loss: 9.22953477e-05
Iter: 1631 loss: 9.22271065e-05
Iter: 1632 loss: 9.20596867e-05
Iter: 1633 loss: 9.27801739e-05
Iter: 1634 loss: 9.20252496e-05
Iter: 1635 loss: 9.18937949e-05
Iter: 1636 loss: 9.19636732e-05
Iter: 1637 loss: 9.18062797e-05
Iter: 1638 loss: 9.16273129e-05
Iter: 1639 loss: 9.18153091e-05
Iter: 1640 loss: 9.15306446e-05
Iter: 1641 loss: 9.13493132e-05
Iter: 1642 loss: 9.27414512e-05
Iter: 1643 loss: 9.13345e-05
Iter: 1644 loss: 9.11871466e-05
Iter: 1645 loss: 9.13541808e-05
Iter: 1646 loss: 9.11074094e-05
Iter: 1647 loss: 9.09202427e-05
Iter: 1648 loss: 9.16127319e-05
Iter: 1649 loss: 9.0872054e-05
Iter: 1650 loss: 9.07367066e-05
Iter: 1651 loss: 9.07460344e-05
Iter: 1652 loss: 9.0631067e-05
Iter: 1653 loss: 9.04835615e-05
Iter: 1654 loss: 9.04835615e-05
Iter: 1655 loss: 9.03976834e-05
Iter: 1656 loss: 9.13146068e-05
Iter: 1657 loss: 9.03913606e-05
Iter: 1658 loss: 9.03114415e-05
Iter: 1659 loss: 9.02447719e-05
Iter: 1660 loss: 9.0221045e-05
Iter: 1661 loss: 9.00832529e-05
Iter: 1662 loss: 9.01642488e-05
Iter: 1663 loss: 8.99943552e-05
Iter: 1664 loss: 8.98510116e-05
Iter: 1665 loss: 9.032659e-05
Iter: 1666 loss: 8.9813926e-05
Iter: 1667 loss: 8.966311e-05
Iter: 1668 loss: 8.97245554e-05
Iter: 1669 loss: 8.95605917e-05
Iter: 1670 loss: 8.93856632e-05
Iter: 1671 loss: 8.98928556e-05
Iter: 1672 loss: 8.93311517e-05
Iter: 1673 loss: 8.91812379e-05
Iter: 1674 loss: 8.95897101e-05
Iter: 1675 loss: 8.91287418e-05
Iter: 1676 loss: 8.8957e-05
Iter: 1677 loss: 8.95552221e-05
Iter: 1678 loss: 8.89112634e-05
Iter: 1679 loss: 8.87663555e-05
Iter: 1680 loss: 8.93360411e-05
Iter: 1681 loss: 8.87315691e-05
Iter: 1682 loss: 8.85909612e-05
Iter: 1683 loss: 8.85120244e-05
Iter: 1684 loss: 8.84489e-05
Iter: 1685 loss: 8.82944441e-05
Iter: 1686 loss: 9.0795038e-05
Iter: 1687 loss: 8.82937238e-05
Iter: 1688 loss: 8.81888e-05
Iter: 1689 loss: 8.94724217e-05
Iter: 1690 loss: 8.81873639e-05
Iter: 1691 loss: 8.80891603e-05
Iter: 1692 loss: 8.8128123e-05
Iter: 1693 loss: 8.8022105e-05
Iter: 1694 loss: 8.7909837e-05
Iter: 1695 loss: 8.80109146e-05
Iter: 1696 loss: 8.78431456e-05
Iter: 1697 loss: 8.77336497e-05
Iter: 1698 loss: 8.79084473e-05
Iter: 1699 loss: 8.76816193e-05
Iter: 1700 loss: 8.75413898e-05
Iter: 1701 loss: 8.77548882e-05
Iter: 1702 loss: 8.74715333e-05
Iter: 1703 loss: 8.73436875e-05
Iter: 1704 loss: 8.75745245e-05
Iter: 1705 loss: 8.72876699e-05
Iter: 1706 loss: 8.71370503e-05
Iter: 1707 loss: 8.73258905e-05
Iter: 1708 loss: 8.70591175e-05
Iter: 1709 loss: 8.68824063e-05
Iter: 1710 loss: 8.78284918e-05
Iter: 1711 loss: 8.68538264e-05
Iter: 1712 loss: 8.670635e-05
Iter: 1713 loss: 8.71047741e-05
Iter: 1714 loss: 8.66534247e-05
Iter: 1715 loss: 8.64926405e-05
Iter: 1716 loss: 8.660819e-05
Iter: 1717 loss: 8.63919e-05
Iter: 1718 loss: 8.6232074e-05
Iter: 1719 loss: 8.73420286e-05
Iter: 1720 loss: 8.62178567e-05
Iter: 1721 loss: 8.61040753e-05
Iter: 1722 loss: 8.76667909e-05
Iter: 1723 loss: 8.61029112e-05
Iter: 1724 loss: 8.60073487e-05
Iter: 1725 loss: 8.62821616e-05
Iter: 1726 loss: 8.59746651e-05
Iter: 1727 loss: 8.59005231e-05
Iter: 1728 loss: 8.59230786e-05
Iter: 1729 loss: 8.58495187e-05
Iter: 1730 loss: 8.57500127e-05
Iter: 1731 loss: 8.57249397e-05
Iter: 1732 loss: 8.5662643e-05
Iter: 1733 loss: 8.55228354e-05
Iter: 1734 loss: 8.6302447e-05
Iter: 1735 loss: 8.55016187e-05
Iter: 1736 loss: 8.5398744e-05
Iter: 1737 loss: 8.53411475e-05
Iter: 1738 loss: 8.52971498e-05
Iter: 1739 loss: 8.51303193e-05
Iter: 1740 loss: 8.5750522e-05
Iter: 1741 loss: 8.5090971e-05
Iter: 1742 loss: 8.49494099e-05
Iter: 1743 loss: 8.53710517e-05
Iter: 1744 loss: 8.49057469e-05
Iter: 1745 loss: 8.47419506e-05
Iter: 1746 loss: 8.51396908e-05
Iter: 1747 loss: 8.46839539e-05
Iter: 1748 loss: 8.45288159e-05
Iter: 1749 loss: 8.50124343e-05
Iter: 1750 loss: 8.44819515e-05
Iter: 1751 loss: 8.43382295e-05
Iter: 1752 loss: 8.44837341e-05
Iter: 1753 loss: 8.42606751e-05
Iter: 1754 loss: 8.41765723e-05
Iter: 1755 loss: 8.41578e-05
Iter: 1756 loss: 8.40759749e-05
Iter: 1757 loss: 8.42658483e-05
Iter: 1758 loss: 8.40477733e-05
Iter: 1759 loss: 8.3964107e-05
Iter: 1760 loss: 8.39599525e-05
Iter: 1761 loss: 8.38970227e-05
Iter: 1762 loss: 8.37840344e-05
Iter: 1763 loss: 8.3879735e-05
Iter: 1764 loss: 8.37152475e-05
Iter: 1765 loss: 8.36004474e-05
Iter: 1766 loss: 8.41876827e-05
Iter: 1767 loss: 8.35850078e-05
Iter: 1768 loss: 8.34835082e-05
Iter: 1769 loss: 8.33946251e-05
Iter: 1770 loss: 8.33677404e-05
Iter: 1771 loss: 8.32122532e-05
Iter: 1772 loss: 8.40549619e-05
Iter: 1773 loss: 8.31866782e-05
Iter: 1774 loss: 8.30457211e-05
Iter: 1775 loss: 8.31652214e-05
Iter: 1776 loss: 8.2959843e-05
Iter: 1777 loss: 8.27971453e-05
Iter: 1778 loss: 8.41500878e-05
Iter: 1779 loss: 8.27850454e-05
Iter: 1780 loss: 8.26667238e-05
Iter: 1781 loss: 8.27632393e-05
Iter: 1782 loss: 8.25967509e-05
Iter: 1783 loss: 8.24450908e-05
Iter: 1784 loss: 8.27749172e-05
Iter: 1785 loss: 8.238788e-05
Iter: 1786 loss: 8.22855582e-05
Iter: 1787 loss: 8.2287e-05
Iter: 1788 loss: 8.21907888e-05
Iter: 1789 loss: 8.258444e-05
Iter: 1790 loss: 8.21713547e-05
Iter: 1791 loss: 8.20968417e-05
Iter: 1792 loss: 8.21308349e-05
Iter: 1793 loss: 8.20438436e-05
Iter: 1794 loss: 8.19523339e-05
Iter: 1795 loss: 8.20178248e-05
Iter: 1796 loss: 8.18982371e-05
Iter: 1797 loss: 8.17932232e-05
Iter: 1798 loss: 8.20902278e-05
Iter: 1799 loss: 8.17561522e-05
Iter: 1800 loss: 8.1633676e-05
Iter: 1801 loss: 8.16432585e-05
Iter: 1802 loss: 8.15398089e-05
Iter: 1803 loss: 8.14018858e-05
Iter: 1804 loss: 8.19390116e-05
Iter: 1805 loss: 8.1368984e-05
Iter: 1806 loss: 8.12284561e-05
Iter: 1807 loss: 8.13145e-05
Iter: 1808 loss: 8.11377759e-05
Iter: 1809 loss: 8.10072233e-05
Iter: 1810 loss: 8.26107862e-05
Iter: 1811 loss: 8.10074125e-05
Iter: 1812 loss: 8.08971163e-05
Iter: 1813 loss: 8.08920449e-05
Iter: 1814 loss: 8.08092882e-05
Iter: 1815 loss: 8.06636526e-05
Iter: 1816 loss: 8.12243379e-05
Iter: 1817 loss: 8.06288954e-05
Iter: 1818 loss: 8.05268937e-05
Iter: 1819 loss: 8.15668463e-05
Iter: 1820 loss: 8.0520811e-05
Iter: 1821 loss: 8.04252268e-05
Iter: 1822 loss: 8.10574275e-05
Iter: 1823 loss: 8.04163283e-05
Iter: 1824 loss: 8.03436778e-05
Iter: 1825 loss: 8.03541479e-05
Iter: 1826 loss: 8.02879804e-05
Iter: 1827 loss: 8.0198748e-05
Iter: 1828 loss: 8.02700961e-05
Iter: 1829 loss: 8.01412825e-05
Iter: 1830 loss: 8.00351481e-05
Iter: 1831 loss: 8.01992e-05
Iter: 1832 loss: 7.9984311e-05
Iter: 1833 loss: 7.98539622e-05
Iter: 1834 loss: 8.01063e-05
Iter: 1835 loss: 7.97981193e-05
Iter: 1836 loss: 7.96875102e-05
Iter: 1837 loss: 7.9855512e-05
Iter: 1838 loss: 7.96341919e-05
Iter: 1839 loss: 7.94957887e-05
Iter: 1840 loss: 7.97473767e-05
Iter: 1841 loss: 7.94373336e-05
Iter: 1842 loss: 7.93124709e-05
Iter: 1843 loss: 8.00060079e-05
Iter: 1844 loss: 7.92961073e-05
Iter: 1845 loss: 7.91709608e-05
Iter: 1846 loss: 7.93684e-05
Iter: 1847 loss: 7.91137572e-05
Iter: 1848 loss: 7.89981277e-05
Iter: 1849 loss: 7.94302832e-05
Iter: 1850 loss: 7.89708283e-05
Iter: 1851 loss: 7.88612233e-05
Iter: 1852 loss: 7.90912236e-05
Iter: 1853 loss: 7.88169491e-05
Iter: 1854 loss: 7.87348326e-05
Iter: 1855 loss: 7.87273311e-05
Iter: 1856 loss: 7.86743694e-05
Iter: 1857 loss: 7.86405508e-05
Iter: 1858 loss: 7.86219316e-05
Iter: 1859 loss: 7.85298471e-05
Iter: 1860 loss: 7.85712909e-05
Iter: 1861 loss: 7.84720687e-05
Iter: 1862 loss: 7.83628639e-05
Iter: 1863 loss: 7.8566045e-05
Iter: 1864 loss: 7.83136929e-05
Iter: 1865 loss: 7.81947674e-05
Iter: 1866 loss: 7.85633456e-05
Iter: 1867 loss: 7.81599665e-05
Iter: 1868 loss: 7.80553382e-05
Iter: 1869 loss: 7.80532282e-05
Iter: 1870 loss: 7.79731636e-05
Iter: 1871 loss: 7.78242e-05
Iter: 1872 loss: 7.84486911e-05
Iter: 1873 loss: 7.77950117e-05
Iter: 1874 loss: 7.76720844e-05
Iter: 1875 loss: 7.78937756e-05
Iter: 1876 loss: 7.76205125e-05
Iter: 1877 loss: 7.7476041e-05
Iter: 1878 loss: 7.83672804e-05
Iter: 1879 loss: 7.74578148e-05
Iter: 1880 loss: 7.73539068e-05
Iter: 1881 loss: 7.73828215e-05
Iter: 1882 loss: 7.72774074e-05
Iter: 1883 loss: 7.7129509e-05
Iter: 1884 loss: 7.77438545e-05
Iter: 1885 loss: 7.70969491e-05
Iter: 1886 loss: 7.70592305e-05
Iter: 1887 loss: 7.70314946e-05
Iter: 1888 loss: 7.69826875e-05
Iter: 1889 loss: 7.69296748e-05
Iter: 1890 loss: 7.6921322e-05
Iter: 1891 loss: 7.6831e-05
Iter: 1892 loss: 7.69524413e-05
Iter: 1893 loss: 7.6783821e-05
Iter: 1894 loss: 7.66939e-05
Iter: 1895 loss: 7.68298414e-05
Iter: 1896 loss: 7.66502781e-05
Iter: 1897 loss: 7.65486911e-05
Iter: 1898 loss: 7.67965394e-05
Iter: 1899 loss: 7.65147561e-05
Iter: 1900 loss: 7.64103897e-05
Iter: 1901 loss: 7.64725773e-05
Iter: 1902 loss: 7.633926e-05
Iter: 1903 loss: 7.62261116e-05
Iter: 1904 loss: 7.66318699e-05
Iter: 1905 loss: 7.61958363e-05
Iter: 1906 loss: 7.60775583e-05
Iter: 1907 loss: 7.62367054e-05
Iter: 1908 loss: 7.60192779e-05
Iter: 1909 loss: 7.59041795e-05
Iter: 1910 loss: 7.70196057e-05
Iter: 1911 loss: 7.58991373e-05
Iter: 1912 loss: 7.58064125e-05
Iter: 1913 loss: 7.57697271e-05
Iter: 1914 loss: 7.57176895e-05
Iter: 1915 loss: 7.55895599e-05
Iter: 1916 loss: 7.6438635e-05
Iter: 1917 loss: 7.55759829e-05
Iter: 1918 loss: 7.55275105e-05
Iter: 1919 loss: 7.55214933e-05
Iter: 1920 loss: 7.54639041e-05
Iter: 1921 loss: 7.54106513e-05
Iter: 1922 loss: 7.53989589e-05
Iter: 1923 loss: 7.5307471e-05
Iter: 1924 loss: 7.55571818e-05
Iter: 1925 loss: 7.52777269e-05
Iter: 1926 loss: 7.52012129e-05
Iter: 1927 loss: 7.52817359e-05
Iter: 1928 loss: 7.51581101e-05
Iter: 1929 loss: 7.50652762e-05
Iter: 1930 loss: 7.52734195e-05
Iter: 1931 loss: 7.50264444e-05
Iter: 1932 loss: 7.49271276e-05
Iter: 1933 loss: 7.50676845e-05
Iter: 1934 loss: 7.48739913e-05
Iter: 1935 loss: 7.47708546e-05
Iter: 1936 loss: 7.49118626e-05
Iter: 1937 loss: 7.47201775e-05
Iter: 1938 loss: 7.45977304e-05
Iter: 1939 loss: 7.50315739e-05
Iter: 1940 loss: 7.45665893e-05
Iter: 1941 loss: 7.44645877e-05
Iter: 1942 loss: 7.49187457e-05
Iter: 1943 loss: 7.44449935e-05
Iter: 1944 loss: 7.43394921e-05
Iter: 1945 loss: 7.45090365e-05
Iter: 1946 loss: 7.42873744e-05
Iter: 1947 loss: 7.41778276e-05
Iter: 1948 loss: 7.44362624e-05
Iter: 1949 loss: 7.41375698e-05
Iter: 1950 loss: 7.40775722e-05
Iter: 1951 loss: 7.40709365e-05
Iter: 1952 loss: 7.40057876e-05
Iter: 1953 loss: 7.40100368e-05
Iter: 1954 loss: 7.39545794e-05
Iter: 1955 loss: 7.38785311e-05
Iter: 1956 loss: 7.41365802e-05
Iter: 1957 loss: 7.38606832e-05
Iter: 1958 loss: 7.37906666e-05
Iter: 1959 loss: 7.38676245e-05
Iter: 1960 loss: 7.37514929e-05
Iter: 1961 loss: 7.36676229e-05
Iter: 1962 loss: 7.37496739e-05
Iter: 1963 loss: 7.36215661e-05
Iter: 1964 loss: 7.35261565e-05
Iter: 1965 loss: 7.38096278e-05
Iter: 1966 loss: 7.34958303e-05
Iter: 1967 loss: 7.33925408e-05
Iter: 1968 loss: 7.34037458e-05
Iter: 1969 loss: 7.33129855e-05
Iter: 1970 loss: 7.31906621e-05
Iter: 1971 loss: 7.39014795e-05
Iter: 1972 loss: 7.3176765e-05
Iter: 1973 loss: 7.30808533e-05
Iter: 1974 loss: 7.3296178e-05
Iter: 1975 loss: 7.30433094e-05
Iter: 1976 loss: 7.29396925e-05
Iter: 1977 loss: 7.35245412e-05
Iter: 1978 loss: 7.29248932e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/500_500_500_500_1
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0
+ date
Mon Oct 26 14:24:03 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi -1 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd17224bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1722f0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd17224b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172344f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172363950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172363a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172335488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1722a1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172227c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172227ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172182950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1720fc1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172118268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1721189d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172110d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172110e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172110ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172110d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1720e8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd172007e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1720077b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd171fe98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd171fe9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd171fb3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd171f517b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd171f51598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd171f51f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd171f247b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd171ef2378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd148a18bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd171ef2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd148a06378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd148a06f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1489ad7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd14897a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd14897f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.17336321e-06
Iter: 2 loss: 2.63072343e-06
Iter: 3 loss: 2.58520095e-06
Iter: 4 loss: 2.31796207e-06
Iter: 5 loss: 2.61013724e-06
Iter: 6 loss: 2.17208299e-06
Iter: 7 loss: 2.0587479e-06
Iter: 8 loss: 2.65142262e-06
Iter: 9 loss: 2.04098433e-06
Iter: 10 loss: 1.96652536e-06
Iter: 11 loss: 2.88669389e-06
Iter: 12 loss: 1.96577912e-06
Iter: 13 loss: 1.91640402e-06
Iter: 14 loss: 1.87265289e-06
Iter: 15 loss: 1.85990007e-06
Iter: 16 loss: 1.83614418e-06
Iter: 17 loss: 1.96606106e-06
Iter: 18 loss: 1.83267139e-06
Iter: 19 loss: 1.79631638e-06
Iter: 20 loss: 1.76978983e-06
Iter: 21 loss: 1.75740047e-06
Iter: 22 loss: 1.7119437e-06
Iter: 23 loss: 1.64006781e-06
Iter: 24 loss: 1.63935226e-06
Iter: 25 loss: 1.68815905e-06
Iter: 26 loss: 1.61285175e-06
Iter: 27 loss: 1.5928324e-06
Iter: 28 loss: 1.55683e-06
Iter: 29 loss: 2.43391719e-06
Iter: 30 loss: 1.55683415e-06
Iter: 31 loss: 1.53084386e-06
Iter: 32 loss: 1.52488e-06
Iter: 33 loss: 1.50813753e-06
Iter: 34 loss: 1.500091e-06
Iter: 35 loss: 1.49784046e-06
Iter: 36 loss: 1.4856239e-06
Iter: 37 loss: 1.46477942e-06
Iter: 38 loss: 1.46474758e-06
Iter: 39 loss: 1.43217108e-06
Iter: 40 loss: 1.42294471e-06
Iter: 41 loss: 1.40306383e-06
Iter: 42 loss: 1.3854152e-06
Iter: 43 loss: 1.37816642e-06
Iter: 44 loss: 1.35859398e-06
Iter: 45 loss: 1.36876963e-06
Iter: 46 loss: 1.34568472e-06
Iter: 47 loss: 1.32249443e-06
Iter: 48 loss: 1.28778083e-06
Iter: 49 loss: 1.28701822e-06
Iter: 50 loss: 1.2811754e-06
Iter: 51 loss: 1.27150668e-06
Iter: 52 loss: 1.25662586e-06
Iter: 53 loss: 1.22647032e-06
Iter: 54 loss: 1.77753941e-06
Iter: 55 loss: 1.22600318e-06
Iter: 56 loss: 1.20220511e-06
Iter: 57 loss: 1.22410825e-06
Iter: 58 loss: 1.1884764e-06
Iter: 59 loss: 1.18665776e-06
Iter: 60 loss: 1.17527202e-06
Iter: 61 loss: 1.16713727e-06
Iter: 62 loss: 1.14570355e-06
Iter: 63 loss: 1.30115723e-06
Iter: 64 loss: 1.14112561e-06
Iter: 65 loss: 1.12031546e-06
Iter: 66 loss: 1.16906e-06
Iter: 67 loss: 1.11264126e-06
Iter: 68 loss: 1.098452e-06
Iter: 69 loss: 1.09751181e-06
Iter: 70 loss: 1.08652227e-06
Iter: 71 loss: 1.05653453e-06
Iter: 72 loss: 1.24222379e-06
Iter: 73 loss: 1.04863466e-06
Iter: 74 loss: 1.02515128e-06
Iter: 75 loss: 1.02503168e-06
Iter: 76 loss: 1.01026194e-06
Iter: 77 loss: 1.01024898e-06
Iter: 78 loss: 1.00213765e-06
Iter: 79 loss: 9.91882e-07
Iter: 80 loss: 9.91065235e-07
Iter: 81 loss: 9.78152116e-07
Iter: 82 loss: 1.00792522e-06
Iter: 83 loss: 9.73352257e-07
Iter: 84 loss: 9.64967398e-07
Iter: 85 loss: 9.64577907e-07
Iter: 86 loss: 9.60538273e-07
Iter: 87 loss: 9.48077513e-07
Iter: 88 loss: 9.75892135e-07
Iter: 89 loss: 9.40664734e-07
Iter: 90 loss: 9.25969516e-07
Iter: 91 loss: 1.15438024e-06
Iter: 92 loss: 9.25961899e-07
Iter: 93 loss: 9.12104667e-07
Iter: 94 loss: 1.00317106e-06
Iter: 95 loss: 9.10629183e-07
Iter: 96 loss: 9.05258958e-07
Iter: 97 loss: 8.95566529e-07
Iter: 98 loss: 1.13171336e-06
Iter: 99 loss: 8.95572043e-07
Iter: 100 loss: 8.92385515e-07
Iter: 101 loss: 8.90895251e-07
Iter: 102 loss: 8.85629049e-07
Iter: 103 loss: 8.76203103e-07
Iter: 104 loss: 1.10834048e-06
Iter: 105 loss: 8.76213619e-07
Iter: 106 loss: 8.67404083e-07
Iter: 107 loss: 8.74333296e-07
Iter: 108 loss: 8.62055629e-07
Iter: 109 loss: 8.5840361e-07
Iter: 110 loss: 8.56322572e-07
Iter: 111 loss: 8.51225764e-07
Iter: 112 loss: 8.4355645e-07
Iter: 113 loss: 8.4339581e-07
Iter: 114 loss: 8.35251626e-07
Iter: 115 loss: 8.67174947e-07
Iter: 116 loss: 8.33363117e-07
Iter: 117 loss: 8.30544764e-07
Iter: 118 loss: 8.30410158e-07
Iter: 119 loss: 8.27773249e-07
Iter: 120 loss: 8.23100095e-07
Iter: 121 loss: 8.23117773e-07
Iter: 122 loss: 8.18411252e-07
Iter: 123 loss: 8.12233679e-07
Iter: 124 loss: 8.11866073e-07
Iter: 125 loss: 8.09432379e-07
Iter: 126 loss: 8.06776711e-07
Iter: 127 loss: 8.03147259e-07
Iter: 128 loss: 7.94849427e-07
Iter: 129 loss: 9.02498925e-07
Iter: 130 loss: 7.94278776e-07
Iter: 131 loss: 7.88744728e-07
Iter: 132 loss: 8.40168809e-07
Iter: 133 loss: 7.88499619e-07
Iter: 134 loss: 7.83122232e-07
Iter: 135 loss: 8.20802313e-07
Iter: 136 loss: 7.82620191e-07
Iter: 137 loss: 7.80528069e-07
Iter: 138 loss: 7.7702208e-07
Iter: 139 loss: 7.76997638e-07
Iter: 140 loss: 7.75054502e-07
Iter: 141 loss: 7.74878345e-07
Iter: 142 loss: 7.72295095e-07
Iter: 143 loss: 7.69277108e-07
Iter: 144 loss: 7.689315e-07
Iter: 145 loss: 7.64343838e-07
Iter: 146 loss: 7.67276219e-07
Iter: 147 loss: 7.61440333e-07
Iter: 148 loss: 7.56491602e-07
Iter: 149 loss: 7.93531342e-07
Iter: 150 loss: 7.56098132e-07
Iter: 151 loss: 7.51618472e-07
Iter: 152 loss: 7.72818e-07
Iter: 153 loss: 7.50826302e-07
Iter: 154 loss: 7.4850368e-07
Iter: 155 loss: 7.43924375e-07
Iter: 156 loss: 8.33034164e-07
Iter: 157 loss: 7.43876967e-07
Iter: 158 loss: 7.43582575e-07
Iter: 159 loss: 7.42164104e-07
Iter: 160 loss: 7.40320615e-07
Iter: 161 loss: 7.37233563e-07
Iter: 162 loss: 7.37216112e-07
Iter: 163 loss: 7.34233879e-07
Iter: 164 loss: 7.34527475e-07
Iter: 165 loss: 7.3193371e-07
Iter: 166 loss: 7.29576527e-07
Iter: 167 loss: 7.29127521e-07
Iter: 168 loss: 7.27354234e-07
Iter: 169 loss: 7.22944492e-07
Iter: 170 loss: 7.64616e-07
Iter: 171 loss: 7.22351615e-07
Iter: 172 loss: 7.19352897e-07
Iter: 173 loss: 7.44848421e-07
Iter: 174 loss: 7.19178615e-07
Iter: 175 loss: 7.17123726e-07
Iter: 176 loss: 7.17016803e-07
Iter: 177 loss: 7.15998112e-07
Iter: 178 loss: 7.13343866e-07
Iter: 179 loss: 7.36862603e-07
Iter: 180 loss: 7.12922883e-07
Iter: 181 loss: 7.09511482e-07
Iter: 182 loss: 7.33239176e-07
Iter: 183 loss: 7.09175708e-07
Iter: 184 loss: 7.06904416e-07
Iter: 185 loss: 7.28518103e-07
Iter: 186 loss: 7.06806247e-07
Iter: 187 loss: 7.04921263e-07
Iter: 188 loss: 7.01032604e-07
Iter: 189 loss: 7.70496285e-07
Iter: 190 loss: 7.00953251e-07
Iter: 191 loss: 6.98142742e-07
Iter: 192 loss: 7.21435867e-07
Iter: 193 loss: 6.97960218e-07
Iter: 194 loss: 6.95677272e-07
Iter: 195 loss: 7.24548158e-07
Iter: 196 loss: 6.95665562e-07
Iter: 197 loss: 6.94852531e-07
Iter: 198 loss: 6.92804065e-07
Iter: 199 loss: 7.1066944e-07
Iter: 200 loss: 6.92494666e-07
Iter: 201 loss: 6.91297032e-07
Iter: 202 loss: 6.91036348e-07
Iter: 203 loss: 6.89414492e-07
Iter: 204 loss: 6.86084377e-07
Iter: 205 loss: 7.45506782e-07
Iter: 206 loss: 6.86014687e-07
Iter: 207 loss: 6.83310816e-07
Iter: 208 loss: 6.8520859e-07
Iter: 209 loss: 6.81624783e-07
Iter: 210 loss: 6.82860559e-07
Iter: 211 loss: 6.80569201e-07
Iter: 212 loss: 6.79629863e-07
Iter: 213 loss: 6.77186506e-07
Iter: 214 loss: 6.97897576e-07
Iter: 215 loss: 6.76771492e-07
Iter: 216 loss: 6.74588e-07
Iter: 217 loss: 6.95062056e-07
Iter: 218 loss: 6.74509693e-07
Iter: 219 loss: 6.72915803e-07
Iter: 220 loss: 6.89745775e-07
Iter: 221 loss: 6.72881697e-07
Iter: 222 loss: 6.71465386e-07
Iter: 223 loss: 6.70106886e-07
Iter: 224 loss: 6.69801523e-07
Iter: 225 loss: 6.68018572e-07
Iter: 226 loss: 6.67023073e-07
Iter: 227 loss: 6.66221581e-07
Iter: 228 loss: 6.66922233e-07
Iter: 229 loss: 6.65255e-07
Iter: 230 loss: 6.6466788e-07
Iter: 231 loss: 6.63067794e-07
Iter: 232 loss: 6.73986278e-07
Iter: 233 loss: 6.62688308e-07
Iter: 234 loss: 6.60886485e-07
Iter: 235 loss: 6.69522194e-07
Iter: 236 loss: 6.60556e-07
Iter: 237 loss: 6.58016575e-07
Iter: 238 loss: 6.67442237e-07
Iter: 239 loss: 6.57387545e-07
Iter: 240 loss: 6.56180589e-07
Iter: 241 loss: 6.54563223e-07
Iter: 242 loss: 6.54479607e-07
Iter: 243 loss: 6.52746678e-07
Iter: 244 loss: 6.55506369e-07
Iter: 245 loss: 6.51926655e-07
Iter: 246 loss: 6.51382379e-07
Iter: 247 loss: 6.50760683e-07
Iter: 248 loss: 6.50359539e-07
Iter: 249 loss: 6.49034121e-07
Iter: 250 loss: 6.49901096e-07
Iter: 251 loss: 6.47893557e-07
Iter: 252 loss: 6.46071385e-07
Iter: 253 loss: 6.46057458e-07
Iter: 254 loss: 6.44661384e-07
Iter: 255 loss: 6.53468476e-07
Iter: 256 loss: 6.44488864e-07
Iter: 257 loss: 6.43265821e-07
Iter: 258 loss: 6.41922156e-07
Iter: 259 loss: 6.41698932e-07
Iter: 260 loss: 6.40402504e-07
Iter: 261 loss: 6.4247439e-07
Iter: 262 loss: 6.39816278e-07
Iter: 263 loss: 6.38491258e-07
Iter: 264 loss: 6.38528832e-07
Iter: 265 loss: 6.37460801e-07
Iter: 266 loss: 6.36525e-07
Iter: 267 loss: 6.36172558e-07
Iter: 268 loss: 6.35466961e-07
Iter: 269 loss: 6.33493642e-07
Iter: 270 loss: 6.42899749e-07
Iter: 271 loss: 6.32790716e-07
Iter: 272 loss: 6.30513966e-07
Iter: 273 loss: 6.38249048e-07
Iter: 274 loss: 6.29909039e-07
Iter: 275 loss: 6.30629245e-07
Iter: 276 loss: 6.29097713e-07
Iter: 277 loss: 6.28636883e-07
Iter: 278 loss: 6.2777724e-07
Iter: 279 loss: 6.46150852e-07
Iter: 280 loss: 6.27757345e-07
Iter: 281 loss: 6.27547536e-07
Iter: 282 loss: 6.27385e-07
Iter: 283 loss: 6.27002692e-07
Iter: 284 loss: 6.25904e-07
Iter: 285 loss: 6.2972515e-07
Iter: 286 loss: 6.2538993e-07
Iter: 287 loss: 6.24268921e-07
Iter: 288 loss: 6.31050796e-07
Iter: 289 loss: 6.24128177e-07
Iter: 290 loss: 6.22693733e-07
Iter: 291 loss: 6.28427529e-07
Iter: 292 loss: 6.22384391e-07
Iter: 293 loss: 6.21473646e-07
Iter: 294 loss: 6.22345e-07
Iter: 295 loss: 6.20957508e-07
Iter: 296 loss: 6.19877596e-07
Iter: 297 loss: 6.19655339e-07
Iter: 298 loss: 6.18956165e-07
Iter: 299 loss: 6.17636829e-07
Iter: 300 loss: 6.20456376e-07
Iter: 301 loss: 6.171216e-07
Iter: 302 loss: 6.16815214e-07
Iter: 303 loss: 6.16428906e-07
Iter: 304 loss: 6.15995077e-07
Iter: 305 loss: 6.14723376e-07
Iter: 306 loss: 6.19878165e-07
Iter: 307 loss: 6.14232135e-07
Iter: 308 loss: 6.14635269e-07
Iter: 309 loss: 6.138348e-07
Iter: 310 loss: 6.13413476e-07
Iter: 311 loss: 6.12560257e-07
Iter: 312 loss: 6.27025315e-07
Iter: 313 loss: 6.12538543e-07
Iter: 314 loss: 6.1219373e-07
Iter: 315 loss: 6.12082545e-07
Iter: 316 loss: 6.11678217e-07
Iter: 317 loss: 6.1093715e-07
Iter: 318 loss: 6.27605e-07
Iter: 319 loss: 6.10939196e-07
Iter: 320 loss: 6.10164761e-07
Iter: 321 loss: 6.10596601e-07
Iter: 322 loss: 6.09637141e-07
Iter: 323 loss: 6.09022663e-07
Iter: 324 loss: 6.0895286e-07
Iter: 325 loss: 6.08516e-07
Iter: 326 loss: 6.08032849e-07
Iter: 327 loss: 6.07951392e-07
Iter: 328 loss: 6.07168e-07
Iter: 329 loss: 6.07532684e-07
Iter: 330 loss: 6.06659e-07
Iter: 331 loss: 6.0587513e-07
Iter: 332 loss: 6.071798e-07
Iter: 333 loss: 6.05503374e-07
Iter: 334 loss: 6.04976833e-07
Iter: 335 loss: 6.04978823e-07
Iter: 336 loss: 6.04329784e-07
Iter: 337 loss: 6.03243507e-07
Iter: 338 loss: 6.03231456e-07
Iter: 339 loss: 6.02562e-07
Iter: 340 loss: 6.08139544e-07
Iter: 341 loss: 6.02520345e-07
Iter: 342 loss: 6.01675424e-07
Iter: 343 loss: 6.02319119e-07
Iter: 344 loss: 6.01153317e-07
Iter: 345 loss: 6.0068794e-07
Iter: 346 loss: 6.06393485e-07
Iter: 347 loss: 6.0068146e-07
Iter: 348 loss: 6.00175213e-07
Iter: 349 loss: 6.0023126e-07
Iter: 350 loss: 5.99779582e-07
Iter: 351 loss: 5.99322675e-07
Iter: 352 loss: 5.98935401e-07
Iter: 353 loss: 5.98803069e-07
Iter: 354 loss: 5.98490374e-07
Iter: 355 loss: 5.98428812e-07
Iter: 356 loss: 5.98088889e-07
Iter: 357 loss: 5.97677513e-07
Iter: 358 loss: 5.97644544e-07
Iter: 359 loss: 5.96991526e-07
Iter: 360 loss: 5.97080657e-07
Iter: 361 loss: 5.96500172e-07
Iter: 362 loss: 5.95673328e-07
Iter: 363 loss: 5.95876941e-07
Iter: 364 loss: 5.95052711e-07
Iter: 365 loss: 5.94193807e-07
Iter: 366 loss: 6.06359436e-07
Iter: 367 loss: 5.941979e-07
Iter: 368 loss: 5.93301877e-07
Iter: 369 loss: 5.95615347e-07
Iter: 370 loss: 5.93001857e-07
Iter: 371 loss: 5.92583774e-07
Iter: 372 loss: 5.92476511e-07
Iter: 373 loss: 5.92218839e-07
Iter: 374 loss: 5.91573894e-07
Iter: 375 loss: 5.99721943e-07
Iter: 376 loss: 5.91561673e-07
Iter: 377 loss: 5.91277853e-07
Iter: 378 loss: 5.91274841e-07
Iter: 379 loss: 5.91041385e-07
Iter: 380 loss: 5.90502282e-07
Iter: 381 loss: 5.92309e-07
Iter: 382 loss: 5.9034312e-07
Iter: 383 loss: 5.89973752e-07
Iter: 384 loss: 5.89413617e-07
Iter: 385 loss: 5.89413389e-07
Iter: 386 loss: 5.88969328e-07
Iter: 387 loss: 5.88963246e-07
Iter: 388 loss: 5.88552098e-07
Iter: 389 loss: 5.88061766e-07
Iter: 390 loss: 5.87993441e-07
Iter: 391 loss: 5.87199281e-07
Iter: 392 loss: 5.88524699e-07
Iter: 393 loss: 5.86829174e-07
Iter: 394 loss: 5.86153647e-07
Iter: 395 loss: 5.8761475e-07
Iter: 396 loss: 5.85908879e-07
Iter: 397 loss: 5.85346811e-07
Iter: 398 loss: 5.87457066e-07
Iter: 399 loss: 5.85212945e-07
Iter: 400 loss: 5.8461103e-07
Iter: 401 loss: 5.90483523e-07
Iter: 402 loss: 5.84579652e-07
Iter: 403 loss: 5.84297936e-07
Iter: 404 loss: 5.83714382e-07
Iter: 405 loss: 5.93576942e-07
Iter: 406 loss: 5.83706765e-07
Iter: 407 loss: 5.83328415e-07
Iter: 408 loss: 5.83255314e-07
Iter: 409 loss: 5.82975758e-07
Iter: 410 loss: 5.82357188e-07
Iter: 411 loss: 5.9242e-07
Iter: 412 loss: 5.8234383e-07
Iter: 413 loss: 5.81631639e-07
Iter: 414 loss: 5.91594244e-07
Iter: 415 loss: 5.81642723e-07
Iter: 416 loss: 5.81347308e-07
Iter: 417 loss: 5.80827873e-07
Iter: 418 loss: 5.92488277e-07
Iter: 419 loss: 5.80835717e-07
Iter: 420 loss: 5.80315941e-07
Iter: 421 loss: 5.80096867e-07
Iter: 422 loss: 5.79838911e-07
Iter: 423 loss: 5.79680318e-07
Iter: 424 loss: 5.79462494e-07
Iter: 425 loss: 5.79142068e-07
Iter: 426 loss: 5.78636445e-07
Iter: 427 loss: 5.78647587e-07
Iter: 428 loss: 5.78040044e-07
Iter: 429 loss: 5.78607285e-07
Iter: 430 loss: 5.77700632e-07
Iter: 431 loss: 5.77136e-07
Iter: 432 loss: 5.78175957e-07
Iter: 433 loss: 5.76893228e-07
Iter: 434 loss: 5.7658923e-07
Iter: 435 loss: 5.76576895e-07
Iter: 436 loss: 5.76189223e-07
Iter: 437 loss: 5.76230605e-07
Iter: 438 loss: 5.75875106e-07
Iter: 439 loss: 5.75585034e-07
Iter: 440 loss: 5.76372372e-07
Iter: 441 loss: 5.75494141e-07
Iter: 442 loss: 5.75012166e-07
Iter: 443 loss: 5.74841408e-07
Iter: 444 loss: 5.74563614e-07
Iter: 445 loss: 5.74249611e-07
Iter: 446 loss: 5.78690276e-07
Iter: 447 loss: 5.74243927e-07
Iter: 448 loss: 5.7385364e-07
Iter: 449 loss: 5.73160605e-07
Iter: 450 loss: 5.73157195e-07
Iter: 451 loss: 5.72599106e-07
Iter: 452 loss: 5.74016667e-07
Iter: 453 loss: 5.72396971e-07
Iter: 454 loss: 5.71861733e-07
Iter: 455 loss: 5.72898102e-07
Iter: 456 loss: 5.71657381e-07
Iter: 457 loss: 5.71355429e-07
Iter: 458 loss: 5.71303929e-07
Iter: 459 loss: 5.71103214e-07
Iter: 460 loss: 5.70799045e-07
Iter: 461 loss: 5.70792679e-07
Iter: 462 loss: 5.70440136e-07
Iter: 463 loss: 5.70474299e-07
Iter: 464 loss: 5.70193379e-07
Iter: 465 loss: 5.69632107e-07
Iter: 466 loss: 5.71255896e-07
Iter: 467 loss: 5.69480619e-07
Iter: 468 loss: 5.69300255e-07
Iter: 469 loss: 5.69233805e-07
Iter: 470 loss: 5.68984376e-07
Iter: 471 loss: 5.68372457e-07
Iter: 472 loss: 5.74581e-07
Iter: 473 loss: 5.68290488e-07
Iter: 474 loss: 5.68167e-07
Iter: 475 loss: 5.67976e-07
Iter: 476 loss: 5.67753261e-07
Iter: 477 loss: 5.67377469e-07
Iter: 478 loss: 5.67377413e-07
Iter: 479 loss: 5.67329948e-07
Iter: 480 loss: 5.67210805e-07
Iter: 481 loss: 5.67098823e-07
Iter: 482 loss: 5.66867186e-07
Iter: 483 loss: 5.70425357e-07
Iter: 484 loss: 5.66863946e-07
Iter: 485 loss: 5.66593599e-07
Iter: 486 loss: 5.66394078e-07
Iter: 487 loss: 5.66335473e-07
Iter: 488 loss: 5.6599572e-07
Iter: 489 loss: 5.65987762e-07
Iter: 490 loss: 5.65725259e-07
Iter: 491 loss: 5.65333721e-07
Iter: 492 loss: 5.65329287e-07
Iter: 493 loss: 5.65072469e-07
Iter: 494 loss: 5.64543484e-07
Iter: 495 loss: 5.74682076e-07
Iter: 496 loss: 5.64526772e-07
Iter: 497 loss: 5.64108745e-07
Iter: 498 loss: 5.64614652e-07
Iter: 499 loss: 5.63901835e-07
Iter: 500 loss: 5.63619437e-07
Iter: 501 loss: 5.64048491e-07
Iter: 502 loss: 5.63506433e-07
Iter: 503 loss: 5.63149797e-07
Iter: 504 loss: 5.64119262e-07
Iter: 505 loss: 5.63005869e-07
Iter: 506 loss: 5.6273791e-07
Iter: 507 loss: 5.62741434e-07
Iter: 508 loss: 5.62515481e-07
Iter: 509 loss: 5.62208527e-07
Iter: 510 loss: 5.62198068e-07
Iter: 511 loss: 5.62014e-07
Iter: 512 loss: 5.61958586e-07
Iter: 513 loss: 5.61802835e-07
Iter: 514 loss: 5.61386173e-07
Iter: 515 loss: 5.65459743e-07
Iter: 516 loss: 5.61329557e-07
Iter: 517 loss: 5.61196714e-07
Iter: 518 loss: 5.61062393e-07
Iter: 519 loss: 5.60952e-07
Iter: 520 loss: 5.60759077e-07
Iter: 521 loss: 5.60768058e-07
Iter: 522 loss: 5.60505896e-07
Iter: 523 loss: 5.61827619e-07
Iter: 524 loss: 5.60461899e-07
Iter: 525 loss: 5.60228273e-07
Iter: 526 loss: 5.59871467e-07
Iter: 527 loss: 5.59870671e-07
Iter: 528 loss: 5.59443151e-07
Iter: 529 loss: 5.61902823e-07
Iter: 530 loss: 5.59389491e-07
Iter: 531 loss: 5.59018815e-07
Iter: 532 loss: 5.60315414e-07
Iter: 533 loss: 5.58919339e-07
Iter: 534 loss: 5.58678437e-07
Iter: 535 loss: 5.59014836e-07
Iter: 536 loss: 5.58551392e-07
Iter: 537 loss: 5.58298325e-07
Iter: 538 loss: 5.60348553e-07
Iter: 539 loss: 5.5825376e-07
Iter: 540 loss: 5.58004558e-07
Iter: 541 loss: 5.59124373e-07
Iter: 542 loss: 5.57937483e-07
Iter: 543 loss: 5.57751378e-07
Iter: 544 loss: 5.57884107e-07
Iter: 545 loss: 5.57647e-07
Iter: 546 loss: 5.57330395e-07
Iter: 547 loss: 5.5810375e-07
Iter: 548 loss: 5.57214378e-07
Iter: 549 loss: 5.56937209e-07
Iter: 550 loss: 5.56998089e-07
Iter: 551 loss: 5.5675082e-07
Iter: 552 loss: 5.56262535e-07
Iter: 553 loss: 5.58882675e-07
Iter: 554 loss: 5.56200121e-07
Iter: 555 loss: 5.56026521e-07
Iter: 556 loss: 5.5609928e-07
Iter: 557 loss: 5.55896e-07
Iter: 558 loss: 5.55562281e-07
Iter: 559 loss: 5.56037378e-07
Iter: 560 loss: 5.55413919e-07
Iter: 561 loss: 5.55176712e-07
Iter: 562 loss: 5.54901e-07
Iter: 563 loss: 5.5487692e-07
Iter: 564 loss: 5.54502265e-07
Iter: 565 loss: 5.58687702e-07
Iter: 566 loss: 5.54497547e-07
Iter: 567 loss: 5.54271537e-07
Iter: 568 loss: 5.54292285e-07
Iter: 569 loss: 5.54092253e-07
Iter: 570 loss: 5.53808263e-07
Iter: 571 loss: 5.54411031e-07
Iter: 572 loss: 5.53687755e-07
Iter: 573 loss: 5.53488292e-07
Iter: 574 loss: 5.53475786e-07
Iter: 575 loss: 5.53312361e-07
Iter: 576 loss: 5.53067707e-07
Iter: 577 loss: 5.53069356e-07
Iter: 578 loss: 5.52826918e-07
Iter: 579 loss: 5.52821461e-07
Iter: 580 loss: 5.52620804e-07
Iter: 581 loss: 5.52417077e-07
Iter: 582 loss: 5.52368476e-07
Iter: 583 loss: 5.52244842e-07
Iter: 584 loss: 5.52230347e-07
Iter: 585 loss: 5.52100914e-07
Iter: 586 loss: 5.51856658e-07
Iter: 587 loss: 5.57324142e-07
Iter: 588 loss: 5.51854271e-07
Iter: 589 loss: 5.51728817e-07
Iter: 590 loss: 5.51717392e-07
Iter: 591 loss: 5.51611663e-07
Iter: 592 loss: 5.5135547e-07
Iter: 593 loss: 5.54466908e-07
Iter: 594 loss: 5.51337621e-07
Iter: 595 loss: 5.51016115e-07
Iter: 596 loss: 5.51791e-07
Iter: 597 loss: 5.5089663e-07
Iter: 598 loss: 5.50534196e-07
Iter: 599 loss: 5.50822392e-07
Iter: 600 loss: 5.5032956e-07
Iter: 601 loss: 5.49976335e-07
Iter: 602 loss: 5.51345806e-07
Iter: 603 loss: 5.49888455e-07
Iter: 604 loss: 5.49715082e-07
Iter: 605 loss: 5.49682795e-07
Iter: 606 loss: 5.49500953e-07
Iter: 607 loss: 5.4947543e-07
Iter: 608 loss: 5.49336391e-07
Iter: 609 loss: 5.49164383e-07
Iter: 610 loss: 5.50319214e-07
Iter: 611 loss: 5.49142669e-07
Iter: 612 loss: 5.48928e-07
Iter: 613 loss: 5.49135507e-07
Iter: 614 loss: 5.48813e-07
Iter: 615 loss: 5.48626872e-07
Iter: 616 loss: 5.49245271e-07
Iter: 617 loss: 5.48554e-07
Iter: 618 loss: 5.48271601e-07
Iter: 619 loss: 5.48391483e-07
Iter: 620 loss: 5.48068783e-07
Iter: 621 loss: 5.47841751e-07
Iter: 622 loss: 5.49179731e-07
Iter: 623 loss: 5.47810032e-07
Iter: 624 loss: 5.47583454e-07
Iter: 625 loss: 5.48247726e-07
Iter: 626 loss: 5.47495461e-07
Iter: 627 loss: 5.47395814e-07
Iter: 628 loss: 5.47311686e-07
Iter: 629 loss: 5.47284685e-07
Iter: 630 loss: 5.47048444e-07
Iter: 631 loss: 5.47507909e-07
Iter: 632 loss: 5.46969829e-07
Iter: 633 loss: 5.46742342e-07
Iter: 634 loss: 5.46560898e-07
Iter: 635 loss: 5.46518379e-07
Iter: 636 loss: 5.46154752e-07
Iter: 637 loss: 5.49515107e-07
Iter: 638 loss: 5.46144463e-07
Iter: 639 loss: 5.45851719e-07
Iter: 640 loss: 5.48787057e-07
Iter: 641 loss: 5.45848934e-07
Iter: 642 loss: 5.45697389e-07
Iter: 643 loss: 5.45582e-07
Iter: 644 loss: 5.45528e-07
Iter: 645 loss: 5.4530409e-07
Iter: 646 loss: 5.45306079e-07
Iter: 647 loss: 5.45204e-07
Iter: 648 loss: 5.45196883e-07
Iter: 649 loss: 5.45124806e-07
Iter: 650 loss: 5.44990939e-07
Iter: 651 loss: 5.46199601e-07
Iter: 652 loss: 5.44980082e-07
Iter: 653 loss: 5.44875888e-07
Iter: 654 loss: 5.44739351e-07
Iter: 655 loss: 5.44737532e-07
Iter: 656 loss: 5.44575414e-07
Iter: 657 loss: 5.46580736e-07
Iter: 658 loss: 5.44566547e-07
Iter: 659 loss: 5.44461727e-07
Iter: 660 loss: 5.44300292e-07
Iter: 661 loss: 5.44304214e-07
Iter: 662 loss: 5.44091279e-07
Iter: 663 loss: 5.44703312e-07
Iter: 664 loss: 5.4402642e-07
Iter: 665 loss: 5.43761587e-07
Iter: 666 loss: 5.43802912e-07
Iter: 667 loss: 5.4358236e-07
Iter: 668 loss: 5.432845e-07
Iter: 669 loss: 5.43155e-07
Iter: 670 loss: 5.43008e-07
Iter: 671 loss: 5.42763814e-07
Iter: 672 loss: 5.42761256e-07
Iter: 673 loss: 5.425062e-07
Iter: 674 loss: 5.44282329e-07
Iter: 675 loss: 5.42486362e-07
Iter: 676 loss: 5.42401267e-07
Iter: 677 loss: 5.42649559e-07
Iter: 678 loss: 5.42358862e-07
Iter: 679 loss: 5.42237217e-07
Iter: 680 loss: 5.42626822e-07
Iter: 681 loss: 5.42208852e-07
Iter: 682 loss: 5.42102043e-07
Iter: 683 loss: 5.42165594e-07
Iter: 684 loss: 5.42051055e-07
Iter: 685 loss: 5.41873931e-07
Iter: 686 loss: 5.42061969e-07
Iter: 687 loss: 5.41772749e-07
Iter: 688 loss: 5.41595e-07
Iter: 689 loss: 5.41764678e-07
Iter: 690 loss: 5.41504164e-07
Iter: 691 loss: 5.41263489e-07
Iter: 692 loss: 5.42759722e-07
Iter: 693 loss: 5.41250529e-07
Iter: 694 loss: 5.41126326e-07
Iter: 695 loss: 5.41055272e-07
Iter: 696 loss: 5.41013719e-07
Iter: 697 loss: 5.40831934e-07
Iter: 698 loss: 5.41551969e-07
Iter: 699 loss: 5.40808799e-07
Iter: 700 loss: 5.40682e-07
Iter: 701 loss: 5.40715291e-07
Iter: 702 loss: 5.40601263e-07
Iter: 703 loss: 5.40465237e-07
Iter: 704 loss: 5.40717792e-07
Iter: 705 loss: 5.40385827e-07
Iter: 706 loss: 5.40286e-07
Iter: 707 loss: 5.40281292e-07
Iter: 708 loss: 5.40160499e-07
Iter: 709 loss: 5.39849452e-07
Iter: 710 loss: 5.43407282e-07
Iter: 711 loss: 5.39837572e-07
Iter: 712 loss: 5.39682503e-07
Iter: 713 loss: 5.39633334e-07
Iter: 714 loss: 5.39533289e-07
Iter: 715 loss: 5.39498274e-07
Iter: 716 loss: 5.3944791e-07
Iter: 717 loss: 5.39301595e-07
Iter: 718 loss: 5.40661404e-07
Iter: 719 loss: 5.3929125e-07
Iter: 720 loss: 5.39194957e-07
Iter: 721 loss: 5.3906092e-07
Iter: 722 loss: 5.39057055e-07
Iter: 723 loss: 5.38919721e-07
Iter: 724 loss: 5.40894405e-07
Iter: 725 loss: 5.38913923e-07
Iter: 726 loss: 5.38799668e-07
Iter: 727 loss: 5.38769427e-07
Iter: 728 loss: 5.38706217e-07
Iter: 729 loss: 5.38541087e-07
Iter: 730 loss: 5.38421887e-07
Iter: 731 loss: 5.3836726e-07
Iter: 732 loss: 5.38098902e-07
Iter: 733 loss: 5.38227e-07
Iter: 734 loss: 5.3790842e-07
Iter: 735 loss: 5.3763813e-07
Iter: 736 loss: 5.40684e-07
Iter: 737 loss: 5.37635685e-07
Iter: 738 loss: 5.3753871e-07
Iter: 739 loss: 5.37531719e-07
Iter: 740 loss: 5.37433834e-07
Iter: 741 loss: 5.37574465e-07
Iter: 742 loss: 5.3738853e-07
Iter: 743 loss: 5.37305766e-07
Iter: 744 loss: 5.37644041e-07
Iter: 745 loss: 5.37299286e-07
Iter: 746 loss: 5.37184178e-07
Iter: 747 loss: 5.37078563e-07
Iter: 748 loss: 5.37051847e-07
Iter: 749 loss: 5.3696823e-07
Iter: 750 loss: 5.36957032e-07
Iter: 751 loss: 5.36861307e-07
Iter: 752 loss: 5.36760808e-07
Iter: 753 loss: 5.36755181e-07
Iter: 754 loss: 5.36619496e-07
Iter: 755 loss: 5.37454127e-07
Iter: 756 loss: 5.36608866e-07
Iter: 757 loss: 5.36468519e-07
Iter: 758 loss: 5.36769335e-07
Iter: 759 loss: 5.36429297e-07
Iter: 760 loss: 5.36319e-07
Iter: 761 loss: 5.36282869e-07
Iter: 762 loss: 5.36213633e-07
Iter: 763 loss: 5.36083348e-07
Iter: 764 loss: 5.36073912e-07
Iter: 765 loss: 5.35978302e-07
Iter: 766 loss: 5.35737456e-07
Iter: 767 loss: 5.3603128e-07
Iter: 768 loss: 5.35621552e-07
Iter: 769 loss: 5.35385311e-07
Iter: 770 loss: 5.37471e-07
Iter: 771 loss: 5.35379513e-07
Iter: 772 loss: 5.35206e-07
Iter: 773 loss: 5.35204663e-07
Iter: 774 loss: 5.35121274e-07
Iter: 775 loss: 5.35156232e-07
Iter: 776 loss: 5.35058575e-07
Iter: 777 loss: 5.34939318e-07
Iter: 778 loss: 5.35523213e-07
Iter: 779 loss: 5.3491874e-07
Iter: 780 loss: 5.34828928e-07
Iter: 781 loss: 5.34948754e-07
Iter: 782 loss: 5.34788342e-07
Iter: 783 loss: 5.34665674e-07
Iter: 784 loss: 5.34814262e-07
Iter: 785 loss: 5.34596893e-07
Iter: 786 loss: 5.34514527e-07
Iter: 787 loss: 5.34774244e-07
Iter: 788 loss: 5.3447809e-07
Iter: 789 loss: 5.34365881e-07
Iter: 790 loss: 5.34608716e-07
Iter: 791 loss: 5.34306537e-07
Iter: 792 loss: 5.34152889e-07
Iter: 793 loss: 5.34102696e-07
Iter: 794 loss: 5.34034484e-07
Iter: 795 loss: 5.33833941e-07
Iter: 796 loss: 5.34169e-07
Iter: 797 loss: 5.33770162e-07
Iter: 798 loss: 5.33620664e-07
Iter: 799 loss: 5.33704451e-07
Iter: 800 loss: 5.33528066e-07
Iter: 801 loss: 5.33297225e-07
Iter: 802 loss: 5.34062451e-07
Iter: 803 loss: 5.33227478e-07
Iter: 804 loss: 5.33171715e-07
Iter: 805 loss: 5.33140565e-07
Iter: 806 loss: 5.33049274e-07
Iter: 807 loss: 5.32976969e-07
Iter: 808 loss: 5.32968784e-07
Iter: 809 loss: 5.3284532e-07
Iter: 810 loss: 5.34185119e-07
Iter: 811 loss: 5.32846514e-07
Iter: 812 loss: 5.32769945e-07
Iter: 813 loss: 5.32698778e-07
Iter: 814 loss: 5.3267388e-07
Iter: 815 loss: 5.32555703e-07
Iter: 816 loss: 5.33951379e-07
Iter: 817 loss: 5.32548e-07
Iter: 818 loss: 5.32474e-07
Iter: 819 loss: 5.3242e-07
Iter: 820 loss: 5.32405238e-07
Iter: 821 loss: 5.32273532e-07
Iter: 822 loss: 5.33145226e-07
Iter: 823 loss: 5.32260856e-07
Iter: 824 loss: 5.32121476e-07
Iter: 825 loss: 5.32132276e-07
Iter: 826 loss: 5.32020692e-07
Iter: 827 loss: 5.31865453e-07
Iter: 828 loss: 5.31840158e-07
Iter: 829 loss: 5.3170362e-07
Iter: 830 loss: 5.31508647e-07
Iter: 831 loss: 5.32631645e-07
Iter: 832 loss: 5.31488467e-07
Iter: 833 loss: 5.31356307e-07
Iter: 834 loss: 5.32194406e-07
Iter: 835 loss: 5.31327714e-07
Iter: 836 loss: 5.31259161e-07
Iter: 837 loss: 5.32110676e-07
Iter: 838 loss: 5.31264561e-07
Iter: 839 loss: 5.31200044e-07
Iter: 840 loss: 5.31158548e-07
Iter: 841 loss: 5.311108e-07
Iter: 842 loss: 5.31064586e-07
Iter: 843 loss: 5.31051e-07
Iter: 844 loss: 5.30996886e-07
Iter: 845 loss: 5.30884904e-07
Iter: 846 loss: 5.33437458e-07
Iter: 847 loss: 5.30877458e-07
Iter: 848 loss: 5.30785e-07
Iter: 849 loss: 5.3223846e-07
Iter: 850 loss: 5.30775537e-07
Iter: 851 loss: 5.30697321e-07
Iter: 852 loss: 5.30576131e-07
Iter: 853 loss: 5.30565785e-07
Iter: 854 loss: 5.30486091e-07
Iter: 855 loss: 5.30486375e-07
Iter: 856 loss: 5.30414695e-07
Iter: 857 loss: 5.30390423e-07
Iter: 858 loss: 5.30364048e-07
Iter: 859 loss: 5.30274406e-07
Iter: 860 loss: 5.30235695e-07
Iter: 861 loss: 5.30190675e-07
Iter: 862 loss: 5.30068178e-07
Iter: 863 loss: 5.30272871e-07
Iter: 864 loss: 5.30017473e-07
Iter: 865 loss: 5.29899125e-07
Iter: 866 loss: 5.30503712e-07
Iter: 867 loss: 5.29884346e-07
Iter: 868 loss: 5.29789645e-07
Iter: 869 loss: 5.30820103e-07
Iter: 870 loss: 5.29783563e-07
Iter: 871 loss: 5.29673798e-07
Iter: 872 loss: 5.29675901e-07
Iter: 873 loss: 5.29613089e-07
Iter: 874 loss: 5.29503723e-07
Iter: 875 loss: 5.29507133e-07
Iter: 876 loss: 5.29413114e-07
Iter: 877 loss: 5.29230874e-07
Iter: 878 loss: 5.30920488e-07
Iter: 879 loss: 5.29217687e-07
Iter: 880 loss: 5.29136628e-07
Iter: 881 loss: 5.29360136e-07
Iter: 882 loss: 5.29126169e-07
Iter: 883 loss: 5.29011459e-07
Iter: 884 loss: 5.2885855e-07
Iter: 885 loss: 5.28864803e-07
Iter: 886 loss: 5.28715e-07
Iter: 887 loss: 5.29740191e-07
Iter: 888 loss: 5.2870314e-07
Iter: 889 loss: 5.28559e-07
Iter: 890 loss: 5.29116846e-07
Iter: 891 loss: 5.28531416e-07
Iter: 892 loss: 5.28421879e-07
Iter: 893 loss: 5.2847696e-07
Iter: 894 loss: 5.28369924e-07
Iter: 895 loss: 5.28250382e-07
Iter: 896 loss: 5.28346163e-07
Iter: 897 loss: 5.28178077e-07
Iter: 898 loss: 5.28059786e-07
Iter: 899 loss: 5.28551709e-07
Iter: 900 loss: 5.28038754e-07
Iter: 901 loss: 5.27954626e-07
Iter: 902 loss: 5.28809835e-07
Iter: 903 loss: 5.27941438e-07
Iter: 904 loss: 5.27846282e-07
Iter: 905 loss: 5.27879592e-07
Iter: 906 loss: 5.27784948e-07
Iter: 907 loss: 5.27653185e-07
Iter: 908 loss: 5.27458155e-07
Iter: 909 loss: 5.27444854e-07
Iter: 910 loss: 5.27495558e-07
Iter: 911 loss: 5.27366296e-07
Iter: 912 loss: 5.2729871e-07
Iter: 913 loss: 5.27137558e-07
Iter: 914 loss: 5.29588647e-07
Iter: 915 loss: 5.27126133e-07
Iter: 916 loss: 5.26942642e-07
Iter: 917 loss: 5.29012596e-07
Iter: 918 loss: 5.26949577e-07
Iter: 919 loss: 5.26860958e-07
Iter: 920 loss: 5.26755571e-07
Iter: 921 loss: 5.26744316e-07
Iter: 922 loss: 5.26692872e-07
Iter: 923 loss: 5.26665417e-07
Iter: 924 loss: 5.26619374e-07
Iter: 925 loss: 5.26533086e-07
Iter: 926 loss: 5.26525923e-07
Iter: 927 loss: 5.26408826e-07
Iter: 928 loss: 5.26805309e-07
Iter: 929 loss: 5.26391887e-07
Iter: 930 loss: 5.26257054e-07
Iter: 931 loss: 5.27131817e-07
Iter: 932 loss: 5.26244776e-07
Iter: 933 loss: 5.26159056e-07
Iter: 934 loss: 5.2630179e-07
Iter: 935 loss: 5.26126655e-07
Iter: 936 loss: 5.26047302e-07
Iter: 937 loss: 5.26047529e-07
Iter: 938 loss: 5.25990913e-07
Iter: 939 loss: 5.25853e-07
Iter: 940 loss: 5.26540362e-07
Iter: 941 loss: 5.25817541e-07
Iter: 942 loss: 5.25673045e-07
Iter: 943 loss: 5.27203497e-07
Iter: 944 loss: 5.25677592e-07
Iter: 945 loss: 5.25544749e-07
Iter: 946 loss: 5.26232725e-07
Iter: 947 loss: 5.25514849e-07
Iter: 948 loss: 5.25432483e-07
Iter: 949 loss: 5.25727444e-07
Iter: 950 loss: 5.25431631e-07
Iter: 951 loss: 5.25328289e-07
Iter: 952 loss: 5.25179928e-07
Iter: 953 loss: 5.25165092e-07
Iter: 954 loss: 5.25138262e-07
Iter: 955 loss: 5.25114046e-07
Iter: 956 loss: 5.25070675e-07
Iter: 957 loss: 5.24985e-07
Iter: 958 loss: 5.24982283e-07
Iter: 959 loss: 5.24879738e-07
Iter: 960 loss: 5.25046744e-07
Iter: 961 loss: 5.2485e-07
Iter: 962 loss: 5.24724214e-07
Iter: 963 loss: 5.25395876e-07
Iter: 964 loss: 5.2471e-07
Iter: 965 loss: 5.24592e-07
Iter: 966 loss: 5.2452981e-07
Iter: 967 loss: 5.24470181e-07
Iter: 968 loss: 5.24369568e-07
Iter: 969 loss: 5.24366556e-07
Iter: 970 loss: 5.24246502e-07
Iter: 971 loss: 5.24180564e-07
Iter: 972 loss: 5.24131337e-07
Iter: 973 loss: 5.24011227e-07
Iter: 974 loss: 5.24003894e-07
Iter: 975 loss: 5.23922267e-07
Iter: 976 loss: 5.23959102e-07
Iter: 977 loss: 5.23871222e-07
Iter: 978 loss: 5.23841322e-07
Iter: 979 loss: 5.23757649e-07
Iter: 980 loss: 5.25120072e-07
Iter: 981 loss: 5.23758217e-07
Iter: 982 loss: 5.23663402e-07
Iter: 983 loss: 5.23656468e-07
Iter: 984 loss: 5.23626852e-07
Iter: 985 loss: 5.2352209e-07
Iter: 986 loss: 5.24761845e-07
Iter: 987 loss: 5.23515325e-07
Iter: 988 loss: 5.23465928e-07
Iter: 989 loss: 5.23430742e-07
Iter: 990 loss: 5.233764e-07
Iter: 991 loss: 5.23211668e-07
Iter: 992 loss: 5.24609504e-07
Iter: 993 loss: 5.23194728e-07
Iter: 994 loss: 5.23004132e-07
Iter: 995 loss: 5.23202857e-07
Iter: 996 loss: 5.22888683e-07
Iter: 997 loss: 5.22786763e-07
Iter: 998 loss: 5.2277943e-07
Iter: 999 loss: 5.22718949e-07
Iter: 1000 loss: 5.22605944e-07
Iter: 1001 loss: 5.22600317e-07
Iter: 1002 loss: 5.22540461e-07
Iter: 1003 loss: 5.22538301e-07
Iter: 1004 loss: 5.22490723e-07
Iter: 1005 loss: 5.22457754e-07
Iter: 1006 loss: 5.2242433e-07
Iter: 1007 loss: 5.22335029e-07
Iter: 1008 loss: 5.22480946e-07
Iter: 1009 loss: 5.22282619e-07
Iter: 1010 loss: 5.22219239e-07
Iter: 1011 loss: 5.22129426e-07
Iter: 1012 loss: 5.22117944e-07
Iter: 1013 loss: 5.22040352e-07
Iter: 1014 loss: 5.22035577e-07
Iter: 1015 loss: 5.21953439e-07
Iter: 1016 loss: 5.21863853e-07
Iter: 1017 loss: 5.21864365e-07
Iter: 1018 loss: 5.21778531e-07
Iter: 1019 loss: 5.21754885e-07
Iter: 1020 loss: 5.21701793e-07
Iter: 1021 loss: 5.21654e-07
Iter: 1022 loss: 5.2163648e-07
Iter: 1023 loss: 5.2154985e-07
Iter: 1024 loss: 5.21493575e-07
Iter: 1025 loss: 5.21477659e-07
Iter: 1026 loss: 5.21420645e-07
Iter: 1027 loss: 5.21414904e-07
Iter: 1028 loss: 5.21371646e-07
Iter: 1029 loss: 5.21287575e-07
Iter: 1030 loss: 5.22107371e-07
Iter: 1031 loss: 5.21284846e-07
Iter: 1032 loss: 5.21210666e-07
Iter: 1033 loss: 5.21137167e-07
Iter: 1034 loss: 5.21120455e-07
Iter: 1035 loss: 5.21028255e-07
Iter: 1036 loss: 5.21033485e-07
Iter: 1037 loss: 5.20982553e-07
Iter: 1038 loss: 5.20889046e-07
Iter: 1039 loss: 5.21724246e-07
Iter: 1040 loss: 5.20867161e-07
Iter: 1041 loss: 5.20768822e-07
Iter: 1042 loss: 5.21910749e-07
Iter: 1043 loss: 5.207595e-07
Iter: 1044 loss: 5.20668095e-07
Iter: 1045 loss: 5.2094839e-07
Iter: 1046 loss: 5.2060966e-07
Iter: 1047 loss: 5.20522917e-07
Iter: 1048 loss: 5.2043481e-07
Iter: 1049 loss: 5.20427079e-07
Iter: 1050 loss: 5.20376148e-07
Iter: 1051 loss: 5.20354e-07
Iter: 1052 loss: 5.20308e-07
Iter: 1053 loss: 5.20251149e-07
Iter: 1054 loss: 5.20245806e-07
Iter: 1055 loss: 5.20123308e-07
Iter: 1056 loss: 5.20351477e-07
Iter: 1057 loss: 5.200597e-07
Iter: 1058 loss: 5.19961418e-07
Iter: 1059 loss: 5.19770765e-07
Iter: 1060 loss: 5.19768491e-07
Iter: 1061 loss: 5.19769401e-07
Iter: 1062 loss: 5.19721482e-07
Iter: 1063 loss: 5.19665718e-07
Iter: 1064 loss: 5.19555442e-07
Iter: 1065 loss: 5.20875233e-07
Iter: 1066 loss: 5.1955891e-07
Iter: 1067 loss: 5.19563741e-07
Iter: 1068 loss: 5.19519631e-07
Iter: 1069 loss: 5.19466312e-07
Iter: 1070 loss: 5.19455625e-07
Iter: 1071 loss: 5.19439141e-07
Iter: 1072 loss: 5.19325965e-07
Iter: 1073 loss: 5.19763148e-07
Iter: 1074 loss: 5.19316131e-07
Iter: 1075 loss: 5.19201e-07
Iter: 1076 loss: 5.2026985e-07
Iter: 1077 loss: 5.19211312e-07
Iter: 1078 loss: 5.19119908e-07
Iter: 1079 loss: 5.18964043e-07
Iter: 1080 loss: 5.21724701e-07
Iter: 1081 loss: 5.18959268e-07
Iter: 1082 loss: 5.18840693e-07
Iter: 1083 loss: 5.19578e-07
Iter: 1084 loss: 5.18807724e-07
Iter: 1085 loss: 5.18821e-07
Iter: 1086 loss: 5.18775607e-07
Iter: 1087 loss: 5.1874764e-07
Iter: 1088 loss: 5.18666184e-07
Iter: 1089 loss: 5.1911627e-07
Iter: 1090 loss: 5.18641855e-07
Iter: 1091 loss: 5.18613092e-07
Iter: 1092 loss: 5.18600075e-07
Iter: 1093 loss: 5.18553634e-07
Iter: 1094 loss: 5.18477293e-07
Iter: 1095 loss: 5.18470131e-07
Iter: 1096 loss: 5.18383445e-07
Iter: 1097 loss: 5.18402885e-07
Iter: 1098 loss: 5.18312049e-07
Iter: 1099 loss: 5.18179775e-07
Iter: 1100 loss: 5.19152366e-07
Iter: 1101 loss: 5.18162096e-07
Iter: 1102 loss: 5.18112415e-07
Iter: 1103 loss: 5.18081265e-07
Iter: 1104 loss: 5.18061142e-07
Iter: 1105 loss: 5.17984063e-07
Iter: 1106 loss: 5.17977128e-07
Iter: 1107 loss: 5.17943818e-07
Iter: 1108 loss: 5.17853891e-07
Iter: 1109 loss: 5.19760306e-07
Iter: 1110 loss: 5.17850708e-07
Iter: 1111 loss: 5.17719627e-07
Iter: 1112 loss: 5.18061256e-07
Iter: 1113 loss: 5.17691092e-07
Iter: 1114 loss: 5.17633566e-07
Iter: 1115 loss: 5.17611056e-07
Iter: 1116 loss: 5.17551769e-07
Iter: 1117 loss: 5.17486342e-07
Iter: 1118 loss: 5.17476224e-07
Iter: 1119 loss: 5.17427225e-07
Iter: 1120 loss: 5.17431261e-07
Iter: 1121 loss: 5.1738823e-07
Iter: 1122 loss: 5.1733906e-07
Iter: 1123 loss: 5.17341846e-07
Iter: 1124 loss: 5.17280341e-07
Iter: 1125 loss: 5.1751e-07
Iter: 1126 loss: 5.17270223e-07
Iter: 1127 loss: 5.17198089e-07
Iter: 1128 loss: 5.17195701e-07
Iter: 1129 loss: 5.17132776e-07
Iter: 1130 loss: 5.17067463e-07
Iter: 1131 loss: 5.16914952e-07
Iter: 1132 loss: 5.19769173e-07
Iter: 1133 loss: 5.16903697e-07
Iter: 1134 loss: 5.16733394e-07
Iter: 1135 loss: 5.1754904e-07
Iter: 1136 loss: 5.1669258e-07
Iter: 1137 loss: 5.16738169e-07
Iter: 1138 loss: 5.16625732e-07
Iter: 1139 loss: 5.16585089e-07
Iter: 1140 loss: 5.16584464e-07
Iter: 1141 loss: 5.1654024e-07
Iter: 1142 loss: 5.16481e-07
Iter: 1143 loss: 5.1657e-07
Iter: 1144 loss: 5.16452701e-07
Iter: 1145 loss: 5.1639563e-07
Iter: 1146 loss: 5.16309797e-07
Iter: 1147 loss: 5.16309342e-07
Iter: 1148 loss: 5.1623357e-07
Iter: 1149 loss: 5.16232092e-07
Iter: 1150 loss: 5.16155694e-07
Iter: 1151 loss: 5.16038426e-07
Iter: 1152 loss: 5.1603115e-07
Iter: 1153 loss: 5.15953388e-07
Iter: 1154 loss: 5.15948273e-07
Iter: 1155 loss: 5.15891202e-07
Iter: 1156 loss: 5.15807301e-07
Iter: 1157 loss: 5.15804402e-07
Iter: 1158 loss: 5.15746819e-07
Iter: 1159 loss: 5.15752163e-07
Iter: 1160 loss: 5.15687816e-07
Iter: 1161 loss: 5.15575096e-07
Iter: 1162 loss: 5.17943704e-07
Iter: 1163 loss: 5.1557663e-07
Iter: 1164 loss: 5.15517172e-07
Iter: 1165 loss: 5.15495174e-07
Iter: 1166 loss: 5.15460329e-07
Iter: 1167 loss: 5.15393708e-07
Iter: 1168 loss: 5.162197e-07
Iter: 1169 loss: 5.15392344e-07
Iter: 1170 loss: 5.15341867e-07
Iter: 1171 loss: 5.15261263e-07
Iter: 1172 loss: 5.15254612e-07
Iter: 1173 loss: 5.15160309e-07
Iter: 1174 loss: 5.15166789e-07
Iter: 1175 loss: 5.15091926e-07
Iter: 1176 loss: 5.1501479e-07
Iter: 1177 loss: 5.15011493e-07
Iter: 1178 loss: 5.14936573e-07
Iter: 1179 loss: 5.150427e-07
Iter: 1180 loss: 5.14889507e-07
Iter: 1181 loss: 5.14827207e-07
Iter: 1182 loss: 5.15308784e-07
Iter: 1183 loss: 5.14825842e-07
Iter: 1184 loss: 5.14750468e-07
Iter: 1185 loss: 5.14651447e-07
Iter: 1186 loss: 5.14650935e-07
Iter: 1187 loss: 5.14570502e-07
Iter: 1188 loss: 5.14558678e-07
Iter: 1189 loss: 5.14492967e-07
Iter: 1190 loss: 5.14332442e-07
Iter: 1191 loss: 5.16318039e-07
Iter: 1192 loss: 5.14324711e-07
Iter: 1193 loss: 5.14219835e-07
Iter: 1194 loss: 5.1445835e-07
Iter: 1195 loss: 5.14176122e-07
Iter: 1196 loss: 5.14108251e-07
Iter: 1197 loss: 5.14563737e-07
Iter: 1198 loss: 5.14099838e-07
Iter: 1199 loss: 5.14025e-07
Iter: 1200 loss: 5.14604665e-07
Iter: 1201 loss: 5.14030489e-07
Iter: 1202 loss: 5.13972623e-07
Iter: 1203 loss: 5.13955627e-07
Iter: 1204 loss: 5.13925727e-07
Iter: 1205 loss: 5.1385905e-07
Iter: 1206 loss: 5.14416229e-07
Iter: 1207 loss: 5.1385075e-07
Iter: 1208 loss: 5.13792372e-07
Iter: 1209 loss: 5.13713644e-07
Iter: 1210 loss: 5.1371012e-07
Iter: 1211 loss: 5.13621e-07
Iter: 1212 loss: 5.14735802e-07
Iter: 1213 loss: 5.13623718e-07
Iter: 1214 loss: 5.13562327e-07
Iter: 1215 loss: 5.13658108e-07
Iter: 1216 loss: 5.13538e-07
Iter: 1217 loss: 5.1347854e-07
Iter: 1218 loss: 5.13763098e-07
Iter: 1219 loss: 5.13461828e-07
Iter: 1220 loss: 5.13391342e-07
Iter: 1221 loss: 5.13310795e-07
Iter: 1222 loss: 5.13298232e-07
Iter: 1223 loss: 5.13238319e-07
Iter: 1224 loss: 5.13222744e-07
Iter: 1225 loss: 5.13189548e-07
Iter: 1226 loss: 5.13086036e-07
Iter: 1227 loss: 5.14156739e-07
Iter: 1228 loss: 5.13065402e-07
Iter: 1229 loss: 5.12986503e-07
Iter: 1230 loss: 5.13602458e-07
Iter: 1231 loss: 5.12976953e-07
Iter: 1232 loss: 5.12935969e-07
Iter: 1233 loss: 5.12927727e-07
Iter: 1234 loss: 5.12875374e-07
Iter: 1235 loss: 5.1283223e-07
Iter: 1236 loss: 5.12822794e-07
Iter: 1237 loss: 5.12718543e-07
Iter: 1238 loss: 5.1312071e-07
Iter: 1239 loss: 5.12701376e-07
Iter: 1240 loss: 5.12626116e-07
Iter: 1241 loss: 5.12667555e-07
Iter: 1242 loss: 5.12577458e-07
Iter: 1243 loss: 5.12483723e-07
Iter: 1244 loss: 5.1255688e-07
Iter: 1245 loss: 5.12445865e-07
Iter: 1246 loss: 5.12358724e-07
Iter: 1247 loss: 5.12364e-07
Iter: 1248 loss: 5.12316888e-07
Iter: 1249 loss: 5.12323595e-07
Iter: 1250 loss: 5.12286704e-07
Iter: 1251 loss: 5.12230486e-07
Iter: 1252 loss: 5.12573365e-07
Iter: 1253 loss: 5.12228382e-07
Iter: 1254 loss: 5.12189e-07
Iter: 1255 loss: 5.12179781e-07
Iter: 1256 loss: 5.1216864e-07
Iter: 1257 loss: 5.12074223e-07
Iter: 1258 loss: 5.12113104e-07
Iter: 1259 loss: 5.12037047e-07
Iter: 1260 loss: 5.11938197e-07
Iter: 1261 loss: 5.11780399e-07
Iter: 1262 loss: 5.15222382e-07
Iter: 1263 loss: 5.1178597e-07
Iter: 1264 loss: 5.11627093e-07
Iter: 1265 loss: 5.13943235e-07
Iter: 1266 loss: 5.11613734e-07
Iter: 1267 loss: 5.11600547e-07
Iter: 1268 loss: 5.1157383e-07
Iter: 1269 loss: 5.11559392e-07
Iter: 1270 loss: 5.11551207e-07
Iter: 1271 loss: 5.11518692e-07
Iter: 1272 loss: 5.11481176e-07
Iter: 1273 loss: 5.1155547e-07
Iter: 1274 loss: 5.11463895e-07
Iter: 1275 loss: 5.1141825e-07
Iter: 1276 loss: 5.11412168e-07
Iter: 1277 loss: 5.11371752e-07
Iter: 1278 loss: 5.11331677e-07
Iter: 1279 loss: 5.11791939e-07
Iter: 1280 loss: 5.11328665e-07
Iter: 1281 loss: 5.11249709e-07
Iter: 1282 loss: 5.1115353e-07
Iter: 1283 loss: 5.1114e-07
Iter: 1284 loss: 5.11032226e-07
Iter: 1285 loss: 5.12384872e-07
Iter: 1286 loss: 5.11034e-07
Iter: 1287 loss: 5.10957875e-07
Iter: 1288 loss: 5.10976349e-07
Iter: 1289 loss: 5.10899554e-07
Iter: 1290 loss: 5.10824862e-07
Iter: 1291 loss: 5.11871349e-07
Iter: 1292 loss: 5.10808945e-07
Iter: 1293 loss: 5.10778705e-07
Iter: 1294 loss: 5.1071197e-07
Iter: 1295 loss: 5.12428869e-07
Iter: 1296 loss: 5.10701113e-07
Iter: 1297 loss: 5.10631708e-07
Iter: 1298 loss: 5.10673431e-07
Iter: 1299 loss: 5.10577706e-07
Iter: 1300 loss: 5.10523591e-07
Iter: 1301 loss: 5.10860048e-07
Iter: 1302 loss: 5.10505913e-07
Iter: 1303 loss: 5.10452196e-07
Iter: 1304 loss: 5.10396262e-07
Iter: 1305 loss: 5.1037614e-07
Iter: 1306 loss: 5.10253585e-07
Iter: 1307 loss: 5.11144265e-07
Iter: 1308 loss: 5.10248924e-07
Iter: 1309 loss: 5.10194809e-07
Iter: 1310 loss: 5.10080554e-07
Iter: 1311 loss: 5.12231281e-07
Iter: 1312 loss: 5.10083339e-07
Iter: 1313 loss: 5.10011432e-07
Iter: 1314 loss: 5.10020925e-07
Iter: 1315 loss: 5.09959477e-07
Iter: 1316 loss: 5.10006601e-07
Iter: 1317 loss: 5.09917527e-07
Iter: 1318 loss: 5.09858751e-07
Iter: 1319 loss: 5.10086522e-07
Iter: 1320 loss: 5.0983067e-07
Iter: 1321 loss: 5.09761435e-07
Iter: 1322 loss: 5.09707661e-07
Iter: 1323 loss: 5.09696406e-07
Iter: 1324 loss: 5.09634106e-07
Iter: 1325 loss: 5.0962592e-07
Iter: 1326 loss: 5.09579422e-07
Iter: 1327 loss: 5.09523147e-07
Iter: 1328 loss: 5.09507686e-07
Iter: 1329 loss: 5.09436859e-07
Iter: 1330 loss: 5.09672759e-07
Iter: 1331 loss: 5.09404913e-07
Iter: 1332 loss: 5.09359097e-07
Iter: 1333 loss: 5.09329197e-07
Iter: 1334 loss: 5.09311917e-07
Iter: 1335 loss: 5.09232791e-07
Iter: 1336 loss: 5.09449137e-07
Iter: 1337 loss: 5.09200504e-07
Iter: 1338 loss: 5.09144684e-07
Iter: 1339 loss: 5.09144e-07
Iter: 1340 loss: 5.09096935e-07
Iter: 1341 loss: 5.08993139e-07
Iter: 1342 loss: 5.10263476e-07
Iter: 1343 loss: 5.08978417e-07
Iter: 1344 loss: 5.08911683e-07
Iter: 1345 loss: 5.08912763e-07
Iter: 1346 loss: 5.08863934e-07
Iter: 1347 loss: 5.09034919e-07
Iter: 1348 loss: 5.08851883e-07
Iter: 1349 loss: 5.08791118e-07
Iter: 1350 loss: 5.08944311e-07
Iter: 1351 loss: 5.08774235e-07
Iter: 1352 loss: 5.08716766e-07
Iter: 1353 loss: 5.08665494e-07
Iter: 1354 loss: 5.08651169e-07
Iter: 1355 loss: 5.08584776e-07
Iter: 1356 loss: 5.09296569e-07
Iter: 1357 loss: 5.08577955e-07
Iter: 1358 loss: 5.08508037e-07
Iter: 1359 loss: 5.08536118e-07
Iter: 1360 loss: 5.084745e-07
Iter: 1361 loss: 5.08393555e-07
Iter: 1362 loss: 5.08914184e-07
Iter: 1363 loss: 5.08381504e-07
Iter: 1364 loss: 5.08305277e-07
Iter: 1365 loss: 5.08317498e-07
Iter: 1366 loss: 5.08250366e-07
Iter: 1367 loss: 5.08175617e-07
Iter: 1368 loss: 5.08120365e-07
Iter: 1369 loss: 5.08081257e-07
Iter: 1370 loss: 5.08031064e-07
Iter: 1371 loss: 5.08005883e-07
Iter: 1372 loss: 5.0794506e-07
Iter: 1373 loss: 5.07883499e-07
Iter: 1374 loss: 5.07868265e-07
Iter: 1375 loss: 5.07772597e-07
Iter: 1376 loss: 5.07813752e-07
Iter: 1377 loss: 5.0772303e-07
Iter: 1378 loss: 5.07620825e-07
Iter: 1379 loss: 5.07624691e-07
Iter: 1380 loss: 5.07538516e-07
Iter: 1381 loss: 5.07771e-07
Iter: 1382 loss: 5.07540449e-07
Iter: 1383 loss: 5.0748065e-07
Iter: 1384 loss: 5.07412437e-07
Iter: 1385 loss: 5.07400273e-07
Iter: 1386 loss: 5.07333482e-07
Iter: 1387 loss: 5.08094217e-07
Iter: 1388 loss: 5.07340701e-07
Iter: 1389 loss: 5.07276582e-07
Iter: 1390 loss: 5.07437676e-07
Iter: 1391 loss: 5.07257255e-07
Iter: 1392 loss: 5.07216328e-07
Iter: 1393 loss: 5.0734775e-07
Iter: 1394 loss: 5.07186883e-07
Iter: 1395 loss: 5.07119694e-07
Iter: 1396 loss: 5.07253048e-07
Iter: 1397 loss: 5.07115033e-07
Iter: 1398 loss: 5.0705637e-07
Iter: 1399 loss: 5.06914205e-07
Iter: 1400 loss: 5.08569883e-07
Iter: 1401 loss: 5.06915285e-07
Iter: 1402 loss: 5.06836102e-07
Iter: 1403 loss: 5.06828542e-07
Iter: 1404 loss: 5.06746687e-07
Iter: 1405 loss: 5.06853439e-07
Iter: 1406 loss: 5.06717e-07
Iter: 1407 loss: 5.06644369e-07
Iter: 1408 loss: 5.06536935e-07
Iter: 1409 loss: 5.06543643e-07
Iter: 1410 loss: 5.06498964e-07
Iter: 1411 loss: 5.06476738e-07
Iter: 1412 loss: 5.0643041e-07
Iter: 1413 loss: 5.06563879e-07
Iter: 1414 loss: 5.06416313e-07
Iter: 1415 loss: 5.063863e-07
Iter: 1416 loss: 5.06351626e-07
Iter: 1417 loss: 5.06343383e-07
Iter: 1418 loss: 5.06285062e-07
Iter: 1419 loss: 5.06665913e-07
Iter: 1420 loss: 5.06278298e-07
Iter: 1421 loss: 5.06221909e-07
Iter: 1422 loss: 5.06350091e-07
Iter: 1423 loss: 5.0621e-07
Iter: 1424 loss: 5.06142442e-07
Iter: 1425 loss: 5.06213041e-07
Iter: 1426 loss: 5.06117829e-07
Iter: 1427 loss: 5.06033302e-07
Iter: 1428 loss: 5.06523065e-07
Iter: 1429 loss: 5.06028528e-07
Iter: 1430 loss: 5.0596617e-07
Iter: 1431 loss: 5.05879768e-07
Iter: 1432 loss: 5.05887442e-07
Iter: 1433 loss: 5.05804906e-07
Iter: 1434 loss: 5.05804337e-07
Iter: 1435 loss: 5.05744765e-07
Iter: 1436 loss: 5.05960486e-07
Iter: 1437 loss: 5.05718447e-07
Iter: 1438 loss: 5.05682e-07
Iter: 1439 loss: 5.05588332e-07
Iter: 1440 loss: 5.07587629e-07
Iter: 1441 loss: 5.05589924e-07
Iter: 1442 loss: 5.05498519e-07
Iter: 1443 loss: 5.05504886e-07
Iter: 1444 loss: 5.05440937e-07
Iter: 1445 loss: 5.05574917e-07
Iter: 1446 loss: 5.05410583e-07
Iter: 1447 loss: 5.05329638e-07
Iter: 1448 loss: 5.05362152e-07
Iter: 1449 loss: 5.05259266e-07
Iter: 1450 loss: 5.05192247e-07
Iter: 1451 loss: 5.05566504e-07
Iter: 1452 loss: 5.05180424e-07
Iter: 1453 loss: 5.05110563e-07
Iter: 1454 loss: 5.05441392e-07
Iter: 1455 loss: 5.05104254e-07
Iter: 1456 loss: 5.05059347e-07
Iter: 1457 loss: 5.05095841e-07
Iter: 1458 loss: 5.05019614e-07
Iter: 1459 loss: 5.04973173e-07
Iter: 1460 loss: 5.05663252e-07
Iter: 1461 loss: 5.0496368e-07
Iter: 1462 loss: 5.04915704e-07
Iter: 1463 loss: 5.04839818e-07
Iter: 1464 loss: 5.06314e-07
Iter: 1465 loss: 5.04829814e-07
Iter: 1466 loss: 5.04721868e-07
Iter: 1467 loss: 5.05172352e-07
Iter: 1468 loss: 5.04718514e-07
Iter: 1469 loss: 5.04613809e-07
Iter: 1470 loss: 5.05586627e-07
Iter: 1471 loss: 5.04616196e-07
Iter: 1472 loss: 5.04553384e-07
Iter: 1473 loss: 5.04477896e-07
Iter: 1474 loss: 5.04472837e-07
Iter: 1475 loss: 5.04408149e-07
Iter: 1476 loss: 5.04407467e-07
Iter: 1477 loss: 5.04357274e-07
Iter: 1478 loss: 5.04583682e-07
Iter: 1479 loss: 5.04342097e-07
Iter: 1480 loss: 5.04292075e-07
Iter: 1481 loss: 5.04321918e-07
Iter: 1482 loss: 5.04276159e-07
Iter: 1483 loss: 5.04203911e-07
Iter: 1484 loss: 5.0433232e-07
Iter: 1485 loss: 5.0419203e-07
Iter: 1486 loss: 5.04127968e-07
Iter: 1487 loss: 5.04403886e-07
Iter: 1488 loss: 5.04116656e-07
Iter: 1489 loss: 5.04062655e-07
Iter: 1490 loss: 5.0405032e-07
Iter: 1491 loss: 5.04019397e-07
Iter: 1492 loss: 5.03943568e-07
Iter: 1493 loss: 5.04833224e-07
Iter: 1494 loss: 5.03942374e-07
Iter: 1495 loss: 5.03895535e-07
Iter: 1496 loss: 5.03836077e-07
Iter: 1497 loss: 5.03843637e-07
Iter: 1498 loss: 5.03759622e-07
Iter: 1499 loss: 5.03875867e-07
Iter: 1500 loss: 5.03717956e-07
Iter: 1501 loss: 5.03665206e-07
Iter: 1502 loss: 5.03653609e-07
Iter: 1503 loss: 5.03628598e-07
Iter: 1504 loss: 5.03540264e-07
Iter: 1505 loss: 5.0494e-07
Iter: 1506 loss: 5.03529179e-07
Iter: 1507 loss: 5.03464037e-07
Iter: 1508 loss: 5.04060552e-07
Iter: 1509 loss: 5.03450678e-07
Iter: 1510 loss: 5.03367232e-07
Iter: 1511 loss: 5.04087666e-07
Iter: 1512 loss: 5.03371e-07
Iter: 1513 loss: 5.03322042e-07
Iter: 1514 loss: 5.03328067e-07
Iter: 1515 loss: 5.03302488e-07
Iter: 1516 loss: 5.03250305e-07
Iter: 1517 loss: 5.03350805e-07
Iter: 1518 loss: 5.03215745e-07
Iter: 1519 loss: 5.03175897e-07
Iter: 1520 loss: 5.03534125e-07
Iter: 1521 loss: 5.03172487e-07
Iter: 1522 loss: 5.03134459e-07
Iter: 1523 loss: 5.03134402e-07
Iter: 1524 loss: 5.03105184e-07
Iter: 1525 loss: 5.03048341e-07
Iter: 1526 loss: 5.03563797e-07
Iter: 1527 loss: 5.03051183e-07
Iter: 1528 loss: 5.03000365e-07
Iter: 1529 loss: 5.02969783e-07
Iter: 1530 loss: 5.02958699e-07
Iter: 1531 loss: 5.02884916e-07
Iter: 1532 loss: 5.0285081e-07
Iter: 1533 loss: 5.02810053e-07
Iter: 1534 loss: 5.02762759e-07
Iter: 1535 loss: 5.027415e-07
Iter: 1536 loss: 5.02700516e-07
Iter: 1537 loss: 5.02632247e-07
Iter: 1538 loss: 5.02626222e-07
Iter: 1539 loss: 5.0255835e-07
Iter: 1540 loss: 5.0268261e-07
Iter: 1541 loss: 5.02539365e-07
Iter: 1542 loss: 5.02484227e-07
Iter: 1543 loss: 5.02461944e-07
Iter: 1544 loss: 5.02420392e-07
Iter: 1545 loss: 5.02373155e-07
Iter: 1546 loss: 5.02364287e-07
Iter: 1547 loss: 5.02286298e-07
Iter: 1548 loss: 5.0233632e-07
Iter: 1549 loss: 5.02228829e-07
Iter: 1550 loss: 5.021422e-07
Iter: 1551 loss: 5.03257866e-07
Iter: 1552 loss: 5.02146918e-07
Iter: 1553 loss: 5.02083765e-07
Iter: 1554 loss: 5.02143848e-07
Iter: 1555 loss: 5.02045e-07
Iter: 1556 loss: 5.01994805e-07
Iter: 1557 loss: 5.02669536e-07
Iter: 1558 loss: 5.02004923e-07
Iter: 1559 loss: 5.01963711e-07
Iter: 1560 loss: 5.01928298e-07
Iter: 1561 loss: 5.01911643e-07
Iter: 1562 loss: 5.0186145e-07
Iter: 1563 loss: 5.01856675e-07
Iter: 1564 loss: 5.01818e-07
Iter: 1565 loss: 5.01773627e-07
Iter: 1566 loss: 5.01772831e-07
Iter: 1567 loss: 5.01729176e-07
Iter: 1568 loss: 5.01671821e-07
Iter: 1569 loss: 5.0166426e-07
Iter: 1570 loss: 5.01600084e-07
Iter: 1571 loss: 5.01598265e-07
Iter: 1572 loss: 5.01542331e-07
Iter: 1573 loss: 5.01473096e-07
Iter: 1574 loss: 5.01484124e-07
Iter: 1575 loss: 5.01421823e-07
Iter: 1576 loss: 5.01464342e-07
Iter: 1577 loss: 5.0138658e-07
Iter: 1578 loss: 5.01336558e-07
Iter: 1579 loss: 5.01369357e-07
Iter: 1580 loss: 5.01304953e-07
Iter: 1581 loss: 5.01255101e-07
Iter: 1582 loss: 5.0158917e-07
Iter: 1583 loss: 5.01247769e-07
Iter: 1584 loss: 5.01201157e-07
Iter: 1585 loss: 5.01211787e-07
Iter: 1586 loss: 5.01160343e-07
Iter: 1587 loss: 5.01099e-07
Iter: 1588 loss: 5.0151624e-07
Iter: 1589 loss: 5.01090199e-07
Iter: 1590 loss: 5.01043132e-07
Iter: 1591 loss: 5.01045065e-07
Iter: 1592 loss: 5.00978047e-07
Iter: 1593 loss: 5.00917281e-07
Iter: 1594 loss: 5.00862882e-07
Iter: 1595 loss: 5.00849296e-07
Iter: 1596 loss: 5.0076676e-07
Iter: 1597 loss: 5.01360262e-07
Iter: 1598 loss: 5.00750389e-07
Iter: 1599 loss: 5.00700821e-07
Iter: 1600 loss: 5.00700139e-07
Iter: 1601 loss: 5.00677061e-07
Iter: 1602 loss: 5.00636816e-07
Iter: 1603 loss: 5.01541308e-07
Iter: 1604 loss: 5.00635338e-07
Iter: 1605 loss: 5.00580029e-07
Iter: 1606 loss: 5.00764259e-07
Iter: 1607 loss: 5.0056957e-07
Iter: 1608 loss: 5.00509145e-07
Iter: 1609 loss: 5.01056206e-07
Iter: 1610 loss: 5.00502e-07
Iter: 1611 loss: 5.0046674e-07
Iter: 1612 loss: 5.00449346e-07
Iter: 1613 loss: 5.00430247e-07
Iter: 1614 loss: 5.00388637e-07
Iter: 1615 loss: 5.00531428e-07
Iter: 1616 loss: 5.00367207e-07
Iter: 1617 loss: 5.0032844e-07
Iter: 1618 loss: 5.00703379e-07
Iter: 1619 loss: 5.00324404e-07
Iter: 1620 loss: 5.00290071e-07
Iter: 1621 loss: 5.00290241e-07
Iter: 1622 loss: 5.00252e-07
Iter: 1623 loss: 5.00217197e-07
Iter: 1624 loss: 5.00670524e-07
Iter: 1625 loss: 5.00230044e-07
Iter: 1626 loss: 5.00197189e-07
Iter: 1627 loss: 5.00132501e-07
Iter: 1628 loss: 5.0092217e-07
Iter: 1629 loss: 5.00120393e-07
Iter: 1630 loss: 5.00033536e-07
Iter: 1631 loss: 5.00333499e-07
Iter: 1632 loss: 5.00013e-07
Iter: 1633 loss: 5.00000908e-07
Iter: 1634 loss: 4.99993234e-07
Iter: 1635 loss: 4.99945259e-07
Iter: 1636 loss: 4.99895862e-07
Iter: 1637 loss: 4.99901489e-07
Iter: 1638 loss: 4.99867554e-07
Iter: 1639 loss: 5.00021e-07
Iter: 1640 loss: 4.99865337e-07
Iter: 1641 loss: 4.99835096e-07
Iter: 1642 loss: 5.00077476e-07
Iter: 1643 loss: 4.99843622e-07
Iter: 1644 loss: 4.99810312e-07
Iter: 1645 loss: 4.99801047e-07
Iter: 1646 loss: 4.99786893e-07
Iter: 1647 loss: 4.99756197e-07
Iter: 1648 loss: 4.99767452e-07
Iter: 1649 loss: 4.99720727e-07
Iter: 1650 loss: 4.99682358e-07
Iter: 1651 loss: 4.99990733e-07
Iter: 1652 loss: 4.9966809e-07
Iter: 1653 loss: 4.99632165e-07
Iter: 1654 loss: 4.99608518e-07
Iter: 1655 loss: 4.99591351e-07
Iter: 1656 loss: 4.99561679e-07
Iter: 1657 loss: 4.99558e-07
Iter: 1658 loss: 4.99536441e-07
Iter: 1659 loss: 4.99538942e-07
Iter: 1660 loss: 4.99509781e-07
Iter: 1661 loss: 4.99486248e-07
Iter: 1662 loss: 4.99464704e-07
Iter: 1663 loss: 4.99461066e-07
Iter: 1664 loss: 4.9940121e-07
Iter: 1665 loss: 4.99473515e-07
Iter: 1666 loss: 4.99381031e-07
Iter: 1667 loss: 4.99324642e-07
Iter: 1668 loss: 4.99510861e-07
Iter: 1669 loss: 4.99304633e-07
Iter: 1670 loss: 4.99280532e-07
Iter: 1671 loss: 4.99284738e-07
Iter: 1672 loss: 4.9925552e-07
Iter: 1673 loss: 4.99508872e-07
Iter: 1674 loss: 4.99254384e-07
Iter: 1675 loss: 4.99231646e-07
Iter: 1676 loss: 4.99258704e-07
Iter: 1677 loss: 4.9922744e-07
Iter: 1678 loss: 4.99207204e-07
Iter: 1679 loss: 4.99217833e-07
Iter: 1680 loss: 4.99195608e-07
Iter: 1681 loss: 4.99176281e-07
Iter: 1682 loss: 4.99356361e-07
Iter: 1683 loss: 4.99170142e-07
Iter: 1684 loss: 4.99141663e-07
Iter: 1685 loss: 4.99120688e-07
Iter: 1686 loss: 4.99116368e-07
Iter: 1687 loss: 4.99094369e-07
Iter: 1688 loss: 4.99089708e-07
Iter: 1689 loss: 4.99067482e-07
Iter: 1690 loss: 4.99060206e-07
Iter: 1691 loss: 4.99051453e-07
Iter: 1692 loss: 4.99040482e-07
Iter: 1693 loss: 4.99095677e-07
Iter: 1694 loss: 4.99028715e-07
Iter: 1695 loss: 4.98996144e-07
Iter: 1696 loss: 4.99076066e-07
Iter: 1697 loss: 4.98996542e-07
Iter: 1698 loss: 4.98993472e-07
Iter: 1699 loss: 4.98942427e-07
Iter: 1700 loss: 4.99622445e-07
Iter: 1701 loss: 4.98945155e-07
Iter: 1702 loss: 4.9891753e-07
Iter: 1703 loss: 4.98958912e-07
Iter: 1704 loss: 4.98897634e-07
Iter: 1705 loss: 4.98857e-07
Iter: 1706 loss: 4.99154453e-07
Iter: 1707 loss: 4.98856e-07
Iter: 1708 loss: 4.98820327e-07
Iter: 1709 loss: 4.99110342e-07
Iter: 1710 loss: 4.98814416e-07
Iter: 1711 loss: 4.98802365e-07
Iter: 1712 loss: 4.98818167e-07
Iter: 1713 loss: 4.98788097e-07
Iter: 1714 loss: 4.98765189e-07
Iter: 1715 loss: 4.98795089e-07
Iter: 1716 loss: 4.98757686e-07
Iter: 1717 loss: 4.98727047e-07
Iter: 1718 loss: 4.98891e-07
Iter: 1719 loss: 4.98719089e-07
Iter: 1720 loss: 4.98697887e-07
Iter: 1721 loss: 4.98671682e-07
Iter: 1722 loss: 4.98657812e-07
Iter: 1723 loss: 4.98645477e-07
Iter: 1724 loss: 4.98648149e-07
Iter: 1725 loss: 4.98623535e-07
Iter: 1726 loss: 4.98579595e-07
Iter: 1727 loss: 4.99130238e-07
Iter: 1728 loss: 4.98580903e-07
Iter: 1729 loss: 4.98557597e-07
Iter: 1730 loss: 4.98565612e-07
Iter: 1731 loss: 4.98529062e-07
Iter: 1732 loss: 4.98490351e-07
Iter: 1733 loss: 4.99499436e-07
Iter: 1734 loss: 4.9848876e-07
Iter: 1735 loss: 4.98451357e-07
Iter: 1736 loss: 4.9843e-07
Iter: 1737 loss: 4.98418e-07
Iter: 1738 loss: 4.98372799e-07
Iter: 1739 loss: 4.98337045e-07
Iter: 1740 loss: 4.98330905e-07
Iter: 1741 loss: 4.9829822e-07
Iter: 1742 loss: 4.98746886e-07
Iter: 1743 loss: 4.98297709e-07
Iter: 1744 loss: 4.98279405e-07
Iter: 1745 loss: 4.98314307e-07
Iter: 1746 loss: 4.98257e-07
Iter: 1747 loss: 4.98232453e-07
Iter: 1748 loss: 4.98189e-07
Iter: 1749 loss: 4.99114e-07
Iter: 1750 loss: 4.98181578e-07
Iter: 1751 loss: 4.98134341e-07
Iter: 1752 loss: 4.98285544e-07
Iter: 1753 loss: 4.98115128e-07
Iter: 1754 loss: 4.98089491e-07
Iter: 1755 loss: 4.98085456e-07
Iter: 1756 loss: 4.98047143e-07
Iter: 1757 loss: 4.98051e-07
Iter: 1758 loss: 4.98035831e-07
Iter: 1759 loss: 4.97991152e-07
Iter: 1760 loss: 4.97993312e-07
Iter: 1761 loss: 4.97945052e-07
Iter: 1762 loss: 4.97928e-07
Iter: 1763 loss: 4.98266445e-07
Iter: 1764 loss: 4.97928909e-07
Iter: 1765 loss: 4.97891449e-07
Iter: 1766 loss: 4.98103248e-07
Iter: 1767 loss: 4.978898e-07
Iter: 1768 loss: 4.97874396e-07
Iter: 1769 loss: 4.97856263e-07
Iter: 1770 loss: 4.9790583e-07
Iter: 1771 loss: 4.97834151e-07
Iter: 1772 loss: 4.97815108e-07
Iter: 1773 loss: 4.97803512e-07
Iter: 1774 loss: 4.97787937e-07
Iter: 1775 loss: 4.97734277e-07
Iter: 1776 loss: 4.98768145e-07
Iter: 1777 loss: 4.97734845e-07
Iter: 1778 loss: 4.97719e-07
Iter: 1779 loss: 4.97715064e-07
Iter: 1780 loss: 4.97690735e-07
Iter: 1781 loss: 4.97705059e-07
Iter: 1782 loss: 4.97657936e-07
Iter: 1783 loss: 4.976323e-07
Iter: 1784 loss: 4.9762059e-07
Iter: 1785 loss: 4.97606266e-07
Iter: 1786 loss: 4.97566589e-07
Iter: 1787 loss: 4.97618657e-07
Iter: 1788 loss: 4.97544931e-07
Iter: 1789 loss: 4.97534757e-07
Iter: 1790 loss: 4.97527196e-07
Iter: 1791 loss: 4.97509234e-07
Iter: 1792 loss: 4.97460746e-07
Iter: 1793 loss: 4.97683118e-07
Iter: 1794 loss: 4.97445171e-07
Iter: 1795 loss: 4.9740089e-07
Iter: 1796 loss: 4.97404812e-07
Iter: 1797 loss: 4.97376789e-07
Iter: 1798 loss: 4.97491442e-07
Iter: 1799 loss: 4.97371957e-07
Iter: 1800 loss: 4.9732472e-07
Iter: 1801 loss: 4.97385258e-07
Iter: 1802 loss: 4.97307269e-07
Iter: 1803 loss: 4.97284418e-07
Iter: 1804 loss: 4.97258725e-07
Iter: 1805 loss: 4.97256e-07
Iter: 1806 loss: 4.97217059e-07
Iter: 1807 loss: 4.9722172e-07
Iter: 1808 loss: 4.97202166e-07
Iter: 1809 loss: 4.97164478e-07
Iter: 1810 loss: 4.97894234e-07
Iter: 1811 loss: 4.97159363e-07
Iter: 1812 loss: 4.9712321e-07
Iter: 1813 loss: 4.97113774e-07
Iter: 1814 loss: 4.97103656e-07
Iter: 1815 loss: 4.97086717e-07
Iter: 1816 loss: 4.97072563e-07
Iter: 1817 loss: 4.97024644e-07
Iter: 1818 loss: 4.97002929e-07
Iter: 1819 loss: 4.96974508e-07
Iter: 1820 loss: 4.96942334e-07
Iter: 1821 loss: 4.97284248e-07
Iter: 1822 loss: 4.96940686e-07
Iter: 1823 loss: 4.96912776e-07
Iter: 1824 loss: 4.96950577e-07
Iter: 1825 loss: 4.96889299e-07
Iter: 1826 loss: 4.96847463e-07
Iter: 1827 loss: 4.96841892e-07
Iter: 1828 loss: 4.96825805e-07
Iter: 1829 loss: 4.96776579e-07
Iter: 1830 loss: 4.9751759e-07
Iter: 1831 loss: 4.9677692e-07
Iter: 1832 loss: 4.96738267e-07
Iter: 1833 loss: 4.96829841e-07
Iter: 1834 loss: 4.9672019e-07
Iter: 1835 loss: 4.96678354e-07
Iter: 1836 loss: 4.96750147e-07
Iter: 1837 loss: 4.96659254e-07
Iter: 1838 loss: 4.96626058e-07
Iter: 1839 loss: 4.96572738e-07
Iter: 1840 loss: 4.96569328e-07
Iter: 1841 loss: 4.96556e-07
Iter: 1842 loss: 4.96536359e-07
Iter: 1843 loss: 4.96512939e-07
Iter: 1844 loss: 4.96445807e-07
Iter: 1845 loss: 4.97467227e-07
Iter: 1846 loss: 4.96442681e-07
Iter: 1847 loss: 4.96410223e-07
Iter: 1848 loss: 4.96419489e-07
Iter: 1849 loss: 4.96368557e-07
Iter: 1850 loss: 4.96427447e-07
Iter: 1851 loss: 4.96347297e-07
Iter: 1852 loss: 4.96305688e-07
Iter: 1853 loss: 4.96373673e-07
Iter: 1854 loss: 4.96292444e-07
Iter: 1855 loss: 4.96260157e-07
Iter: 1856 loss: 4.96486905e-07
Iter: 1857 loss: 4.96256348e-07
Iter: 1858 loss: 4.96218945e-07
Iter: 1859 loss: 4.96187567e-07
Iter: 1860 loss: 4.96192456e-07
Iter: 1861 loss: 4.96145276e-07
Iter: 1862 loss: 4.96505095e-07
Iter: 1863 loss: 4.96137545e-07
Iter: 1864 loss: 4.96068424e-07
Iter: 1865 loss: 4.9648e-07
Iter: 1866 loss: 4.96061091e-07
Iter: 1867 loss: 4.96033408e-07
Iter: 1868 loss: 4.9597287e-07
Iter: 1869 loss: 4.95974177e-07
Iter: 1870 loss: 4.95898348e-07
Iter: 1871 loss: 4.96089797e-07
Iter: 1872 loss: 4.95869813e-07
Iter: 1873 loss: 4.95823e-07
Iter: 1874 loss: 4.95824679e-07
Iter: 1875 loss: 4.95790687e-07
Iter: 1876 loss: 4.95748168e-07
Iter: 1877 loss: 4.9573373e-07
Iter: 1878 loss: 4.95669383e-07
Iter: 1879 loss: 4.95937797e-07
Iter: 1880 loss: 4.9568041e-07
Iter: 1881 loss: 4.95629195e-07
Iter: 1882 loss: 4.9595053e-07
Iter: 1883 loss: 4.95627091e-07
Iter: 1884 loss: 4.95595373e-07
Iter: 1885 loss: 4.95552058e-07
Iter: 1886 loss: 4.95548818e-07
Iter: 1887 loss: 4.9548845e-07
Iter: 1888 loss: 4.9611441e-07
Iter: 1889 loss: 4.95482766e-07
Iter: 1890 loss: 4.9543678e-07
Iter: 1891 loss: 4.95391532e-07
Iter: 1892 loss: 4.95391419e-07
Iter: 1893 loss: 4.95338327e-07
Iter: 1894 loss: 4.95512722e-07
Iter: 1895 loss: 4.95329e-07
Iter: 1896 loss: 4.95302629e-07
Iter: 1897 loss: 4.95704398e-07
Iter: 1898 loss: 4.95301833e-07
Iter: 1899 loss: 4.95273412e-07
Iter: 1900 loss: 4.95231689e-07
Iter: 1901 loss: 4.95221798e-07
Iter: 1902 loss: 4.95177346e-07
Iter: 1903 loss: 4.95235099e-07
Iter: 1904 loss: 4.95145173e-07
Iter: 1905 loss: 4.9508435e-07
Iter: 1906 loss: 4.95251e-07
Iter: 1907 loss: 4.95083896e-07
Iter: 1908 loss: 4.95028871e-07
Iter: 1909 loss: 4.95539894e-07
Iter: 1910 loss: 4.95018298e-07
Iter: 1911 loss: 4.94986239e-07
Iter: 1912 loss: 4.94982828e-07
Iter: 1913 loss: 4.94965661e-07
Iter: 1914 loss: 4.94934852e-07
Iter: 1915 loss: 4.94926951e-07
Iter: 1916 loss: 4.94924507e-07
Iter: 1917 loss: 4.94891879e-07
Iter: 1918 loss: 4.95417851e-07
Iter: 1919 loss: 4.94897677e-07
Iter: 1920 loss: 4.94863e-07
Iter: 1921 loss: 4.95220831e-07
Iter: 1922 loss: 4.9485709e-07
Iter: 1923 loss: 4.94839639e-07
Iter: 1924 loss: 4.94805136e-07
Iter: 1925 loss: 4.9480559e-07
Iter: 1926 loss: 4.94758297e-07
Iter: 1927 loss: 4.94891765e-07
Iter: 1928 loss: 4.94748861e-07
Iter: 1929 loss: 4.94703897e-07
Iter: 1930 loss: 4.94803e-07
Iter: 1931 loss: 4.94703784e-07
Iter: 1932 loss: 4.94682695e-07
Iter: 1933 loss: 4.94735218e-07
Iter: 1934 loss: 4.94659901e-07
Iter: 1935 loss: 4.94608855e-07
Iter: 1936 loss: 4.94633753e-07
Iter: 1937 loss: 4.94588051e-07
Iter: 1938 loss: 4.94529e-07
Iter: 1939 loss: 4.9470259e-07
Iter: 1940 loss: 4.94539336e-07
Iter: 1941 loss: 4.94502729e-07
Iter: 1942 loss: 4.94504491e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.4
+ date
Mon Oct 26 14:35:34 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.4/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.4_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi -1 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.4_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea75f21598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea5069c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea75f21268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea505e6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea505e6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50604d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea505a08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50565840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50565598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50585378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea504f3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50507730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50507e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea504d26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50473c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50473730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea5049b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea5041bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea503be950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea503aaf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea503c49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50399598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50399488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea502e4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea502e40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea502fd378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea502c6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50265510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea502650d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50233730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea5023d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea502146a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea50214268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea5021e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea501dbbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea501750d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.17845513e-06
Iter: 2 loss: 3.34803508e-06
Iter: 3 loss: 3.34515835e-06
Iter: 4 loss: 2.9115181e-06
Iter: 5 loss: 4.00217368e-06
Iter: 6 loss: 2.7603628e-06
Iter: 7 loss: 2.59446688e-06
Iter: 8 loss: 3.6237966e-06
Iter: 9 loss: 2.57488e-06
Iter: 10 loss: 2.4356093e-06
Iter: 11 loss: 2.94040365e-06
Iter: 12 loss: 2.40055192e-06
Iter: 13 loss: 2.3074067e-06
Iter: 14 loss: 2.41630687e-06
Iter: 15 loss: 2.25794065e-06
Iter: 16 loss: 2.22428844e-06
Iter: 17 loss: 2.22260906e-06
Iter: 18 loss: 2.19062235e-06
Iter: 19 loss: 2.11428573e-06
Iter: 20 loss: 2.97179031e-06
Iter: 21 loss: 2.1068322e-06
Iter: 22 loss: 2.00758063e-06
Iter: 23 loss: 2.06394225e-06
Iter: 24 loss: 1.94292807e-06
Iter: 25 loss: 1.87566798e-06
Iter: 26 loss: 1.85787815e-06
Iter: 27 loss: 1.83498628e-06
Iter: 28 loss: 1.77782e-06
Iter: 29 loss: 2.31885315e-06
Iter: 30 loss: 1.76985657e-06
Iter: 31 loss: 1.7202342e-06
Iter: 32 loss: 1.92289599e-06
Iter: 33 loss: 1.7093339e-06
Iter: 34 loss: 1.67215546e-06
Iter: 35 loss: 1.6908225e-06
Iter: 36 loss: 1.64730818e-06
Iter: 37 loss: 1.60374452e-06
Iter: 38 loss: 2.20051697e-06
Iter: 39 loss: 1.60359241e-06
Iter: 40 loss: 1.55139583e-06
Iter: 41 loss: 1.64160963e-06
Iter: 42 loss: 1.52824202e-06
Iter: 43 loss: 1.48954643e-06
Iter: 44 loss: 1.52220844e-06
Iter: 45 loss: 1.46657726e-06
Iter: 46 loss: 1.41788382e-06
Iter: 47 loss: 1.96411906e-06
Iter: 48 loss: 1.41699491e-06
Iter: 49 loss: 1.40215752e-06
Iter: 50 loss: 1.39695567e-06
Iter: 51 loss: 1.3886355e-06
Iter: 52 loss: 1.3661836e-06
Iter: 53 loss: 1.47142339e-06
Iter: 54 loss: 1.36202823e-06
Iter: 55 loss: 1.33973e-06
Iter: 56 loss: 1.41132341e-06
Iter: 57 loss: 1.33340598e-06
Iter: 58 loss: 1.31943591e-06
Iter: 59 loss: 1.29149362e-06
Iter: 60 loss: 1.82437395e-06
Iter: 61 loss: 1.29118064e-06
Iter: 62 loss: 1.27719829e-06
Iter: 63 loss: 1.27154e-06
Iter: 64 loss: 1.25456597e-06
Iter: 65 loss: 1.20992718e-06
Iter: 66 loss: 1.54276916e-06
Iter: 67 loss: 1.20071513e-06
Iter: 68 loss: 1.163805e-06
Iter: 69 loss: 1.36160668e-06
Iter: 70 loss: 1.15821422e-06
Iter: 71 loss: 1.13540978e-06
Iter: 72 loss: 1.20239429e-06
Iter: 73 loss: 1.12840166e-06
Iter: 74 loss: 1.11036661e-06
Iter: 75 loss: 1.20049924e-06
Iter: 76 loss: 1.10736323e-06
Iter: 77 loss: 1.09277266e-06
Iter: 78 loss: 1.09250095e-06
Iter: 79 loss: 1.08590143e-06
Iter: 80 loss: 1.07101164e-06
Iter: 81 loss: 1.2743933e-06
Iter: 82 loss: 1.07013466e-06
Iter: 83 loss: 1.05027243e-06
Iter: 84 loss: 1.28719216e-06
Iter: 85 loss: 1.05000618e-06
Iter: 86 loss: 1.04202763e-06
Iter: 87 loss: 1.02891045e-06
Iter: 88 loss: 1.02885156e-06
Iter: 89 loss: 1.023599e-06
Iter: 90 loss: 1.02099375e-06
Iter: 91 loss: 1.01621777e-06
Iter: 92 loss: 1.00563648e-06
Iter: 93 loss: 1.15544412e-06
Iter: 94 loss: 1.00506873e-06
Iter: 95 loss: 9.90270337e-07
Iter: 96 loss: 1.03335685e-06
Iter: 97 loss: 9.85693305e-07
Iter: 98 loss: 9.76903607e-07
Iter: 99 loss: 9.76830734e-07
Iter: 100 loss: 9.68253744e-07
Iter: 101 loss: 9.53699214e-07
Iter: 102 loss: 9.53656e-07
Iter: 103 loss: 9.40357324e-07
Iter: 104 loss: 9.49467051e-07
Iter: 105 loss: 9.31993213e-07
Iter: 106 loss: 9.21641345e-07
Iter: 107 loss: 9.60427087e-07
Iter: 108 loss: 9.19108743e-07
Iter: 109 loss: 9.09688254e-07
Iter: 110 loss: 9.30461624e-07
Iter: 111 loss: 9.06119567e-07
Iter: 112 loss: 8.97871246e-07
Iter: 113 loss: 8.97380289e-07
Iter: 114 loss: 8.928846e-07
Iter: 115 loss: 8.82478162e-07
Iter: 116 loss: 1.01383944e-06
Iter: 117 loss: 8.81729875e-07
Iter: 118 loss: 8.7139955e-07
Iter: 119 loss: 8.7124829e-07
Iter: 120 loss: 8.66672508e-07
Iter: 121 loss: 8.5677641e-07
Iter: 122 loss: 1.00843693e-06
Iter: 123 loss: 8.56403744e-07
Iter: 124 loss: 8.5515228e-07
Iter: 125 loss: 8.52097799e-07
Iter: 126 loss: 8.48952e-07
Iter: 127 loss: 8.44049509e-07
Iter: 128 loss: 8.4399619e-07
Iter: 129 loss: 8.39927964e-07
Iter: 130 loss: 8.37624441e-07
Iter: 131 loss: 8.35880769e-07
Iter: 132 loss: 8.28415295e-07
Iter: 133 loss: 8.82697464e-07
Iter: 134 loss: 8.27794338e-07
Iter: 135 loss: 8.21793151e-07
Iter: 136 loss: 8.74437433e-07
Iter: 137 loss: 8.21465505e-07
Iter: 138 loss: 8.16407692e-07
Iter: 139 loss: 8.10327322e-07
Iter: 140 loss: 8.0972e-07
Iter: 141 loss: 8.03145042e-07
Iter: 142 loss: 8.01583724e-07
Iter: 143 loss: 7.97448e-07
Iter: 144 loss: 7.9582793e-07
Iter: 145 loss: 7.94701691e-07
Iter: 146 loss: 7.91915568e-07
Iter: 147 loss: 8.00334135e-07
Iter: 148 loss: 7.91094067e-07
Iter: 149 loss: 7.89309297e-07
Iter: 150 loss: 7.86931366e-07
Iter: 151 loss: 7.86799831e-07
Iter: 152 loss: 7.81215476e-07
Iter: 153 loss: 7.89863293e-07
Iter: 154 loss: 7.78597041e-07
Iter: 155 loss: 7.74709179e-07
Iter: 156 loss: 7.69798589e-07
Iter: 157 loss: 7.69434e-07
Iter: 158 loss: 7.6528454e-07
Iter: 159 loss: 7.64634933e-07
Iter: 160 loss: 7.61909348e-07
Iter: 161 loss: 7.5560888e-07
Iter: 162 loss: 8.33815648e-07
Iter: 163 loss: 7.55147653e-07
Iter: 164 loss: 7.51254902e-07
Iter: 165 loss: 7.88604211e-07
Iter: 166 loss: 7.51087555e-07
Iter: 167 loss: 7.49289256e-07
Iter: 168 loss: 7.49195578e-07
Iter: 169 loss: 7.47697072e-07
Iter: 170 loss: 7.48151706e-07
Iter: 171 loss: 7.46620628e-07
Iter: 172 loss: 7.44311137e-07
Iter: 173 loss: 7.38903623e-07
Iter: 174 loss: 8.03612636e-07
Iter: 175 loss: 7.38448534e-07
Iter: 176 loss: 7.32635897e-07
Iter: 177 loss: 7.78886601e-07
Iter: 178 loss: 7.32241688e-07
Iter: 179 loss: 7.31161549e-07
Iter: 180 loss: 7.30177874e-07
Iter: 181 loss: 7.28760938e-07
Iter: 182 loss: 7.2526268e-07
Iter: 183 loss: 7.60671242e-07
Iter: 184 loss: 7.24842039e-07
Iter: 185 loss: 7.24981e-07
Iter: 186 loss: 7.23386052e-07
Iter: 187 loss: 7.22474056e-07
Iter: 188 loss: 7.20094363e-07
Iter: 189 loss: 7.38595247e-07
Iter: 190 loss: 7.19613695e-07
Iter: 191 loss: 7.17407147e-07
Iter: 192 loss: 7.38605e-07
Iter: 193 loss: 7.17332966e-07
Iter: 194 loss: 7.14493581e-07
Iter: 195 loss: 7.1409994e-07
Iter: 196 loss: 7.12061819e-07
Iter: 197 loss: 7.08942707e-07
Iter: 198 loss: 7.05064508e-07
Iter: 199 loss: 7.04744139e-07
Iter: 200 loss: 7.02214095e-07
Iter: 201 loss: 7.02077671e-07
Iter: 202 loss: 7.00014539e-07
Iter: 203 loss: 7.21632489e-07
Iter: 204 loss: 6.99948941e-07
Iter: 205 loss: 6.98833105e-07
Iter: 206 loss: 6.98203e-07
Iter: 207 loss: 6.97734208e-07
Iter: 208 loss: 6.96053576e-07
Iter: 209 loss: 6.95471044e-07
Iter: 210 loss: 6.94527955e-07
Iter: 211 loss: 6.92760068e-07
Iter: 212 loss: 7.1704585e-07
Iter: 213 loss: 6.92745914e-07
Iter: 214 loss: 6.90485649e-07
Iter: 215 loss: 6.89622084e-07
Iter: 216 loss: 6.88386763e-07
Iter: 217 loss: 6.86292196e-07
Iter: 218 loss: 6.96323866e-07
Iter: 219 loss: 6.85890939e-07
Iter: 220 loss: 6.83445307e-07
Iter: 221 loss: 6.90570573e-07
Iter: 222 loss: 6.8269128e-07
Iter: 223 loss: 6.81584652e-07
Iter: 224 loss: 6.80020548e-07
Iter: 225 loss: 6.79974619e-07
Iter: 226 loss: 6.78796539e-07
Iter: 227 loss: 6.78580932e-07
Iter: 228 loss: 6.77754315e-07
Iter: 229 loss: 6.757856e-07
Iter: 230 loss: 6.96201255e-07
Iter: 231 loss: 6.75540036e-07
Iter: 232 loss: 6.7316023e-07
Iter: 233 loss: 6.74031e-07
Iter: 234 loss: 6.71506598e-07
Iter: 235 loss: 6.70365239e-07
Iter: 236 loss: 6.69811243e-07
Iter: 237 loss: 6.68164034e-07
Iter: 238 loss: 6.66428321e-07
Iter: 239 loss: 6.66127e-07
Iter: 240 loss: 6.64196705e-07
Iter: 241 loss: 6.73975137e-07
Iter: 242 loss: 6.63872243e-07
Iter: 243 loss: 6.62630327e-07
Iter: 244 loss: 6.62945297e-07
Iter: 245 loss: 6.61704462e-07
Iter: 246 loss: 6.60898422e-07
Iter: 247 loss: 6.60747e-07
Iter: 248 loss: 6.59713407e-07
Iter: 249 loss: 6.57227929e-07
Iter: 250 loss: 6.84203656e-07
Iter: 251 loss: 6.56967245e-07
Iter: 252 loss: 6.56532222e-07
Iter: 253 loss: 6.55948043e-07
Iter: 254 loss: 6.54979885e-07
Iter: 255 loss: 6.52759695e-07
Iter: 256 loss: 6.80261223e-07
Iter: 257 loss: 6.52591211e-07
Iter: 258 loss: 6.50415529e-07
Iter: 259 loss: 6.51717471e-07
Iter: 260 loss: 6.4900297e-07
Iter: 261 loss: 6.47800391e-07
Iter: 262 loss: 6.47791353e-07
Iter: 263 loss: 6.46442913e-07
Iter: 264 loss: 6.51407674e-07
Iter: 265 loss: 6.46107821e-07
Iter: 266 loss: 6.45044565e-07
Iter: 267 loss: 6.42219447e-07
Iter: 268 loss: 6.63281e-07
Iter: 269 loss: 6.41632937e-07
Iter: 270 loss: 6.38746542e-07
Iter: 271 loss: 6.54730059e-07
Iter: 272 loss: 6.38326e-07
Iter: 273 loss: 6.36006e-07
Iter: 274 loss: 6.47269189e-07
Iter: 275 loss: 6.35602646e-07
Iter: 276 loss: 6.36172445e-07
Iter: 277 loss: 6.35007893e-07
Iter: 278 loss: 6.34523417e-07
Iter: 279 loss: 6.33253762e-07
Iter: 280 loss: 6.42085638e-07
Iter: 281 loss: 6.32989213e-07
Iter: 282 loss: 6.31574608e-07
Iter: 283 loss: 6.39494488e-07
Iter: 284 loss: 6.31378157e-07
Iter: 285 loss: 6.29903752e-07
Iter: 286 loss: 6.40037797e-07
Iter: 287 loss: 6.29763349e-07
Iter: 288 loss: 6.28285875e-07
Iter: 289 loss: 6.28249438e-07
Iter: 290 loss: 6.27099325e-07
Iter: 291 loss: 6.25911525e-07
Iter: 292 loss: 6.3578841e-07
Iter: 293 loss: 6.2583689e-07
Iter: 294 loss: 6.24636868e-07
Iter: 295 loss: 6.25354744e-07
Iter: 296 loss: 6.23867038e-07
Iter: 297 loss: 6.23090784e-07
Iter: 298 loss: 6.22684411e-07
Iter: 299 loss: 6.22320385e-07
Iter: 300 loss: 6.22013886e-07
Iter: 301 loss: 6.21720233e-07
Iter: 302 loss: 6.21346317e-07
Iter: 303 loss: 6.20232697e-07
Iter: 304 loss: 6.23374603e-07
Iter: 305 loss: 6.19634648e-07
Iter: 306 loss: 6.17590331e-07
Iter: 307 loss: 6.16958459e-07
Iter: 308 loss: 6.15741669e-07
Iter: 309 loss: 6.13563316e-07
Iter: 310 loss: 6.33984314e-07
Iter: 311 loss: 6.13486918e-07
Iter: 312 loss: 6.1324306e-07
Iter: 313 loss: 6.12901715e-07
Iter: 314 loss: 6.12278598e-07
Iter: 315 loss: 6.12163149e-07
Iter: 316 loss: 6.11747282e-07
Iter: 317 loss: 6.11284463e-07
Iter: 318 loss: 6.11713631e-07
Iter: 319 loss: 6.11021846e-07
Iter: 320 loss: 6.10337622e-07
Iter: 321 loss: 6.15981889e-07
Iter: 322 loss: 6.10320342e-07
Iter: 323 loss: 6.09859626e-07
Iter: 324 loss: 6.09358608e-07
Iter: 325 loss: 6.0930779e-07
Iter: 326 loss: 6.08267158e-07
Iter: 327 loss: 6.116382e-07
Iter: 328 loss: 6.07979814e-07
Iter: 329 loss: 6.07000686e-07
Iter: 330 loss: 6.07974073e-07
Iter: 331 loss: 6.06464e-07
Iter: 332 loss: 6.05583637e-07
Iter: 333 loss: 6.06600793e-07
Iter: 334 loss: 6.05112234e-07
Iter: 335 loss: 6.0465095e-07
Iter: 336 loss: 6.04561933e-07
Iter: 337 loss: 6.04262823e-07
Iter: 338 loss: 6.03504475e-07
Iter: 339 loss: 6.11689302e-07
Iter: 340 loss: 6.03428703e-07
Iter: 341 loss: 6.02676096e-07
Iter: 342 loss: 6.02463501e-07
Iter: 343 loss: 6.02001649e-07
Iter: 344 loss: 6.00554e-07
Iter: 345 loss: 6.01667239e-07
Iter: 346 loss: 5.996651e-07
Iter: 347 loss: 5.98897941e-07
Iter: 348 loss: 5.98640042e-07
Iter: 349 loss: 5.97591452e-07
Iter: 350 loss: 5.98469e-07
Iter: 351 loss: 5.96972882e-07
Iter: 352 loss: 5.96492043e-07
Iter: 353 loss: 5.9778472e-07
Iter: 354 loss: 5.96347206e-07
Iter: 355 loss: 5.95660651e-07
Iter: 356 loss: 5.97573603e-07
Iter: 357 loss: 5.95445954e-07
Iter: 358 loss: 5.95147867e-07
Iter: 359 loss: 5.95457891e-07
Iter: 360 loss: 5.94971652e-07
Iter: 361 loss: 5.94477854e-07
Iter: 362 loss: 5.95747451e-07
Iter: 363 loss: 5.94303287e-07
Iter: 364 loss: 5.93883613e-07
Iter: 365 loss: 5.93417496e-07
Iter: 366 loss: 5.93344794e-07
Iter: 367 loss: 5.92422452e-07
Iter: 368 loss: 5.95329311e-07
Iter: 369 loss: 5.92151764e-07
Iter: 370 loss: 5.91118294e-07
Iter: 371 loss: 5.95458232e-07
Iter: 372 loss: 5.90910247e-07
Iter: 373 loss: 5.90356137e-07
Iter: 374 loss: 5.89847957e-07
Iter: 375 loss: 5.89704655e-07
Iter: 376 loss: 5.89070055e-07
Iter: 377 loss: 5.90193167e-07
Iter: 378 loss: 5.88785383e-07
Iter: 379 loss: 5.8807484e-07
Iter: 380 loss: 5.8976434e-07
Iter: 381 loss: 5.87831437e-07
Iter: 382 loss: 5.8760088e-07
Iter: 383 loss: 5.87449335e-07
Iter: 384 loss: 5.87070133e-07
Iter: 385 loss: 5.85993234e-07
Iter: 386 loss: 5.92161484e-07
Iter: 387 loss: 5.85659222e-07
Iter: 388 loss: 5.85178441e-07
Iter: 389 loss: 5.85014675e-07
Iter: 390 loss: 5.843184e-07
Iter: 391 loss: 5.84146051e-07
Iter: 392 loss: 5.8369983e-07
Iter: 393 loss: 5.83343422e-07
Iter: 394 loss: 5.88097862e-07
Iter: 395 loss: 5.83340238e-07
Iter: 396 loss: 5.83010774e-07
Iter: 397 loss: 5.82656128e-07
Iter: 398 loss: 5.8259684e-07
Iter: 399 loss: 5.82199618e-07
Iter: 400 loss: 5.83616725e-07
Iter: 401 loss: 5.8207894e-07
Iter: 402 loss: 5.81609072e-07
Iter: 403 loss: 5.83959263e-07
Iter: 404 loss: 5.81516645e-07
Iter: 405 loss: 5.81110214e-07
Iter: 406 loss: 5.81091285e-07
Iter: 407 loss: 5.80774e-07
Iter: 408 loss: 5.80134156e-07
Iter: 409 loss: 5.79311404e-07
Iter: 410 loss: 5.79249672e-07
Iter: 411 loss: 5.78271397e-07
Iter: 412 loss: 5.81159668e-07
Iter: 413 loss: 5.77950914e-07
Iter: 414 loss: 5.7713828e-07
Iter: 415 loss: 5.79383368e-07
Iter: 416 loss: 5.76892035e-07
Iter: 417 loss: 5.76864636e-07
Iter: 418 loss: 5.76523178e-07
Iter: 419 loss: 5.76199795e-07
Iter: 420 loss: 5.75492209e-07
Iter: 421 loss: 5.87374075e-07
Iter: 422 loss: 5.75472e-07
Iter: 423 loss: 5.74794058e-07
Iter: 424 loss: 5.766002e-07
Iter: 425 loss: 5.74550768e-07
Iter: 426 loss: 5.73620923e-07
Iter: 427 loss: 5.80349365e-07
Iter: 428 loss: 5.73528496e-07
Iter: 429 loss: 5.7318573e-07
Iter: 430 loss: 5.72729448e-07
Iter: 431 loss: 5.72700287e-07
Iter: 432 loss: 5.72266799e-07
Iter: 433 loss: 5.72235535e-07
Iter: 434 loss: 5.72038857e-07
Iter: 435 loss: 5.71523742e-07
Iter: 436 loss: 5.75037461e-07
Iter: 437 loss: 5.71402097e-07
Iter: 438 loss: 5.71066266e-07
Iter: 439 loss: 5.71047963e-07
Iter: 440 loss: 5.70630846e-07
Iter: 441 loss: 5.70128464e-07
Iter: 442 loss: 5.70058603e-07
Iter: 443 loss: 5.69481301e-07
Iter: 444 loss: 5.68603127e-07
Iter: 445 loss: 5.68584539e-07
Iter: 446 loss: 5.67753546e-07
Iter: 447 loss: 5.77016579e-07
Iter: 448 loss: 5.677378e-07
Iter: 449 loss: 5.67271854e-07
Iter: 450 loss: 5.67272423e-07
Iter: 451 loss: 5.67007362e-07
Iter: 452 loss: 5.69306849e-07
Iter: 453 loss: 5.67002871e-07
Iter: 454 loss: 5.66739e-07
Iter: 455 loss: 5.66215192e-07
Iter: 456 loss: 5.76180298e-07
Iter: 457 loss: 5.66220365e-07
Iter: 458 loss: 5.65747769e-07
Iter: 459 loss: 5.69976464e-07
Iter: 460 loss: 5.65716391e-07
Iter: 461 loss: 5.65124139e-07
Iter: 462 loss: 5.65427968e-07
Iter: 463 loss: 5.64727259e-07
Iter: 464 loss: 5.64348056e-07
Iter: 465 loss: 5.64419111e-07
Iter: 466 loss: 5.64057189e-07
Iter: 467 loss: 5.63657579e-07
Iter: 468 loss: 5.63646608e-07
Iter: 469 loss: 5.63471531e-07
Iter: 470 loss: 5.63026e-07
Iter: 471 loss: 5.67566758e-07
Iter: 472 loss: 5.62966534e-07
Iter: 473 loss: 5.62908099e-07
Iter: 474 loss: 5.62722789e-07
Iter: 475 loss: 5.62569312e-07
Iter: 476 loss: 5.62119396e-07
Iter: 477 loss: 5.6497629e-07
Iter: 478 loss: 5.62018613e-07
Iter: 479 loss: 5.61490879e-07
Iter: 480 loss: 5.61236106e-07
Iter: 481 loss: 5.60994181e-07
Iter: 482 loss: 5.60571948e-07
Iter: 483 loss: 5.60462524e-07
Iter: 484 loss: 5.60194508e-07
Iter: 485 loss: 5.5978029e-07
Iter: 486 loss: 5.60713772e-07
Iter: 487 loss: 5.5963028e-07
Iter: 488 loss: 5.59232262e-07
Iter: 489 loss: 5.59249372e-07
Iter: 490 loss: 5.58922238e-07
Iter: 491 loss: 5.58396437e-07
Iter: 492 loss: 5.58386773e-07
Iter: 493 loss: 5.58132683e-07
Iter: 494 loss: 5.58102101e-07
Iter: 495 loss: 5.5775638e-07
Iter: 496 loss: 5.57370186e-07
Iter: 497 loss: 5.5731681e-07
Iter: 498 loss: 5.56865302e-07
Iter: 499 loss: 5.58267914e-07
Iter: 500 loss: 5.56729901e-07
Iter: 501 loss: 5.56432155e-07
Iter: 502 loss: 5.60295177e-07
Iter: 503 loss: 5.56432894e-07
Iter: 504 loss: 5.56199211e-07
Iter: 505 loss: 5.56696477e-07
Iter: 506 loss: 5.56108716e-07
Iter: 507 loss: 5.55941369e-07
Iter: 508 loss: 5.55658914e-07
Iter: 509 loss: 5.62618311e-07
Iter: 510 loss: 5.55660677e-07
Iter: 511 loss: 5.55563133e-07
Iter: 512 loss: 5.55488725e-07
Iter: 513 loss: 5.55333145e-07
Iter: 514 loss: 5.54923304e-07
Iter: 515 loss: 5.58072315e-07
Iter: 516 loss: 5.54851056e-07
Iter: 517 loss: 5.54427857e-07
Iter: 518 loss: 5.57959311e-07
Iter: 519 loss: 5.54398412e-07
Iter: 520 loss: 5.54087592e-07
Iter: 521 loss: 5.58252566e-07
Iter: 522 loss: 5.54085091e-07
Iter: 523 loss: 5.53824293e-07
Iter: 524 loss: 5.53656832e-07
Iter: 525 loss: 5.53564462e-07
Iter: 526 loss: 5.53321172e-07
Iter: 527 loss: 5.53965037e-07
Iter: 528 loss: 5.532562e-07
Iter: 529 loss: 5.53040877e-07
Iter: 530 loss: 5.53044e-07
Iter: 531 loss: 5.52922415e-07
Iter: 532 loss: 5.52589256e-07
Iter: 533 loss: 5.5442365e-07
Iter: 534 loss: 5.52465337e-07
Iter: 535 loss: 5.51934932e-07
Iter: 536 loss: 5.55322117e-07
Iter: 537 loss: 5.51891162e-07
Iter: 538 loss: 5.51516905e-07
Iter: 539 loss: 5.56419423e-07
Iter: 540 loss: 5.51513722e-07
Iter: 541 loss: 5.51296466e-07
Iter: 542 loss: 5.50970185e-07
Iter: 543 loss: 5.5095478e-07
Iter: 544 loss: 5.50626396e-07
Iter: 545 loss: 5.52013148e-07
Iter: 546 loss: 5.50580467e-07
Iter: 547 loss: 5.50248103e-07
Iter: 548 loss: 5.53007169e-07
Iter: 549 loss: 5.50213144e-07
Iter: 550 loss: 5.50075072e-07
Iter: 551 loss: 5.49787558e-07
Iter: 552 loss: 5.54144435e-07
Iter: 553 loss: 5.49764536e-07
Iter: 554 loss: 5.49547963e-07
Iter: 555 loss: 5.49545064e-07
Iter: 556 loss: 5.49290803e-07
Iter: 557 loss: 5.49340484e-07
Iter: 558 loss: 5.49109643e-07
Iter: 559 loss: 5.48835544e-07
Iter: 560 loss: 5.48532739e-07
Iter: 561 loss: 5.48488174e-07
Iter: 562 loss: 5.48381706e-07
Iter: 563 loss: 5.48304115e-07
Iter: 564 loss: 5.48085723e-07
Iter: 565 loss: 5.47828904e-07
Iter: 566 loss: 5.47787408e-07
Iter: 567 loss: 5.47575667e-07
Iter: 568 loss: 5.47854256e-07
Iter: 569 loss: 5.47453055e-07
Iter: 570 loss: 5.47219202e-07
Iter: 571 loss: 5.49502374e-07
Iter: 572 loss: 5.47201353e-07
Iter: 573 loss: 5.46974718e-07
Iter: 574 loss: 5.46980857e-07
Iter: 575 loss: 5.46802312e-07
Iter: 576 loss: 5.46465628e-07
Iter: 577 loss: 5.46115075e-07
Iter: 578 loss: 5.46035551e-07
Iter: 579 loss: 5.458038e-07
Iter: 580 loss: 5.45754119e-07
Iter: 581 loss: 5.45488717e-07
Iter: 582 loss: 5.45227351e-07
Iter: 583 loss: 5.45186e-07
Iter: 584 loss: 5.44952456e-07
Iter: 585 loss: 5.45266289e-07
Iter: 586 loss: 5.44804834e-07
Iter: 587 loss: 5.44571662e-07
Iter: 588 loss: 5.44573595e-07
Iter: 589 loss: 5.44423415e-07
Iter: 590 loss: 5.44023578e-07
Iter: 591 loss: 5.47578907e-07
Iter: 592 loss: 5.43968e-07
Iter: 593 loss: 5.43618853e-07
Iter: 594 loss: 5.4752195e-07
Iter: 595 loss: 5.43621354e-07
Iter: 596 loss: 5.43262e-07
Iter: 597 loss: 5.44684553e-07
Iter: 598 loss: 5.43179112e-07
Iter: 599 loss: 5.42934345e-07
Iter: 600 loss: 5.42591067e-07
Iter: 601 loss: 5.42584587e-07
Iter: 602 loss: 5.42393821e-07
Iter: 603 loss: 5.42363182e-07
Iter: 604 loss: 5.42240628e-07
Iter: 605 loss: 5.42348289e-07
Iter: 606 loss: 5.42167527e-07
Iter: 607 loss: 5.42004614e-07
Iter: 608 loss: 5.4205e-07
Iter: 609 loss: 5.41873305e-07
Iter: 610 loss: 5.41697887e-07
Iter: 611 loss: 5.43229248e-07
Iter: 612 loss: 5.41676059e-07
Iter: 613 loss: 5.41466704e-07
Iter: 614 loss: 5.41650877e-07
Iter: 615 loss: 5.41339944e-07
Iter: 616 loss: 5.41122233e-07
Iter: 617 loss: 5.40711142e-07
Iter: 618 loss: 5.48972878e-07
Iter: 619 loss: 5.4069676e-07
Iter: 620 loss: 5.40688632e-07
Iter: 621 loss: 5.40477402e-07
Iter: 622 loss: 5.40316364e-07
Iter: 623 loss: 5.40008955e-07
Iter: 624 loss: 5.40003498e-07
Iter: 625 loss: 5.39779251e-07
Iter: 626 loss: 5.40391341e-07
Iter: 627 loss: 5.39707116e-07
Iter: 628 loss: 5.39537155e-07
Iter: 629 loss: 5.39535904e-07
Iter: 630 loss: 5.3942631e-07
Iter: 631 loss: 5.39197913e-07
Iter: 632 loss: 5.42639555e-07
Iter: 633 loss: 5.39198822e-07
Iter: 634 loss: 5.38934387e-07
Iter: 635 loss: 5.40755877e-07
Iter: 636 loss: 5.3890767e-07
Iter: 637 loss: 5.38670236e-07
Iter: 638 loss: 5.38792847e-07
Iter: 639 loss: 5.38505276e-07
Iter: 640 loss: 5.38199401e-07
Iter: 641 loss: 5.3856968e-07
Iter: 642 loss: 5.38039558e-07
Iter: 643 loss: 5.37695939e-07
Iter: 644 loss: 5.38647e-07
Iter: 645 loss: 5.37590836e-07
Iter: 646 loss: 5.37285871e-07
Iter: 647 loss: 5.41721647e-07
Iter: 648 loss: 5.37293488e-07
Iter: 649 loss: 5.37161441e-07
Iter: 650 loss: 5.3693168e-07
Iter: 651 loss: 5.42637167e-07
Iter: 652 loss: 5.36930031e-07
Iter: 653 loss: 5.36751315e-07
Iter: 654 loss: 5.36758762e-07
Iter: 655 loss: 5.36578113e-07
Iter: 656 loss: 5.36358812e-07
Iter: 657 loss: 5.36324706e-07
Iter: 658 loss: 5.36077891e-07
Iter: 659 loss: 5.35984327e-07
Iter: 660 loss: 5.35847221e-07
Iter: 661 loss: 5.35915206e-07
Iter: 662 loss: 5.35711479e-07
Iter: 663 loss: 5.35575793e-07
Iter: 664 loss: 5.3533671e-07
Iter: 665 loss: 5.40805559e-07
Iter: 666 loss: 5.35323124e-07
Iter: 667 loss: 5.35140657e-07
Iter: 668 loss: 5.36902292e-07
Iter: 669 loss: 5.35140771e-07
Iter: 670 loss: 5.34938181e-07
Iter: 671 loss: 5.35763604e-07
Iter: 672 loss: 5.34909077e-07
Iter: 673 loss: 5.34746164e-07
Iter: 674 loss: 5.34587571e-07
Iter: 675 loss: 5.34544824e-07
Iter: 676 loss: 5.34250489e-07
Iter: 677 loss: 5.34781179e-07
Iter: 678 loss: 5.34127e-07
Iter: 679 loss: 5.33894422e-07
Iter: 680 loss: 5.33886293e-07
Iter: 681 loss: 5.33702064e-07
Iter: 682 loss: 5.33401e-07
Iter: 683 loss: 5.3340932e-07
Iter: 684 loss: 5.3328597e-07
Iter: 685 loss: 5.33276193e-07
Iter: 686 loss: 5.33148182e-07
Iter: 687 loss: 5.33229866e-07
Iter: 688 loss: 5.33072296e-07
Iter: 689 loss: 5.3291933e-07
Iter: 690 loss: 5.32610102e-07
Iter: 691 loss: 5.3767576e-07
Iter: 692 loss: 5.32590661e-07
Iter: 693 loss: 5.32356239e-07
Iter: 694 loss: 5.32360275e-07
Iter: 695 loss: 5.32074694e-07
Iter: 696 loss: 5.32248521e-07
Iter: 697 loss: 5.31882222e-07
Iter: 698 loss: 5.31643e-07
Iter: 699 loss: 5.31536443e-07
Iter: 700 loss: 5.31404567e-07
Iter: 701 loss: 5.31175e-07
Iter: 702 loss: 5.31155649e-07
Iter: 703 loss: 5.31037927e-07
Iter: 704 loss: 5.31014848e-07
Iter: 705 loss: 5.30942316e-07
Iter: 706 loss: 5.30798388e-07
Iter: 707 loss: 5.30828288e-07
Iter: 708 loss: 5.30673219e-07
Iter: 709 loss: 5.30511443e-07
Iter: 710 loss: 5.32407569e-07
Iter: 711 loss: 5.30505304e-07
Iter: 712 loss: 5.3032079e-07
Iter: 713 loss: 5.3022552e-07
Iter: 714 loss: 5.30153898e-07
Iter: 715 loss: 5.29972624e-07
Iter: 716 loss: 5.30220063e-07
Iter: 717 loss: 5.29873887e-07
Iter: 718 loss: 5.29622866e-07
Iter: 719 loss: 5.31492674e-07
Iter: 720 loss: 5.29610475e-07
Iter: 721 loss: 5.29462397e-07
Iter: 722 loss: 5.29238037e-07
Iter: 723 loss: 5.2923906e-07
Iter: 724 loss: 5.29059776e-07
Iter: 725 loss: 5.29896965e-07
Iter: 726 loss: 5.29011459e-07
Iter: 727 loss: 5.28884243e-07
Iter: 728 loss: 5.28866565e-07
Iter: 729 loss: 5.28793237e-07
Iter: 730 loss: 5.28597241e-07
Iter: 731 loss: 5.29335921e-07
Iter: 732 loss: 5.28492e-07
Iter: 733 loss: 5.28343435e-07
Iter: 734 loss: 5.28307851e-07
Iter: 735 loss: 5.2818018e-07
Iter: 736 loss: 5.28025737e-07
Iter: 737 loss: 5.27994757e-07
Iter: 738 loss: 5.27807288e-07
Iter: 739 loss: 5.28397436e-07
Iter: 740 loss: 5.27755788e-07
Iter: 741 loss: 5.27598331e-07
Iter: 742 loss: 5.28810347e-07
Iter: 743 loss: 5.27585144e-07
Iter: 744 loss: 5.27425357e-07
Iter: 745 loss: 5.27755674e-07
Iter: 746 loss: 5.2735237e-07
Iter: 747 loss: 5.27233738e-07
Iter: 748 loss: 5.27122666e-07
Iter: 749 loss: 5.27086115e-07
Iter: 750 loss: 5.26959866e-07
Iter: 751 loss: 5.26948952e-07
Iter: 752 loss: 5.26841632e-07
Iter: 753 loss: 5.26581289e-07
Iter: 754 loss: 5.28950409e-07
Iter: 755 loss: 5.26536e-07
Iter: 756 loss: 5.26278029e-07
Iter: 757 loss: 5.28038697e-07
Iter: 758 loss: 5.26235851e-07
Iter: 759 loss: 5.26145186e-07
Iter: 760 loss: 5.26129384e-07
Iter: 761 loss: 5.26049632e-07
Iter: 762 loss: 5.25876658e-07
Iter: 763 loss: 5.2889061e-07
Iter: 764 loss: 5.25862e-07
Iter: 765 loss: 5.25737164e-07
Iter: 766 loss: 5.25739e-07
Iter: 767 loss: 5.25592441e-07
Iter: 768 loss: 5.25477844e-07
Iter: 769 loss: 5.25432029e-07
Iter: 770 loss: 5.2524922e-07
Iter: 771 loss: 5.25623477e-07
Iter: 772 loss: 5.25173959e-07
Iter: 773 loss: 5.24986547e-07
Iter: 774 loss: 5.25538269e-07
Iter: 775 loss: 5.24923394e-07
Iter: 776 loss: 5.24739164e-07
Iter: 777 loss: 5.26665644e-07
Iter: 778 loss: 5.24738766e-07
Iter: 779 loss: 5.245995e-07
Iter: 780 loss: 5.24463417e-07
Iter: 781 loss: 5.24448865e-07
Iter: 782 loss: 5.24402765e-07
Iter: 783 loss: 5.24371387e-07
Iter: 784 loss: 5.24292147e-07
Iter: 785 loss: 5.24156633e-07
Iter: 786 loss: 5.26695544e-07
Iter: 787 loss: 5.24148561e-07
Iter: 788 loss: 5.23956203e-07
Iter: 789 loss: 5.23837457e-07
Iter: 790 loss: 5.23778795e-07
Iter: 791 loss: 5.2377527e-07
Iter: 792 loss: 5.23636686e-07
Iter: 793 loss: 5.23500034e-07
Iter: 794 loss: 5.23307904e-07
Iter: 795 loss: 5.23299548e-07
Iter: 796 loss: 5.23145218e-07
Iter: 797 loss: 5.24004747e-07
Iter: 798 loss: 5.23119525e-07
Iter: 799 loss: 5.22931316e-07
Iter: 800 loss: 5.23580923e-07
Iter: 801 loss: 5.22878736e-07
Iter: 802 loss: 5.22769369e-07
Iter: 803 loss: 5.22777555e-07
Iter: 804 loss: 5.22680466e-07
Iter: 805 loss: 5.22524147e-07
Iter: 806 loss: 5.22771415e-07
Iter: 807 loss: 5.22451387e-07
Iter: 808 loss: 5.22330481e-07
Iter: 809 loss: 5.23996277e-07
Iter: 810 loss: 5.22335e-07
Iter: 811 loss: 5.22205596e-07
Iter: 812 loss: 5.21918764e-07
Iter: 813 loss: 5.26050087e-07
Iter: 814 loss: 5.21896709e-07
Iter: 815 loss: 5.21704635e-07
Iter: 816 loss: 5.21699917e-07
Iter: 817 loss: 5.21506081e-07
Iter: 818 loss: 5.21486413e-07
Iter: 819 loss: 5.21348852e-07
Iter: 820 loss: 5.21175139e-07
Iter: 821 loss: 5.21021775e-07
Iter: 822 loss: 5.20978119e-07
Iter: 823 loss: 5.20834078e-07
Iter: 824 loss: 5.20838739e-07
Iter: 825 loss: 5.20682192e-07
Iter: 826 loss: 5.21253355e-07
Iter: 827 loss: 5.20633876e-07
Iter: 828 loss: 5.205344e-07
Iter: 829 loss: 5.20447884e-07
Iter: 830 loss: 5.20431115e-07
Iter: 831 loss: 5.20255355e-07
Iter: 832 loss: 5.21929906e-07
Iter: 833 loss: 5.20242e-07
Iter: 834 loss: 5.20126264e-07
Iter: 835 loss: 5.20019285e-07
Iter: 836 loss: 5.19976538e-07
Iter: 837 loss: 5.19821924e-07
Iter: 838 loss: 5.20627e-07
Iter: 839 loss: 5.1979481e-07
Iter: 840 loss: 5.19681066e-07
Iter: 841 loss: 5.20944e-07
Iter: 842 loss: 5.19677087e-07
Iter: 843 loss: 5.19575963e-07
Iter: 844 loss: 5.19512e-07
Iter: 845 loss: 5.19462674e-07
Iter: 846 loss: 5.19343075e-07
Iter: 847 loss: 5.19807259e-07
Iter: 848 loss: 5.1931994e-07
Iter: 849 loss: 5.19167429e-07
Iter: 850 loss: 5.19642867e-07
Iter: 851 loss: 5.19125479e-07
Iter: 852 loss: 5.19024e-07
Iter: 853 loss: 5.18803915e-07
Iter: 854 loss: 5.23115148e-07
Iter: 855 loss: 5.18805791e-07
Iter: 856 loss: 5.18568e-07
Iter: 857 loss: 5.19586536e-07
Iter: 858 loss: 5.18529419e-07
Iter: 859 loss: 5.1847735e-07
Iter: 860 loss: 5.18412776e-07
Iter: 861 loss: 5.18359741e-07
Iter: 862 loss: 5.18244065e-07
Iter: 863 loss: 5.19246498e-07
Iter: 864 loss: 5.18222464e-07
Iter: 865 loss: 5.18108891e-07
Iter: 866 loss: 5.18118668e-07
Iter: 867 loss: 5.18004072e-07
Iter: 868 loss: 5.17844342e-07
Iter: 869 loss: 5.21955e-07
Iter: 870 loss: 5.17847582e-07
Iter: 871 loss: 5.17640729e-07
Iter: 872 loss: 5.18320689e-07
Iter: 873 loss: 5.17596959e-07
Iter: 874 loss: 5.17428589e-07
Iter: 875 loss: 5.1844637e-07
Iter: 876 loss: 5.17411763e-07
Iter: 877 loss: 5.17246121e-07
Iter: 878 loss: 5.17937849e-07
Iter: 879 loss: 5.17225544e-07
Iter: 880 loss: 5.17145565e-07
Iter: 881 loss: 5.17146475e-07
Iter: 882 loss: 5.17059391e-07
Iter: 883 loss: 5.16970658e-07
Iter: 884 loss: 5.18518505e-07
Iter: 885 loss: 5.16975547e-07
Iter: 886 loss: 5.16922e-07
Iter: 887 loss: 5.16781313e-07
Iter: 888 loss: 5.1797457e-07
Iter: 889 loss: 5.16769148e-07
Iter: 890 loss: 5.16563603e-07
Iter: 891 loss: 5.16287969e-07
Iter: 892 loss: 5.16286377e-07
Iter: 893 loss: 5.16037062e-07
Iter: 894 loss: 5.16033481e-07
Iter: 895 loss: 5.15980787e-07
Iter: 896 loss: 5.15908255e-07
Iter: 897 loss: 5.15869488e-07
Iter: 898 loss: 5.15768761e-07
Iter: 899 loss: 5.1606213e-07
Iter: 900 loss: 5.15709473e-07
Iter: 901 loss: 5.15772854e-07
Iter: 902 loss: 5.15640579e-07
Iter: 903 loss: 5.15611703e-07
Iter: 904 loss: 5.15504553e-07
Iter: 905 loss: 5.16719865e-07
Iter: 906 loss: 5.15487898e-07
Iter: 907 loss: 5.15367333e-07
Iter: 908 loss: 5.15293266e-07
Iter: 909 loss: 5.15244778e-07
Iter: 910 loss: 5.15075953e-07
Iter: 911 loss: 5.17319108e-07
Iter: 912 loss: 5.15082093e-07
Iter: 913 loss: 5.14935e-07
Iter: 914 loss: 5.1511654e-07
Iter: 915 loss: 5.14871886e-07
Iter: 916 loss: 5.14737962e-07
Iter: 917 loss: 5.14945953e-07
Iter: 918 loss: 5.14673502e-07
Iter: 919 loss: 5.14571298e-07
Iter: 920 loss: 5.15810655e-07
Iter: 921 loss: 5.14566409e-07
Iter: 922 loss: 5.14475403e-07
Iter: 923 loss: 5.14453291e-07
Iter: 924 loss: 5.14379394e-07
Iter: 925 loss: 5.14272529e-07
Iter: 926 loss: 5.14032649e-07
Iter: 927 loss: 5.1859962e-07
Iter: 928 loss: 5.14029068e-07
Iter: 929 loss: 5.1371012e-07
Iter: 930 loss: 5.14646445e-07
Iter: 931 loss: 5.13615248e-07
Iter: 932 loss: 5.1371336e-07
Iter: 933 loss: 5.13512305e-07
Iter: 934 loss: 5.13405212e-07
Iter: 935 loss: 5.13190685e-07
Iter: 936 loss: 5.16454861e-07
Iter: 937 loss: 5.13194493e-07
Iter: 938 loss: 5.13026521e-07
Iter: 939 loss: 5.1308507e-07
Iter: 940 loss: 5.12925055e-07
Iter: 941 loss: 5.12825864e-07
Iter: 942 loss: 5.12816e-07
Iter: 943 loss: 5.12693532e-07
Iter: 944 loss: 5.12759527e-07
Iter: 945 loss: 5.12629754e-07
Iter: 946 loss: 5.12539486e-07
Iter: 947 loss: 5.12944894e-07
Iter: 948 loss: 5.12532779e-07
Iter: 949 loss: 5.12409258e-07
Iter: 950 loss: 5.12440124e-07
Iter: 951 loss: 5.12337124e-07
Iter: 952 loss: 5.12222755e-07
Iter: 953 loss: 5.12584052e-07
Iter: 954 loss: 5.12196266e-07
Iter: 955 loss: 5.12061206e-07
Iter: 956 loss: 5.12574047e-07
Iter: 957 loss: 5.12034489e-07
Iter: 958 loss: 5.11924156e-07
Iter: 959 loss: 5.12161819e-07
Iter: 960 loss: 5.11865437e-07
Iter: 961 loss: 5.11768746e-07
Iter: 962 loss: 5.11634937e-07
Iter: 963 loss: 5.1164244e-07
Iter: 964 loss: 5.11456278e-07
Iter: 965 loss: 5.11738961e-07
Iter: 966 loss: 5.11354756e-07
Iter: 967 loss: 5.11401481e-07
Iter: 968 loss: 5.11269491e-07
Iter: 969 loss: 5.11204576e-07
Iter: 970 loss: 5.11076621e-07
Iter: 971 loss: 5.11985206e-07
Iter: 972 loss: 5.11025462e-07
Iter: 973 loss: 5.10850441e-07
Iter: 974 loss: 5.10819177e-07
Iter: 975 loss: 5.1069685e-07
Iter: 976 loss: 5.10546499e-07
Iter: 977 loss: 5.12743895e-07
Iter: 978 loss: 5.10540531e-07
Iter: 979 loss: 5.10420364e-07
Iter: 980 loss: 5.11954113e-07
Iter: 981 loss: 5.10408654e-07
Iter: 982 loss: 5.10319296e-07
Iter: 983 loss: 5.10188e-07
Iter: 984 loss: 5.12955467e-07
Iter: 985 loss: 5.10175937e-07
Iter: 986 loss: 5.10106e-07
Iter: 987 loss: 5.10071629e-07
Iter: 988 loss: 5.10015298e-07
Iter: 989 loss: 5.09894733e-07
Iter: 990 loss: 5.12037275e-07
Iter: 991 loss: 5.09884899e-07
Iter: 992 loss: 5.09757228e-07
Iter: 993 loss: 5.10681048e-07
Iter: 994 loss: 5.09758365e-07
Iter: 995 loss: 5.09663835e-07
Iter: 996 loss: 5.10326345e-07
Iter: 997 loss: 5.09639335e-07
Iter: 998 loss: 5.09548386e-07
Iter: 999 loss: 5.0958e-07
Iter: 1000 loss: 5.09473e-07
Iter: 1001 loss: 5.09381948e-07
Iter: 1002 loss: 5.09286167e-07
Iter: 1003 loss: 5.09257177e-07
Iter: 1004 loss: 5.0922597e-07
Iter: 1005 loss: 5.09192091e-07
Iter: 1006 loss: 5.09117115e-07
Iter: 1007 loss: 5.08916514e-07
Iter: 1008 loss: 5.09647e-07
Iter: 1009 loss: 5.08832159e-07
Iter: 1010 loss: 5.08659241e-07
Iter: 1011 loss: 5.09830784e-07
Iter: 1012 loss: 5.08644348e-07
Iter: 1013 loss: 5.08553171e-07
Iter: 1014 loss: 5.08807e-07
Iter: 1015 loss: 5.08538051e-07
Iter: 1016 loss: 5.08445908e-07
Iter: 1017 loss: 5.0833853e-07
Iter: 1018 loss: 5.08335916e-07
Iter: 1019 loss: 5.08265771e-07
Iter: 1020 loss: 5.08252583e-07
Iter: 1021 loss: 5.0818619e-07
Iter: 1022 loss: 5.08146059e-07
Iter: 1023 loss: 5.08121673e-07
Iter: 1024 loss: 5.08022197e-07
Iter: 1025 loss: 5.07764526e-07
Iter: 1026 loss: 5.09350684e-07
Iter: 1027 loss: 5.07712912e-07
Iter: 1028 loss: 5.07540278e-07
Iter: 1029 loss: 5.07526522e-07
Iter: 1030 loss: 5.07375034e-07
Iter: 1031 loss: 5.08237747e-07
Iter: 1032 loss: 5.07350819e-07
Iter: 1033 loss: 5.0726959e-07
Iter: 1034 loss: 5.0711742e-07
Iter: 1035 loss: 5.10599364e-07
Iter: 1036 loss: 5.07120717e-07
Iter: 1037 loss: 5.06974629e-07
Iter: 1038 loss: 5.08806238e-07
Iter: 1039 loss: 5.0696309e-07
Iter: 1040 loss: 5.06846391e-07
Iter: 1041 loss: 5.06876859e-07
Iter: 1042 loss: 5.06767719e-07
Iter: 1043 loss: 5.06706556e-07
Iter: 1044 loss: 5.06701326e-07
Iter: 1045 loss: 5.06625383e-07
Iter: 1046 loss: 5.06462925e-07
Iter: 1047 loss: 5.07781124e-07
Iter: 1048 loss: 5.06423135e-07
Iter: 1049 loss: 5.06346737e-07
Iter: 1050 loss: 5.06315871e-07
Iter: 1051 loss: 5.06245215e-07
Iter: 1052 loss: 5.06208607e-07
Iter: 1053 loss: 5.06172171e-07
Iter: 1054 loss: 5.06079914e-07
Iter: 1055 loss: 5.06852416e-07
Iter: 1056 loss: 5.06073832e-07
Iter: 1057 loss: 5.05974413e-07
Iter: 1058 loss: 5.06049446e-07
Iter: 1059 loss: 5.05915182e-07
Iter: 1060 loss: 5.05812e-07
Iter: 1061 loss: 5.05813944e-07
Iter: 1062 loss: 5.05725666e-07
Iter: 1063 loss: 5.05626758e-07
Iter: 1064 loss: 5.05622666e-07
Iter: 1065 loss: 5.05559683e-07
Iter: 1066 loss: 5.05400862e-07
Iter: 1067 loss: 5.0758922e-07
Iter: 1068 loss: 5.05403e-07
Iter: 1069 loss: 5.05212029e-07
Iter: 1070 loss: 5.05526714e-07
Iter: 1071 loss: 5.05130402e-07
Iter: 1072 loss: 5.04971808e-07
Iter: 1073 loss: 5.04973286e-07
Iter: 1074 loss: 5.04879267e-07
Iter: 1075 loss: 5.0577006e-07
Iter: 1076 loss: 5.04871764e-07
Iter: 1077 loss: 5.04797299e-07
Iter: 1078 loss: 5.04668947e-07
Iter: 1079 loss: 5.04673e-07
Iter: 1080 loss: 5.04565094e-07
Iter: 1081 loss: 5.05293883e-07
Iter: 1082 loss: 5.0455e-07
Iter: 1083 loss: 5.04425e-07
Iter: 1084 loss: 5.04629213e-07
Iter: 1085 loss: 5.04362561e-07
Iter: 1086 loss: 5.04245634e-07
Iter: 1087 loss: 5.04168781e-07
Iter: 1088 loss: 5.04126547e-07
Iter: 1089 loss: 5.04013315e-07
Iter: 1090 loss: 5.04006948e-07
Iter: 1091 loss: 5.03926969e-07
Iter: 1092 loss: 5.03931744e-07
Iter: 1093 loss: 5.03864271e-07
Iter: 1094 loss: 5.03787533e-07
Iter: 1095 loss: 5.04800141e-07
Iter: 1096 loss: 5.03783326e-07
Iter: 1097 loss: 5.03727279e-07
Iter: 1098 loss: 5.03659351e-07
Iter: 1099 loss: 5.03646447e-07
Iter: 1100 loss: 5.03558965e-07
Iter: 1101 loss: 5.03348247e-07
Iter: 1102 loss: 5.06795629e-07
Iter: 1103 loss: 5.03347451e-07
Iter: 1104 loss: 5.03121214e-07
Iter: 1105 loss: 5.06467131e-07
Iter: 1106 loss: 5.03127922e-07
Iter: 1107 loss: 5.02998091e-07
Iter: 1108 loss: 5.04761204e-07
Iter: 1109 loss: 5.02995931e-07
Iter: 1110 loss: 5.02902708e-07
Iter: 1111 loss: 5.03407875e-07
Iter: 1112 loss: 5.02893158e-07
Iter: 1113 loss: 5.02832791e-07
Iter: 1114 loss: 5.02768501e-07
Iter: 1115 loss: 5.0275554e-07
Iter: 1116 loss: 5.02661749e-07
Iter: 1117 loss: 5.02668286e-07
Iter: 1118 loss: 5.02623323e-07
Iter: 1119 loss: 5.02508783e-07
Iter: 1120 loss: 5.03845058e-07
Iter: 1121 loss: 5.02498e-07
Iter: 1122 loss: 5.02361729e-07
Iter: 1123 loss: 5.02770206e-07
Iter: 1124 loss: 5.02323701e-07
Iter: 1125 loss: 5.02129296e-07
Iter: 1126 loss: 5.03228648e-07
Iter: 1127 loss: 5.02111902e-07
Iter: 1128 loss: 5.02031071e-07
Iter: 1129 loss: 5.02248781e-07
Iter: 1130 loss: 5.01988211e-07
Iter: 1131 loss: 5.018602e-07
Iter: 1132 loss: 5.0195132e-07
Iter: 1133 loss: 5.01787554e-07
Iter: 1134 loss: 5.0170695e-07
Iter: 1135 loss: 5.01696604e-07
Iter: 1136 loss: 5.0163078e-07
Iter: 1137 loss: 5.01517718e-07
Iter: 1138 loss: 5.01697627e-07
Iter: 1139 loss: 5.01470822e-07
Iter: 1140 loss: 5.01362e-07
Iter: 1141 loss: 5.01721502e-07
Iter: 1142 loss: 5.01341788e-07
Iter: 1143 loss: 5.01238219e-07
Iter: 1144 loss: 5.02763726e-07
Iter: 1145 loss: 5.01229351e-07
Iter: 1146 loss: 5.01153e-07
Iter: 1147 loss: 5.01162503e-07
Iter: 1148 loss: 5.01080876e-07
Iter: 1149 loss: 5.00992201e-07
Iter: 1150 loss: 5.01e-07
Iter: 1151 loss: 5.00928877e-07
Iter: 1152 loss: 5.00860097e-07
Iter: 1153 loss: 5.00842134e-07
Iter: 1154 loss: 5.00812121e-07
Iter: 1155 loss: 5.00731744e-07
Iter: 1156 loss: 5.01080763e-07
Iter: 1157 loss: 5.00694568e-07
Iter: 1158 loss: 5.00613965e-07
Iter: 1159 loss: 5.01641182e-07
Iter: 1160 loss: 5.00616295e-07
Iter: 1161 loss: 5.00509202e-07
Iter: 1162 loss: 5.00776082e-07
Iter: 1163 loss: 5.00488341e-07
Iter: 1164 loss: 5.00397164e-07
Iter: 1165 loss: 5.00248234e-07
Iter: 1166 loss: 5.03802426e-07
Iter: 1167 loss: 5.00248e-07
Iter: 1168 loss: 5.00084468e-07
Iter: 1169 loss: 5.00086685e-07
Iter: 1170 loss: 5.00010572e-07
Iter: 1171 loss: 4.99871817e-07
Iter: 1172 loss: 4.99869316e-07
Iter: 1173 loss: 4.99776888e-07
Iter: 1174 loss: 4.99951057e-07
Iter: 1175 loss: 4.99721068e-07
Iter: 1176 loss: 4.99651037e-07
Iter: 1177 loss: 5.00591796e-07
Iter: 1178 loss: 4.99647399e-07
Iter: 1179 loss: 4.99577709e-07
Iter: 1180 loss: 4.99678492e-07
Iter: 1181 loss: 4.9954059e-07
Iter: 1182 loss: 4.9945578e-07
Iter: 1183 loss: 4.99394275e-07
Iter: 1184 loss: 4.99381599e-07
Iter: 1185 loss: 4.99266775e-07
Iter: 1186 loss: 5.00776e-07
Iter: 1187 loss: 4.99264047e-07
Iter: 1188 loss: 4.99160251e-07
Iter: 1189 loss: 4.98981649e-07
Iter: 1190 loss: 4.98963288e-07
Iter: 1191 loss: 4.98800716e-07
Iter: 1192 loss: 4.98825273e-07
Iter: 1193 loss: 4.98675433e-07
Iter: 1194 loss: 4.9867333e-07
Iter: 1195 loss: 4.98580391e-07
Iter: 1196 loss: 4.98535542e-07
Iter: 1197 loss: 4.98419e-07
Iter: 1198 loss: 5.00111469e-07
Iter: 1199 loss: 4.98426971e-07
Iter: 1200 loss: 4.98347163e-07
Iter: 1201 loss: 4.98346424e-07
Iter: 1202 loss: 4.98263944e-07
Iter: 1203 loss: 4.98136615e-07
Iter: 1204 loss: 4.98132692e-07
Iter: 1205 loss: 4.98008944e-07
Iter: 1206 loss: 4.98011786e-07
Iter: 1207 loss: 4.97911515e-07
Iter: 1208 loss: 4.97763608e-07
Iter: 1209 loss: 4.97760141e-07
Iter: 1210 loss: 4.97628434e-07
Iter: 1211 loss: 4.98407189e-07
Iter: 1212 loss: 4.97594e-07
Iter: 1213 loss: 4.97539872e-07
Iter: 1214 loss: 4.97571705e-07
Iter: 1215 loss: 4.97499343e-07
Iter: 1216 loss: 4.97438236e-07
Iter: 1217 loss: 4.97555959e-07
Iter: 1218 loss: 4.97399128e-07
Iter: 1219 loss: 4.97324095e-07
Iter: 1220 loss: 4.98142469e-07
Iter: 1221 loss: 4.97331825e-07
Iter: 1222 loss: 4.97271913e-07
Iter: 1223 loss: 4.97093197e-07
Iter: 1224 loss: 4.9813957e-07
Iter: 1225 loss: 4.97067e-07
Iter: 1226 loss: 4.96964503e-07
Iter: 1227 loss: 4.9695916e-07
Iter: 1228 loss: 4.96840869e-07
Iter: 1229 loss: 4.97183862e-07
Iter: 1230 loss: 4.96789255e-07
Iter: 1231 loss: 4.96725761e-07
Iter: 1232 loss: 4.96718087e-07
Iter: 1233 loss: 4.96671191e-07
Iter: 1234 loss: 4.96593429e-07
Iter: 1235 loss: 4.96595646e-07
Iter: 1236 loss: 4.96559949e-07
Iter: 1237 loss: 4.9645314e-07
Iter: 1238 loss: 4.97262818e-07
Iter: 1239 loss: 4.96435632e-07
Iter: 1240 loss: 4.96344114e-07
Iter: 1241 loss: 4.97332849e-07
Iter: 1242 loss: 4.9633752e-07
Iter: 1243 loss: 4.96237476e-07
Iter: 1244 loss: 4.96511e-07
Iter: 1245 loss: 4.96213147e-07
Iter: 1246 loss: 4.96114239e-07
Iter: 1247 loss: 4.96172788e-07
Iter: 1248 loss: 4.96024086e-07
Iter: 1249 loss: 4.95930806e-07
Iter: 1250 loss: 4.96179439e-07
Iter: 1251 loss: 4.95890617e-07
Iter: 1252 loss: 4.9581547e-07
Iter: 1253 loss: 4.95824111e-07
Iter: 1254 loss: 4.95771417e-07
Iter: 1255 loss: 4.95726567e-07
Iter: 1256 loss: 4.95711902e-07
Iter: 1257 loss: 4.95649942e-07
Iter: 1258 loss: 4.95661197e-07
Iter: 1259 loss: 4.95592758e-07
Iter: 1260 loss: 4.95557742e-07
Iter: 1261 loss: 4.95543759e-07
Iter: 1262 loss: 4.9551312e-07
Iter: 1263 loss: 4.954e-07
Iter: 1264 loss: 4.96256632e-07
Iter: 1265 loss: 4.95389827e-07
Iter: 1266 loss: 4.95325935e-07
Iter: 1267 loss: 4.95326049e-07
Iter: 1268 loss: 4.95252266e-07
Iter: 1269 loss: 4.95202357e-07
Iter: 1270 loss: 4.95176e-07
Iter: 1271 loss: 4.95114875e-07
Iter: 1272 loss: 4.95371523e-07
Iter: 1273 loss: 4.95106065e-07
Iter: 1274 loss: 4.95063432e-07
Iter: 1275 loss: 4.9514631e-07
Iter: 1276 loss: 4.95047857e-07
Iter: 1277 loss: 4.95013296e-07
Iter: 1278 loss: 4.95014888e-07
Iter: 1279 loss: 4.94971346e-07
Iter: 1280 loss: 4.94921437e-07
Iter: 1281 loss: 4.94963274e-07
Iter: 1282 loss: 4.94881533e-07
Iter: 1283 loss: 4.94816504e-07
Iter: 1284 loss: 4.95185134e-07
Iter: 1285 loss: 4.9480127e-07
Iter: 1286 loss: 4.94710491e-07
Iter: 1287 loss: 4.94847427e-07
Iter: 1288 loss: 4.94667745e-07
Iter: 1289 loss: 4.94593678e-07
Iter: 1290 loss: 4.94466065e-07
Iter: 1291 loss: 4.94461e-07
Iter: 1292 loss: 4.9438961e-07
Iter: 1293 loss: 4.94388189e-07
Iter: 1294 loss: 4.94293886e-07
Iter: 1295 loss: 4.94714186e-07
Iter: 1296 loss: 4.9427166e-07
Iter: 1297 loss: 4.94234e-07
Iter: 1298 loss: 4.94151891e-07
Iter: 1299 loss: 4.95921029e-07
Iter: 1300 loss: 4.94146207e-07
Iter: 1301 loss: 4.94094763e-07
Iter: 1302 loss: 4.94073163e-07
Iter: 1303 loss: 4.94038431e-07
Iter: 1304 loss: 4.93965103e-07
Iter: 1305 loss: 4.95340373e-07
Iter: 1306 loss: 4.93975108e-07
Iter: 1307 loss: 4.9393941e-07
Iter: 1308 loss: 4.9394032e-07
Iter: 1309 loss: 4.93877e-07
Iter: 1310 loss: 4.93910079e-07
Iter: 1311 loss: 4.9384164e-07
Iter: 1312 loss: 4.93794346e-07
Iter: 1313 loss: 4.938193e-07
Iter: 1314 loss: 4.93762741e-07
Iter: 1315 loss: 4.93714197e-07
Iter: 1316 loss: 4.93918833e-07
Iter: 1317 loss: 4.93688674e-07
Iter: 1318 loss: 4.9363365e-07
Iter: 1319 loss: 4.94116932e-07
Iter: 1320 loss: 4.93620405e-07
Iter: 1321 loss: 4.93591642e-07
Iter: 1322 loss: 4.93495691e-07
Iter: 1323 loss: 4.94930873e-07
Iter: 1324 loss: 4.93496032e-07
Iter: 1325 loss: 4.93408834e-07
Iter: 1326 loss: 4.93890866e-07
Iter: 1327 loss: 4.93375126e-07
Iter: 1328 loss: 4.93312257e-07
Iter: 1329 loss: 4.93783e-07
Iter: 1330 loss: 4.93303048e-07
Iter: 1331 loss: 4.93238076e-07
Iter: 1332 loss: 4.9315247e-07
Iter: 1333 loss: 4.95210656e-07
Iter: 1334 loss: 4.93155198e-07
Iter: 1335 loss: 4.93081757e-07
Iter: 1336 loss: 4.93084883e-07
Iter: 1337 loss: 4.93008088e-07
Iter: 1338 loss: 4.93046286e-07
Iter: 1339 loss: 4.92955451e-07
Iter: 1340 loss: 4.92904746e-07
Iter: 1341 loss: 4.9304964e-07
Iter: 1342 loss: 4.92890763e-07
Iter: 1343 loss: 4.92824711e-07
Iter: 1344 loss: 4.92960226e-07
Iter: 1345 loss: 4.92804702e-07
Iter: 1346 loss: 4.92693744e-07
Iter: 1347 loss: 4.92665379e-07
Iter: 1348 loss: 4.92633546e-07
Iter: 1349 loss: 4.92516961e-07
Iter: 1350 loss: 4.92677e-07
Iter: 1351 loss: 4.92457957e-07
Iter: 1352 loss: 4.92424533e-07
Iter: 1353 loss: 4.92412141e-07
Iter: 1354 loss: 4.92352569e-07
Iter: 1355 loss: 4.9232284e-07
Iter: 1356 loss: 4.92308118e-07
Iter: 1357 loss: 4.92251502e-07
Iter: 1358 loss: 4.92395259e-07
Iter: 1359 loss: 4.92240588e-07
Iter: 1360 loss: 4.9220472e-07
Iter: 1361 loss: 4.92666061e-07
Iter: 1362 loss: 4.92199661e-07
Iter: 1363 loss: 4.92159529e-07
Iter: 1364 loss: 4.92081085e-07
Iter: 1365 loss: 4.9364985e-07
Iter: 1366 loss: 4.92081199e-07
Iter: 1367 loss: 4.91978255e-07
Iter: 1368 loss: 4.92057779e-07
Iter: 1369 loss: 4.91925448e-07
Iter: 1370 loss: 4.91824267e-07
Iter: 1371 loss: 4.91825233e-07
Iter: 1372 loss: 4.91777541e-07
Iter: 1373 loss: 4.91675905e-07
Iter: 1374 loss: 4.9323063e-07
Iter: 1375 loss: 4.91676e-07
Iter: 1376 loss: 4.91634125e-07
Iter: 1377 loss: 4.9161622e-07
Iter: 1378 loss: 4.91576429e-07
Iter: 1379 loss: 4.91515038e-07
Iter: 1380 loss: 4.93024345e-07
Iter: 1381 loss: 4.91507535e-07
Iter: 1382 loss: 4.91442506e-07
Iter: 1383 loss: 4.91576941e-07
Iter: 1384 loss: 4.91404e-07
Iter: 1385 loss: 4.91353035e-07
Iter: 1386 loss: 4.91800165e-07
Iter: 1387 loss: 4.91343712e-07
Iter: 1388 loss: 4.91277092e-07
Iter: 1389 loss: 4.91471837e-07
Iter: 1390 loss: 4.91257083e-07
Iter: 1391 loss: 4.91211267e-07
Iter: 1392 loss: 4.91170454e-07
Iter: 1393 loss: 4.91145e-07
Iter: 1394 loss: 4.91090589e-07
Iter: 1395 loss: 4.91089395e-07
Iter: 1396 loss: 4.91034882e-07
Iter: 1397 loss: 4.91117817e-07
Iter: 1398 loss: 4.91006915e-07
Iter: 1399 loss: 4.90986736e-07
Iter: 1400 loss: 4.9091409e-07
Iter: 1401 loss: 4.90916705e-07
Iter: 1402 loss: 4.90907951e-07
Iter: 1403 loss: 4.90890272e-07
Iter: 1404 loss: 4.90856223e-07
Iter: 1405 loss: 4.90809839e-07
Iter: 1406 loss: 4.9164089e-07
Iter: 1407 loss: 4.90811658e-07
Iter: 1408 loss: 4.90770503e-07
Iter: 1409 loss: 4.90767e-07
Iter: 1410 loss: 4.9073293e-07
Iter: 1411 loss: 4.90659261e-07
Iter: 1412 loss: 4.91945457e-07
Iter: 1413 loss: 4.90658863e-07
Iter: 1414 loss: 4.90601451e-07
Iter: 1415 loss: 4.90819275e-07
Iter: 1416 loss: 4.90584512e-07
Iter: 1417 loss: 4.90543925e-07
Iter: 1418 loss: 4.90623222e-07
Iter: 1419 loss: 4.90512775e-07
Iter: 1420 loss: 4.90479692e-07
Iter: 1421 loss: 4.90486798e-07
Iter: 1422 loss: 4.90454e-07
Iter: 1423 loss: 4.90369928e-07
Iter: 1424 loss: 4.92047832e-07
Iter: 1425 loss: 4.90356911e-07
Iter: 1426 loss: 4.90284549e-07
Iter: 1427 loss: 4.90251864e-07
Iter: 1428 loss: 4.90209175e-07
Iter: 1429 loss: 4.9012e-07
Iter: 1430 loss: 4.90397724e-07
Iter: 1431 loss: 4.90091622e-07
Iter: 1432 loss: 4.90015054e-07
Iter: 1433 loss: 4.90008574e-07
Iter: 1434 loss: 4.90001526e-07
Iter: 1435 loss: 4.89923536e-07
Iter: 1436 loss: 4.90663297e-07
Iter: 1437 loss: 4.89922343e-07
Iter: 1438 loss: 4.89906142e-07
Iter: 1439 loss: 4.89892e-07
Iter: 1440 loss: 4.89859588e-07
Iter: 1441 loss: 4.89818376e-07
Iter: 1442 loss: 4.89810077e-07
Iter: 1443 loss: 4.89755337e-07
Iter: 1444 loss: 4.89730041e-07
Iter: 1445 loss: 4.89685362e-07
Iter: 1446 loss: 4.89610443e-07
Iter: 1447 loss: 4.91014475e-07
Iter: 1448 loss: 4.89609079e-07
Iter: 1449 loss: 4.89577133e-07
Iter: 1450 loss: 4.89542742e-07
Iter: 1451 loss: 4.89540753e-07
Iter: 1452 loss: 4.89489366e-07
Iter: 1453 loss: 4.89647107e-07
Iter: 1454 loss: 4.89481693e-07
Iter: 1455 loss: 4.89437241e-07
Iter: 1456 loss: 4.89423712e-07
Iter: 1457 loss: 4.89394097e-07
Iter: 1458 loss: 4.8936721e-07
Iter: 1459 loss: 4.89404897e-07
Iter: 1460 loss: 4.89326396e-07
Iter: 1461 loss: 4.89309627e-07
Iter: 1462 loss: 4.89260913e-07
Iter: 1463 loss: 4.89267507e-07
Iter: 1464 loss: 4.89190541e-07
Iter: 1465 loss: 4.89392676e-07
Iter: 1466 loss: 4.8916e-07
Iter: 1467 loss: 4.89067247e-07
Iter: 1468 loss: 4.89080662e-07
Iter: 1469 loss: 4.88970954e-07
Iter: 1470 loss: 4.88869546e-07
Iter: 1471 loss: 4.89124886e-07
Iter: 1472 loss: 4.88846467e-07
Iter: 1473 loss: 4.88815431e-07
Iter: 1474 loss: 4.88786327e-07
Iter: 1475 loss: 4.88761316e-07
Iter: 1476 loss: 4.88682645e-07
Iter: 1477 loss: 4.89528816e-07
Iter: 1478 loss: 4.88679746e-07
Iter: 1479 loss: 4.88666728e-07
Iter: 1480 loss: 4.88638875e-07
Iter: 1481 loss: 4.88607611e-07
Iter: 1482 loss: 4.88553042e-07
Iter: 1483 loss: 4.89438435e-07
Iter: 1484 loss: 4.88541048e-07
Iter: 1485 loss: 4.88485853e-07
Iter: 1486 loss: 4.88745684e-07
Iter: 1487 loss: 4.88470846e-07
Iter: 1488 loss: 4.88390867e-07
Iter: 1489 loss: 4.88484e-07
Iter: 1490 loss: 4.88331466e-07
Iter: 1491 loss: 4.88263936e-07
Iter: 1492 loss: 4.88345052e-07
Iter: 1493 loss: 4.88221303e-07
Iter: 1494 loss: 4.88116314e-07
Iter: 1495 loss: 4.88395e-07
Iter: 1496 loss: 4.88091871e-07
Iter: 1497 loss: 4.88028036e-07
Iter: 1498 loss: 4.88155706e-07
Iter: 1499 loss: 4.8801212e-07
Iter: 1500 loss: 4.87951638e-07
Iter: 1501 loss: 4.88536557e-07
Iter: 1502 loss: 4.87951638e-07
Iter: 1503 loss: 4.87899342e-07
Iter: 1504 loss: 4.87905709e-07
Iter: 1505 loss: 4.8786535e-07
Iter: 1506 loss: 4.87826071e-07
Iter: 1507 loss: 4.88083515e-07
Iter: 1508 loss: 4.87828117e-07
Iter: 1509 loss: 4.87733132e-07
Iter: 1510 loss: 4.87843351e-07
Iter: 1511 loss: 4.87699708e-07
Iter: 1512 loss: 4.87653551e-07
Iter: 1513 loss: 4.87552938e-07
Iter: 1514 loss: 4.8755328e-07
Iter: 1515 loss: 4.87569309e-07
Iter: 1516 loss: 4.87497573e-07
Iter: 1517 loss: 4.87485238e-07
Iter: 1518 loss: 4.87415377e-07
Iter: 1519 loss: 4.87574766e-07
Iter: 1520 loss: 4.87359785e-07
Iter: 1521 loss: 4.87300554e-07
Iter: 1522 loss: 4.88213686e-07
Iter: 1523 loss: 4.87301e-07
Iter: 1524 loss: 4.87248712e-07
Iter: 1525 loss: 4.87743705e-07
Iter: 1526 loss: 4.87249224e-07
Iter: 1527 loss: 4.87210855e-07
Iter: 1528 loss: 4.87144348e-07
Iter: 1529 loss: 4.87846251e-07
Iter: 1530 loss: 4.87120701e-07
Iter: 1531 loss: 4.8706022e-07
Iter: 1532 loss: 4.88115234e-07
Iter: 1533 loss: 4.87053114e-07
Iter: 1534 loss: 4.87014063e-07
Iter: 1535 loss: 4.86903e-07
Iter: 1536 loss: 4.86907425e-07
Iter: 1537 loss: 4.86825172e-07
Iter: 1538 loss: 4.87402872e-07
Iter: 1539 loss: 4.86808858e-07
Iter: 1540 loss: 4.86778617e-07
Iter: 1541 loss: 4.86776116e-07
Iter: 1542 loss: 4.86732e-07
Iter: 1543 loss: 4.86830686e-07
Iter: 1544 loss: 4.86719387e-07
Iter: 1545 loss: 4.86667261e-07
Iter: 1546 loss: 4.86720523e-07
Iter: 1547 loss: 4.86648162e-07
Iter: 1548 loss: 4.8661559e-07
Iter: 1549 loss: 4.86689885e-07
Iter: 1550 loss: 4.86583247e-07
Iter: 1551 loss: 4.86519298e-07
Iter: 1552 loss: 4.86916861e-07
Iter: 1553 loss: 4.86518559e-07
Iter: 1554 loss: 4.86466547e-07
Iter: 1555 loss: 4.86339218e-07
Iter: 1556 loss: 4.87206876e-07
Iter: 1557 loss: 4.86318186e-07
Iter: 1558 loss: 4.86185172e-07
Iter: 1559 loss: 4.87315162e-07
Iter: 1560 loss: 4.86176759e-07
Iter: 1561 loss: 4.86138674e-07
Iter: 1562 loss: 4.86132365e-07
Iter: 1563 loss: 4.86107581e-07
Iter: 1564 loss: 4.86017541e-07
Iter: 1565 loss: 4.8642778e-07
Iter: 1566 loss: 4.85988721e-07
Iter: 1567 loss: 4.85887369e-07
Iter: 1568 loss: 4.86946647e-07
Iter: 1569 loss: 4.85886289e-07
Iter: 1570 loss: 4.85781754e-07
Iter: 1571 loss: 4.86136742e-07
Iter: 1572 loss: 4.85771182e-07
Iter: 1573 loss: 4.85686087e-07
Iter: 1574 loss: 4.85638452e-07
Iter: 1575 loss: 4.85600481e-07
Iter: 1576 loss: 4.85496457e-07
Iter: 1577 loss: 4.86751162e-07
Iter: 1578 loss: 4.8549208e-07
Iter: 1579 loss: 4.85419264e-07
Iter: 1580 loss: 4.855886e-07
Iter: 1581 loss: 4.8539448e-07
Iter: 1582 loss: 4.85304895e-07
Iter: 1583 loss: 4.85676424e-07
Iter: 1584 loss: 4.85306373e-07
Iter: 1585 loss: 4.85263627e-07
Iter: 1586 loss: 4.85545e-07
Iter: 1587 loss: 4.85261126e-07
Iter: 1588 loss: 4.85227474e-07
Iter: 1589 loss: 4.8516722e-07
Iter: 1590 loss: 4.85157273e-07
Iter: 1591 loss: 4.85099463e-07
Iter: 1592 loss: 4.8505575e-07
Iter: 1593 loss: 4.85038356e-07
Iter: 1594 loss: 4.84952693e-07
Iter: 1595 loss: 4.86232295e-07
Iter: 1596 loss: 4.8495292e-07
Iter: 1597 loss: 4.84867201e-07
Iter: 1598 loss: 4.84894713e-07
Iter: 1599 loss: 4.84788814e-07
Iter: 1600 loss: 4.84733278e-07
Iter: 1601 loss: 4.84813654e-07
Iter: 1602 loss: 4.84691043e-07
Iter: 1603 loss: 4.84643294e-07
Iter: 1604 loss: 4.84644886e-07
Iter: 1605 loss: 4.84622262e-07
Iter: 1606 loss: 4.84585e-07
Iter: 1607 loss: 4.84573548e-07
Iter: 1608 loss: 4.84531654e-07
Iter: 1609 loss: 4.84800807e-07
Iter: 1610 loss: 4.84524264e-07
Iter: 1611 loss: 4.8448652e-07
Iter: 1612 loss: 4.84579459e-07
Iter: 1613 loss: 4.8445645e-07
Iter: 1614 loss: 4.84413931e-07
Iter: 1615 loss: 4.84402e-07
Iter: 1616 loss: 4.84365557e-07
Iter: 1617 loss: 4.84277678e-07
Iter: 1618 loss: 4.84768407e-07
Iter: 1619 loss: 4.84270515e-07
Iter: 1620 loss: 4.84196164e-07
Iter: 1621 loss: 4.84636416e-07
Iter: 1622 loss: 4.8419605e-07
Iter: 1623 loss: 4.84143072e-07
Iter: 1624 loss: 4.84093903e-07
Iter: 1625 loss: 4.84091061e-07
Iter: 1626 loss: 4.84028533e-07
Iter: 1627 loss: 4.84368229e-07
Iter: 1628 loss: 4.84023644e-07
Iter: 1629 loss: 4.83977e-07
Iter: 1630 loss: 4.83974645e-07
Iter: 1631 loss: 4.8395367e-07
Iter: 1632 loss: 4.83894155e-07
Iter: 1633 loss: 4.84461452e-07
Iter: 1634 loss: 4.83892109e-07
Iter: 1635 loss: 4.83809799e-07
Iter: 1636 loss: 4.84243571e-07
Iter: 1637 loss: 4.83788199e-07
Iter: 1638 loss: 4.83716065e-07
Iter: 1639 loss: 4.83707481e-07
Iter: 1640 loss: 4.83645749e-07
Iter: 1641 loss: 4.83544738e-07
Iter: 1642 loss: 4.83702593e-07
Iter: 1643 loss: 4.8348943e-07
Iter: 1644 loss: 4.83440203e-07
Iter: 1645 loss: 4.83433894e-07
Iter: 1646 loss: 4.83392682e-07
Iter: 1647 loss: 4.83369718e-07
Iter: 1648 loss: 4.83353e-07
Iter: 1649 loss: 4.83303e-07
Iter: 1650 loss: 4.83873919e-07
Iter: 1651 loss: 4.83302529e-07
Iter: 1652 loss: 4.83250631e-07
Iter: 1653 loss: 4.83530698e-07
Iter: 1654 loss: 4.83246822e-07
Iter: 1655 loss: 4.83213341e-07
Iter: 1656 loss: 4.83153201e-07
Iter: 1657 loss: 4.83153258e-07
Iter: 1658 loss: 4.83068675e-07
Iter: 1659 loss: 4.83033659e-07
Iter: 1660 loss: 4.83002225e-07
Iter: 1661 loss: 4.82929579e-07
Iter: 1662 loss: 4.82929522e-07
Iter: 1663 loss: 4.82853238e-07
Iter: 1664 loss: 4.82743644e-07
Iter: 1665 loss: 4.82731934e-07
Iter: 1666 loss: 4.82665769e-07
Iter: 1667 loss: 4.83063559e-07
Iter: 1668 loss: 4.82645248e-07
Iter: 1669 loss: 4.82607845e-07
Iter: 1670 loss: 4.83423605e-07
Iter: 1671 loss: 4.82609e-07
Iter: 1672 loss: 4.82562882e-07
Iter: 1673 loss: 4.82475798e-07
Iter: 1674 loss: 4.83581232e-07
Iter: 1675 loss: 4.82456869e-07
Iter: 1676 loss: 4.82388771e-07
Iter: 1677 loss: 4.82385701e-07
Iter: 1678 loss: 4.82320047e-07
Iter: 1679 loss: 4.8232306e-07
Iter: 1680 loss: 4.82288158e-07
Iter: 1681 loss: 4.82203234e-07
Iter: 1682 loss: 4.82031737e-07
Iter: 1683 loss: 4.82029293e-07
Iter: 1684 loss: 4.81957215e-07
Iter: 1685 loss: 4.81945051e-07
Iter: 1686 loss: 4.8186007e-07
Iter: 1687 loss: 4.82156395e-07
Iter: 1688 loss: 4.81847451e-07
Iter: 1689 loss: 4.81788845e-07
Iter: 1690 loss: 4.81750135e-07
Iter: 1691 loss: 4.81732627e-07
Iter: 1692 loss: 4.81672259e-07
Iter: 1693 loss: 4.81787879e-07
Iter: 1694 loss: 4.81624284e-07
Iter: 1695 loss: 4.81582674e-07
Iter: 1696 loss: 4.81582845e-07
Iter: 1697 loss: 4.8155357e-07
Iter: 1698 loss: 4.81430902e-07
Iter: 1699 loss: 4.82373537e-07
Iter: 1700 loss: 4.81422887e-07
Iter: 1701 loss: 4.81292943e-07
Iter: 1702 loss: 4.81340237e-07
Iter: 1703 loss: 4.81184713e-07
Iter: 1704 loss: 4.81122129e-07
Iter: 1705 loss: 4.81089501e-07
Iter: 1706 loss: 4.81014808e-07
Iter: 1707 loss: 4.80991048e-07
Iter: 1708 loss: 4.80944891e-07
Iter: 1709 loss: 4.80876338e-07
Iter: 1710 loss: 4.80873268e-07
Iter: 1711 loss: 4.80830238e-07
Iter: 1712 loss: 4.80826657e-07
Iter: 1713 loss: 4.8079437e-07
Iter: 1714 loss: 4.80731501e-07
Iter: 1715 loss: 4.80678523e-07
Iter: 1716 loss: 4.80670508e-07
Iter: 1717 loss: 4.80588255e-07
Iter: 1718 loss: 4.81319944e-07
Iter: 1719 loss: 4.80574386e-07
Iter: 1720 loss: 4.80496396e-07
Iter: 1721 loss: 4.8082444e-07
Iter: 1722 loss: 4.80472352e-07
Iter: 1723 loss: 4.80412496e-07
Iter: 1724 loss: 4.80366907e-07
Iter: 1725 loss: 4.80344852e-07
Iter: 1726 loss: 4.80213146e-07
Iter: 1727 loss: 4.80567053e-07
Iter: 1728 loss: 4.80194956e-07
Iter: 1729 loss: 4.80134304e-07
Iter: 1730 loss: 4.80406584e-07
Iter: 1731 loss: 4.80125664e-07
Iter: 1732 loss: 4.80052961e-07
Iter: 1733 loss: 4.80738379e-07
Iter: 1734 loss: 4.80058873e-07
Iter: 1735 loss: 4.80030849e-07
Iter: 1736 loss: 4.79924665e-07
Iter: 1737 loss: 4.80502536e-07
Iter: 1738 loss: 4.79903292e-07
Iter: 1739 loss: 4.79768346e-07
Iter: 1740 loss: 4.80086214e-07
Iter: 1741 loss: 4.79703544e-07
Iter: 1742 loss: 4.79555581e-07
Iter: 1743 loss: 4.79916423e-07
Iter: 1744 loss: 4.79496407e-07
Iter: 1745 loss: 4.79406708e-07
Iter: 1746 loss: 4.79401535e-07
Iter: 1747 loss: 4.79330538e-07
Iter: 1748 loss: 4.79261075e-07
Iter: 1749 loss: 4.79259484e-07
Iter: 1750 loss: 4.7917132e-07
Iter: 1751 loss: 4.79175867e-07
Iter: 1752 loss: 4.79120104e-07
Iter: 1753 loss: 4.79051494e-07
Iter: 1754 loss: 4.79036203e-07
Iter: 1755 loss: 4.78955371e-07
Iter: 1756 loss: 4.79296318e-07
Iter: 1757 loss: 4.78918309e-07
Iter: 1758 loss: 4.78762445e-07
Iter: 1759 loss: 4.7944178e-07
Iter: 1760 loss: 4.78760569e-07
Iter: 1761 loss: 4.7868e-07
Iter: 1762 loss: 4.78647735e-07
Iter: 1763 loss: 4.786001e-07
Iter: 1764 loss: 4.78488573e-07
Iter: 1765 loss: 4.78922175e-07
Iter: 1766 loss: 4.78452762e-07
Iter: 1767 loss: 4.78377046e-07
Iter: 1768 loss: 4.78417803e-07
Iter: 1769 loss: 4.78301104e-07
Iter: 1770 loss: 4.7822266e-07
Iter: 1771 loss: 4.78941729e-07
Iter: 1772 loss: 4.78232e-07
Iter: 1773 loss: 4.78157403e-07
Iter: 1774 loss: 4.7828604e-07
Iter: 1775 loss: 4.78106529e-07
Iter: 1776 loss: 4.78036668e-07
Iter: 1777 loss: 4.77909452e-07
Iter: 1778 loss: 4.81052211e-07
Iter: 1779 loss: 4.77898709e-07
Iter: 1780 loss: 4.77798949e-07
Iter: 1781 loss: 4.77792071e-07
Iter: 1782 loss: 4.77692538e-07
Iter: 1783 loss: 4.77887284e-07
Iter: 1784 loss: 4.77651042e-07
Iter: 1785 loss: 4.77614e-07
Iter: 1786 loss: 4.77732442e-07
Iter: 1787 loss: 4.77584535e-07
Iter: 1788 loss: 4.77499611e-07
Iter: 1789 loss: 4.77630124e-07
Iter: 1790 loss: 4.77460162e-07
Iter: 1791 loss: 4.77416449e-07
Iter: 1792 loss: 4.7745516e-07
Iter: 1793 loss: 4.77380922e-07
Iter: 1794 loss: 4.77331128e-07
Iter: 1795 loss: 4.77326523e-07
Iter: 1796 loss: 4.77284289e-07
Iter: 1797 loss: 4.77210619e-07
Iter: 1798 loss: 4.77788547e-07
Iter: 1799 loss: 4.77166054e-07
Iter: 1800 loss: 4.77015533e-07
Iter: 1801 loss: 4.77811682e-07
Iter: 1802 loss: 4.7699e-07
Iter: 1803 loss: 4.768998e-07
Iter: 1804 loss: 4.77074764e-07
Iter: 1805 loss: 4.76832781e-07
Iter: 1806 loss: 4.76817632e-07
Iter: 1807 loss: 4.76790603e-07
Iter: 1808 loss: 4.76773067e-07
Iter: 1809 loss: 4.76714945e-07
Iter: 1810 loss: 4.77718061e-07
Iter: 1811 loss: 4.76712813e-07
Iter: 1812 loss: 4.76641787e-07
Iter: 1813 loss: 4.76589378e-07
Iter: 1814 loss: 4.76570534e-07
Iter: 1815 loss: 4.76586308e-07
Iter: 1816 loss: 4.76538901e-07
Iter: 1817 loss: 4.76495018e-07
Iter: 1818 loss: 4.76412708e-07
Iter: 1819 loss: 4.77287074e-07
Iter: 1820 loss: 4.76411628e-07
Iter: 1821 loss: 4.76281912e-07
Iter: 1822 loss: 4.76444058e-07
Iter: 1823 loss: 4.76215e-07
Iter: 1824 loss: 4.76161773e-07
Iter: 1825 loss: 4.76136506e-07
Iter: 1826 loss: 4.76110557e-07
Iter: 1827 loss: 4.76050104e-07
Iter: 1828 loss: 4.77070614e-07
Iter: 1829 loss: 4.76062922e-07
Iter: 1830 loss: 4.76041492e-07
Iter: 1831 loss: 4.76027e-07
Iter: 1832 loss: 4.76001389e-07
Iter: 1833 loss: 4.75958586e-07
Iter: 1834 loss: 4.75958444e-07
Iter: 1835 loss: 4.7591476e-07
Iter: 1836 loss: 4.75881677e-07
Iter: 1837 loss: 4.75851834e-07
Iter: 1838 loss: 4.75809514e-07
Iter: 1839 loss: 4.75833218e-07
Iter: 1840 loss: 4.75756565e-07
Iter: 1841 loss: 4.75676188e-07
Iter: 1842 loss: 4.76096147e-07
Iter: 1843 loss: 4.75678718e-07
Iter: 1844 loss: 4.75581089e-07
Iter: 1845 loss: 4.7566283e-07
Iter: 1846 loss: 4.75557613e-07
Iter: 1847 loss: 4.75499945e-07
Iter: 1848 loss: 4.75937043e-07
Iter: 1849 loss: 4.75511115e-07
Iter: 1850 loss: 4.75453078e-07
Iter: 1851 loss: 4.75530072e-07
Iter: 1852 loss: 4.75425679e-07
Iter: 1853 loss: 4.75379409e-07
Iter: 1854 loss: 4.75255547e-07
Iter: 1855 loss: 4.76634625e-07
Iter: 1856 loss: 4.75250204e-07
Iter: 1857 loss: 4.75202512e-07
Iter: 1858 loss: 4.75191541e-07
Iter: 1859 loss: 4.75118441e-07
Iter: 1860 loss: 4.75052644e-07
Iter: 1861 loss: 4.75041929e-07
Iter: 1862 loss: 4.74969056e-07
Iter: 1863 loss: 4.7485986e-07
Iter: 1864 loss: 4.74864e-07
Iter: 1865 loss: 4.74925912e-07
Iter: 1866 loss: 4.74834025e-07
Iter: 1867 loss: 4.74800657e-07
Iter: 1868 loss: 4.74772435e-07
Iter: 1869 loss: 4.74768655e-07
Iter: 1870 loss: 4.74727727e-07
Iter: 1871 loss: 4.74836412e-07
Iter: 1872 loss: 4.74703398e-07
Iter: 1873 loss: 4.74663523e-07
Iter: 1874 loss: 4.74607532e-07
Iter: 1875 loss: 4.74612506e-07
Iter: 1876 loss: 4.74533778e-07
Iter: 1877 loss: 4.74507772e-07
Iter: 1878 loss: 4.74453657e-07
Iter: 1879 loss: 4.74381665e-07
Iter: 1880 loss: 4.75122079e-07
Iter: 1881 loss: 4.74382972e-07
Iter: 1882 loss: 4.7430234e-07
Iter: 1883 loss: 4.74297877e-07
Iter: 1884 loss: 4.74252545e-07
Iter: 1885 loss: 4.74175067e-07
Iter: 1886 loss: 4.74348099e-07
Iter: 1887 loss: 4.74116746e-07
Iter: 1888 loss: 4.74058652e-07
Iter: 1889 loss: 4.74522579e-07
Iter: 1890 loss: 4.74064137e-07
Iter: 1891 loss: 4.73968953e-07
Iter: 1892 loss: 4.74038245e-07
Iter: 1893 loss: 4.73932175e-07
Iter: 1894 loss: 4.73860581e-07
Iter: 1895 loss: 4.74070958e-07
Iter: 1896 loss: 4.73831165e-07
Iter: 1897 loss: 4.73762924e-07
Iter: 1898 loss: 4.74076671e-07
Iter: 1899 loss: 4.73757325e-07
Iter: 1900 loss: 4.73704432e-07
Iter: 1901 loss: 4.73578609e-07
Iter: 1902 loss: 4.75300936e-07
Iter: 1903 loss: 4.73576677e-07
Iter: 1904 loss: 4.7351935e-07
Iter: 1905 loss: 4.73514234e-07
Iter: 1906 loss: 4.73432294e-07
Iter: 1907 loss: 4.73410381e-07
Iter: 1908 loss: 4.73379828e-07
Iter: 1909 loss: 4.73327418e-07
Iter: 1910 loss: 4.73324633e-07
Iter: 1911 loss: 4.73271086e-07
Iter: 1912 loss: 4.73164278e-07
Iter: 1913 loss: 4.75513474e-07
Iter: 1914 loss: 4.73174197e-07
Iter: 1915 loss: 4.73053717e-07
Iter: 1916 loss: 4.73313435e-07
Iter: 1917 loss: 4.73018474e-07
Iter: 1918 loss: 4.72934744e-07
Iter: 1919 loss: 4.73388098e-07
Iter: 1920 loss: 4.72913541e-07
Iter: 1921 loss: 4.7284226e-07
Iter: 1922 loss: 4.72848285e-07
Iter: 1923 loss: 4.72785558e-07
Iter: 1924 loss: 4.72695348e-07
Iter: 1925 loss: 4.73472312e-07
Iter: 1926 loss: 4.72689067e-07
Iter: 1927 loss: 4.72608718e-07
Iter: 1928 loss: 4.72812047e-07
Iter: 1929 loss: 4.72561624e-07
Iter: 1930 loss: 4.72517513e-07
Iter: 1931 loss: 4.73172491e-07
Iter: 1932 loss: 4.72516803e-07
Iter: 1933 loss: 4.72465359e-07
Iter: 1934 loss: 4.72369436e-07
Iter: 1935 loss: 4.7419519e-07
Iter: 1936 loss: 4.72372221e-07
Iter: 1937 loss: 4.72283887e-07
Iter: 1938 loss: 4.72899615e-07
Iter: 1939 loss: 4.72276724e-07
Iter: 1940 loss: 4.72173582e-07
Iter: 1941 loss: 4.72391946e-07
Iter: 1942 loss: 4.72151299e-07
Iter: 1943 loss: 4.7206214e-07
Iter: 1944 loss: 4.72461068e-07
Iter: 1945 loss: 4.72046821e-07
Iter: 1946 loss: 4.71968065e-07
Iter: 1947 loss: 4.72043553e-07
Iter: 1948 loss: 4.71936033e-07
Iter: 1949 loss: 4.71859323e-07
Iter: 1950 loss: 4.71903206e-07
Iter: 1951 loss: 4.71826553e-07
Iter: 1952 loss: 4.71712042e-07
Iter: 1953 loss: 4.72024283e-07
Iter: 1954 loss: 4.7169118e-07
Iter: 1955 loss: 4.71619842e-07
Iter: 1956 loss: 4.71786905e-07
Iter: 1957 loss: 4.71581529e-07
Iter: 1958 loss: 4.71502545e-07
Iter: 1959 loss: 4.7159466e-07
Iter: 1960 loss: 4.71450051e-07
Iter: 1961 loss: 4.71362142e-07
Iter: 1962 loss: 4.72043666e-07
Iter: 1963 loss: 4.71357623e-07
Iter: 1964 loss: 4.71305384e-07
Iter: 1965 loss: 4.71433907e-07
Iter: 1966 loss: 4.71282135e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.8
+ date
Mon Oct 26 14:47:21 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.8/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.8_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi -1 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.8_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0777e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1abc01df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1abc092400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1abc092488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1abc0c4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1abc0c4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa06fa400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa068f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa067e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa067ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0602e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0624d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0637620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa05e7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa058d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0577bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0577b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa05036a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa04c77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa04c8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa04c2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa04c2d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0471598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa04716a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0415400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0404bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa03e77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa03d4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa03807b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0380c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa03802f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa03027b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0321620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa03219d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa02f97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa0283840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.58702561e-06
Iter: 2 loss: 3.45545641e-06
Iter: 3 loss: 3.38944506e-06
Iter: 4 loss: 2.99070962e-06
Iter: 5 loss: 3.66496965e-06
Iter: 6 loss: 2.81123494e-06
Iter: 7 loss: 2.59044646e-06
Iter: 8 loss: 3.42622525e-06
Iter: 9 loss: 2.53765938e-06
Iter: 10 loss: 2.39646693e-06
Iter: 11 loss: 4.16763851e-06
Iter: 12 loss: 2.39520773e-06
Iter: 13 loss: 2.31474496e-06
Iter: 14 loss: 2.3116113e-06
Iter: 15 loss: 2.24940027e-06
Iter: 16 loss: 2.20860511e-06
Iter: 17 loss: 2.20578568e-06
Iter: 18 loss: 2.16753347e-06
Iter: 19 loss: 2.09633208e-06
Iter: 20 loss: 3.70587077e-06
Iter: 21 loss: 2.09619725e-06
Iter: 22 loss: 2.01779e-06
Iter: 23 loss: 1.91659774e-06
Iter: 24 loss: 1.90962714e-06
Iter: 25 loss: 2.0188877e-06
Iter: 26 loss: 1.87329692e-06
Iter: 27 loss: 1.84543183e-06
Iter: 28 loss: 1.79369499e-06
Iter: 29 loss: 2.96882718e-06
Iter: 30 loss: 1.79358551e-06
Iter: 31 loss: 1.75674984e-06
Iter: 32 loss: 1.77625077e-06
Iter: 33 loss: 1.73236731e-06
Iter: 34 loss: 1.69313807e-06
Iter: 35 loss: 1.83236352e-06
Iter: 36 loss: 1.68309089e-06
Iter: 37 loss: 1.65491792e-06
Iter: 38 loss: 1.65467054e-06
Iter: 39 loss: 1.62859328e-06
Iter: 40 loss: 1.56929127e-06
Iter: 41 loss: 2.35274e-06
Iter: 42 loss: 1.56549982e-06
Iter: 43 loss: 1.50544565e-06
Iter: 44 loss: 2.4429537e-06
Iter: 45 loss: 1.50541393e-06
Iter: 46 loss: 1.4660352e-06
Iter: 47 loss: 1.65750021e-06
Iter: 48 loss: 1.45915465e-06
Iter: 49 loss: 1.43109492e-06
Iter: 50 loss: 1.42012607e-06
Iter: 51 loss: 1.40495308e-06
Iter: 52 loss: 1.38004691e-06
Iter: 53 loss: 1.37873633e-06
Iter: 54 loss: 1.36411427e-06
Iter: 55 loss: 1.33917194e-06
Iter: 56 loss: 1.33914193e-06
Iter: 57 loss: 1.31700733e-06
Iter: 58 loss: 1.32308264e-06
Iter: 59 loss: 1.30102308e-06
Iter: 60 loss: 1.28814065e-06
Iter: 61 loss: 1.283492e-06
Iter: 62 loss: 1.2675373e-06
Iter: 63 loss: 1.22565871e-06
Iter: 64 loss: 1.54297902e-06
Iter: 65 loss: 1.21719859e-06
Iter: 66 loss: 1.17536445e-06
Iter: 67 loss: 1.31223794e-06
Iter: 68 loss: 1.16368267e-06
Iter: 69 loss: 1.14804186e-06
Iter: 70 loss: 1.14578813e-06
Iter: 71 loss: 1.12979171e-06
Iter: 72 loss: 1.16689228e-06
Iter: 73 loss: 1.12385965e-06
Iter: 74 loss: 1.11235386e-06
Iter: 75 loss: 1.12221539e-06
Iter: 76 loss: 1.10556527e-06
Iter: 77 loss: 1.09273992e-06
Iter: 78 loss: 1.24081e-06
Iter: 79 loss: 1.09252562e-06
Iter: 80 loss: 1.08348786e-06
Iter: 81 loss: 1.07911239e-06
Iter: 82 loss: 1.07475125e-06
Iter: 83 loss: 1.0593551e-06
Iter: 84 loss: 1.09427788e-06
Iter: 85 loss: 1.05355161e-06
Iter: 86 loss: 1.03155799e-06
Iter: 87 loss: 1.08047107e-06
Iter: 88 loss: 1.02313902e-06
Iter: 89 loss: 1.00902423e-06
Iter: 90 loss: 9.92861374e-07
Iter: 91 loss: 9.90863668e-07
Iter: 92 loss: 9.7416455e-07
Iter: 93 loss: 1.13844874e-06
Iter: 94 loss: 9.73572583e-07
Iter: 95 loss: 9.60315e-07
Iter: 96 loss: 1.12050589e-06
Iter: 97 loss: 9.60179477e-07
Iter: 98 loss: 9.55053e-07
Iter: 99 loss: 9.43269356e-07
Iter: 100 loss: 1.09130542e-06
Iter: 101 loss: 9.42424663e-07
Iter: 102 loss: 9.2987375e-07
Iter: 103 loss: 9.77950549e-07
Iter: 104 loss: 9.2690567e-07
Iter: 105 loss: 9.18780358e-07
Iter: 106 loss: 9.18319188e-07
Iter: 107 loss: 9.1164128e-07
Iter: 108 loss: 8.9873e-07
Iter: 109 loss: 1.16391891e-06
Iter: 110 loss: 8.98657845e-07
Iter: 111 loss: 8.9087672e-07
Iter: 112 loss: 8.90374622e-07
Iter: 113 loss: 8.83021301e-07
Iter: 114 loss: 8.834852e-07
Iter: 115 loss: 8.77260675e-07
Iter: 116 loss: 8.69415317e-07
Iter: 117 loss: 9.0773824e-07
Iter: 118 loss: 8.68029417e-07
Iter: 119 loss: 8.62551815e-07
Iter: 120 loss: 9.18709304e-07
Iter: 121 loss: 8.62405955e-07
Iter: 122 loss: 8.58554699e-07
Iter: 123 loss: 8.51679e-07
Iter: 124 loss: 1.02130036e-06
Iter: 125 loss: 8.51683183e-07
Iter: 126 loss: 8.44029955e-07
Iter: 127 loss: 8.47378146e-07
Iter: 128 loss: 8.38789106e-07
Iter: 129 loss: 8.34925402e-07
Iter: 130 loss: 8.33539843e-07
Iter: 131 loss: 8.28117209e-07
Iter: 132 loss: 8.15771728e-07
Iter: 133 loss: 9.78589469e-07
Iter: 134 loss: 8.14968473e-07
Iter: 135 loss: 8.04586193e-07
Iter: 136 loss: 8.35528e-07
Iter: 137 loss: 8.01409328e-07
Iter: 138 loss: 7.96764823e-07
Iter: 139 loss: 7.96536426e-07
Iter: 140 loss: 7.91472758e-07
Iter: 141 loss: 7.95535357e-07
Iter: 142 loss: 7.88445277e-07
Iter: 143 loss: 7.84401209e-07
Iter: 144 loss: 7.90829517e-07
Iter: 145 loss: 7.82528048e-07
Iter: 146 loss: 7.77785658e-07
Iter: 147 loss: 8.22360789e-07
Iter: 148 loss: 7.7758e-07
Iter: 149 loss: 7.74498119e-07
Iter: 150 loss: 7.70881115e-07
Iter: 151 loss: 7.70461554e-07
Iter: 152 loss: 7.64985487e-07
Iter: 153 loss: 8.18120327e-07
Iter: 154 loss: 7.64788751e-07
Iter: 155 loss: 7.60462513e-07
Iter: 156 loss: 7.66011169e-07
Iter: 157 loss: 7.58240901e-07
Iter: 158 loss: 7.54743724e-07
Iter: 159 loss: 7.509733e-07
Iter: 160 loss: 7.50402705e-07
Iter: 161 loss: 7.46368528e-07
Iter: 162 loss: 7.96500331e-07
Iter: 163 loss: 7.46336809e-07
Iter: 164 loss: 7.42313773e-07
Iter: 165 loss: 7.61347792e-07
Iter: 166 loss: 7.4161369e-07
Iter: 167 loss: 7.38987126e-07
Iter: 168 loss: 7.32890101e-07
Iter: 169 loss: 8.06421269e-07
Iter: 170 loss: 7.32371291e-07
Iter: 171 loss: 7.25206178e-07
Iter: 172 loss: 7.4781974e-07
Iter: 173 loss: 7.23159133e-07
Iter: 174 loss: 7.18054082e-07
Iter: 175 loss: 7.17681928e-07
Iter: 176 loss: 7.14771033e-07
Iter: 177 loss: 7.10425809e-07
Iter: 178 loss: 7.10293193e-07
Iter: 179 loss: 7.08236257e-07
Iter: 180 loss: 7.07712957e-07
Iter: 181 loss: 7.05622e-07
Iter: 182 loss: 7.03011892e-07
Iter: 183 loss: 7.02803732e-07
Iter: 184 loss: 7.00170347e-07
Iter: 185 loss: 7.30793545e-07
Iter: 186 loss: 7.00136866e-07
Iter: 187 loss: 6.97760129e-07
Iter: 188 loss: 7.00575356e-07
Iter: 189 loss: 6.96491156e-07
Iter: 190 loss: 6.9375676e-07
Iter: 191 loss: 6.9141413e-07
Iter: 192 loss: 6.90664592e-07
Iter: 193 loss: 6.86561577e-07
Iter: 194 loss: 6.8989516e-07
Iter: 195 loss: 6.84101451e-07
Iter: 196 loss: 6.81919687e-07
Iter: 197 loss: 6.81120355e-07
Iter: 198 loss: 6.78982701e-07
Iter: 199 loss: 6.7478851e-07
Iter: 200 loss: 7.60057844e-07
Iter: 201 loss: 6.74761282e-07
Iter: 202 loss: 6.71960038e-07
Iter: 203 loss: 6.8022814e-07
Iter: 204 loss: 6.71100452e-07
Iter: 205 loss: 6.69158e-07
Iter: 206 loss: 6.69152428e-07
Iter: 207 loss: 6.66993287e-07
Iter: 208 loss: 6.64926631e-07
Iter: 209 loss: 6.6445142e-07
Iter: 210 loss: 6.62234697e-07
Iter: 211 loss: 6.74513444e-07
Iter: 212 loss: 6.61918307e-07
Iter: 213 loss: 6.58845238e-07
Iter: 214 loss: 6.5991776e-07
Iter: 215 loss: 6.56656425e-07
Iter: 216 loss: 6.53597567e-07
Iter: 217 loss: 6.63324556e-07
Iter: 218 loss: 6.52701033e-07
Iter: 219 loss: 6.50126935e-07
Iter: 220 loss: 6.81039523e-07
Iter: 221 loss: 6.50084758e-07
Iter: 222 loss: 6.48815e-07
Iter: 223 loss: 6.48379569e-07
Iter: 224 loss: 6.47641855e-07
Iter: 225 loss: 6.45860837e-07
Iter: 226 loss: 6.44806732e-07
Iter: 227 loss: 6.44061856e-07
Iter: 228 loss: 6.42846317e-07
Iter: 229 loss: 6.42681869e-07
Iter: 230 loss: 6.41186944e-07
Iter: 231 loss: 6.40359758e-07
Iter: 232 loss: 6.39715836e-07
Iter: 233 loss: 6.37899802e-07
Iter: 234 loss: 6.34650178e-07
Iter: 235 loss: 7.14188673e-07
Iter: 236 loss: 6.34656885e-07
Iter: 237 loss: 6.30945351e-07
Iter: 238 loss: 6.68551706e-07
Iter: 239 loss: 6.30820409e-07
Iter: 240 loss: 6.27951636e-07
Iter: 241 loss: 6.64817264e-07
Iter: 242 loss: 6.27937709e-07
Iter: 243 loss: 6.26949543e-07
Iter: 244 loss: 6.25926873e-07
Iter: 245 loss: 6.25731786e-07
Iter: 246 loss: 6.24539553e-07
Iter: 247 loss: 6.24494305e-07
Iter: 248 loss: 6.23748861e-07
Iter: 249 loss: 6.2225638e-07
Iter: 250 loss: 6.51949904e-07
Iter: 251 loss: 6.22238133e-07
Iter: 252 loss: 6.20964386e-07
Iter: 253 loss: 6.20938579e-07
Iter: 254 loss: 6.19905279e-07
Iter: 255 loss: 6.18241529e-07
Iter: 256 loss: 6.18227944e-07
Iter: 257 loss: 6.15818124e-07
Iter: 258 loss: 6.16436864e-07
Iter: 259 loss: 6.140599e-07
Iter: 260 loss: 6.11328971e-07
Iter: 261 loss: 6.25139137e-07
Iter: 262 loss: 6.10874395e-07
Iter: 263 loss: 6.08721223e-07
Iter: 264 loss: 6.08725145e-07
Iter: 265 loss: 6.07853451e-07
Iter: 266 loss: 6.06263029e-07
Iter: 267 loss: 6.4413382e-07
Iter: 268 loss: 6.06257629e-07
Iter: 269 loss: 6.04893e-07
Iter: 270 loss: 6.07176617e-07
Iter: 271 loss: 6.04293518e-07
Iter: 272 loss: 6.03361855e-07
Iter: 273 loss: 6.03262947e-07
Iter: 274 loss: 6.02359876e-07
Iter: 275 loss: 6.00572832e-07
Iter: 276 loss: 6.32663e-07
Iter: 277 loss: 6.00559929e-07
Iter: 278 loss: 5.99264183e-07
Iter: 279 loss: 5.99238774e-07
Iter: 280 loss: 5.97856797e-07
Iter: 281 loss: 5.95744041e-07
Iter: 282 loss: 5.95713e-07
Iter: 283 loss: 5.94671235e-07
Iter: 284 loss: 5.94533617e-07
Iter: 285 loss: 5.9365118e-07
Iter: 286 loss: 5.93588538e-07
Iter: 287 loss: 5.92901529e-07
Iter: 288 loss: 5.91952698e-07
Iter: 289 loss: 5.93377422e-07
Iter: 290 loss: 5.9150625e-07
Iter: 291 loss: 5.90467835e-07
Iter: 292 loss: 5.9163483e-07
Iter: 293 loss: 5.89889851e-07
Iter: 294 loss: 5.89097908e-07
Iter: 295 loss: 5.89068065e-07
Iter: 296 loss: 5.88313696e-07
Iter: 297 loss: 5.86660178e-07
Iter: 298 loss: 6.10878089e-07
Iter: 299 loss: 5.86538818e-07
Iter: 300 loss: 5.84831696e-07
Iter: 301 loss: 5.85315945e-07
Iter: 302 loss: 5.83571932e-07
Iter: 303 loss: 5.82611619e-07
Iter: 304 loss: 5.82430516e-07
Iter: 305 loss: 5.81372262e-07
Iter: 306 loss: 5.81798304e-07
Iter: 307 loss: 5.80667461e-07
Iter: 308 loss: 5.79806e-07
Iter: 309 loss: 5.82836947e-07
Iter: 310 loss: 5.79581581e-07
Iter: 311 loss: 5.78498486e-07
Iter: 312 loss: 5.8187743e-07
Iter: 313 loss: 5.78173172e-07
Iter: 314 loss: 5.77594278e-07
Iter: 315 loss: 5.78996946e-07
Iter: 316 loss: 5.7739004e-07
Iter: 317 loss: 5.76605316e-07
Iter: 318 loss: 5.78360414e-07
Iter: 319 loss: 5.76295292e-07
Iter: 320 loss: 5.75596914e-07
Iter: 321 loss: 5.75042804e-07
Iter: 322 loss: 5.74833393e-07
Iter: 323 loss: 5.73579314e-07
Iter: 324 loss: 5.7590546e-07
Iter: 325 loss: 5.73025773e-07
Iter: 326 loss: 5.72256624e-07
Iter: 327 loss: 5.72228601e-07
Iter: 328 loss: 5.71485771e-07
Iter: 329 loss: 5.71765327e-07
Iter: 330 loss: 5.70968041e-07
Iter: 331 loss: 5.70308032e-07
Iter: 332 loss: 5.69268195e-07
Iter: 333 loss: 5.69251313e-07
Iter: 334 loss: 5.68165831e-07
Iter: 335 loss: 5.81374479e-07
Iter: 336 loss: 5.6814622e-07
Iter: 337 loss: 5.67046186e-07
Iter: 338 loss: 5.71559781e-07
Iter: 339 loss: 5.66802328e-07
Iter: 340 loss: 5.66083031e-07
Iter: 341 loss: 5.65817345e-07
Iter: 342 loss: 5.65422965e-07
Iter: 343 loss: 5.64300592e-07
Iter: 344 loss: 5.76086506e-07
Iter: 345 loss: 5.64262791e-07
Iter: 346 loss: 5.63661501e-07
Iter: 347 loss: 5.63349033e-07
Iter: 348 loss: 5.63069761e-07
Iter: 349 loss: 5.62439141e-07
Iter: 350 loss: 5.62432206e-07
Iter: 351 loss: 5.62118657e-07
Iter: 352 loss: 5.61481613e-07
Iter: 353 loss: 5.73216482e-07
Iter: 354 loss: 5.61472405e-07
Iter: 355 loss: 5.6064016e-07
Iter: 356 loss: 5.6260086e-07
Iter: 357 loss: 5.60359126e-07
Iter: 358 loss: 5.59621526e-07
Iter: 359 loss: 5.648605e-07
Iter: 360 loss: 5.59554792e-07
Iter: 361 loss: 5.5871584e-07
Iter: 362 loss: 5.60710305e-07
Iter: 363 loss: 5.58434408e-07
Iter: 364 loss: 5.5775007e-07
Iter: 365 loss: 5.5659558e-07
Iter: 366 loss: 5.56598422e-07
Iter: 367 loss: 5.5566295e-07
Iter: 368 loss: 5.67411689e-07
Iter: 369 loss: 5.55646409e-07
Iter: 370 loss: 5.54998451e-07
Iter: 371 loss: 5.62524463e-07
Iter: 372 loss: 5.54990379e-07
Iter: 373 loss: 5.54618282e-07
Iter: 374 loss: 5.54160295e-07
Iter: 375 loss: 5.54119083e-07
Iter: 376 loss: 5.53681048e-07
Iter: 377 loss: 5.53656037e-07
Iter: 378 loss: 5.53354766e-07
Iter: 379 loss: 5.52761549e-07
Iter: 380 loss: 5.66882136e-07
Iter: 381 loss: 5.5276e-07
Iter: 382 loss: 5.52288839e-07
Iter: 383 loss: 5.52273e-07
Iter: 384 loss: 5.51860921e-07
Iter: 385 loss: 5.50900268e-07
Iter: 386 loss: 5.62960622e-07
Iter: 387 loss: 5.50817731e-07
Iter: 388 loss: 5.49752258e-07
Iter: 389 loss: 5.55441716e-07
Iter: 390 loss: 5.49592301e-07
Iter: 391 loss: 5.48814114e-07
Iter: 392 loss: 5.5311034e-07
Iter: 393 loss: 5.48698324e-07
Iter: 394 loss: 5.48054061e-07
Iter: 395 loss: 5.54910912e-07
Iter: 396 loss: 5.48042806e-07
Iter: 397 loss: 5.47692764e-07
Iter: 398 loss: 5.47081356e-07
Iter: 399 loss: 5.47072375e-07
Iter: 400 loss: 5.46469892e-07
Iter: 401 loss: 5.47424065e-07
Iter: 402 loss: 5.46204944e-07
Iter: 403 loss: 5.45643161e-07
Iter: 404 loss: 5.45630201e-07
Iter: 405 loss: 5.45197338e-07
Iter: 406 loss: 5.44365491e-07
Iter: 407 loss: 5.62725404e-07
Iter: 408 loss: 5.44363616e-07
Iter: 409 loss: 5.43923079e-07
Iter: 410 loss: 5.43850604e-07
Iter: 411 loss: 5.43382782e-07
Iter: 412 loss: 5.42648195e-07
Iter: 413 loss: 5.42644045e-07
Iter: 414 loss: 5.42166617e-07
Iter: 415 loss: 5.42153543e-07
Iter: 416 loss: 5.41703059e-07
Iter: 417 loss: 5.41336e-07
Iter: 418 loss: 5.41199256e-07
Iter: 419 loss: 5.40740757e-07
Iter: 420 loss: 5.41361487e-07
Iter: 421 loss: 5.40499173e-07
Iter: 422 loss: 5.39880944e-07
Iter: 423 loss: 5.40707674e-07
Iter: 424 loss: 5.39577854e-07
Iter: 425 loss: 5.39026246e-07
Iter: 426 loss: 5.47524394e-07
Iter: 427 loss: 5.39035909e-07
Iter: 428 loss: 5.38511813e-07
Iter: 429 loss: 5.37624828e-07
Iter: 430 loss: 5.37633866e-07
Iter: 431 loss: 5.36766663e-07
Iter: 432 loss: 5.37987148e-07
Iter: 433 loss: 5.36367224e-07
Iter: 434 loss: 5.36057087e-07
Iter: 435 loss: 5.35957724e-07
Iter: 436 loss: 5.35576646e-07
Iter: 437 loss: 5.35089669e-07
Iter: 438 loss: 5.35060508e-07
Iter: 439 loss: 5.34713308e-07
Iter: 440 loss: 5.34712228e-07
Iter: 441 loss: 5.34381286e-07
Iter: 442 loss: 5.34097182e-07
Iter: 443 loss: 5.34010951e-07
Iter: 444 loss: 5.33577e-07
Iter: 445 loss: 5.36255413e-07
Iter: 446 loss: 5.33522211e-07
Iter: 447 loss: 5.32979129e-07
Iter: 448 loss: 5.32889658e-07
Iter: 449 loss: 5.32509091e-07
Iter: 450 loss: 5.31936053e-07
Iter: 451 loss: 5.32119202e-07
Iter: 452 loss: 5.31522744e-07
Iter: 453 loss: 5.30824821e-07
Iter: 454 loss: 5.35708182e-07
Iter: 455 loss: 5.3076792e-07
Iter: 456 loss: 5.30284e-07
Iter: 457 loss: 5.3558972e-07
Iter: 458 loss: 5.30267414e-07
Iter: 459 loss: 5.29847171e-07
Iter: 460 loss: 5.2967e-07
Iter: 461 loss: 5.29429428e-07
Iter: 462 loss: 5.29006286e-07
Iter: 463 loss: 5.29148451e-07
Iter: 464 loss: 5.28709222e-07
Iter: 465 loss: 5.28319731e-07
Iter: 466 loss: 5.34219794e-07
Iter: 467 loss: 5.28318765e-07
Iter: 468 loss: 5.27865723e-07
Iter: 469 loss: 5.27791826e-07
Iter: 470 loss: 5.27486634e-07
Iter: 471 loss: 5.27127213e-07
Iter: 472 loss: 5.29802492e-07
Iter: 473 loss: 5.27093334e-07
Iter: 474 loss: 5.26709755e-07
Iter: 475 loss: 5.27120164e-07
Iter: 476 loss: 5.26473e-07
Iter: 477 loss: 5.26131714e-07
Iter: 478 loss: 5.2704047e-07
Iter: 479 loss: 5.26004726e-07
Iter: 480 loss: 5.25564133e-07
Iter: 481 loss: 5.26942074e-07
Iter: 482 loss: 5.25427765e-07
Iter: 483 loss: 5.25154519e-07
Iter: 484 loss: 5.24872462e-07
Iter: 485 loss: 5.2481505e-07
Iter: 486 loss: 5.24353936e-07
Iter: 487 loss: 5.26104202e-07
Iter: 488 loss: 5.24242864e-07
Iter: 489 loss: 5.23779363e-07
Iter: 490 loss: 5.27420411e-07
Iter: 491 loss: 5.23751851e-07
Iter: 492 loss: 5.23280789e-07
Iter: 493 loss: 5.23725e-07
Iter: 494 loss: 5.23033464e-07
Iter: 495 loss: 5.22659093e-07
Iter: 496 loss: 5.22356288e-07
Iter: 497 loss: 5.22238565e-07
Iter: 498 loss: 5.21875791e-07
Iter: 499 loss: 5.21874881e-07
Iter: 500 loss: 5.21523702e-07
Iter: 501 loss: 5.22673872e-07
Iter: 502 loss: 5.21435368e-07
Iter: 503 loss: 5.21220272e-07
Iter: 504 loss: 5.21385232e-07
Iter: 505 loss: 5.21090442e-07
Iter: 506 loss: 5.2072e-07
Iter: 507 loss: 5.22035862e-07
Iter: 508 loss: 5.20635865e-07
Iter: 509 loss: 5.2037467e-07
Iter: 510 loss: 5.20357503e-07
Iter: 511 loss: 5.20157755e-07
Iter: 512 loss: 5.1969613e-07
Iter: 513 loss: 5.22362939e-07
Iter: 514 loss: 5.19623484e-07
Iter: 515 loss: 5.19333241e-07
Iter: 516 loss: 5.18801244e-07
Iter: 517 loss: 5.31747844e-07
Iter: 518 loss: 5.1879141e-07
Iter: 519 loss: 5.18333422e-07
Iter: 520 loss: 5.22375728e-07
Iter: 521 loss: 5.18292836e-07
Iter: 522 loss: 5.18008846e-07
Iter: 523 loss: 5.21859192e-07
Iter: 524 loss: 5.18005777e-07
Iter: 525 loss: 5.177709e-07
Iter: 526 loss: 5.18148454e-07
Iter: 527 loss: 5.17652666e-07
Iter: 528 loss: 5.17424041e-07
Iter: 529 loss: 5.17039382e-07
Iter: 530 loss: 5.1705814e-07
Iter: 531 loss: 5.16650687e-07
Iter: 532 loss: 5.1990645e-07
Iter: 533 loss: 5.16643638e-07
Iter: 534 loss: 5.16255966e-07
Iter: 535 loss: 5.17911872e-07
Iter: 536 loss: 5.16165869e-07
Iter: 537 loss: 5.15836689e-07
Iter: 538 loss: 5.15580268e-07
Iter: 539 loss: 5.15477325e-07
Iter: 540 loss: 5.14969145e-07
Iter: 541 loss: 5.22107712e-07
Iter: 542 loss: 5.14964313e-07
Iter: 543 loss: 5.14732051e-07
Iter: 544 loss: 5.14615749e-07
Iter: 545 loss: 5.14514227e-07
Iter: 546 loss: 5.1422046e-07
Iter: 547 loss: 5.18792831e-07
Iter: 548 loss: 5.14229e-07
Iter: 549 loss: 5.14046e-07
Iter: 550 loss: 5.13664133e-07
Iter: 551 loss: 5.19758032e-07
Iter: 552 loss: 5.13643954e-07
Iter: 553 loss: 5.13256623e-07
Iter: 554 loss: 5.14943224e-07
Iter: 555 loss: 5.13167492e-07
Iter: 556 loss: 5.12857582e-07
Iter: 557 loss: 5.15287695e-07
Iter: 558 loss: 5.12824954e-07
Iter: 559 loss: 5.12512372e-07
Iter: 560 loss: 5.13643613e-07
Iter: 561 loss: 5.12430233e-07
Iter: 562 loss: 5.12136353e-07
Iter: 563 loss: 5.11685187e-07
Iter: 564 loss: 5.11673647e-07
Iter: 565 loss: 5.11278245e-07
Iter: 566 loss: 5.14705221e-07
Iter: 567 loss: 5.11261078e-07
Iter: 568 loss: 5.10958159e-07
Iter: 569 loss: 5.14750184e-07
Iter: 570 loss: 5.10953043e-07
Iter: 571 loss: 5.10755967e-07
Iter: 572 loss: 5.10491361e-07
Iter: 573 loss: 5.10480845e-07
Iter: 574 loss: 5.10204814e-07
Iter: 575 loss: 5.10205723e-07
Iter: 576 loss: 5.10023085e-07
Iter: 577 loss: 5.09672759e-07
Iter: 578 loss: 5.16577302e-07
Iter: 579 loss: 5.09676283e-07
Iter: 580 loss: 5.09393885e-07
Iter: 581 loss: 5.0938786e-07
Iter: 582 loss: 5.09198117e-07
Iter: 583 loss: 5.08769745e-07
Iter: 584 loss: 5.16055366e-07
Iter: 585 loss: 5.0875758e-07
Iter: 586 loss: 5.08350467e-07
Iter: 587 loss: 5.09863185e-07
Iter: 588 loss: 5.08257699e-07
Iter: 589 loss: 5.07959044e-07
Iter: 590 loss: 5.09847666e-07
Iter: 591 loss: 5.07914876e-07
Iter: 592 loss: 5.07670279e-07
Iter: 593 loss: 5.09457209e-07
Iter: 594 loss: 5.07648565e-07
Iter: 595 loss: 5.07416644e-07
Iter: 596 loss: 5.06953825e-07
Iter: 597 loss: 5.15695092e-07
Iter: 598 loss: 5.06939557e-07
Iter: 599 loss: 5.06465199e-07
Iter: 600 loss: 5.07128846e-07
Iter: 601 loss: 5.06219e-07
Iter: 602 loss: 5.0607423e-07
Iter: 603 loss: 5.05893922e-07
Iter: 604 loss: 5.05701053e-07
Iter: 605 loss: 5.05303774e-07
Iter: 606 loss: 5.128569e-07
Iter: 607 loss: 5.05284e-07
Iter: 608 loss: 5.05041839e-07
Iter: 609 loss: 5.05019898e-07
Iter: 610 loss: 5.04830211e-07
Iter: 611 loss: 5.04527861e-07
Iter: 612 loss: 5.045124e-07
Iter: 613 loss: 5.04302477e-07
Iter: 614 loss: 5.04296736e-07
Iter: 615 loss: 5.04121886e-07
Iter: 616 loss: 5.03773208e-07
Iter: 617 loss: 5.10504549e-07
Iter: 618 loss: 5.03775709e-07
Iter: 619 loss: 5.03346712e-07
Iter: 620 loss: 5.04173727e-07
Iter: 621 loss: 5.03174874e-07
Iter: 622 loss: 5.02733656e-07
Iter: 623 loss: 5.04489719e-07
Iter: 624 loss: 5.02652824e-07
Iter: 625 loss: 5.02238663e-07
Iter: 626 loss: 5.06153128e-07
Iter: 627 loss: 5.02224623e-07
Iter: 628 loss: 5.01911472e-07
Iter: 629 loss: 5.01826094e-07
Iter: 630 loss: 5.01647321e-07
Iter: 631 loss: 5.01323825e-07
Iter: 632 loss: 5.00946669e-07
Iter: 633 loss: 5.00910403e-07
Iter: 634 loss: 5.00962074e-07
Iter: 635 loss: 5.00699571e-07
Iter: 636 loss: 5.00484646e-07
Iter: 637 loss: 5.00102715e-07
Iter: 638 loss: 5.07663913e-07
Iter: 639 loss: 5.00098508e-07
Iter: 640 loss: 4.99766e-07
Iter: 641 loss: 5.04875061e-07
Iter: 642 loss: 4.99766e-07
Iter: 643 loss: 4.99429234e-07
Iter: 644 loss: 4.98999157e-07
Iter: 645 loss: 4.98971417e-07
Iter: 646 loss: 4.98639281e-07
Iter: 647 loss: 4.98640418e-07
Iter: 648 loss: 4.98341478e-07
Iter: 649 loss: 4.97929477e-07
Iter: 650 loss: 4.97910094e-07
Iter: 651 loss: 4.97510086e-07
Iter: 652 loss: 4.98972383e-07
Iter: 653 loss: 4.97402311e-07
Iter: 654 loss: 4.9710269e-07
Iter: 655 loss: 4.98541e-07
Iter: 656 loss: 4.97044e-07
Iter: 657 loss: 4.96801363e-07
Iter: 658 loss: 4.99351643e-07
Iter: 659 loss: 4.96801e-07
Iter: 660 loss: 4.9660764e-07
Iter: 661 loss: 4.96386576e-07
Iter: 662 loss: 4.96358211e-07
Iter: 663 loss: 4.96005555e-07
Iter: 664 loss: 4.9562118e-07
Iter: 665 loss: 4.95569225e-07
Iter: 666 loss: 4.95220831e-07
Iter: 667 loss: 4.95201789e-07
Iter: 668 loss: 4.94807409e-07
Iter: 669 loss: 4.95258519e-07
Iter: 670 loss: 4.94604592e-07
Iter: 671 loss: 4.94396431e-07
Iter: 672 loss: 4.95769882e-07
Iter: 673 loss: 4.94378924e-07
Iter: 674 loss: 4.94126141e-07
Iter: 675 loss: 4.94203732e-07
Iter: 676 loss: 4.93926223e-07
Iter: 677 loss: 4.93720336e-07
Iter: 678 loss: 4.94756137e-07
Iter: 679 loss: 4.93680034e-07
Iter: 680 loss: 4.9342384e-07
Iter: 681 loss: 4.93590392e-07
Iter: 682 loss: 4.93239042e-07
Iter: 683 loss: 4.93001949e-07
Iter: 684 loss: 4.92902245e-07
Iter: 685 loss: 4.92777872e-07
Iter: 686 loss: 4.92443064e-07
Iter: 687 loss: 4.94171104e-07
Iter: 688 loss: 4.92378717e-07
Iter: 689 loss: 4.92135655e-07
Iter: 690 loss: 4.9564045e-07
Iter: 691 loss: 4.92133722e-07
Iter: 692 loss: 4.91962624e-07
Iter: 693 loss: 4.92005711e-07
Iter: 694 loss: 4.91835351e-07
Iter: 695 loss: 4.91634182e-07
Iter: 696 loss: 4.91566823e-07
Iter: 697 loss: 4.91456262e-07
Iter: 698 loss: 4.91201376e-07
Iter: 699 loss: 4.91664821e-07
Iter: 700 loss: 4.91077458e-07
Iter: 701 loss: 4.90775278e-07
Iter: 702 loss: 4.90777552e-07
Iter: 703 loss: 4.90652724e-07
Iter: 704 loss: 4.90468437e-07
Iter: 705 loss: 4.904565e-07
Iter: 706 loss: 4.90203774e-07
Iter: 707 loss: 4.92976255e-07
Iter: 708 loss: 4.90207754e-07
Iter: 709 loss: 4.90080083e-07
Iter: 710 loss: 4.89951105e-07
Iter: 711 loss: 4.89907961e-07
Iter: 712 loss: 4.89696959e-07
Iter: 713 loss: 4.91680282e-07
Iter: 714 loss: 4.89681156e-07
Iter: 715 loss: 4.89577815e-07
Iter: 716 loss: 4.89375793e-07
Iter: 717 loss: 4.93603466e-07
Iter: 718 loss: 4.89372951e-07
Iter: 719 loss: 4.8913e-07
Iter: 720 loss: 4.89720833e-07
Iter: 721 loss: 4.89041668e-07
Iter: 722 loss: 4.88766887e-07
Iter: 723 loss: 4.90926709e-07
Iter: 724 loss: 4.88750402e-07
Iter: 725 loss: 4.88486762e-07
Iter: 726 loss: 4.88845899e-07
Iter: 727 loss: 4.88349826e-07
Iter: 728 loss: 4.8813132e-07
Iter: 729 loss: 4.8791793e-07
Iter: 730 loss: 4.87867737e-07
Iter: 731 loss: 4.87558395e-07
Iter: 732 loss: 4.88861815e-07
Iter: 733 loss: 4.87491263e-07
Iter: 734 loss: 4.87448574e-07
Iter: 735 loss: 4.8738292e-07
Iter: 736 loss: 4.87290777e-07
Iter: 737 loss: 4.87096941e-07
Iter: 738 loss: 4.90412049e-07
Iter: 739 loss: 4.87092507e-07
Iter: 740 loss: 4.8695756e-07
Iter: 741 loss: 4.86952956e-07
Iter: 742 loss: 4.86807949e-07
Iter: 743 loss: 4.86496106e-07
Iter: 744 loss: 4.90496859e-07
Iter: 745 loss: 4.86472345e-07
Iter: 746 loss: 4.86394754e-07
Iter: 747 loss: 4.86322392e-07
Iter: 748 loss: 4.86177555e-07
Iter: 749 loss: 4.85869e-07
Iter: 750 loss: 4.89971058e-07
Iter: 751 loss: 4.85860539e-07
Iter: 752 loss: 4.85553187e-07
Iter: 753 loss: 4.87057889e-07
Iter: 754 loss: 4.85506689e-07
Iter: 755 loss: 4.85379928e-07
Iter: 756 loss: 4.85353439e-07
Iter: 757 loss: 4.85252599e-07
Iter: 758 loss: 4.85374414e-07
Iter: 759 loss: 4.85202861e-07
Iter: 760 loss: 4.8507485e-07
Iter: 761 loss: 4.8494627e-07
Iter: 762 loss: 4.84941893e-07
Iter: 763 loss: 4.84732141e-07
Iter: 764 loss: 4.84692919e-07
Iter: 765 loss: 4.84552061e-07
Iter: 766 loss: 4.84329234e-07
Iter: 767 loss: 4.87402076e-07
Iter: 768 loss: 4.84327188e-07
Iter: 769 loss: 4.84083955e-07
Iter: 770 loss: 4.84913357e-07
Iter: 771 loss: 4.8402444e-07
Iter: 772 loss: 4.83885287e-07
Iter: 773 loss: 4.84011821e-07
Iter: 774 loss: 4.83794e-07
Iter: 775 loss: 4.83607e-07
Iter: 776 loss: 4.85035912e-07
Iter: 777 loss: 4.83581971e-07
Iter: 778 loss: 4.83502504e-07
Iter: 779 loss: 4.83457427e-07
Iter: 780 loss: 4.83422127e-07
Iter: 781 loss: 4.83254212e-07
Iter: 782 loss: 4.84237546e-07
Iter: 783 loss: 4.83237329e-07
Iter: 784 loss: 4.8314007e-07
Iter: 785 loss: 4.82955159e-07
Iter: 786 loss: 4.84821157e-07
Iter: 787 loss: 4.82921564e-07
Iter: 788 loss: 4.82651842e-07
Iter: 789 loss: 4.83114491e-07
Iter: 790 loss: 4.82515418e-07
Iter: 791 loss: 4.82292251e-07
Iter: 792 loss: 4.8481769e-07
Iter: 793 loss: 4.8228253e-07
Iter: 794 loss: 4.82114e-07
Iter: 795 loss: 4.84318662e-07
Iter: 796 loss: 4.82118367e-07
Iter: 797 loss: 4.82031965e-07
Iter: 798 loss: 4.81871666e-07
Iter: 799 loss: 4.84558e-07
Iter: 800 loss: 4.81874736e-07
Iter: 801 loss: 4.81686413e-07
Iter: 802 loss: 4.82640473e-07
Iter: 803 loss: 4.81667371e-07
Iter: 804 loss: 4.81567326e-07
Iter: 805 loss: 4.82910934e-07
Iter: 806 loss: 4.81569543e-07
Iter: 807 loss: 4.81443351e-07
Iter: 808 loss: 4.81302038e-07
Iter: 809 loss: 4.81273e-07
Iter: 810 loss: 4.81128e-07
Iter: 811 loss: 4.82000303e-07
Iter: 812 loss: 4.81109623e-07
Iter: 813 loss: 4.80951599e-07
Iter: 814 loss: 4.81766051e-07
Iter: 815 loss: 4.80916242e-07
Iter: 816 loss: 4.80833933e-07
Iter: 817 loss: 4.8099389e-07
Iter: 818 loss: 4.80787207e-07
Iter: 819 loss: 4.80645554e-07
Iter: 820 loss: 4.80814208e-07
Iter: 821 loss: 4.80587346e-07
Iter: 822 loss: 4.80502081e-07
Iter: 823 loss: 4.80388906e-07
Iter: 824 loss: 4.80358494e-07
Iter: 825 loss: 4.80194e-07
Iter: 826 loss: 4.80267317e-07
Iter: 827 loss: 4.80061374e-07
Iter: 828 loss: 4.79824394e-07
Iter: 829 loss: 4.81013444e-07
Iter: 830 loss: 4.79791254e-07
Iter: 831 loss: 4.79630671e-07
Iter: 832 loss: 4.80749918e-07
Iter: 833 loss: 4.79608047e-07
Iter: 834 loss: 4.79538357e-07
Iter: 835 loss: 4.79525397e-07
Iter: 836 loss: 4.79469463e-07
Iter: 837 loss: 4.79341224e-07
Iter: 838 loss: 4.81996778e-07
Iter: 839 loss: 4.793431e-07
Iter: 840 loss: 4.79214521e-07
Iter: 841 loss: 4.79576101e-07
Iter: 842 loss: 4.7917888e-07
Iter: 843 loss: 4.79054393e-07
Iter: 844 loss: 4.79524715e-07
Iter: 845 loss: 4.79033474e-07
Iter: 846 loss: 4.78948948e-07
Iter: 847 loss: 4.80421647e-07
Iter: 848 loss: 4.78953e-07
Iter: 849 loss: 4.78880963e-07
Iter: 850 loss: 4.78746188e-07
Iter: 851 loss: 4.81508209e-07
Iter: 852 loss: 4.78750565e-07
Iter: 853 loss: 4.78661832e-07
Iter: 854 loss: 4.78660127e-07
Iter: 855 loss: 4.78598281e-07
Iter: 856 loss: 4.78505058e-07
Iter: 857 loss: 4.80534e-07
Iter: 858 loss: 4.78507388e-07
Iter: 859 loss: 4.78444804e-07
Iter: 860 loss: 4.78431105e-07
Iter: 861 loss: 4.7837932e-07
Iter: 862 loss: 4.78272682e-07
Iter: 863 loss: 4.7826984e-07
Iter: 864 loss: 4.78185939e-07
Iter: 865 loss: 4.78128641e-07
Iter: 866 loss: 4.78072536e-07
Iter: 867 loss: 4.78012169e-07
Iter: 868 loss: 4.77995911e-07
Iter: 869 loss: 4.77904109e-07
Iter: 870 loss: 4.77847152e-07
Iter: 871 loss: 4.77816855e-07
Iter: 872 loss: 4.77704873e-07
Iter: 873 loss: 4.77652748e-07
Iter: 874 loss: 4.77593403e-07
Iter: 875 loss: 4.77469712e-07
Iter: 876 loss: 4.78982315e-07
Iter: 877 loss: 4.77468575e-07
Iter: 878 loss: 4.77408605e-07
Iter: 879 loss: 4.78397567e-07
Iter: 880 loss: 4.77382514e-07
Iter: 881 loss: 4.77325e-07
Iter: 882 loss: 4.77291167e-07
Iter: 883 loss: 4.77265246e-07
Iter: 884 loss: 4.77199478e-07
Iter: 885 loss: 4.77613582e-07
Iter: 886 loss: 4.77199706e-07
Iter: 887 loss: 4.7709716e-07
Iter: 888 loss: 4.77134506e-07
Iter: 889 loss: 4.7703611e-07
Iter: 890 loss: 4.76933565e-07
Iter: 891 loss: 4.76852222e-07
Iter: 892 loss: 4.76827665e-07
Iter: 893 loss: 4.76650939e-07
Iter: 894 loss: 4.78957872e-07
Iter: 895 loss: 4.76653724e-07
Iter: 896 loss: 4.76554646e-07
Iter: 897 loss: 4.76321446e-07
Iter: 898 loss: 4.80019764e-07
Iter: 899 loss: 4.76314e-07
Iter: 900 loss: 4.76143271e-07
Iter: 901 loss: 4.78635684e-07
Iter: 902 loss: 4.76150205e-07
Iter: 903 loss: 4.76013611e-07
Iter: 904 loss: 4.77110063e-07
Iter: 905 loss: 4.76002725e-07
Iter: 906 loss: 4.75917091e-07
Iter: 907 loss: 4.75849447e-07
Iter: 908 loss: 4.75811646e-07
Iter: 909 loss: 4.75683379e-07
Iter: 910 loss: 4.75675932e-07
Iter: 911 loss: 4.75573728e-07
Iter: 912 loss: 4.75391204e-07
Iter: 913 loss: 4.76820759e-07
Iter: 914 loss: 4.75377107e-07
Iter: 915 loss: 4.75213454e-07
Iter: 916 loss: 4.75937554e-07
Iter: 917 loss: 4.75182702e-07
Iter: 918 loss: 4.74990117e-07
Iter: 919 loss: 4.74921421e-07
Iter: 920 loss: 4.74832262e-07
Iter: 921 loss: 4.74663636e-07
Iter: 922 loss: 4.7636243e-07
Iter: 923 loss: 4.74662e-07
Iter: 924 loss: 4.74478384e-07
Iter: 925 loss: 4.74468266e-07
Iter: 926 loss: 4.7432448e-07
Iter: 927 loss: 4.74165972e-07
Iter: 928 loss: 4.74244359e-07
Iter: 929 loss: 4.74069594e-07
Iter: 930 loss: 4.73961904e-07
Iter: 931 loss: 4.73942464e-07
Iter: 932 loss: 4.73867374e-07
Iter: 933 loss: 4.73697128e-07
Iter: 934 loss: 4.75574382e-07
Iter: 935 loss: 4.73670667e-07
Iter: 936 loss: 4.7348675e-07
Iter: 937 loss: 4.73759428e-07
Iter: 938 loss: 4.73388326e-07
Iter: 939 loss: 4.73238799e-07
Iter: 940 loss: 4.73230216e-07
Iter: 941 loss: 4.73105217e-07
Iter: 942 loss: 4.7278229e-07
Iter: 943 loss: 4.77249671e-07
Iter: 944 loss: 4.72769415e-07
Iter: 945 loss: 4.72451234e-07
Iter: 946 loss: 4.72920277e-07
Iter: 947 loss: 4.72309068e-07
Iter: 948 loss: 4.72031616e-07
Iter: 949 loss: 4.74029889e-07
Iter: 950 loss: 4.71994383e-07
Iter: 951 loss: 4.71838462e-07
Iter: 952 loss: 4.71833516e-07
Iter: 953 loss: 4.71756493e-07
Iter: 954 loss: 4.715898e-07
Iter: 955 loss: 4.73625e-07
Iter: 956 loss: 4.715655e-07
Iter: 957 loss: 4.71448743e-07
Iter: 958 loss: 4.7143979e-07
Iter: 959 loss: 4.7131482e-07
Iter: 960 loss: 4.71158444e-07
Iter: 961 loss: 4.71145739e-07
Iter: 962 loss: 4.70928398e-07
Iter: 963 loss: 4.71327894e-07
Iter: 964 loss: 4.70840916e-07
Iter: 965 loss: 4.70551527e-07
Iter: 966 loss: 4.71449766e-07
Iter: 967 loss: 4.70489852e-07
Iter: 968 loss: 4.70179742e-07
Iter: 969 loss: 4.71480604e-07
Iter: 970 loss: 4.70110535e-07
Iter: 971 loss: 4.69954387e-07
Iter: 972 loss: 4.69713484e-07
Iter: 973 loss: 4.69704844e-07
Iter: 974 loss: 4.69628e-07
Iter: 975 loss: 4.69577742e-07
Iter: 976 loss: 4.69449049e-07
Iter: 977 loss: 4.69313534e-07
Iter: 978 loss: 4.6928335e-07
Iter: 979 loss: 4.69100428e-07
Iter: 980 loss: 4.69058421e-07
Iter: 981 loss: 4.68946183e-07
Iter: 982 loss: 4.68765393e-07
Iter: 983 loss: 4.687586e-07
Iter: 984 loss: 4.68549729e-07
Iter: 985 loss: 4.68535575e-07
Iter: 986 loss: 4.68380961e-07
Iter: 987 loss: 4.68220207e-07
Iter: 988 loss: 4.68915687e-07
Iter: 989 loss: 4.68173909e-07
Iter: 990 loss: 4.67975752e-07
Iter: 991 loss: 4.6875229e-07
Iter: 992 loss: 4.67926526e-07
Iter: 993 loss: 4.67807439e-07
Iter: 994 loss: 4.67769894e-07
Iter: 995 loss: 4.67699977e-07
Iter: 996 loss: 4.67535301e-07
Iter: 997 loss: 4.6833037e-07
Iter: 998 loss: 4.67503497e-07
Iter: 999 loss: 4.67379095e-07
Iter: 1000 loss: 4.68642781e-07
Iter: 1001 loss: 4.67373354e-07
Iter: 1002 loss: 4.67262367e-07
Iter: 1003 loss: 4.67050626e-07
Iter: 1004 loss: 4.71757573e-07
Iter: 1005 loss: 4.67051819e-07
Iter: 1006 loss: 4.66810491e-07
Iter: 1007 loss: 4.67121708e-07
Iter: 1008 loss: 4.66696093e-07
Iter: 1009 loss: 4.6671974e-07
Iter: 1010 loss: 4.6658414e-07
Iter: 1011 loss: 4.66512148e-07
Iter: 1012 loss: 4.66305806e-07
Iter: 1013 loss: 4.67438355e-07
Iter: 1014 loss: 4.66239698e-07
Iter: 1015 loss: 4.66038898e-07
Iter: 1016 loss: 4.67479396e-07
Iter: 1017 loss: 4.66015166e-07
Iter: 1018 loss: 4.65960795e-07
Iter: 1019 loss: 4.6592649e-07
Iter: 1020 loss: 4.65854754e-07
Iter: 1021 loss: 4.65684337e-07
Iter: 1022 loss: 4.67317477e-07
Iter: 1023 loss: 4.65653358e-07
Iter: 1024 loss: 4.65546805e-07
Iter: 1025 loss: 4.65543309e-07
Iter: 1026 loss: 4.65425728e-07
Iter: 1027 loss: 4.65254232e-07
Iter: 1028 loss: 4.65254061e-07
Iter: 1029 loss: 4.65061134e-07
Iter: 1030 loss: 4.65582616e-07
Iter: 1031 loss: 4.64998806e-07
Iter: 1032 loss: 4.64849933e-07
Iter: 1033 loss: 4.66940406e-07
Iter: 1034 loss: 4.64861699e-07
Iter: 1035 loss: 4.64734342e-07
Iter: 1036 loss: 4.64985646e-07
Iter: 1037 loss: 4.64687133e-07
Iter: 1038 loss: 4.64577909e-07
Iter: 1039 loss: 4.64423039e-07
Iter: 1040 loss: 4.64415763e-07
Iter: 1041 loss: 4.64250718e-07
Iter: 1042 loss: 4.64860108e-07
Iter: 1043 loss: 4.64222438e-07
Iter: 1044 loss: 4.64051851e-07
Iter: 1045 loss: 4.6608443e-07
Iter: 1046 loss: 4.64056825e-07
Iter: 1047 loss: 4.63946151e-07
Iter: 1048 loss: 4.63665543e-07
Iter: 1049 loss: 4.66405794e-07
Iter: 1050 loss: 4.63611173e-07
Iter: 1051 loss: 4.63331759e-07
Iter: 1052 loss: 4.64078312e-07
Iter: 1053 loss: 4.63256896e-07
Iter: 1054 loss: 4.63272585e-07
Iter: 1055 loss: 4.63161342e-07
Iter: 1056 loss: 4.63055869e-07
Iter: 1057 loss: 4.62867774e-07
Iter: 1058 loss: 4.66729347e-07
Iter: 1059 loss: 4.62863113e-07
Iter: 1060 loss: 4.62754826e-07
Iter: 1061 loss: 4.64226559e-07
Iter: 1062 loss: 4.6275531e-07
Iter: 1063 loss: 4.62617265e-07
Iter: 1064 loss: 4.62597313e-07
Iter: 1065 loss: 4.62517477e-07
Iter: 1066 loss: 4.62389494e-07
Iter: 1067 loss: 4.62200774e-07
Iter: 1068 loss: 4.62182527e-07
Iter: 1069 loss: 4.61944239e-07
Iter: 1070 loss: 4.65085321e-07
Iter: 1071 loss: 4.61952851e-07
Iter: 1072 loss: 4.61756059e-07
Iter: 1073 loss: 4.62708556e-07
Iter: 1074 loss: 4.61728462e-07
Iter: 1075 loss: 4.61580271e-07
Iter: 1076 loss: 4.61555658e-07
Iter: 1077 loss: 4.61452373e-07
Iter: 1078 loss: 4.61257116e-07
Iter: 1079 loss: 4.61502253e-07
Iter: 1080 loss: 4.61164092e-07
Iter: 1081 loss: 4.61158e-07
Iter: 1082 loss: 4.61106197e-07
Iter: 1083 loss: 4.61047506e-07
Iter: 1084 loss: 4.60896331e-07
Iter: 1085 loss: 4.61865881e-07
Iter: 1086 loss: 4.60863191e-07
Iter: 1087 loss: 4.60681974e-07
Iter: 1088 loss: 4.61032414e-07
Iter: 1089 loss: 4.6060083e-07
Iter: 1090 loss: 4.60487286e-07
Iter: 1091 loss: 4.60467732e-07
Iter: 1092 loss: 4.60329204e-07
Iter: 1093 loss: 4.60071419e-07
Iter: 1094 loss: 4.64152151e-07
Iter: 1095 loss: 4.60060789e-07
Iter: 1096 loss: 4.59981123e-07
Iter: 1097 loss: 4.59940452e-07
Iter: 1098 loss: 4.59840663e-07
Iter: 1099 loss: 4.59655439e-07
Iter: 1100 loss: 4.59657144e-07
Iter: 1101 loss: 4.59539535e-07
Iter: 1102 loss: 4.60182093e-07
Iter: 1103 loss: 4.59515235e-07
Iter: 1104 loss: 4.59421358e-07
Iter: 1105 loss: 4.60155491e-07
Iter: 1106 loss: 4.59397313e-07
Iter: 1107 loss: 4.59330153e-07
Iter: 1108 loss: 4.59375315e-07
Iter: 1109 loss: 4.592805e-07
Iter: 1110 loss: 4.59149277e-07
Iter: 1111 loss: 4.59097237e-07
Iter: 1112 loss: 4.59027291e-07
Iter: 1113 loss: 4.58853862e-07
Iter: 1114 loss: 4.59965236e-07
Iter: 1115 loss: 4.58839963e-07
Iter: 1116 loss: 4.5866517e-07
Iter: 1117 loss: 4.59612977e-07
Iter: 1118 loss: 4.58628818e-07
Iter: 1119 loss: 4.58527836e-07
Iter: 1120 loss: 4.58326099e-07
Iter: 1121 loss: 4.62330661e-07
Iter: 1122 loss: 4.58319732e-07
Iter: 1123 loss: 4.58240947e-07
Iter: 1124 loss: 4.58232648e-07
Iter: 1125 loss: 4.58138118e-07
Iter: 1126 loss: 4.58387063e-07
Iter: 1127 loss: 4.58108303e-07
Iter: 1128 loss: 4.58042848e-07
Iter: 1129 loss: 4.57953547e-07
Iter: 1130 loss: 4.57941837e-07
Iter: 1131 loss: 4.57801917e-07
Iter: 1132 loss: 4.5906296e-07
Iter: 1133 loss: 4.57802344e-07
Iter: 1134 loss: 4.57728191e-07
Iter: 1135 loss: 4.57518e-07
Iter: 1136 loss: 4.59135492e-07
Iter: 1137 loss: 4.57489136e-07
Iter: 1138 loss: 4.57315707e-07
Iter: 1139 loss: 4.57307351e-07
Iter: 1140 loss: 4.57159018e-07
Iter: 1141 loss: 4.58070048e-07
Iter: 1142 loss: 4.57119086e-07
Iter: 1143 loss: 4.57031e-07
Iter: 1144 loss: 4.57033508e-07
Iter: 1145 loss: 4.56949408e-07
Iter: 1146 loss: 4.56849421e-07
Iter: 1147 loss: 4.57117068e-07
Iter: 1148 loss: 4.56815457e-07
Iter: 1149 loss: 4.5674858e-07
Iter: 1150 loss: 4.56750428e-07
Iter: 1151 loss: 4.5669978e-07
Iter: 1152 loss: 4.56599764e-07
Iter: 1153 loss: 4.566119e-07
Iter: 1154 loss: 4.56495684e-07
Iter: 1155 loss: 4.56363921e-07
Iter: 1156 loss: 4.5636267e-07
Iter: 1157 loss: 4.56273369e-07
Iter: 1158 loss: 4.56248188e-07
Iter: 1159 loss: 4.56103862e-07
Iter: 1160 loss: 4.56071291e-07
Iter: 1161 loss: 4.55988982e-07
Iter: 1162 loss: 4.55897094e-07
Iter: 1163 loss: 4.56440176e-07
Iter: 1164 loss: 4.55885186e-07
Iter: 1165 loss: 4.55769566e-07
Iter: 1166 loss: 4.55800972e-07
Iter: 1167 loss: 4.55676741e-07
Iter: 1168 loss: 4.55600173e-07
Iter: 1169 loss: 4.55634392e-07
Iter: 1170 loss: 4.55553e-07
Iter: 1171 loss: 4.55464857e-07
Iter: 1172 loss: 4.55937453e-07
Iter: 1173 loss: 4.55459542e-07
Iter: 1174 loss: 4.55355462e-07
Iter: 1175 loss: 4.55667418e-07
Iter: 1176 loss: 4.55321413e-07
Iter: 1177 loss: 4.55223073e-07
Iter: 1178 loss: 4.55196414e-07
Iter: 1179 loss: 4.55117089e-07
Iter: 1180 loss: 4.54988538e-07
Iter: 1181 loss: 4.55558e-07
Iter: 1182 loss: 4.54955625e-07
Iter: 1183 loss: 4.54820849e-07
Iter: 1184 loss: 4.5583613e-07
Iter: 1185 loss: 4.54816416e-07
Iter: 1186 loss: 4.54719327e-07
Iter: 1187 loss: 4.54596346e-07
Iter: 1188 loss: 4.54586484e-07
Iter: 1189 loss: 4.54459212e-07
Iter: 1190 loss: 4.54752239e-07
Iter: 1191 loss: 4.54420331e-07
Iter: 1192 loss: 4.54428402e-07
Iter: 1193 loss: 4.54372127e-07
Iter: 1194 loss: 4.54338675e-07
Iter: 1195 loss: 4.54244883e-07
Iter: 1196 loss: 4.55586076e-07
Iter: 1197 loss: 4.54244088e-07
Iter: 1198 loss: 4.5414609e-07
Iter: 1199 loss: 4.54803057e-07
Iter: 1200 loss: 4.54142139e-07
Iter: 1201 loss: 4.54036922e-07
Iter: 1202 loss: 4.54103713e-07
Iter: 1203 loss: 4.53960808e-07
Iter: 1204 loss: 4.5389055e-07
Iter: 1205 loss: 4.53764642e-07
Iter: 1206 loss: 4.5375694e-07
Iter: 1207 loss: 4.53731701e-07
Iter: 1208 loss: 4.53686368e-07
Iter: 1209 loss: 4.53620174e-07
Iter: 1210 loss: 4.53606361e-07
Iter: 1211 loss: 4.53571488e-07
Iter: 1212 loss: 4.53511632e-07
Iter: 1213 loss: 4.53729712e-07
Iter: 1214 loss: 4.5347474e-07
Iter: 1215 loss: 4.5340505e-07
Iter: 1216 loss: 4.5366474e-07
Iter: 1217 loss: 4.53395415e-07
Iter: 1218 loss: 4.53319814e-07
Iter: 1219 loss: 4.53411275e-07
Iter: 1220 loss: 4.5328477e-07
Iter: 1221 loss: 4.53196947e-07
Iter: 1222 loss: 4.53066662e-07
Iter: 1223 loss: 4.53064729e-07
Iter: 1224 loss: 4.52907898e-07
Iter: 1225 loss: 4.53664541e-07
Iter: 1226 loss: 4.52880442e-07
Iter: 1227 loss: 4.52773662e-07
Iter: 1228 loss: 4.52768347e-07
Iter: 1229 loss: 4.52713721e-07
Iter: 1230 loss: 4.52603956e-07
Iter: 1231 loss: 4.54738881e-07
Iter: 1232 loss: 4.52597874e-07
Iter: 1233 loss: 4.52565018e-07
Iter: 1234 loss: 4.52538359e-07
Iter: 1235 loss: 4.52506072e-07
Iter: 1236 loss: 4.52421546e-07
Iter: 1237 loss: 4.53421194e-07
Iter: 1238 loss: 4.52412507e-07
Iter: 1239 loss: 4.52321729e-07
Iter: 1240 loss: 4.52363111e-07
Iter: 1241 loss: 4.52267614e-07
Iter: 1242 loss: 4.52200908e-07
Iter: 1243 loss: 4.52186327e-07
Iter: 1244 loss: 4.52120645e-07
Iter: 1245 loss: 4.52004315e-07
Iter: 1246 loss: 4.5319652e-07
Iter: 1247 loss: 4.51984391e-07
Iter: 1248 loss: 4.51925246e-07
Iter: 1249 loss: 4.51907937e-07
Iter: 1250 loss: 4.5183134e-07
Iter: 1251 loss: 4.52019208e-07
Iter: 1252 loss: 4.51808262e-07
Iter: 1253 loss: 4.51727658e-07
Iter: 1254 loss: 4.5163938e-07
Iter: 1255 loss: 4.51631934e-07
Iter: 1256 loss: 4.51535641e-07
Iter: 1257 loss: 4.51667574e-07
Iter: 1258 loss: 4.51473056e-07
Iter: 1259 loss: 4.51435625e-07
Iter: 1260 loss: 4.51413484e-07
Iter: 1261 loss: 4.51357266e-07
Iter: 1262 loss: 4.51276549e-07
Iter: 1263 loss: 4.51269386e-07
Iter: 1264 loss: 4.51179204e-07
Iter: 1265 loss: 4.51548658e-07
Iter: 1266 loss: 4.51171729e-07
Iter: 1267 loss: 4.51080837e-07
Iter: 1268 loss: 4.51513841e-07
Iter: 1269 loss: 4.51068672e-07
Iter: 1270 loss: 4.51010266e-07
Iter: 1271 loss: 4.50909e-07
Iter: 1272 loss: 4.53172731e-07
Iter: 1273 loss: 4.50907e-07
Iter: 1274 loss: 4.50857982e-07
Iter: 1275 loss: 4.50854714e-07
Iter: 1276 loss: 4.50813388e-07
Iter: 1277 loss: 4.50879838e-07
Iter: 1278 loss: 4.50779396e-07
Iter: 1279 loss: 4.50734603e-07
Iter: 1280 loss: 4.50716556e-07
Iter: 1281 loss: 4.50695694e-07
Iter: 1282 loss: 4.50610088e-07
Iter: 1283 loss: 4.51126112e-07
Iter: 1284 loss: 4.50603125e-07
Iter: 1285 loss: 4.50548526e-07
Iter: 1286 loss: 4.50531246e-07
Iter: 1287 loss: 4.5049012e-07
Iter: 1288 loss: 4.5040386e-07
Iter: 1289 loss: 4.50524539e-07
Iter: 1290 loss: 4.50336245e-07
Iter: 1291 loss: 4.50282585e-07
Iter: 1292 loss: 4.50446066e-07
Iter: 1293 loss: 4.50247285e-07
Iter: 1294 loss: 4.50219716e-07
Iter: 1295 loss: 4.50200645e-07
Iter: 1296 loss: 4.5017029e-07
Iter: 1297 loss: 4.50112623e-07
Iter: 1298 loss: 4.50526358e-07
Iter: 1299 loss: 4.5009898e-07
Iter: 1300 loss: 4.5002929e-07
Iter: 1301 loss: 4.50038613e-07
Iter: 1302 loss: 4.49952495e-07
Iter: 1303 loss: 4.49814848e-07
Iter: 1304 loss: 4.52776902e-07
Iter: 1305 loss: 4.49806635e-07
Iter: 1306 loss: 4.49691072e-07
Iter: 1307 loss: 4.49916257e-07
Iter: 1308 loss: 4.4964122e-07
Iter: 1309 loss: 4.49563e-07
Iter: 1310 loss: 4.49546974e-07
Iter: 1311 loss: 4.49499453e-07
Iter: 1312 loss: 4.49437835e-07
Iter: 1313 loss: 4.49428171e-07
Iter: 1314 loss: 4.49378263e-07
Iter: 1315 loss: 4.49957128e-07
Iter: 1316 loss: 4.4936391e-07
Iter: 1317 loss: 4.49312239e-07
Iter: 1318 loss: 4.49366098e-07
Iter: 1319 loss: 4.49278502e-07
Iter: 1320 loss: 4.49216657e-07
Iter: 1321 loss: 4.49238513e-07
Iter: 1322 loss: 4.49175218e-07
Iter: 1323 loss: 4.49087452e-07
Iter: 1324 loss: 4.49163707e-07
Iter: 1325 loss: 4.49045729e-07
Iter: 1326 loss: 4.48943723e-07
Iter: 1327 loss: 4.49756612e-07
Iter: 1328 loss: 4.48947446e-07
Iter: 1329 loss: 4.48860561e-07
Iter: 1330 loss: 4.49070683e-07
Iter: 1331 loss: 4.48815229e-07
Iter: 1332 loss: 4.48748949e-07
Iter: 1333 loss: 4.48663769e-07
Iter: 1334 loss: 4.48665361e-07
Iter: 1335 loss: 4.48625201e-07
Iter: 1336 loss: 4.48601924e-07
Iter: 1337 loss: 4.48558097e-07
Iter: 1338 loss: 4.48462117e-07
Iter: 1339 loss: 4.49591028e-07
Iter: 1340 loss: 4.48461435e-07
Iter: 1341 loss: 4.48390495e-07
Iter: 1342 loss: 4.49025436e-07
Iter: 1343 loss: 4.48378614e-07
Iter: 1344 loss: 4.48292269e-07
Iter: 1345 loss: 4.48492131e-07
Iter: 1346 loss: 4.48267201e-07
Iter: 1347 loss: 4.48203508e-07
Iter: 1348 loss: 4.48203e-07
Iter: 1349 loss: 4.48159454e-07
Iter: 1350 loss: 4.48064725e-07
Iter: 1351 loss: 4.48817843e-07
Iter: 1352 loss: 4.48071546e-07
Iter: 1353 loss: 4.48028743e-07
Iter: 1354 loss: 4.47979062e-07
Iter: 1355 loss: 4.47973605e-07
Iter: 1356 loss: 4.47912839e-07
Iter: 1357 loss: 4.47961497e-07
Iter: 1358 loss: 4.47884872e-07
Iter: 1359 loss: 4.47848777e-07
Iter: 1360 loss: 4.47812852e-07
Iter: 1361 loss: 4.47806656e-07
Iter: 1362 loss: 4.4773509e-07
Iter: 1363 loss: 4.47826778e-07
Iter: 1364 loss: 4.47696209e-07
Iter: 1365 loss: 4.47630157e-07
Iter: 1366 loss: 4.47975111e-07
Iter: 1367 loss: 4.47605288e-07
Iter: 1368 loss: 4.47543755e-07
Iter: 1369 loss: 4.47561945e-07
Iter: 1370 loss: 4.47501094e-07
Iter: 1371 loss: 4.47412503e-07
Iter: 1372 loss: 4.48380803e-07
Iter: 1373 loss: 4.47411111e-07
Iter: 1374 loss: 4.47372457e-07
Iter: 1375 loss: 4.47273862e-07
Iter: 1376 loss: 4.48687757e-07
Iter: 1377 loss: 4.47264256e-07
Iter: 1378 loss: 4.47184959e-07
Iter: 1379 loss: 4.47191695e-07
Iter: 1380 loss: 4.47116207e-07
Iter: 1381 loss: 4.47162336e-07
Iter: 1382 loss: 4.47066498e-07
Iter: 1383 loss: 4.47002549e-07
Iter: 1384 loss: 4.47166343e-07
Iter: 1385 loss: 4.46971399e-07
Iter: 1386 loss: 4.46878346e-07
Iter: 1387 loss: 4.47102082e-07
Iter: 1388 loss: 4.46843046e-07
Iter: 1389 loss: 4.46773328e-07
Iter: 1390 loss: 4.46954687e-07
Iter: 1391 loss: 4.46766762e-07
Iter: 1392 loss: 4.46687778e-07
Iter: 1393 loss: 4.46773441e-07
Iter: 1394 loss: 4.46640456e-07
Iter: 1395 loss: 4.46560904e-07
Iter: 1396 loss: 4.46675699e-07
Iter: 1397 loss: 4.46507727e-07
Iter: 1398 loss: 4.46428231e-07
Iter: 1399 loss: 4.46901424e-07
Iter: 1400 loss: 4.46431613e-07
Iter: 1401 loss: 4.46362208e-07
Iter: 1402 loss: 4.46281348e-07
Iter: 1403 loss: 4.4625807e-07
Iter: 1404 loss: 4.46189233e-07
Iter: 1405 loss: 4.46176756e-07
Iter: 1406 loss: 4.46125739e-07
Iter: 1407 loss: 4.46022909e-07
Iter: 1408 loss: 4.47838204e-07
Iter: 1409 loss: 4.4602973e-07
Iter: 1410 loss: 4.45908427e-07
Iter: 1411 loss: 4.45961462e-07
Iter: 1412 loss: 4.45857324e-07
Iter: 1413 loss: 4.45840669e-07
Iter: 1414 loss: 4.45790079e-07
Iter: 1415 loss: 4.45737555e-07
Iter: 1416 loss: 4.45632111e-07
Iter: 1417 loss: 4.45629809e-07
Iter: 1418 loss: 4.45570066e-07
Iter: 1419 loss: 4.45563529e-07
Iter: 1420 loss: 4.45514047e-07
Iter: 1421 loss: 4.45542e-07
Iter: 1422 loss: 4.45461581e-07
Iter: 1423 loss: 4.45403941e-07
Iter: 1424 loss: 4.45362105e-07
Iter: 1425 loss: 4.45338799e-07
Iter: 1426 loss: 4.4520425e-07
Iter: 1427 loss: 4.4653828e-07
Iter: 1428 loss: 4.45216699e-07
Iter: 1429 loss: 4.4515329e-07
Iter: 1430 loss: 4.45063904e-07
Iter: 1431 loss: 4.45062597e-07
Iter: 1432 loss: 4.44910029e-07
Iter: 1433 loss: 4.45598744e-07
Iter: 1434 loss: 4.4489542e-07
Iter: 1435 loss: 4.44810837e-07
Iter: 1436 loss: 4.44774287e-07
Iter: 1437 loss: 4.44713862e-07
Iter: 1438 loss: 4.44674839e-07
Iter: 1439 loss: 4.44638886e-07
Iter: 1440 loss: 4.44591706e-07
Iter: 1441 loss: 4.44485778e-07
Iter: 1442 loss: 4.45688215e-07
Iter: 1443 loss: 4.44472704e-07
Iter: 1444 loss: 4.44415349e-07
Iter: 1445 loss: 4.44408585e-07
Iter: 1446 loss: 4.44343868e-07
Iter: 1447 loss: 4.444029e-07
Iter: 1448 loss: 4.44298678e-07
Iter: 1449 loss: 4.44246183e-07
Iter: 1450 loss: 4.44281341e-07
Iter: 1451 loss: 4.44188061e-07
Iter: 1452 loss: 4.44105041e-07
Iter: 1453 loss: 4.45165085e-07
Iter: 1454 loss: 4.44102568e-07
Iter: 1455 loss: 4.44045412e-07
Iter: 1456 loss: 4.43948721e-07
Iter: 1457 loss: 4.43935676e-07
Iter: 1458 loss: 4.43837962e-07
Iter: 1459 loss: 4.43838388e-07
Iter: 1460 loss: 4.43762019e-07
Iter: 1461 loss: 4.43775946e-07
Iter: 1462 loss: 4.43721945e-07
Iter: 1463 loss: 4.43655949e-07
Iter: 1464 loss: 4.43963017e-07
Iter: 1465 loss: 4.43641454e-07
Iter: 1466 loss: 4.43579438e-07
Iter: 1467 loss: 4.43528762e-07
Iter: 1468 loss: 4.43500312e-07
Iter: 1469 loss: 4.43442218e-07
Iter: 1470 loss: 4.4343642e-07
Iter: 1471 loss: 4.43363319e-07
Iter: 1472 loss: 4.43262877e-07
Iter: 1473 loss: 4.43259694e-07
Iter: 1474 loss: 4.43132592e-07
Iter: 1475 loss: 4.43468423e-07
Iter: 1476 loss: 4.43112356e-07
Iter: 1477 loss: 4.43009583e-07
Iter: 1478 loss: 4.44503286e-07
Iter: 1479 loss: 4.43016489e-07
Iter: 1480 loss: 4.42946146e-07
Iter: 1481 loss: 4.42874807e-07
Iter: 1482 loss: 4.42860369e-07
Iter: 1483 loss: 4.42769363e-07
Iter: 1484 loss: 4.43685252e-07
Iter: 1485 loss: 4.42759813e-07
Iter: 1486 loss: 4.42674093e-07
Iter: 1487 loss: 4.42786387e-07
Iter: 1488 loss: 4.42641266e-07
Iter: 1489 loss: 4.42591784e-07
Iter: 1490 loss: 4.42810972e-07
Iter: 1491 loss: 4.42588316e-07
Iter: 1492 loss: 4.42520076e-07
Iter: 1493 loss: 4.42488044e-07
Iter: 1494 loss: 4.42455502e-07
Iter: 1495 loss: 4.42389364e-07
Iter: 1496 loss: 4.42621626e-07
Iter: 1497 loss: 4.42348977e-07
Iter: 1498 loss: 4.42260443e-07
Iter: 1499 loss: 4.42399198e-07
Iter: 1500 loss: 4.42210137e-07
Iter: 1501 loss: 4.42123621e-07
Iter: 1502 loss: 4.42256493e-07
Iter: 1503 loss: 4.42085195e-07
Iter: 1504 loss: 4.41972588e-07
Iter: 1505 loss: 4.43208108e-07
Iter: 1506 loss: 4.41976596e-07
Iter: 1507 loss: 4.41902415e-07
Iter: 1508 loss: 4.41811125e-07
Iter: 1509 loss: 4.4180959e-07
Iter: 1510 loss: 4.41750785e-07
Iter: 1511 loss: 4.41749052e-07
Iter: 1512 loss: 4.41691e-07
Iter: 1513 loss: 4.41570251e-07
Iter: 1514 loss: 4.41566868e-07
Iter: 1515 loss: 4.41476686e-07
Iter: 1516 loss: 4.42668238e-07
Iter: 1517 loss: 4.41476971e-07
Iter: 1518 loss: 4.4137937e-07
Iter: 1519 loss: 4.41403671e-07
Iter: 1520 loss: 4.41322925e-07
Iter: 1521 loss: 4.41229474e-07
Iter: 1522 loss: 4.41893917e-07
Iter: 1523 loss: 4.41213359e-07
Iter: 1524 loss: 4.41137274e-07
Iter: 1525 loss: 4.41265172e-07
Iter: 1526 loss: 4.41107943e-07
Iter: 1527 loss: 4.41040612e-07
Iter: 1528 loss: 4.40988458e-07
Iter: 1529 loss: 4.40960207e-07
Iter: 1530 loss: 4.40852659e-07
Iter: 1531 loss: 4.41745271e-07
Iter: 1532 loss: 4.40844047e-07
Iter: 1533 loss: 4.40770265e-07
Iter: 1534 loss: 4.40704923e-07
Iter: 1535 loss: 4.40670647e-07
Iter: 1536 loss: 4.4062358e-07
Iter: 1537 loss: 4.40619232e-07
Iter: 1538 loss: 4.40552014e-07
Iter: 1539 loss: 4.40434945e-07
Iter: 1540 loss: 4.42378962e-07
Iter: 1541 loss: 4.40431165e-07
Iter: 1542 loss: 4.4033078e-07
Iter: 1543 loss: 4.41719067e-07
Iter: 1544 loss: 4.40329529e-07
Iter: 1545 loss: 4.40230963e-07
Iter: 1546 loss: 4.40358548e-07
Iter: 1547 loss: 4.40176024e-07
Iter: 1548 loss: 4.40103406e-07
Iter: 1549 loss: 4.40552924e-07
Iter: 1550 loss: 4.40088343e-07
Iter: 1551 loss: 4.400286e-07
Iter: 1552 loss: 4.40104031e-07
Iter: 1553 loss: 4.39999184e-07
Iter: 1554 loss: 4.39950327e-07
Iter: 1555 loss: 4.40292155e-07
Iter: 1556 loss: 4.39939413e-07
Iter: 1557 loss: 4.39894166e-07
Iter: 1558 loss: 4.40213341e-07
Iter: 1559 loss: 4.39877283e-07
Iter: 1560 loss: 4.39843802e-07
Iter: 1561 loss: 4.39756178e-07
Iter: 1562 loss: 4.39754842e-07
Iter: 1563 loss: 4.39674238e-07
Iter: 1564 loss: 4.40082971e-07
Iter: 1565 loss: 4.39663097e-07
Iter: 1566 loss: 4.39571522e-07
Iter: 1567 loss: 4.39559585e-07
Iter: 1568 loss: 4.39485319e-07
Iter: 1569 loss: 4.39409291e-07
Iter: 1570 loss: 4.40615167e-07
Iter: 1571 loss: 4.39405966e-07
Iter: 1572 loss: 4.39303733e-07
Iter: 1573 loss: 4.39313112e-07
Iter: 1574 loss: 4.39236942e-07
Iter: 1575 loss: 4.39149176e-07
Iter: 1576 loss: 4.39375299e-07
Iter: 1577 loss: 4.39114473e-07
Iter: 1578 loss: 4.39038303e-07
Iter: 1579 loss: 4.39723038e-07
Iter: 1580 loss: 4.39033727e-07
Iter: 1581 loss: 4.38977963e-07
Iter: 1582 loss: 4.3897802e-07
Iter: 1583 loss: 4.38928055e-07
Iter: 1584 loss: 4.3883918e-07
Iter: 1585 loss: 4.39241148e-07
Iter: 1586 loss: 4.38822781e-07
Iter: 1587 loss: 4.38755279e-07
Iter: 1588 loss: 4.38692553e-07
Iter: 1589 loss: 4.3867874e-07
Iter: 1590 loss: 4.38582788e-07
Iter: 1591 loss: 4.39967351e-07
Iter: 1592 loss: 4.38590121e-07
Iter: 1593 loss: 4.38508607e-07
Iter: 1594 loss: 4.38379857e-07
Iter: 1595 loss: 4.38375793e-07
Iter: 1596 loss: 4.38253551e-07
Iter: 1597 loss: 4.38621555e-07
Iter: 1598 loss: 4.38204893e-07
Iter: 1599 loss: 4.38101637e-07
Iter: 1600 loss: 4.39189535e-07
Iter: 1601 loss: 4.3809365e-07
Iter: 1602 loss: 4.38031321e-07
Iter: 1603 loss: 4.38009437e-07
Iter: 1604 loss: 4.37984397e-07
Iter: 1605 loss: 4.37884069e-07
Iter: 1606 loss: 4.3788404e-07
Iter: 1607 loss: 4.37835723e-07
Iter: 1608 loss: 4.37749463e-07
Iter: 1609 loss: 4.39962662e-07
Iter: 1610 loss: 4.37746905e-07
Iter: 1611 loss: 4.37644815e-07
Iter: 1612 loss: 4.3902088e-07
Iter: 1613 loss: 4.37638761e-07
Iter: 1614 loss: 4.375797e-07
Iter: 1615 loss: 4.37488438e-07
Iter: 1616 loss: 4.37488382e-07
Iter: 1617 loss: 4.37411188e-07
Iter: 1618 loss: 4.37414172e-07
Iter: 1619 loss: 4.37366737e-07
Iter: 1620 loss: 4.37240544e-07
Iter: 1621 loss: 4.38775913e-07
Iter: 1622 loss: 4.37241567e-07
Iter: 1623 loss: 4.37199e-07
Iter: 1624 loss: 4.37172957e-07
Iter: 1625 loss: 4.37125067e-07
Iter: 1626 loss: 4.37156018e-07
Iter: 1627 loss: 4.37093462e-07
Iter: 1628 loss: 4.3703696e-07
Iter: 1629 loss: 4.36996373e-07
Iter: 1630 loss: 4.36985772e-07
Iter: 1631 loss: 4.36870408e-07
Iter: 1632 loss: 4.36688254e-07
Iter: 1633 loss: 4.40167071e-07
Iter: 1634 loss: 4.36690073e-07
Iter: 1635 loss: 4.36550067e-07
Iter: 1636 loss: 4.38779e-07
Iter: 1637 loss: 4.36552313e-07
Iter: 1638 loss: 4.36451842e-07
Iter: 1639 loss: 4.3754514e-07
Iter: 1640 loss: 4.36452126e-07
Iter: 1641 loss: 4.36373512e-07
Iter: 1642 loss: 4.36378173e-07
Iter: 1643 loss: 4.36312177e-07
Iter: 1644 loss: 4.36224354e-07
Iter: 1645 loss: 4.36842186e-07
Iter: 1646 loss: 4.36218954e-07
Iter: 1647 loss: 4.36134087e-07
Iter: 1648 loss: 4.36115101e-07
Iter: 1649 loss: 4.3607136e-07
Iter: 1650 loss: 4.35980184e-07
Iter: 1651 loss: 4.36494588e-07
Iter: 1652 loss: 4.35977114e-07
Iter: 1653 loss: 4.35870675e-07
Iter: 1654 loss: 4.35812808e-07
Iter: 1655 loss: 4.35781544e-07
Iter: 1656 loss: 4.35655977e-07
Iter: 1657 loss: 4.36054449e-07
Iter: 1658 loss: 4.35642818e-07
Iter: 1659 loss: 4.35513755e-07
Iter: 1660 loss: 4.36088158e-07
Iter: 1661 loss: 4.35505e-07
Iter: 1662 loss: 4.35423345e-07
Iter: 1663 loss: 4.3535573e-07
Iter: 1664 loss: 4.35336915e-07
Iter: 1665 loss: 4.35209358e-07
Iter: 1666 loss: 4.35636252e-07
Iter: 1667 loss: 4.3519745e-07
Iter: 1668 loss: 4.35090215e-07
Iter: 1669 loss: 4.35198e-07
Iter: 1670 loss: 4.35042068e-07
Iter: 1671 loss: 4.34920821e-07
Iter: 1672 loss: 4.35387904e-07
Iter: 1673 loss: 4.34896634e-07
Iter: 1674 loss: 4.34770698e-07
Iter: 1675 loss: 4.35642875e-07
Iter: 1676 loss: 4.34773312e-07
Iter: 1677 loss: 4.34672671e-07
Iter: 1678 loss: 4.34790138e-07
Iter: 1679 loss: 4.34620461e-07
Iter: 1680 loss: 4.34514391e-07
Iter: 1681 loss: 4.35222091e-07
Iter: 1682 loss: 4.34520075e-07
Iter: 1683 loss: 4.3442833e-07
Iter: 1684 loss: 4.3440636e-07
Iter: 1685 loss: 4.34358014e-07
Iter: 1686 loss: 4.34309953e-07
Iter: 1687 loss: 4.34295032e-07
Iter: 1688 loss: 4.3426266e-07
Iter: 1689 loss: 4.34155737e-07
Iter: 1690 loss: 4.3586715e-07
Iter: 1691 loss: 4.34137746e-07
Iter: 1692 loss: 4.34059757e-07
Iter: 1693 loss: 4.34592067e-07
Iter: 1694 loss: 4.34049412e-07
Iter: 1695 loss: 4.33932058e-07
Iter: 1696 loss: 4.34127173e-07
Iter: 1697 loss: 4.3388502e-07
Iter: 1698 loss: 4.33787676e-07
Iter: 1699 loss: 4.33645425e-07
Iter: 1700 loss: 4.33629566e-07
Iter: 1701 loss: 4.33479187e-07
Iter: 1702 loss: 4.34938045e-07
Iter: 1703 loss: 4.33466965e-07
Iter: 1704 loss: 4.33325567e-07
Iter: 1705 loss: 4.33852108e-07
Iter: 1706 loss: 4.3329112e-07
Iter: 1707 loss: 4.33229303e-07
Iter: 1708 loss: 4.33396337e-07
Iter: 1709 loss: 4.33198579e-07
Iter: 1710 loss: 4.33096602e-07
Iter: 1711 loss: 4.33631357e-07
Iter: 1712 loss: 4.33103423e-07
Iter: 1713 loss: 4.3301128e-07
Iter: 1714 loss: 4.33061615e-07
Iter: 1715 loss: 4.32966885e-07
Iter: 1716 loss: 4.32852971e-07
Iter: 1717 loss: 4.33293195e-07
Iter: 1718 loss: 4.32828045e-07
Iter: 1719 loss: 4.32708362e-07
Iter: 1720 loss: 4.32978112e-07
Iter: 1721 loss: 4.32668429e-07
Iter: 1722 loss: 4.32575e-07
Iter: 1723 loss: 4.32549257e-07
Iter: 1724 loss: 4.32499746e-07
Iter: 1725 loss: 4.32361958e-07
Iter: 1726 loss: 4.34207863e-07
Iter: 1727 loss: 4.32366278e-07
Iter: 1728 loss: 4.32291813e-07
Iter: 1729 loss: 4.32147147e-07
Iter: 1730 loss: 4.34551225e-07
Iter: 1731 loss: 4.32142343e-07
Iter: 1732 loss: 4.32013167e-07
Iter: 1733 loss: 4.32367926e-07
Iter: 1734 loss: 4.31964622e-07
Iter: 1735 loss: 4.31932534e-07
Iter: 1736 loss: 4.31904e-07
Iter: 1737 loss: 4.31842636e-07
Iter: 1738 loss: 4.31729973e-07
Iter: 1739 loss: 4.33845059e-07
Iter: 1740 loss: 4.31737078e-07
Iter: 1741 loss: 4.31643798e-07
Iter: 1742 loss: 4.31909882e-07
Iter: 1743 loss: 4.31606679e-07
Iter: 1744 loss: 4.31499444e-07
Iter: 1745 loss: 4.31924207e-07
Iter: 1746 loss: 4.31468976e-07
Iter: 1747 loss: 4.31378965e-07
Iter: 1748 loss: 4.31518174e-07
Iter: 1749 loss: 4.31333262e-07
Iter: 1750 loss: 4.31206246e-07
Iter: 1751 loss: 4.32281951e-07
Iter: 1752 loss: 4.31196838e-07
Iter: 1753 loss: 4.31123226e-07
Iter: 1754 loss: 4.31043247e-07
Iter: 1755 loss: 4.31023068e-07
Iter: 1756 loss: 4.3089085e-07
Iter: 1757 loss: 4.32087461e-07
Iter: 1758 loss: 4.3089176e-07
Iter: 1759 loss: 4.30802231e-07
Iter: 1760 loss: 4.30702755e-07
Iter: 1761 loss: 4.30676977e-07
Iter: 1762 loss: 4.30615216e-07
Iter: 1763 loss: 4.30613028e-07
Iter: 1764 loss: 4.30550472e-07
Iter: 1765 loss: 4.304718e-07
Iter: 1766 loss: 4.30459409e-07
Iter: 1767 loss: 4.3039671e-07
Iter: 1768 loss: 4.31077581e-07
Iter: 1769 loss: 4.30380084e-07
Iter: 1770 loss: 4.30295415e-07
Iter: 1771 loss: 4.30539785e-07
Iter: 1772 loss: 4.30274781e-07
Iter: 1773 loss: 4.30224361e-07
Iter: 1774 loss: 4.30143473e-07
Iter: 1775 loss: 4.3228215e-07
Iter: 1776 loss: 4.30141938e-07
Iter: 1777 loss: 4.30040359e-07
Iter: 1778 loss: 4.30725095e-07
Iter: 1779 loss: 4.30028905e-07
Iter: 1780 loss: 4.29919737e-07
Iter: 1781 loss: 4.30161322e-07
Iter: 1782 loss: 4.29870738e-07
Iter: 1783 loss: 4.29786496e-07
Iter: 1784 loss: 4.29792863e-07
Iter: 1785 loss: 4.2973366e-07
Iter: 1786 loss: 4.29716579e-07
Iter: 1787 loss: 4.29682245e-07
Iter: 1788 loss: 4.29615113e-07
Iter: 1789 loss: 4.30064745e-07
Iter: 1790 loss: 4.29604512e-07
Iter: 1791 loss: 4.29530246e-07
Iter: 1792 loss: 4.29544627e-07
Iter: 1793 loss: 4.29474198e-07
Iter: 1794 loss: 4.29395584e-07
Iter: 1795 loss: 4.29523396e-07
Iter: 1796 loss: 4.29363297e-07
Iter: 1797 loss: 4.29254669e-07
Iter: 1798 loss: 4.2965965e-07
Iter: 1799 loss: 4.2924114e-07
Iter: 1800 loss: 4.29144251e-07
Iter: 1801 loss: 4.29116483e-07
Iter: 1802 loss: 4.29078625e-07
Iter: 1803 loss: 4.28966871e-07
Iter: 1804 loss: 4.30492832e-07
Iter: 1805 loss: 4.28978581e-07
Iter: 1806 loss: 4.28905025e-07
Iter: 1807 loss: 4.28747512e-07
Iter: 1808 loss: 4.30929362e-07
Iter: 1809 loss: 4.2874143e-07
Iter: 1810 loss: 4.28584286e-07
Iter: 1811 loss: 4.28903775e-07
Iter: 1812 loss: 4.28527869e-07
Iter: 1813 loss: 4.28430837e-07
Iter: 1814 loss: 4.28419781e-07
Iter: 1815 loss: 4.28321528e-07
Iter: 1816 loss: 4.28508656e-07
Iter: 1817 loss: 4.28285773e-07
Iter: 1818 loss: 4.28173735e-07
Iter: 1819 loss: 4.2860438e-07
Iter: 1820 loss: 4.28144915e-07
Iter: 1821 loss: 4.28063288e-07
Iter: 1822 loss: 4.28006558e-07
Iter: 1823 loss: 4.27989022e-07
Iter: 1824 loss: 4.27811784e-07
Iter: 1825 loss: 4.29387626e-07
Iter: 1826 loss: 4.27814655e-07
Iter: 1827 loss: 4.27710887e-07
Iter: 1828 loss: 4.27586912e-07
Iter: 1829 loss: 4.27582279e-07
Iter: 1830 loss: 4.27502357e-07
Iter: 1831 loss: 4.27489653e-07
Iter: 1832 loss: 4.27411067e-07
Iter: 1833 loss: 4.27246022e-07
Iter: 1834 loss: 4.30069e-07
Iter: 1835 loss: 4.27250853e-07
Iter: 1836 loss: 4.27149416e-07
Iter: 1837 loss: 4.27135546e-07
Iter: 1838 loss: 4.27023537e-07
Iter: 1839 loss: 4.27027089e-07
Iter: 1840 loss: 4.26946087e-07
Iter: 1841 loss: 4.26828535e-07
Iter: 1842 loss: 4.26742787e-07
Iter: 1843 loss: 4.26714479e-07
Iter: 1844 loss: 4.26549576e-07
Iter: 1845 loss: 4.26885322e-07
Iter: 1846 loss: 4.26491738e-07
Iter: 1847 loss: 4.26328711e-07
Iter: 1848 loss: 4.26857781e-07
Iter: 1849 loss: 4.26301739e-07
Iter: 1850 loss: 4.26171027e-07
Iter: 1851 loss: 4.27546297e-07
Iter: 1852 loss: 4.26177962e-07
Iter: 1853 loss: 4.26065583e-07
Iter: 1854 loss: 4.26005045e-07
Iter: 1855 loss: 4.25968267e-07
Iter: 1856 loss: 4.25844604e-07
Iter: 1857 loss: 4.27761194e-07
Iter: 1858 loss: 4.25846792e-07
Iter: 1859 loss: 4.25755218e-07
Iter: 1860 loss: 4.25585085e-07
Iter: 1861 loss: 4.25593e-07
Iter: 1862 loss: 4.25421831e-07
Iter: 1863 loss: 4.25484671e-07
Iter: 1864 loss: 4.25305927e-07
Iter: 1865 loss: 4.25213955e-07
Iter: 1866 loss: 4.25213443e-07
Iter: 1867 loss: 4.25090917e-07
Iter: 1868 loss: 4.25214353e-07
Iter: 1869 loss: 4.25042799e-07
Iter: 1870 loss: 4.24971233e-07
Iter: 1871 loss: 4.24956625e-07
Iter: 1872 loss: 4.24912457e-07
Iter: 1873 loss: 4.2483228e-07
Iter: 1874 loss: 4.24831939e-07
Iter: 1875 loss: 4.24763471e-07
Iter: 1876 loss: 4.2462e-07
Iter: 1877 loss: 4.26341842e-07
Iter: 1878 loss: 4.24615223e-07
Iter: 1879 loss: 4.244244e-07
Iter: 1880 loss: 4.26093692e-07
Iter: 1881 loss: 4.24411951e-07
Iter: 1882 loss: 4.24237328e-07
Iter: 1883 loss: 4.24466407e-07
Iter: 1884 loss: 4.241559e-07
Iter: 1885 loss: 4.23967435e-07
Iter: 1886 loss: 4.25226801e-07
Iter: 1887 loss: 4.23966867e-07
Iter: 1888 loss: 4.2385031e-07
Iter: 1889 loss: 4.23845421e-07
Iter: 1890 loss: 4.23731109e-07
Iter: 1891 loss: 4.23663039e-07
Iter: 1892 loss: 4.23654e-07
Iter: 1893 loss: 4.23588176e-07
Iter: 1894 loss: 4.23472244e-07
Iter: 1895 loss: 4.26242025e-07
Iter: 1896 loss: 4.23470965e-07
Iter: 1897 loss: 4.23377429e-07
Iter: 1898 loss: 4.23596e-07
Iter: 1899 loss: 4.23322376e-07
Iter: 1900 loss: 4.23197378e-07
Iter: 1901 loss: 4.2410278e-07
Iter: 1902 loss: 4.23194308e-07
Iter: 1903 loss: 4.23075875e-07
Iter: 1904 loss: 4.23015763e-07
Iter: 1905 loss: 4.2298575e-07
Iter: 1906 loss: 4.2283574e-07
Iter: 1907 loss: 4.23052938e-07
Iter: 1908 loss: 4.22785263e-07
Iter: 1909 loss: 4.22740982e-07
Iter: 1910 loss: 4.22707586e-07
Iter: 1911 loss: 4.22642159e-07
Iter: 1912 loss: 4.22497806e-07
Iter: 1913 loss: 4.23663607e-07
Iter: 1914 loss: 4.22483083e-07
Iter: 1915 loss: 4.22424876e-07
Iter: 1916 loss: 4.22397363e-07
Iter: 1917 loss: 4.22334722e-07
Iter: 1918 loss: 4.22377695e-07
Iter: 1919 loss: 4.22302293e-07
Iter: 1920 loss: 4.2222527e-07
Iter: 1921 loss: 4.22372466e-07
Iter: 1922 loss: 4.22191192e-07
Iter: 1923 loss: 4.22107973e-07
Iter: 1924 loss: 4.2222581e-07
Iter: 1925 loss: 4.22076198e-07
Iter: 1926 loss: 4.21960976e-07
Iter: 1927 loss: 4.2242641e-07
Iter: 1928 loss: 4.21946936e-07
Iter: 1929 loss: 4.21870766e-07
Iter: 1930 loss: 4.21815855e-07
Iter: 1931 loss: 4.21803065e-07
Iter: 1932 loss: 4.21741248e-07
Iter: 1933 loss: 4.21744915e-07
Iter: 1934 loss: 4.21683978e-07
Iter: 1935 loss: 4.21604909e-07
Iter: 1936 loss: 4.21602721e-07
Iter: 1937 loss: 4.21520269e-07
Iter: 1938 loss: 4.2164703e-07
Iter: 1939 loss: 4.21492643e-07
Iter: 1940 loss: 4.21440291e-07
Iter: 1941 loss: 4.220596e-07
Iter: 1942 loss: 4.21433413e-07
Iter: 1943 loss: 4.2137512e-07
Iter: 1944 loss: 4.21318703e-07
Iter: 1945 loss: 4.21313246e-07
Iter: 1946 loss: 4.21222126e-07
Iter: 1947 loss: 4.21246455e-07
Iter: 1948 loss: 4.21153544e-07
Iter: 1949 loss: 4.21069757e-07
Iter: 1950 loss: 4.2106717e-07
Iter: 1951 loss: 4.21025817e-07
Iter: 1952 loss: 4.20997139e-07
Iter: 1953 loss: 4.20979859e-07
Iter: 1954 loss: 4.20906815e-07
Iter: 1955 loss: 4.21183586e-07
Iter: 1956 loss: 4.20889137e-07
Iter: 1957 loss: 4.20803872e-07
Iter: 1958 loss: 4.21366053e-07
Iter: 1959 loss: 4.20809329e-07
Iter: 1960 loss: 4.20744641e-07
Iter: 1961 loss: 4.20620239e-07
Iter: 1962 loss: 4.20628851e-07
Iter: 1963 loss: 4.20529233e-07
Iter: 1964 loss: 4.21333596e-07
Iter: 1965 loss: 4.20518802e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.2
+ date
Mon Oct 26 14:58:25 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.2/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.2_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi -1 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.2_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be2ccf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be361f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be361d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be3e3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be3f7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be3f7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be27b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be222a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be222d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be1eb510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be1a6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be1be048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be1be510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be161488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be1076a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be0ea268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be0eab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be0f6bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be0a8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be04ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be0590d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f13be040ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f139db8b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f139db6e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f139db6e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f139db34840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f139daf49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f139dacc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f139dacc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f139dacc620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f139da6e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f137835b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f137835bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1378351a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f137830dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f137830d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.51072753e-06
Iter: 2 loss: 4.45419528e-06
Iter: 3 loss: 3.92531092e-06
Iter: 4 loss: 3.49488914e-06
Iter: 5 loss: 4.31863737e-06
Iter: 6 loss: 3.31463093e-06
Iter: 7 loss: 3.08081917e-06
Iter: 8 loss: 4.53935354e-06
Iter: 9 loss: 3.05365438e-06
Iter: 10 loss: 2.89560535e-06
Iter: 11 loss: 3.08369272e-06
Iter: 12 loss: 2.81208895e-06
Iter: 13 loss: 2.69375255e-06
Iter: 14 loss: 3.32804802e-06
Iter: 15 loss: 2.67590895e-06
Iter: 16 loss: 2.57399324e-06
Iter: 17 loss: 3.09202278e-06
Iter: 18 loss: 2.55732857e-06
Iter: 19 loss: 2.49097593e-06
Iter: 20 loss: 2.41823818e-06
Iter: 21 loss: 2.40739769e-06
Iter: 22 loss: 2.2993429e-06
Iter: 23 loss: 2.23877169e-06
Iter: 24 loss: 2.1915871e-06
Iter: 25 loss: 2.25820963e-06
Iter: 26 loss: 2.13860653e-06
Iter: 27 loss: 2.09510563e-06
Iter: 28 loss: 2.00687145e-06
Iter: 29 loss: 3.62383571e-06
Iter: 30 loss: 2.00545946e-06
Iter: 31 loss: 1.93872529e-06
Iter: 32 loss: 2.10922099e-06
Iter: 33 loss: 1.91569734e-06
Iter: 34 loss: 1.856366e-06
Iter: 35 loss: 1.92414609e-06
Iter: 36 loss: 1.82457575e-06
Iter: 37 loss: 1.7562113e-06
Iter: 38 loss: 2.00775435e-06
Iter: 39 loss: 1.73933461e-06
Iter: 40 loss: 1.66779978e-06
Iter: 41 loss: 1.77011361e-06
Iter: 42 loss: 1.63290076e-06
Iter: 43 loss: 1.65944027e-06
Iter: 44 loss: 1.60144918e-06
Iter: 45 loss: 1.58237742e-06
Iter: 46 loss: 1.53518283e-06
Iter: 47 loss: 2.00234081e-06
Iter: 48 loss: 1.5292261e-06
Iter: 49 loss: 1.48374602e-06
Iter: 50 loss: 1.74952493e-06
Iter: 51 loss: 1.47778519e-06
Iter: 52 loss: 1.45459489e-06
Iter: 53 loss: 1.45373383e-06
Iter: 54 loss: 1.43962211e-06
Iter: 55 loss: 1.43027114e-06
Iter: 56 loss: 1.42495549e-06
Iter: 57 loss: 1.40036718e-06
Iter: 58 loss: 1.46610228e-06
Iter: 59 loss: 1.39217161e-06
Iter: 60 loss: 1.36299059e-06
Iter: 61 loss: 1.43539432e-06
Iter: 62 loss: 1.35270875e-06
Iter: 63 loss: 1.32648847e-06
Iter: 64 loss: 1.30372223e-06
Iter: 65 loss: 1.2967156e-06
Iter: 66 loss: 1.2620651e-06
Iter: 67 loss: 1.33235039e-06
Iter: 68 loss: 1.24803364e-06
Iter: 69 loss: 1.19970628e-06
Iter: 70 loss: 1.65214715e-06
Iter: 71 loss: 1.19768856e-06
Iter: 72 loss: 1.18292621e-06
Iter: 73 loss: 1.16918818e-06
Iter: 74 loss: 1.16571027e-06
Iter: 75 loss: 1.14363024e-06
Iter: 76 loss: 1.16466549e-06
Iter: 77 loss: 1.13102965e-06
Iter: 78 loss: 1.10963447e-06
Iter: 79 loss: 1.12908742e-06
Iter: 80 loss: 1.09720554e-06
Iter: 81 loss: 1.10456222e-06
Iter: 82 loss: 1.08677739e-06
Iter: 83 loss: 1.07681228e-06
Iter: 84 loss: 1.04898595e-06
Iter: 85 loss: 1.19604726e-06
Iter: 86 loss: 1.04015442e-06
Iter: 87 loss: 1.01284786e-06
Iter: 88 loss: 1.18970411e-06
Iter: 89 loss: 1.00983584e-06
Iter: 90 loss: 9.98275937e-07
Iter: 91 loss: 9.96800281e-07
Iter: 92 loss: 9.86889e-07
Iter: 93 loss: 9.71991881e-07
Iter: 94 loss: 9.71674353e-07
Iter: 95 loss: 9.63386924e-07
Iter: 96 loss: 9.63158527e-07
Iter: 97 loss: 9.54818574e-07
Iter: 98 loss: 9.41218445e-07
Iter: 99 loss: 9.41121698e-07
Iter: 100 loss: 9.30738e-07
Iter: 101 loss: 9.36204799e-07
Iter: 102 loss: 9.23882112e-07
Iter: 103 loss: 9.13374038e-07
Iter: 104 loss: 9.13034228e-07
Iter: 105 loss: 9.0403023e-07
Iter: 106 loss: 9.06542937e-07
Iter: 107 loss: 8.97539735e-07
Iter: 108 loss: 8.87706051e-07
Iter: 109 loss: 8.87332817e-07
Iter: 110 loss: 8.7973666e-07
Iter: 111 loss: 8.66503115e-07
Iter: 112 loss: 9.17834086e-07
Iter: 113 loss: 8.63411969e-07
Iter: 114 loss: 8.54648306e-07
Iter: 115 loss: 8.71899374e-07
Iter: 116 loss: 8.51030336e-07
Iter: 117 loss: 8.51058189e-07
Iter: 118 loss: 8.46902253e-07
Iter: 119 loss: 8.44566557e-07
Iter: 120 loss: 8.37312086e-07
Iter: 121 loss: 8.52193409e-07
Iter: 122 loss: 8.32834075e-07
Iter: 123 loss: 8.22396089e-07
Iter: 124 loss: 8.66144603e-07
Iter: 125 loss: 8.20214e-07
Iter: 126 loss: 8.14214218e-07
Iter: 127 loss: 8.13463487e-07
Iter: 128 loss: 8.08633672e-07
Iter: 129 loss: 8.00699581e-07
Iter: 130 loss: 8.006607e-07
Iter: 131 loss: 7.94692824e-07
Iter: 132 loss: 8.60669161e-07
Iter: 133 loss: 7.9455333e-07
Iter: 134 loss: 7.88436353e-07
Iter: 135 loss: 7.95375854e-07
Iter: 136 loss: 7.85131135e-07
Iter: 137 loss: 7.81141466e-07
Iter: 138 loss: 7.75095145e-07
Iter: 139 loss: 7.7499277e-07
Iter: 140 loss: 7.74472e-07
Iter: 141 loss: 7.71763439e-07
Iter: 142 loss: 7.68425878e-07
Iter: 143 loss: 7.61255706e-07
Iter: 144 loss: 8.73276917e-07
Iter: 145 loss: 7.6099559e-07
Iter: 146 loss: 7.5360515e-07
Iter: 147 loss: 7.58757892e-07
Iter: 148 loss: 7.48968432e-07
Iter: 149 loss: 7.40429755e-07
Iter: 150 loss: 7.63101866e-07
Iter: 151 loss: 7.37565188e-07
Iter: 152 loss: 7.30413717e-07
Iter: 153 loss: 7.88215061e-07
Iter: 154 loss: 7.29929809e-07
Iter: 155 loss: 7.27374697e-07
Iter: 156 loss: 7.26764256e-07
Iter: 157 loss: 7.24363076e-07
Iter: 158 loss: 7.2203261e-07
Iter: 159 loss: 7.21472929e-07
Iter: 160 loss: 7.17651574e-07
Iter: 161 loss: 7.18207389e-07
Iter: 162 loss: 7.14779844e-07
Iter: 163 loss: 7.10961444e-07
Iter: 164 loss: 7.35526385e-07
Iter: 165 loss: 7.10537847e-07
Iter: 166 loss: 7.06120545e-07
Iter: 167 loss: 7.23260143e-07
Iter: 168 loss: 7.05092816e-07
Iter: 169 loss: 7.02088812e-07
Iter: 170 loss: 6.97175096e-07
Iter: 171 loss: 6.97137921e-07
Iter: 172 loss: 6.95080871e-07
Iter: 173 loss: 6.94314281e-07
Iter: 174 loss: 6.92072263e-07
Iter: 175 loss: 6.90691422e-07
Iter: 176 loss: 6.89791477e-07
Iter: 177 loss: 6.8679185e-07
Iter: 178 loss: 6.83801773e-07
Iter: 179 loss: 6.83175585e-07
Iter: 180 loss: 6.81858069e-07
Iter: 181 loss: 6.80326e-07
Iter: 182 loss: 6.79019422e-07
Iter: 183 loss: 6.74826197e-07
Iter: 184 loss: 6.79284483e-07
Iter: 185 loss: 6.71547809e-07
Iter: 186 loss: 6.66042581e-07
Iter: 187 loss: 7.30673207e-07
Iter: 188 loss: 6.65969537e-07
Iter: 189 loss: 6.63322908e-07
Iter: 190 loss: 6.63115e-07
Iter: 191 loss: 6.60827311e-07
Iter: 192 loss: 6.58272e-07
Iter: 193 loss: 6.57940404e-07
Iter: 194 loss: 6.5568e-07
Iter: 195 loss: 6.62554385e-07
Iter: 196 loss: 6.55039173e-07
Iter: 197 loss: 6.52508845e-07
Iter: 198 loss: 6.62400566e-07
Iter: 199 loss: 6.5190352e-07
Iter: 200 loss: 6.49254787e-07
Iter: 201 loss: 6.61000286e-07
Iter: 202 loss: 6.48726598e-07
Iter: 203 loss: 6.46998046e-07
Iter: 204 loss: 6.43366548e-07
Iter: 205 loss: 7.03222327e-07
Iter: 206 loss: 6.43254e-07
Iter: 207 loss: 6.42015948e-07
Iter: 208 loss: 6.41216e-07
Iter: 209 loss: 6.39394727e-07
Iter: 210 loss: 6.35716674e-07
Iter: 211 loss: 7.01518559e-07
Iter: 212 loss: 6.35634535e-07
Iter: 213 loss: 6.33362902e-07
Iter: 214 loss: 6.55952135e-07
Iter: 215 loss: 6.33296509e-07
Iter: 216 loss: 6.31138846e-07
Iter: 217 loss: 6.46626518e-07
Iter: 218 loss: 6.30964507e-07
Iter: 219 loss: 6.29435e-07
Iter: 220 loss: 6.26766621e-07
Iter: 221 loss: 6.26768554e-07
Iter: 222 loss: 6.2403825e-07
Iter: 223 loss: 6.24086852e-07
Iter: 224 loss: 6.21853701e-07
Iter: 225 loss: 6.2164662e-07
Iter: 226 loss: 6.20180458e-07
Iter: 227 loss: 6.18456909e-07
Iter: 228 loss: 6.16170041e-07
Iter: 229 loss: 6.16045327e-07
Iter: 230 loss: 6.13782845e-07
Iter: 231 loss: 6.1277035e-07
Iter: 232 loss: 6.11654968e-07
Iter: 233 loss: 6.10651455e-07
Iter: 234 loss: 6.09957283e-07
Iter: 235 loss: 6.08659491e-07
Iter: 236 loss: 6.07966513e-07
Iter: 237 loss: 6.07384436e-07
Iter: 238 loss: 6.05980063e-07
Iter: 239 loss: 6.096775e-07
Iter: 240 loss: 6.05525656e-07
Iter: 241 loss: 6.04106845e-07
Iter: 242 loss: 6.09839049e-07
Iter: 243 loss: 6.03802278e-07
Iter: 244 loss: 6.02255e-07
Iter: 245 loss: 6.06318736e-07
Iter: 246 loss: 6.01741817e-07
Iter: 247 loss: 6.00304361e-07
Iter: 248 loss: 5.98071e-07
Iter: 249 loss: 5.98051884e-07
Iter: 250 loss: 5.96680138e-07
Iter: 251 loss: 5.96372388e-07
Iter: 252 loss: 5.94873086e-07
Iter: 253 loss: 5.92217134e-07
Iter: 254 loss: 5.92214576e-07
Iter: 255 loss: 5.90172249e-07
Iter: 256 loss: 5.9305404e-07
Iter: 257 loss: 5.89166632e-07
Iter: 258 loss: 5.88323246e-07
Iter: 259 loss: 5.88112812e-07
Iter: 260 loss: 5.87098612e-07
Iter: 261 loss: 5.89960052e-07
Iter: 262 loss: 5.86788815e-07
Iter: 263 loss: 5.86183546e-07
Iter: 264 loss: 5.84402869e-07
Iter: 265 loss: 5.90033e-07
Iter: 266 loss: 5.83522706e-07
Iter: 267 loss: 5.83784868e-07
Iter: 268 loss: 5.82513678e-07
Iter: 269 loss: 5.81390225e-07
Iter: 270 loss: 5.79257971e-07
Iter: 271 loss: 6.25722919e-07
Iter: 272 loss: 5.79245e-07
Iter: 273 loss: 5.77062053e-07
Iter: 274 loss: 5.78314371e-07
Iter: 275 loss: 5.75618742e-07
Iter: 276 loss: 5.73344209e-07
Iter: 277 loss: 5.73341481e-07
Iter: 278 loss: 5.71752366e-07
Iter: 279 loss: 5.75116e-07
Iter: 280 loss: 5.71146757e-07
Iter: 281 loss: 5.70206794e-07
Iter: 282 loss: 5.73053626e-07
Iter: 283 loss: 5.69928716e-07
Iter: 284 loss: 5.6902843e-07
Iter: 285 loss: 5.74298724e-07
Iter: 286 loss: 5.68895643e-07
Iter: 287 loss: 5.67931124e-07
Iter: 288 loss: 5.6794596e-07
Iter: 289 loss: 5.67166694e-07
Iter: 290 loss: 5.66238384e-07
Iter: 291 loss: 5.64510401e-07
Iter: 292 loss: 6.03500666e-07
Iter: 293 loss: 5.64509378e-07
Iter: 294 loss: 5.65166943e-07
Iter: 295 loss: 5.63760182e-07
Iter: 296 loss: 5.62982621e-07
Iter: 297 loss: 5.61027264e-07
Iter: 298 loss: 5.77277262e-07
Iter: 299 loss: 5.60716728e-07
Iter: 300 loss: 5.58797637e-07
Iter: 301 loss: 5.64681272e-07
Iter: 302 loss: 5.58230909e-07
Iter: 303 loss: 5.56967166e-07
Iter: 304 loss: 5.64922175e-07
Iter: 305 loss: 5.56816303e-07
Iter: 306 loss: 5.56337113e-07
Iter: 307 loss: 5.56220073e-07
Iter: 308 loss: 5.55817564e-07
Iter: 309 loss: 5.54792848e-07
Iter: 310 loss: 5.61684715e-07
Iter: 311 loss: 5.54573944e-07
Iter: 312 loss: 5.53341579e-07
Iter: 313 loss: 5.59993396e-07
Iter: 314 loss: 5.53164227e-07
Iter: 315 loss: 5.51706762e-07
Iter: 316 loss: 5.58551392e-07
Iter: 317 loss: 5.51441133e-07
Iter: 318 loss: 5.50467803e-07
Iter: 319 loss: 5.49256583e-07
Iter: 320 loss: 5.49162451e-07
Iter: 321 loss: 5.48026094e-07
Iter: 322 loss: 5.4795e-07
Iter: 323 loss: 5.4721e-07
Iter: 324 loss: 5.48696732e-07
Iter: 325 loss: 5.46904e-07
Iter: 326 loss: 5.46297e-07
Iter: 327 loss: 5.45844614e-07
Iter: 328 loss: 5.45649073e-07
Iter: 329 loss: 5.44777095e-07
Iter: 330 loss: 5.48094249e-07
Iter: 331 loss: 5.44561885e-07
Iter: 332 loss: 5.44060072e-07
Iter: 333 loss: 5.44005502e-07
Iter: 334 loss: 5.43598787e-07
Iter: 335 loss: 5.42405871e-07
Iter: 336 loss: 5.47590957e-07
Iter: 337 loss: 5.4195948e-07
Iter: 338 loss: 5.40199892e-07
Iter: 339 loss: 5.41469717e-07
Iter: 340 loss: 5.39087637e-07
Iter: 341 loss: 5.37549397e-07
Iter: 342 loss: 5.47989316e-07
Iter: 343 loss: 5.37381538e-07
Iter: 344 loss: 5.37083395e-07
Iter: 345 loss: 5.36748075e-07
Iter: 346 loss: 5.36261609e-07
Iter: 347 loss: 5.35923562e-07
Iter: 348 loss: 5.35750814e-07
Iter: 349 loss: 5.35167487e-07
Iter: 350 loss: 5.34459502e-07
Iter: 351 loss: 5.34384412e-07
Iter: 352 loss: 5.34157e-07
Iter: 353 loss: 5.33854291e-07
Iter: 354 loss: 5.33492653e-07
Iter: 355 loss: 5.32443096e-07
Iter: 356 loss: 5.37756875e-07
Iter: 357 loss: 5.32082822e-07
Iter: 358 loss: 5.31497335e-07
Iter: 359 loss: 5.3129736e-07
Iter: 360 loss: 5.30558964e-07
Iter: 361 loss: 5.3004112e-07
Iter: 362 loss: 5.2981386e-07
Iter: 363 loss: 5.28893736e-07
Iter: 364 loss: 5.32359195e-07
Iter: 365 loss: 5.28660166e-07
Iter: 366 loss: 5.28119585e-07
Iter: 367 loss: 5.36243192e-07
Iter: 368 loss: 5.28123337e-07
Iter: 369 loss: 5.27567522e-07
Iter: 370 loss: 5.27218333e-07
Iter: 371 loss: 5.27017505e-07
Iter: 372 loss: 5.26489714e-07
Iter: 373 loss: 5.26769099e-07
Iter: 374 loss: 5.26185147e-07
Iter: 375 loss: 5.25493931e-07
Iter: 376 loss: 5.24992629e-07
Iter: 377 loss: 5.24772304e-07
Iter: 378 loss: 5.2468431e-07
Iter: 379 loss: 5.24240249e-07
Iter: 380 loss: 5.23833e-07
Iter: 381 loss: 5.22816435e-07
Iter: 382 loss: 5.33018635e-07
Iter: 383 loss: 5.2269553e-07
Iter: 384 loss: 5.21761308e-07
Iter: 385 loss: 5.27375732e-07
Iter: 386 loss: 5.21646484e-07
Iter: 387 loss: 5.21024731e-07
Iter: 388 loss: 5.21033485e-07
Iter: 389 loss: 5.20620119e-07
Iter: 390 loss: 5.2008761e-07
Iter: 391 loss: 5.20059871e-07
Iter: 392 loss: 5.19744447e-07
Iter: 393 loss: 5.19704713e-07
Iter: 394 loss: 5.19373316e-07
Iter: 395 loss: 5.18751733e-07
Iter: 396 loss: 5.32776539e-07
Iter: 397 loss: 5.18755883e-07
Iter: 398 loss: 5.18176194e-07
Iter: 399 loss: 5.20804178e-07
Iter: 400 loss: 5.1806e-07
Iter: 401 loss: 5.17454851e-07
Iter: 402 loss: 5.22326218e-07
Iter: 403 loss: 5.17411877e-07
Iter: 404 loss: 5.16886928e-07
Iter: 405 loss: 5.16219302e-07
Iter: 406 loss: 5.16156547e-07
Iter: 407 loss: 5.15278771e-07
Iter: 408 loss: 5.153845e-07
Iter: 409 loss: 5.14599037e-07
Iter: 410 loss: 5.13655323e-07
Iter: 411 loss: 5.17715705e-07
Iter: 412 loss: 5.13460122e-07
Iter: 413 loss: 5.13133898e-07
Iter: 414 loss: 5.13064492e-07
Iter: 415 loss: 5.12652434e-07
Iter: 416 loss: 5.12207748e-07
Iter: 417 loss: 5.12131066e-07
Iter: 418 loss: 5.11611915e-07
Iter: 419 loss: 5.11432631e-07
Iter: 420 loss: 5.11127098e-07
Iter: 421 loss: 5.10820769e-07
Iter: 422 loss: 5.1073647e-07
Iter: 423 loss: 5.10340783e-07
Iter: 424 loss: 5.09529457e-07
Iter: 425 loss: 5.24216603e-07
Iter: 426 loss: 5.09527752e-07
Iter: 427 loss: 5.0900212e-07
Iter: 428 loss: 5.09003712e-07
Iter: 429 loss: 5.08432663e-07
Iter: 430 loss: 5.07822278e-07
Iter: 431 loss: 5.07760603e-07
Iter: 432 loss: 5.07070126e-07
Iter: 433 loss: 5.07967457e-07
Iter: 434 loss: 5.06721335e-07
Iter: 435 loss: 5.06663753e-07
Iter: 436 loss: 5.06441495e-07
Iter: 437 loss: 5.06201332e-07
Iter: 438 loss: 5.05791263e-07
Iter: 439 loss: 5.05781088e-07
Iter: 440 loss: 5.05460548e-07
Iter: 441 loss: 5.0593485e-07
Iter: 442 loss: 5.05281264e-07
Iter: 443 loss: 5.04788375e-07
Iter: 444 loss: 5.04594823e-07
Iter: 445 loss: 5.04310833e-07
Iter: 446 loss: 5.03630645e-07
Iter: 447 loss: 5.04454306e-07
Iter: 448 loss: 5.03283218e-07
Iter: 449 loss: 5.03454942e-07
Iter: 450 loss: 5.03025149e-07
Iter: 451 loss: 5.02793284e-07
Iter: 452 loss: 5.0224844e-07
Iter: 453 loss: 5.09345284e-07
Iter: 454 loss: 5.02231785e-07
Iter: 455 loss: 5.01689e-07
Iter: 456 loss: 5.02974387e-07
Iter: 457 loss: 5.01520219e-07
Iter: 458 loss: 5.00979183e-07
Iter: 459 loss: 5.01419436e-07
Iter: 460 loss: 5.00646365e-07
Iter: 461 loss: 5.00296437e-07
Iter: 462 loss: 5.0021697e-07
Iter: 463 loss: 4.99945713e-07
Iter: 464 loss: 4.99273256e-07
Iter: 465 loss: 5.05371077e-07
Iter: 466 loss: 4.9917e-07
Iter: 467 loss: 4.988716e-07
Iter: 468 loss: 4.98716872e-07
Iter: 469 loss: 4.98414465e-07
Iter: 470 loss: 4.97884457e-07
Iter: 471 loss: 4.97878773e-07
Iter: 472 loss: 4.97336373e-07
Iter: 473 loss: 4.97936412e-07
Iter: 474 loss: 4.97036581e-07
Iter: 475 loss: 4.96920677e-07
Iter: 476 loss: 4.96731388e-07
Iter: 477 loss: 4.9656e-07
Iter: 478 loss: 4.9609e-07
Iter: 479 loss: 5.0013881e-07
Iter: 480 loss: 4.96032328e-07
Iter: 481 loss: 4.95715881e-07
Iter: 482 loss: 4.95713721e-07
Iter: 483 loss: 4.9539733e-07
Iter: 484 loss: 4.95433824e-07
Iter: 485 loss: 4.951645e-07
Iter: 486 loss: 4.94783e-07
Iter: 487 loss: 4.95451332e-07
Iter: 488 loss: 4.94607548e-07
Iter: 489 loss: 4.94182e-07
Iter: 490 loss: 4.93756488e-07
Iter: 491 loss: 4.93665311e-07
Iter: 492 loss: 4.93184814e-07
Iter: 493 loss: 4.93186917e-07
Iter: 494 loss: 4.92884283e-07
Iter: 495 loss: 4.96302619e-07
Iter: 496 loss: 4.92880758e-07
Iter: 497 loss: 4.92672712e-07
Iter: 498 loss: 4.92245761e-07
Iter: 499 loss: 4.99216526e-07
Iter: 500 loss: 4.92229788e-07
Iter: 501 loss: 4.91909418e-07
Iter: 502 loss: 4.9188327e-07
Iter: 503 loss: 4.91610763e-07
Iter: 504 loss: 4.91090532e-07
Iter: 505 loss: 5.02751e-07
Iter: 506 loss: 4.91096273e-07
Iter: 507 loss: 4.90563423e-07
Iter: 508 loss: 4.91510832e-07
Iter: 509 loss: 4.9035981e-07
Iter: 510 loss: 4.89895569e-07
Iter: 511 loss: 4.89883121e-07
Iter: 512 loss: 4.89701165e-07
Iter: 513 loss: 4.89280353e-07
Iter: 514 loss: 4.92975175e-07
Iter: 515 loss: 4.89205149e-07
Iter: 516 loss: 4.88684577e-07
Iter: 517 loss: 4.91342348e-07
Iter: 518 loss: 4.88606361e-07
Iter: 519 loss: 4.88452e-07
Iter: 520 loss: 4.88378589e-07
Iter: 521 loss: 4.88243415e-07
Iter: 522 loss: 4.87899285e-07
Iter: 523 loss: 4.92028278e-07
Iter: 524 loss: 4.87859779e-07
Iter: 525 loss: 4.87388263e-07
Iter: 526 loss: 4.88820547e-07
Iter: 527 loss: 4.87259513e-07
Iter: 528 loss: 4.86781175e-07
Iter: 529 loss: 4.86653e-07
Iter: 530 loss: 4.86377076e-07
Iter: 531 loss: 4.85942337e-07
Iter: 532 loss: 4.85921078e-07
Iter: 533 loss: 4.85524765e-07
Iter: 534 loss: 4.86970748e-07
Iter: 535 loss: 4.85410851e-07
Iter: 536 loss: 4.85166083e-07
Iter: 537 loss: 4.85023e-07
Iter: 538 loss: 4.84935583e-07
Iter: 539 loss: 4.84671034e-07
Iter: 540 loss: 4.84658585e-07
Iter: 541 loss: 4.8445645e-07
Iter: 542 loss: 4.8403615e-07
Iter: 543 loss: 4.90915284e-07
Iter: 544 loss: 4.84038821e-07
Iter: 545 loss: 4.83630629e-07
Iter: 546 loss: 4.84061104e-07
Iter: 547 loss: 4.83412919e-07
Iter: 548 loss: 4.83208282e-07
Iter: 549 loss: 4.83134556e-07
Iter: 550 loss: 4.82864209e-07
Iter: 551 loss: 4.82371092e-07
Iter: 552 loss: 4.93545e-07
Iter: 553 loss: 4.82355063e-07
Iter: 554 loss: 4.82018891e-07
Iter: 555 loss: 4.84076736e-07
Iter: 556 loss: 4.81961649e-07
Iter: 557 loss: 4.81613142e-07
Iter: 558 loss: 4.84015288e-07
Iter: 559 loss: 4.8157824e-07
Iter: 560 loss: 4.8138412e-07
Iter: 561 loss: 4.81212567e-07
Iter: 562 loss: 4.81147481e-07
Iter: 563 loss: 4.80880885e-07
Iter: 564 loss: 4.8146785e-07
Iter: 565 loss: 4.80786298e-07
Iter: 566 loss: 4.8044086e-07
Iter: 567 loss: 4.80462802e-07
Iter: 568 loss: 4.80152949e-07
Iter: 569 loss: 4.79820585e-07
Iter: 570 loss: 4.84621069e-07
Iter: 571 loss: 4.79836103e-07
Iter: 572 loss: 4.79464802e-07
Iter: 573 loss: 4.797418e-07
Iter: 574 loss: 4.79241407e-07
Iter: 575 loss: 4.78883749e-07
Iter: 576 loss: 4.78720949e-07
Iter: 577 loss: 4.78531661e-07
Iter: 578 loss: 4.78371078e-07
Iter: 579 loss: 4.7827757e-07
Iter: 580 loss: 4.78095785e-07
Iter: 581 loss: 4.77930882e-07
Iter: 582 loss: 4.77871765e-07
Iter: 583 loss: 4.77665083e-07
Iter: 584 loss: 4.77822596e-07
Iter: 585 loss: 4.77544916e-07
Iter: 586 loss: 4.77436629e-07
Iter: 587 loss: 4.77398544e-07
Iter: 588 loss: 4.77309e-07
Iter: 589 loss: 4.77033552e-07
Iter: 590 loss: 4.77383537e-07
Iter: 591 loss: 4.76815984e-07
Iter: 592 loss: 4.76348674e-07
Iter: 593 loss: 4.79461903e-07
Iter: 594 loss: 4.7630607e-07
Iter: 595 loss: 4.76005425e-07
Iter: 596 loss: 4.76005653e-07
Iter: 597 loss: 4.75730076e-07
Iter: 598 loss: 4.75375629e-07
Iter: 599 loss: 4.75357893e-07
Iter: 600 loss: 4.74960842e-07
Iter: 601 loss: 4.7668874e-07
Iter: 602 loss: 4.74886917e-07
Iter: 603 loss: 4.74618133e-07
Iter: 604 loss: 4.75139586e-07
Iter: 605 loss: 4.74491287e-07
Iter: 606 loss: 4.74218837e-07
Iter: 607 loss: 4.7810363e-07
Iter: 608 loss: 4.74218098e-07
Iter: 609 loss: 4.74036881e-07
Iter: 610 loss: 4.73868056e-07
Iter: 611 loss: 4.73831562e-07
Iter: 612 loss: 4.73620133e-07
Iter: 613 loss: 4.75814318e-07
Iter: 614 loss: 4.73608708e-07
Iter: 615 loss: 4.73351719e-07
Iter: 616 loss: 4.73095724e-07
Iter: 617 loss: 4.73044736e-07
Iter: 618 loss: 4.72682217e-07
Iter: 619 loss: 4.72969958e-07
Iter: 620 loss: 4.72464137e-07
Iter: 621 loss: 4.72300172e-07
Iter: 622 loss: 4.72216811e-07
Iter: 623 loss: 4.72073168e-07
Iter: 624 loss: 4.71766981e-07
Iter: 625 loss: 4.76444399e-07
Iter: 626 loss: 4.71766697e-07
Iter: 627 loss: 4.71499476e-07
Iter: 628 loss: 4.72529138e-07
Iter: 629 loss: 4.71438767e-07
Iter: 630 loss: 4.71194198e-07
Iter: 631 loss: 4.73930356e-07
Iter: 632 loss: 4.71179447e-07
Iter: 633 loss: 4.71030432e-07
Iter: 634 loss: 4.70790184e-07
Iter: 635 loss: 4.70782766e-07
Iter: 636 loss: 4.70444888e-07
Iter: 637 loss: 4.71066357e-07
Iter: 638 loss: 4.70298062e-07
Iter: 639 loss: 4.69960753e-07
Iter: 640 loss: 4.71191072e-07
Iter: 641 loss: 4.69890892e-07
Iter: 642 loss: 4.69488896e-07
Iter: 643 loss: 4.71507491e-07
Iter: 644 loss: 4.69435e-07
Iter: 645 loss: 4.6910273e-07
Iter: 646 loss: 4.69273402e-07
Iter: 647 loss: 4.68870155e-07
Iter: 648 loss: 4.68688313e-07
Iter: 649 loss: 4.68674415e-07
Iter: 650 loss: 4.68562291e-07
Iter: 651 loss: 4.68306126e-07
Iter: 652 loss: 4.73112436e-07
Iter: 653 loss: 4.68305728e-07
Iter: 654 loss: 4.68143753e-07
Iter: 655 loss: 4.68147846e-07
Iter: 656 loss: 4.67992095e-07
Iter: 657 loss: 4.68256104e-07
Iter: 658 loss: 4.67917772e-07
Iter: 659 loss: 4.6775591e-07
Iter: 660 loss: 4.67474166e-07
Iter: 661 loss: 4.73739362e-07
Iter: 662 loss: 4.67472773e-07
Iter: 663 loss: 4.67245684e-07
Iter: 664 loss: 4.67235083e-07
Iter: 665 loss: 4.66951064e-07
Iter: 666 loss: 4.66968316e-07
Iter: 667 loss: 4.66735059e-07
Iter: 668 loss: 4.6647483e-07
Iter: 669 loss: 4.66546169e-07
Iter: 670 loss: 4.66299184e-07
Iter: 671 loss: 4.65962842e-07
Iter: 672 loss: 4.68027736e-07
Iter: 673 loss: 4.65939877e-07
Iter: 674 loss: 4.65744108e-07
Iter: 675 loss: 4.67186169e-07
Iter: 676 loss: 4.6574155e-07
Iter: 677 loss: 4.65583469e-07
Iter: 678 loss: 4.66101596e-07
Iter: 679 loss: 4.65544474e-07
Iter: 680 loss: 4.65413905e-07
Iter: 681 loss: 4.6545847e-07
Iter: 682 loss: 4.65332448e-07
Iter: 683 loss: 4.65143955e-07
Iter: 684 loss: 4.66103131e-07
Iter: 685 loss: 4.65109508e-07
Iter: 686 loss: 4.64953303e-07
Iter: 687 loss: 4.64680596e-07
Iter: 688 loss: 4.70175081e-07
Iter: 689 loss: 4.64665618e-07
Iter: 690 loss: 4.64547725e-07
Iter: 691 loss: 4.64477523e-07
Iter: 692 loss: 4.6434053e-07
Iter: 693 loss: 4.63978921e-07
Iter: 694 loss: 4.67946506e-07
Iter: 695 loss: 4.63952176e-07
Iter: 696 loss: 4.63625156e-07
Iter: 697 loss: 4.65143785e-07
Iter: 698 loss: 4.63565385e-07
Iter: 699 loss: 4.63394031e-07
Iter: 700 loss: 4.6336541e-07
Iter: 701 loss: 4.63276336e-07
Iter: 702 loss: 4.63060928e-07
Iter: 703 loss: 4.63055756e-07
Iter: 704 loss: 4.62871498e-07
Iter: 705 loss: 4.63383714e-07
Iter: 706 loss: 4.62816189e-07
Iter: 707 loss: 4.62597257e-07
Iter: 708 loss: 4.63745721e-07
Iter: 709 loss: 4.62566504e-07
Iter: 710 loss: 4.62349135e-07
Iter: 711 loss: 4.63043733e-07
Iter: 712 loss: 4.62311618e-07
Iter: 713 loss: 4.62072592e-07
Iter: 714 loss: 4.62107607e-07
Iter: 715 loss: 4.61904932e-07
Iter: 716 loss: 4.61726074e-07
Iter: 717 loss: 4.6172039e-07
Iter: 718 loss: 4.61579589e-07
Iter: 719 loss: 4.61310037e-07
Iter: 720 loss: 4.61308957e-07
Iter: 721 loss: 4.6111964e-07
Iter: 722 loss: 4.63868957e-07
Iter: 723 loss: 4.6111893e-07
Iter: 724 loss: 4.60925463e-07
Iter: 725 loss: 4.61393512e-07
Iter: 726 loss: 4.60843864e-07
Iter: 727 loss: 4.60755075e-07
Iter: 728 loss: 4.60524262e-07
Iter: 729 loss: 4.65451819e-07
Iter: 730 loss: 4.60520681e-07
Iter: 731 loss: 4.603761e-07
Iter: 732 loss: 4.60370245e-07
Iter: 733 loss: 4.60185277e-07
Iter: 734 loss: 4.60127751e-07
Iter: 735 loss: 4.60040042e-07
Iter: 736 loss: 4.59828385e-07
Iter: 737 loss: 4.59652938e-07
Iter: 738 loss: 4.59591888e-07
Iter: 739 loss: 4.59286468e-07
Iter: 740 loss: 4.62516908e-07
Iter: 741 loss: 4.59265038e-07
Iter: 742 loss: 4.59038233e-07
Iter: 743 loss: 4.60162198e-07
Iter: 744 loss: 4.5899867e-07
Iter: 745 loss: 4.58818135e-07
Iter: 746 loss: 4.59400326e-07
Iter: 747 loss: 4.58756233e-07
Iter: 748 loss: 4.58587465e-07
Iter: 749 loss: 4.58762258e-07
Iter: 750 loss: 4.584831e-07
Iter: 751 loss: 4.58281079e-07
Iter: 752 loss: 4.60119935e-07
Iter: 753 loss: 4.58263287e-07
Iter: 754 loss: 4.58172508e-07
Iter: 755 loss: 4.57933083e-07
Iter: 756 loss: 4.61277352e-07
Iter: 757 loss: 4.57929559e-07
Iter: 758 loss: 4.57786683e-07
Iter: 759 loss: 4.57756073e-07
Iter: 760 loss: 4.57640539e-07
Iter: 761 loss: 4.57345777e-07
Iter: 762 loss: 4.60693684e-07
Iter: 763 loss: 4.57319544e-07
Iter: 764 loss: 4.57028079e-07
Iter: 765 loss: 4.57210433e-07
Iter: 766 loss: 4.56846834e-07
Iter: 767 loss: 4.56728401e-07
Iter: 768 loss: 4.56663855e-07
Iter: 769 loss: 4.56560485e-07
Iter: 770 loss: 4.56332373e-07
Iter: 771 loss: 4.58506292e-07
Iter: 772 loss: 4.56298096e-07
Iter: 773 loss: 4.56041562e-07
Iter: 774 loss: 4.56576231e-07
Iter: 775 loss: 4.55918439e-07
Iter: 776 loss: 4.5578139e-07
Iter: 777 loss: 4.55754105e-07
Iter: 778 loss: 4.5564488e-07
Iter: 779 loss: 4.55854206e-07
Iter: 780 loss: 4.55568e-07
Iter: 781 loss: 4.55409605e-07
Iter: 782 loss: 4.55603413e-07
Iter: 783 loss: 4.55330337e-07
Iter: 784 loss: 4.55118425e-07
Iter: 785 loss: 4.55680492e-07
Iter: 786 loss: 4.55060217e-07
Iter: 787 loss: 4.54834236e-07
Iter: 788 loss: 4.55322521e-07
Iter: 789 loss: 4.54735755e-07
Iter: 790 loss: 4.54593334e-07
Iter: 791 loss: 4.54718304e-07
Iter: 792 loss: 4.54501588e-07
Iter: 793 loss: 4.54256451e-07
Iter: 794 loss: 4.55790712e-07
Iter: 795 loss: 4.54227148e-07
Iter: 796 loss: 4.54114968e-07
Iter: 797 loss: 4.53940345e-07
Iter: 798 loss: 4.53938355e-07
Iter: 799 loss: 4.53775186e-07
Iter: 800 loss: 4.5377783e-07
Iter: 801 loss: 4.53612358e-07
Iter: 802 loss: 4.53701404e-07
Iter: 803 loss: 4.53502935e-07
Iter: 804 loss: 4.53348292e-07
Iter: 805 loss: 4.53042844e-07
Iter: 806 loss: 4.59976434e-07
Iter: 807 loss: 4.53042645e-07
Iter: 808 loss: 4.52688852e-07
Iter: 809 loss: 4.53965e-07
Iter: 810 loss: 4.52577098e-07
Iter: 811 loss: 4.52345375e-07
Iter: 812 loss: 4.55937624e-07
Iter: 813 loss: 4.52342249e-07
Iter: 814 loss: 4.52154808e-07
Iter: 815 loss: 4.53276527e-07
Iter: 816 loss: 4.52135907e-07
Iter: 817 loss: 4.52001018e-07
Iter: 818 loss: 4.51993202e-07
Iter: 819 loss: 4.51892504e-07
Iter: 820 loss: 4.51726947e-07
Iter: 821 loss: 4.53635664e-07
Iter: 822 loss: 4.51731e-07
Iter: 823 loss: 4.51609822e-07
Iter: 824 loss: 4.51540018e-07
Iter: 825 loss: 4.51503553e-07
Iter: 826 loss: 4.51315771e-07
Iter: 827 loss: 4.51966059e-07
Iter: 828 loss: 4.51261144e-07
Iter: 829 loss: 4.51066256e-07
Iter: 830 loss: 4.51900348e-07
Iter: 831 loss: 4.51012568e-07
Iter: 832 loss: 4.5088683e-07
Iter: 833 loss: 4.5060915e-07
Iter: 834 loss: 4.5619953e-07
Iter: 835 loss: 4.5060051e-07
Iter: 836 loss: 4.50478694e-07
Iter: 837 loss: 4.50449306e-07
Iter: 838 loss: 4.50302849e-07
Iter: 839 loss: 4.50147581e-07
Iter: 840 loss: 4.50140845e-07
Iter: 841 loss: 4.49968269e-07
Iter: 842 loss: 4.49937659e-07
Iter: 843 loss: 4.49837444e-07
Iter: 844 loss: 4.49601544e-07
Iter: 845 loss: 4.49836421e-07
Iter: 846 loss: 4.49460117e-07
Iter: 847 loss: 4.49177605e-07
Iter: 848 loss: 4.50498419e-07
Iter: 849 loss: 4.49123547e-07
Iter: 850 loss: 4.48987066e-07
Iter: 851 loss: 4.48961316e-07
Iter: 852 loss: 4.48876449e-07
Iter: 853 loss: 4.48630772e-07
Iter: 854 loss: 4.5095851e-07
Iter: 855 loss: 4.48612724e-07
Iter: 856 loss: 4.48440915e-07
Iter: 857 loss: 4.48424544e-07
Iter: 858 loss: 4.48276353e-07
Iter: 859 loss: 4.48218941e-07
Iter: 860 loss: 4.4815539e-07
Iter: 861 loss: 4.47976447e-07
Iter: 862 loss: 4.48517881e-07
Iter: 863 loss: 4.47925743e-07
Iter: 864 loss: 4.47772663e-07
Iter: 865 loss: 4.49373061e-07
Iter: 866 loss: 4.47771811e-07
Iter: 867 loss: 4.476301e-07
Iter: 868 loss: 4.47578088e-07
Iter: 869 loss: 4.47510644e-07
Iter: 870 loss: 4.47341847e-07
Iter: 871 loss: 4.47773118e-07
Iter: 872 loss: 4.4727e-07
Iter: 873 loss: 4.47039923e-07
Iter: 874 loss: 4.47756349e-07
Iter: 875 loss: 4.46958865e-07
Iter: 876 loss: 4.46796605e-07
Iter: 877 loss: 4.4657412e-07
Iter: 878 loss: 4.46569118e-07
Iter: 879 loss: 4.46277625e-07
Iter: 880 loss: 4.46854358e-07
Iter: 881 loss: 4.4617741e-07
Iter: 882 loss: 4.45967828e-07
Iter: 883 loss: 4.48666412e-07
Iter: 884 loss: 4.45970727e-07
Iter: 885 loss: 4.45808979e-07
Iter: 886 loss: 4.47532557e-07
Iter: 887 loss: 4.45799344e-07
Iter: 888 loss: 4.45724879e-07
Iter: 889 loss: 4.45522744e-07
Iter: 890 loss: 4.48230537e-07
Iter: 891 loss: 4.45505833e-07
Iter: 892 loss: 4.45351702e-07
Iter: 893 loss: 4.45343403e-07
Iter: 894 loss: 4.45193e-07
Iter: 895 loss: 4.4506595e-07
Iter: 896 loss: 4.45031418e-07
Iter: 897 loss: 4.44908892e-07
Iter: 898 loss: 4.46445881e-07
Iter: 899 loss: 4.449073e-07
Iter: 900 loss: 4.44761099e-07
Iter: 901 loss: 4.44684474e-07
Iter: 902 loss: 4.4461396e-07
Iter: 903 loss: 4.44436864e-07
Iter: 904 loss: 4.45776493e-07
Iter: 905 loss: 4.44438427e-07
Iter: 906 loss: 4.4434384e-07
Iter: 907 loss: 4.44632462e-07
Iter: 908 loss: 4.44298166e-07
Iter: 909 loss: 4.44182177e-07
Iter: 910 loss: 4.44075539e-07
Iter: 911 loss: 4.44035237e-07
Iter: 912 loss: 4.43851491e-07
Iter: 913 loss: 4.4380954e-07
Iter: 914 loss: 4.43706483e-07
Iter: 915 loss: 4.43402087e-07
Iter: 916 loss: 4.43678118e-07
Iter: 917 loss: 4.43250485e-07
Iter: 918 loss: 4.42882964e-07
Iter: 919 loss: 4.43910722e-07
Iter: 920 loss: 4.4274384e-07
Iter: 921 loss: 4.42612759e-07
Iter: 922 loss: 4.42593887e-07
Iter: 923 loss: 4.42413977e-07
Iter: 924 loss: 4.42603067e-07
Iter: 925 loss: 4.42291338e-07
Iter: 926 loss: 4.42160228e-07
Iter: 927 loss: 4.42363e-07
Iter: 928 loss: 4.42093494e-07
Iter: 929 loss: 4.41955848e-07
Iter: 930 loss: 4.4359669e-07
Iter: 931 loss: 4.4196139e-07
Iter: 932 loss: 4.41849579e-07
Iter: 933 loss: 4.41690361e-07
Iter: 934 loss: 4.41690702e-07
Iter: 935 loss: 4.41599752e-07
Iter: 936 loss: 4.4156377e-07
Iter: 937 loss: 4.41490528e-07
Iter: 938 loss: 4.41282054e-07
Iter: 939 loss: 4.42535054e-07
Iter: 940 loss: 4.41221175e-07
Iter: 941 loss: 4.40973309e-07
Iter: 942 loss: 4.42445355e-07
Iter: 943 loss: 4.40940681e-07
Iter: 944 loss: 4.40723568e-07
Iter: 945 loss: 4.42959845e-07
Iter: 946 loss: 4.40707197e-07
Iter: 947 loss: 4.40579811e-07
Iter: 948 loss: 4.40709e-07
Iter: 949 loss: 4.40506653e-07
Iter: 950 loss: 4.403866e-07
Iter: 951 loss: 4.40296361e-07
Iter: 952 loss: 4.40241e-07
Iter: 953 loss: 4.40053327e-07
Iter: 954 loss: 4.40554e-07
Iter: 955 loss: 4.39984774e-07
Iter: 956 loss: 4.3990218e-07
Iter: 957 loss: 4.39872025e-07
Iter: 958 loss: 4.3980134e-07
Iter: 959 loss: 4.39635471e-07
Iter: 960 loss: 4.42091164e-07
Iter: 961 loss: 4.39612961e-07
Iter: 962 loss: 4.39452776e-07
Iter: 963 loss: 4.41225836e-07
Iter: 964 loss: 4.39447149e-07
Iter: 965 loss: 4.39257121e-07
Iter: 966 loss: 4.39315841e-07
Iter: 967 loss: 4.39113194e-07
Iter: 968 loss: 4.38975e-07
Iter: 969 loss: 4.40344621e-07
Iter: 970 loss: 4.38976826e-07
Iter: 971 loss: 4.38853533e-07
Iter: 972 loss: 4.38822326e-07
Iter: 973 loss: 4.38717478e-07
Iter: 974 loss: 4.3861337e-07
Iter: 975 loss: 4.38642587e-07
Iter: 976 loss: 4.38515571e-07
Iter: 977 loss: 4.38398274e-07
Iter: 978 loss: 4.40024081e-07
Iter: 979 loss: 4.38400207e-07
Iter: 980 loss: 4.38294904e-07
Iter: 981 loss: 4.3835513e-07
Iter: 982 loss: 4.38232803e-07
Iter: 983 loss: 4.38125e-07
Iter: 984 loss: 4.3811076e-07
Iter: 985 loss: 4.38017821e-07
Iter: 986 loss: 4.37841067e-07
Iter: 987 loss: 4.38074039e-07
Iter: 988 loss: 4.37754181e-07
Iter: 989 loss: 4.37672583e-07
Iter: 990 loss: 4.37646889e-07
Iter: 991 loss: 4.37559805e-07
Iter: 992 loss: 4.37346216e-07
Iter: 993 loss: 4.40896684e-07
Iter: 994 loss: 4.37354117e-07
Iter: 995 loss: 4.37200924e-07
Iter: 996 loss: 4.38886673e-07
Iter: 997 loss: 4.37202e-07
Iter: 998 loss: 4.37051256e-07
Iter: 999 loss: 4.37968481e-07
Iter: 1000 loss: 4.37035965e-07
Iter: 1001 loss: 4.36946266e-07
Iter: 1002 loss: 4.36890161e-07
Iter: 1003 loss: 4.36847188e-07
Iter: 1004 loss: 4.36704511e-07
Iter: 1005 loss: 4.38302806e-07
Iter: 1006 loss: 4.36694336e-07
Iter: 1007 loss: 4.36611259e-07
Iter: 1008 loss: 4.36442491e-07
Iter: 1009 loss: 4.38416976e-07
Iter: 1010 loss: 4.36447e-07
Iter: 1011 loss: 4.36291884e-07
Iter: 1012 loss: 4.36294499e-07
Iter: 1013 loss: 4.3613818e-07
Iter: 1014 loss: 4.36323887e-07
Iter: 1015 loss: 4.36050271e-07
Iter: 1016 loss: 4.35901086e-07
Iter: 1017 loss: 4.35871328e-07
Iter: 1018 loss: 4.35766822e-07
Iter: 1019 loss: 4.35578329e-07
Iter: 1020 loss: 4.35812751e-07
Iter: 1021 loss: 4.35463221e-07
Iter: 1022 loss: 4.35305083e-07
Iter: 1023 loss: 4.36599294e-07
Iter: 1024 loss: 4.35288825e-07
Iter: 1025 loss: 4.35135803e-07
Iter: 1026 loss: 4.36536368e-07
Iter: 1027 loss: 4.35127049e-07
Iter: 1028 loss: 4.35029222e-07
Iter: 1029 loss: 4.34858748e-07
Iter: 1030 loss: 4.37766062e-07
Iter: 1031 loss: 4.34865029e-07
Iter: 1032 loss: 4.34761e-07
Iter: 1033 loss: 4.34728918e-07
Iter: 1034 loss: 4.34627765e-07
Iter: 1035 loss: 4.34475737e-07
Iter: 1036 loss: 4.3446849e-07
Iter: 1037 loss: 4.34366257e-07
Iter: 1038 loss: 4.3437376e-07
Iter: 1039 loss: 4.34274568e-07
Iter: 1040 loss: 4.34116032e-07
Iter: 1041 loss: 4.37805511e-07
Iter: 1042 loss: 4.34123081e-07
Iter: 1043 loss: 4.33967955e-07
Iter: 1044 loss: 4.33878313e-07
Iter: 1045 loss: 4.33815927e-07
Iter: 1046 loss: 4.33739444e-07
Iter: 1047 loss: 4.33710738e-07
Iter: 1048 loss: 4.3360717e-07
Iter: 1049 loss: 4.33538162e-07
Iter: 1050 loss: 4.33486264e-07
Iter: 1051 loss: 4.33329944e-07
Iter: 1052 loss: 4.33118828e-07
Iter: 1053 loss: 4.3311519e-07
Iter: 1054 loss: 4.32902198e-07
Iter: 1055 loss: 4.3322396e-07
Iter: 1056 loss: 4.32782713e-07
Iter: 1057 loss: 4.32567475e-07
Iter: 1058 loss: 4.33043169e-07
Iter: 1059 loss: 4.32491618e-07
Iter: 1060 loss: 4.32404363e-07
Iter: 1061 loss: 4.32357751e-07
Iter: 1062 loss: 4.32289198e-07
Iter: 1063 loss: 4.32195719e-07
Iter: 1064 loss: 4.32192792e-07
Iter: 1065 loss: 4.32066e-07
Iter: 1066 loss: 4.33346599e-07
Iter: 1067 loss: 4.32068418e-07
Iter: 1068 loss: 4.31962974e-07
Iter: 1069 loss: 4.31902436e-07
Iter: 1070 loss: 4.31867846e-07
Iter: 1071 loss: 4.31724118e-07
Iter: 1072 loss: 4.33263068e-07
Iter: 1073 loss: 4.31732531e-07
Iter: 1074 loss: 4.31613529e-07
Iter: 1075 loss: 4.315458e-07
Iter: 1076 loss: 4.31498421e-07
Iter: 1077 loss: 4.31351282e-07
Iter: 1078 loss: 4.31389083e-07
Iter: 1079 loss: 4.31225e-07
Iter: 1080 loss: 4.31045237e-07
Iter: 1081 loss: 4.31508056e-07
Iter: 1082 loss: 4.30992856e-07
Iter: 1083 loss: 4.30928338e-07
Iter: 1084 loss: 4.30912337e-07
Iter: 1085 loss: 4.30851e-07
Iter: 1086 loss: 4.30689056e-07
Iter: 1087 loss: 4.32561421e-07
Iter: 1088 loss: 4.30673566e-07
Iter: 1089 loss: 4.30524864e-07
Iter: 1090 loss: 4.30461228e-07
Iter: 1091 loss: 4.30378833e-07
Iter: 1092 loss: 4.30169507e-07
Iter: 1093 loss: 4.31912468e-07
Iter: 1094 loss: 4.30150266e-07
Iter: 1095 loss: 4.29952536e-07
Iter: 1096 loss: 4.31769848e-07
Iter: 1097 loss: 4.29933095e-07
Iter: 1098 loss: 4.29789878e-07
Iter: 1099 loss: 4.2967261e-07
Iter: 1100 loss: 4.29645695e-07
Iter: 1101 loss: 4.29537522e-07
Iter: 1102 loss: 4.29527773e-07
Iter: 1103 loss: 4.29416758e-07
Iter: 1104 loss: 4.2942122e-07
Iter: 1105 loss: 4.29334193e-07
Iter: 1106 loss: 4.29250377e-07
Iter: 1107 loss: 4.29253589e-07
Iter: 1108 loss: 4.29184411e-07
Iter: 1109 loss: 4.29118813e-07
Iter: 1110 loss: 4.29104318e-07
Iter: 1111 loss: 4.28995776e-07
Iter: 1112 loss: 4.29072713e-07
Iter: 1113 loss: 4.28929411e-07
Iter: 1114 loss: 4.28792703e-07
Iter: 1115 loss: 4.29258819e-07
Iter: 1116 loss: 4.28759449e-07
Iter: 1117 loss: 4.28609496e-07
Iter: 1118 loss: 4.29774047e-07
Iter: 1119 loss: 4.28593751e-07
Iter: 1120 loss: 4.28496833e-07
Iter: 1121 loss: 4.28320931e-07
Iter: 1122 loss: 4.32224567e-07
Iter: 1123 loss: 4.28319822e-07
Iter: 1124 loss: 4.28120757e-07
Iter: 1125 loss: 4.28694023e-07
Iter: 1126 loss: 4.28059423e-07
Iter: 1127 loss: 4.27897817e-07
Iter: 1128 loss: 4.28109047e-07
Iter: 1129 loss: 4.2782e-07
Iter: 1130 loss: 4.27738627e-07
Iter: 1131 loss: 4.27717112e-07
Iter: 1132 loss: 4.27604846e-07
Iter: 1133 loss: 4.27748546e-07
Iter: 1134 loss: 4.27557438e-07
Iter: 1135 loss: 4.27484139e-07
Iter: 1136 loss: 4.27457451e-07
Iter: 1137 loss: 4.27406434e-07
Iter: 1138 loss: 4.27268048e-07
Iter: 1139 loss: 4.2829339e-07
Iter: 1140 loss: 4.27248e-07
Iter: 1141 loss: 4.27158568e-07
Iter: 1142 loss: 4.27274188e-07
Iter: 1143 loss: 4.27115509e-07
Iter: 1144 loss: 4.26989857e-07
Iter: 1145 loss: 4.27229423e-07
Iter: 1146 loss: 4.26925e-07
Iter: 1147 loss: 4.26821714e-07
Iter: 1148 loss: 4.26823277e-07
Iter: 1149 loss: 4.26742815e-07
Iter: 1150 loss: 4.26612303e-07
Iter: 1151 loss: 4.26850704e-07
Iter: 1152 loss: 4.26549775e-07
Iter: 1153 loss: 4.26417046e-07
Iter: 1154 loss: 4.26564583e-07
Iter: 1155 loss: 4.26370889e-07
Iter: 1156 loss: 4.26222556e-07
Iter: 1157 loss: 4.27147882e-07
Iter: 1158 loss: 4.26214712e-07
Iter: 1159 loss: 4.26117936e-07
Iter: 1160 loss: 4.26125609e-07
Iter: 1161 loss: 4.26057483e-07
Iter: 1162 loss: 4.25916483e-07
Iter: 1163 loss: 4.2704113e-07
Iter: 1164 loss: 4.25886014e-07
Iter: 1165 loss: 4.2569522e-07
Iter: 1166 loss: 4.26451749e-07
Iter: 1167 loss: 4.25663075e-07
Iter: 1168 loss: 4.25587785e-07
Iter: 1169 loss: 4.25575791e-07
Iter: 1170 loss: 4.2548578e-07
Iter: 1171 loss: 4.25372804e-07
Iter: 1172 loss: 4.25377607e-07
Iter: 1173 loss: 4.25272333e-07
Iter: 1174 loss: 4.26089684e-07
Iter: 1175 loss: 4.25285322e-07
Iter: 1176 loss: 4.25180133e-07
Iter: 1177 loss: 4.25307917e-07
Iter: 1178 loss: 4.25125364e-07
Iter: 1179 loss: 4.25035807e-07
Iter: 1180 loss: 4.25366e-07
Iter: 1181 loss: 4.25027167e-07
Iter: 1182 loss: 4.24909189e-07
Iter: 1183 loss: 4.24898815e-07
Iter: 1184 loss: 4.24816079e-07
Iter: 1185 loss: 4.24708105e-07
Iter: 1186 loss: 4.24594873e-07
Iter: 1187 loss: 4.24569805e-07
Iter: 1188 loss: 4.2439126e-07
Iter: 1189 loss: 4.24892221e-07
Iter: 1190 loss: 4.24344762e-07
Iter: 1191 loss: 4.24280017e-07
Iter: 1192 loss: 4.24265238e-07
Iter: 1193 loss: 4.24161215e-07
Iter: 1194 loss: 4.2417193e-07
Iter: 1195 loss: 4.24098801e-07
Iter: 1196 loss: 4.2396789e-07
Iter: 1197 loss: 4.23812281e-07
Iter: 1198 loss: 4.23815209e-07
Iter: 1199 loss: 4.23651784e-07
Iter: 1200 loss: 4.25125421e-07
Iter: 1201 loss: 4.23647151e-07
Iter: 1202 loss: 4.23525506e-07
Iter: 1203 loss: 4.23925655e-07
Iter: 1204 loss: 4.23492111e-07
Iter: 1205 loss: 4.23413667e-07
Iter: 1206 loss: 4.23418442e-07
Iter: 1207 loss: 4.23344318e-07
Iter: 1208 loss: 4.23265647e-07
Iter: 1209 loss: 4.23270365e-07
Iter: 1210 loss: 4.2319482e-07
Iter: 1211 loss: 4.23189022e-07
Iter: 1212 loss: 4.23109896e-07
Iter: 1213 loss: 4.22992684e-07
Iter: 1214 loss: 4.22984613e-07
Iter: 1215 loss: 4.22904e-07
Iter: 1216 loss: 4.23951462e-07
Iter: 1217 loss: 4.22885279e-07
Iter: 1218 loss: 4.22786115e-07
Iter: 1219 loss: 4.22669444e-07
Iter: 1220 loss: 4.2266538e-07
Iter: 1221 loss: 4.22496e-07
Iter: 1222 loss: 4.22470862e-07
Iter: 1223 loss: 4.2235672e-07
Iter: 1224 loss: 4.22276486e-07
Iter: 1225 loss: 4.22261479e-07
Iter: 1226 loss: 4.22158053e-07
Iter: 1227 loss: 4.22184883e-07
Iter: 1228 loss: 4.22094558e-07
Iter: 1229 loss: 4.21996731e-07
Iter: 1230 loss: 4.21909192e-07
Iter: 1231 loss: 4.21896573e-07
Iter: 1232 loss: 4.21730761e-07
Iter: 1233 loss: 4.22440849e-07
Iter: 1234 loss: 4.2170862e-07
Iter: 1235 loss: 4.21541472e-07
Iter: 1236 loss: 4.22185508e-07
Iter: 1237 loss: 4.21504154e-07
Iter: 1238 loss: 4.21380605e-07
Iter: 1239 loss: 4.22952667e-07
Iter: 1240 loss: 4.21381401e-07
Iter: 1241 loss: 4.21286558e-07
Iter: 1242 loss: 4.21229146e-07
Iter: 1243 loss: 4.21172928e-07
Iter: 1244 loss: 4.21032382e-07
Iter: 1245 loss: 4.21834102e-07
Iter: 1246 loss: 4.21005609e-07
Iter: 1247 loss: 4.20875125e-07
Iter: 1248 loss: 4.21384584e-07
Iter: 1249 loss: 4.20855031e-07
Iter: 1250 loss: 4.20789434e-07
Iter: 1251 loss: 4.20771e-07
Iter: 1252 loss: 4.2072412e-07
Iter: 1253 loss: 4.20593608e-07
Iter: 1254 loss: 4.20934867e-07
Iter: 1255 loss: 4.20572633e-07
Iter: 1256 loss: 4.20476454e-07
Iter: 1257 loss: 4.20389028e-07
Iter: 1258 loss: 4.20361118e-07
Iter: 1259 loss: 4.20286085e-07
Iter: 1260 loss: 4.20272329e-07
Iter: 1261 loss: 4.20193146e-07
Iter: 1262 loss: 4.20086451e-07
Iter: 1263 loss: 4.20071956e-07
Iter: 1264 loss: 4.19943433e-07
Iter: 1265 loss: 4.19793651e-07
Iter: 1266 loss: 4.19769691e-07
Iter: 1267 loss: 4.19560195e-07
Iter: 1268 loss: 4.20338949e-07
Iter: 1269 loss: 4.19517221e-07
Iter: 1270 loss: 4.19377272e-07
Iter: 1271 loss: 4.20384822e-07
Iter: 1272 loss: 4.1937318e-07
Iter: 1273 loss: 4.19328643e-07
Iter: 1274 loss: 4.19326966e-07
Iter: 1275 loss: 4.19270123e-07
Iter: 1276 loss: 4.19150751e-07
Iter: 1277 loss: 4.21025817e-07
Iter: 1278 loss: 4.19152826e-07
Iter: 1279 loss: 4.19091236e-07
Iter: 1280 loss: 4.19070972e-07
Iter: 1281 loss: 4.19026094e-07
Iter: 1282 loss: 4.18892824e-07
Iter: 1283 loss: 4.21046934e-07
Iter: 1284 loss: 4.18901863e-07
Iter: 1285 loss: 4.18776636e-07
Iter: 1286 loss: 4.20224922e-07
Iter: 1287 loss: 4.18781553e-07
Iter: 1288 loss: 4.18695691e-07
Iter: 1289 loss: 4.18637455e-07
Iter: 1290 loss: 4.18596301e-07
Iter: 1291 loss: 4.18485712e-07
Iter: 1292 loss: 4.18656612e-07
Iter: 1293 loss: 4.18425088e-07
Iter: 1294 loss: 4.18305405e-07
Iter: 1295 loss: 4.1830026e-07
Iter: 1296 loss: 4.1823526e-07
Iter: 1297 loss: 4.18289801e-07
Iter: 1298 loss: 4.18189444e-07
Iter: 1299 loss: 4.18109209e-07
Iter: 1300 loss: 4.1799106e-07
Iter: 1301 loss: 4.17989838e-07
Iter: 1302 loss: 4.1783656e-07
Iter: 1303 loss: 4.18076183e-07
Iter: 1304 loss: 4.17757406e-07
Iter: 1305 loss: 4.17590059e-07
Iter: 1306 loss: 4.18210846e-07
Iter: 1307 loss: 4.17537535e-07
Iter: 1308 loss: 4.17488366e-07
Iter: 1309 loss: 4.17458665e-07
Iter: 1310 loss: 4.17387639e-07
Iter: 1311 loss: 4.17251385e-07
Iter: 1312 loss: 4.18698221e-07
Iter: 1313 loss: 4.17228364e-07
Iter: 1314 loss: 4.17206479e-07
Iter: 1315 loss: 4.17143042e-07
Iter: 1316 loss: 4.17100807e-07
Iter: 1317 loss: 4.17000876e-07
Iter: 1318 loss: 4.18325413e-07
Iter: 1319 loss: 4.16983653e-07
Iter: 1320 loss: 4.16872183e-07
Iter: 1321 loss: 4.18407751e-07
Iter: 1322 loss: 4.16876475e-07
Iter: 1323 loss: 4.16800248e-07
Iter: 1324 loss: 4.16821848e-07
Iter: 1325 loss: 4.16730046e-07
Iter: 1326 loss: 4.1663219e-07
Iter: 1327 loss: 4.168229e-07
Iter: 1328 loss: 4.16610931e-07
Iter: 1329 loss: 4.16501962e-07
Iter: 1330 loss: 4.1709896e-07
Iter: 1331 loss: 4.16484e-07
Iter: 1332 loss: 4.16401235e-07
Iter: 1333 loss: 4.16316936e-07
Iter: 1334 loss: 4.16298519e-07
Iter: 1335 loss: 4.16188044e-07
Iter: 1336 loss: 4.16790783e-07
Iter: 1337 loss: 4.16175396e-07
Iter: 1338 loss: 4.16082202e-07
Iter: 1339 loss: 4.16063756e-07
Iter: 1340 loss: 4.16007481e-07
Iter: 1341 loss: 4.15907152e-07
Iter: 1342 loss: 4.17158873e-07
Iter: 1343 loss: 4.15902662e-07
Iter: 1344 loss: 4.15801424e-07
Iter: 1345 loss: 4.16062562e-07
Iter: 1346 loss: 4.15775503e-07
Iter: 1347 loss: 4.15696206e-07
Iter: 1348 loss: 4.15821262e-07
Iter: 1349 loss: 4.15666818e-07
Iter: 1350 loss: 4.15550971e-07
Iter: 1351 loss: 4.15785593e-07
Iter: 1352 loss: 4.15507742e-07
Iter: 1353 loss: 4.15462e-07
Iter: 1354 loss: 4.1543376e-07
Iter: 1355 loss: 4.15405623e-07
Iter: 1356 loss: 4.15303873e-07
Iter: 1357 loss: 4.16232837e-07
Iter: 1358 loss: 4.15302793e-07
Iter: 1359 loss: 4.15223894e-07
Iter: 1360 loss: 4.15198372e-07
Iter: 1361 loss: 4.15179215e-07
Iter: 1362 loss: 4.15088778e-07
Iter: 1363 loss: 4.15961836e-07
Iter: 1364 loss: 4.15079398e-07
Iter: 1365 loss: 4.15007435e-07
Iter: 1366 loss: 4.14984612e-07
Iter: 1367 loss: 4.14938e-07
Iter: 1368 loss: 4.14843043e-07
Iter: 1369 loss: 4.14792282e-07
Iter: 1370 loss: 4.1475846e-07
Iter: 1371 loss: 4.14611378e-07
Iter: 1372 loss: 4.16075e-07
Iter: 1373 loss: 4.14604841e-07
Iter: 1374 loss: 4.14510282e-07
Iter: 1375 loss: 4.14346601e-07
Iter: 1376 loss: 4.14355924e-07
Iter: 1377 loss: 4.14209524e-07
Iter: 1378 loss: 4.15507827e-07
Iter: 1379 loss: 4.14208159e-07
Iter: 1380 loss: 4.14134149e-07
Iter: 1381 loss: 4.14133439e-07
Iter: 1382 loss: 4.14068751e-07
Iter: 1383 loss: 4.1399926e-07
Iter: 1384 loss: 4.13994201e-07
Iter: 1385 loss: 4.13913511e-07
Iter: 1386 loss: 4.14969804e-07
Iter: 1387 loss: 4.13907884e-07
Iter: 1388 loss: 4.13836688e-07
Iter: 1389 loss: 4.13754435e-07
Iter: 1390 loss: 4.1374318e-07
Iter: 1391 loss: 4.13633302e-07
Iter: 1392 loss: 4.13592346e-07
Iter: 1393 loss: 4.13539397e-07
Iter: 1394 loss: 4.13470985e-07
Iter: 1395 loss: 4.13460157e-07
Iter: 1396 loss: 4.13389387e-07
Iter: 1397 loss: 4.13276638e-07
Iter: 1398 loss: 4.13273284e-07
Iter: 1399 loss: 4.13200695e-07
Iter: 1400 loss: 4.14024782e-07
Iter: 1401 loss: 4.13182022e-07
Iter: 1402 loss: 4.13111223e-07
Iter: 1403 loss: 4.13578306e-07
Iter: 1404 loss: 4.13110399e-07
Iter: 1405 loss: 4.13055488e-07
Iter: 1406 loss: 4.12921452e-07
Iter: 1407 loss: 4.14635537e-07
Iter: 1408 loss: 4.12908321e-07
Iter: 1409 loss: 4.12799523e-07
Iter: 1410 loss: 4.13488976e-07
Iter: 1411 loss: 4.12793554e-07
Iter: 1412 loss: 4.12663212e-07
Iter: 1413 loss: 4.12858128e-07
Iter: 1414 loss: 4.12609268e-07
Iter: 1415 loss: 4.12499048e-07
Iter: 1416 loss: 4.12988015e-07
Iter: 1417 loss: 4.12483075e-07
Iter: 1418 loss: 4.12344065e-07
Iter: 1419 loss: 4.13301308e-07
Iter: 1420 loss: 4.12335083e-07
Iter: 1421 loss: 4.1228094e-07
Iter: 1422 loss: 4.122912e-07
Iter: 1423 loss: 4.12229156e-07
Iter: 1424 loss: 4.12123256e-07
Iter: 1425 loss: 4.12467671e-07
Iter: 1426 loss: 4.12103248e-07
Iter: 1427 loss: 4.12020938e-07
Iter: 1428 loss: 4.11945962e-07
Iter: 1429 loss: 4.11938146e-07
Iter: 1430 loss: 4.11854955e-07
Iter: 1431 loss: 4.12636666e-07
Iter: 1432 loss: 4.11841313e-07
Iter: 1433 loss: 4.11758947e-07
Iter: 1434 loss: 4.11938231e-07
Iter: 1435 loss: 4.11730355e-07
Iter: 1436 loss: 4.11651911e-07
Iter: 1437 loss: 4.11601e-07
Iter: 1438 loss: 4.11575058e-07
Iter: 1439 loss: 4.11439203e-07
Iter: 1440 loss: 4.13074645e-07
Iter: 1441 loss: 4.11444859e-07
Iter: 1442 loss: 4.11390829e-07
Iter: 1443 loss: 4.11255627e-07
Iter: 1444 loss: 4.13308157e-07
Iter: 1445 loss: 4.11258e-07
Iter: 1446 loss: 4.11127758e-07
Iter: 1447 loss: 4.11538082e-07
Iter: 1448 loss: 4.11077053e-07
Iter: 1449 loss: 4.10963736e-07
Iter: 1450 loss: 4.11722e-07
Iter: 1451 loss: 4.1094583e-07
Iter: 1452 loss: 4.1087165e-07
Iter: 1453 loss: 4.11469415e-07
Iter: 1454 loss: 4.10868523e-07
Iter: 1455 loss: 4.10790278e-07
Iter: 1456 loss: 4.10860878e-07
Iter: 1457 loss: 4.10745855e-07
Iter: 1458 loss: 4.10654309e-07
Iter: 1459 loss: 4.10726074e-07
Iter: 1460 loss: 4.10617019e-07
Iter: 1461 loss: 4.10499041e-07
Iter: 1462 loss: 4.11442045e-07
Iter: 1463 loss: 4.1050464e-07
Iter: 1464 loss: 4.10440947e-07
Iter: 1465 loss: 4.10300458e-07
Iter: 1466 loss: 4.11828694e-07
Iter: 1467 loss: 4.10276357e-07
Iter: 1468 loss: 4.10146811e-07
Iter: 1469 loss: 4.11138842e-07
Iter: 1470 loss: 4.10135783e-07
Iter: 1471 loss: 4.0999339e-07
Iter: 1472 loss: 4.10753e-07
Iter: 1473 loss: 4.09963093e-07
Iter: 1474 loss: 4.09865635e-07
Iter: 1475 loss: 4.09850799e-07
Iter: 1476 loss: 4.09771076e-07
Iter: 1477 loss: 4.09644144e-07
Iter: 1478 loss: 4.10972376e-07
Iter: 1479 loss: 4.09627432e-07
Iter: 1480 loss: 4.09557401e-07
Iter: 1481 loss: 4.09464747e-07
Iter: 1482 loss: 4.09464235e-07
Iter: 1483 loss: 4.09343613e-07
Iter: 1484 loss: 4.094166e-07
Iter: 1485 loss: 4.09257183e-07
Iter: 1486 loss: 4.09139147e-07
Iter: 1487 loss: 4.10770866e-07
Iter: 1488 loss: 4.09146821e-07
Iter: 1489 loss: 4.09040183e-07
Iter: 1490 loss: 4.09109674e-07
Iter: 1491 loss: 4.08987489e-07
Iter: 1492 loss: 4.08884063e-07
Iter: 1493 loss: 4.08873149e-07
Iter: 1494 loss: 4.08828441e-07
Iter: 1495 loss: 4.08751362e-07
Iter: 1496 loss: 4.08742636e-07
Iter: 1497 loss: 4.08632275e-07
Iter: 1498 loss: 4.09510051e-07
Iter: 1499 loss: 4.08622384e-07
Iter: 1500 loss: 4.08547095e-07
Iter: 1501 loss: 4.08420419e-07
Iter: 1502 loss: 4.08410898e-07
Iter: 1503 loss: 4.08326201e-07
Iter: 1504 loss: 4.09699055e-07
Iter: 1505 loss: 4.08311507e-07
Iter: 1506 loss: 4.08217062e-07
Iter: 1507 loss: 4.08128415e-07
Iter: 1508 loss: 4.08104171e-07
Iter: 1509 loss: 4.08005633e-07
Iter: 1510 loss: 4.08272797e-07
Iter: 1511 loss: 4.07975563e-07
Iter: 1512 loss: 4.07879213e-07
Iter: 1513 loss: 4.07874808e-07
Iter: 1514 loss: 4.07834079e-07
Iter: 1515 loss: 4.07727924e-07
Iter: 1516 loss: 4.09726425e-07
Iter: 1517 loss: 4.07724031e-07
Iter: 1518 loss: 4.0763922e-07
Iter: 1519 loss: 4.07677078e-07
Iter: 1520 loss: 4.07576039e-07
Iter: 1521 loss: 4.0744149e-07
Iter: 1522 loss: 4.07677106e-07
Iter: 1523 loss: 4.07407242e-07
Iter: 1524 loss: 4.07263485e-07
Iter: 1525 loss: 4.08089761e-07
Iter: 1526 loss: 4.07264054e-07
Iter: 1527 loss: 4.07158097e-07
Iter: 1528 loss: 4.08150413e-07
Iter: 1529 loss: 4.07164691e-07
Iter: 1530 loss: 4.07055325e-07
Iter: 1531 loss: 4.07174127e-07
Iter: 1532 loss: 4.06998083e-07
Iter: 1533 loss: 4.06904491e-07
Iter: 1534 loss: 4.07162702e-07
Iter: 1535 loss: 4.0688073e-07
Iter: 1536 loss: 4.06770937e-07
Iter: 1537 loss: 4.0707971e-07
Iter: 1538 loss: 4.06740924e-07
Iter: 1539 loss: 4.06668164e-07
Iter: 1540 loss: 4.06542227e-07
Iter: 1541 loss: 4.06536941e-07
Iter: 1542 loss: 4.06475209e-07
Iter: 1543 loss: 4.06469667e-07
Iter: 1544 loss: 4.0638821e-07
Iter: 1545 loss: 4.06252752e-07
Iter: 1546 loss: 4.06254e-07
Iter: 1547 loss: 4.06140146e-07
Iter: 1548 loss: 4.07020593e-07
Iter: 1549 loss: 4.06131278e-07
Iter: 1550 loss: 4.06012276e-07
Iter: 1551 loss: 4.06508207e-07
Iter: 1552 loss: 4.05982462e-07
Iter: 1553 loss: 4.05918144e-07
Iter: 1554 loss: 4.05767537e-07
Iter: 1555 loss: 4.08068587e-07
Iter: 1556 loss: 4.0575506e-07
Iter: 1557 loss: 4.05634296e-07
Iter: 1558 loss: 4.06604983e-07
Iter: 1559 loss: 4.05623808e-07
Iter: 1560 loss: 4.05540845e-07
Iter: 1561 loss: 4.05893616e-07
Iter: 1562 loss: 4.05516857e-07
Iter: 1563 loss: 4.05427045e-07
Iter: 1564 loss: 4.06194346e-07
Iter: 1565 loss: 4.05435713e-07
Iter: 1566 loss: 4.05371736e-07
Iter: 1567 loss: 4.05361305e-07
Iter: 1568 loss: 4.05315319e-07
Iter: 1569 loss: 4.05230821e-07
Iter: 1570 loss: 4.05721892e-07
Iter: 1571 loss: 4.05215161e-07
Iter: 1572 loss: 4.05141236e-07
Iter: 1573 loss: 4.05126912e-07
Iter: 1574 loss: 4.05063844e-07
Iter: 1575 loss: 4.04975509e-07
Iter: 1576 loss: 4.04958939e-07
Iter: 1577 loss: 4.04906928e-07
Iter: 1578 loss: 4.04778461e-07
Iter: 1579 loss: 4.04781474e-07
Iter: 1580 loss: 4.04720595e-07
Iter: 1581 loss: 4.04652269e-07
Iter: 1582 loss: 4.04645334e-07
Iter: 1583 loss: 4.04566947e-07
Iter: 1584 loss: 4.04561717e-07
Iter: 1585 loss: 4.04486968e-07
Iter: 1586 loss: 4.04413498e-07
Iter: 1587 loss: 4.04400907e-07
Iter: 1588 loss: 4.04314505e-07
Iter: 1589 loss: 4.04336191e-07
Iter: 1590 loss: 4.04267098e-07
Iter: 1591 loss: 4.04175e-07
Iter: 1592 loss: 4.04711216e-07
Iter: 1593 loss: 4.04161199e-07
Iter: 1594 loss: 4.0408969e-07
Iter: 1595 loss: 4.04223044e-07
Iter: 1596 loss: 4.0405925e-07
Iter: 1597 loss: 4.03968471e-07
Iter: 1598 loss: 4.04307173e-07
Iter: 1599 loss: 4.03948462e-07
Iter: 1600 loss: 4.03866181e-07
Iter: 1601 loss: 4.03891249e-07
Iter: 1602 loss: 4.03792683e-07
Iter: 1603 loss: 4.0369315e-07
Iter: 1604 loss: 4.04505954e-07
Iter: 1605 loss: 4.03689171e-07
Iter: 1606 loss: 4.03588587e-07
Iter: 1607 loss: 4.03471176e-07
Iter: 1608 loss: 4.03478168e-07
Iter: 1609 loss: 4.03347826e-07
Iter: 1610 loss: 4.04770248e-07
Iter: 1611 loss: 4.03358115e-07
Iter: 1612 loss: 4.03246219e-07
Iter: 1613 loss: 4.03452674e-07
Iter: 1614 loss: 4.03214329e-07
Iter: 1615 loss: 4.03149272e-07
Iter: 1616 loss: 4.03283366e-07
Iter: 1617 loss: 4.03120509e-07
Iter: 1618 loss: 4.03026263e-07
Iter: 1619 loss: 4.03401515e-07
Iter: 1620 loss: 4.03006595e-07
Iter: 1621 loss: 4.02940714e-07
Iter: 1622 loss: 4.029325e-07
Iter: 1623 loss: 4.02891033e-07
Iter: 1624 loss: 4.02818955e-07
Iter: 1625 loss: 4.03227091e-07
Iter: 1626 loss: 4.02807643e-07
Iter: 1627 loss: 4.02728233e-07
Iter: 1628 loss: 4.02845927e-07
Iter: 1629 loss: 4.02711237e-07
Iter: 1630 loss: 4.02614944e-07
Iter: 1631 loss: 4.02955607e-07
Iter: 1632 loss: 4.02607668e-07
Iter: 1633 loss: 4.02513393e-07
Iter: 1634 loss: 4.0249239e-07
Iter: 1635 loss: 4.02447682e-07
Iter: 1636 loss: 4.02338145e-07
Iter: 1637 loss: 4.03300731e-07
Iter: 1638 loss: 4.02330556e-07
Iter: 1639 loss: 4.02262231e-07
Iter: 1640 loss: 4.02235372e-07
Iter: 1641 loss: 4.02194189e-07
Iter: 1642 loss: 4.02094315e-07
Iter: 1643 loss: 4.02357074e-07
Iter: 1644 loss: 4.02058106e-07
Iter: 1645 loss: 4.0197e-07
Iter: 1646 loss: 4.02756143e-07
Iter: 1647 loss: 4.0196e-07
Iter: 1648 loss: 4.01886666e-07
Iter: 1649 loss: 4.0181726e-07
Iter: 1650 loss: 4.01814674e-07
Iter: 1651 loss: 4.01740351e-07
Iter: 1652 loss: 4.01737736e-07
Iter: 1653 loss: 4.0167339e-07
Iter: 1654 loss: 4.01575278e-07
Iter: 1655 loss: 4.03619168e-07
Iter: 1656 loss: 4.0156857e-07
Iter: 1657 loss: 4.01458e-07
Iter: 1658 loss: 4.02395727e-07
Iter: 1659 loss: 4.01461534e-07
Iter: 1660 loss: 4.01358648e-07
Iter: 1661 loss: 4.01674754e-07
Iter: 1662 loss: 4.0133753e-07
Iter: 1663 loss: 4.0124749e-07
Iter: 1664 loss: 4.01466366e-07
Iter: 1665 loss: 4.0123723e-07
Iter: 1666 loss: 4.0112738e-07
Iter: 1667 loss: 4.01302202e-07
Iter: 1668 loss: 4.01079092e-07
Iter: 1669 loss: 4.00978195e-07
Iter: 1670 loss: 4.01270313e-07
Iter: 1671 loss: 4.0095054e-07
Iter: 1672 loss: 4.00851661e-07
Iter: 1673 loss: 4.01023925e-07
Iter: 1674 loss: 4.00816418e-07
Iter: 1675 loss: 4.00733825e-07
Iter: 1676 loss: 4.00764407e-07
Iter: 1677 loss: 4.00674622e-07
Iter: 1678 loss: 4.00586799e-07
Iter: 1679 loss: 4.00583303e-07
Iter: 1680 loss: 4.00502302e-07
Iter: 1681 loss: 4.00468593e-07
Iter: 1682 loss: 4.00450745e-07
Iter: 1683 loss: 4.00370311e-07
Iter: 1684 loss: 4.01077671e-07
Iter: 1685 loss: 4.00360676e-07
Iter: 1686 loss: 4.00282033e-07
Iter: 1687 loss: 4.00204158e-07
Iter: 1688 loss: 4.00188924e-07
Iter: 1689 loss: 4.00079671e-07
Iter: 1690 loss: 4.00404701e-07
Iter: 1691 loss: 4.00041273e-07
Iter: 1692 loss: 3.99954445e-07
Iter: 1693 loss: 4.00830857e-07
Iter: 1694 loss: 3.99942479e-07
Iter: 1695 loss: 3.99878502e-07
Iter: 1696 loss: 3.99964279e-07
Iter: 1697 loss: 3.99842691e-07
Iter: 1698 loss: 3.99757226e-07
Iter: 1699 loss: 4.00143307e-07
Iter: 1700 loss: 3.99749979e-07
Iter: 1701 loss: 3.99690265e-07
Iter: 1702 loss: 3.99803e-07
Iter: 1703 loss: 3.99650276e-07
Iter: 1704 loss: 3.99571974e-07
Iter: 1705 loss: 3.99737729e-07
Iter: 1706 loss: 3.99551936e-07
Iter: 1707 loss: 3.99464909e-07
Iter: 1708 loss: 3.9949208e-07
Iter: 1709 loss: 3.99422959e-07
Iter: 1710 loss: 3.99369014e-07
Iter: 1711 loss: 3.99362307e-07
Iter: 1712 loss: 3.99314672e-07
Iter: 1713 loss: 3.99262859e-07
Iter: 1714 loss: 3.99257942e-07
Iter: 1715 loss: 3.99173359e-07
Iter: 1716 loss: 3.99570126e-07
Iter: 1717 loss: 3.99155709e-07
Iter: 1718 loss: 3.99083262e-07
Iter: 1719 loss: 3.99325188e-07
Iter: 1720 loss: 3.99047167e-07
Iter: 1721 loss: 3.9899993e-07
Iter: 1722 loss: 3.98968268e-07
Iter: 1723 loss: 3.98936038e-07
Iter: 1724 loss: 3.98872174e-07
Iter: 1725 loss: 3.98873055e-07
Iter: 1726 loss: 3.98818599e-07
Iter: 1727 loss: 3.98858e-07
Iter: 1728 loss: 3.98795976e-07
Iter: 1729 loss: 3.98731288e-07
Iter: 1730 loss: 3.98964829e-07
Iter: 1731 loss: 3.98717532e-07
Iter: 1732 loss: 3.98677741e-07
Iter: 1733 loss: 3.98755219e-07
Iter: 1734 loss: 3.98654038e-07
Iter: 1735 loss: 3.98604016e-07
Iter: 1736 loss: 3.98783982e-07
Iter: 1737 loss: 3.98579e-07
Iter: 1738 loss: 3.98530176e-07
Iter: 1739 loss: 3.98506359e-07
Iter: 1740 loss: 3.98482342e-07
Iter: 1741 loss: 3.98429478e-07
Iter: 1742 loss: 3.99124019e-07
Iter: 1743 loss: 3.98431183e-07
Iter: 1744 loss: 3.98370474e-07
Iter: 1745 loss: 3.9840404e-07
Iter: 1746 loss: 3.98338869e-07
Iter: 1747 loss: 3.98276455e-07
Iter: 1748 loss: 3.98361635e-07
Iter: 1749 loss: 3.98238853e-07
Iter: 1750 loss: 3.98174e-07
Iter: 1751 loss: 3.98865552e-07
Iter: 1752 loss: 3.98158761e-07
Iter: 1753 loss: 3.98123e-07
Iter: 1754 loss: 3.98025122e-07
Iter: 1755 loss: 3.99518854e-07
Iter: 1756 loss: 3.98015942e-07
Iter: 1757 loss: 3.97960207e-07
Iter: 1758 loss: 3.97958587e-07
Iter: 1759 loss: 3.97908479e-07
Iter: 1760 loss: 3.97980841e-07
Iter: 1761 loss: 3.97872412e-07
Iter: 1762 loss: 3.97824721e-07
Iter: 1763 loss: 3.9803453e-07
Iter: 1764 loss: 3.97813949e-07
Iter: 1765 loss: 3.97775239e-07
Iter: 1766 loss: 3.97823726e-07
Iter: 1767 loss: 3.97774585e-07
Iter: 1768 loss: 3.97710892e-07
Iter: 1769 loss: 3.97894325e-07
Iter: 1770 loss: 3.9770066e-07
Iter: 1771 loss: 3.97647341e-07
Iter: 1772 loss: 3.97641145e-07
Iter: 1773 loss: 3.97591322e-07
Iter: 1774 loss: 3.9753553e-07
Iter: 1775 loss: 3.97785072e-07
Iter: 1776 loss: 3.97529192e-07
Iter: 1777 loss: 3.9746115e-07
Iter: 1778 loss: 3.97804797e-07
Iter: 1779 loss: 3.97447167e-07
Iter: 1780 loss: 3.97404733e-07
Iter: 1781 loss: 3.9735616e-07
Iter: 1782 loss: 3.9735238e-07
Iter: 1783 loss: 3.9727729e-07
Iter: 1784 loss: 3.97281724e-07
Iter: 1785 loss: 3.97238466e-07
Iter: 1786 loss: 3.97141235e-07
Iter: 1787 loss: 3.97149563e-07
Iter: 1788 loss: 3.97107272e-07
Iter: 1789 loss: 3.97918171e-07
Iter: 1790 loss: 3.9709758e-07
Iter: 1791 loss: 3.97035024e-07
Iter: 1792 loss: 3.9715772e-07
Iter: 1793 loss: 3.97014162e-07
Iter: 1794 loss: 3.96960559e-07
Iter: 1795 loss: 3.9718077e-07
Iter: 1796 loss: 3.96960786e-07
Iter: 1797 loss: 3.96913521e-07
Iter: 1798 loss: 3.96954533e-07
Iter: 1799 loss: 3.96886207e-07
Iter: 1800 loss: 3.96829648e-07
Iter: 1801 loss: 3.96910593e-07
Iter: 1802 loss: 3.96808588e-07
Iter: 1803 loss: 3.9671994e-07
Iter: 1804 loss: 3.96935548e-07
Iter: 1805 loss: 3.96688677e-07
Iter: 1806 loss: 3.96631947e-07
Iter: 1807 loss: 3.96794348e-07
Iter: 1808 loss: 3.96610062e-07
Iter: 1809 loss: 3.9656706e-07
Iter: 1810 loss: 3.96977413e-07
Iter: 1811 loss: 3.96560949e-07
Iter: 1812 loss: 3.96497654e-07
Iter: 1813 loss: 3.96443369e-07
Iter: 1814 loss: 3.96441294e-07
Iter: 1815 loss: 3.96371945e-07
Iter: 1816 loss: 3.96383683e-07
Iter: 1817 loss: 3.96329824e-07
Iter: 1818 loss: 3.96270707e-07
Iter: 1819 loss: 3.96262152e-07
Iter: 1820 loss: 3.96209145e-07
Iter: 1821 loss: 3.96549751e-07
Iter: 1822 loss: 3.96204626e-07
Iter: 1823 loss: 3.96165063e-07
Iter: 1824 loss: 3.96308735e-07
Iter: 1825 loss: 3.96143889e-07
Iter: 1826 loss: 3.96100859e-07
Iter: 1827 loss: 3.96195929e-07
Iter: 1828 loss: 3.96085397e-07
Iter: 1829 loss: 3.96033272e-07
Iter: 1830 loss: 3.96118537e-07
Iter: 1831 loss: 3.95999024e-07
Iter: 1832 loss: 3.95938343e-07
Iter: 1833 loss: 3.95983307e-07
Iter: 1834 loss: 3.95906e-07
Iter: 1835 loss: 3.95854045e-07
Iter: 1836 loss: 3.96432341e-07
Iter: 1837 loss: 3.95845404e-07
Iter: 1838 loss: 3.95792938e-07
Iter: 1839 loss: 3.95796235e-07
Iter: 1840 loss: 3.95754512e-07
Iter: 1841 loss: 3.95694627e-07
Iter: 1842 loss: 3.96334514e-07
Iter: 1843 loss: 3.95683173e-07
Iter: 1844 loss: 3.95629399e-07
Iter: 1845 loss: 3.9559535e-07
Iter: 1846 loss: 3.95580258e-07
Iter: 1847 loss: 3.95525973e-07
Iter: 1848 loss: 3.96263033e-07
Iter: 1849 loss: 3.95521852e-07
Iter: 1850 loss: 3.95475e-07
Iter: 1851 loss: 3.95437723e-07
Iter: 1852 loss: 3.95407e-07
Iter: 1853 loss: 3.95355869e-07
Iter: 1854 loss: 3.9539367e-07
Iter: 1855 loss: 3.95318807e-07
Iter: 1856 loss: 3.95253352e-07
Iter: 1857 loss: 3.96098073e-07
Iter: 1858 loss: 3.95250254e-07
Iter: 1859 loss: 3.95174425e-07
Iter: 1860 loss: 3.95208474e-07
Iter: 1861 loss: 3.95148732e-07
Iter: 1862 loss: 3.95066905e-07
Iter: 1863 loss: 3.9525861e-07
Iter: 1864 loss: 3.95037489e-07
Iter: 1865 loss: 3.94962768e-07
Iter: 1866 loss: 3.94982351e-07
Iter: 1867 loss: 3.94910899e-07
Iter: 1868 loss: 3.94828362e-07
Iter: 1869 loss: 3.95298486e-07
Iter: 1870 loss: 3.94813156e-07
Iter: 1871 loss: 3.94734343e-07
Iter: 1872 loss: 3.94851043e-07
Iter: 1873 loss: 3.94693927e-07
Iter: 1874 loss: 3.94628103e-07
Iter: 1875 loss: 3.95379402e-07
Iter: 1876 loss: 3.94623754e-07
Iter: 1877 loss: 3.94575636e-07
Iter: 1878 loss: 3.94532151e-07
Iter: 1879 loss: 3.94523056e-07
Iter: 1880 loss: 3.94458539e-07
Iter: 1881 loss: 3.94759809e-07
Iter: 1882 loss: 3.94442452e-07
Iter: 1883 loss: 3.94372648e-07
Iter: 1884 loss: 3.94663346e-07
Iter: 1885 loss: 3.94355823e-07
Iter: 1886 loss: 3.94304323e-07
Iter: 1887 loss: 3.94208229e-07
Iter: 1888 loss: 3.94210844e-07
Iter: 1889 loss: 3.9415508e-07
Iter: 1890 loss: 3.94140528e-07
Iter: 1891 loss: 3.94105143e-07
Iter: 1892 loss: 3.94082178e-07
Iter: 1893 loss: 3.94047277e-07
Iter: 1894 loss: 3.93973949e-07
Iter: 1895 loss: 3.94386689e-07
Iter: 1896 loss: 3.93971078e-07
Iter: 1897 loss: 3.93906618e-07
Iter: 1898 loss: 3.9387794e-07
Iter: 1899 loss: 3.93860262e-07
Iter: 1900 loss: 3.93778635e-07
Iter: 1901 loss: 3.93792675e-07
Iter: 1902 loss: 3.93715908e-07
Iter: 1903 loss: 3.93618478e-07
Iter: 1904 loss: 3.94942e-07
Iter: 1905 loss: 3.93625072e-07
Iter: 1906 loss: 3.93551602e-07
Iter: 1907 loss: 3.93604239e-07
Iter: 1908 loss: 3.9350482e-07
Iter: 1909 loss: 3.93430298e-07
Iter: 1910 loss: 3.9425143e-07
Iter: 1911 loss: 3.93433254e-07
Iter: 1912 loss: 3.93380361e-07
Iter: 1913 loss: 3.93297114e-07
Iter: 1914 loss: 3.95382187e-07
Iter: 1915 loss: 3.93289497e-07
Iter: 1916 loss: 3.93202498e-07
Iter: 1917 loss: 3.93197837e-07
Iter: 1918 loss: 3.9314591e-07
Iter: 1919 loss: 3.93097508e-07
Iter: 1920 loss: 3.9308776e-07
Iter: 1921 loss: 3.93009856e-07
Iter: 1922 loss: 3.9300707e-07
Iter: 1923 loss: 3.92954746e-07
Iter: 1924 loss: 3.92922175e-07
Iter: 1925 loss: 3.92894378e-07
Iter: 1926 loss: 3.92858624e-07
Iter: 1927 loss: 3.9277927e-07
Iter: 1928 loss: 3.94138681e-07
Iter: 1929 loss: 3.92776599e-07
Iter: 1930 loss: 3.92746671e-07
Iter: 1931 loss: 3.92733227e-07
Iter: 1932 loss: 3.92704379e-07
Iter: 1933 loss: 3.92630284e-07
Iter: 1934 loss: 3.93781647e-07
Iter: 1935 loss: 3.9262153e-07
Iter: 1936 loss: 3.92539533e-07
Iter: 1937 loss: 3.92814059e-07
Iter: 1938 loss: 3.92517e-07
Iter: 1939 loss: 3.92448868e-07
Iter: 1940 loss: 3.92562242e-07
Iter: 1941 loss: 3.92406548e-07
Iter: 1942 loss: 3.92308323e-07
Iter: 1943 loss: 3.92961937e-07
Iter: 1944 loss: 3.92311051e-07
Iter: 1945 loss: 3.92230447e-07
Iter: 1946 loss: 3.92326541e-07
Iter: 1947 loss: 3.9220231e-07
Iter: 1948 loss: 3.9213478e-07
Iter: 1949 loss: 3.92042409e-07
Iter: 1950 loss: 3.92031865e-07
Iter: 1951 loss: 3.91939579e-07
Iter: 1952 loss: 3.92613856e-07
Iter: 1953 loss: 3.9192372e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.6
+ date
Mon Oct 26 15:10:27 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.6/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.6_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.6_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.6_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi -1 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.6_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23842c6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23842c6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23842900d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f238421a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f238434a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f238434a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f238434a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23841d0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23841d09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23841d0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23840db400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23840dfe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23840e9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23840e9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2384028b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f238407c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23840732f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f238401d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f237067d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2370624f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23706229d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23705fc7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f237058c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23705a38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23705a3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f237054e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23704f38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2370523840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2370523488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f237049f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23704ac840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2370457378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23704577b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2370488b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2370432c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f237040a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.19084437e-06
Iter: 2 loss: 7.55034034e-06
Iter: 3 loss: 5.44143177e-06
Iter: 4 loss: 4.93041898e-06
Iter: 5 loss: 4.94857477e-06
Iter: 6 loss: 4.52698259e-06
Iter: 7 loss: 4.18693571e-06
Iter: 8 loss: 6.86976455e-06
Iter: 9 loss: 4.16346074e-06
Iter: 10 loss: 3.94195695e-06
Iter: 11 loss: 4.01238958e-06
Iter: 12 loss: 3.78430195e-06
Iter: 13 loss: 3.63672e-06
Iter: 14 loss: 5.35988147e-06
Iter: 15 loss: 3.63444633e-06
Iter: 16 loss: 3.5046246e-06
Iter: 17 loss: 3.44329919e-06
Iter: 18 loss: 3.37986376e-06
Iter: 19 loss: 3.24178882e-06
Iter: 20 loss: 3.81125665e-06
Iter: 21 loss: 3.21180687e-06
Iter: 22 loss: 3.07567529e-06
Iter: 23 loss: 3.01171804e-06
Iter: 24 loss: 2.94480219e-06
Iter: 25 loss: 2.82363726e-06
Iter: 26 loss: 4.07065772e-06
Iter: 27 loss: 2.82011729e-06
Iter: 28 loss: 2.7027636e-06
Iter: 29 loss: 3.54653457e-06
Iter: 30 loss: 2.69258362e-06
Iter: 31 loss: 2.62820117e-06
Iter: 32 loss: 2.57626584e-06
Iter: 33 loss: 2.55701138e-06
Iter: 34 loss: 2.49098434e-06
Iter: 35 loss: 2.78258858e-06
Iter: 36 loss: 2.47784828e-06
Iter: 37 loss: 2.43155273e-06
Iter: 38 loss: 2.42983788e-06
Iter: 39 loss: 2.4069418e-06
Iter: 40 loss: 2.35853508e-06
Iter: 41 loss: 3.15017746e-06
Iter: 42 loss: 2.35713628e-06
Iter: 43 loss: 2.29144689e-06
Iter: 44 loss: 2.73398337e-06
Iter: 45 loss: 2.28492627e-06
Iter: 46 loss: 2.23832512e-06
Iter: 47 loss: 2.22771e-06
Iter: 48 loss: 2.19769163e-06
Iter: 49 loss: 2.13211661e-06
Iter: 50 loss: 2.50128824e-06
Iter: 51 loss: 2.12300415e-06
Iter: 52 loss: 2.05595052e-06
Iter: 53 loss: 2.09212476e-06
Iter: 54 loss: 2.01183957e-06
Iter: 55 loss: 1.96126e-06
Iter: 56 loss: 2.04923549e-06
Iter: 57 loss: 1.93876849e-06
Iter: 58 loss: 1.87290152e-06
Iter: 59 loss: 1.98823136e-06
Iter: 60 loss: 1.84386329e-06
Iter: 61 loss: 1.80280688e-06
Iter: 62 loss: 2.12564737e-06
Iter: 63 loss: 1.79990491e-06
Iter: 64 loss: 1.75478147e-06
Iter: 65 loss: 1.9539159e-06
Iter: 66 loss: 1.74581794e-06
Iter: 67 loss: 1.71563761e-06
Iter: 68 loss: 1.69675172e-06
Iter: 69 loss: 1.68469455e-06
Iter: 70 loss: 1.65343533e-06
Iter: 71 loss: 1.98153975e-06
Iter: 72 loss: 1.65263077e-06
Iter: 73 loss: 1.61277444e-06
Iter: 74 loss: 1.62646393e-06
Iter: 75 loss: 1.58457215e-06
Iter: 76 loss: 1.55445912e-06
Iter: 77 loss: 1.59082924e-06
Iter: 78 loss: 1.53858173e-06
Iter: 79 loss: 1.49391303e-06
Iter: 80 loss: 1.64863593e-06
Iter: 81 loss: 1.48214349e-06
Iter: 82 loss: 1.45822275e-06
Iter: 83 loss: 1.50527967e-06
Iter: 84 loss: 1.44840715e-06
Iter: 85 loss: 1.42204988e-06
Iter: 86 loss: 1.56411966e-06
Iter: 87 loss: 1.4180705e-06
Iter: 88 loss: 1.39552708e-06
Iter: 89 loss: 1.38443636e-06
Iter: 90 loss: 1.37369943e-06
Iter: 91 loss: 1.34634865e-06
Iter: 92 loss: 1.41857299e-06
Iter: 93 loss: 1.33714389e-06
Iter: 94 loss: 1.31022557e-06
Iter: 95 loss: 1.4102643e-06
Iter: 96 loss: 1.30359581e-06
Iter: 97 loss: 1.28370857e-06
Iter: 98 loss: 1.4624394e-06
Iter: 99 loss: 1.28276588e-06
Iter: 100 loss: 1.26160171e-06
Iter: 101 loss: 1.29827094e-06
Iter: 102 loss: 1.25222277e-06
Iter: 103 loss: 1.2331725e-06
Iter: 104 loss: 1.20353354e-06
Iter: 105 loss: 1.2031727e-06
Iter: 106 loss: 1.22059464e-06
Iter: 107 loss: 1.19206516e-06
Iter: 108 loss: 1.1830216e-06
Iter: 109 loss: 1.16047033e-06
Iter: 110 loss: 1.37528377e-06
Iter: 111 loss: 1.15738703e-06
Iter: 112 loss: 1.14397085e-06
Iter: 113 loss: 1.14369345e-06
Iter: 114 loss: 1.13362114e-06
Iter: 115 loss: 1.15338673e-06
Iter: 116 loss: 1.12946714e-06
Iter: 117 loss: 1.12088105e-06
Iter: 118 loss: 1.11659938e-06
Iter: 119 loss: 1.11251688e-06
Iter: 120 loss: 1.09958876e-06
Iter: 121 loss: 1.24950293e-06
Iter: 122 loss: 1.09940424e-06
Iter: 123 loss: 1.09245e-06
Iter: 124 loss: 1.07740584e-06
Iter: 125 loss: 1.30726949e-06
Iter: 126 loss: 1.07679466e-06
Iter: 127 loss: 1.06130142e-06
Iter: 128 loss: 1.21081121e-06
Iter: 129 loss: 1.06069956e-06
Iter: 130 loss: 1.04824369e-06
Iter: 131 loss: 1.057399e-06
Iter: 132 loss: 1.04059711e-06
Iter: 133 loss: 1.03193679e-06
Iter: 134 loss: 1.03093475e-06
Iter: 135 loss: 1.02376566e-06
Iter: 136 loss: 1.02146032e-06
Iter: 137 loss: 1.01730143e-06
Iter: 138 loss: 1.00887655e-06
Iter: 139 loss: 1.01827186e-06
Iter: 140 loss: 1.00426814e-06
Iter: 141 loss: 9.95652158e-07
Iter: 142 loss: 9.95538699e-07
Iter: 143 loss: 9.92367291e-07
Iter: 144 loss: 9.83532345e-07
Iter: 145 loss: 1.02909394e-06
Iter: 146 loss: 9.80673576e-07
Iter: 147 loss: 9.71196073e-07
Iter: 148 loss: 9.71043391e-07
Iter: 149 loss: 9.64288802e-07
Iter: 150 loss: 9.59472573e-07
Iter: 151 loss: 9.57090265e-07
Iter: 152 loss: 9.49213586e-07
Iter: 153 loss: 9.91119578e-07
Iter: 154 loss: 9.48036586e-07
Iter: 155 loss: 9.39000415e-07
Iter: 156 loss: 9.57595148e-07
Iter: 157 loss: 9.35444348e-07
Iter: 158 loss: 9.28048e-07
Iter: 159 loss: 9.22092795e-07
Iter: 160 loss: 9.19899264e-07
Iter: 161 loss: 9.09448204e-07
Iter: 162 loss: 9.62589183e-07
Iter: 163 loss: 9.07709705e-07
Iter: 164 loss: 8.98813482e-07
Iter: 165 loss: 9.536721e-07
Iter: 166 loss: 8.97758696e-07
Iter: 167 loss: 8.91105174e-07
Iter: 168 loss: 9.63676371e-07
Iter: 169 loss: 8.90965794e-07
Iter: 170 loss: 8.86274051e-07
Iter: 171 loss: 8.79153333e-07
Iter: 172 loss: 8.79040215e-07
Iter: 173 loss: 8.75748071e-07
Iter: 174 loss: 8.75028604e-07
Iter: 175 loss: 8.70724705e-07
Iter: 176 loss: 8.6542957e-07
Iter: 177 loss: 8.64964363e-07
Iter: 178 loss: 8.60281375e-07
Iter: 179 loss: 8.68753204e-07
Iter: 180 loss: 8.58287081e-07
Iter: 181 loss: 8.54336292e-07
Iter: 182 loss: 9.11507243e-07
Iter: 183 loss: 8.54332654e-07
Iter: 184 loss: 8.50995036e-07
Iter: 185 loss: 8.44675924e-07
Iter: 186 loss: 9.77919171e-07
Iter: 187 loss: 8.44644092e-07
Iter: 188 loss: 8.40580185e-07
Iter: 189 loss: 8.40553412e-07
Iter: 190 loss: 8.36347738e-07
Iter: 191 loss: 8.34151933e-07
Iter: 192 loss: 8.32222099e-07
Iter: 193 loss: 8.27724421e-07
Iter: 194 loss: 8.24052165e-07
Iter: 195 loss: 8.22737888e-07
Iter: 196 loss: 8.15794465e-07
Iter: 197 loss: 8.94922096e-07
Iter: 198 loss: 8.15683904e-07
Iter: 199 loss: 8.12037229e-07
Iter: 200 loss: 8.52963922e-07
Iter: 201 loss: 8.11956568e-07
Iter: 202 loss: 8.08293635e-07
Iter: 203 loss: 8.08722348e-07
Iter: 204 loss: 8.05465675e-07
Iter: 205 loss: 8.01229305e-07
Iter: 206 loss: 8.10971869e-07
Iter: 207 loss: 7.99641157e-07
Iter: 208 loss: 7.98264239e-07
Iter: 209 loss: 7.97627308e-07
Iter: 210 loss: 7.96263294e-07
Iter: 211 loss: 7.92263222e-07
Iter: 212 loss: 8.05106311e-07
Iter: 213 loss: 7.90284e-07
Iter: 214 loss: 7.86681085e-07
Iter: 215 loss: 7.86681369e-07
Iter: 216 loss: 7.83216706e-07
Iter: 217 loss: 7.9268392e-07
Iter: 218 loss: 7.82094219e-07
Iter: 219 loss: 7.79178094e-07
Iter: 220 loss: 7.75235549e-07
Iter: 221 loss: 7.75040803e-07
Iter: 222 loss: 7.70426425e-07
Iter: 223 loss: 7.70435e-07
Iter: 224 loss: 7.66807432e-07
Iter: 225 loss: 7.64185415e-07
Iter: 226 loss: 7.62964e-07
Iter: 227 loss: 7.58950932e-07
Iter: 228 loss: 7.63841058e-07
Iter: 229 loss: 7.56835277e-07
Iter: 230 loss: 7.52726351e-07
Iter: 231 loss: 7.7937932e-07
Iter: 232 loss: 7.52314349e-07
Iter: 233 loss: 7.49197625e-07
Iter: 234 loss: 7.98108431e-07
Iter: 235 loss: 7.49202059e-07
Iter: 236 loss: 7.47544277e-07
Iter: 237 loss: 7.46813498e-07
Iter: 238 loss: 7.45980856e-07
Iter: 239 loss: 7.43369299e-07
Iter: 240 loss: 7.50858362e-07
Iter: 241 loss: 7.42545581e-07
Iter: 242 loss: 7.38854283e-07
Iter: 243 loss: 7.51504729e-07
Iter: 244 loss: 7.37872597e-07
Iter: 245 loss: 7.36027516e-07
Iter: 246 loss: 7.31927116e-07
Iter: 247 loss: 7.88497232e-07
Iter: 248 loss: 7.3169906e-07
Iter: 249 loss: 7.29000931e-07
Iter: 250 loss: 7.28720295e-07
Iter: 251 loss: 7.26249596e-07
Iter: 252 loss: 7.27663632e-07
Iter: 253 loss: 7.24615177e-07
Iter: 254 loss: 7.22106449e-07
Iter: 255 loss: 7.23165272e-07
Iter: 256 loss: 7.20402909e-07
Iter: 257 loss: 7.18468243e-07
Iter: 258 loss: 7.1839986e-07
Iter: 259 loss: 7.17156695e-07
Iter: 260 loss: 7.13997565e-07
Iter: 261 loss: 7.41506e-07
Iter: 262 loss: 7.13506438e-07
Iter: 263 loss: 7.10629536e-07
Iter: 264 loss: 7.34048285e-07
Iter: 265 loss: 7.10446727e-07
Iter: 266 loss: 7.08110292e-07
Iter: 267 loss: 7.26288818e-07
Iter: 268 loss: 7.07960112e-07
Iter: 269 loss: 7.05325476e-07
Iter: 270 loss: 7.07189201e-07
Iter: 271 loss: 7.03696855e-07
Iter: 272 loss: 7.01493491e-07
Iter: 273 loss: 7.05957518e-07
Iter: 274 loss: 7.00571832e-07
Iter: 275 loss: 6.98682811e-07
Iter: 276 loss: 6.98663598e-07
Iter: 277 loss: 6.97310611e-07
Iter: 278 loss: 6.95193e-07
Iter: 279 loss: 6.9517273e-07
Iter: 280 loss: 6.93198e-07
Iter: 281 loss: 6.96499455e-07
Iter: 282 loss: 6.92330218e-07
Iter: 283 loss: 6.90810339e-07
Iter: 284 loss: 6.90795e-07
Iter: 285 loss: 6.89536705e-07
Iter: 286 loss: 6.87769102e-07
Iter: 287 loss: 6.87693955e-07
Iter: 288 loss: 6.86029352e-07
Iter: 289 loss: 6.96007078e-07
Iter: 290 loss: 6.85805276e-07
Iter: 291 loss: 6.83762778e-07
Iter: 292 loss: 6.8620551e-07
Iter: 293 loss: 6.82682185e-07
Iter: 294 loss: 6.80740413e-07
Iter: 295 loss: 6.78863785e-07
Iter: 296 loss: 6.78423248e-07
Iter: 297 loss: 6.75408899e-07
Iter: 298 loss: 6.97907524e-07
Iter: 299 loss: 6.75167371e-07
Iter: 300 loss: 6.73473323e-07
Iter: 301 loss: 6.73430804e-07
Iter: 302 loss: 6.72224132e-07
Iter: 303 loss: 6.6982e-07
Iter: 304 loss: 7.16117199e-07
Iter: 305 loss: 6.69795554e-07
Iter: 306 loss: 6.69428e-07
Iter: 307 loss: 6.68674602e-07
Iter: 308 loss: 6.67824111e-07
Iter: 309 loss: 6.67570134e-07
Iter: 310 loss: 6.67065819e-07
Iter: 311 loss: 6.65934692e-07
Iter: 312 loss: 6.64637128e-07
Iter: 313 loss: 6.6450923e-07
Iter: 314 loss: 6.63103378e-07
Iter: 315 loss: 6.78753679e-07
Iter: 316 loss: 6.63076605e-07
Iter: 317 loss: 6.61525519e-07
Iter: 318 loss: 6.63018454e-07
Iter: 319 loss: 6.60644e-07
Iter: 320 loss: 6.58844442e-07
Iter: 321 loss: 6.578897e-07
Iter: 322 loss: 6.57039152e-07
Iter: 323 loss: 6.5536e-07
Iter: 324 loss: 6.55332428e-07
Iter: 325 loss: 6.53759571e-07
Iter: 326 loss: 6.51518064e-07
Iter: 327 loss: 6.51458e-07
Iter: 328 loss: 6.49589651e-07
Iter: 329 loss: 6.53778216e-07
Iter: 330 loss: 6.48908099e-07
Iter: 331 loss: 6.48116611e-07
Iter: 332 loss: 6.47878323e-07
Iter: 333 loss: 6.46971785e-07
Iter: 334 loss: 6.46559897e-07
Iter: 335 loss: 6.46108504e-07
Iter: 336 loss: 6.45036607e-07
Iter: 337 loss: 6.48406399e-07
Iter: 338 loss: 6.44750912e-07
Iter: 339 loss: 6.43432031e-07
Iter: 340 loss: 6.50987147e-07
Iter: 341 loss: 6.43277417e-07
Iter: 342 loss: 6.42392308e-07
Iter: 343 loss: 6.40970313e-07
Iter: 344 loss: 6.40962185e-07
Iter: 345 loss: 6.39185771e-07
Iter: 346 loss: 6.42012139e-07
Iter: 347 loss: 6.38348638e-07
Iter: 348 loss: 6.36532604e-07
Iter: 349 loss: 6.60795195e-07
Iter: 350 loss: 6.36519417e-07
Iter: 351 loss: 6.35240724e-07
Iter: 352 loss: 6.35771e-07
Iter: 353 loss: 6.34335436e-07
Iter: 354 loss: 6.33176626e-07
Iter: 355 loss: 6.33727439e-07
Iter: 356 loss: 6.32399235e-07
Iter: 357 loss: 6.30868158e-07
Iter: 358 loss: 6.49393428e-07
Iter: 359 loss: 6.30845e-07
Iter: 360 loss: 6.30218324e-07
Iter: 361 loss: 6.28819407e-07
Iter: 362 loss: 6.48186585e-07
Iter: 363 loss: 6.28763132e-07
Iter: 364 loss: 6.27264512e-07
Iter: 365 loss: 6.40883172e-07
Iter: 366 loss: 6.27193231e-07
Iter: 367 loss: 6.26293797e-07
Iter: 368 loss: 6.26280723e-07
Iter: 369 loss: 6.25675511e-07
Iter: 370 loss: 6.24384825e-07
Iter: 371 loss: 6.45330715e-07
Iter: 372 loss: 6.24353675e-07
Iter: 373 loss: 6.23863684e-07
Iter: 374 loss: 6.23573783e-07
Iter: 375 loss: 6.22859432e-07
Iter: 376 loss: 6.21077788e-07
Iter: 377 loss: 6.36858317e-07
Iter: 378 loss: 6.20785613e-07
Iter: 379 loss: 6.19083323e-07
Iter: 380 loss: 6.30715e-07
Iter: 381 loss: 6.18888691e-07
Iter: 382 loss: 6.17808155e-07
Iter: 383 loss: 6.22504558e-07
Iter: 384 loss: 6.17579644e-07
Iter: 385 loss: 6.16394118e-07
Iter: 386 loss: 6.21177605e-07
Iter: 387 loss: 6.16095065e-07
Iter: 388 loss: 6.15280499e-07
Iter: 389 loss: 6.15109e-07
Iter: 390 loss: 6.14587293e-07
Iter: 391 loss: 6.13674615e-07
Iter: 392 loss: 6.23391259e-07
Iter: 393 loss: 6.13665691e-07
Iter: 394 loss: 6.12722e-07
Iter: 395 loss: 6.11290602e-07
Iter: 396 loss: 6.112748e-07
Iter: 397 loss: 6.09492758e-07
Iter: 398 loss: 6.13137047e-07
Iter: 399 loss: 6.08761638e-07
Iter: 400 loss: 6.07452762e-07
Iter: 401 loss: 6.24631241e-07
Iter: 402 loss: 6.07446282e-07
Iter: 403 loss: 6.06022468e-07
Iter: 404 loss: 6.07452421e-07
Iter: 405 loss: 6.05230412e-07
Iter: 406 loss: 6.04192223e-07
Iter: 407 loss: 6.0684647e-07
Iter: 408 loss: 6.03836497e-07
Iter: 409 loss: 6.02367777e-07
Iter: 410 loss: 6.09318704e-07
Iter: 411 loss: 6.02112095e-07
Iter: 412 loss: 6.0160221e-07
Iter: 413 loss: 6.0092782e-07
Iter: 414 loss: 6.00890871e-07
Iter: 415 loss: 5.99850523e-07
Iter: 416 loss: 6.02383238e-07
Iter: 417 loss: 5.99457849e-07
Iter: 418 loss: 5.98692623e-07
Iter: 419 loss: 5.98692623e-07
Iter: 420 loss: 5.98187114e-07
Iter: 421 loss: 5.97467647e-07
Iter: 422 loss: 5.97434337e-07
Iter: 423 loss: 5.96255e-07
Iter: 424 loss: 5.96531038e-07
Iter: 425 loss: 5.95388144e-07
Iter: 426 loss: 5.94306243e-07
Iter: 427 loss: 5.94293056e-07
Iter: 428 loss: 5.93687162e-07
Iter: 429 loss: 5.9237891e-07
Iter: 430 loss: 6.11385872e-07
Iter: 431 loss: 5.92317861e-07
Iter: 432 loss: 5.90951288e-07
Iter: 433 loss: 5.99560281e-07
Iter: 434 loss: 5.90802074e-07
Iter: 435 loss: 5.89959882e-07
Iter: 436 loss: 5.89952265e-07
Iter: 437 loss: 5.89471085e-07
Iter: 438 loss: 5.89018782e-07
Iter: 439 loss: 5.88919534e-07
Iter: 440 loss: 5.88627586e-07
Iter: 441 loss: 5.88534249e-07
Iter: 442 loss: 5.88202965e-07
Iter: 443 loss: 5.87309444e-07
Iter: 444 loss: 5.92742651e-07
Iter: 445 loss: 5.87046e-07
Iter: 446 loss: 5.86150122e-07
Iter: 447 loss: 5.89758486e-07
Iter: 448 loss: 5.8596072e-07
Iter: 449 loss: 5.85085274e-07
Iter: 450 loss: 5.91965431e-07
Iter: 451 loss: 5.85025532e-07
Iter: 452 loss: 5.84166798e-07
Iter: 453 loss: 5.85168209e-07
Iter: 454 loss: 5.83727683e-07
Iter: 455 loss: 5.82764073e-07
Iter: 456 loss: 5.83560791e-07
Iter: 457 loss: 5.82215193e-07
Iter: 458 loss: 5.8133719e-07
Iter: 459 loss: 5.86852877e-07
Iter: 460 loss: 5.81229187e-07
Iter: 461 loss: 5.80279391e-07
Iter: 462 loss: 5.81248344e-07
Iter: 463 loss: 5.79765e-07
Iter: 464 loss: 5.7915679e-07
Iter: 465 loss: 5.79086077e-07
Iter: 466 loss: 5.78674417e-07
Iter: 467 loss: 5.77935793e-07
Iter: 468 loss: 5.8695e-07
Iter: 469 loss: 5.77921924e-07
Iter: 470 loss: 5.77280048e-07
Iter: 471 loss: 5.80347546e-07
Iter: 472 loss: 5.77167611e-07
Iter: 473 loss: 5.76761295e-07
Iter: 474 loss: 5.76714172e-07
Iter: 475 loss: 5.7644462e-07
Iter: 476 loss: 5.75625336e-07
Iter: 477 loss: 5.78363029e-07
Iter: 478 loss: 5.75394381e-07
Iter: 479 loss: 5.74938838e-07
Iter: 480 loss: 5.7369931e-07
Iter: 481 loss: 5.82676876e-07
Iter: 482 loss: 5.73427883e-07
Iter: 483 loss: 5.72173064e-07
Iter: 484 loss: 5.721663e-07
Iter: 485 loss: 5.71420969e-07
Iter: 486 loss: 5.79196751e-07
Iter: 487 loss: 5.71423698e-07
Iter: 488 loss: 5.70875613e-07
Iter: 489 loss: 5.7054632e-07
Iter: 490 loss: 5.70324e-07
Iter: 491 loss: 5.69660301e-07
Iter: 492 loss: 5.7262173e-07
Iter: 493 loss: 5.69534336e-07
Iter: 494 loss: 5.68986081e-07
Iter: 495 loss: 5.72469446e-07
Iter: 496 loss: 5.68914629e-07
Iter: 497 loss: 5.68333405e-07
Iter: 498 loss: 5.67340066e-07
Iter: 499 loss: 5.6734109e-07
Iter: 500 loss: 5.66277208e-07
Iter: 501 loss: 5.69466e-07
Iter: 502 loss: 5.65945413e-07
Iter: 503 loss: 5.65694165e-07
Iter: 504 loss: 5.65447237e-07
Iter: 505 loss: 5.65042342e-07
Iter: 506 loss: 5.64306504e-07
Iter: 507 loss: 5.80855044e-07
Iter: 508 loss: 5.64289053e-07
Iter: 509 loss: 5.64033087e-07
Iter: 510 loss: 5.63895355e-07
Iter: 511 loss: 5.63549861e-07
Iter: 512 loss: 5.62836703e-07
Iter: 513 loss: 5.74568e-07
Iter: 514 loss: 5.62803166e-07
Iter: 515 loss: 5.62150092e-07
Iter: 516 loss: 5.62619107e-07
Iter: 517 loss: 5.61760089e-07
Iter: 518 loss: 5.61125148e-07
Iter: 519 loss: 5.68835958e-07
Iter: 520 loss: 5.61123159e-07
Iter: 521 loss: 5.60522722e-07
Iter: 522 loss: 5.62247351e-07
Iter: 523 loss: 5.6036157e-07
Iter: 524 loss: 5.59926e-07
Iter: 525 loss: 5.59709179e-07
Iter: 526 loss: 5.59505622e-07
Iter: 527 loss: 5.58710212e-07
Iter: 528 loss: 5.60222361e-07
Iter: 529 loss: 5.58345e-07
Iter: 530 loss: 5.57713065e-07
Iter: 531 loss: 5.5770181e-07
Iter: 532 loss: 5.57356e-07
Iter: 533 loss: 5.56578e-07
Iter: 534 loss: 5.67596317e-07
Iter: 535 loss: 5.56536747e-07
Iter: 536 loss: 5.55849681e-07
Iter: 537 loss: 5.63951232e-07
Iter: 538 loss: 5.55838767e-07
Iter: 539 loss: 5.55289489e-07
Iter: 540 loss: 5.60674039e-07
Iter: 541 loss: 5.55271072e-07
Iter: 542 loss: 5.54859923e-07
Iter: 543 loss: 5.54500616e-07
Iter: 544 loss: 5.54399e-07
Iter: 545 loss: 5.53941277e-07
Iter: 546 loss: 5.53941618e-07
Iter: 547 loss: 5.53691166e-07
Iter: 548 loss: 5.52971301e-07
Iter: 549 loss: 5.55257657e-07
Iter: 550 loss: 5.52616712e-07
Iter: 551 loss: 5.518271e-07
Iter: 552 loss: 5.56892701e-07
Iter: 553 loss: 5.51736491e-07
Iter: 554 loss: 5.51287371e-07
Iter: 555 loss: 5.51283563e-07
Iter: 556 loss: 5.50881282e-07
Iter: 557 loss: 5.50753555e-07
Iter: 558 loss: 5.50523964e-07
Iter: 559 loss: 5.50052391e-07
Iter: 560 loss: 5.50689094e-07
Iter: 561 loss: 5.49824449e-07
Iter: 562 loss: 5.49317633e-07
Iter: 563 loss: 5.53089421e-07
Iter: 564 loss: 5.49279889e-07
Iter: 565 loss: 5.4883418e-07
Iter: 566 loss: 5.49471508e-07
Iter: 567 loss: 5.48609364e-07
Iter: 568 loss: 5.47993523e-07
Iter: 569 loss: 5.47394052e-07
Iter: 570 loss: 5.47282639e-07
Iter: 571 loss: 5.46481033e-07
Iter: 572 loss: 5.53775521e-07
Iter: 573 loss: 5.46459773e-07
Iter: 574 loss: 5.45901798e-07
Iter: 575 loss: 5.53913424e-07
Iter: 576 loss: 5.45905323e-07
Iter: 577 loss: 5.45593821e-07
Iter: 578 loss: 5.45447733e-07
Iter: 579 loss: 5.45297439e-07
Iter: 580 loss: 5.44700072e-07
Iter: 581 loss: 5.46857109e-07
Iter: 582 loss: 5.44519594e-07
Iter: 583 loss: 5.44240947e-07
Iter: 584 loss: 5.4375721e-07
Iter: 585 loss: 5.43764941e-07
Iter: 586 loss: 5.43259148e-07
Iter: 587 loss: 5.45487126e-07
Iter: 588 loss: 5.43162855e-07
Iter: 589 loss: 5.4273994e-07
Iter: 590 loss: 5.47703848e-07
Iter: 591 loss: 5.42750456e-07
Iter: 592 loss: 5.42346299e-07
Iter: 593 loss: 5.41475913e-07
Iter: 594 loss: 5.55259419e-07
Iter: 595 loss: 5.41463919e-07
Iter: 596 loss: 5.40779922e-07
Iter: 597 loss: 5.46957722e-07
Iter: 598 loss: 5.40759572e-07
Iter: 599 loss: 5.40150381e-07
Iter: 600 loss: 5.42284795e-07
Iter: 601 loss: 5.3999679e-07
Iter: 602 loss: 5.39457687e-07
Iter: 603 loss: 5.4167856e-07
Iter: 604 loss: 5.39343318e-07
Iter: 605 loss: 5.38974518e-07
Iter: 606 loss: 5.38590825e-07
Iter: 607 loss: 5.38511927e-07
Iter: 608 loss: 5.38045128e-07
Iter: 609 loss: 5.38055e-07
Iter: 610 loss: 5.37645406e-07
Iter: 611 loss: 5.38874474e-07
Iter: 612 loss: 5.37539108e-07
Iter: 613 loss: 5.37321569e-07
Iter: 614 loss: 5.39133111e-07
Iter: 615 loss: 5.37315486e-07
Iter: 616 loss: 5.37073e-07
Iter: 617 loss: 5.36550488e-07
Iter: 618 loss: 5.43971964e-07
Iter: 619 loss: 5.36524112e-07
Iter: 620 loss: 5.3603253e-07
Iter: 621 loss: 5.35622235e-07
Iter: 622 loss: 5.35465801e-07
Iter: 623 loss: 5.34808e-07
Iter: 624 loss: 5.44300349e-07
Iter: 625 loss: 5.34811306e-07
Iter: 626 loss: 5.3420888e-07
Iter: 627 loss: 5.38167853e-07
Iter: 628 loss: 5.3414567e-07
Iter: 629 loss: 5.33750836e-07
Iter: 630 loss: 5.33316324e-07
Iter: 631 loss: 5.33252148e-07
Iter: 632 loss: 5.32899946e-07
Iter: 633 loss: 5.38263066e-07
Iter: 634 loss: 5.32908e-07
Iter: 635 loss: 5.3257213e-07
Iter: 636 loss: 5.32958325e-07
Iter: 637 loss: 5.32409786e-07
Iter: 638 loss: 5.32031e-07
Iter: 639 loss: 5.32567356e-07
Iter: 640 loss: 5.31852e-07
Iter: 641 loss: 5.31452372e-07
Iter: 642 loss: 5.32208333e-07
Iter: 643 loss: 5.31303328e-07
Iter: 644 loss: 5.30987e-07
Iter: 645 loss: 5.30999273e-07
Iter: 646 loss: 5.30706188e-07
Iter: 647 loss: 5.30328464e-07
Iter: 648 loss: 5.3029521e-07
Iter: 649 loss: 5.29846261e-07
Iter: 650 loss: 5.29836598e-07
Iter: 651 loss: 5.2962082e-07
Iter: 652 loss: 5.29004637e-07
Iter: 653 loss: 5.33235436e-07
Iter: 654 loss: 5.28845533e-07
Iter: 655 loss: 5.28181261e-07
Iter: 656 loss: 5.32025524e-07
Iter: 657 loss: 5.28080477e-07
Iter: 658 loss: 5.27714178e-07
Iter: 659 loss: 5.31854084e-07
Iter: 660 loss: 5.27706e-07
Iter: 661 loss: 5.27357201e-07
Iter: 662 loss: 5.28002488e-07
Iter: 663 loss: 5.27198949e-07
Iter: 664 loss: 5.26956057e-07
Iter: 665 loss: 5.26720441e-07
Iter: 666 loss: 5.26655185e-07
Iter: 667 loss: 5.26227097e-07
Iter: 668 loss: 5.29881959e-07
Iter: 669 loss: 5.26195493e-07
Iter: 670 loss: 5.25812084e-07
Iter: 671 loss: 5.26496251e-07
Iter: 672 loss: 5.25624387e-07
Iter: 673 loss: 5.25216592e-07
Iter: 674 loss: 5.24844154e-07
Iter: 675 loss: 5.2474104e-07
Iter: 676 loss: 5.24163966e-07
Iter: 677 loss: 5.323148e-07
Iter: 678 loss: 5.24165102e-07
Iter: 679 loss: 5.23840185e-07
Iter: 680 loss: 5.27036264e-07
Iter: 681 loss: 5.23810684e-07
Iter: 682 loss: 5.23611106e-07
Iter: 683 loss: 5.23674544e-07
Iter: 684 loss: 5.23466611e-07
Iter: 685 loss: 5.23036249e-07
Iter: 686 loss: 5.22960249e-07
Iter: 687 loss: 5.22650112e-07
Iter: 688 loss: 5.22382379e-07
Iter: 689 loss: 5.22176833e-07
Iter: 690 loss: 5.22089636e-07
Iter: 691 loss: 5.21686275e-07
Iter: 692 loss: 5.24637244e-07
Iter: 693 loss: 5.2164205e-07
Iter: 694 loss: 5.21352035e-07
Iter: 695 loss: 5.24669531e-07
Iter: 696 loss: 5.21344873e-07
Iter: 697 loss: 5.21124321e-07
Iter: 698 loss: 5.20655192e-07
Iter: 699 loss: 5.28108899e-07
Iter: 700 loss: 5.2064172e-07
Iter: 701 loss: 5.20171511e-07
Iter: 702 loss: 5.2289522e-07
Iter: 703 loss: 5.20105914e-07
Iter: 704 loss: 5.19603532e-07
Iter: 705 loss: 5.22093842e-07
Iter: 706 loss: 5.19494961e-07
Iter: 707 loss: 5.19075911e-07
Iter: 708 loss: 5.19824084e-07
Iter: 709 loss: 5.18910724e-07
Iter: 710 loss: 5.18604054e-07
Iter: 711 loss: 5.19719435e-07
Iter: 712 loss: 5.18524359e-07
Iter: 713 loss: 5.18165052e-07
Iter: 714 loss: 5.20650588e-07
Iter: 715 loss: 5.18150728e-07
Iter: 716 loss: 5.17868898e-07
Iter: 717 loss: 5.18399e-07
Iter: 718 loss: 5.17768626e-07
Iter: 719 loss: 5.17554497e-07
Iter: 720 loss: 5.19856e-07
Iter: 721 loss: 5.17563308e-07
Iter: 722 loss: 5.17400736e-07
Iter: 723 loss: 5.17045805e-07
Iter: 724 loss: 5.19850232e-07
Iter: 725 loss: 5.16958835e-07
Iter: 726 loss: 5.16566729e-07
Iter: 727 loss: 5.17378112e-07
Iter: 728 loss: 5.16412854e-07
Iter: 729 loss: 5.16076057e-07
Iter: 730 loss: 5.16058776e-07
Iter: 731 loss: 5.15750855e-07
Iter: 732 loss: 5.15648139e-07
Iter: 733 loss: 5.15471811e-07
Iter: 734 loss: 5.15025135e-07
Iter: 735 loss: 5.14751946e-07
Iter: 736 loss: 5.14592102e-07
Iter: 737 loss: 5.1444988e-07
Iter: 738 loss: 5.14335909e-07
Iter: 739 loss: 5.14135195e-07
Iter: 740 loss: 5.13974328e-07
Iter: 741 loss: 5.1391919e-07
Iter: 742 loss: 5.13565567e-07
Iter: 743 loss: 5.14243311e-07
Iter: 744 loss: 5.13437385e-07
Iter: 745 loss: 5.13173745e-07
Iter: 746 loss: 5.1663676e-07
Iter: 747 loss: 5.131634e-07
Iter: 748 loss: 5.12932502e-07
Iter: 749 loss: 5.13155669e-07
Iter: 750 loss: 5.12781071e-07
Iter: 751 loss: 5.12522831e-07
Iter: 752 loss: 5.14150031e-07
Iter: 753 loss: 5.12497252e-07
Iter: 754 loss: 5.12248221e-07
Iter: 755 loss: 5.12076213e-07
Iter: 756 loss: 5.11988389e-07
Iter: 757 loss: 5.11657731e-07
Iter: 758 loss: 5.11052804e-07
Iter: 759 loss: 5.25689245e-07
Iter: 760 loss: 5.11055134e-07
Iter: 761 loss: 5.10450718e-07
Iter: 762 loss: 5.16338901e-07
Iter: 763 loss: 5.10407858e-07
Iter: 764 loss: 5.09988695e-07
Iter: 765 loss: 5.09988e-07
Iter: 766 loss: 5.09779909e-07
Iter: 767 loss: 5.09511324e-07
Iter: 768 loss: 5.09478753e-07
Iter: 769 loss: 5.09221195e-07
Iter: 770 loss: 5.10288601e-07
Iter: 771 loss: 5.09134168e-07
Iter: 772 loss: 5.08843073e-07
Iter: 773 loss: 5.11203268e-07
Iter: 774 loss: 5.08822666e-07
Iter: 775 loss: 5.08577386e-07
Iter: 776 loss: 5.08444145e-07
Iter: 777 loss: 5.08315907e-07
Iter: 778 loss: 5.08022367e-07
Iter: 779 loss: 5.10258189e-07
Iter: 780 loss: 5.08010316e-07
Iter: 781 loss: 5.0771223e-07
Iter: 782 loss: 5.08395203e-07
Iter: 783 loss: 5.07604227e-07
Iter: 784 loss: 5.07305572e-07
Iter: 785 loss: 5.08506446e-07
Iter: 786 loss: 5.07254413e-07
Iter: 787 loss: 5.0698975e-07
Iter: 788 loss: 5.08075686e-07
Iter: 789 loss: 5.06946435e-07
Iter: 790 loss: 5.06740435e-07
Iter: 791 loss: 5.06274432e-07
Iter: 792 loss: 5.1325253e-07
Iter: 793 loss: 5.06262325e-07
Iter: 794 loss: 5.05856406e-07
Iter: 795 loss: 5.06945753e-07
Iter: 796 loss: 5.05722426e-07
Iter: 797 loss: 5.05409901e-07
Iter: 798 loss: 5.10591121e-07
Iter: 799 loss: 5.05414732e-07
Iter: 800 loss: 5.05089361e-07
Iter: 801 loss: 5.05927e-07
Iter: 802 loss: 5.04994205e-07
Iter: 803 loss: 5.04783316e-07
Iter: 804 loss: 5.04327033e-07
Iter: 805 loss: 5.09933557e-07
Iter: 806 loss: 5.04276386e-07
Iter: 807 loss: 5.04037303e-07
Iter: 808 loss: 5.03965168e-07
Iter: 809 loss: 5.0366873e-07
Iter: 810 loss: 5.03547426e-07
Iter: 811 loss: 5.03398837e-07
Iter: 812 loss: 5.03043566e-07
Iter: 813 loss: 5.05242497e-07
Iter: 814 loss: 5.03012245e-07
Iter: 815 loss: 5.02796524e-07
Iter: 816 loss: 5.05786204e-07
Iter: 817 loss: 5.0279607e-07
Iter: 818 loss: 5.02627131e-07
Iter: 819 loss: 5.0244563e-07
Iter: 820 loss: 5.02436251e-07
Iter: 821 loss: 5.02134526e-07
Iter: 822 loss: 5.04355285e-07
Iter: 823 loss: 5.02117359e-07
Iter: 824 loss: 5.01898569e-07
Iter: 825 loss: 5.01751799e-07
Iter: 826 loss: 5.01666932e-07
Iter: 827 loss: 5.01380214e-07
Iter: 828 loss: 5.01098043e-07
Iter: 829 loss: 5.01045292e-07
Iter: 830 loss: 5.00649094e-07
Iter: 831 loss: 5.04276386e-07
Iter: 832 loss: 5.00624083e-07
Iter: 833 loss: 5.0036e-07
Iter: 834 loss: 5.00353508e-07
Iter: 835 loss: 5.00172746e-07
Iter: 836 loss: 4.99726298e-07
Iter: 837 loss: 5.0325167e-07
Iter: 838 loss: 4.99639668e-07
Iter: 839 loss: 4.99218515e-07
Iter: 840 loss: 5.0361723e-07
Iter: 841 loss: 4.99210898e-07
Iter: 842 loss: 4.98906672e-07
Iter: 843 loss: 5.02110822e-07
Iter: 844 loss: 4.98884447e-07
Iter: 845 loss: 4.98645647e-07
Iter: 846 loss: 4.98575446e-07
Iter: 847 loss: 4.98424129e-07
Iter: 848 loss: 4.98229156e-07
Iter: 849 loss: 5.01530053e-07
Iter: 850 loss: 4.98223812e-07
Iter: 851 loss: 4.98002635e-07
Iter: 852 loss: 4.97852284e-07
Iter: 853 loss: 4.97760652e-07
Iter: 854 loss: 4.97545216e-07
Iter: 855 loss: 4.97539304e-07
Iter: 856 loss: 4.97365534e-07
Iter: 857 loss: 4.97339101e-07
Iter: 858 loss: 4.97234282e-07
Iter: 859 loss: 4.97034875e-07
Iter: 860 loss: 4.96771236e-07
Iter: 861 loss: 4.96719508e-07
Iter: 862 loss: 4.96404823e-07
Iter: 863 loss: 4.97119515e-07
Iter: 864 loss: 4.96280109e-07
Iter: 865 loss: 4.95946836e-07
Iter: 866 loss: 4.99295766e-07
Iter: 867 loss: 4.95904203e-07
Iter: 868 loss: 4.9562334e-07
Iter: 869 loss: 4.96576831e-07
Iter: 870 loss: 4.95529434e-07
Iter: 871 loss: 4.95343897e-07
Iter: 872 loss: 4.94900689e-07
Iter: 873 loss: 5.00423e-07
Iter: 874 loss: 4.9486e-07
Iter: 875 loss: 4.9447226e-07
Iter: 876 loss: 4.94477206e-07
Iter: 877 loss: 4.94061226e-07
Iter: 878 loss: 4.95219524e-07
Iter: 879 loss: 4.9392645e-07
Iter: 880 loss: 4.93608297e-07
Iter: 881 loss: 4.94107e-07
Iter: 882 loss: 4.93474431e-07
Iter: 883 loss: 4.9306675e-07
Iter: 884 loss: 4.96991902e-07
Iter: 885 loss: 4.93070218e-07
Iter: 886 loss: 4.92850518e-07
Iter: 887 loss: 4.93100117e-07
Iter: 888 loss: 4.92750701e-07
Iter: 889 loss: 4.92520769e-07
Iter: 890 loss: 4.93905418e-07
Iter: 891 loss: 4.92491836e-07
Iter: 892 loss: 4.92342565e-07
Iter: 893 loss: 4.9214e-07
Iter: 894 loss: 4.92124798e-07
Iter: 895 loss: 4.91847743e-07
Iter: 896 loss: 4.91846663e-07
Iter: 897 loss: 4.91603259e-07
Iter: 898 loss: 4.91194101e-07
Iter: 899 loss: 4.93009509e-07
Iter: 900 loss: 4.91129867e-07
Iter: 901 loss: 4.90842353e-07
Iter: 902 loss: 4.95127267e-07
Iter: 903 loss: 4.90838772e-07
Iter: 904 loss: 4.9057968e-07
Iter: 905 loss: 4.9013164e-07
Iter: 906 loss: 5.0051824e-07
Iter: 907 loss: 4.90132834e-07
Iter: 908 loss: 4.89766137e-07
Iter: 909 loss: 4.90175637e-07
Iter: 910 loss: 4.89583897e-07
Iter: 911 loss: 4.89215608e-07
Iter: 912 loss: 4.89204e-07
Iter: 913 loss: 4.88942419e-07
Iter: 914 loss: 4.89339925e-07
Iter: 915 loss: 4.88812759e-07
Iter: 916 loss: 4.88653029e-07
Iter: 917 loss: 4.90851392e-07
Iter: 918 loss: 4.88639785e-07
Iter: 919 loss: 4.88485512e-07
Iter: 920 loss: 4.88394903e-07
Iter: 921 loss: 4.88316118e-07
Iter: 922 loss: 4.88158321e-07
Iter: 923 loss: 4.90205e-07
Iter: 924 loss: 4.88155081e-07
Iter: 925 loss: 4.8799825e-07
Iter: 926 loss: 4.876606e-07
Iter: 927 loss: 4.93057314e-07
Iter: 928 loss: 4.87646503e-07
Iter: 929 loss: 4.87308853e-07
Iter: 930 loss: 4.88886258e-07
Iter: 931 loss: 4.8725633e-07
Iter: 932 loss: 4.86946419e-07
Iter: 933 loss: 4.86891736e-07
Iter: 934 loss: 4.86664135e-07
Iter: 935 loss: 4.86445117e-07
Iter: 936 loss: 4.86427098e-07
Iter: 937 loss: 4.86192107e-07
Iter: 938 loss: 4.8623815e-07
Iter: 939 loss: 4.86002932e-07
Iter: 940 loss: 4.85763223e-07
Iter: 941 loss: 4.85573082e-07
Iter: 942 loss: 4.85493274e-07
Iter: 943 loss: 4.85207124e-07
Iter: 944 loss: 4.87466764e-07
Iter: 945 loss: 4.85172222e-07
Iter: 946 loss: 4.84994871e-07
Iter: 947 loss: 4.87535544e-07
Iter: 948 loss: 4.8498589e-07
Iter: 949 loss: 4.84866632e-07
Iter: 950 loss: 4.84801944e-07
Iter: 951 loss: 4.84764087e-07
Iter: 952 loss: 4.84529039e-07
Iter: 953 loss: 4.86496504e-07
Iter: 954 loss: 4.84524605e-07
Iter: 955 loss: 4.84375278e-07
Iter: 956 loss: 4.84220266e-07
Iter: 957 loss: 4.84201223e-07
Iter: 958 loss: 4.83864255e-07
Iter: 959 loss: 4.8516e-07
Iter: 960 loss: 4.83781093e-07
Iter: 961 loss: 4.83600559e-07
Iter: 962 loss: 4.83364204e-07
Iter: 963 loss: 4.83365227e-07
Iter: 964 loss: 4.82964765e-07
Iter: 965 loss: 4.83695e-07
Iter: 966 loss: 4.82816745e-07
Iter: 967 loss: 4.8256652e-07
Iter: 968 loss: 4.82574592e-07
Iter: 969 loss: 4.82412133e-07
Iter: 970 loss: 4.83113865e-07
Iter: 971 loss: 4.82372286e-07
Iter: 972 loss: 4.82221822e-07
Iter: 973 loss: 4.81987797e-07
Iter: 974 loss: 4.81993766e-07
Iter: 975 loss: 4.81706195e-07
Iter: 976 loss: 4.82044413e-07
Iter: 977 loss: 4.81559e-07
Iter: 978 loss: 4.81364737e-07
Iter: 979 loss: 4.81356892e-07
Iter: 980 loss: 4.81164875e-07
Iter: 981 loss: 4.81195116e-07
Iter: 982 loss: 4.81023278e-07
Iter: 983 loss: 4.80810741e-07
Iter: 984 loss: 4.82947485e-07
Iter: 985 loss: 4.80797837e-07
Iter: 986 loss: 4.80574897e-07
Iter: 987 loss: 4.80340589e-07
Iter: 988 loss: 4.80327571e-07
Iter: 989 loss: 4.80133338e-07
Iter: 990 loss: 4.8010736e-07
Iter: 991 loss: 4.79977871e-07
Iter: 992 loss: 4.79684957e-07
Iter: 993 loss: 4.83301e-07
Iter: 994 loss: 4.7966796e-07
Iter: 995 loss: 4.79443429e-07
Iter: 996 loss: 4.8129516e-07
Iter: 997 loss: 4.79406879e-07
Iter: 998 loss: 4.79246523e-07
Iter: 999 loss: 4.79457526e-07
Iter: 1000 loss: 4.79150344e-07
Iter: 1001 loss: 4.7894207e-07
Iter: 1002 loss: 4.81029417e-07
Iter: 1003 loss: 4.78947356e-07
Iter: 1004 loss: 4.78764377e-07
Iter: 1005 loss: 4.78540187e-07
Iter: 1006 loss: 4.7854e-07
Iter: 1007 loss: 4.78230504e-07
Iter: 1008 loss: 4.78463789e-07
Iter: 1009 loss: 4.78032348e-07
Iter: 1010 loss: 4.77786e-07
Iter: 1011 loss: 4.81778841e-07
Iter: 1012 loss: 4.77791218e-07
Iter: 1013 loss: 4.77553272e-07
Iter: 1014 loss: 4.78405525e-07
Iter: 1015 loss: 4.77484036e-07
Iter: 1016 loss: 4.77297817e-07
Iter: 1017 loss: 4.77631943e-07
Iter: 1018 loss: 4.7720232e-07
Iter: 1019 loss: 4.76927653e-07
Iter: 1020 loss: 4.78003528e-07
Iter: 1021 loss: 4.76881951e-07
Iter: 1022 loss: 4.76778212e-07
Iter: 1023 loss: 4.77366257e-07
Iter: 1024 loss: 4.76738251e-07
Iter: 1025 loss: 4.76589207e-07
Iter: 1026 loss: 4.76402192e-07
Iter: 1027 loss: 4.76384599e-07
Iter: 1028 loss: 4.76166122e-07
Iter: 1029 loss: 4.76431e-07
Iter: 1030 loss: 4.76071619e-07
Iter: 1031 loss: 4.75798458e-07
Iter: 1032 loss: 4.75863402e-07
Iter: 1033 loss: 4.75608147e-07
Iter: 1034 loss: 4.75362498e-07
Iter: 1035 loss: 4.75371223e-07
Iter: 1036 loss: 4.75172442e-07
Iter: 1037 loss: 4.75676416e-07
Iter: 1038 loss: 4.75078764e-07
Iter: 1039 loss: 4.74895728e-07
Iter: 1040 loss: 4.74587068e-07
Iter: 1041 loss: 4.74584937e-07
Iter: 1042 loss: 4.74308e-07
Iter: 1043 loss: 4.75248044e-07
Iter: 1044 loss: 4.74226056e-07
Iter: 1045 loss: 4.74025e-07
Iter: 1046 loss: 4.74011131e-07
Iter: 1047 loss: 4.73910916e-07
Iter: 1048 loss: 4.74204e-07
Iter: 1049 loss: 4.73842789e-07
Iter: 1050 loss: 4.73748088e-07
Iter: 1051 loss: 4.74565212e-07
Iter: 1052 loss: 4.73729045e-07
Iter: 1053 loss: 4.73604729e-07
Iter: 1054 loss: 4.73414104e-07
Iter: 1055 loss: 4.78318441e-07
Iter: 1056 loss: 4.73407397e-07
Iter: 1057 loss: 4.7322925e-07
Iter: 1058 loss: 4.73228e-07
Iter: 1059 loss: 4.73127102e-07
Iter: 1060 loss: 4.7286602e-07
Iter: 1061 loss: 4.74683276e-07
Iter: 1062 loss: 4.72815458e-07
Iter: 1063 loss: 4.72493866e-07
Iter: 1064 loss: 4.73921432e-07
Iter: 1065 loss: 4.72433271e-07
Iter: 1066 loss: 4.72158433e-07
Iter: 1067 loss: 4.7319736e-07
Iter: 1068 loss: 4.72083656e-07
Iter: 1069 loss: 4.71861284e-07
Iter: 1070 loss: 4.7504497e-07
Iter: 1071 loss: 4.71859494e-07
Iter: 1072 loss: 4.71738701e-07
Iter: 1073 loss: 4.71622116e-07
Iter: 1074 loss: 4.71596479e-07
Iter: 1075 loss: 4.7139585e-07
Iter: 1076 loss: 4.71639169e-07
Iter: 1077 loss: 4.71291344e-07
Iter: 1078 loss: 4.71106233e-07
Iter: 1079 loss: 4.7140702e-07
Iter: 1080 loss: 4.71036e-07
Iter: 1081 loss: 4.70785892e-07
Iter: 1082 loss: 4.72560316e-07
Iter: 1083 loss: 4.70737348e-07
Iter: 1084 loss: 4.70583643e-07
Iter: 1085 loss: 4.72086981e-07
Iter: 1086 loss: 4.70572616e-07
Iter: 1087 loss: 4.70409276e-07
Iter: 1088 loss: 4.70214871e-07
Iter: 1089 loss: 4.70204839e-07
Iter: 1090 loss: 4.69948702e-07
Iter: 1091 loss: 4.72171166e-07
Iter: 1092 loss: 4.69979966e-07
Iter: 1093 loss: 4.69748528e-07
Iter: 1094 loss: 4.69615799e-07
Iter: 1095 loss: 4.69523712e-07
Iter: 1096 loss: 4.69273573e-07
Iter: 1097 loss: 4.69277637e-07
Iter: 1098 loss: 4.69085364e-07
Iter: 1099 loss: 4.68857877e-07
Iter: 1100 loss: 4.7063e-07
Iter: 1101 loss: 4.6885387e-07
Iter: 1102 loss: 4.68676717e-07
Iter: 1103 loss: 4.69731958e-07
Iter: 1104 loss: 4.68653781e-07
Iter: 1105 loss: 4.68486775e-07
Iter: 1106 loss: 4.6939914e-07
Iter: 1107 loss: 4.68459916e-07
Iter: 1108 loss: 4.68351857e-07
Iter: 1109 loss: 4.68199687e-07
Iter: 1110 loss: 4.68189057e-07
Iter: 1111 loss: 4.67968476e-07
Iter: 1112 loss: 4.67979675e-07
Iter: 1113 loss: 4.67792631e-07
Iter: 1114 loss: 4.67560312e-07
Iter: 1115 loss: 4.67554656e-07
Iter: 1116 loss: 4.67366817e-07
Iter: 1117 loss: 4.67713107e-07
Iter: 1118 loss: 4.67288714e-07
Iter: 1119 loss: 4.67089023e-07
Iter: 1120 loss: 4.68315761e-07
Iter: 1121 loss: 4.67070976e-07
Iter: 1122 loss: 4.66866112e-07
Iter: 1123 loss: 4.66693763e-07
Iter: 1124 loss: 4.66633082e-07
Iter: 1125 loss: 4.66515957e-07
Iter: 1126 loss: 4.6651e-07
Iter: 1127 loss: 4.66413923e-07
Iter: 1128 loss: 4.66180154e-07
Iter: 1129 loss: 4.68345064e-07
Iter: 1130 loss: 4.66159207e-07
Iter: 1131 loss: 4.65952155e-07
Iter: 1132 loss: 4.66616314e-07
Iter: 1133 loss: 4.65890679e-07
Iter: 1134 loss: 4.65657308e-07
Iter: 1135 loss: 4.66392549e-07
Iter: 1136 loss: 4.65562209e-07
Iter: 1137 loss: 4.65419362e-07
Iter: 1138 loss: 4.65426183e-07
Iter: 1139 loss: 4.65248263e-07
Iter: 1140 loss: 4.65129801e-07
Iter: 1141 loss: 4.65081939e-07
Iter: 1142 loss: 4.64826599e-07
Iter: 1143 loss: 4.65384318e-07
Iter: 1144 loss: 4.64742442e-07
Iter: 1145 loss: 4.64536697e-07
Iter: 1146 loss: 4.6442517e-07
Iter: 1147 loss: 4.6433837e-07
Iter: 1148 loss: 4.64234972e-07
Iter: 1149 loss: 4.64162525e-07
Iter: 1150 loss: 4.64062083e-07
Iter: 1151 loss: 4.64032865e-07
Iter: 1152 loss: 4.63973663e-07
Iter: 1153 loss: 4.63792702e-07
Iter: 1154 loss: 4.64880429e-07
Iter: 1155 loss: 4.63776871e-07
Iter: 1156 loss: 4.63653322e-07
Iter: 1157 loss: 4.63681857e-07
Iter: 1158 loss: 4.63573173e-07
Iter: 1159 loss: 4.63397782e-07
Iter: 1160 loss: 4.64261262e-07
Iter: 1161 loss: 4.63357935e-07
Iter: 1162 loss: 4.63250103e-07
Iter: 1163 loss: 4.62981291e-07
Iter: 1164 loss: 4.66580701e-07
Iter: 1165 loss: 4.62964039e-07
Iter: 1166 loss: 4.62690025e-07
Iter: 1167 loss: 4.62947412e-07
Iter: 1168 loss: 4.62523445e-07
Iter: 1169 loss: 4.62237381e-07
Iter: 1170 loss: 4.64931247e-07
Iter: 1171 loss: 4.62226353e-07
Iter: 1172 loss: 4.62031409e-07
Iter: 1173 loss: 4.62968842e-07
Iter: 1174 loss: 4.61990822e-07
Iter: 1175 loss: 4.61757679e-07
Iter: 1176 loss: 4.63112656e-07
Iter: 1177 loss: 4.61729087e-07
Iter: 1178 loss: 4.61590304e-07
Iter: 1179 loss: 4.61477271e-07
Iter: 1180 loss: 4.61452203e-07
Iter: 1181 loss: 4.61250579e-07
Iter: 1182 loss: 4.6169464e-07
Iter: 1183 loss: 4.61178303e-07
Iter: 1184 loss: 4.60982392e-07
Iter: 1185 loss: 4.63201104e-07
Iter: 1186 loss: 4.60961132e-07
Iter: 1187 loss: 4.607883e-07
Iter: 1188 loss: 4.6143083e-07
Iter: 1189 loss: 4.60761896e-07
Iter: 1190 loss: 4.60623568e-07
Iter: 1191 loss: 4.61045886e-07
Iter: 1192 loss: 4.60591082e-07
Iter: 1193 loss: 4.60443601e-07
Iter: 1194 loss: 4.60458864e-07
Iter: 1195 loss: 4.60328209e-07
Iter: 1196 loss: 4.60193093e-07
Iter: 1197 loss: 4.61799601e-07
Iter: 1198 loss: 4.60199544e-07
Iter: 1199 loss: 4.60087165e-07
Iter: 1200 loss: 4.59827106e-07
Iter: 1201 loss: 4.63352251e-07
Iter: 1202 loss: 4.59808859e-07
Iter: 1203 loss: 4.59567161e-07
Iter: 1204 loss: 4.59876105e-07
Iter: 1205 loss: 4.59470812e-07
Iter: 1206 loss: 4.59177471e-07
Iter: 1207 loss: 4.60688568e-07
Iter: 1208 loss: 4.59128017e-07
Iter: 1209 loss: 4.58884358e-07
Iter: 1210 loss: 4.59175084e-07
Iter: 1211 loss: 4.58757711e-07
Iter: 1212 loss: 4.58740487e-07
Iter: 1213 loss: 4.58643626e-07
Iter: 1214 loss: 4.58555405e-07
Iter: 1215 loss: 4.58331e-07
Iter: 1216 loss: 4.59338452e-07
Iter: 1217 loss: 4.58276844e-07
Iter: 1218 loss: 4.58100203e-07
Iter: 1219 loss: 4.58100033e-07
Iter: 1220 loss: 4.57981969e-07
Iter: 1221 loss: 4.59966316e-07
Iter: 1222 loss: 4.57982793e-07
Iter: 1223 loss: 4.57886472e-07
Iter: 1224 loss: 4.57809307e-07
Iter: 1225 loss: 4.57781482e-07
Iter: 1226 loss: 4.57617944e-07
Iter: 1227 loss: 4.5845178e-07
Iter: 1228 loss: 4.57595149e-07
Iter: 1229 loss: 4.57487204e-07
Iter: 1230 loss: 4.57540722e-07
Iter: 1231 loss: 4.57427376e-07
Iter: 1232 loss: 4.57264889e-07
Iter: 1233 loss: 4.57671518e-07
Iter: 1234 loss: 4.57200258e-07
Iter: 1235 loss: 4.57035156e-07
Iter: 1236 loss: 4.57008525e-07
Iter: 1237 loss: 4.56900608e-07
Iter: 1238 loss: 4.56687246e-07
Iter: 1239 loss: 4.56804457e-07
Iter: 1240 loss: 4.56556563e-07
Iter: 1241 loss: 4.56327825e-07
Iter: 1242 loss: 4.57048628e-07
Iter: 1243 loss: 4.56266548e-07
Iter: 1244 loss: 4.56038066e-07
Iter: 1245 loss: 4.56747529e-07
Iter: 1246 loss: 4.55983866e-07
Iter: 1247 loss: 4.55892405e-07
Iter: 1248 loss: 4.55856309e-07
Iter: 1249 loss: 4.55772124e-07
Iter: 1250 loss: 4.5556942e-07
Iter: 1251 loss: 4.58801452e-07
Iter: 1252 loss: 4.55550605e-07
Iter: 1253 loss: 4.55337187e-07
Iter: 1254 loss: 4.55770476e-07
Iter: 1255 loss: 4.55238421e-07
Iter: 1256 loss: 4.55148779e-07
Iter: 1257 loss: 4.55111433e-07
Iter: 1258 loss: 4.55020569e-07
Iter: 1259 loss: 4.54863681e-07
Iter: 1260 loss: 4.58565694e-07
Iter: 1261 loss: 4.54859332e-07
Iter: 1262 loss: 4.54646909e-07
Iter: 1263 loss: 4.56063731e-07
Iter: 1264 loss: 4.54637075e-07
Iter: 1265 loss: 4.5450281e-07
Iter: 1266 loss: 4.54578327e-07
Iter: 1267 loss: 4.54413339e-07
Iter: 1268 loss: 4.54228143e-07
Iter: 1269 loss: 4.54588928e-07
Iter: 1270 loss: 4.54132817e-07
Iter: 1271 loss: 4.53979879e-07
Iter: 1272 loss: 4.54148903e-07
Iter: 1273 loss: 4.53914822e-07
Iter: 1274 loss: 4.53757593e-07
Iter: 1275 loss: 4.53559352e-07
Iter: 1276 loss: 4.53523285e-07
Iter: 1277 loss: 4.53303869e-07
Iter: 1278 loss: 4.56834897e-07
Iter: 1279 loss: 4.53301425e-07
Iter: 1280 loss: 4.53133453e-07
Iter: 1281 loss: 4.532329e-07
Iter: 1282 loss: 4.53009022e-07
Iter: 1283 loss: 4.52862025e-07
Iter: 1284 loss: 4.52864015e-07
Iter: 1285 loss: 4.52731e-07
Iter: 1286 loss: 4.5244775e-07
Iter: 1287 loss: 4.56059865e-07
Iter: 1288 loss: 4.52437575e-07
Iter: 1289 loss: 4.52209406e-07
Iter: 1290 loss: 4.54160244e-07
Iter: 1291 loss: 4.52187e-07
Iter: 1292 loss: 4.51964326e-07
Iter: 1293 loss: 4.54240705e-07
Iter: 1294 loss: 4.51961512e-07
Iter: 1295 loss: 4.51859933e-07
Iter: 1296 loss: 4.51862604e-07
Iter: 1297 loss: 4.51770234e-07
Iter: 1298 loss: 4.5164208e-07
Iter: 1299 loss: 4.52787589e-07
Iter: 1300 loss: 4.51646599e-07
Iter: 1301 loss: 4.51554286e-07
Iter: 1302 loss: 4.51545873e-07
Iter: 1303 loss: 4.51506196e-07
Iter: 1304 loss: 4.51323388e-07
Iter: 1305 loss: 4.51369829e-07
Iter: 1306 loss: 4.51199185e-07
Iter: 1307 loss: 4.51029962e-07
Iter: 1308 loss: 4.50883817e-07
Iter: 1309 loss: 4.50840872e-07
Iter: 1310 loss: 4.50518087e-07
Iter: 1311 loss: 4.51114545e-07
Iter: 1312 loss: 4.50381776e-07
Iter: 1313 loss: 4.50057939e-07
Iter: 1314 loss: 4.51786264e-07
Iter: 1315 loss: 4.49995753e-07
Iter: 1316 loss: 4.49769942e-07
Iter: 1317 loss: 4.52397387e-07
Iter: 1318 loss: 4.49772301e-07
Iter: 1319 loss: 4.49600577e-07
Iter: 1320 loss: 4.50680801e-07
Iter: 1321 loss: 4.49596143e-07
Iter: 1322 loss: 4.49488368e-07
Iter: 1323 loss: 4.49364393e-07
Iter: 1324 loss: 4.49358765e-07
Iter: 1325 loss: 4.49197387e-07
Iter: 1326 loss: 4.50138714e-07
Iter: 1327 loss: 4.49187183e-07
Iter: 1328 loss: 4.4906264e-07
Iter: 1329 loss: 4.50876314e-07
Iter: 1330 loss: 4.49062043e-07
Iter: 1331 loss: 4.48973879e-07
Iter: 1332 loss: 4.48811193e-07
Iter: 1333 loss: 4.50355969e-07
Iter: 1334 loss: 4.48805281e-07
Iter: 1335 loss: 4.48568699e-07
Iter: 1336 loss: 4.5037379e-07
Iter: 1337 loss: 4.48547524e-07
Iter: 1338 loss: 4.4842534e-07
Iter: 1339 loss: 4.48456262e-07
Iter: 1340 loss: 4.48325125e-07
Iter: 1341 loss: 4.48135893e-07
Iter: 1342 loss: 4.48636456e-07
Iter: 1343 loss: 4.48050145e-07
Iter: 1344 loss: 4.47855967e-07
Iter: 1345 loss: 4.47922559e-07
Iter: 1346 loss: 4.47716928e-07
Iter: 1347 loss: 4.47518971e-07
Iter: 1348 loss: 4.47444762e-07
Iter: 1349 loss: 4.4734773e-07
Iter: 1350 loss: 4.47068146e-07
Iter: 1351 loss: 4.477792e-07
Iter: 1352 loss: 4.46995671e-07
Iter: 1353 loss: 4.46771622e-07
Iter: 1354 loss: 4.49332305e-07
Iter: 1355 loss: 4.46756701e-07
Iter: 1356 loss: 4.46597767e-07
Iter: 1357 loss: 4.48484627e-07
Iter: 1358 loss: 4.46591798e-07
Iter: 1359 loss: 4.4645941e-07
Iter: 1360 loss: 4.46254944e-07
Iter: 1361 loss: 4.4627086e-07
Iter: 1362 loss: 4.46110505e-07
Iter: 1363 loss: 4.47757373e-07
Iter: 1364 loss: 4.46108402e-07
Iter: 1365 loss: 4.45958335e-07
Iter: 1366 loss: 4.46625e-07
Iter: 1367 loss: 4.45925878e-07
Iter: 1368 loss: 4.45786782e-07
Iter: 1369 loss: 4.45637312e-07
Iter: 1370 loss: 4.45604883e-07
Iter: 1371 loss: 4.45465133e-07
Iter: 1372 loss: 4.45457545e-07
Iter: 1373 loss: 4.45332887e-07
Iter: 1374 loss: 4.45296791e-07
Iter: 1375 loss: 4.45239323e-07
Iter: 1376 loss: 4.45103836e-07
Iter: 1377 loss: 4.45293182e-07
Iter: 1378 loss: 4.45036619e-07
Iter: 1379 loss: 4.44888713e-07
Iter: 1380 loss: 4.45995255e-07
Iter: 1381 loss: 4.44858813e-07
Iter: 1382 loss: 4.44737395e-07
Iter: 1383 loss: 4.44554615e-07
Iter: 1384 loss: 4.44549869e-07
Iter: 1385 loss: 4.44306579e-07
Iter: 1386 loss: 4.44489842e-07
Iter: 1387 loss: 4.44172827e-07
Iter: 1388 loss: 4.43876729e-07
Iter: 1389 loss: 4.45080673e-07
Iter: 1390 loss: 4.43809654e-07
Iter: 1391 loss: 4.43674082e-07
Iter: 1392 loss: 4.43674338e-07
Iter: 1393 loss: 4.43526e-07
Iter: 1394 loss: 4.4351674e-07
Iter: 1395 loss: 4.43418969e-07
Iter: 1396 loss: 4.43277202e-07
Iter: 1397 loss: 4.43789816e-07
Iter: 1398 loss: 4.43236956e-07
Iter: 1399 loss: 4.43090727e-07
Iter: 1400 loss: 4.44579285e-07
Iter: 1401 loss: 4.43096923e-07
Iter: 1402 loss: 4.4302729e-07
Iter: 1403 loss: 4.42894475e-07
Iter: 1404 loss: 4.4499069e-07
Iter: 1405 loss: 4.42894077e-07
Iter: 1406 loss: 4.42807334e-07
Iter: 1407 loss: 4.42802559e-07
Iter: 1408 loss: 4.42713826e-07
Iter: 1409 loss: 4.42520445e-07
Iter: 1410 loss: 4.44491093e-07
Iter: 1411 loss: 4.42493331e-07
Iter: 1412 loss: 4.4226249e-07
Iter: 1413 loss: 4.44803845e-07
Iter: 1414 loss: 4.42250951e-07
Iter: 1415 loss: 4.42095654e-07
Iter: 1416 loss: 4.42578823e-07
Iter: 1417 loss: 4.42081159e-07
Iter: 1418 loss: 4.4192825e-07
Iter: 1419 loss: 4.41790206e-07
Iter: 1420 loss: 4.41749137e-07
Iter: 1421 loss: 4.41572752e-07
Iter: 1422 loss: 4.42669318e-07
Iter: 1423 loss: 4.41541573e-07
Iter: 1424 loss: 4.41375562e-07
Iter: 1425 loss: 4.41492915e-07
Iter: 1426 loss: 4.41261363e-07
Iter: 1427 loss: 4.41255e-07
Iter: 1428 loss: 4.41202531e-07
Iter: 1429 loss: 4.4114546e-07
Iter: 1430 loss: 4.40968364e-07
Iter: 1431 loss: 4.42539516e-07
Iter: 1432 loss: 4.40936674e-07
Iter: 1433 loss: 4.40916097e-07
Iter: 1434 loss: 4.40857093e-07
Iter: 1435 loss: 4.40771629e-07
Iter: 1436 loss: 4.40613974e-07
Iter: 1437 loss: 4.42715418e-07
Iter: 1438 loss: 4.40613235e-07
Iter: 1439 loss: 4.40448417e-07
Iter: 1440 loss: 4.40934969e-07
Iter: 1441 loss: 4.40371252e-07
Iter: 1442 loss: 4.40211323e-07
Iter: 1443 loss: 4.42900159e-07
Iter: 1444 loss: 4.40222948e-07
Iter: 1445 loss: 4.40131771e-07
Iter: 1446 loss: 4.39943676e-07
Iter: 1447 loss: 4.41959941e-07
Iter: 1448 loss: 4.39915652e-07
Iter: 1449 loss: 4.39716587e-07
Iter: 1450 loss: 4.41472679e-07
Iter: 1451 loss: 4.39723181e-07
Iter: 1452 loss: 4.39513144e-07
Iter: 1453 loss: 4.40514896e-07
Iter: 1454 loss: 4.39485092e-07
Iter: 1455 loss: 4.39373508e-07
Iter: 1456 loss: 4.39257207e-07
Iter: 1457 loss: 4.3924e-07
Iter: 1458 loss: 4.39065246e-07
Iter: 1459 loss: 4.39964253e-07
Iter: 1460 loss: 4.39022273e-07
Iter: 1461 loss: 4.38861321e-07
Iter: 1462 loss: 4.38955936e-07
Iter: 1463 loss: 4.38756928e-07
Iter: 1464 loss: 4.38606094e-07
Iter: 1465 loss: 4.40437333e-07
Iter: 1466 loss: 4.38614308e-07
Iter: 1467 loss: 4.38453924e-07
Iter: 1468 loss: 4.38559226e-07
Iter: 1469 loss: 4.38336514e-07
Iter: 1470 loss: 4.38198157e-07
Iter: 1471 loss: 4.38515457e-07
Iter: 1472 loss: 4.38150209e-07
Iter: 1473 loss: 4.38031634e-07
Iter: 1474 loss: 4.38046044e-07
Iter: 1475 loss: 4.37970016e-07
Iter: 1476 loss: 4.37765607e-07
Iter: 1477 loss: 4.38993141e-07
Iter: 1478 loss: 4.37718654e-07
Iter: 1479 loss: 4.37523767e-07
Iter: 1480 loss: 4.38206769e-07
Iter: 1481 loss: 4.37476587e-07
Iter: 1482 loss: 4.37376599e-07
Iter: 1483 loss: 4.37358494e-07
Iter: 1484 loss: 4.37284712e-07
Iter: 1485 loss: 4.37178244e-07
Iter: 1486 loss: 4.3717003e-07
Iter: 1487 loss: 4.37028262e-07
Iter: 1488 loss: 4.37582031e-07
Iter: 1489 loss: 4.37007202e-07
Iter: 1490 loss: 4.36850542e-07
Iter: 1491 loss: 4.37146355e-07
Iter: 1492 loss: 4.36772211e-07
Iter: 1493 loss: 4.36658468e-07
Iter: 1494 loss: 4.3645241e-07
Iter: 1495 loss: 4.36460937e-07
Iter: 1496 loss: 4.36236462e-07
Iter: 1497 loss: 4.37950973e-07
Iter: 1498 loss: 4.36213725e-07
Iter: 1499 loss: 4.36029779e-07
Iter: 1500 loss: 4.37166136e-07
Iter: 1501 loss: 4.36022304e-07
Iter: 1502 loss: 4.35883521e-07
Iter: 1503 loss: 4.36979576e-07
Iter: 1504 loss: 4.35882583e-07
Iter: 1505 loss: 4.3577063e-07
Iter: 1506 loss: 4.35739651e-07
Iter: 1507 loss: 4.35663196e-07
Iter: 1508 loss: 4.35540187e-07
Iter: 1509 loss: 4.35543484e-07
Iter: 1510 loss: 4.35437585e-07
Iter: 1511 loss: 4.35289138e-07
Iter: 1512 loss: 4.35285131e-07
Iter: 1513 loss: 4.35154845e-07
Iter: 1514 loss: 4.35030017e-07
Iter: 1515 loss: 4.34994831e-07
Iter: 1516 loss: 4.34776666e-07
Iter: 1517 loss: 4.35585491e-07
Iter: 1518 loss: 4.34728e-07
Iter: 1519 loss: 4.34530364e-07
Iter: 1520 loss: 4.35267737e-07
Iter: 1521 loss: 4.34476448e-07
Iter: 1522 loss: 4.34359947e-07
Iter: 1523 loss: 4.34354774e-07
Iter: 1524 loss: 4.34282356e-07
Iter: 1525 loss: 4.34176229e-07
Iter: 1526 loss: 4.34174922e-07
Iter: 1527 loss: 4.34031165e-07
Iter: 1528 loss: 4.34223807e-07
Iter: 1529 loss: 4.33976595e-07
Iter: 1530 loss: 4.33794639e-07
Iter: 1531 loss: 4.34980819e-07
Iter: 1532 loss: 4.33770282e-07
Iter: 1533 loss: 4.33682317e-07
Iter: 1534 loss: 4.33512412e-07
Iter: 1535 loss: 4.33513e-07
Iter: 1536 loss: 4.33469779e-07
Iter: 1537 loss: 4.33422883e-07
Iter: 1538 loss: 4.3334e-07
Iter: 1539 loss: 4.33370644e-07
Iter: 1540 loss: 4.33291916e-07
Iter: 1541 loss: 4.33204463e-07
Iter: 1542 loss: 4.33882406e-07
Iter: 1543 loss: 4.3319784e-07
Iter: 1544 loss: 4.33126559e-07
Iter: 1545 loss: 4.32996046e-07
Iter: 1546 loss: 4.32989253e-07
Iter: 1547 loss: 4.32862407e-07
Iter: 1548 loss: 4.32983285e-07
Iter: 1549 loss: 4.32826425e-07
Iter: 1550 loss: 4.32650722e-07
Iter: 1551 loss: 4.3317965e-07
Iter: 1552 loss: 4.32593652e-07
Iter: 1553 loss: 4.32458933e-07
Iter: 1554 loss: 4.32617696e-07
Iter: 1555 loss: 4.32369745e-07
Iter: 1556 loss: 4.32222549e-07
Iter: 1557 loss: 4.32568442e-07
Iter: 1558 loss: 4.32163176e-07
Iter: 1559 loss: 4.3199168e-07
Iter: 1560 loss: 4.32506596e-07
Iter: 1561 loss: 4.31941373e-07
Iter: 1562 loss: 4.3180728e-07
Iter: 1563 loss: 4.31799862e-07
Iter: 1564 loss: 4.31728722e-07
Iter: 1565 loss: 4.31613159e-07
Iter: 1566 loss: 4.31610175e-07
Iter: 1567 loss: 4.31498165e-07
Iter: 1568 loss: 4.32366022e-07
Iter: 1569 loss: 4.31491856e-07
Iter: 1570 loss: 4.31343381e-07
Iter: 1571 loss: 4.31326811e-07
Iter: 1572 loss: 4.31225146e-07
Iter: 1573 loss: 4.31069935e-07
Iter: 1574 loss: 4.31233985e-07
Iter: 1575 loss: 4.3098504e-07
Iter: 1576 loss: 4.30915406e-07
Iter: 1577 loss: 4.30869846e-07
Iter: 1578 loss: 4.3081053e-07
Iter: 1579 loss: 4.30731347e-07
Iter: 1580 loss: 4.30722082e-07
Iter: 1581 loss: 4.30591342e-07
Iter: 1582 loss: 4.31321752e-07
Iter: 1583 loss: 4.30563659e-07
Iter: 1584 loss: 4.30480128e-07
Iter: 1585 loss: 4.30374854e-07
Iter: 1586 loss: 4.30358654e-07
Iter: 1587 loss: 4.30229903e-07
Iter: 1588 loss: 4.30236241e-07
Iter: 1589 loss: 4.30121304e-07
Iter: 1590 loss: 4.29991587e-07
Iter: 1591 loss: 4.3117879e-07
Iter: 1592 loss: 4.29959897e-07
Iter: 1593 loss: 4.29859853e-07
Iter: 1594 loss: 4.30089642e-07
Iter: 1595 loss: 4.29792038e-07
Iter: 1596 loss: 4.29653483e-07
Iter: 1597 loss: 4.30310422e-07
Iter: 1598 loss: 4.29621565e-07
Iter: 1599 loss: 4.29523197e-07
Iter: 1600 loss: 4.31057686e-07
Iter: 1601 loss: 4.29517e-07
Iter: 1602 loss: 4.29443503e-07
Iter: 1603 loss: 4.29305771e-07
Iter: 1604 loss: 4.32635176e-07
Iter: 1605 loss: 4.2930526e-07
Iter: 1606 loss: 4.29155193e-07
Iter: 1607 loss: 4.30051387e-07
Iter: 1608 loss: 4.29136833e-07
Iter: 1609 loss: 4.29005354e-07
Iter: 1610 loss: 4.29417526e-07
Iter: 1611 loss: 4.28959424e-07
Iter: 1612 loss: 4.28846022e-07
Iter: 1613 loss: 4.29008452e-07
Iter: 1614 loss: 4.28794067e-07
Iter: 1615 loss: 4.28732e-07
Iter: 1616 loss: 4.2871784e-07
Iter: 1617 loss: 4.28679954e-07
Iter: 1618 loss: 4.28550436e-07
Iter: 1619 loss: 4.29372903e-07
Iter: 1620 loss: 4.28536339e-07
Iter: 1621 loss: 4.28436067e-07
Iter: 1622 loss: 4.2898381e-07
Iter: 1623 loss: 4.28409635e-07
Iter: 1624 loss: 4.28293873e-07
Iter: 1625 loss: 4.29258876e-07
Iter: 1626 loss: 4.28292196e-07
Iter: 1627 loss: 4.28204288e-07
Iter: 1628 loss: 4.28083638e-07
Iter: 1629 loss: 4.28088e-07
Iter: 1630 loss: 4.27945736e-07
Iter: 1631 loss: 4.27982542e-07
Iter: 1632 loss: 4.27838103e-07
Iter: 1633 loss: 4.27650548e-07
Iter: 1634 loss: 4.28639e-07
Iter: 1635 loss: 4.27623405e-07
Iter: 1636 loss: 4.27494484e-07
Iter: 1637 loss: 4.27502982e-07
Iter: 1638 loss: 4.27403393e-07
Iter: 1639 loss: 4.27591544e-07
Iter: 1640 loss: 4.27360476e-07
Iter: 1641 loss: 4.27290956e-07
Iter: 1642 loss: 4.27302723e-07
Iter: 1643 loss: 4.27235875e-07
Iter: 1644 loss: 4.27118778e-07
Iter: 1645 loss: 4.27337199e-07
Iter: 1646 loss: 4.27073246e-07
Iter: 1647 loss: 4.26915193e-07
Iter: 1648 loss: 4.27583728e-07
Iter: 1649 loss: 4.26872901e-07
Iter: 1650 loss: 4.26775813e-07
Iter: 1651 loss: 4.26696602e-07
Iter: 1652 loss: 4.26653202e-07
Iter: 1653 loss: 4.26629384e-07
Iter: 1654 loss: 4.26581863e-07
Iter: 1655 loss: 4.26525901e-07
Iter: 1656 loss: 4.26399367e-07
Iter: 1657 loss: 4.27527084e-07
Iter: 1658 loss: 4.2636205e-07
Iter: 1659 loss: 4.26247368e-07
Iter: 1660 loss: 4.26209226e-07
Iter: 1661 loss: 4.2614127e-07
Iter: 1662 loss: 4.26121602e-07
Iter: 1663 loss: 4.26057966e-07
Iter: 1664 loss: 4.25982194e-07
Iter: 1665 loss: 4.25857735e-07
Iter: 1666 loss: 4.28313712e-07
Iter: 1667 loss: 4.25859128e-07
Iter: 1668 loss: 4.25736857e-07
Iter: 1669 loss: 4.25934701e-07
Iter: 1670 loss: 4.25681094e-07
Iter: 1671 loss: 4.25554646e-07
Iter: 1672 loss: 4.26661074e-07
Iter: 1673 loss: 4.25541032e-07
Iter: 1674 loss: 4.25462e-07
Iter: 1675 loss: 4.25352e-07
Iter: 1676 loss: 4.25341625e-07
Iter: 1677 loss: 4.25189342e-07
Iter: 1678 loss: 4.25636415e-07
Iter: 1679 loss: 4.25138523e-07
Iter: 1680 loss: 4.25027906e-07
Iter: 1681 loss: 4.26697198e-07
Iter: 1682 loss: 4.25008096e-07
Iter: 1683 loss: 4.24889265e-07
Iter: 1684 loss: 4.24862776e-07
Iter: 1685 loss: 4.24779557e-07
Iter: 1686 loss: 4.24652228e-07
Iter: 1687 loss: 4.25623369e-07
Iter: 1688 loss: 4.24643929e-07
Iter: 1689 loss: 4.24538882e-07
Iter: 1690 loss: 4.25343217e-07
Iter: 1691 loss: 4.24535841e-07
Iter: 1692 loss: 4.24467942e-07
Iter: 1693 loss: 4.24303892e-07
Iter: 1694 loss: 4.26402295e-07
Iter: 1695 loss: 4.24297895e-07
Iter: 1696 loss: 4.24168036e-07
Iter: 1697 loss: 4.24363463e-07
Iter: 1698 loss: 4.24100222e-07
Iter: 1699 loss: 4.24073619e-07
Iter: 1700 loss: 4.24032066e-07
Iter: 1701 loss: 4.23982726e-07
Iter: 1702 loss: 4.23874098e-07
Iter: 1703 loss: 4.25970342e-07
Iter: 1704 loss: 4.23867846e-07
Iter: 1705 loss: 4.23741369e-07
Iter: 1706 loss: 4.24279591e-07
Iter: 1707 loss: 4.23712038e-07
Iter: 1708 loss: 4.23637e-07
Iter: 1709 loss: 4.25004089e-07
Iter: 1710 loss: 4.23626346e-07
Iter: 1711 loss: 4.23568338e-07
Iter: 1712 loss: 4.23421142e-07
Iter: 1713 loss: 4.24609e-07
Iter: 1714 loss: 4.23398063e-07
Iter: 1715 loss: 4.2325172e-07
Iter: 1716 loss: 4.24333763e-07
Iter: 1717 loss: 4.23234582e-07
Iter: 1718 loss: 4.23108418e-07
Iter: 1719 loss: 4.23511864e-07
Iter: 1720 loss: 4.23075846e-07
Iter: 1721 loss: 4.22946073e-07
Iter: 1722 loss: 4.24103888e-07
Iter: 1723 loss: 4.22945817e-07
Iter: 1724 loss: 4.22866748e-07
Iter: 1725 loss: 4.22955964e-07
Iter: 1726 loss: 4.22806e-07
Iter: 1727 loss: 4.22714209e-07
Iter: 1728 loss: 4.23792699e-07
Iter: 1729 loss: 4.22715857e-07
Iter: 1730 loss: 4.22652306e-07
Iter: 1731 loss: 4.22563488e-07
Iter: 1732 loss: 4.22561754e-07
Iter: 1733 loss: 4.224637e-07
Iter: 1734 loss: 4.22687435e-07
Iter: 1735 loss: 4.22408391e-07
Iter: 1736 loss: 4.22297148e-07
Iter: 1737 loss: 4.22303515e-07
Iter: 1738 loss: 4.22227401e-07
Iter: 1739 loss: 4.22137333e-07
Iter: 1740 loss: 4.22129347e-07
Iter: 1741 loss: 4.22038568e-07
Iter: 1742 loss: 4.23098641e-07
Iter: 1743 loss: 4.22027597e-07
Iter: 1744 loss: 4.21936932e-07
Iter: 1745 loss: 4.21796187e-07
Iter: 1746 loss: 4.21782261e-07
Iter: 1747 loss: 4.2164902e-07
Iter: 1748 loss: 4.21956599e-07
Iter: 1749 loss: 4.216202e-07
Iter: 1750 loss: 4.2146155e-07
Iter: 1751 loss: 4.21447623e-07
Iter: 1752 loss: 4.21350535e-07
Iter: 1753 loss: 4.21232755e-07
Iter: 1754 loss: 4.21221e-07
Iter: 1755 loss: 4.21140072e-07
Iter: 1756 loss: 4.21324501e-07
Iter: 1757 loss: 4.21089368e-07
Iter: 1758 loss: 4.21001204e-07
Iter: 1759 loss: 4.21638589e-07
Iter: 1760 loss: 4.20992251e-07
Iter: 1761 loss: 4.2091196e-07
Iter: 1762 loss: 4.20979774e-07
Iter: 1763 loss: 4.20870947e-07
Iter: 1764 loss: 4.20786193e-07
Iter: 1765 loss: 4.2060924e-07
Iter: 1766 loss: 4.20610917e-07
Iter: 1767 loss: 4.20457667e-07
Iter: 1768 loss: 4.20461959e-07
Iter: 1769 loss: 4.20356514e-07
Iter: 1770 loss: 4.20864154e-07
Iter: 1771 loss: 4.20343241e-07
Iter: 1772 loss: 4.20241037e-07
Iter: 1773 loss: 4.20077015e-07
Iter: 1774 loss: 4.24014161e-07
Iter: 1775 loss: 4.20068261e-07
Iter: 1776 loss: 4.20026709e-07
Iter: 1777 loss: 4.1998473e-07
Iter: 1778 loss: 4.1992925e-07
Iter: 1779 loss: 4.19788336e-07
Iter: 1780 loss: 4.20930292e-07
Iter: 1781 loss: 4.19774665e-07
Iter: 1782 loss: 4.19616271e-07
Iter: 1783 loss: 4.20243566e-07
Iter: 1784 loss: 4.19591458e-07
Iter: 1785 loss: 4.19475043e-07
Iter: 1786 loss: 4.19487407e-07
Iter: 1787 loss: 4.19378921e-07
Iter: 1788 loss: 4.19289591e-07
Iter: 1789 loss: 4.19286977e-07
Iter: 1790 loss: 4.191958e-07
Iter: 1791 loss: 4.19372498e-07
Iter: 1792 loss: 4.19160671e-07
Iter: 1793 loss: 4.19056761e-07
Iter: 1794 loss: 4.19406319e-07
Iter: 1795 loss: 4.19033199e-07
Iter: 1796 loss: 4.18936224e-07
Iter: 1797 loss: 4.19034393e-07
Iter: 1798 loss: 4.18895297e-07
Iter: 1799 loss: 4.18811226e-07
Iter: 1800 loss: 4.18813954e-07
Iter: 1801 loss: 4.18760123e-07
Iter: 1802 loss: 4.18635182e-07
Iter: 1803 loss: 4.1863791e-07
Iter: 1804 loss: 4.18536388e-07
Iter: 1805 loss: 4.18453595e-07
Iter: 1806 loss: 4.196433e-07
Iter: 1807 loss: 4.18455841e-07
Iter: 1808 loss: 4.18358638e-07
Iter: 1809 loss: 4.18271298e-07
Iter: 1810 loss: 4.18254047e-07
Iter: 1811 loss: 4.18218377e-07
Iter: 1812 loss: 4.18185579e-07
Iter: 1813 loss: 4.18135983e-07
Iter: 1814 loss: 4.18015247e-07
Iter: 1815 loss: 4.19205548e-07
Iter: 1816 loss: 4.18010416e-07
Iter: 1817 loss: 4.17870694e-07
Iter: 1818 loss: 4.18414714e-07
Iter: 1819 loss: 4.1784898e-07
Iter: 1820 loss: 4.17734441e-07
Iter: 1821 loss: 4.17634197e-07
Iter: 1822 loss: 4.17605378e-07
Iter: 1823 loss: 4.1750377e-07
Iter: 1824 loss: 4.17484472e-07
Iter: 1825 loss: 4.17382239e-07
Iter: 1826 loss: 4.17745838e-07
Iter: 1827 loss: 4.17365584e-07
Iter: 1828 loss: 4.17273043e-07
Iter: 1829 loss: 4.17478475e-07
Iter: 1830 loss: 4.17212647e-07
Iter: 1831 loss: 4.17152194e-07
Iter: 1832 loss: 4.1709859e-07
Iter: 1833 loss: 4.17066076e-07
Iter: 1834 loss: 4.16955857e-07
Iter: 1835 loss: 4.17797708e-07
Iter: 1836 loss: 4.16959494e-07
Iter: 1837 loss: 4.16828527e-07
Iter: 1838 loss: 4.16968305e-07
Iter: 1839 loss: 4.16766113e-07
Iter: 1840 loss: 4.16630542e-07
Iter: 1841 loss: 4.16708872e-07
Iter: 1842 loss: 4.16555139e-07
Iter: 1843 loss: 4.16404021e-07
Iter: 1844 loss: 4.16514496e-07
Iter: 1845 loss: 4.16327083e-07
Iter: 1846 loss: 4.1619387e-07
Iter: 1847 loss: 4.18098523e-07
Iter: 1848 loss: 4.16197452e-07
Iter: 1849 loss: 4.16062448e-07
Iter: 1850 loss: 4.16196286e-07
Iter: 1851 loss: 4.15994975e-07
Iter: 1852 loss: 4.15849286e-07
Iter: 1853 loss: 4.15712492e-07
Iter: 1854 loss: 4.15684696e-07
Iter: 1855 loss: 4.15518741e-07
Iter: 1856 loss: 4.16916549e-07
Iter: 1857 loss: 4.15503479e-07
Iter: 1858 loss: 4.1541324e-07
Iter: 1859 loss: 4.15398233e-07
Iter: 1860 loss: 4.15300036e-07
Iter: 1861 loss: 4.15386864e-07
Iter: 1862 loss: 4.15230062e-07
Iter: 1863 loss: 4.15147099e-07
Iter: 1864 loss: 4.15952542e-07
Iter: 1865 loss: 4.15130444e-07
Iter: 1866 loss: 4.15070986e-07
Iter: 1867 loss: 4.14963949e-07
Iter: 1868 loss: 4.17414697e-07
Iter: 1869 loss: 4.14950762e-07
Iter: 1870 loss: 4.14847023e-07
Iter: 1871 loss: 4.16176078e-07
Iter: 1872 loss: 4.14854924e-07
Iter: 1873 loss: 4.1474172e-07
Iter: 1874 loss: 4.1489821e-07
Iter: 1875 loss: 4.14703948e-07
Iter: 1876 loss: 4.14614732e-07
Iter: 1877 loss: 4.14579262e-07
Iter: 1878 loss: 4.14538476e-07
Iter: 1879 loss: 4.1440552e-07
Iter: 1880 loss: 4.15155824e-07
Iter: 1881 loss: 4.14389348e-07
Iter: 1882 loss: 4.14289644e-07
Iter: 1883 loss: 4.15085566e-07
Iter: 1884 loss: 4.14295016e-07
Iter: 1885 loss: 4.14203896e-07
Iter: 1886 loss: 4.14099304e-07
Iter: 1887 loss: 4.14078897e-07
Iter: 1888 loss: 4.13969076e-07
Iter: 1889 loss: 4.14124344e-07
Iter: 1890 loss: 4.13917547e-07
Iter: 1891 loss: 4.13790445e-07
Iter: 1892 loss: 4.14674219e-07
Iter: 1893 loss: 4.13776661e-07
Iter: 1894 loss: 4.13684603e-07
Iter: 1895 loss: 4.15005729e-07
Iter: 1896 loss: 4.13691936e-07
Iter: 1897 loss: 4.13656835e-07
Iter: 1898 loss: 4.13738121e-07
Iter: 1899 loss: 4.136364e-07
Iter: 1900 loss: 4.13565544e-07
Iter: 1901 loss: 4.13419343e-07
Iter: 1902 loss: 4.15629e-07
Iter: 1903 loss: 4.13411044e-07
Iter: 1904 loss: 4.13286e-07
Iter: 1905 loss: 4.14077903e-07
Iter: 1906 loss: 4.13261688e-07
Iter: 1907 loss: 4.13157579e-07
Iter: 1908 loss: 4.14422715e-07
Iter: 1909 loss: 4.13160592e-07
Iter: 1910 loss: 4.13093915e-07
Iter: 1911 loss: 4.13019563e-07
Iter: 1912 loss: 4.13005921e-07
Iter: 1913 loss: 4.12892518e-07
Iter: 1914 loss: 4.12969086e-07
Iter: 1915 loss: 4.12835504e-07
Iter: 1916 loss: 4.12687143e-07
Iter: 1917 loss: 4.14235984e-07
Iter: 1918 loss: 4.12684329e-07
Iter: 1919 loss: 4.12595e-07
Iter: 1920 loss: 4.13150417e-07
Iter: 1921 loss: 4.12579823e-07
Iter: 1922 loss: 4.12517039e-07
Iter: 1923 loss: 4.12401562e-07
Iter: 1924 loss: 4.14381702e-07
Iter: 1925 loss: 4.12406735e-07
Iter: 1926 loss: 4.1228418e-07
Iter: 1927 loss: 4.13341297e-07
Iter: 1928 loss: 4.12277501e-07
Iter: 1929 loss: 4.12208692e-07
Iter: 1930 loss: 4.12211023e-07
Iter: 1931 loss: 4.1215884e-07
Iter: 1932 loss: 4.1209509e-07
Iter: 1933 loss: 4.12089463e-07
Iter: 1934 loss: 4.11972763e-07
Iter: 1935 loss: 4.12502e-07
Iter: 1936 loss: 4.11954545e-07
Iter: 1937 loss: 4.11886219e-07
Iter: 1938 loss: 4.11766507e-07
Iter: 1939 loss: 4.14325967e-07
Iter: 1940 loss: 4.1176844e-07
Iter: 1941 loss: 4.1165174e-07
Iter: 1942 loss: 4.13113355e-07
Iter: 1943 loss: 4.11646283e-07
Iter: 1944 loss: 4.11538792e-07
Iter: 1945 loss: 4.11846145e-07
Iter: 1946 loss: 4.11506221e-07
Iter: 1947 loss: 4.11406461e-07
Iter: 1948 loss: 4.11256394e-07
Iter: 1949 loss: 4.11261027e-07
Iter: 1950 loss: 4.11148619e-07
Iter: 1951 loss: 4.1115041e-07
Iter: 1952 loss: 4.11065969e-07
Iter: 1953 loss: 4.11440567e-07
Iter: 1954 loss: 4.11041242e-07
Iter: 1955 loss: 4.10968795e-07
Iter: 1956 loss: 4.10965725e-07
Iter: 1957 loss: 4.10892227e-07
Iter: 1958 loss: 4.10792524e-07
Iter: 1959 loss: 4.10707315e-07
Iter: 1960 loss: 4.10675483e-07
Iter: 1961 loss: 4.10589166e-07
Iter: 1962 loss: 4.10582629e-07
Iter: 1963 loss: 4.10500718e-07
Iter: 1964 loss: 4.1092693e-07
Iter: 1965 loss: 4.10485654e-07
Iter: 1966 loss: 4.10418806e-07
Iter: 1967 loss: 4.10518084e-07
Iter: 1968 loss: 4.10381375e-07
Iter: 1969 loss: 4.10282524e-07
Iter: 1970 loss: 4.10246287e-07
Iter: 1971 loss: 4.10196321e-07
Iter: 1972 loss: 4.10106963e-07
Iter: 1973 loss: 4.10134191e-07
Iter: 1974 loss: 4.10037558e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2
+ date
Mon Oct 26 15:22:00 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi -1 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac76bee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac76bef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac76f6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac77bcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac77a41e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac77a4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac7651378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac75ea7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac75ea488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac75b8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac75b8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac75987b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac7598a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac75508c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac74abd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac74ab840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac751b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac74fa950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac7487950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac7429f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac744b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac73b7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac73b7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac740c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac740c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac736b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac7324950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbaa0e46510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbaa0e46378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbaa0e66730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbac72eb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbaa0e256a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbaa0e25378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbaa0e1c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbaa0dd9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbaa0db10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.75939931e-05
Iter: 2 loss: 2.04712633e-05
Iter: 3 loss: 1.52008e-05
Iter: 4 loss: 1.41105575e-05
Iter: 5 loss: 1.46623888e-05
Iter: 6 loss: 1.33843223e-05
Iter: 7 loss: 1.2299176e-05
Iter: 8 loss: 1.45706472e-05
Iter: 9 loss: 1.1868945e-05
Iter: 10 loss: 1.11808658e-05
Iter: 11 loss: 1.35540076e-05
Iter: 12 loss: 1.09986804e-05
Iter: 13 loss: 1.0527363e-05
Iter: 14 loss: 1.26201121e-05
Iter: 15 loss: 1.04339069e-05
Iter: 16 loss: 9.99379881e-06
Iter: 17 loss: 9.30210263e-06
Iter: 18 loss: 9.29408452e-06
Iter: 19 loss: 8.75621299e-06
Iter: 20 loss: 8.72166129e-06
Iter: 21 loss: 8.34426919e-06
Iter: 22 loss: 7.70054703e-06
Iter: 23 loss: 7.69979124e-06
Iter: 24 loss: 7.13060854e-06
Iter: 25 loss: 1.07353517e-05
Iter: 26 loss: 7.06592527e-06
Iter: 27 loss: 6.67256427e-06
Iter: 28 loss: 7.75050285e-06
Iter: 29 loss: 6.54452197e-06
Iter: 30 loss: 6.17066962e-06
Iter: 31 loss: 7.66287121e-06
Iter: 32 loss: 6.08584196e-06
Iter: 33 loss: 5.80362075e-06
Iter: 34 loss: 6.53038114e-06
Iter: 35 loss: 5.70692555e-06
Iter: 36 loss: 5.7953589e-06
Iter: 37 loss: 5.60857416e-06
Iter: 38 loss: 5.54421786e-06
Iter: 39 loss: 5.42185171e-06
Iter: 40 loss: 8.04228148e-06
Iter: 41 loss: 5.42140697e-06
Iter: 42 loss: 5.26746499e-06
Iter: 43 loss: 5.9450731e-06
Iter: 44 loss: 5.23662402e-06
Iter: 45 loss: 5.11005692e-06
Iter: 46 loss: 5.0429976e-06
Iter: 47 loss: 4.98560803e-06
Iter: 48 loss: 4.81902134e-06
Iter: 49 loss: 6.3949883e-06
Iter: 50 loss: 4.81232337e-06
Iter: 51 loss: 4.68928e-06
Iter: 52 loss: 4.58815157e-06
Iter: 53 loss: 4.55226291e-06
Iter: 54 loss: 4.35178799e-06
Iter: 55 loss: 5.72239833e-06
Iter: 56 loss: 4.33240029e-06
Iter: 57 loss: 4.19311527e-06
Iter: 58 loss: 5.10528662e-06
Iter: 59 loss: 4.17851379e-06
Iter: 60 loss: 4.10637722e-06
Iter: 61 loss: 3.93989194e-06
Iter: 62 loss: 6.02842e-06
Iter: 63 loss: 3.92773563e-06
Iter: 64 loss: 3.80250594e-06
Iter: 65 loss: 3.80154734e-06
Iter: 66 loss: 3.70825774e-06
Iter: 67 loss: 3.80316146e-06
Iter: 68 loss: 3.65589858e-06
Iter: 69 loss: 3.59247133e-06
Iter: 70 loss: 4.44650232e-06
Iter: 71 loss: 3.59212208e-06
Iter: 72 loss: 3.55890324e-06
Iter: 73 loss: 3.55847533e-06
Iter: 74 loss: 3.52052939e-06
Iter: 75 loss: 3.4395639e-06
Iter: 76 loss: 4.73476121e-06
Iter: 77 loss: 3.43695206e-06
Iter: 78 loss: 3.38069867e-06
Iter: 79 loss: 3.94915196e-06
Iter: 80 loss: 3.37905885e-06
Iter: 81 loss: 3.31927595e-06
Iter: 82 loss: 3.23659333e-06
Iter: 83 loss: 3.23304948e-06
Iter: 84 loss: 3.16154023e-06
Iter: 85 loss: 3.92380343e-06
Iter: 86 loss: 3.15978377e-06
Iter: 87 loss: 3.09644611e-06
Iter: 88 loss: 3.13529563e-06
Iter: 89 loss: 3.05571893e-06
Iter: 90 loss: 2.99496287e-06
Iter: 91 loss: 3.31772981e-06
Iter: 92 loss: 2.98567647e-06
Iter: 93 loss: 2.9466778e-06
Iter: 94 loss: 3.27462681e-06
Iter: 95 loss: 2.94439e-06
Iter: 96 loss: 2.9121461e-06
Iter: 97 loss: 2.83971895e-06
Iter: 98 loss: 3.81758036e-06
Iter: 99 loss: 2.83541567e-06
Iter: 100 loss: 2.77403865e-06
Iter: 101 loss: 3.18925026e-06
Iter: 102 loss: 2.76799142e-06
Iter: 103 loss: 2.70917258e-06
Iter: 104 loss: 2.82177371e-06
Iter: 105 loss: 2.68454187e-06
Iter: 106 loss: 2.63172524e-06
Iter: 107 loss: 2.88247e-06
Iter: 108 loss: 2.62226786e-06
Iter: 109 loss: 2.59248327e-06
Iter: 110 loss: 2.59009494e-06
Iter: 111 loss: 2.55525788e-06
Iter: 112 loss: 2.5233071e-06
Iter: 113 loss: 2.51497067e-06
Iter: 114 loss: 2.47907337e-06
Iter: 115 loss: 2.53561325e-06
Iter: 116 loss: 2.46236596e-06
Iter: 117 loss: 2.42802662e-06
Iter: 118 loss: 2.77968866e-06
Iter: 119 loss: 2.42701617e-06
Iter: 120 loss: 2.4048386e-06
Iter: 121 loss: 2.37979498e-06
Iter: 122 loss: 2.37643235e-06
Iter: 123 loss: 2.35762627e-06
Iter: 124 loss: 2.35730317e-06
Iter: 125 loss: 2.33988794e-06
Iter: 126 loss: 2.30397632e-06
Iter: 127 loss: 2.92978166e-06
Iter: 128 loss: 2.30331079e-06
Iter: 129 loss: 2.25925419e-06
Iter: 130 loss: 2.63909533e-06
Iter: 131 loss: 2.25675035e-06
Iter: 132 loss: 2.22960489e-06
Iter: 133 loss: 2.39667861e-06
Iter: 134 loss: 2.22636186e-06
Iter: 135 loss: 2.20720176e-06
Iter: 136 loss: 2.167714e-06
Iter: 137 loss: 2.86701925e-06
Iter: 138 loss: 2.16696435e-06
Iter: 139 loss: 2.13324483e-06
Iter: 140 loss: 2.43821273e-06
Iter: 141 loss: 2.13162525e-06
Iter: 142 loss: 2.10348458e-06
Iter: 143 loss: 2.1747378e-06
Iter: 144 loss: 2.09387326e-06
Iter: 145 loss: 2.07552898e-06
Iter: 146 loss: 2.27809528e-06
Iter: 147 loss: 2.07509129e-06
Iter: 148 loss: 2.05143124e-06
Iter: 149 loss: 2.10557414e-06
Iter: 150 loss: 2.04264506e-06
Iter: 151 loss: 2.03263812e-06
Iter: 152 loss: 2.02548745e-06
Iter: 153 loss: 2.02199e-06
Iter: 154 loss: 2.00407567e-06
Iter: 155 loss: 2.07014637e-06
Iter: 156 loss: 1.99963961e-06
Iter: 157 loss: 1.98005182e-06
Iter: 158 loss: 2.02508227e-06
Iter: 159 loss: 1.97268196e-06
Iter: 160 loss: 1.95903908e-06
Iter: 161 loss: 1.95925895e-06
Iter: 162 loss: 1.94804579e-06
Iter: 163 loss: 1.92764492e-06
Iter: 164 loss: 2.07280141e-06
Iter: 165 loss: 1.92576249e-06
Iter: 166 loss: 1.91273443e-06
Iter: 167 loss: 1.95485063e-06
Iter: 168 loss: 1.90910487e-06
Iter: 169 loss: 1.8978526e-06
Iter: 170 loss: 1.90258697e-06
Iter: 171 loss: 1.89017294e-06
Iter: 172 loss: 1.8706703e-06
Iter: 173 loss: 1.91899e-06
Iter: 174 loss: 1.86380657e-06
Iter: 175 loss: 1.85006729e-06
Iter: 176 loss: 1.84263263e-06
Iter: 177 loss: 1.83653196e-06
Iter: 178 loss: 1.8170424e-06
Iter: 179 loss: 1.91918662e-06
Iter: 180 loss: 1.81400242e-06
Iter: 181 loss: 1.79921551e-06
Iter: 182 loss: 1.83360237e-06
Iter: 183 loss: 1.79383733e-06
Iter: 184 loss: 1.78569746e-06
Iter: 185 loss: 1.78287019e-06
Iter: 186 loss: 1.77907805e-06
Iter: 187 loss: 1.76803621e-06
Iter: 188 loss: 1.81160544e-06
Iter: 189 loss: 1.76348794e-06
Iter: 190 loss: 1.75065327e-06
Iter: 191 loss: 1.87577359e-06
Iter: 192 loss: 1.75020932e-06
Iter: 193 loss: 1.73846888e-06
Iter: 194 loss: 1.79047106e-06
Iter: 195 loss: 1.73613421e-06
Iter: 196 loss: 1.72830221e-06
Iter: 197 loss: 1.72476143e-06
Iter: 198 loss: 1.72088369e-06
Iter: 199 loss: 1.70806516e-06
Iter: 200 loss: 1.74267893e-06
Iter: 201 loss: 1.70380554e-06
Iter: 202 loss: 1.68881309e-06
Iter: 203 loss: 1.75038849e-06
Iter: 204 loss: 1.68551992e-06
Iter: 205 loss: 1.67524956e-06
Iter: 206 loss: 1.70130647e-06
Iter: 207 loss: 1.67170901e-06
Iter: 208 loss: 1.66116752e-06
Iter: 209 loss: 1.68457507e-06
Iter: 210 loss: 1.65715301e-06
Iter: 211 loss: 1.64641324e-06
Iter: 212 loss: 1.69706448e-06
Iter: 213 loss: 1.64446453e-06
Iter: 214 loss: 1.63653476e-06
Iter: 215 loss: 1.62317804e-06
Iter: 216 loss: 1.62325136e-06
Iter: 217 loss: 1.60876687e-06
Iter: 218 loss: 1.63410095e-06
Iter: 219 loss: 1.60248624e-06
Iter: 220 loss: 1.59908359e-06
Iter: 221 loss: 1.59485205e-06
Iter: 222 loss: 1.58750959e-06
Iter: 223 loss: 1.62251285e-06
Iter: 224 loss: 1.58612579e-06
Iter: 225 loss: 1.58217426e-06
Iter: 226 loss: 1.57101192e-06
Iter: 227 loss: 1.63265702e-06
Iter: 228 loss: 1.56765418e-06
Iter: 229 loss: 1.56058059e-06
Iter: 230 loss: 1.5602709e-06
Iter: 231 loss: 1.55149132e-06
Iter: 232 loss: 1.54534018e-06
Iter: 233 loss: 1.54217878e-06
Iter: 234 loss: 1.53484643e-06
Iter: 235 loss: 1.58418447e-06
Iter: 236 loss: 1.53405585e-06
Iter: 237 loss: 1.52837117e-06
Iter: 238 loss: 1.54073621e-06
Iter: 239 loss: 1.52615189e-06
Iter: 240 loss: 1.51751044e-06
Iter: 241 loss: 1.51763334e-06
Iter: 242 loss: 1.51057191e-06
Iter: 243 loss: 1.50258677e-06
Iter: 244 loss: 1.56969509e-06
Iter: 245 loss: 1.5021302e-06
Iter: 246 loss: 1.4953298e-06
Iter: 247 loss: 1.4954353e-06
Iter: 248 loss: 1.48994218e-06
Iter: 249 loss: 1.47847402e-06
Iter: 250 loss: 1.51040354e-06
Iter: 251 loss: 1.47476021e-06
Iter: 252 loss: 1.46799152e-06
Iter: 253 loss: 1.47278e-06
Iter: 254 loss: 1.4636787e-06
Iter: 255 loss: 1.45367949e-06
Iter: 256 loss: 1.47341973e-06
Iter: 257 loss: 1.44954618e-06
Iter: 258 loss: 1.44069577e-06
Iter: 259 loss: 1.47142896e-06
Iter: 260 loss: 1.43838429e-06
Iter: 261 loss: 1.43299656e-06
Iter: 262 loss: 1.43186912e-06
Iter: 263 loss: 1.42950989e-06
Iter: 264 loss: 1.42334216e-06
Iter: 265 loss: 1.47084165e-06
Iter: 266 loss: 1.422163e-06
Iter: 267 loss: 1.4162631e-06
Iter: 268 loss: 1.43566479e-06
Iter: 269 loss: 1.41459282e-06
Iter: 270 loss: 1.40794361e-06
Iter: 271 loss: 1.4675793e-06
Iter: 272 loss: 1.40764269e-06
Iter: 273 loss: 1.40340092e-06
Iter: 274 loss: 1.40012799e-06
Iter: 275 loss: 1.39882059e-06
Iter: 276 loss: 1.3929232e-06
Iter: 277 loss: 1.39944746e-06
Iter: 278 loss: 1.38967596e-06
Iter: 279 loss: 1.38200835e-06
Iter: 280 loss: 1.46239881e-06
Iter: 281 loss: 1.38185374e-06
Iter: 282 loss: 1.37790528e-06
Iter: 283 loss: 1.37381653e-06
Iter: 284 loss: 1.37313612e-06
Iter: 285 loss: 1.3675899e-06
Iter: 286 loss: 1.44162232e-06
Iter: 287 loss: 1.36753897e-06
Iter: 288 loss: 1.36344624e-06
Iter: 289 loss: 1.36044309e-06
Iter: 290 loss: 1.35910841e-06
Iter: 291 loss: 1.35183802e-06
Iter: 292 loss: 1.36375752e-06
Iter: 293 loss: 1.34855111e-06
Iter: 294 loss: 1.34292202e-06
Iter: 295 loss: 1.36781478e-06
Iter: 296 loss: 1.34176105e-06
Iter: 297 loss: 1.33539072e-06
Iter: 298 loss: 1.34460038e-06
Iter: 299 loss: 1.33233482e-06
Iter: 300 loss: 1.33277172e-06
Iter: 301 loss: 1.33082051e-06
Iter: 302 loss: 1.32892546e-06
Iter: 303 loss: 1.32308401e-06
Iter: 304 loss: 1.34069637e-06
Iter: 305 loss: 1.3201701e-06
Iter: 306 loss: 1.31440697e-06
Iter: 307 loss: 1.33768845e-06
Iter: 308 loss: 1.31306797e-06
Iter: 309 loss: 1.30768285e-06
Iter: 310 loss: 1.34791219e-06
Iter: 311 loss: 1.30724288e-06
Iter: 312 loss: 1.30152102e-06
Iter: 313 loss: 1.31177444e-06
Iter: 314 loss: 1.2990489e-06
Iter: 315 loss: 1.29573732e-06
Iter: 316 loss: 1.29377599e-06
Iter: 317 loss: 1.29238913e-06
Iter: 318 loss: 1.28832016e-06
Iter: 319 loss: 1.34988136e-06
Iter: 320 loss: 1.28829652e-06
Iter: 321 loss: 1.2851267e-06
Iter: 322 loss: 1.28998909e-06
Iter: 323 loss: 1.28357033e-06
Iter: 324 loss: 1.2802243e-06
Iter: 325 loss: 1.27858334e-06
Iter: 326 loss: 1.27691078e-06
Iter: 327 loss: 1.2723076e-06
Iter: 328 loss: 1.33078413e-06
Iter: 329 loss: 1.27226781e-06
Iter: 330 loss: 1.2690864e-06
Iter: 331 loss: 1.26410066e-06
Iter: 332 loss: 1.26405939e-06
Iter: 333 loss: 1.25988959e-06
Iter: 334 loss: 1.28862587e-06
Iter: 335 loss: 1.25948031e-06
Iter: 336 loss: 1.25491374e-06
Iter: 337 loss: 1.25404244e-06
Iter: 338 loss: 1.25102088e-06
Iter: 339 loss: 1.25462805e-06
Iter: 340 loss: 1.2489354e-06
Iter: 341 loss: 1.24724045e-06
Iter: 342 loss: 1.2447133e-06
Iter: 343 loss: 1.24463668e-06
Iter: 344 loss: 1.24194696e-06
Iter: 345 loss: 1.23994141e-06
Iter: 346 loss: 1.23908444e-06
Iter: 347 loss: 1.23533425e-06
Iter: 348 loss: 1.25301688e-06
Iter: 349 loss: 1.23468385e-06
Iter: 350 loss: 1.23083703e-06
Iter: 351 loss: 1.26318912e-06
Iter: 352 loss: 1.23063955e-06
Iter: 353 loss: 1.22811707e-06
Iter: 354 loss: 1.22327663e-06
Iter: 355 loss: 1.32854143e-06
Iter: 356 loss: 1.22324423e-06
Iter: 357 loss: 1.22038898e-06
Iter: 358 loss: 1.25939277e-06
Iter: 359 loss: 1.22035431e-06
Iter: 360 loss: 1.21732864e-06
Iter: 361 loss: 1.22306892e-06
Iter: 362 loss: 1.21603034e-06
Iter: 363 loss: 1.21307994e-06
Iter: 364 loss: 1.21835672e-06
Iter: 365 loss: 1.21180415e-06
Iter: 366 loss: 1.20915024e-06
Iter: 367 loss: 1.21351582e-06
Iter: 368 loss: 1.20786899e-06
Iter: 369 loss: 1.20357072e-06
Iter: 370 loss: 1.21243215e-06
Iter: 371 loss: 1.20194431e-06
Iter: 372 loss: 1.19894207e-06
Iter: 373 loss: 1.19519814e-06
Iter: 374 loss: 1.1949229e-06
Iter: 375 loss: 1.19533695e-06
Iter: 376 loss: 1.19337687e-06
Iter: 377 loss: 1.1914899e-06
Iter: 378 loss: 1.19130493e-06
Iter: 379 loss: 1.18988055e-06
Iter: 380 loss: 1.18762637e-06
Iter: 381 loss: 1.18449077e-06
Iter: 382 loss: 1.18437436e-06
Iter: 383 loss: 1.18115213e-06
Iter: 384 loss: 1.18710329e-06
Iter: 385 loss: 1.17980426e-06
Iter: 386 loss: 1.17682737e-06
Iter: 387 loss: 1.17682634e-06
Iter: 388 loss: 1.17472644e-06
Iter: 389 loss: 1.17771719e-06
Iter: 390 loss: 1.17367335e-06
Iter: 391 loss: 1.1711079e-06
Iter: 392 loss: 1.16608362e-06
Iter: 393 loss: 1.2677167e-06
Iter: 394 loss: 1.16609863e-06
Iter: 395 loss: 1.16197555e-06
Iter: 396 loss: 1.21806806e-06
Iter: 397 loss: 1.16192257e-06
Iter: 398 loss: 1.15836929e-06
Iter: 399 loss: 1.17769878e-06
Iter: 400 loss: 1.15780335e-06
Iter: 401 loss: 1.15559396e-06
Iter: 402 loss: 1.15864714e-06
Iter: 403 loss: 1.15441208e-06
Iter: 404 loss: 1.15271962e-06
Iter: 405 loss: 1.1545136e-06
Iter: 406 loss: 1.15177374e-06
Iter: 407 loss: 1.14921102e-06
Iter: 408 loss: 1.16155104e-06
Iter: 409 loss: 1.14872682e-06
Iter: 410 loss: 1.1468062e-06
Iter: 411 loss: 1.14550176e-06
Iter: 412 loss: 1.14479724e-06
Iter: 413 loss: 1.14203203e-06
Iter: 414 loss: 1.15965781e-06
Iter: 415 loss: 1.14172042e-06
Iter: 416 loss: 1.1404311e-06
Iter: 417 loss: 1.14016257e-06
Iter: 418 loss: 1.13937131e-06
Iter: 419 loss: 1.13698968e-06
Iter: 420 loss: 1.14138527e-06
Iter: 421 loss: 1.13539363e-06
Iter: 422 loss: 1.13132376e-06
Iter: 423 loss: 1.14111185e-06
Iter: 424 loss: 1.12980501e-06
Iter: 425 loss: 1.12641123e-06
Iter: 426 loss: 1.1423765e-06
Iter: 427 loss: 1.12577527e-06
Iter: 428 loss: 1.12281555e-06
Iter: 429 loss: 1.16533738e-06
Iter: 430 loss: 1.12281668e-06
Iter: 431 loss: 1.12081443e-06
Iter: 432 loss: 1.12029034e-06
Iter: 433 loss: 1.11908071e-06
Iter: 434 loss: 1.11712234e-06
Iter: 435 loss: 1.11628913e-06
Iter: 436 loss: 1.11529152e-06
Iter: 437 loss: 1.11242025e-06
Iter: 438 loss: 1.14634986e-06
Iter: 439 loss: 1.11234226e-06
Iter: 440 loss: 1.11047621e-06
Iter: 441 loss: 1.11871543e-06
Iter: 442 loss: 1.110011e-06
Iter: 443 loss: 1.10809356e-06
Iter: 444 loss: 1.10664826e-06
Iter: 445 loss: 1.10604117e-06
Iter: 446 loss: 1.10353403e-06
Iter: 447 loss: 1.10920564e-06
Iter: 448 loss: 1.10265148e-06
Iter: 449 loss: 1.09977e-06
Iter: 450 loss: 1.1163761e-06
Iter: 451 loss: 1.09943915e-06
Iter: 452 loss: 1.09704285e-06
Iter: 453 loss: 1.10921792e-06
Iter: 454 loss: 1.09668144e-06
Iter: 455 loss: 1.09501423e-06
Iter: 456 loss: 1.09502571e-06
Iter: 457 loss: 1.09393568e-06
Iter: 458 loss: 1.09083851e-06
Iter: 459 loss: 1.10969017e-06
Iter: 460 loss: 1.09002815e-06
Iter: 461 loss: 1.08774725e-06
Iter: 462 loss: 1.08679023e-06
Iter: 463 loss: 1.0856005e-06
Iter: 464 loss: 1.08343056e-06
Iter: 465 loss: 1.11725592e-06
Iter: 466 loss: 1.08344295e-06
Iter: 467 loss: 1.08110498e-06
Iter: 468 loss: 1.08163272e-06
Iter: 469 loss: 1.07940718e-06
Iter: 470 loss: 1.07856488e-06
Iter: 471 loss: 1.07782728e-06
Iter: 472 loss: 1.07716687e-06
Iter: 473 loss: 1.07501535e-06
Iter: 474 loss: 1.07761787e-06
Iter: 475 loss: 1.07337644e-06
Iter: 476 loss: 1.07047481e-06
Iter: 477 loss: 1.11342661e-06
Iter: 478 loss: 1.07048845e-06
Iter: 479 loss: 1.06831646e-06
Iter: 480 loss: 1.07432516e-06
Iter: 481 loss: 1.06759876e-06
Iter: 482 loss: 1.06527045e-06
Iter: 483 loss: 1.07763344e-06
Iter: 484 loss: 1.06493371e-06
Iter: 485 loss: 1.0635888e-06
Iter: 486 loss: 1.06401387e-06
Iter: 487 loss: 1.06270465e-06
Iter: 488 loss: 1.06117977e-06
Iter: 489 loss: 1.0839417e-06
Iter: 490 loss: 1.06120535e-06
Iter: 491 loss: 1.05997424e-06
Iter: 492 loss: 1.06422829e-06
Iter: 493 loss: 1.05969161e-06
Iter: 494 loss: 1.05878962e-06
Iter: 495 loss: 1.05738764e-06
Iter: 496 loss: 1.05734284e-06
Iter: 497 loss: 1.05534309e-06
Iter: 498 loss: 1.05542244e-06
Iter: 499 loss: 1.05373385e-06
Iter: 500 loss: 1.0513404e-06
Iter: 501 loss: 1.05638628e-06
Iter: 502 loss: 1.0505139e-06
Iter: 503 loss: 1.04846026e-06
Iter: 504 loss: 1.05810955e-06
Iter: 505 loss: 1.04804985e-06
Iter: 506 loss: 1.04658386e-06
Iter: 507 loss: 1.06400773e-06
Iter: 508 loss: 1.04655578e-06
Iter: 509 loss: 1.04479966e-06
Iter: 510 loss: 1.04354137e-06
Iter: 511 loss: 1.04290484e-06
Iter: 512 loss: 1.04078708e-06
Iter: 513 loss: 1.04387686e-06
Iter: 514 loss: 1.03974924e-06
Iter: 515 loss: 1.03783179e-06
Iter: 516 loss: 1.0434776e-06
Iter: 517 loss: 1.03722755e-06
Iter: 518 loss: 1.03544926e-06
Iter: 519 loss: 1.05942092e-06
Iter: 520 loss: 1.03542811e-06
Iter: 521 loss: 1.03439015e-06
Iter: 522 loss: 1.03457978e-06
Iter: 523 loss: 1.03359923e-06
Iter: 524 loss: 1.03216507e-06
Iter: 525 loss: 1.03000502e-06
Iter: 526 loss: 1.02997626e-06
Iter: 527 loss: 1.02793956e-06
Iter: 528 loss: 1.02792524e-06
Iter: 529 loss: 1.02715103e-06
Iter: 530 loss: 1.02707611e-06
Iter: 531 loss: 1.02620038e-06
Iter: 532 loss: 1.02477577e-06
Iter: 533 loss: 1.0247627e-06
Iter: 534 loss: 1.0235492e-06
Iter: 535 loss: 1.02632112e-06
Iter: 536 loss: 1.02311697e-06
Iter: 537 loss: 1.0214726e-06
Iter: 538 loss: 1.02116564e-06
Iter: 539 loss: 1.02006732e-06
Iter: 540 loss: 1.01830756e-06
Iter: 541 loss: 1.02339084e-06
Iter: 542 loss: 1.01774697e-06
Iter: 543 loss: 1.01577587e-06
Iter: 544 loss: 1.01617229e-06
Iter: 545 loss: 1.01431613e-06
Iter: 546 loss: 1.01349258e-06
Iter: 547 loss: 1.01323383e-06
Iter: 548 loss: 1.01209048e-06
Iter: 549 loss: 1.01120781e-06
Iter: 550 loss: 1.01081082e-06
Iter: 551 loss: 1.00935972e-06
Iter: 552 loss: 1.01329374e-06
Iter: 553 loss: 1.00886746e-06
Iter: 554 loss: 1.00740954e-06
Iter: 555 loss: 1.01283047e-06
Iter: 556 loss: 1.00707098e-06
Iter: 557 loss: 1.00502803e-06
Iter: 558 loss: 1.00415673e-06
Iter: 559 loss: 1.0031531e-06
Iter: 560 loss: 1.00161265e-06
Iter: 561 loss: 1.00298678e-06
Iter: 562 loss: 1.00070667e-06
Iter: 563 loss: 9.98790256e-07
Iter: 564 loss: 1.00621503e-06
Iter: 565 loss: 9.98357336e-07
Iter: 566 loss: 9.96759809e-07
Iter: 567 loss: 1.00227044e-06
Iter: 568 loss: 9.96336667e-07
Iter: 569 loss: 9.95820642e-07
Iter: 570 loss: 9.95528467e-07
Iter: 571 loss: 9.94650463e-07
Iter: 572 loss: 9.93036792e-07
Iter: 573 loss: 1.0312599e-06
Iter: 574 loss: 9.93051572e-07
Iter: 575 loss: 9.91695e-07
Iter: 576 loss: 9.92859214e-07
Iter: 577 loss: 9.90915396e-07
Iter: 578 loss: 9.89321e-07
Iter: 579 loss: 9.9333613e-07
Iter: 580 loss: 9.88691681e-07
Iter: 581 loss: 9.87558678e-07
Iter: 582 loss: 1.00242835e-06
Iter: 583 loss: 9.8753e-07
Iter: 584 loss: 9.86518899e-07
Iter: 585 loss: 9.84488793e-07
Iter: 586 loss: 1.02532317e-06
Iter: 587 loss: 9.84503686e-07
Iter: 588 loss: 9.84211624e-07
Iter: 589 loss: 9.83555651e-07
Iter: 590 loss: 9.82788265e-07
Iter: 591 loss: 9.81290896e-07
Iter: 592 loss: 1.01217461e-06
Iter: 593 loss: 9.81249514e-07
Iter: 594 loss: 9.79896e-07
Iter: 595 loss: 9.88189186e-07
Iter: 596 loss: 9.7970451e-07
Iter: 597 loss: 9.78767275e-07
Iter: 598 loss: 9.84191161e-07
Iter: 599 loss: 9.78601861e-07
Iter: 600 loss: 9.77385639e-07
Iter: 601 loss: 9.77121317e-07
Iter: 602 loss: 9.76312549e-07
Iter: 603 loss: 9.75135549e-07
Iter: 604 loss: 9.73363e-07
Iter: 605 loss: 9.73305532e-07
Iter: 606 loss: 9.73245733e-07
Iter: 607 loss: 9.72371708e-07
Iter: 608 loss: 9.71474833e-07
Iter: 609 loss: 9.75374064e-07
Iter: 610 loss: 9.71297368e-07
Iter: 611 loss: 9.70858764e-07
Iter: 612 loss: 9.69703933e-07
Iter: 613 loss: 9.77815603e-07
Iter: 614 loss: 9.69427447e-07
Iter: 615 loss: 9.67989536e-07
Iter: 616 loss: 9.73339297e-07
Iter: 617 loss: 9.67664164e-07
Iter: 618 loss: 9.66378934e-07
Iter: 619 loss: 9.78483172e-07
Iter: 620 loss: 9.66352e-07
Iter: 621 loss: 9.65280151e-07
Iter: 622 loss: 9.66010361e-07
Iter: 623 loss: 9.64648734e-07
Iter: 624 loss: 9.63697e-07
Iter: 625 loss: 9.729971e-07
Iter: 626 loss: 9.63635102e-07
Iter: 627 loss: 9.62688773e-07
Iter: 628 loss: 9.63838147e-07
Iter: 629 loss: 9.62207764e-07
Iter: 630 loss: 9.61250635e-07
Iter: 631 loss: 9.61569185e-07
Iter: 632 loss: 9.60547368e-07
Iter: 633 loss: 9.59480303e-07
Iter: 634 loss: 9.6290637e-07
Iter: 635 loss: 9.59124691e-07
Iter: 636 loss: 9.57781708e-07
Iter: 637 loss: 9.62579634e-07
Iter: 638 loss: 9.5745736e-07
Iter: 639 loss: 9.5605742e-07
Iter: 640 loss: 9.57966677e-07
Iter: 641 loss: 9.55361656e-07
Iter: 642 loss: 9.54391453e-07
Iter: 643 loss: 9.5737073e-07
Iter: 644 loss: 9.54094617e-07
Iter: 645 loss: 9.52619303e-07
Iter: 646 loss: 9.59989734e-07
Iter: 647 loss: 9.52402672e-07
Iter: 648 loss: 9.51863399e-07
Iter: 649 loss: 9.50709932e-07
Iter: 650 loss: 9.66421339e-07
Iter: 651 loss: 9.50635581e-07
Iter: 652 loss: 9.49149523e-07
Iter: 653 loss: 9.50858237e-07
Iter: 654 loss: 9.48348202e-07
Iter: 655 loss: 9.47456101e-07
Iter: 656 loss: 9.47435183e-07
Iter: 657 loss: 9.4655303e-07
Iter: 658 loss: 9.47934268e-07
Iter: 659 loss: 9.46123805e-07
Iter: 660 loss: 9.45422812e-07
Iter: 661 loss: 9.51383811e-07
Iter: 662 loss: 9.45383363e-07
Iter: 663 loss: 9.44738133e-07
Iter: 664 loss: 9.44808562e-07
Iter: 665 loss: 9.4415941e-07
Iter: 666 loss: 9.4319256e-07
Iter: 667 loss: 9.42675911e-07
Iter: 668 loss: 9.42177508e-07
Iter: 669 loss: 9.40917744e-07
Iter: 670 loss: 9.47591161e-07
Iter: 671 loss: 9.40736811e-07
Iter: 672 loss: 9.39336701e-07
Iter: 673 loss: 9.44609837e-07
Iter: 674 loss: 9.3894846e-07
Iter: 675 loss: 9.3812838e-07
Iter: 676 loss: 9.42109637e-07
Iter: 677 loss: 9.37979507e-07
Iter: 678 loss: 9.37431594e-07
Iter: 679 loss: 9.4168314e-07
Iter: 680 loss: 9.37392201e-07
Iter: 681 loss: 9.36588663e-07
Iter: 682 loss: 9.35477374e-07
Iter: 683 loss: 9.35435196e-07
Iter: 684 loss: 9.34761715e-07
Iter: 685 loss: 9.34865909e-07
Iter: 686 loss: 9.34247055e-07
Iter: 687 loss: 9.33242859e-07
Iter: 688 loss: 9.34277068e-07
Iter: 689 loss: 9.32675789e-07
Iter: 690 loss: 9.31636293e-07
Iter: 691 loss: 9.35540299e-07
Iter: 692 loss: 9.31399541e-07
Iter: 693 loss: 9.30396254e-07
Iter: 694 loss: 9.42089287e-07
Iter: 695 loss: 9.30422402e-07
Iter: 696 loss: 9.29722432e-07
Iter: 697 loss: 9.30418423e-07
Iter: 698 loss: 9.29363239e-07
Iter: 699 loss: 9.28486315e-07
Iter: 700 loss: 9.30952297e-07
Iter: 701 loss: 9.28201587e-07
Iter: 702 loss: 9.27381507e-07
Iter: 703 loss: 9.26714733e-07
Iter: 704 loss: 9.26444272e-07
Iter: 705 loss: 9.25267614e-07
Iter: 706 loss: 9.29272801e-07
Iter: 707 loss: 9.25010795e-07
Iter: 708 loss: 9.24003075e-07
Iter: 709 loss: 9.37523055e-07
Iter: 710 loss: 9.2401541e-07
Iter: 711 loss: 9.23224377e-07
Iter: 712 loss: 9.2270443e-07
Iter: 713 loss: 9.22437721e-07
Iter: 714 loss: 9.22269919e-07
Iter: 715 loss: 9.21881508e-07
Iter: 716 loss: 9.21565743e-07
Iter: 717 loss: 9.20559842e-07
Iter: 718 loss: 9.25648806e-07
Iter: 719 loss: 9.20289153e-07
Iter: 720 loss: 9.1941e-07
Iter: 721 loss: 9.2711764e-07
Iter: 722 loss: 9.19390516e-07
Iter: 723 loss: 9.1874881e-07
Iter: 724 loss: 9.1774956e-07
Iter: 725 loss: 9.17734212e-07
Iter: 726 loss: 9.16555393e-07
Iter: 727 loss: 9.26363441e-07
Iter: 728 loss: 9.16459726e-07
Iter: 729 loss: 9.15509872e-07
Iter: 730 loss: 9.19950594e-07
Iter: 731 loss: 9.15331157e-07
Iter: 732 loss: 9.1419065e-07
Iter: 733 loss: 9.181461e-07
Iter: 734 loss: 9.1391837e-07
Iter: 735 loss: 9.13255292e-07
Iter: 736 loss: 9.14825e-07
Iter: 737 loss: 9.1303076e-07
Iter: 738 loss: 9.12239898e-07
Iter: 739 loss: 9.12378766e-07
Iter: 740 loss: 9.11616326e-07
Iter: 741 loss: 9.10601e-07
Iter: 742 loss: 9.12643088e-07
Iter: 743 loss: 9.10157041e-07
Iter: 744 loss: 9.09128403e-07
Iter: 745 loss: 9.09907271e-07
Iter: 746 loss: 9.08589357e-07
Iter: 747 loss: 9.08147229e-07
Iter: 748 loss: 9.07982155e-07
Iter: 749 loss: 9.07505523e-07
Iter: 750 loss: 9.08465836e-07
Iter: 751 loss: 9.07381093e-07
Iter: 752 loss: 9.06754963e-07
Iter: 753 loss: 9.08026095e-07
Iter: 754 loss: 9.0649786e-07
Iter: 755 loss: 9.06039872e-07
Iter: 756 loss: 9.05026582e-07
Iter: 757 loss: 9.21234346e-07
Iter: 758 loss: 9.05036075e-07
Iter: 759 loss: 9.03931e-07
Iter: 760 loss: 9.08927916e-07
Iter: 761 loss: 9.03748571e-07
Iter: 762 loss: 9.02771603e-07
Iter: 763 loss: 9.03722821e-07
Iter: 764 loss: 9.02218972e-07
Iter: 765 loss: 9.01105636e-07
Iter: 766 loss: 9.06359219e-07
Iter: 767 loss: 9.00893951e-07
Iter: 768 loss: 9.00092175e-07
Iter: 769 loss: 9.01884505e-07
Iter: 770 loss: 8.9973696e-07
Iter: 771 loss: 8.99081783e-07
Iter: 772 loss: 8.99048302e-07
Iter: 773 loss: 8.98546375e-07
Iter: 774 loss: 8.97898076e-07
Iter: 775 loss: 8.97867892e-07
Iter: 776 loss: 8.96855568e-07
Iter: 777 loss: 9.04217529e-07
Iter: 778 loss: 8.96771894e-07
Iter: 779 loss: 8.96291226e-07
Iter: 780 loss: 8.97093798e-07
Iter: 781 loss: 8.9606965e-07
Iter: 782 loss: 8.9555391e-07
Iter: 783 loss: 8.957158e-07
Iter: 784 loss: 8.95188748e-07
Iter: 785 loss: 8.9452044e-07
Iter: 786 loss: 8.94508617e-07
Iter: 787 loss: 8.942161e-07
Iter: 788 loss: 8.93792276e-07
Iter: 789 loss: 8.93791878e-07
Iter: 790 loss: 8.93075e-07
Iter: 791 loss: 8.93776701e-07
Iter: 792 loss: 8.92720777e-07
Iter: 793 loss: 8.91894729e-07
Iter: 794 loss: 8.91910304e-07
Iter: 795 loss: 8.9124012e-07
Iter: 796 loss: 8.9029038e-07
Iter: 797 loss: 8.93017386e-07
Iter: 798 loss: 8.90015826e-07
Iter: 799 loss: 8.89163914e-07
Iter: 800 loss: 8.92642788e-07
Iter: 801 loss: 8.88945408e-07
Iter: 802 loss: 8.88047907e-07
Iter: 803 loss: 8.88749184e-07
Iter: 804 loss: 8.87501585e-07
Iter: 805 loss: 8.86507166e-07
Iter: 806 loss: 8.90626609e-07
Iter: 807 loss: 8.8629838e-07
Iter: 808 loss: 8.8589286e-07
Iter: 809 loss: 8.85776956e-07
Iter: 810 loss: 8.85462612e-07
Iter: 811 loss: 8.84881558e-07
Iter: 812 loss: 8.99114355e-07
Iter: 813 loss: 8.84893382e-07
Iter: 814 loss: 8.84183692e-07
Iter: 815 loss: 8.88761633e-07
Iter: 816 loss: 8.84095527e-07
Iter: 817 loss: 8.83598091e-07
Iter: 818 loss: 8.88880663e-07
Iter: 819 loss: 8.83595249e-07
Iter: 820 loss: 8.83130269e-07
Iter: 821 loss: 8.8412304e-07
Iter: 822 loss: 8.82968266e-07
Iter: 823 loss: 8.82515906e-07
Iter: 824 loss: 8.81687583e-07
Iter: 825 loss: 8.98693827e-07
Iter: 826 loss: 8.81657797e-07
Iter: 827 loss: 8.81061283e-07
Iter: 828 loss: 8.87020747e-07
Iter: 829 loss: 8.81090045e-07
Iter: 830 loss: 8.8043e-07
Iter: 831 loss: 8.81329754e-07
Iter: 832 loss: 8.80123594e-07
Iter: 833 loss: 8.79595e-07
Iter: 834 loss: 8.80042535e-07
Iter: 835 loss: 8.79319032e-07
Iter: 836 loss: 8.78612298e-07
Iter: 837 loss: 8.78570404e-07
Iter: 838 loss: 8.78041135e-07
Iter: 839 loss: 8.77190075e-07
Iter: 840 loss: 8.84443e-07
Iter: 841 loss: 8.77134426e-07
Iter: 842 loss: 8.7658043e-07
Iter: 843 loss: 8.78236733e-07
Iter: 844 loss: 8.76374543e-07
Iter: 845 loss: 8.75921501e-07
Iter: 846 loss: 8.82544782e-07
Iter: 847 loss: 8.75941282e-07
Iter: 848 loss: 8.75493242e-07
Iter: 849 loss: 8.74788896e-07
Iter: 850 loss: 8.74782529e-07
Iter: 851 loss: 8.74093814e-07
Iter: 852 loss: 8.79215804e-07
Iter: 853 loss: 8.74040381e-07
Iter: 854 loss: 8.73409419e-07
Iter: 855 loss: 8.80263315e-07
Iter: 856 loss: 8.73407544e-07
Iter: 857 loss: 8.73147656e-07
Iter: 858 loss: 8.72797159e-07
Iter: 859 loss: 8.72801593e-07
Iter: 860 loss: 8.72271642e-07
Iter: 861 loss: 8.73433692e-07
Iter: 862 loss: 8.72076e-07
Iter: 863 loss: 8.71504199e-07
Iter: 864 loss: 8.71304451e-07
Iter: 865 loss: 8.70909275e-07
Iter: 866 loss: 8.70512736e-07
Iter: 867 loss: 8.70498184e-07
Iter: 868 loss: 8.70103747e-07
Iter: 869 loss: 8.69298049e-07
Iter: 870 loss: 8.8408234e-07
Iter: 871 loss: 8.69290034e-07
Iter: 872 loss: 8.6856403e-07
Iter: 873 loss: 8.73101158e-07
Iter: 874 loss: 8.6844932e-07
Iter: 875 loss: 8.67833194e-07
Iter: 876 loss: 8.67621907e-07
Iter: 877 loss: 8.67222809e-07
Iter: 878 loss: 8.66452694e-07
Iter: 879 loss: 8.75422415e-07
Iter: 880 loss: 8.66416485e-07
Iter: 881 loss: 8.65652339e-07
Iter: 882 loss: 8.70042356e-07
Iter: 883 loss: 8.65547236e-07
Iter: 884 loss: 8.64997105e-07
Iter: 885 loss: 8.65154959e-07
Iter: 886 loss: 8.64608467e-07
Iter: 887 loss: 8.64375352e-07
Iter: 888 loss: 8.64260755e-07
Iter: 889 loss: 8.64002345e-07
Iter: 890 loss: 8.6374223e-07
Iter: 891 loss: 8.63668902e-07
Iter: 892 loss: 8.633383e-07
Iter: 893 loss: 8.62885031e-07
Iter: 894 loss: 8.62817956e-07
Iter: 895 loss: 8.62079901e-07
Iter: 896 loss: 8.66167e-07
Iter: 897 loss: 8.61970477e-07
Iter: 898 loss: 8.61442e-07
Iter: 899 loss: 8.62413572e-07
Iter: 900 loss: 8.61191609e-07
Iter: 901 loss: 8.6067115e-07
Iter: 902 loss: 8.63407536e-07
Iter: 903 loss: 8.60591683e-07
Iter: 904 loss: 8.60006e-07
Iter: 905 loss: 8.60280295e-07
Iter: 906 loss: 8.59629665e-07
Iter: 907 loss: 8.58982787e-07
Iter: 908 loss: 8.58574481e-07
Iter: 909 loss: 8.58360522e-07
Iter: 910 loss: 8.57476607e-07
Iter: 911 loss: 8.62696481e-07
Iter: 912 loss: 8.57342229e-07
Iter: 913 loss: 8.565695e-07
Iter: 914 loss: 8.59084707e-07
Iter: 915 loss: 8.56347185e-07
Iter: 916 loss: 8.55918529e-07
Iter: 917 loss: 8.55870553e-07
Iter: 918 loss: 8.55607254e-07
Iter: 919 loss: 8.55230894e-07
Iter: 920 loss: 8.55204e-07
Iter: 921 loss: 8.54914219e-07
Iter: 922 loss: 8.54839072e-07
Iter: 923 loss: 8.54637619e-07
Iter: 924 loss: 8.54119094e-07
Iter: 925 loss: 8.58444764e-07
Iter: 926 loss: 8.54068105e-07
Iter: 927 loss: 8.53466474e-07
Iter: 928 loss: 8.54348173e-07
Iter: 929 loss: 8.53230119e-07
Iter: 930 loss: 8.52557037e-07
Iter: 931 loss: 8.55741405e-07
Iter: 932 loss: 8.52396e-07
Iter: 933 loss: 8.5182819e-07
Iter: 934 loss: 8.55438e-07
Iter: 935 loss: 8.5177021e-07
Iter: 936 loss: 8.51248956e-07
Iter: 937 loss: 8.51196091e-07
Iter: 938 loss: 8.50798301e-07
Iter: 939 loss: 8.50145796e-07
Iter: 940 loss: 8.50721506e-07
Iter: 941 loss: 8.49729531e-07
Iter: 942 loss: 8.49035473e-07
Iter: 943 loss: 8.56765723e-07
Iter: 944 loss: 8.48998525e-07
Iter: 945 loss: 8.48373247e-07
Iter: 946 loss: 8.48028321e-07
Iter: 947 loss: 8.47762635e-07
Iter: 948 loss: 8.47126955e-07
Iter: 949 loss: 8.51997584e-07
Iter: 950 loss: 8.47107e-07
Iter: 951 loss: 8.4669233e-07
Iter: 952 loss: 8.51361392e-07
Iter: 953 loss: 8.46678802e-07
Iter: 954 loss: 8.46321313e-07
Iter: 955 loss: 8.46709781e-07
Iter: 956 loss: 8.4611878e-07
Iter: 957 loss: 8.45653062e-07
Iter: 958 loss: 8.48918717e-07
Iter: 959 loss: 8.456459e-07
Iter: 960 loss: 8.45441718e-07
Iter: 961 loss: 8.44827355e-07
Iter: 962 loss: 8.46514354e-07
Iter: 963 loss: 8.44471401e-07
Iter: 964 loss: 8.43737e-07
Iter: 965 loss: 8.51064442e-07
Iter: 966 loss: 8.43716521e-07
Iter: 967 loss: 8.43175371e-07
Iter: 968 loss: 8.46070634e-07
Iter: 969 loss: 8.43107216e-07
Iter: 970 loss: 8.42517807e-07
Iter: 971 loss: 8.4404644e-07
Iter: 972 loss: 8.42283953e-07
Iter: 973 loss: 8.41804422e-07
Iter: 974 loss: 8.41476094e-07
Iter: 975 loss: 8.41296355e-07
Iter: 976 loss: 8.40611392e-07
Iter: 977 loss: 8.46456885e-07
Iter: 978 loss: 8.40585244e-07
Iter: 979 loss: 8.39933e-07
Iter: 980 loss: 8.41275664e-07
Iter: 981 loss: 8.3967825e-07
Iter: 982 loss: 8.39232882e-07
Iter: 983 loss: 8.39566269e-07
Iter: 984 loss: 8.38935875e-07
Iter: 985 loss: 8.38501705e-07
Iter: 986 loss: 8.38522737e-07
Iter: 987 loss: 8.38133246e-07
Iter: 988 loss: 8.39050813e-07
Iter: 989 loss: 8.38040307e-07
Iter: 990 loss: 8.37737275e-07
Iter: 991 loss: 8.41101325e-07
Iter: 992 loss: 8.37705215e-07
Iter: 993 loss: 8.37516666e-07
Iter: 994 loss: 8.36959885e-07
Iter: 995 loss: 8.38227947e-07
Iter: 996 loss: 8.36684308e-07
Iter: 997 loss: 8.36038794e-07
Iter: 998 loss: 8.38566962e-07
Iter: 999 loss: 8.35881679e-07
Iter: 1000 loss: 8.35116055e-07
Iter: 1001 loss: 8.38808887e-07
Iter: 1002 loss: 8.35032438e-07
Iter: 1003 loss: 8.34385787e-07
Iter: 1004 loss: 8.39662e-07
Iter: 1005 loss: 8.34326443e-07
Iter: 1006 loss: 8.33903641e-07
Iter: 1007 loss: 8.33374088e-07
Iter: 1008 loss: 8.33356353e-07
Iter: 1009 loss: 8.32647061e-07
Iter: 1010 loss: 8.38158769e-07
Iter: 1011 loss: 8.32588455e-07
Iter: 1012 loss: 8.32042474e-07
Iter: 1013 loss: 8.3522167e-07
Iter: 1014 loss: 8.31957436e-07
Iter: 1015 loss: 8.31550437e-07
Iter: 1016 loss: 8.32338628e-07
Iter: 1017 loss: 8.31385307e-07
Iter: 1018 loss: 8.30954605e-07
Iter: 1019 loss: 8.31943112e-07
Iter: 1020 loss: 8.30802151e-07
Iter: 1021 loss: 8.30449608e-07
Iter: 1022 loss: 8.35638559e-07
Iter: 1023 loss: 8.30436591e-07
Iter: 1024 loss: 8.30209956e-07
Iter: 1025 loss: 8.30910153e-07
Iter: 1026 loss: 8.30111276e-07
Iter: 1027 loss: 8.29789144e-07
Iter: 1028 loss: 8.29260728e-07
Iter: 1029 loss: 8.29243277e-07
Iter: 1030 loss: 8.28780912e-07
Iter: 1031 loss: 8.29037049e-07
Iter: 1032 loss: 8.28476118e-07
Iter: 1033 loss: 8.27836516e-07
Iter: 1034 loss: 8.27852318e-07
Iter: 1035 loss: 8.27307133e-07
Iter: 1036 loss: 8.26954192e-07
Iter: 1037 loss: 8.26887288e-07
Iter: 1038 loss: 8.26523546e-07
Iter: 1039 loss: 8.26227165e-07
Iter: 1040 loss: 8.2611075e-07
Iter: 1041 loss: 8.25604502e-07
Iter: 1042 loss: 8.26187147e-07
Iter: 1043 loss: 8.25286e-07
Iter: 1044 loss: 8.24722349e-07
Iter: 1045 loss: 8.29171142e-07
Iter: 1046 loss: 8.24703e-07
Iter: 1047 loss: 8.24214339e-07
Iter: 1048 loss: 8.2542573e-07
Iter: 1049 loss: 8.24037045e-07
Iter: 1050 loss: 8.23622372e-07
Iter: 1051 loss: 8.23994583e-07
Iter: 1052 loss: 8.23396817e-07
Iter: 1053 loss: 8.22911829e-07
Iter: 1054 loss: 8.28805241e-07
Iter: 1055 loss: 8.22911375e-07
Iter: 1056 loss: 8.22510287e-07
Iter: 1057 loss: 8.23581729e-07
Iter: 1058 loss: 8.22417292e-07
Iter: 1059 loss: 8.22000914e-07
Iter: 1060 loss: 8.22556672e-07
Iter: 1061 loss: 8.2177155e-07
Iter: 1062 loss: 8.21352614e-07
Iter: 1063 loss: 8.20967e-07
Iter: 1064 loss: 8.20854609e-07
Iter: 1065 loss: 8.20225e-07
Iter: 1066 loss: 8.20252524e-07
Iter: 1067 loss: 8.19733145e-07
Iter: 1068 loss: 8.19043237e-07
Iter: 1069 loss: 8.24488779e-07
Iter: 1070 loss: 8.1902e-07
Iter: 1071 loss: 8.18497369e-07
Iter: 1072 loss: 8.23419668e-07
Iter: 1073 loss: 8.18453941e-07
Iter: 1074 loss: 8.18018748e-07
Iter: 1075 loss: 8.1793155e-07
Iter: 1076 loss: 8.17634827e-07
Iter: 1077 loss: 8.17050818e-07
Iter: 1078 loss: 8.1676e-07
Iter: 1079 loss: 8.16454303e-07
Iter: 1080 loss: 8.16261036e-07
Iter: 1081 loss: 8.1607817e-07
Iter: 1082 loss: 8.15810893e-07
Iter: 1083 loss: 8.15174872e-07
Iter: 1084 loss: 8.24224912e-07
Iter: 1085 loss: 8.1514554e-07
Iter: 1086 loss: 8.14665611e-07
Iter: 1087 loss: 8.14635e-07
Iter: 1088 loss: 8.14215e-07
Iter: 1089 loss: 8.15808221e-07
Iter: 1090 loss: 8.14095074e-07
Iter: 1091 loss: 8.13705356e-07
Iter: 1092 loss: 8.14891678e-07
Iter: 1093 loss: 8.13590191e-07
Iter: 1094 loss: 8.13254815e-07
Iter: 1095 loss: 8.12851113e-07
Iter: 1096 loss: 8.12844121e-07
Iter: 1097 loss: 8.12343956e-07
Iter: 1098 loss: 8.133336e-07
Iter: 1099 loss: 8.1216956e-07
Iter: 1100 loss: 8.11668087e-07
Iter: 1101 loss: 8.12772214e-07
Iter: 1102 loss: 8.11509153e-07
Iter: 1103 loss: 8.11008363e-07
Iter: 1104 loss: 8.12461451e-07
Iter: 1105 loss: 8.10850793e-07
Iter: 1106 loss: 8.10385814e-07
Iter: 1107 loss: 8.15369162e-07
Iter: 1108 loss: 8.10375184e-07
Iter: 1109 loss: 8.09977564e-07
Iter: 1110 loss: 8.09290839e-07
Iter: 1111 loss: 8.25157656e-07
Iter: 1112 loss: 8.09289872e-07
Iter: 1113 loss: 8.08648906e-07
Iter: 1114 loss: 8.14985242e-07
Iter: 1115 loss: 8.08633558e-07
Iter: 1116 loss: 8.0819882e-07
Iter: 1117 loss: 8.10052427e-07
Iter: 1118 loss: 8.08120433e-07
Iter: 1119 loss: 8.07555921e-07
Iter: 1120 loss: 8.07463607e-07
Iter: 1121 loss: 8.0707764e-07
Iter: 1122 loss: 8.06989192e-07
Iter: 1123 loss: 8.06783532e-07
Iter: 1124 loss: 8.06567812e-07
Iter: 1125 loss: 8.06408252e-07
Iter: 1126 loss: 8.06330263e-07
Iter: 1127 loss: 8.05940772e-07
Iter: 1128 loss: 8.0685254e-07
Iter: 1129 loss: 8.05776892e-07
Iter: 1130 loss: 8.05553611e-07
Iter: 1131 loss: 8.05176057e-07
Iter: 1132 loss: 8.05184129e-07
Iter: 1133 loss: 8.04665547e-07
Iter: 1134 loss: 8.05448906e-07
Iter: 1135 loss: 8.04377692e-07
Iter: 1136 loss: 8.03831767e-07
Iter: 1137 loss: 8.05877221e-07
Iter: 1138 loss: 8.03663283e-07
Iter: 1139 loss: 8.031e-07
Iter: 1140 loss: 8.07756692e-07
Iter: 1141 loss: 8.03053297e-07
Iter: 1142 loss: 8.0255586e-07
Iter: 1143 loss: 8.03309831e-07
Iter: 1144 loss: 8.02340651e-07
Iter: 1145 loss: 8.01929275e-07
Iter: 1146 loss: 8.02841669e-07
Iter: 1147 loss: 8.01783926e-07
Iter: 1148 loss: 8.01374483e-07
Iter: 1149 loss: 8.01311558e-07
Iter: 1150 loss: 8.01021315e-07
Iter: 1151 loss: 8.00416046e-07
Iter: 1152 loss: 8.07108961e-07
Iter: 1153 loss: 8.00439807e-07
Iter: 1154 loss: 8.00152293e-07
Iter: 1155 loss: 8.0292557e-07
Iter: 1156 loss: 8.00118414e-07
Iter: 1157 loss: 7.99881491e-07
Iter: 1158 loss: 8.00523253e-07
Iter: 1159 loss: 7.99749614e-07
Iter: 1160 loss: 7.9951019e-07
Iter: 1161 loss: 7.99904e-07
Iter: 1162 loss: 7.99408554e-07
Iter: 1163 loss: 7.99128031e-07
Iter: 1164 loss: 7.99044869e-07
Iter: 1165 loss: 7.98896622e-07
Iter: 1166 loss: 7.98467795e-07
Iter: 1167 loss: 7.97865141e-07
Iter: 1168 loss: 7.97847861e-07
Iter: 1169 loss: 7.97127e-07
Iter: 1170 loss: 8.01636588e-07
Iter: 1171 loss: 7.97028633e-07
Iter: 1172 loss: 7.96499137e-07
Iter: 1173 loss: 8.01360216e-07
Iter: 1174 loss: 7.96467e-07
Iter: 1175 loss: 7.96041888e-07
Iter: 1176 loss: 7.98533563e-07
Iter: 1177 loss: 7.95973392e-07
Iter: 1178 loss: 7.95656945e-07
Iter: 1179 loss: 7.95361302e-07
Iter: 1180 loss: 7.95279902e-07
Iter: 1181 loss: 7.94760467e-07
Iter: 1182 loss: 7.96445249e-07
Iter: 1183 loss: 7.94562538e-07
Iter: 1184 loss: 7.94013374e-07
Iter: 1185 loss: 7.97284e-07
Iter: 1186 loss: 7.93928507e-07
Iter: 1187 loss: 7.93568631e-07
Iter: 1188 loss: 7.97487587e-07
Iter: 1189 loss: 7.93547713e-07
Iter: 1190 loss: 7.93271454e-07
Iter: 1191 loss: 7.95353344e-07
Iter: 1192 loss: 7.93265258e-07
Iter: 1193 loss: 7.93055847e-07
Iter: 1194 loss: 7.92819833e-07
Iter: 1195 loss: 7.92772198e-07
Iter: 1196 loss: 7.92438414e-07
Iter: 1197 loss: 7.94170433e-07
Iter: 1198 loss: 7.9237077e-07
Iter: 1199 loss: 7.92087576e-07
Iter: 1200 loss: 7.91462355e-07
Iter: 1201 loss: 8.00666726e-07
Iter: 1202 loss: 7.91444677e-07
Iter: 1203 loss: 7.90787453e-07
Iter: 1204 loss: 7.94343407e-07
Iter: 1205 loss: 7.90673141e-07
Iter: 1206 loss: 7.90031265e-07
Iter: 1207 loss: 7.91348498e-07
Iter: 1208 loss: 7.89849707e-07
Iter: 1209 loss: 7.8957305e-07
Iter: 1210 loss: 7.89512171e-07
Iter: 1211 loss: 7.89293551e-07
Iter: 1212 loss: 7.88894397e-07
Iter: 1213 loss: 7.88877458e-07
Iter: 1214 loss: 7.88397756e-07
Iter: 1215 loss: 7.88888087e-07
Iter: 1216 loss: 7.88095178e-07
Iter: 1217 loss: 7.87562e-07
Iter: 1218 loss: 7.91911646e-07
Iter: 1219 loss: 7.87510771e-07
Iter: 1220 loss: 7.87145154e-07
Iter: 1221 loss: 7.90611e-07
Iter: 1222 loss: 7.87136742e-07
Iter: 1223 loss: 7.86935345e-07
Iter: 1224 loss: 7.89741762e-07
Iter: 1225 loss: 7.86945748e-07
Iter: 1226 loss: 7.86780902e-07
Iter: 1227 loss: 7.86582405e-07
Iter: 1228 loss: 7.8654648e-07
Iter: 1229 loss: 7.86240889e-07
Iter: 1230 loss: 7.86995486e-07
Iter: 1231 loss: 7.86136752e-07
Iter: 1232 loss: 7.85821157e-07
Iter: 1233 loss: 7.8578671e-07
Iter: 1234 loss: 7.85563657e-07
Iter: 1235 loss: 7.85074349e-07
Iter: 1236 loss: 7.85360726e-07
Iter: 1237 loss: 7.84784163e-07
Iter: 1238 loss: 7.84235795e-07
Iter: 1239 loss: 7.84360054e-07
Iter: 1240 loss: 7.83819303e-07
Iter: 1241 loss: 7.83563905e-07
Iter: 1242 loss: 7.83514224e-07
Iter: 1243 loss: 7.83189421e-07
Iter: 1244 loss: 7.83220457e-07
Iter: 1245 loss: 7.82931409e-07
Iter: 1246 loss: 7.8255141e-07
Iter: 1247 loss: 7.83031169e-07
Iter: 1248 loss: 7.82383381e-07
Iter: 1249 loss: 7.81927326e-07
Iter: 1250 loss: 7.82701136e-07
Iter: 1251 loss: 7.81743495e-07
Iter: 1252 loss: 7.81404651e-07
Iter: 1253 loss: 7.81411e-07
Iter: 1254 loss: 7.81231506e-07
Iter: 1255 loss: 7.82458699e-07
Iter: 1256 loss: 7.81212634e-07
Iter: 1257 loss: 7.81007543e-07
Iter: 1258 loss: 7.80755272e-07
Iter: 1259 loss: 7.80721393e-07
Iter: 1260 loss: 7.80340542e-07
Iter: 1261 loss: 7.81421477e-07
Iter: 1262 loss: 7.80237201e-07
Iter: 1263 loss: 7.79836569e-07
Iter: 1264 loss: 7.79731295e-07
Iter: 1265 loss: 7.79442132e-07
Iter: 1266 loss: 7.78899675e-07
Iter: 1267 loss: 7.79911204e-07
Iter: 1268 loss: 7.78697881e-07
Iter: 1269 loss: 7.78153094e-07
Iter: 1270 loss: 7.78738126e-07
Iter: 1271 loss: 7.77828e-07
Iter: 1272 loss: 7.7734262e-07
Iter: 1273 loss: 7.80871517e-07
Iter: 1274 loss: 7.77275261e-07
Iter: 1275 loss: 7.76908792e-07
Iter: 1276 loss: 7.8108161e-07
Iter: 1277 loss: 7.76917545e-07
Iter: 1278 loss: 7.76622187e-07
Iter: 1279 loss: 7.76492755e-07
Iter: 1280 loss: 7.76325919e-07
Iter: 1281 loss: 7.75959279e-07
Iter: 1282 loss: 7.75706894e-07
Iter: 1283 loss: 7.75588603e-07
Iter: 1284 loss: 7.75237822e-07
Iter: 1285 loss: 7.75218666e-07
Iter: 1286 loss: 7.74963269e-07
Iter: 1287 loss: 7.77180787e-07
Iter: 1288 loss: 7.74948e-07
Iter: 1289 loss: 7.74723901e-07
Iter: 1290 loss: 7.75098101e-07
Iter: 1291 loss: 7.7461857e-07
Iter: 1292 loss: 7.74357204e-07
Iter: 1293 loss: 7.74137391e-07
Iter: 1294 loss: 7.74050534e-07
Iter: 1295 loss: 7.73624265e-07
Iter: 1296 loss: 7.75616741e-07
Iter: 1297 loss: 7.73522402e-07
Iter: 1298 loss: 7.73170143e-07
Iter: 1299 loss: 7.7287649e-07
Iter: 1300 loss: 7.72765134e-07
Iter: 1301 loss: 7.7232994e-07
Iter: 1302 loss: 7.74595605e-07
Iter: 1303 loss: 7.72277701e-07
Iter: 1304 loss: 7.71878547e-07
Iter: 1305 loss: 7.72153385e-07
Iter: 1306 loss: 7.71604391e-07
Iter: 1307 loss: 7.71212854e-07
Iter: 1308 loss: 7.75399144e-07
Iter: 1309 loss: 7.71199439e-07
Iter: 1310 loss: 7.70854399e-07
Iter: 1311 loss: 7.72276621e-07
Iter: 1312 loss: 7.70781412e-07
Iter: 1313 loss: 7.70523e-07
Iter: 1314 loss: 7.70198e-07
Iter: 1315 loss: 7.70145448e-07
Iter: 1316 loss: 7.69696896e-07
Iter: 1317 loss: 7.719417e-07
Iter: 1318 loss: 7.69625331e-07
Iter: 1319 loss: 7.69504254e-07
Iter: 1320 loss: 7.69445592e-07
Iter: 1321 loss: 7.69269377e-07
Iter: 1322 loss: 7.69249482e-07
Iter: 1323 loss: 7.69124085e-07
Iter: 1324 loss: 7.68806331e-07
Iter: 1325 loss: 7.68556561e-07
Iter: 1326 loss: 7.68469476e-07
Iter: 1327 loss: 7.68081293e-07
Iter: 1328 loss: 7.70091049e-07
Iter: 1329 loss: 7.68062137e-07
Iter: 1330 loss: 7.67679751e-07
Iter: 1331 loss: 7.67561119e-07
Iter: 1332 loss: 7.67317943e-07
Iter: 1333 loss: 7.66911114e-07
Iter: 1334 loss: 7.67655592e-07
Iter: 1335 loss: 7.66719268e-07
Iter: 1336 loss: 7.66237633e-07
Iter: 1337 loss: 7.66798735e-07
Iter: 1338 loss: 7.6601458e-07
Iter: 1339 loss: 7.65547838e-07
Iter: 1340 loss: 7.7041625e-07
Iter: 1341 loss: 7.65558298e-07
Iter: 1342 loss: 7.652352e-07
Iter: 1343 loss: 7.66514518e-07
Iter: 1344 loss: 7.65190066e-07
Iter: 1345 loss: 7.64794436e-07
Iter: 1346 loss: 7.64650395e-07
Iter: 1347 loss: 7.64473953e-07
Iter: 1348 loss: 7.64077868e-07
Iter: 1349 loss: 7.64883396e-07
Iter: 1350 loss: 7.63905064e-07
Iter: 1351 loss: 7.63614764e-07
Iter: 1352 loss: 7.63601236e-07
Iter: 1353 loss: 7.63307753e-07
Iter: 1354 loss: 7.64082074e-07
Iter: 1355 loss: 7.63222715e-07
Iter: 1356 loss: 7.62931734e-07
Iter: 1357 loss: 7.63273306e-07
Iter: 1358 loss: 7.62758589e-07
Iter: 1359 loss: 7.62452373e-07
Iter: 1360 loss: 7.62638479e-07
Iter: 1361 loss: 7.6223364e-07
Iter: 1362 loss: 7.61877288e-07
Iter: 1363 loss: 7.63362721e-07
Iter: 1364 loss: 7.61771503e-07
Iter: 1365 loss: 7.61480351e-07
Iter: 1366 loss: 7.61537422e-07
Iter: 1367 loss: 7.61271963e-07
Iter: 1368 loss: 7.6091095e-07
Iter: 1369 loss: 7.60832677e-07
Iter: 1370 loss: 7.60631337e-07
Iter: 1371 loss: 7.60176e-07
Iter: 1372 loss: 7.646629e-07
Iter: 1373 loss: 7.60157661e-07
Iter: 1374 loss: 7.59819613e-07
Iter: 1375 loss: 7.6108671e-07
Iter: 1376 loss: 7.59756801e-07
Iter: 1377 loss: 7.59343948e-07
Iter: 1378 loss: 7.59731563e-07
Iter: 1379 loss: 7.59078944e-07
Iter: 1380 loss: 7.58724127e-07
Iter: 1381 loss: 7.59375325e-07
Iter: 1382 loss: 7.58568376e-07
Iter: 1383 loss: 7.58182694e-07
Iter: 1384 loss: 7.59366571e-07
Iter: 1385 loss: 7.58039505e-07
Iter: 1386 loss: 7.57771318e-07
Iter: 1387 loss: 7.57739599e-07
Iter: 1388 loss: 7.57583678e-07
Iter: 1389 loss: 7.57407861e-07
Iter: 1390 loss: 7.57403598e-07
Iter: 1391 loss: 7.57078169e-07
Iter: 1392 loss: 7.57636485e-07
Iter: 1393 loss: 7.56913551e-07
Iter: 1394 loss: 7.5666e-07
Iter: 1395 loss: 7.57663e-07
Iter: 1396 loss: 7.56586587e-07
Iter: 1397 loss: 7.56329314e-07
Iter: 1398 loss: 7.56490067e-07
Iter: 1399 loss: 7.56189138e-07
Iter: 1400 loss: 7.55890426e-07
Iter: 1401 loss: 7.55851659e-07
Iter: 1402 loss: 7.55627298e-07
Iter: 1403 loss: 7.5517795e-07
Iter: 1404 loss: 7.55842564e-07
Iter: 1405 loss: 7.5499571e-07
Iter: 1406 loss: 7.5454534e-07
Iter: 1407 loss: 7.59060583e-07
Iter: 1408 loss: 7.54537496e-07
Iter: 1409 loss: 7.54073767e-07
Iter: 1410 loss: 7.55092742e-07
Iter: 1411 loss: 7.53931317e-07
Iter: 1412 loss: 7.53593326e-07
Iter: 1413 loss: 7.54100427e-07
Iter: 1414 loss: 7.53415407e-07
Iter: 1415 loss: 7.53019549e-07
Iter: 1416 loss: 7.53243683e-07
Iter: 1417 loss: 7.52759377e-07
Iter: 1418 loss: 7.52847768e-07
Iter: 1419 loss: 7.52552751e-07
Iter: 1420 loss: 7.52406834e-07
Iter: 1421 loss: 7.52099766e-07
Iter: 1422 loss: 7.58581734e-07
Iter: 1423 loss: 7.52111532e-07
Iter: 1424 loss: 7.51749553e-07
Iter: 1425 loss: 7.53025404e-07
Iter: 1426 loss: 7.5170442e-07
Iter: 1427 loss: 7.51430548e-07
Iter: 1428 loss: 7.52009214e-07
Iter: 1429 loss: 7.51299524e-07
Iter: 1430 loss: 7.51083178e-07
Iter: 1431 loss: 7.52026438e-07
Iter: 1432 loss: 7.50988761e-07
Iter: 1433 loss: 7.5077287e-07
Iter: 1434 loss: 7.50553113e-07
Iter: 1435 loss: 7.50505706e-07
Iter: 1436 loss: 7.50117806e-07
Iter: 1437 loss: 7.5036246e-07
Iter: 1438 loss: 7.49847914e-07
Iter: 1439 loss: 7.49395e-07
Iter: 1440 loss: 7.5184812e-07
Iter: 1441 loss: 7.49338369e-07
Iter: 1442 loss: 7.48993784e-07
Iter: 1443 loss: 7.5333412e-07
Iter: 1444 loss: 7.4898054e-07
Iter: 1445 loss: 7.4871889e-07
Iter: 1446 loss: 7.48542448e-07
Iter: 1447 loss: 7.48423929e-07
Iter: 1448 loss: 7.48028867e-07
Iter: 1449 loss: 7.48631521e-07
Iter: 1450 loss: 7.47807348e-07
Iter: 1451 loss: 7.47601632e-07
Iter: 1452 loss: 7.47557408e-07
Iter: 1453 loss: 7.4733714e-07
Iter: 1454 loss: 7.47505965e-07
Iter: 1455 loss: 7.47201568e-07
Iter: 1456 loss: 7.46952196e-07
Iter: 1457 loss: 7.46894784e-07
Iter: 1458 loss: 7.46737214e-07
Iter: 1459 loss: 7.46346757e-07
Iter: 1460 loss: 7.48012553e-07
Iter: 1461 loss: 7.46266096e-07
Iter: 1462 loss: 7.46055662e-07
Iter: 1463 loss: 7.4650427e-07
Iter: 1464 loss: 7.45996829e-07
Iter: 1465 loss: 7.45674356e-07
Iter: 1466 loss: 7.45751322e-07
Iter: 1467 loss: 7.45491775e-07
Iter: 1468 loss: 7.45135367e-07
Iter: 1469 loss: 7.4569914e-07
Iter: 1470 loss: 7.44967e-07
Iter: 1471 loss: 7.44581712e-07
Iter: 1472 loss: 7.44616955e-07
Iter: 1473 loss: 7.44245085e-07
Iter: 1474 loss: 7.44028739e-07
Iter: 1475 loss: 7.43996964e-07
Iter: 1476 loss: 7.43746227e-07
Iter: 1477 loss: 7.43726446e-07
Iter: 1478 loss: 7.43525447e-07
Iter: 1479 loss: 7.43184387e-07
Iter: 1480 loss: 7.4342654e-07
Iter: 1481 loss: 7.4300641e-07
Iter: 1482 loss: 7.42694851e-07
Iter: 1483 loss: 7.46396154e-07
Iter: 1484 loss: 7.42684506e-07
Iter: 1485 loss: 7.42382781e-07
Iter: 1486 loss: 7.44649583e-07
Iter: 1487 loss: 7.42375619e-07
Iter: 1488 loss: 7.42216855e-07
Iter: 1489 loss: 7.42041493e-07
Iter: 1490 loss: 7.42044051e-07
Iter: 1491 loss: 7.41707595e-07
Iter: 1492 loss: 7.42958832e-07
Iter: 1493 loss: 7.41661552e-07
Iter: 1494 loss: 7.41463055e-07
Iter: 1495 loss: 7.41784106e-07
Iter: 1496 loss: 7.41371139e-07
Iter: 1497 loss: 7.41134841e-07
Iter: 1498 loss: 7.41548433e-07
Iter: 1499 loss: 7.41032409e-07
Iter: 1500 loss: 7.40777637e-07
Iter: 1501 loss: 7.41107442e-07
Iter: 1502 loss: 7.40614e-07
Iter: 1503 loss: 7.40279802e-07
Iter: 1504 loss: 7.40037137e-07
Iter: 1505 loss: 7.3991896e-07
Iter: 1506 loss: 7.39565678e-07
Iter: 1507 loss: 7.4394103e-07
Iter: 1508 loss: 7.39565792e-07
Iter: 1509 loss: 7.39239681e-07
Iter: 1510 loss: 7.40911673e-07
Iter: 1511 loss: 7.39217285e-07
Iter: 1512 loss: 7.38880203e-07
Iter: 1513 loss: 7.38901576e-07
Iter: 1514 loss: 7.38657945e-07
Iter: 1515 loss: 7.38382937e-07
Iter: 1516 loss: 7.39143957e-07
Iter: 1517 loss: 7.38263168e-07
Iter: 1518 loss: 7.38056883e-07
Iter: 1519 loss: 7.38049494e-07
Iter: 1520 loss: 7.37940809e-07
Iter: 1521 loss: 7.37701441e-07
Iter: 1522 loss: 7.37724804e-07
Iter: 1523 loss: 7.37477421e-07
Iter: 1524 loss: 7.38475364e-07
Iter: 1525 loss: 7.37427399e-07
Iter: 1526 loss: 7.37161201e-07
Iter: 1527 loss: 7.3723993e-07
Iter: 1528 loss: 7.3703e-07
Iter: 1529 loss: 7.3672004e-07
Iter: 1530 loss: 7.37511641e-07
Iter: 1531 loss: 7.36645688e-07
Iter: 1532 loss: 7.36310255e-07
Iter: 1533 loss: 7.3677927e-07
Iter: 1534 loss: 7.3617332e-07
Iter: 1535 loss: 7.3580054e-07
Iter: 1536 loss: 7.35729031e-07
Iter: 1537 loss: 7.35489721e-07
Iter: 1538 loss: 7.35069534e-07
Iter: 1539 loss: 7.37711503e-07
Iter: 1540 loss: 7.35001663e-07
Iter: 1541 loss: 7.34697551e-07
Iter: 1542 loss: 7.36073e-07
Iter: 1543 loss: 7.34643265e-07
Iter: 1544 loss: 7.3430283e-07
Iter: 1545 loss: 7.35614151e-07
Iter: 1546 loss: 7.3422882e-07
Iter: 1547 loss: 7.33993033e-07
Iter: 1548 loss: 7.33783509e-07
Iter: 1549 loss: 7.33761567e-07
Iter: 1550 loss: 7.3371865e-07
Iter: 1551 loss: 7.33582681e-07
Iter: 1552 loss: 7.33452282e-07
Iter: 1553 loss: 7.33204502e-07
Iter: 1554 loss: 7.33205127e-07
Iter: 1555 loss: 7.32978719e-07
Iter: 1556 loss: 7.33825459e-07
Iter: 1557 loss: 7.32923581e-07
Iter: 1558 loss: 7.32644537e-07
Iter: 1559 loss: 7.3258e-07
Iter: 1560 loss: 7.32417732e-07
Iter: 1561 loss: 7.32149658e-07
Iter: 1562 loss: 7.32711214e-07
Iter: 1563 loss: 7.32024091e-07
Iter: 1564 loss: 7.31678e-07
Iter: 1565 loss: 7.3289965e-07
Iter: 1566 loss: 7.31558316e-07
Iter: 1567 loss: 7.31215039e-07
Iter: 1568 loss: 7.31711339e-07
Iter: 1569 loss: 7.3107833e-07
Iter: 1570 loss: 7.30737099e-07
Iter: 1571 loss: 7.31210321e-07
Iter: 1572 loss: 7.30576062e-07
Iter: 1573 loss: 7.30212e-07
Iter: 1574 loss: 7.31278817e-07
Iter: 1575 loss: 7.30114152e-07
Iter: 1576 loss: 7.29773546e-07
Iter: 1577 loss: 7.33143963e-07
Iter: 1578 loss: 7.29779515e-07
Iter: 1579 loss: 7.29522299e-07
Iter: 1580 loss: 7.29815611e-07
Iter: 1581 loss: 7.29361545e-07
Iter: 1582 loss: 7.29105523e-07
Iter: 1583 loss: 7.29990063e-07
Iter: 1584 loss: 7.29047e-07
Iter: 1585 loss: 7.28776399e-07
Iter: 1586 loss: 7.31081286e-07
Iter: 1587 loss: 7.2876577e-07
Iter: 1588 loss: 7.28627924e-07
Iter: 1589 loss: 7.28353598e-07
Iter: 1590 loss: 7.32325134e-07
Iter: 1591 loss: 7.28333589e-07
Iter: 1592 loss: 7.27959105e-07
Iter: 1593 loss: 7.31671662e-07
Iter: 1594 loss: 7.27949669e-07
Iter: 1595 loss: 7.27767201e-07
Iter: 1596 loss: 7.27560632e-07
Iter: 1597 loss: 7.27510269e-07
Iter: 1598 loss: 7.27150791e-07
Iter: 1599 loss: 7.27643908e-07
Iter: 1600 loss: 7.27003851e-07
Iter: 1601 loss: 7.26588496e-07
Iter: 1602 loss: 7.28987345e-07
Iter: 1603 loss: 7.26546034e-07
Iter: 1604 loss: 7.26260282e-07
Iter: 1605 loss: 7.26083044e-07
Iter: 1606 loss: 7.25971745e-07
Iter: 1607 loss: 7.25712539e-07
Iter: 1608 loss: 7.28849216e-07
Iter: 1609 loss: 7.25686334e-07
Iter: 1610 loss: 7.25462314e-07
Iter: 1611 loss: 7.2560141e-07
Iter: 1612 loss: 7.25283087e-07
Iter: 1613 loss: 7.25002224e-07
Iter: 1614 loss: 7.28921066e-07
Iter: 1615 loss: 7.25003702e-07
Iter: 1616 loss: 7.24808785e-07
Iter: 1617 loss: 7.24779397e-07
Iter: 1618 loss: 7.24659685e-07
Iter: 1619 loss: 7.24518713e-07
Iter: 1620 loss: 7.24519737e-07
Iter: 1621 loss: 7.24311349e-07
Iter: 1622 loss: 7.23996436e-07
Iter: 1623 loss: 7.23988137e-07
Iter: 1624 loss: 7.23763037e-07
Iter: 1625 loss: 7.24356084e-07
Iter: 1626 loss: 7.23663447e-07
Iter: 1627 loss: 7.23403161e-07
Iter: 1628 loss: 7.24823053e-07
Iter: 1629 loss: 7.23356152e-07
Iter: 1630 loss: 7.2316891e-07
Iter: 1631 loss: 7.22898278e-07
Iter: 1632 loss: 7.22876166e-07
Iter: 1633 loss: 7.22578818e-07
Iter: 1634 loss: 7.24091194e-07
Iter: 1635 loss: 7.22543234e-07
Iter: 1636 loss: 7.22254413e-07
Iter: 1637 loss: 7.23675839e-07
Iter: 1638 loss: 7.2220007e-07
Iter: 1639 loss: 7.21973834e-07
Iter: 1640 loss: 7.22106165e-07
Iter: 1641 loss: 7.21834567e-07
Iter: 1642 loss: 7.21579681e-07
Iter: 1643 loss: 7.21420292e-07
Iter: 1644 loss: 7.21372203e-07
Iter: 1645 loss: 7.21042738e-07
Iter: 1646 loss: 7.2101767e-07
Iter: 1647 loss: 7.20814342e-07
Iter: 1648 loss: 7.21379479e-07
Iter: 1649 loss: 7.20759544e-07
Iter: 1650 loss: 7.20543312e-07
Iter: 1651 loss: 7.20975663e-07
Iter: 1652 loss: 7.20472769e-07
Iter: 1653 loss: 7.20235334e-07
Iter: 1654 loss: 7.2266181e-07
Iter: 1655 loss: 7.20230446e-07
Iter: 1656 loss: 7.20109597e-07
Iter: 1657 loss: 7.19903539e-07
Iter: 1658 loss: 7.23358255e-07
Iter: 1659 loss: 7.19887453e-07
Iter: 1660 loss: 7.19633704e-07
Iter: 1661 loss: 7.21467813e-07
Iter: 1662 loss: 7.19610568e-07
Iter: 1663 loss: 7.1933357e-07
Iter: 1664 loss: 7.19634158e-07
Iter: 1665 loss: 7.19210902e-07
Iter: 1666 loss: 7.18928845e-07
Iter: 1667 loss: 7.1847694e-07
Iter: 1668 loss: 7.18463468e-07
Iter: 1669 loss: 7.18306467e-07
Iter: 1670 loss: 7.18254228e-07
Iter: 1671 loss: 7.18052092e-07
Iter: 1672 loss: 7.18142928e-07
Iter: 1673 loss: 7.17933062e-07
Iter: 1674 loss: 7.17700914e-07
Iter: 1675 loss: 7.17509e-07
Iter: 1676 loss: 7.17433409e-07
Iter: 1677 loss: 7.17178068e-07
Iter: 1678 loss: 7.18164415e-07
Iter: 1679 loss: 7.17076773e-07
Iter: 1680 loss: 7.16840646e-07
Iter: 1681 loss: 7.17888042e-07
Iter: 1682 loss: 7.16768682e-07
Iter: 1683 loss: 7.16470311e-07
Iter: 1684 loss: 7.17486614e-07
Iter: 1685 loss: 7.16425461e-07
Iter: 1686 loss: 7.16334625e-07
Iter: 1687 loss: 7.16313e-07
Iter: 1688 loss: 7.16209684e-07
Iter: 1689 loss: 7.16048589e-07
Iter: 1690 loss: 7.19098637e-07
Iter: 1691 loss: 7.16024942e-07
Iter: 1692 loss: 7.15797796e-07
Iter: 1693 loss: 7.15794272e-07
Iter: 1694 loss: 7.15611748e-07
Iter: 1695 loss: 7.15441729e-07
Iter: 1696 loss: 7.15448209e-07
Iter: 1697 loss: 7.15247211e-07
Iter: 1698 loss: 7.14897283e-07
Iter: 1699 loss: 7.22377308e-07
Iter: 1700 loss: 7.14924738e-07
Iter: 1701 loss: 7.14676617e-07
Iter: 1702 loss: 7.16622367e-07
Iter: 1703 loss: 7.1465422e-07
Iter: 1704 loss: 7.14485395e-07
Iter: 1705 loss: 7.14614885e-07
Iter: 1706 loss: 7.14355053e-07
Iter: 1707 loss: 7.14100963e-07
Iter: 1708 loss: 7.15956048e-07
Iter: 1709 loss: 7.14094881e-07
Iter: 1710 loss: 7.13903091e-07
Iter: 1711 loss: 7.13644e-07
Iter: 1712 loss: 7.13647751e-07
Iter: 1713 loss: 7.13397e-07
Iter: 1714 loss: 7.13423674e-07
Iter: 1715 loss: 7.13260647e-07
Iter: 1716 loss: 7.13180611e-07
Iter: 1717 loss: 7.13102e-07
Iter: 1718 loss: 7.12891506e-07
Iter: 1719 loss: 7.12899464e-07
Iter: 1720 loss: 7.12744622e-07
Iter: 1721 loss: 7.12879739e-07
Iter: 1722 loss: 7.12644464e-07
Iter: 1723 loss: 7.12485075e-07
Iter: 1724 loss: 7.12335918e-07
Iter: 1725 loss: 7.12310168e-07
Iter: 1726 loss: 7.12044141e-07
Iter: 1727 loss: 7.12549252e-07
Iter: 1728 loss: 7.11900043e-07
Iter: 1729 loss: 7.11721214e-07
Iter: 1730 loss: 7.11722e-07
Iter: 1731 loss: 7.11604287e-07
Iter: 1732 loss: 7.11361508e-07
Iter: 1733 loss: 7.15488113e-07
Iter: 1734 loss: 7.11362645e-07
Iter: 1735 loss: 7.11058192e-07
Iter: 1736 loss: 7.1277492e-07
Iter: 1737 loss: 7.11006351e-07
Iter: 1738 loss: 7.10802965e-07
Iter: 1739 loss: 7.10497716e-07
Iter: 1740 loss: 7.10481515e-07
Iter: 1741 loss: 7.10217932e-07
Iter: 1742 loss: 7.10195309e-07
Iter: 1743 loss: 7.10018412e-07
Iter: 1744 loss: 7.10585368e-07
Iter: 1745 loss: 7.09959068e-07
Iter: 1746 loss: 7.09755227e-07
Iter: 1747 loss: 7.09844528e-07
Iter: 1748 loss: 7.09646542e-07
Iter: 1749 loss: 7.09398421e-07
Iter: 1750 loss: 7.11743041e-07
Iter: 1751 loss: 7.09396204e-07
Iter: 1752 loss: 7.09247956e-07
Iter: 1753 loss: 7.10945869e-07
Iter: 1754 loss: 7.09253698e-07
Iter: 1755 loss: 7.091719e-07
Iter: 1756 loss: 7.08976586e-07
Iter: 1757 loss: 7.11638108e-07
Iter: 1758 loss: 7.08968e-07
Iter: 1759 loss: 7.08681114e-07
Iter: 1760 loss: 7.09034111e-07
Iter: 1761 loss: 7.08594e-07
Iter: 1762 loss: 7.0837325e-07
Iter: 1763 loss: 7.1124856e-07
Iter: 1764 loss: 7.08375296e-07
Iter: 1765 loss: 7.08215282e-07
Iter: 1766 loss: 7.08547532e-07
Iter: 1767 loss: 7.08140362e-07
Iter: 1768 loss: 7.07964148e-07
Iter: 1769 loss: 7.07728702e-07
Iter: 1770 loss: 7.07713525e-07
Iter: 1771 loss: 7.07396111e-07
Iter: 1772 loss: 7.0842259e-07
Iter: 1773 loss: 7.07325398e-07
Iter: 1774 loss: 7.06944547e-07
Iter: 1775 loss: 7.07546405e-07
Iter: 1776 loss: 7.06748949e-07
Iter: 1777 loss: 7.06523394e-07
Iter: 1778 loss: 7.06506626e-07
Iter: 1779 loss: 7.06301876e-07
Iter: 1780 loss: 7.06189098e-07
Iter: 1781 loss: 7.06108381e-07
Iter: 1782 loss: 7.05917046e-07
Iter: 1783 loss: 7.05900391e-07
Iter: 1784 loss: 7.05773118e-07
Iter: 1785 loss: 7.0671058e-07
Iter: 1786 loss: 7.05770503e-07
Iter: 1787 loss: 7.05672903e-07
Iter: 1788 loss: 7.0574049e-07
Iter: 1789 loss: 7.05608272e-07
Iter: 1790 loss: 7.05459911e-07
Iter: 1791 loss: 7.05279831e-07
Iter: 1792 loss: 7.05277102e-07
Iter: 1793 loss: 7.05067407e-07
Iter: 1794 loss: 7.06363892e-07
Iter: 1795 loss: 7.05042794e-07
Iter: 1796 loss: 7.04821446e-07
Iter: 1797 loss: 7.05339744e-07
Iter: 1798 loss: 7.04731519e-07
Iter: 1799 loss: 7.04549166e-07
Iter: 1800 loss: 7.05183083e-07
Iter: 1801 loss: 7.04475895e-07
Iter: 1802 loss: 7.04319177e-07
Iter: 1803 loss: 7.03994147e-07
Iter: 1804 loss: 7.09636936e-07
Iter: 1805 loss: 7.04016657e-07
Iter: 1806 loss: 7.03716466e-07
Iter: 1807 loss: 7.0729692e-07
Iter: 1808 loss: 7.03718513e-07
Iter: 1809 loss: 7.03471073e-07
Iter: 1810 loss: 7.03666728e-07
Iter: 1811 loss: 7.0332834e-07
Iter: 1812 loss: 7.03047874e-07
Iter: 1813 loss: 7.06335072e-07
Iter: 1814 loss: 7.03005469e-07
Iter: 1815 loss: 7.02845682e-07
Iter: 1816 loss: 7.02770535e-07
Iter: 1817 loss: 7.02699481e-07
Iter: 1818 loss: 7.02413558e-07
Iter: 1819 loss: 7.06120034e-07
Iter: 1820 loss: 7.02417537e-07
Iter: 1821 loss: 7.02261218e-07
Iter: 1822 loss: 7.0288246e-07
Iter: 1823 loss: 7.0219096e-07
Iter: 1824 loss: 7.02069485e-07
Iter: 1825 loss: 7.01950398e-07
Iter: 1826 loss: 7.01912313e-07
Iter: 1827 loss: 7.01697445e-07
Iter: 1828 loss: 7.01534191e-07
Iter: 1829 loss: 7.01463136e-07
Iter: 1830 loss: 7.01344788e-07
Iter: 1831 loss: 7.01276917e-07
Iter: 1832 loss: 7.01184263e-07
Iter: 1833 loss: 7.01105e-07
Iter: 1834 loss: 7.01061595e-07
Iter: 1835 loss: 7.00798239e-07
Iter: 1836 loss: 7.00424607e-07
Iter: 1837 loss: 7.00420514e-07
Iter: 1838 loss: 7.00020678e-07
Iter: 1839 loss: 7.01883891e-07
Iter: 1840 loss: 6.99977647e-07
Iter: 1841 loss: 6.99645511e-07
Iter: 1842 loss: 7.01233148e-07
Iter: 1843 loss: 6.99587076e-07
Iter: 1844 loss: 6.9932e-07
Iter: 1845 loss: 7.01506224e-07
Iter: 1846 loss: 6.99289785e-07
Iter: 1847 loss: 6.99106408e-07
Iter: 1848 loss: 6.99581847e-07
Iter: 1849 loss: 6.99022053e-07
Iter: 1850 loss: 6.98861868e-07
Iter: 1851 loss: 7.00605483e-07
Iter: 1852 loss: 6.98839585e-07
Iter: 1853 loss: 6.98678036e-07
Iter: 1854 loss: 6.99185762e-07
Iter: 1855 loss: 6.98656606e-07
Iter: 1856 loss: 6.98543772e-07
Iter: 1857 loss: 6.98676217e-07
Iter: 1858 loss: 6.98461463e-07
Iter: 1859 loss: 6.98307133e-07
Iter: 1860 loss: 6.98131544e-07
Iter: 1861 loss: 6.98083e-07
Iter: 1862 loss: 6.97950441e-07
Iter: 1863 loss: 7.00651583e-07
Iter: 1864 loss: 6.97959e-07
Iter: 1865 loss: 6.97809242e-07
Iter: 1866 loss: 6.9819373e-07
Iter: 1867 loss: 6.97778546e-07
Iter: 1868 loss: 6.97605117e-07
Iter: 1869 loss: 6.97322889e-07
Iter: 1870 loss: 6.97336077e-07
Iter: 1871 loss: 6.97056635e-07
Iter: 1872 loss: 6.99001475e-07
Iter: 1873 loss: 6.97005476e-07
Iter: 1874 loss: 6.96794132e-07
Iter: 1875 loss: 6.9652134e-07
Iter: 1876 loss: 6.96480583e-07
Iter: 1877 loss: 6.96252073e-07
Iter: 1878 loss: 6.96251334e-07
Iter: 1879 loss: 6.96028224e-07
Iter: 1880 loss: 6.96580173e-07
Iter: 1881 loss: 6.95928861e-07
Iter: 1882 loss: 6.95702511e-07
Iter: 1883 loss: 6.96861321e-07
Iter: 1884 loss: 6.95681422e-07
Iter: 1885 loss: 6.95564609e-07
Iter: 1886 loss: 6.95563813e-07
Iter: 1887 loss: 6.95466156e-07
Iter: 1888 loss: 6.9532723e-07
Iter: 1889 loss: 6.9533894e-07
Iter: 1890 loss: 6.95159315e-07
Iter: 1891 loss: 6.95472181e-07
Iter: 1892 loss: 6.95063306e-07
Iter: 1893 loss: 6.94905452e-07
Iter: 1894 loss: 6.9494638e-07
Iter: 1895 loss: 6.94836501e-07
Iter: 1896 loss: 6.94583719e-07
Iter: 1897 loss: 6.96338645e-07
Iter: 1898 loss: 6.94580933e-07
Iter: 1899 loss: 6.94406481e-07
Iter: 1900 loss: 6.946035e-07
Iter: 1901 loss: 6.94350433e-07
Iter: 1902 loss: 6.94151822e-07
Iter: 1903 loss: 6.93960828e-07
Iter: 1904 loss: 6.93926381e-07
Iter: 1905 loss: 6.93662969e-07
Iter: 1906 loss: 6.95733775e-07
Iter: 1907 loss: 6.93607262e-07
Iter: 1908 loss: 6.93424454e-07
Iter: 1909 loss: 6.93211064e-07
Iter: 1910 loss: 6.9318321e-07
Iter: 1911 loss: 6.92946628e-07
Iter: 1912 loss: 6.92938102e-07
Iter: 1913 loss: 6.92753e-07
Iter: 1914 loss: 6.93017455e-07
Iter: 1915 loss: 6.92666049e-07
Iter: 1916 loss: 6.92504955e-07
Iter: 1917 loss: 6.94562232e-07
Iter: 1918 loss: 6.92492392e-07
Iter: 1919 loss: 6.92346475e-07
Iter: 1920 loss: 6.92507911e-07
Iter: 1921 loss: 6.92244555e-07
Iter: 1922 loss: 6.92147694e-07
Iter: 1923 loss: 6.92540368e-07
Iter: 1924 loss: 6.921e-07
Iter: 1925 loss: 6.91950277e-07
Iter: 1926 loss: 6.91676746e-07
Iter: 1927 loss: 6.96657e-07
Iter: 1928 loss: 6.91686296e-07
Iter: 1929 loss: 6.91556124e-07
Iter: 1930 loss: 6.91499508e-07
Iter: 1931 loss: 6.91381956e-07
Iter: 1932 loss: 6.91504965e-07
Iter: 1933 loss: 6.9130806e-07
Iter: 1934 loss: 6.91124512e-07
Iter: 1935 loss: 6.90787033e-07
Iter: 1936 loss: 6.90767422e-07
Iter: 1937 loss: 6.9050509e-07
Iter: 1938 loss: 6.93517563e-07
Iter: 1939 loss: 6.9048167e-07
Iter: 1940 loss: 6.90238608e-07
Iter: 1941 loss: 6.90188926e-07
Iter: 1942 loss: 6.90027036e-07
Iter: 1943 loss: 6.89701835e-07
Iter: 1944 loss: 6.92096251e-07
Iter: 1945 loss: 6.89679382e-07
Iter: 1946 loss: 6.89503452e-07
Iter: 1947 loss: 6.9106062e-07
Iter: 1948 loss: 6.89472301e-07
Iter: 1949 loss: 6.89302453e-07
Iter: 1950 loss: 6.89928527e-07
Iter: 1951 loss: 6.8925084e-07
Iter: 1952 loss: 6.89098329e-07
Iter: 1953 loss: 6.90407603e-07
Iter: 1954 loss: 6.89056947e-07
Iter: 1955 loss: 6.88953492e-07
Iter: 1956 loss: 6.88929504e-07
Iter: 1957 loss: 6.88859814e-07
Iter: 1958 loss: 6.88703e-07
Iter: 1959 loss: 6.88891191e-07
Iter: 1960 loss: 6.88613227e-07
Iter: 1961 loss: 6.88434852e-07
Iter: 1962 loss: 6.88521254e-07
Iter: 1963 loss: 6.88307068e-07
Iter: 1964 loss: 6.88116586e-07
Iter: 1965 loss: 6.88127102e-07
Iter: 1966 loss: 6.87993065e-07
Iter: 1967 loss: 6.8772556e-07
Iter: 1968 loss: 6.92676451e-07
Iter: 1969 loss: 6.87725219e-07
Iter: 1970 loss: 6.87434863e-07
Iter: 1971 loss: 6.89823e-07
Iter: 1972 loss: 6.87441343e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.4
+ date
Mon Oct 26 15:33:11 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.4/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.4_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi -1 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.4_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01907a6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01907529d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0190752c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01a403fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01a402d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01a402dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01906adea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01906adf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0190660378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0190660bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01906ad510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01905e0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0190584840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0190584a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f019054fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f019052e048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0190532400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01904ca9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f019048c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01904a0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f019049ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f019045d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0190424510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0190424268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f019043b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0190387950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01903e19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f019036f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f019036f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f019031db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f019030d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01902f5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01902f52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01902b9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f01902b7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f019025a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000202253883
Iter: 2 loss: 0.000592574652
Iter: 3 loss: 0.000177256661
Iter: 4 loss: 0.000157458693
Iter: 5 loss: 0.000179376497
Iter: 6 loss: 0.000146663195
Iter: 7 loss: 0.000140182616
Iter: 8 loss: 0.000126740662
Iter: 9 loss: 0.000365095912
Iter: 10 loss: 0.000126451676
Iter: 11 loss: 0.000114376
Iter: 12 loss: 0.000107439155
Iter: 13 loss: 0.000102280348
Iter: 14 loss: 9.34724376e-05
Iter: 15 loss: 0.00010119642
Iter: 16 loss: 8.83372559e-05
Iter: 17 loss: 8.38146952e-05
Iter: 18 loss: 9.30826791e-05
Iter: 19 loss: 8.198875e-05
Iter: 20 loss: 7.60234179e-05
Iter: 21 loss: 8.68451e-05
Iter: 22 loss: 7.3442905e-05
Iter: 23 loss: 6.90486631e-05
Iter: 24 loss: 6.56442426e-05
Iter: 25 loss: 6.42536688e-05
Iter: 26 loss: 5.99295527e-05
Iter: 27 loss: 6.49013818e-05
Iter: 28 loss: 5.7612393e-05
Iter: 29 loss: 5.40901456e-05
Iter: 30 loss: 5.34663268e-05
Iter: 31 loss: 5.10789396e-05
Iter: 32 loss: 4.79347691e-05
Iter: 33 loss: 7.23373523e-05
Iter: 34 loss: 4.77042777e-05
Iter: 35 loss: 4.52195527e-05
Iter: 36 loss: 5.14032072e-05
Iter: 37 loss: 4.43472818e-05
Iter: 38 loss: 4.19007229e-05
Iter: 39 loss: 4.7921294e-05
Iter: 40 loss: 4.1033265e-05
Iter: 41 loss: 4.0998144e-05
Iter: 42 loss: 4.02105215e-05
Iter: 43 loss: 3.95053175e-05
Iter: 44 loss: 3.9821236e-05
Iter: 45 loss: 3.90266869e-05
Iter: 46 loss: 3.83107108e-05
Iter: 47 loss: 4.0930976e-05
Iter: 48 loss: 3.81327191e-05
Iter: 49 loss: 3.73228795e-05
Iter: 50 loss: 3.68463807e-05
Iter: 51 loss: 3.65055894e-05
Iter: 52 loss: 3.53734795e-05
Iter: 53 loss: 4.68998151e-05
Iter: 54 loss: 3.53404939e-05
Iter: 55 loss: 3.45151202e-05
Iter: 56 loss: 3.47499736e-05
Iter: 57 loss: 3.39207327e-05
Iter: 58 loss: 3.30906478e-05
Iter: 59 loss: 3.50996816e-05
Iter: 60 loss: 3.27944726e-05
Iter: 61 loss: 3.19660503e-05
Iter: 62 loss: 3.1906573e-05
Iter: 63 loss: 3.12856937e-05
Iter: 64 loss: 3.01472101e-05
Iter: 65 loss: 3.34382494e-05
Iter: 66 loss: 2.97916322e-05
Iter: 67 loss: 2.86773502e-05
Iter: 68 loss: 3.28693677e-05
Iter: 69 loss: 2.84112e-05
Iter: 70 loss: 2.72423804e-05
Iter: 71 loss: 2.97897113e-05
Iter: 72 loss: 2.67899777e-05
Iter: 73 loss: 2.6499978e-05
Iter: 74 loss: 2.64282608e-05
Iter: 75 loss: 2.60916531e-05
Iter: 76 loss: 2.7160806e-05
Iter: 77 loss: 2.5996249e-05
Iter: 78 loss: 2.57088832e-05
Iter: 79 loss: 2.54410243e-05
Iter: 80 loss: 2.53732724e-05
Iter: 81 loss: 2.47753815e-05
Iter: 82 loss: 2.62067897e-05
Iter: 83 loss: 2.45592746e-05
Iter: 84 loss: 2.40317822e-05
Iter: 85 loss: 2.69543743e-05
Iter: 86 loss: 2.3956447e-05
Iter: 87 loss: 2.34097643e-05
Iter: 88 loss: 2.39456567e-05
Iter: 89 loss: 2.31006397e-05
Iter: 90 loss: 2.25934236e-05
Iter: 91 loss: 2.38011071e-05
Iter: 92 loss: 2.24094219e-05
Iter: 93 loss: 2.20121856e-05
Iter: 94 loss: 2.35175485e-05
Iter: 95 loss: 2.19178073e-05
Iter: 96 loss: 2.16058652e-05
Iter: 97 loss: 2.19790127e-05
Iter: 98 loss: 2.14411211e-05
Iter: 99 loss: 2.11001643e-05
Iter: 100 loss: 2.19042704e-05
Iter: 101 loss: 2.09753307e-05
Iter: 102 loss: 2.06228906e-05
Iter: 103 loss: 2.24408741e-05
Iter: 104 loss: 2.05659453e-05
Iter: 105 loss: 2.02508163e-05
Iter: 106 loss: 2.06721415e-05
Iter: 107 loss: 2.00909017e-05
Iter: 108 loss: 1.97722457e-05
Iter: 109 loss: 1.97608952e-05
Iter: 110 loss: 1.95906068e-05
Iter: 111 loss: 1.93458218e-05
Iter: 112 loss: 1.93385167e-05
Iter: 113 loss: 1.90801657e-05
Iter: 114 loss: 2.11340466e-05
Iter: 115 loss: 1.90629053e-05
Iter: 116 loss: 1.88637368e-05
Iter: 117 loss: 1.88720442e-05
Iter: 118 loss: 1.87063924e-05
Iter: 119 loss: 1.84275705e-05
Iter: 120 loss: 2.10720318e-05
Iter: 121 loss: 1.84166711e-05
Iter: 122 loss: 1.82915392e-05
Iter: 123 loss: 1.8424993e-05
Iter: 124 loss: 1.82229196e-05
Iter: 125 loss: 1.80582683e-05
Iter: 126 loss: 1.79061026e-05
Iter: 127 loss: 1.78660648e-05
Iter: 128 loss: 1.74886991e-05
Iter: 129 loss: 1.81050109e-05
Iter: 130 loss: 1.73158278e-05
Iter: 131 loss: 1.70432395e-05
Iter: 132 loss: 1.88552694e-05
Iter: 133 loss: 1.70154381e-05
Iter: 134 loss: 1.68162769e-05
Iter: 135 loss: 1.68450824e-05
Iter: 136 loss: 1.66651225e-05
Iter: 137 loss: 1.63763762e-05
Iter: 138 loss: 1.83602097e-05
Iter: 139 loss: 1.63484274e-05
Iter: 140 loss: 1.6357666e-05
Iter: 141 loss: 1.62675824e-05
Iter: 142 loss: 1.61961725e-05
Iter: 143 loss: 1.60890959e-05
Iter: 144 loss: 1.60866e-05
Iter: 145 loss: 1.59613501e-05
Iter: 146 loss: 1.61606877e-05
Iter: 147 loss: 1.59020237e-05
Iter: 148 loss: 1.57485974e-05
Iter: 149 loss: 1.63159912e-05
Iter: 150 loss: 1.57112245e-05
Iter: 151 loss: 1.56360657e-05
Iter: 152 loss: 1.68097795e-05
Iter: 153 loss: 1.56362e-05
Iter: 154 loss: 1.55717535e-05
Iter: 155 loss: 1.55179478e-05
Iter: 156 loss: 1.55001162e-05
Iter: 157 loss: 1.5379479e-05
Iter: 158 loss: 1.53784458e-05
Iter: 159 loss: 1.52828798e-05
Iter: 160 loss: 1.51018776e-05
Iter: 161 loss: 1.57997329e-05
Iter: 162 loss: 1.50598789e-05
Iter: 163 loss: 1.49133466e-05
Iter: 164 loss: 1.50259166e-05
Iter: 165 loss: 1.4824318e-05
Iter: 166 loss: 1.46396233e-05
Iter: 167 loss: 1.51095101e-05
Iter: 168 loss: 1.45765516e-05
Iter: 169 loss: 1.44200858e-05
Iter: 170 loss: 1.5259775e-05
Iter: 171 loss: 1.43964489e-05
Iter: 172 loss: 1.42769541e-05
Iter: 173 loss: 1.50968681e-05
Iter: 174 loss: 1.42655917e-05
Iter: 175 loss: 1.41443616e-05
Iter: 176 loss: 1.52244138e-05
Iter: 177 loss: 1.41384535e-05
Iter: 178 loss: 1.40820939e-05
Iter: 179 loss: 1.3978517e-05
Iter: 180 loss: 1.63997338e-05
Iter: 181 loss: 1.39783897e-05
Iter: 182 loss: 1.38763453e-05
Iter: 183 loss: 1.514418e-05
Iter: 184 loss: 1.3875595e-05
Iter: 185 loss: 1.38110236e-05
Iter: 186 loss: 1.38802434e-05
Iter: 187 loss: 1.37757088e-05
Iter: 188 loss: 1.36942235e-05
Iter: 189 loss: 1.432314e-05
Iter: 190 loss: 1.36882645e-05
Iter: 191 loss: 1.36297185e-05
Iter: 192 loss: 1.3552095e-05
Iter: 193 loss: 1.35472137e-05
Iter: 194 loss: 1.34429247e-05
Iter: 195 loss: 1.38622436e-05
Iter: 196 loss: 1.34194215e-05
Iter: 197 loss: 1.33384874e-05
Iter: 198 loss: 1.35241417e-05
Iter: 199 loss: 1.33077465e-05
Iter: 200 loss: 1.32062551e-05
Iter: 201 loss: 1.32507084e-05
Iter: 202 loss: 1.31370925e-05
Iter: 203 loss: 1.3005324e-05
Iter: 204 loss: 1.38348259e-05
Iter: 205 loss: 1.29902892e-05
Iter: 206 loss: 1.28887586e-05
Iter: 207 loss: 1.30807457e-05
Iter: 208 loss: 1.28458069e-05
Iter: 209 loss: 1.28155316e-05
Iter: 210 loss: 1.27890044e-05
Iter: 211 loss: 1.27553121e-05
Iter: 212 loss: 1.26825607e-05
Iter: 213 loss: 1.3795142e-05
Iter: 214 loss: 1.26797668e-05
Iter: 215 loss: 1.26091873e-05
Iter: 216 loss: 1.29808541e-05
Iter: 217 loss: 1.25982042e-05
Iter: 218 loss: 1.25253673e-05
Iter: 219 loss: 1.26661052e-05
Iter: 220 loss: 1.24948401e-05
Iter: 221 loss: 1.24410253e-05
Iter: 222 loss: 1.3035452e-05
Iter: 223 loss: 1.2439541e-05
Iter: 224 loss: 1.23942664e-05
Iter: 225 loss: 1.23799327e-05
Iter: 226 loss: 1.23532072e-05
Iter: 227 loss: 1.2294613e-05
Iter: 228 loss: 1.2331715e-05
Iter: 229 loss: 1.22571582e-05
Iter: 230 loss: 1.21895955e-05
Iter: 231 loss: 1.23346408e-05
Iter: 232 loss: 1.21634721e-05
Iter: 233 loss: 1.20990862e-05
Iter: 234 loss: 1.22725141e-05
Iter: 235 loss: 1.2077574e-05
Iter: 236 loss: 1.20075711e-05
Iter: 237 loss: 1.21530429e-05
Iter: 238 loss: 1.19792394e-05
Iter: 239 loss: 1.19006454e-05
Iter: 240 loss: 1.22586152e-05
Iter: 241 loss: 1.18851112e-05
Iter: 242 loss: 1.18593462e-05
Iter: 243 loss: 1.18496946e-05
Iter: 244 loss: 1.1816388e-05
Iter: 245 loss: 1.17474028e-05
Iter: 246 loss: 1.29555156e-05
Iter: 247 loss: 1.17466025e-05
Iter: 248 loss: 1.16809806e-05
Iter: 249 loss: 1.18496127e-05
Iter: 250 loss: 1.16581905e-05
Iter: 251 loss: 1.16033425e-05
Iter: 252 loss: 1.21725807e-05
Iter: 253 loss: 1.16018291e-05
Iter: 254 loss: 1.15696748e-05
Iter: 255 loss: 1.16218598e-05
Iter: 256 loss: 1.15547227e-05
Iter: 257 loss: 1.15073708e-05
Iter: 258 loss: 1.15700022e-05
Iter: 259 loss: 1.14833501e-05
Iter: 260 loss: 1.14413615e-05
Iter: 261 loss: 1.14880195e-05
Iter: 262 loss: 1.14188215e-05
Iter: 263 loss: 1.1376058e-05
Iter: 264 loss: 1.14171589e-05
Iter: 265 loss: 1.13515935e-05
Iter: 266 loss: 1.12928719e-05
Iter: 267 loss: 1.13479837e-05
Iter: 268 loss: 1.12597936e-05
Iter: 269 loss: 1.11906302e-05
Iter: 270 loss: 1.15071298e-05
Iter: 271 loss: 1.11774098e-05
Iter: 272 loss: 1.1126067e-05
Iter: 273 loss: 1.15101893e-05
Iter: 274 loss: 1.11221252e-05
Iter: 275 loss: 1.11001555e-05
Iter: 276 loss: 1.10985902e-05
Iter: 277 loss: 1.10737947e-05
Iter: 278 loss: 1.10499404e-05
Iter: 279 loss: 1.10445371e-05
Iter: 280 loss: 1.1012552e-05
Iter: 281 loss: 1.10094934e-05
Iter: 282 loss: 1.09856119e-05
Iter: 283 loss: 1.0945183e-05
Iter: 284 loss: 1.13033111e-05
Iter: 285 loss: 1.09433768e-05
Iter: 286 loss: 1.09058456e-05
Iter: 287 loss: 1.08894155e-05
Iter: 288 loss: 1.08705935e-05
Iter: 289 loss: 1.0816113e-05
Iter: 290 loss: 1.14156601e-05
Iter: 291 loss: 1.08149288e-05
Iter: 292 loss: 1.07888427e-05
Iter: 293 loss: 1.07837423e-05
Iter: 294 loss: 1.07661217e-05
Iter: 295 loss: 1.07315909e-05
Iter: 296 loss: 1.07449923e-05
Iter: 297 loss: 1.0707683e-05
Iter: 298 loss: 1.06557618e-05
Iter: 299 loss: 1.07193137e-05
Iter: 300 loss: 1.06290681e-05
Iter: 301 loss: 1.05830295e-05
Iter: 302 loss: 1.08750473e-05
Iter: 303 loss: 1.05778436e-05
Iter: 304 loss: 1.05413019e-05
Iter: 305 loss: 1.06398084e-05
Iter: 306 loss: 1.05294803e-05
Iter: 307 loss: 1.04992832e-05
Iter: 308 loss: 1.09787889e-05
Iter: 309 loss: 1.0499085e-05
Iter: 310 loss: 1.04677538e-05
Iter: 311 loss: 1.05347417e-05
Iter: 312 loss: 1.04550736e-05
Iter: 313 loss: 1.04331857e-05
Iter: 314 loss: 1.04031478e-05
Iter: 315 loss: 1.04018218e-05
Iter: 316 loss: 1.03702623e-05
Iter: 317 loss: 1.0674803e-05
Iter: 318 loss: 1.03687553e-05
Iter: 319 loss: 1.03397297e-05
Iter: 320 loss: 1.03482507e-05
Iter: 321 loss: 1.03191633e-05
Iter: 322 loss: 1.02837857e-05
Iter: 323 loss: 1.0619412e-05
Iter: 324 loss: 1.02823742e-05
Iter: 325 loss: 1.02571885e-05
Iter: 326 loss: 1.02459253e-05
Iter: 327 loss: 1.02328277e-05
Iter: 328 loss: 1.02002323e-05
Iter: 329 loss: 1.023405e-05
Iter: 330 loss: 1.01817914e-05
Iter: 331 loss: 1.01373626e-05
Iter: 332 loss: 1.02410077e-05
Iter: 333 loss: 1.01213063e-05
Iter: 334 loss: 1.00877787e-05
Iter: 335 loss: 1.019302e-05
Iter: 336 loss: 1.00781817e-05
Iter: 337 loss: 1.00449051e-05
Iter: 338 loss: 1.01430833e-05
Iter: 339 loss: 1.0034415e-05
Iter: 340 loss: 1.00099132e-05
Iter: 341 loss: 1.00099132e-05
Iter: 342 loss: 9.98920405e-06
Iter: 343 loss: 1.01663027e-05
Iter: 344 loss: 9.98810174e-06
Iter: 345 loss: 9.97609e-06
Iter: 346 loss: 9.94741913e-06
Iter: 347 loss: 1.02714821e-05
Iter: 348 loss: 9.94454149e-06
Iter: 349 loss: 9.91285742e-06
Iter: 350 loss: 1.00458601e-05
Iter: 351 loss: 9.90585067e-06
Iter: 352 loss: 9.87e-06
Iter: 353 loss: 1.00158795e-05
Iter: 354 loss: 9.86198575e-06
Iter: 355 loss: 9.8415012e-06
Iter: 356 loss: 1.00616e-05
Iter: 357 loss: 9.84057533e-06
Iter: 358 loss: 9.8249875e-06
Iter: 359 loss: 9.82957772e-06
Iter: 360 loss: 9.81355879e-06
Iter: 361 loss: 9.79481501e-06
Iter: 362 loss: 9.76902811e-06
Iter: 363 loss: 9.76777847e-06
Iter: 364 loss: 9.72831549e-06
Iter: 365 loss: 9.9415156e-06
Iter: 366 loss: 9.72242742e-06
Iter: 367 loss: 9.69274151e-06
Iter: 368 loss: 9.7064667e-06
Iter: 369 loss: 9.67254891e-06
Iter: 370 loss: 9.63570164e-06
Iter: 371 loss: 9.83427708e-06
Iter: 372 loss: 9.63024831e-06
Iter: 373 loss: 9.60818215e-06
Iter: 374 loss: 9.90624903e-06
Iter: 375 loss: 9.60807211e-06
Iter: 376 loss: 9.59394765e-06
Iter: 377 loss: 9.59377394e-06
Iter: 378 loss: 9.58418332e-06
Iter: 379 loss: 9.56117765e-06
Iter: 380 loss: 9.82851634e-06
Iter: 381 loss: 9.55915129e-06
Iter: 382 loss: 9.5355781e-06
Iter: 383 loss: 9.59885438e-06
Iter: 384 loss: 9.52785376e-06
Iter: 385 loss: 9.50458343e-06
Iter: 386 loss: 9.6949916e-06
Iter: 387 loss: 9.50312278e-06
Iter: 388 loss: 9.48678826e-06
Iter: 389 loss: 9.53446e-06
Iter: 390 loss: 9.4820125e-06
Iter: 391 loss: 9.46227556e-06
Iter: 392 loss: 9.49082823e-06
Iter: 393 loss: 9.45313514e-06
Iter: 394 loss: 9.43311898e-06
Iter: 395 loss: 9.41193684e-06
Iter: 396 loss: 9.40820064e-06
Iter: 397 loss: 9.38088215e-06
Iter: 398 loss: 9.58914916e-06
Iter: 399 loss: 9.37878303e-06
Iter: 400 loss: 9.35819116e-06
Iter: 401 loss: 9.35364369e-06
Iter: 402 loss: 9.34004856e-06
Iter: 403 loss: 9.31170507e-06
Iter: 404 loss: 9.47433637e-06
Iter: 405 loss: 9.30797069e-06
Iter: 406 loss: 9.2868886e-06
Iter: 407 loss: 9.42999759e-06
Iter: 408 loss: 9.28496502e-06
Iter: 409 loss: 9.27097881e-06
Iter: 410 loss: 9.27078872e-06
Iter: 411 loss: 9.25951827e-06
Iter: 412 loss: 9.2386872e-06
Iter: 413 loss: 9.71100417e-06
Iter: 414 loss: 9.23867901e-06
Iter: 415 loss: 9.22020354e-06
Iter: 416 loss: 9.23422795e-06
Iter: 417 loss: 9.20953062e-06
Iter: 418 loss: 9.18665683e-06
Iter: 419 loss: 9.37150799e-06
Iter: 420 loss: 9.18506e-06
Iter: 421 loss: 9.16471708e-06
Iter: 422 loss: 9.1939246e-06
Iter: 423 loss: 9.1550155e-06
Iter: 424 loss: 9.1320162e-06
Iter: 425 loss: 9.27938345e-06
Iter: 426 loss: 9.12962423e-06
Iter: 427 loss: 9.11369898e-06
Iter: 428 loss: 9.0962867e-06
Iter: 429 loss: 9.0937483e-06
Iter: 430 loss: 9.0706144e-06
Iter: 431 loss: 9.17149828e-06
Iter: 432 loss: 9.06569949e-06
Iter: 433 loss: 9.04547414e-06
Iter: 434 loss: 9.08429865e-06
Iter: 435 loss: 9.03715409e-06
Iter: 436 loss: 9.01809653e-06
Iter: 437 loss: 9.06386e-06
Iter: 438 loss: 9.01153817e-06
Iter: 439 loss: 8.99267616e-06
Iter: 440 loss: 9.09657319e-06
Iter: 441 loss: 8.98963663e-06
Iter: 442 loss: 8.97887094e-06
Iter: 443 loss: 8.97826249e-06
Iter: 444 loss: 8.96819165e-06
Iter: 445 loss: 8.95492121e-06
Iter: 446 loss: 8.95378162e-06
Iter: 447 loss: 8.93991091e-06
Iter: 448 loss: 8.92646312e-06
Iter: 449 loss: 8.92359276e-06
Iter: 450 loss: 8.90055162e-06
Iter: 451 loss: 9.12458654e-06
Iter: 452 loss: 8.89989678e-06
Iter: 453 loss: 8.88140312e-06
Iter: 454 loss: 8.93242122e-06
Iter: 455 loss: 8.87524e-06
Iter: 456 loss: 8.86268754e-06
Iter: 457 loss: 9.00484247e-06
Iter: 458 loss: 8.86248745e-06
Iter: 459 loss: 8.85252302e-06
Iter: 460 loss: 8.83594475e-06
Iter: 461 loss: 8.83600842e-06
Iter: 462 loss: 8.81518463e-06
Iter: 463 loss: 8.84632755e-06
Iter: 464 loss: 8.80530479e-06
Iter: 465 loss: 8.78316769e-06
Iter: 466 loss: 8.88596514e-06
Iter: 467 loss: 8.77896127e-06
Iter: 468 loss: 8.7604767e-06
Iter: 469 loss: 8.78824176e-06
Iter: 470 loss: 8.75176102e-06
Iter: 471 loss: 8.73476529e-06
Iter: 472 loss: 8.89366493e-06
Iter: 473 loss: 8.73401314e-06
Iter: 474 loss: 8.72561395e-06
Iter: 475 loss: 8.72537203e-06
Iter: 476 loss: 8.71692373e-06
Iter: 477 loss: 8.71030352e-06
Iter: 478 loss: 8.70782424e-06
Iter: 479 loss: 8.69641553e-06
Iter: 480 loss: 8.68356256e-06
Iter: 481 loss: 8.68159623e-06
Iter: 482 loss: 8.6650125e-06
Iter: 483 loss: 8.83855228e-06
Iter: 484 loss: 8.66423852e-06
Iter: 485 loss: 8.65089896e-06
Iter: 486 loss: 8.70052645e-06
Iter: 487 loss: 8.6477412e-06
Iter: 488 loss: 8.6387372e-06
Iter: 489 loss: 8.69516316e-06
Iter: 490 loss: 8.63737841e-06
Iter: 491 loss: 8.62747402e-06
Iter: 492 loss: 8.61277749e-06
Iter: 493 loss: 8.61269928e-06
Iter: 494 loss: 8.59404827e-06
Iter: 495 loss: 8.62349225e-06
Iter: 496 loss: 8.58504427e-06
Iter: 497 loss: 8.56907354e-06
Iter: 498 loss: 8.65267339e-06
Iter: 499 loss: 8.56676706e-06
Iter: 500 loss: 8.55154e-06
Iter: 501 loss: 8.56316819e-06
Iter: 502 loss: 8.54268e-06
Iter: 503 loss: 8.52712765e-06
Iter: 504 loss: 8.65518086e-06
Iter: 505 loss: 8.52639732e-06
Iter: 506 loss: 8.51545e-06
Iter: 507 loss: 8.64717094e-06
Iter: 508 loss: 8.51534242e-06
Iter: 509 loss: 8.503961e-06
Iter: 510 loss: 8.50923061e-06
Iter: 511 loss: 8.49624575e-06
Iter: 512 loss: 8.48506079e-06
Iter: 513 loss: 8.4734138e-06
Iter: 514 loss: 8.47175579e-06
Iter: 515 loss: 8.45664363e-06
Iter: 516 loss: 8.58104067e-06
Iter: 517 loss: 8.45611885e-06
Iter: 518 loss: 8.44473288e-06
Iter: 519 loss: 8.50304423e-06
Iter: 520 loss: 8.44284841e-06
Iter: 521 loss: 8.43453927e-06
Iter: 522 loss: 8.45587e-06
Iter: 523 loss: 8.4320036e-06
Iter: 524 loss: 8.42038389e-06
Iter: 525 loss: 8.41495512e-06
Iter: 526 loss: 8.40950543e-06
Iter: 527 loss: 8.39676886e-06
Iter: 528 loss: 8.42158624e-06
Iter: 529 loss: 8.39176937e-06
Iter: 530 loss: 8.38111191e-06
Iter: 531 loss: 8.39794302e-06
Iter: 532 loss: 8.37627e-06
Iter: 533 loss: 8.36241452e-06
Iter: 534 loss: 8.38737651e-06
Iter: 535 loss: 8.35628089e-06
Iter: 536 loss: 8.34309321e-06
Iter: 537 loss: 8.42868758e-06
Iter: 538 loss: 8.34174898e-06
Iter: 539 loss: 8.33208651e-06
Iter: 540 loss: 8.44281e-06
Iter: 541 loss: 8.33181548e-06
Iter: 542 loss: 8.32184378e-06
Iter: 543 loss: 8.34400635e-06
Iter: 544 loss: 8.31801663e-06
Iter: 545 loss: 8.3103605e-06
Iter: 546 loss: 8.29895725e-06
Iter: 547 loss: 8.29885357e-06
Iter: 548 loss: 8.28596e-06
Iter: 549 loss: 8.35610263e-06
Iter: 550 loss: 8.28406883e-06
Iter: 551 loss: 8.27284e-06
Iter: 552 loss: 8.33043669e-06
Iter: 553 loss: 8.27092e-06
Iter: 554 loss: 8.26122687e-06
Iter: 555 loss: 8.27634813e-06
Iter: 556 loss: 8.25713141e-06
Iter: 557 loss: 8.24389645e-06
Iter: 558 loss: 8.27838812e-06
Iter: 559 loss: 8.23961727e-06
Iter: 560 loss: 8.23121809e-06
Iter: 561 loss: 8.22632e-06
Iter: 562 loss: 8.22272887e-06
Iter: 563 loss: 8.20966488e-06
Iter: 564 loss: 8.22176571e-06
Iter: 565 loss: 8.20194873e-06
Iter: 566 loss: 8.18496119e-06
Iter: 567 loss: 8.26406722e-06
Iter: 568 loss: 8.18186345e-06
Iter: 569 loss: 8.16943793e-06
Iter: 570 loss: 8.2346e-06
Iter: 571 loss: 8.167739e-06
Iter: 572 loss: 8.15965177e-06
Iter: 573 loss: 8.27152326e-06
Iter: 574 loss: 8.15971907e-06
Iter: 575 loss: 8.15199746e-06
Iter: 576 loss: 8.17847285e-06
Iter: 577 loss: 8.14978375e-06
Iter: 578 loss: 8.14402119e-06
Iter: 579 loss: 8.13518272e-06
Iter: 580 loss: 8.13503357e-06
Iter: 581 loss: 8.12476e-06
Iter: 582 loss: 8.15891e-06
Iter: 583 loss: 8.12181315e-06
Iter: 584 loss: 8.11135396e-06
Iter: 585 loss: 8.16893953e-06
Iter: 586 loss: 8.10971e-06
Iter: 587 loss: 8.10038364e-06
Iter: 588 loss: 8.11396239e-06
Iter: 589 loss: 8.09588892e-06
Iter: 590 loss: 8.0848331e-06
Iter: 591 loss: 8.14972555e-06
Iter: 592 loss: 8.08307777e-06
Iter: 593 loss: 8.07642209e-06
Iter: 594 loss: 8.06402841e-06
Iter: 595 loss: 8.3569912e-06
Iter: 596 loss: 8.06410662e-06
Iter: 597 loss: 8.05001764e-06
Iter: 598 loss: 8.10512e-06
Iter: 599 loss: 8.0468526e-06
Iter: 600 loss: 8.03546e-06
Iter: 601 loss: 8.08333698e-06
Iter: 602 loss: 8.03320563e-06
Iter: 603 loss: 8.02356226e-06
Iter: 604 loss: 8.05183117e-06
Iter: 605 loss: 8.02071372e-06
Iter: 606 loss: 8.01240822e-06
Iter: 607 loss: 8.12871713e-06
Iter: 608 loss: 8.01233455e-06
Iter: 609 loss: 8.00470571e-06
Iter: 610 loss: 8.03276453e-06
Iter: 611 loss: 8.0029331e-06
Iter: 612 loss: 7.99675763e-06
Iter: 613 loss: 7.9865149e-06
Iter: 614 loss: 7.98646e-06
Iter: 615 loss: 7.97483153e-06
Iter: 616 loss: 7.99914869e-06
Iter: 617 loss: 7.97053e-06
Iter: 618 loss: 7.95870073e-06
Iter: 619 loss: 8.0574755e-06
Iter: 620 loss: 7.95790857e-06
Iter: 621 loss: 7.94914195e-06
Iter: 622 loss: 7.96516269e-06
Iter: 623 loss: 7.94541847e-06
Iter: 624 loss: 7.93695e-06
Iter: 625 loss: 8.00243106e-06
Iter: 626 loss: 7.93613799e-06
Iter: 627 loss: 7.93060462e-06
Iter: 628 loss: 7.91931598e-06
Iter: 629 loss: 8.13303177e-06
Iter: 630 loss: 7.91924867e-06
Iter: 631 loss: 7.9081974e-06
Iter: 632 loss: 7.9790525e-06
Iter: 633 loss: 7.90689046e-06
Iter: 634 loss: 7.89812839e-06
Iter: 635 loss: 7.91077218e-06
Iter: 636 loss: 7.89400838e-06
Iter: 637 loss: 7.88246416e-06
Iter: 638 loss: 7.90327613e-06
Iter: 639 loss: 7.87769113e-06
Iter: 640 loss: 7.86905093e-06
Iter: 641 loss: 7.86896908e-06
Iter: 642 loss: 7.86142209e-06
Iter: 643 loss: 7.9098736e-06
Iter: 644 loss: 7.86074634e-06
Iter: 645 loss: 7.85561861e-06
Iter: 646 loss: 7.84833901e-06
Iter: 647 loss: 7.84813528e-06
Iter: 648 loss: 7.83981159e-06
Iter: 649 loss: 7.84822e-06
Iter: 650 loss: 7.83523046e-06
Iter: 651 loss: 7.82561256e-06
Iter: 652 loss: 7.91960156e-06
Iter: 653 loss: 7.8251569e-06
Iter: 654 loss: 7.81759627e-06
Iter: 655 loss: 7.82723691e-06
Iter: 656 loss: 7.81388917e-06
Iter: 657 loss: 7.80549908e-06
Iter: 658 loss: 7.85964e-06
Iter: 659 loss: 7.80471237e-06
Iter: 660 loss: 7.79810216e-06
Iter: 661 loss: 7.78563663e-06
Iter: 662 loss: 8.06428307e-06
Iter: 663 loss: 7.78564e-06
Iter: 664 loss: 7.77531932e-06
Iter: 665 loss: 7.81722156e-06
Iter: 666 loss: 7.77288096e-06
Iter: 667 loss: 7.76219622e-06
Iter: 668 loss: 7.7748391e-06
Iter: 669 loss: 7.75653371e-06
Iter: 670 loss: 7.74378805e-06
Iter: 671 loss: 7.82355528e-06
Iter: 672 loss: 7.74232831e-06
Iter: 673 loss: 7.73587453e-06
Iter: 674 loss: 7.73583633e-06
Iter: 675 loss: 7.73068132e-06
Iter: 676 loss: 7.76093839e-06
Iter: 677 loss: 7.72986459e-06
Iter: 678 loss: 7.72543e-06
Iter: 679 loss: 7.71812302e-06
Iter: 680 loss: 7.7179111e-06
Iter: 681 loss: 7.70955376e-06
Iter: 682 loss: 7.71405212e-06
Iter: 683 loss: 7.7035e-06
Iter: 684 loss: 7.69508551e-06
Iter: 685 loss: 7.81717517e-06
Iter: 686 loss: 7.69493818e-06
Iter: 687 loss: 7.68882092e-06
Iter: 688 loss: 7.69813232e-06
Iter: 689 loss: 7.68557766e-06
Iter: 690 loss: 7.67865458e-06
Iter: 691 loss: 7.7197e-06
Iter: 692 loss: 7.67774873e-06
Iter: 693 loss: 7.67167694e-06
Iter: 694 loss: 7.66375706e-06
Iter: 695 loss: 7.66337052e-06
Iter: 696 loss: 7.6546512e-06
Iter: 697 loss: 7.6654469e-06
Iter: 698 loss: 7.65043114e-06
Iter: 699 loss: 7.63867865e-06
Iter: 700 loss: 7.66637459e-06
Iter: 701 loss: 7.63493699e-06
Iter: 702 loss: 7.62361287e-06
Iter: 703 loss: 7.67885558e-06
Iter: 704 loss: 7.62202399e-06
Iter: 705 loss: 7.61604588e-06
Iter: 706 loss: 7.61606771e-06
Iter: 707 loss: 7.61099272e-06
Iter: 708 loss: 7.64727884e-06
Iter: 709 loss: 7.61061892e-06
Iter: 710 loss: 7.60659896e-06
Iter: 711 loss: 7.60008106e-06
Iter: 712 loss: 7.59994737e-06
Iter: 713 loss: 7.59201248e-06
Iter: 714 loss: 7.59799696e-06
Iter: 715 loss: 7.58721444e-06
Iter: 716 loss: 7.58039187e-06
Iter: 717 loss: 7.66223457e-06
Iter: 718 loss: 7.58027545e-06
Iter: 719 loss: 7.57404177e-06
Iter: 720 loss: 7.57928456e-06
Iter: 721 loss: 7.5703415e-06
Iter: 722 loss: 7.56267173e-06
Iter: 723 loss: 7.60445437e-06
Iter: 724 loss: 7.56178724e-06
Iter: 725 loss: 7.55501651e-06
Iter: 726 loss: 7.5529324e-06
Iter: 727 loss: 7.54898838e-06
Iter: 728 loss: 7.54238272e-06
Iter: 729 loss: 7.5424141e-06
Iter: 730 loss: 7.53672521e-06
Iter: 731 loss: 7.52684446e-06
Iter: 732 loss: 7.56636427e-06
Iter: 733 loss: 7.52461074e-06
Iter: 734 loss: 7.51629341e-06
Iter: 735 loss: 7.53882796e-06
Iter: 736 loss: 7.51349853e-06
Iter: 737 loss: 7.50601066e-06
Iter: 738 loss: 7.56990403e-06
Iter: 739 loss: 7.50573281e-06
Iter: 740 loss: 7.49964784e-06
Iter: 741 loss: 7.57051657e-06
Iter: 742 loss: 7.49943092e-06
Iter: 743 loss: 7.49495666e-06
Iter: 744 loss: 7.48887214e-06
Iter: 745 loss: 7.48858429e-06
Iter: 746 loss: 7.4808836e-06
Iter: 747 loss: 7.4908603e-06
Iter: 748 loss: 7.47752438e-06
Iter: 749 loss: 7.471152e-06
Iter: 750 loss: 7.50853633e-06
Iter: 751 loss: 7.47013e-06
Iter: 752 loss: 7.46306114e-06
Iter: 753 loss: 7.47415925e-06
Iter: 754 loss: 7.45996613e-06
Iter: 755 loss: 7.45270108e-06
Iter: 756 loss: 7.50235495e-06
Iter: 757 loss: 7.452e-06
Iter: 758 loss: 7.4466443e-06
Iter: 759 loss: 7.44850377e-06
Iter: 760 loss: 7.44260069e-06
Iter: 761 loss: 7.43724058e-06
Iter: 762 loss: 7.43323199e-06
Iter: 763 loss: 7.43129431e-06
Iter: 764 loss: 7.42258362e-06
Iter: 765 loss: 7.46412798e-06
Iter: 766 loss: 7.42101838e-06
Iter: 767 loss: 7.41370877e-06
Iter: 768 loss: 7.42875136e-06
Iter: 769 loss: 7.41072881e-06
Iter: 770 loss: 7.40355517e-06
Iter: 771 loss: 7.44887438e-06
Iter: 772 loss: 7.40265705e-06
Iter: 773 loss: 7.39778716e-06
Iter: 774 loss: 7.39788857e-06
Iter: 775 loss: 7.39424331e-06
Iter: 776 loss: 7.38968583e-06
Iter: 777 loss: 7.38942481e-06
Iter: 778 loss: 7.3835854e-06
Iter: 779 loss: 7.38469771e-06
Iter: 780 loss: 7.37904793e-06
Iter: 781 loss: 7.37133314e-06
Iter: 782 loss: 7.39705229e-06
Iter: 783 loss: 7.36962647e-06
Iter: 784 loss: 7.36067113e-06
Iter: 785 loss: 7.40114e-06
Iter: 786 loss: 7.35928461e-06
Iter: 787 loss: 7.35332605e-06
Iter: 788 loss: 7.39493544e-06
Iter: 789 loss: 7.35300273e-06
Iter: 790 loss: 7.34854166e-06
Iter: 791 loss: 7.35094636e-06
Iter: 792 loss: 7.34563355e-06
Iter: 793 loss: 7.34090418e-06
Iter: 794 loss: 7.33484194e-06
Iter: 795 loss: 7.33416118e-06
Iter: 796 loss: 7.32547323e-06
Iter: 797 loss: 7.36819402e-06
Iter: 798 loss: 7.32415629e-06
Iter: 799 loss: 7.31687305e-06
Iter: 800 loss: 7.33437628e-06
Iter: 801 loss: 7.31444561e-06
Iter: 802 loss: 7.30731153e-06
Iter: 803 loss: 7.33454817e-06
Iter: 804 loss: 7.30561806e-06
Iter: 805 loss: 7.30335069e-06
Iter: 806 loss: 7.30194461e-06
Iter: 807 loss: 7.29945714e-06
Iter: 808 loss: 7.29586236e-06
Iter: 809 loss: 7.29559952e-06
Iter: 810 loss: 7.29059684e-06
Iter: 811 loss: 7.28996747e-06
Iter: 812 loss: 7.28664145e-06
Iter: 813 loss: 7.28015e-06
Iter: 814 loss: 7.30928969e-06
Iter: 815 loss: 7.27894439e-06
Iter: 816 loss: 7.27301631e-06
Iter: 817 loss: 7.31057389e-06
Iter: 818 loss: 7.27227643e-06
Iter: 819 loss: 7.26805956e-06
Iter: 820 loss: 7.28231134e-06
Iter: 821 loss: 7.26688631e-06
Iter: 822 loss: 7.26221697e-06
Iter: 823 loss: 7.26466078e-06
Iter: 824 loss: 7.2588291e-06
Iter: 825 loss: 7.25309474e-06
Iter: 826 loss: 7.24532811e-06
Iter: 827 loss: 7.2448056e-06
Iter: 828 loss: 7.23570156e-06
Iter: 829 loss: 7.30040847e-06
Iter: 830 loss: 7.23501762e-06
Iter: 831 loss: 7.22866207e-06
Iter: 832 loss: 7.24065649e-06
Iter: 833 loss: 7.22624327e-06
Iter: 834 loss: 7.21903234e-06
Iter: 835 loss: 7.23774156e-06
Iter: 836 loss: 7.21666584e-06
Iter: 837 loss: 7.21665219e-06
Iter: 838 loss: 7.21329707e-06
Iter: 839 loss: 7.2108669e-06
Iter: 840 loss: 7.20592379e-06
Iter: 841 loss: 7.30858937e-06
Iter: 842 loss: 7.20608296e-06
Iter: 843 loss: 7.19931e-06
Iter: 844 loss: 7.19864875e-06
Iter: 845 loss: 7.19434593e-06
Iter: 846 loss: 7.18732372e-06
Iter: 847 loss: 7.22897903e-06
Iter: 848 loss: 7.18663159e-06
Iter: 849 loss: 7.18127285e-06
Iter: 850 loss: 7.21682682e-06
Iter: 851 loss: 7.18050433e-06
Iter: 852 loss: 7.17594048e-06
Iter: 853 loss: 7.18331e-06
Iter: 854 loss: 7.17396e-06
Iter: 855 loss: 7.16823797e-06
Iter: 856 loss: 7.1792947e-06
Iter: 857 loss: 7.16582326e-06
Iter: 858 loss: 7.16018621e-06
Iter: 859 loss: 7.15618e-06
Iter: 860 loss: 7.15415172e-06
Iter: 861 loss: 7.14754151e-06
Iter: 862 loss: 7.18458796e-06
Iter: 863 loss: 7.14668477e-06
Iter: 864 loss: 7.14063708e-06
Iter: 865 loss: 7.14295948e-06
Iter: 866 loss: 7.13633972e-06
Iter: 867 loss: 7.12756719e-06
Iter: 868 loss: 7.15327178e-06
Iter: 869 loss: 7.12479459e-06
Iter: 870 loss: 7.12585916e-06
Iter: 871 loss: 7.12161227e-06
Iter: 872 loss: 7.11909297e-06
Iter: 873 loss: 7.11405892e-06
Iter: 874 loss: 7.22276491e-06
Iter: 875 loss: 7.11394205e-06
Iter: 876 loss: 7.10793802e-06
Iter: 877 loss: 7.11373332e-06
Iter: 878 loss: 7.1045024e-06
Iter: 879 loss: 7.09935193e-06
Iter: 880 loss: 7.12064866e-06
Iter: 881 loss: 7.09808228e-06
Iter: 882 loss: 7.09300093e-06
Iter: 883 loss: 7.1176355e-06
Iter: 884 loss: 7.09216147e-06
Iter: 885 loss: 7.08718107e-06
Iter: 886 loss: 7.09546566e-06
Iter: 887 loss: 7.08499147e-06
Iter: 888 loss: 7.07964909e-06
Iter: 889 loss: 7.10028644e-06
Iter: 890 loss: 7.07827e-06
Iter: 891 loss: 7.07354138e-06
Iter: 892 loss: 7.07052322e-06
Iter: 893 loss: 7.06890432e-06
Iter: 894 loss: 7.0633123e-06
Iter: 895 loss: 7.07488562e-06
Iter: 896 loss: 7.06054834e-06
Iter: 897 loss: 7.05324237e-06
Iter: 898 loss: 7.05782577e-06
Iter: 899 loss: 7.04807371e-06
Iter: 900 loss: 7.03927935e-06
Iter: 901 loss: 7.09815959e-06
Iter: 902 loss: 7.0382539e-06
Iter: 903 loss: 7.03672868e-06
Iter: 904 loss: 7.03539445e-06
Iter: 905 loss: 7.03239675e-06
Iter: 906 loss: 7.02796797e-06
Iter: 907 loss: 7.02789839e-06
Iter: 908 loss: 7.02258239e-06
Iter: 909 loss: 7.02747548e-06
Iter: 910 loss: 7.01954286e-06
Iter: 911 loss: 7.01438694e-06
Iter: 912 loss: 7.02644957e-06
Iter: 913 loss: 7.01262707e-06
Iter: 914 loss: 7.00695227e-06
Iter: 915 loss: 7.04028753e-06
Iter: 916 loss: 7.00656619e-06
Iter: 917 loss: 7.00149394e-06
Iter: 918 loss: 7.01184672e-06
Iter: 919 loss: 6.999584e-06
Iter: 920 loss: 6.99493057e-06
Iter: 921 loss: 7.01720273e-06
Iter: 922 loss: 6.99403154e-06
Iter: 923 loss: 6.99038037e-06
Iter: 924 loss: 6.98688564e-06
Iter: 925 loss: 6.98622534e-06
Iter: 926 loss: 6.98061194e-06
Iter: 927 loss: 6.98288841e-06
Iter: 928 loss: 6.97668111e-06
Iter: 929 loss: 6.96788e-06
Iter: 930 loss: 6.99922975e-06
Iter: 931 loss: 6.96563939e-06
Iter: 932 loss: 6.95968811e-06
Iter: 933 loss: 7.00481451e-06
Iter: 934 loss: 6.959232e-06
Iter: 935 loss: 6.95662084e-06
Iter: 936 loss: 6.95653853e-06
Iter: 937 loss: 6.95355902e-06
Iter: 938 loss: 6.95152e-06
Iter: 939 loss: 6.95022027e-06
Iter: 940 loss: 6.94639857e-06
Iter: 941 loss: 6.94879554e-06
Iter: 942 loss: 6.94347409e-06
Iter: 943 loss: 6.93914899e-06
Iter: 944 loss: 6.94675145e-06
Iter: 945 loss: 6.9374687e-06
Iter: 946 loss: 6.93294305e-06
Iter: 947 loss: 6.97040741e-06
Iter: 948 loss: 6.93268703e-06
Iter: 949 loss: 6.92925505e-06
Iter: 950 loss: 6.93707352e-06
Iter: 951 loss: 6.92770482e-06
Iter: 952 loss: 6.92409867e-06
Iter: 953 loss: 6.94062055e-06
Iter: 954 loss: 6.92358026e-06
Iter: 955 loss: 6.92032e-06
Iter: 956 loss: 6.91624155e-06
Iter: 957 loss: 6.91568221e-06
Iter: 958 loss: 6.91005334e-06
Iter: 959 loss: 6.90860679e-06
Iter: 960 loss: 6.90534807e-06
Iter: 961 loss: 6.89786884e-06
Iter: 962 loss: 6.94906794e-06
Iter: 963 loss: 6.89723129e-06
Iter: 964 loss: 6.89234275e-06
Iter: 965 loss: 6.90564e-06
Iter: 966 loss: 6.8909826e-06
Iter: 967 loss: 6.88669434e-06
Iter: 968 loss: 6.94603386e-06
Iter: 969 loss: 6.88670661e-06
Iter: 970 loss: 6.88179898e-06
Iter: 971 loss: 6.88458931e-06
Iter: 972 loss: 6.87854572e-06
Iter: 973 loss: 6.87415877e-06
Iter: 974 loss: 6.8739464e-06
Iter: 975 loss: 6.87054171e-06
Iter: 976 loss: 6.86493149e-06
Iter: 977 loss: 6.87325974e-06
Iter: 978 loss: 6.86265184e-06
Iter: 979 loss: 6.85733539e-06
Iter: 980 loss: 6.89561557e-06
Iter: 981 loss: 6.85691612e-06
Iter: 982 loss: 6.85175883e-06
Iter: 983 loss: 6.86312069e-06
Iter: 984 loss: 6.85031e-06
Iter: 985 loss: 6.84578799e-06
Iter: 986 loss: 6.8674758e-06
Iter: 987 loss: 6.84491488e-06
Iter: 988 loss: 6.84104543e-06
Iter: 989 loss: 6.83982444e-06
Iter: 990 loss: 6.83745293e-06
Iter: 991 loss: 6.83225198e-06
Iter: 992 loss: 6.83086546e-06
Iter: 993 loss: 6.82733935e-06
Iter: 994 loss: 6.82104246e-06
Iter: 995 loss: 6.8684235e-06
Iter: 996 loss: 6.82030759e-06
Iter: 997 loss: 6.81494294e-06
Iter: 998 loss: 6.81883648e-06
Iter: 999 loss: 6.81190977e-06
Iter: 1000 loss: 6.80691301e-06
Iter: 1001 loss: 6.80704761e-06
Iter: 1002 loss: 6.80206267e-06
Iter: 1003 loss: 6.82104837e-06
Iter: 1004 loss: 6.80074572e-06
Iter: 1005 loss: 6.79793402e-06
Iter: 1006 loss: 6.79474715e-06
Iter: 1007 loss: 6.79404366e-06
Iter: 1008 loss: 6.78933e-06
Iter: 1009 loss: 6.79642608e-06
Iter: 1010 loss: 6.78717061e-06
Iter: 1011 loss: 6.78183369e-06
Iter: 1012 loss: 6.81115898e-06
Iter: 1013 loss: 6.78099786e-06
Iter: 1014 loss: 6.77556272e-06
Iter: 1015 loss: 6.79378127e-06
Iter: 1016 loss: 6.77394883e-06
Iter: 1017 loss: 6.7703e-06
Iter: 1018 loss: 6.78907236e-06
Iter: 1019 loss: 6.76969557e-06
Iter: 1020 loss: 6.76603304e-06
Iter: 1021 loss: 6.76513173e-06
Iter: 1022 loss: 6.76307036e-06
Iter: 1023 loss: 6.75800175e-06
Iter: 1024 loss: 6.75497859e-06
Iter: 1025 loss: 6.75297542e-06
Iter: 1026 loss: 6.7461433e-06
Iter: 1027 loss: 6.7801966e-06
Iter: 1028 loss: 6.74493913e-06
Iter: 1029 loss: 6.73834256e-06
Iter: 1030 loss: 6.75364572e-06
Iter: 1031 loss: 6.73577642e-06
Iter: 1032 loss: 6.73161412e-06
Iter: 1033 loss: 6.80087896e-06
Iter: 1034 loss: 6.73146224e-06
Iter: 1035 loss: 6.72723854e-06
Iter: 1036 loss: 6.76138734e-06
Iter: 1037 loss: 6.72695069e-06
Iter: 1038 loss: 6.72503256e-06
Iter: 1039 loss: 6.72122223e-06
Iter: 1040 loss: 6.80594303e-06
Iter: 1041 loss: 6.72139868e-06
Iter: 1042 loss: 6.71658836e-06
Iter: 1043 loss: 6.72313581e-06
Iter: 1044 loss: 6.71425551e-06
Iter: 1045 loss: 6.70906138e-06
Iter: 1046 loss: 6.74120656e-06
Iter: 1047 loss: 6.70824556e-06
Iter: 1048 loss: 6.70392819e-06
Iter: 1049 loss: 6.72895931e-06
Iter: 1050 loss: 6.70336522e-06
Iter: 1051 loss: 6.7004903e-06
Iter: 1052 loss: 6.70682e-06
Iter: 1053 loss: 6.69941073e-06
Iter: 1054 loss: 6.6958587e-06
Iter: 1055 loss: 6.69460769e-06
Iter: 1056 loss: 6.69243582e-06
Iter: 1057 loss: 6.68713e-06
Iter: 1058 loss: 6.6874818e-06
Iter: 1059 loss: 6.68265056e-06
Iter: 1060 loss: 6.67718086e-06
Iter: 1061 loss: 6.69716428e-06
Iter: 1062 loss: 6.67589e-06
Iter: 1063 loss: 6.6708335e-06
Iter: 1064 loss: 6.69185692e-06
Iter: 1065 loss: 6.66989808e-06
Iter: 1066 loss: 6.66585902e-06
Iter: 1067 loss: 6.68890789e-06
Iter: 1068 loss: 6.6653638e-06
Iter: 1069 loss: 6.66228607e-06
Iter: 1070 loss: 6.66220649e-06
Iter: 1071 loss: 6.66065353e-06
Iter: 1072 loss: 6.6564553e-06
Iter: 1073 loss: 6.7025112e-06
Iter: 1074 loss: 6.65622065e-06
Iter: 1075 loss: 6.65140942e-06
Iter: 1076 loss: 6.66376854e-06
Iter: 1077 loss: 6.64996787e-06
Iter: 1078 loss: 6.64541676e-06
Iter: 1079 loss: 6.67381209e-06
Iter: 1080 loss: 6.64486743e-06
Iter: 1081 loss: 6.64134677e-06
Iter: 1082 loss: 6.6610728e-06
Iter: 1083 loss: 6.64078334e-06
Iter: 1084 loss: 6.6382554e-06
Iter: 1085 loss: 6.64027e-06
Iter: 1086 loss: 6.6363873e-06
Iter: 1087 loss: 6.63195488e-06
Iter: 1088 loss: 6.63487208e-06
Iter: 1089 loss: 6.62910315e-06
Iter: 1090 loss: 6.62434513e-06
Iter: 1091 loss: 6.62891944e-06
Iter: 1092 loss: 6.62190678e-06
Iter: 1093 loss: 6.6174689e-06
Iter: 1094 loss: 6.62516959e-06
Iter: 1095 loss: 6.61549939e-06
Iter: 1096 loss: 6.61054582e-06
Iter: 1097 loss: 6.62922139e-06
Iter: 1098 loss: 6.60929572e-06
Iter: 1099 loss: 6.60500882e-06
Iter: 1100 loss: 6.6261864e-06
Iter: 1101 loss: 6.60417027e-06
Iter: 1102 loss: 6.60159594e-06
Iter: 1103 loss: 6.60125079e-06
Iter: 1104 loss: 6.59975785e-06
Iter: 1105 loss: 6.59573334e-06
Iter: 1106 loss: 6.6326138e-06
Iter: 1107 loss: 6.59529132e-06
Iter: 1108 loss: 6.59093803e-06
Iter: 1109 loss: 6.60738442e-06
Iter: 1110 loss: 6.58998033e-06
Iter: 1111 loss: 6.58612407e-06
Iter: 1112 loss: 6.60496335e-06
Iter: 1113 loss: 6.58563658e-06
Iter: 1114 loss: 6.5821323e-06
Iter: 1115 loss: 6.59838588e-06
Iter: 1116 loss: 6.58132e-06
Iter: 1117 loss: 6.57853434e-06
Iter: 1118 loss: 6.58319277e-06
Iter: 1119 loss: 6.57712644e-06
Iter: 1120 loss: 6.5736358e-06
Iter: 1121 loss: 6.5822469e-06
Iter: 1122 loss: 6.57245755e-06
Iter: 1123 loss: 6.56945667e-06
Iter: 1124 loss: 6.56823886e-06
Iter: 1125 loss: 6.56661132e-06
Iter: 1126 loss: 6.56252269e-06
Iter: 1127 loss: 6.564675e-06
Iter: 1128 loss: 6.55989834e-06
Iter: 1129 loss: 6.55440635e-06
Iter: 1130 loss: 6.57711644e-06
Iter: 1131 loss: 6.55308077e-06
Iter: 1132 loss: 6.54883661e-06
Iter: 1133 loss: 6.57923692e-06
Iter: 1134 loss: 6.54843234e-06
Iter: 1135 loss: 6.54678206e-06
Iter: 1136 loss: 6.54613541e-06
Iter: 1137 loss: 6.54476889e-06
Iter: 1138 loss: 6.54092855e-06
Iter: 1139 loss: 6.56949305e-06
Iter: 1140 loss: 6.54016731e-06
Iter: 1141 loss: 6.53584402e-06
Iter: 1142 loss: 6.5549084e-06
Iter: 1143 loss: 6.53503776e-06
Iter: 1144 loss: 6.53114284e-06
Iter: 1145 loss: 6.54523683e-06
Iter: 1146 loss: 6.53035431e-06
Iter: 1147 loss: 6.52619701e-06
Iter: 1148 loss: 6.54640098e-06
Iter: 1149 loss: 6.5254967e-06
Iter: 1150 loss: 6.52197969e-06
Iter: 1151 loss: 6.52896688e-06
Iter: 1152 loss: 6.52073959e-06
Iter: 1153 loss: 6.51673327e-06
Iter: 1154 loss: 6.52635708e-06
Iter: 1155 loss: 6.51539267e-06
Iter: 1156 loss: 6.51177288e-06
Iter: 1157 loss: 6.50916263e-06
Iter: 1158 loss: 6.50789843e-06
Iter: 1159 loss: 6.50279708e-06
Iter: 1160 loss: 6.50765514e-06
Iter: 1161 loss: 6.49962294e-06
Iter: 1162 loss: 6.4940532e-06
Iter: 1163 loss: 6.52994686e-06
Iter: 1164 loss: 6.49346975e-06
Iter: 1165 loss: 6.48968762e-06
Iter: 1166 loss: 6.50164475e-06
Iter: 1167 loss: 6.48833793e-06
Iter: 1168 loss: 6.48658579e-06
Iter: 1169 loss: 6.48605601e-06
Iter: 1170 loss: 6.48389596e-06
Iter: 1171 loss: 6.47889237e-06
Iter: 1172 loss: 6.5388208e-06
Iter: 1173 loss: 6.47872503e-06
Iter: 1174 loss: 6.47492971e-06
Iter: 1175 loss: 6.48710557e-06
Iter: 1176 loss: 6.47392471e-06
Iter: 1177 loss: 6.46960962e-06
Iter: 1178 loss: 6.47794423e-06
Iter: 1179 loss: 6.46788e-06
Iter: 1180 loss: 6.46369699e-06
Iter: 1181 loss: 6.50113316e-06
Iter: 1182 loss: 6.4636456e-06
Iter: 1183 loss: 6.46053923e-06
Iter: 1184 loss: 6.46956505e-06
Iter: 1185 loss: 6.4595929e-06
Iter: 1186 loss: 6.45650243e-06
Iter: 1187 loss: 6.46396347e-06
Iter: 1188 loss: 6.4556607e-06
Iter: 1189 loss: 6.45284626e-06
Iter: 1190 loss: 6.45174077e-06
Iter: 1191 loss: 6.45006821e-06
Iter: 1192 loss: 6.44617558e-06
Iter: 1193 loss: 6.44775082e-06
Iter: 1194 loss: 6.44351849e-06
Iter: 1195 loss: 6.43924704e-06
Iter: 1196 loss: 6.46248327e-06
Iter: 1197 loss: 6.43829208e-06
Iter: 1198 loss: 6.434223e-06
Iter: 1199 loss: 6.44046395e-06
Iter: 1200 loss: 6.43239582e-06
Iter: 1201 loss: 6.43131352e-06
Iter: 1202 loss: 6.43e-06
Iter: 1203 loss: 6.42789473e-06
Iter: 1204 loss: 6.42439863e-06
Iter: 1205 loss: 6.42435089e-06
Iter: 1206 loss: 6.42179384e-06
Iter: 1207 loss: 6.42263e-06
Iter: 1208 loss: 6.4200417e-06
Iter: 1209 loss: 6.41564839e-06
Iter: 1210 loss: 6.4260712e-06
Iter: 1211 loss: 6.41418319e-06
Iter: 1212 loss: 6.41015413e-06
Iter: 1213 loss: 6.44655756e-06
Iter: 1214 loss: 6.40999042e-06
Iter: 1215 loss: 6.40699864e-06
Iter: 1216 loss: 6.41637234e-06
Iter: 1217 loss: 6.40632652e-06
Iter: 1218 loss: 6.40385633e-06
Iter: 1219 loss: 6.41111183e-06
Iter: 1220 loss: 6.40279541e-06
Iter: 1221 loss: 6.40022063e-06
Iter: 1222 loss: 6.39877908e-06
Iter: 1223 loss: 6.39761583e-06
Iter: 1224 loss: 6.39336213e-06
Iter: 1225 loss: 6.39457676e-06
Iter: 1226 loss: 6.39049267e-06
Iter: 1227 loss: 6.38622e-06
Iter: 1228 loss: 6.41240058e-06
Iter: 1229 loss: 6.38541906e-06
Iter: 1230 loss: 6.38158963e-06
Iter: 1231 loss: 6.39117025e-06
Iter: 1232 loss: 6.3801258e-06
Iter: 1233 loss: 6.37982521e-06
Iter: 1234 loss: 6.37849371e-06
Iter: 1235 loss: 6.37714311e-06
Iter: 1236 loss: 6.3747384e-06
Iter: 1237 loss: 6.37482117e-06
Iter: 1238 loss: 6.37243511e-06
Iter: 1239 loss: 6.37033645e-06
Iter: 1240 loss: 6.36977302e-06
Iter: 1241 loss: 6.3652642e-06
Iter: 1242 loss: 6.38641632e-06
Iter: 1243 loss: 6.36424465e-06
Iter: 1244 loss: 6.36107e-06
Iter: 1245 loss: 6.39756581e-06
Iter: 1246 loss: 6.36068671e-06
Iter: 1247 loss: 6.35844935e-06
Iter: 1248 loss: 6.36504819e-06
Iter: 1249 loss: 6.35775268e-06
Iter: 1250 loss: 6.35558445e-06
Iter: 1251 loss: 6.3618927e-06
Iter: 1252 loss: 6.35492415e-06
Iter: 1253 loss: 6.35234801e-06
Iter: 1254 loss: 6.35022616e-06
Iter: 1255 loss: 6.34940352e-06
Iter: 1256 loss: 6.34510798e-06
Iter: 1257 loss: 6.34957178e-06
Iter: 1258 loss: 6.34280741e-06
Iter: 1259 loss: 6.33869422e-06
Iter: 1260 loss: 6.3531229e-06
Iter: 1261 loss: 6.33785112e-06
Iter: 1262 loss: 6.33370837e-06
Iter: 1263 loss: 6.34762182e-06
Iter: 1264 loss: 6.33273976e-06
Iter: 1265 loss: 6.3312e-06
Iter: 1266 loss: 6.33071886e-06
Iter: 1267 loss: 6.32847468e-06
Iter: 1268 loss: 6.32606134e-06
Iter: 1269 loss: 6.32591673e-06
Iter: 1270 loss: 6.32305364e-06
Iter: 1271 loss: 6.32096317e-06
Iter: 1272 loss: 6.32014644e-06
Iter: 1273 loss: 6.31558169e-06
Iter: 1274 loss: 6.34144089e-06
Iter: 1275 loss: 6.31488365e-06
Iter: 1276 loss: 6.31222611e-06
Iter: 1277 loss: 6.33752916e-06
Iter: 1278 loss: 6.31198782e-06
Iter: 1279 loss: 6.3097059e-06
Iter: 1280 loss: 6.31754028e-06
Iter: 1281 loss: 6.30906925e-06
Iter: 1282 loss: 6.30659133e-06
Iter: 1283 loss: 6.30965e-06
Iter: 1284 loss: 6.30539307e-06
Iter: 1285 loss: 6.30183786e-06
Iter: 1286 loss: 6.29824171e-06
Iter: 1287 loss: 6.29775923e-06
Iter: 1288 loss: 6.29219448e-06
Iter: 1289 loss: 6.31135936e-06
Iter: 1290 loss: 6.29097485e-06
Iter: 1291 loss: 6.28682483e-06
Iter: 1292 loss: 6.29081e-06
Iter: 1293 loss: 6.28444e-06
Iter: 1294 loss: 6.27931968e-06
Iter: 1295 loss: 6.304625e-06
Iter: 1296 loss: 6.27831287e-06
Iter: 1297 loss: 6.27584859e-06
Iter: 1298 loss: 6.27582403e-06
Iter: 1299 loss: 6.27258487e-06
Iter: 1300 loss: 6.2723766e-06
Iter: 1301 loss: 6.27021745e-06
Iter: 1302 loss: 6.26757e-06
Iter: 1303 loss: 6.26447945e-06
Iter: 1304 loss: 6.26407336e-06
Iter: 1305 loss: 6.25938219e-06
Iter: 1306 loss: 6.28151611e-06
Iter: 1307 loss: 6.2584013e-06
Iter: 1308 loss: 6.25468829e-06
Iter: 1309 loss: 6.28047565e-06
Iter: 1310 loss: 6.25435268e-06
Iter: 1311 loss: 6.25133816e-06
Iter: 1312 loss: 6.26166047e-06
Iter: 1313 loss: 6.25063331e-06
Iter: 1314 loss: 6.24770564e-06
Iter: 1315 loss: 6.25219218e-06
Iter: 1316 loss: 6.24613585e-06
Iter: 1317 loss: 6.24221866e-06
Iter: 1318 loss: 6.24183667e-06
Iter: 1319 loss: 6.23885717e-06
Iter: 1320 loss: 6.23526284e-06
Iter: 1321 loss: 6.24888162e-06
Iter: 1322 loss: 6.23427195e-06
Iter: 1323 loss: 6.23085907e-06
Iter: 1324 loss: 6.22967855e-06
Iter: 1325 loss: 6.22786138e-06
Iter: 1326 loss: 6.22299194e-06
Iter: 1327 loss: 6.2566487e-06
Iter: 1328 loss: 6.2223e-06
Iter: 1329 loss: 6.21966228e-06
Iter: 1330 loss: 6.21972777e-06
Iter: 1331 loss: 6.21692197e-06
Iter: 1332 loss: 6.22204288e-06
Iter: 1333 loss: 6.21582694e-06
Iter: 1334 loss: 6.21388563e-06
Iter: 1335 loss: 6.2105355e-06
Iter: 1336 loss: 6.28733778e-06
Iter: 1337 loss: 6.21064282e-06
Iter: 1338 loss: 6.20648461e-06
Iter: 1339 loss: 6.22764583e-06
Iter: 1340 loss: 6.20583569e-06
Iter: 1341 loss: 6.2027475e-06
Iter: 1342 loss: 6.22486095e-06
Iter: 1343 loss: 6.20260425e-06
Iter: 1344 loss: 6.20001219e-06
Iter: 1345 loss: 6.20777564e-06
Iter: 1346 loss: 6.19922685e-06
Iter: 1347 loss: 6.19637513e-06
Iter: 1348 loss: 6.20177434e-06
Iter: 1349 loss: 6.19517596e-06
Iter: 1350 loss: 6.19169896e-06
Iter: 1351 loss: 6.19273214e-06
Iter: 1352 loss: 6.18947524e-06
Iter: 1353 loss: 6.18657805e-06
Iter: 1354 loss: 6.1880346e-06
Iter: 1355 loss: 6.1845376e-06
Iter: 1356 loss: 6.18017475e-06
Iter: 1357 loss: 6.18232025e-06
Iter: 1358 loss: 6.17718752e-06
Iter: 1359 loss: 6.17262867e-06
Iter: 1360 loss: 6.21275149e-06
Iter: 1361 loss: 6.17231581e-06
Iter: 1362 loss: 6.16980697e-06
Iter: 1363 loss: 6.21002346e-06
Iter: 1364 loss: 6.16967191e-06
Iter: 1365 loss: 6.16710031e-06
Iter: 1366 loss: 6.17546948e-06
Iter: 1367 loss: 6.16648e-06
Iter: 1368 loss: 6.16463285e-06
Iter: 1369 loss: 6.16071702e-06
Iter: 1370 loss: 6.230086e-06
Iter: 1371 loss: 6.16060879e-06
Iter: 1372 loss: 6.15676345e-06
Iter: 1373 loss: 6.18455306e-06
Iter: 1374 loss: 6.1563469e-06
Iter: 1375 loss: 6.15366298e-06
Iter: 1376 loss: 6.1640485e-06
Iter: 1377 loss: 6.15311819e-06
Iter: 1378 loss: 6.15032286e-06
Iter: 1379 loss: 6.15905628e-06
Iter: 1380 loss: 6.14929741e-06
Iter: 1381 loss: 6.14617056e-06
Iter: 1382 loss: 6.1587225e-06
Iter: 1383 loss: 6.14547935e-06
Iter: 1384 loss: 6.14267901e-06
Iter: 1385 loss: 6.14496548e-06
Iter: 1386 loss: 6.1411065e-06
Iter: 1387 loss: 6.13886459e-06
Iter: 1388 loss: 6.13745942e-06
Iter: 1389 loss: 6.13636e-06
Iter: 1390 loss: 6.13230213e-06
Iter: 1391 loss: 6.13753218e-06
Iter: 1392 loss: 6.13026214e-06
Iter: 1393 loss: 6.12656277e-06
Iter: 1394 loss: 6.15010413e-06
Iter: 1395 loss: 6.12616259e-06
Iter: 1396 loss: 6.12348504e-06
Iter: 1397 loss: 6.1457431e-06
Iter: 1398 loss: 6.12344547e-06
Iter: 1399 loss: 6.1206747e-06
Iter: 1400 loss: 6.13779093e-06
Iter: 1401 loss: 6.12026724e-06
Iter: 1402 loss: 6.11871246e-06
Iter: 1403 loss: 6.11547694e-06
Iter: 1404 loss: 6.189347e-06
Iter: 1405 loss: 6.11558698e-06
Iter: 1406 loss: 6.11291034e-06
Iter: 1407 loss: 6.12610165e-06
Iter: 1408 loss: 6.11235828e-06
Iter: 1409 loss: 6.10964798e-06
Iter: 1410 loss: 6.11651922e-06
Iter: 1411 loss: 6.10855932e-06
Iter: 1412 loss: 6.10564e-06
Iter: 1413 loss: 6.11854375e-06
Iter: 1414 loss: 6.10491225e-06
Iter: 1415 loss: 6.10237976e-06
Iter: 1416 loss: 6.11832183e-06
Iter: 1417 loss: 6.10194638e-06
Iter: 1418 loss: 6.0995917e-06
Iter: 1419 loss: 6.10178e-06
Iter: 1420 loss: 6.09824247e-06
Iter: 1421 loss: 6.09613926e-06
Iter: 1422 loss: 6.09422841e-06
Iter: 1423 loss: 6.09358449e-06
Iter: 1424 loss: 6.08965138e-06
Iter: 1425 loss: 6.09421659e-06
Iter: 1426 loss: 6.08762366e-06
Iter: 1427 loss: 6.08370829e-06
Iter: 1428 loss: 6.11121231e-06
Iter: 1429 loss: 6.08342452e-06
Iter: 1430 loss: 6.08027131e-06
Iter: 1431 loss: 6.09514382e-06
Iter: 1432 loss: 6.07954053e-06
Iter: 1433 loss: 6.07693255e-06
Iter: 1434 loss: 6.11288533e-06
Iter: 1435 loss: 6.0769371e-06
Iter: 1436 loss: 6.07537186e-06
Iter: 1437 loss: 6.07250149e-06
Iter: 1438 loss: 6.13592965e-06
Iter: 1439 loss: 6.07244738e-06
Iter: 1440 loss: 6.06957201e-06
Iter: 1441 loss: 6.07495713e-06
Iter: 1442 loss: 6.06846334e-06
Iter: 1443 loss: 6.06503227e-06
Iter: 1444 loss: 6.08500068e-06
Iter: 1445 loss: 6.06490084e-06
Iter: 1446 loss: 6.06242975e-06
Iter: 1447 loss: 6.07345828e-06
Iter: 1448 loss: 6.06205958e-06
Iter: 1449 loss: 6.06003641e-06
Iter: 1450 loss: 6.06978665e-06
Iter: 1451 loss: 6.05959031e-06
Iter: 1452 loss: 6.05759078e-06
Iter: 1453 loss: 6.05938567e-06
Iter: 1454 loss: 6.05658e-06
Iter: 1455 loss: 6.05442256e-06
Iter: 1456 loss: 6.05189871e-06
Iter: 1457 loss: 6.05165178e-06
Iter: 1458 loss: 6.04769048e-06
Iter: 1459 loss: 6.05695823e-06
Iter: 1460 loss: 6.04614161e-06
Iter: 1461 loss: 6.0429029e-06
Iter: 1462 loss: 6.0572911e-06
Iter: 1463 loss: 6.04216211e-06
Iter: 1464 loss: 6.03909211e-06
Iter: 1465 loss: 6.0516436e-06
Iter: 1466 loss: 6.03819899e-06
Iter: 1467 loss: 6.03590843e-06
Iter: 1468 loss: 6.03589e-06
Iter: 1469 loss: 6.03447552e-06
Iter: 1470 loss: 6.03170247e-06
Iter: 1471 loss: 6.07598395e-06
Iter: 1472 loss: 6.03173748e-06
Iter: 1473 loss: 6.0287075e-06
Iter: 1474 loss: 6.0315283e-06
Iter: 1475 loss: 6.0269349e-06
Iter: 1476 loss: 6.02365435e-06
Iter: 1477 loss: 6.05376363e-06
Iter: 1478 loss: 6.02351975e-06
Iter: 1479 loss: 6.02114596e-06
Iter: 1480 loss: 6.02881937e-06
Iter: 1481 loss: 6.02057116e-06
Iter: 1482 loss: 6.01812e-06
Iter: 1483 loss: 6.02665e-06
Iter: 1484 loss: 6.01735792e-06
Iter: 1485 loss: 6.01476222e-06
Iter: 1486 loss: 6.01909414e-06
Iter: 1487 loss: 6.01339343e-06
Iter: 1488 loss: 6.01057673e-06
Iter: 1489 loss: 6.00814565e-06
Iter: 1490 loss: 6.00753901e-06
Iter: 1491 loss: 6.00345447e-06
Iter: 1492 loss: 6.01700413e-06
Iter: 1493 loss: 6.00228122e-06
Iter: 1494 loss: 5.99877694e-06
Iter: 1495 loss: 6.00494e-06
Iter: 1496 loss: 5.99752184e-06
Iter: 1497 loss: 5.99363102e-06
Iter: 1498 loss: 6.01270722e-06
Iter: 1499 loss: 5.99268333e-06
Iter: 1500 loss: 5.99138366e-06
Iter: 1501 loss: 5.99073746e-06
Iter: 1502 loss: 5.9896056e-06
Iter: 1503 loss: 5.98694351e-06
Iter: 1504 loss: 6.02358341e-06
Iter: 1505 loss: 5.98695351e-06
Iter: 1506 loss: 5.98387351e-06
Iter: 1507 loss: 5.98332e-06
Iter: 1508 loss: 5.98125825e-06
Iter: 1509 loss: 5.97734606e-06
Iter: 1510 loss: 6.0102011e-06
Iter: 1511 loss: 5.97704911e-06
Iter: 1512 loss: 5.97415465e-06
Iter: 1513 loss: 5.98476527e-06
Iter: 1514 loss: 5.97339613e-06
Iter: 1515 loss: 5.97032931e-06
Iter: 1516 loss: 5.98050246e-06
Iter: 1517 loss: 5.96952395e-06
Iter: 1518 loss: 5.9666022e-06
Iter: 1519 loss: 5.97564667e-06
Iter: 1520 loss: 5.96589507e-06
Iter: 1521 loss: 5.96328664e-06
Iter: 1522 loss: 5.9620761e-06
Iter: 1523 loss: 5.96072186e-06
Iter: 1524 loss: 5.95724941e-06
Iter: 1525 loss: 5.96199334e-06
Iter: 1526 loss: 5.95532265e-06
Iter: 1527 loss: 5.95113852e-06
Iter: 1528 loss: 5.95782967e-06
Iter: 1529 loss: 5.94910307e-06
Iter: 1530 loss: 5.944853e-06
Iter: 1531 loss: 5.98313909e-06
Iter: 1532 loss: 5.94482754e-06
Iter: 1533 loss: 5.94371977e-06
Iter: 1534 loss: 5.9432241e-06
Iter: 1535 loss: 5.94184712e-06
Iter: 1536 loss: 5.93900313e-06
Iter: 1537 loss: 5.97342296e-06
Iter: 1538 loss: 5.93879668e-06
Iter: 1539 loss: 5.93552977e-06
Iter: 1540 loss: 5.93740788e-06
Iter: 1541 loss: 5.93350796e-06
Iter: 1542 loss: 5.93009281e-06
Iter: 1543 loss: 5.96427435e-06
Iter: 1544 loss: 5.93016921e-06
Iter: 1545 loss: 5.92769857e-06
Iter: 1546 loss: 5.93763525e-06
Iter: 1547 loss: 5.92706783e-06
Iter: 1548 loss: 5.9248523e-06
Iter: 1549 loss: 5.93036e-06
Iter: 1550 loss: 5.92417928e-06
Iter: 1551 loss: 5.92140395e-06
Iter: 1552 loss: 5.92856031e-06
Iter: 1553 loss: 5.92061588e-06
Iter: 1554 loss: 5.91808202e-06
Iter: 1555 loss: 5.91750586e-06
Iter: 1556 loss: 5.91591e-06
Iter: 1557 loss: 5.9128306e-06
Iter: 1558 loss: 5.91458456e-06
Iter: 1559 loss: 5.91060825e-06
Iter: 1560 loss: 5.90641594e-06
Iter: 1561 loss: 5.91832486e-06
Iter: 1562 loss: 5.90496256e-06
Iter: 1563 loss: 5.90181389e-06
Iter: 1564 loss: 5.92840615e-06
Iter: 1565 loss: 5.90174295e-06
Iter: 1566 loss: 5.90044692e-06
Iter: 1567 loss: 5.90020318e-06
Iter: 1568 loss: 5.89899037e-06
Iter: 1569 loss: 5.89596357e-06
Iter: 1570 loss: 5.93526966e-06
Iter: 1571 loss: 5.89581396e-06
Iter: 1572 loss: 5.89237607e-06
Iter: 1573 loss: 5.89465162e-06
Iter: 1574 loss: 5.89016145e-06
Iter: 1575 loss: 5.88689818e-06
Iter: 1576 loss: 5.91637581e-06
Iter: 1577 loss: 5.88678631e-06
Iter: 1578 loss: 5.88420698e-06
Iter: 1579 loss: 5.89644605e-06
Iter: 1580 loss: 5.88358398e-06
Iter: 1581 loss: 5.88108151e-06
Iter: 1582 loss: 5.88530384e-06
Iter: 1583 loss: 5.88015246e-06
Iter: 1584 loss: 5.87692375e-06
Iter: 1585 loss: 5.88554258e-06
Iter: 1586 loss: 5.87595559e-06
Iter: 1587 loss: 5.87295926e-06
Iter: 1588 loss: 5.87523164e-06
Iter: 1589 loss: 5.87120439e-06
Iter: 1590 loss: 5.86804617e-06
Iter: 1591 loss: 5.8670712e-06
Iter: 1592 loss: 5.8655246e-06
Iter: 1593 loss: 5.86062606e-06
Iter: 1594 loss: 5.87880231e-06
Iter: 1595 loss: 5.85931321e-06
Iter: 1596 loss: 5.85569114e-06
Iter: 1597 loss: 5.87371596e-06
Iter: 1598 loss: 5.8549972e-06
Iter: 1599 loss: 5.85330599e-06
Iter: 1600 loss: 5.85310318e-06
Iter: 1601 loss: 5.85113548e-06
Iter: 1602 loss: 5.84739473e-06
Iter: 1603 loss: 5.93285949e-06
Iter: 1604 loss: 5.84743339e-06
Iter: 1605 loss: 5.8435744e-06
Iter: 1606 loss: 5.84507916e-06
Iter: 1607 loss: 5.84126428e-06
Iter: 1608 loss: 5.83707606e-06
Iter: 1609 loss: 5.8626074e-06
Iter: 1610 loss: 5.83666315e-06
Iter: 1611 loss: 5.83332076e-06
Iter: 1612 loss: 5.8507012e-06
Iter: 1613 loss: 5.83270366e-06
Iter: 1614 loss: 5.82936627e-06
Iter: 1615 loss: 5.83505562e-06
Iter: 1616 loss: 5.82788198e-06
Iter: 1617 loss: 5.82425491e-06
Iter: 1618 loss: 5.84198096e-06
Iter: 1619 loss: 5.82375424e-06
Iter: 1620 loss: 5.82089433e-06
Iter: 1621 loss: 5.82431221e-06
Iter: 1622 loss: 5.81955373e-06
Iter: 1623 loss: 5.81702079e-06
Iter: 1624 loss: 5.81414315e-06
Iter: 1625 loss: 5.81367885e-06
Iter: 1626 loss: 5.80907e-06
Iter: 1627 loss: 5.83450901e-06
Iter: 1628 loss: 5.80829783e-06
Iter: 1629 loss: 5.80501228e-06
Iter: 1630 loss: 5.81777749e-06
Iter: 1631 loss: 5.80434698e-06
Iter: 1632 loss: 5.80287588e-06
Iter: 1633 loss: 5.8025239e-06
Iter: 1634 loss: 5.80064352e-06
Iter: 1635 loss: 5.79837e-06
Iter: 1636 loss: 5.79812149e-06
Iter: 1637 loss: 5.79539073e-06
Iter: 1638 loss: 5.79392781e-06
Iter: 1639 loss: 5.79272637e-06
Iter: 1640 loss: 5.7891566e-06
Iter: 1641 loss: 5.80534743e-06
Iter: 1642 loss: 5.78861091e-06
Iter: 1643 loss: 5.78503341e-06
Iter: 1644 loss: 5.80076e-06
Iter: 1645 loss: 5.78423169e-06
Iter: 1646 loss: 5.78079698e-06
Iter: 1647 loss: 5.79203697e-06
Iter: 1648 loss: 5.77967148e-06
Iter: 1649 loss: 5.77671153e-06
Iter: 1650 loss: 5.79256493e-06
Iter: 1651 loss: 5.77642413e-06
Iter: 1652 loss: 5.77401624e-06
Iter: 1653 loss: 5.7758507e-06
Iter: 1654 loss: 5.77261926e-06
Iter: 1655 loss: 5.76978164e-06
Iter: 1656 loss: 5.76598359e-06
Iter: 1657 loss: 5.76592e-06
Iter: 1658 loss: 5.76105549e-06
Iter: 1659 loss: 5.79863672e-06
Iter: 1660 loss: 5.7608172e-06
Iter: 1661 loss: 5.75776903e-06
Iter: 1662 loss: 5.76727462e-06
Iter: 1663 loss: 5.75710328e-06
Iter: 1664 loss: 5.75510876e-06
Iter: 1665 loss: 5.75513332e-06
Iter: 1666 loss: 5.75277409e-06
Iter: 1667 loss: 5.75228523e-06
Iter: 1668 loss: 5.75091508e-06
Iter: 1669 loss: 5.74858768e-06
Iter: 1670 loss: 5.74641126e-06
Iter: 1671 loss: 5.74592104e-06
Iter: 1672 loss: 5.74272144e-06
Iter: 1673 loss: 5.75520244e-06
Iter: 1674 loss: 5.74207343e-06
Iter: 1675 loss: 5.7386992e-06
Iter: 1676 loss: 5.75682861e-06
Iter: 1677 loss: 5.73818215e-06
Iter: 1678 loss: 5.73551142e-06
Iter: 1679 loss: 5.74765454e-06
Iter: 1680 loss: 5.73499619e-06
Iter: 1681 loss: 5.7331622e-06
Iter: 1682 loss: 5.7414e-06
Iter: 1683 loss: 5.73285433e-06
Iter: 1684 loss: 5.73084344e-06
Iter: 1685 loss: 5.73110401e-06
Iter: 1686 loss: 5.72916542e-06
Iter: 1687 loss: 5.72654244e-06
Iter: 1688 loss: 5.72504359e-06
Iter: 1689 loss: 5.72403678e-06
Iter: 1690 loss: 5.72052249e-06
Iter: 1691 loss: 5.74054911e-06
Iter: 1692 loss: 5.71990404e-06
Iter: 1693 loss: 5.71712644e-06
Iter: 1694 loss: 5.72512727e-06
Iter: 1695 loss: 5.71628107e-06
Iter: 1696 loss: 5.71398914e-06
Iter: 1697 loss: 5.74532078e-06
Iter: 1698 loss: 5.71412693e-06
Iter: 1699 loss: 5.7117777e-06
Iter: 1700 loss: 5.71426244e-06
Iter: 1701 loss: 5.71039072e-06
Iter: 1702 loss: 5.70854354e-06
Iter: 1703 loss: 5.70630164e-06
Iter: 1704 loss: 5.70589054e-06
Iter: 1705 loss: 5.70318571e-06
Iter: 1706 loss: 5.7090765e-06
Iter: 1707 loss: 5.70213e-06
Iter: 1708 loss: 5.6988406e-06
Iter: 1709 loss: 5.72135104e-06
Iter: 1710 loss: 5.69837e-06
Iter: 1711 loss: 5.69598433e-06
Iter: 1712 loss: 5.71084047e-06
Iter: 1713 loss: 5.69551503e-06
Iter: 1714 loss: 5.69385111e-06
Iter: 1715 loss: 5.69894792e-06
Iter: 1716 loss: 5.69330859e-06
Iter: 1717 loss: 5.69114036e-06
Iter: 1718 loss: 5.6907038e-06
Iter: 1719 loss: 5.68951555e-06
Iter: 1720 loss: 5.68650057e-06
Iter: 1721 loss: 5.6870058e-06
Iter: 1722 loss: 5.68401174e-06
Iter: 1723 loss: 5.68045971e-06
Iter: 1724 loss: 5.69056283e-06
Iter: 1725 loss: 5.67964071e-06
Iter: 1726 loss: 5.67598454e-06
Iter: 1727 loss: 5.69177519e-06
Iter: 1728 loss: 5.67546522e-06
Iter: 1729 loss: 5.67303368e-06
Iter: 1730 loss: 5.70759312e-06
Iter: 1731 loss: 5.67306233e-06
Iter: 1732 loss: 5.67055577e-06
Iter: 1733 loss: 5.67763072e-06
Iter: 1734 loss: 5.66996096e-06
Iter: 1735 loss: 5.66850713e-06
Iter: 1736 loss: 5.66590734e-06
Iter: 1737 loss: 5.66588369e-06
Iter: 1738 loss: 5.66279596e-06
Iter: 1739 loss: 5.6657982e-06
Iter: 1740 loss: 5.66124254e-06
Iter: 1741 loss: 5.65723712e-06
Iter: 1742 loss: 5.69850545e-06
Iter: 1743 loss: 5.65728533e-06
Iter: 1744 loss: 5.65464461e-06
Iter: 1745 loss: 5.6696922e-06
Iter: 1746 loss: 5.65452e-06
Iter: 1747 loss: 5.65267328e-06
Iter: 1748 loss: 5.65531354e-06
Iter: 1749 loss: 5.65191e-06
Iter: 1750 loss: 5.64956554e-06
Iter: 1751 loss: 5.65081e-06
Iter: 1752 loss: 5.64793481e-06
Iter: 1753 loss: 5.64505353e-06
Iter: 1754 loss: 5.64641687e-06
Iter: 1755 loss: 5.64318361e-06
Iter: 1756 loss: 5.64025322e-06
Iter: 1757 loss: 5.64471429e-06
Iter: 1758 loss: 5.63862432e-06
Iter: 1759 loss: 5.63542926e-06
Iter: 1760 loss: 5.65309165e-06
Iter: 1761 loss: 5.6349736e-06
Iter: 1762 loss: 5.63283629e-06
Iter: 1763 loss: 5.66002655e-06
Iter: 1764 loss: 5.63284e-06
Iter: 1765 loss: 5.63079766e-06
Iter: 1766 loss: 5.63812e-06
Iter: 1767 loss: 5.63037793e-06
Iter: 1768 loss: 5.62903551e-06
Iter: 1769 loss: 5.62611285e-06
Iter: 1770 loss: 5.67845927e-06
Iter: 1771 loss: 5.62604646e-06
Iter: 1772 loss: 5.6227218e-06
Iter: 1773 loss: 5.62634114e-06
Iter: 1774 loss: 5.62092418e-06
Iter: 1775 loss: 5.61779916e-06
Iter: 1776 loss: 5.61776596e-06
Iter: 1777 loss: 5.61565639e-06
Iter: 1778 loss: 5.62541345e-06
Iter: 1779 loss: 5.61536581e-06
Iter: 1780 loss: 5.61381603e-06
Iter: 1781 loss: 5.61510024e-06
Iter: 1782 loss: 5.61272645e-06
Iter: 1783 loss: 5.61021716e-06
Iter: 1784 loss: 5.61504839e-06
Iter: 1785 loss: 5.60912213e-06
Iter: 1786 loss: 5.6067106e-06
Iter: 1787 loss: 5.60886929e-06
Iter: 1788 loss: 5.60539411e-06
Iter: 1789 loss: 5.6029271e-06
Iter: 1790 loss: 5.60390799e-06
Iter: 1791 loss: 5.60099033e-06
Iter: 1792 loss: 5.59776026e-06
Iter: 1793 loss: 5.6130275e-06
Iter: 1794 loss: 5.59727187e-06
Iter: 1795 loss: 5.59487216e-06
Iter: 1796 loss: 5.62269133e-06
Iter: 1797 loss: 5.59471118e-06
Iter: 1798 loss: 5.59261662e-06
Iter: 1799 loss: 5.60250146e-06
Iter: 1800 loss: 5.59210912e-06
Iter: 1801 loss: 5.5903829e-06
Iter: 1802 loss: 5.58675129e-06
Iter: 1803 loss: 5.64121456e-06
Iter: 1804 loss: 5.58666306e-06
Iter: 1805 loss: 5.58256488e-06
Iter: 1806 loss: 5.59076261e-06
Iter: 1807 loss: 5.58094916e-06
Iter: 1808 loss: 5.57773546e-06
Iter: 1809 loss: 5.62786681e-06
Iter: 1810 loss: 5.57778912e-06
Iter: 1811 loss: 5.57549629e-06
Iter: 1812 loss: 5.58363e-06
Iter: 1813 loss: 5.57486646e-06
Iter: 1814 loss: 5.57247904e-06
Iter: 1815 loss: 5.57526e-06
Iter: 1816 loss: 5.57142585e-06
Iter: 1817 loss: 5.56851228e-06
Iter: 1818 loss: 5.57775502e-06
Iter: 1819 loss: 5.56774194e-06
Iter: 1820 loss: 5.56552732e-06
Iter: 1821 loss: 5.56690793e-06
Iter: 1822 loss: 5.56405894e-06
Iter: 1823 loss: 5.56139e-06
Iter: 1824 loss: 5.56108e-06
Iter: 1825 loss: 5.55914312e-06
Iter: 1826 loss: 5.55549286e-06
Iter: 1827 loss: 5.57766407e-06
Iter: 1828 loss: 5.55501629e-06
Iter: 1829 loss: 5.55241968e-06
Iter: 1830 loss: 5.58574266e-06
Iter: 1831 loss: 5.55247971e-06
Iter: 1832 loss: 5.55061069e-06
Iter: 1833 loss: 5.56280338e-06
Iter: 1834 loss: 5.55029328e-06
Iter: 1835 loss: 5.54896951e-06
Iter: 1836 loss: 5.54605322e-06
Iter: 1837 loss: 5.58451e-06
Iter: 1838 loss: 5.54597318e-06
Iter: 1839 loss: 5.54261169e-06
Iter: 1840 loss: 5.55029237e-06
Iter: 1841 loss: 5.5415785e-06
Iter: 1842 loss: 5.53904829e-06
Iter: 1843 loss: 5.56971099e-06
Iter: 1844 loss: 5.53888594e-06
Iter: 1845 loss: 5.53673408e-06
Iter: 1846 loss: 5.54284134e-06
Iter: 1847 loss: 5.53572045e-06
Iter: 1848 loss: 5.53338714e-06
Iter: 1849 loss: 5.53905375e-06
Iter: 1850 loss: 5.53259542e-06
Iter: 1851 loss: 5.53025257e-06
Iter: 1852 loss: 5.53932523e-06
Iter: 1853 loss: 5.52955498e-06
Iter: 1854 loss: 5.52772417e-06
Iter: 1855 loss: 5.5275068e-06
Iter: 1856 loss: 5.52609299e-06
Iter: 1857 loss: 5.5235123e-06
Iter: 1858 loss: 5.52289839e-06
Iter: 1859 loss: 5.52116444e-06
Iter: 1860 loss: 5.5178125e-06
Iter: 1861 loss: 5.54283906e-06
Iter: 1862 loss: 5.51737094e-06
Iter: 1863 loss: 5.51549329e-06
Iter: 1864 loss: 5.51552284e-06
Iter: 1865 loss: 5.51428957e-06
Iter: 1866 loss: 5.52336041e-06
Iter: 1867 loss: 5.51414087e-06
Iter: 1868 loss: 5.5129567e-06
Iter: 1869 loss: 5.51003359e-06
Iter: 1870 loss: 5.53508e-06
Iter: 1871 loss: 5.5093642e-06
Iter: 1872 loss: 5.50619188e-06
Iter: 1873 loss: 5.51518633e-06
Iter: 1874 loss: 5.50501909e-06
Iter: 1875 loss: 5.5021419e-06
Iter: 1876 loss: 5.5284554e-06
Iter: 1877 loss: 5.50196546e-06
Iter: 1878 loss: 5.4992779e-06
Iter: 1879 loss: 5.50688765e-06
Iter: 1880 loss: 5.49839297e-06
Iter: 1881 loss: 5.49581591e-06
Iter: 1882 loss: 5.50456843e-06
Iter: 1883 loss: 5.49515607e-06
Iter: 1884 loss: 5.49274864e-06
Iter: 1885 loss: 5.5016003e-06
Iter: 1886 loss: 5.49216929e-06
Iter: 1887 loss: 5.4901534e-06
Iter: 1888 loss: 5.48895423e-06
Iter: 1889 loss: 5.48816206e-06
Iter: 1890 loss: 5.48467733e-06
Iter: 1891 loss: 5.48654862e-06
Iter: 1892 loss: 5.4824759e-06
Iter: 1893 loss: 5.47915442e-06
Iter: 1894 loss: 5.50106688e-06
Iter: 1895 loss: 5.47876607e-06
Iter: 1896 loss: 5.47691889e-06
Iter: 1897 loss: 5.47685931e-06
Iter: 1898 loss: 5.47529908e-06
Iter: 1899 loss: 5.48448224e-06
Iter: 1900 loss: 5.47515265e-06
Iter: 1901 loss: 5.47360105e-06
Iter: 1902 loss: 5.47014042e-06
Iter: 1903 loss: 5.50828e-06
Iter: 1904 loss: 5.46977753e-06
Iter: 1905 loss: 5.46637921e-06
Iter: 1906 loss: 5.47851778e-06
Iter: 1907 loss: 5.46571027e-06
Iter: 1908 loss: 5.46277442e-06
Iter: 1909 loss: 5.47795025e-06
Iter: 1910 loss: 5.46217507e-06
Iter: 1911 loss: 5.4590837e-06
Iter: 1912 loss: 5.47109357e-06
Iter: 1913 loss: 5.45824878e-06
Iter: 1914 loss: 5.4557554e-06
Iter: 1915 loss: 5.4674797e-06
Iter: 1916 loss: 5.45503508e-06
Iter: 1917 loss: 5.4532029e-06
Iter: 1918 loss: 5.46036927e-06
Iter: 1919 loss: 5.4525467e-06
Iter: 1920 loss: 5.45070725e-06
Iter: 1921 loss: 5.44902105e-06
Iter: 1922 loss: 5.44852446e-06
Iter: 1923 loss: 5.44525938e-06
Iter: 1924 loss: 5.44815612e-06
Iter: 1925 loss: 5.4433508e-06
Iter: 1926 loss: 5.44026898e-06
Iter: 1927 loss: 5.45317289e-06
Iter: 1928 loss: 5.43945225e-06
Iter: 1929 loss: 5.43728356e-06
Iter: 1930 loss: 5.47273885e-06
Iter: 1931 loss: 5.43729493e-06
Iter: 1932 loss: 5.4353668e-06
Iter: 1933 loss: 5.44743762e-06
Iter: 1934 loss: 5.4352704e-06
Iter: 1935 loss: 5.43360375e-06
Iter: 1936 loss: 5.43021906e-06
Iter: 1937 loss: 5.49205743e-06
Iter: 1938 loss: 5.43016495e-06
Iter: 1939 loss: 5.42741645e-06
Iter: 1940 loss: 5.43642636e-06
Iter: 1941 loss: 5.42693533e-06
Iter: 1942 loss: 5.42422231e-06
Iter: 1943 loss: 5.43323495e-06
Iter: 1944 loss: 5.42346652e-06
Iter: 1945 loss: 5.42070893e-06
Iter: 1946 loss: 5.43863871e-06
Iter: 1947 loss: 5.42040789e-06
Iter: 1948 loss: 5.41867939e-06
Iter: 1949 loss: 5.42716225e-06
Iter: 1950 loss: 5.41837107e-06
Iter: 1951 loss: 5.41686768e-06
Iter: 1952 loss: 5.4207303e-06
Iter: 1953 loss: 5.41629379e-06
Iter: 1954 loss: 5.41473673e-06
Iter: 1955 loss: 5.41365625e-06
Iter: 1956 loss: 5.41320878e-06
Iter: 1957 loss: 5.41029658e-06
Iter: 1958 loss: 5.41289501e-06
Iter: 1959 loss: 5.4090151e-06
Iter: 1960 loss: 5.40615838e-06
Iter: 1961 loss: 5.41503732e-06
Iter: 1962 loss: 5.40529709e-06
Iter: 1963 loss: 5.40299288e-06
Iter: 1964 loss: 5.4384418e-06
Iter: 1965 loss: 5.40292e-06
Iter: 1966 loss: 5.40131896e-06
Iter: 1967 loss: 5.41466807e-06
Iter: 1968 loss: 5.40117799e-06
Iter: 1969 loss: 5.39970733e-06
Iter: 1970 loss: 5.39680741e-06
Iter: 1971 loss: 5.45561215e-06
Iter: 1972 loss: 5.39693519e-06
Iter: 1973 loss: 5.39459643e-06
Iter: 1974 loss: 5.39880511e-06
Iter: 1975 loss: 5.39362918e-06
Iter: 1976 loss: 5.39081475e-06
Iter: 1977 loss: 5.39906341e-06
Iter: 1978 loss: 5.39002167e-06
Iter: 1979 loss: 5.38731e-06
Iter: 1980 loss: 5.41508143e-06
Iter: 1981 loss: 5.38705899e-06
Iter: 1982 loss: 5.38566474e-06
Iter: 1983 loss: 5.39067514e-06
Iter: 1984 loss: 5.38512e-06
Iter: 1985 loss: 5.38373661e-06
Iter: 1986 loss: 5.38694894e-06
Iter: 1987 loss: 5.38318909e-06
Iter: 1988 loss: 5.38115819e-06
Iter: 1989 loss: 5.38035465e-06
Iter: 1990 loss: 5.379507e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.8
+ date
Mon Oct 26 15:44:30 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.8/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.8_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi -1 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.8_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc51a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc5dad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc474d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc3ef9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc3ef2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc400d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc3d4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc348598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc349048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc349ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc306b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc29f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc29fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc2c4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc2c47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc23c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc252378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc1e4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc1ec730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc1a9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc1d1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc1d1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc122840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc14a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc122f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc0ad840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc1029d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc07a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc07a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc022620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcdfc021268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcde07ae1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcde07ae378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcde078a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcde07789d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcde070d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.0532373e-05
Iter: 2 loss: 7.41790791e-05
Iter: 3 loss: 5.17730041e-05
Iter: 4 loss: 4.60570482e-05
Iter: 5 loss: 5.17708795e-05
Iter: 6 loss: 4.28077765e-05
Iter: 7 loss: 3.99880373e-05
Iter: 8 loss: 3.47319547e-05
Iter: 9 loss: 0.000152783876
Iter: 10 loss: 3.47217901e-05
Iter: 11 loss: 3.08910385e-05
Iter: 12 loss: 3.08882081e-05
Iter: 13 loss: 2.9068482e-05
Iter: 14 loss: 3.2016862e-05
Iter: 15 loss: 2.8230148e-05
Iter: 16 loss: 2.57967404e-05
Iter: 17 loss: 2.59144726e-05
Iter: 18 loss: 2.38973644e-05
Iter: 19 loss: 2.15433902e-05
Iter: 20 loss: 3.9742823e-05
Iter: 21 loss: 2.13687817e-05
Iter: 22 loss: 1.97234822e-05
Iter: 23 loss: 2.10809139e-05
Iter: 24 loss: 1.87387814e-05
Iter: 25 loss: 1.77000948e-05
Iter: 26 loss: 3.3650922e-05
Iter: 27 loss: 1.769972e-05
Iter: 28 loss: 1.68596162e-05
Iter: 29 loss: 1.71012762e-05
Iter: 30 loss: 1.62544893e-05
Iter: 31 loss: 1.55760426e-05
Iter: 32 loss: 2.08299898e-05
Iter: 33 loss: 1.55264606e-05
Iter: 34 loss: 1.49356392e-05
Iter: 35 loss: 1.48689314e-05
Iter: 36 loss: 1.44411624e-05
Iter: 37 loss: 1.37992774e-05
Iter: 38 loss: 2.29670077e-05
Iter: 39 loss: 1.37978368e-05
Iter: 40 loss: 1.3576624e-05
Iter: 41 loss: 1.5808635e-05
Iter: 42 loss: 1.35696082e-05
Iter: 43 loss: 1.32662262e-05
Iter: 44 loss: 1.33254553e-05
Iter: 45 loss: 1.3040235e-05
Iter: 46 loss: 1.28057673e-05
Iter: 47 loss: 1.2570752e-05
Iter: 48 loss: 1.25231691e-05
Iter: 49 loss: 1.21686535e-05
Iter: 50 loss: 1.19812148e-05
Iter: 51 loss: 1.18196867e-05
Iter: 52 loss: 1.14778886e-05
Iter: 53 loss: 1.14679078e-05
Iter: 54 loss: 1.12520756e-05
Iter: 55 loss: 1.16990168e-05
Iter: 56 loss: 1.11658701e-05
Iter: 57 loss: 1.09002922e-05
Iter: 58 loss: 1.1302538e-05
Iter: 59 loss: 1.0774027e-05
Iter: 60 loss: 1.05957197e-05
Iter: 61 loss: 1.17252039e-05
Iter: 62 loss: 1.0575025e-05
Iter: 63 loss: 1.03949988e-05
Iter: 64 loss: 1.02237809e-05
Iter: 65 loss: 1.01828573e-05
Iter: 66 loss: 9.92469722e-06
Iter: 67 loss: 1.04525243e-05
Iter: 68 loss: 9.82070105e-06
Iter: 69 loss: 9.49477544e-06
Iter: 70 loss: 1.10036381e-05
Iter: 71 loss: 9.43367741e-06
Iter: 72 loss: 9.2692917e-06
Iter: 73 loss: 1.02597141e-05
Iter: 74 loss: 9.24860433e-06
Iter: 75 loss: 9.09741357e-06
Iter: 76 loss: 1.00226944e-05
Iter: 77 loss: 9.07919821e-06
Iter: 78 loss: 8.97085e-06
Iter: 79 loss: 8.96915662e-06
Iter: 80 loss: 8.93008382e-06
Iter: 81 loss: 8.82342283e-06
Iter: 82 loss: 9.4713123e-06
Iter: 83 loss: 8.79433173e-06
Iter: 84 loss: 8.62822344e-06
Iter: 85 loss: 8.74741272e-06
Iter: 86 loss: 8.5260308e-06
Iter: 87 loss: 8.40453e-06
Iter: 88 loss: 9.29285852e-06
Iter: 89 loss: 8.39469612e-06
Iter: 90 loss: 8.27904114e-06
Iter: 91 loss: 8.49105345e-06
Iter: 92 loss: 8.22943821e-06
Iter: 93 loss: 8.14969098e-06
Iter: 94 loss: 8.9082987e-06
Iter: 95 loss: 8.14686427e-06
Iter: 96 loss: 8.07688e-06
Iter: 97 loss: 8.0825921e-06
Iter: 98 loss: 8.02258091e-06
Iter: 99 loss: 7.9194615e-06
Iter: 100 loss: 8.29912e-06
Iter: 101 loss: 7.89395199e-06
Iter: 102 loss: 7.81658218e-06
Iter: 103 loss: 7.71512623e-06
Iter: 104 loss: 7.70880706e-06
Iter: 105 loss: 7.61211095e-06
Iter: 106 loss: 7.60972762e-06
Iter: 107 loss: 7.55646488e-06
Iter: 108 loss: 7.51564858e-06
Iter: 109 loss: 7.49859873e-06
Iter: 110 loss: 7.40893029e-06
Iter: 111 loss: 8.31807301e-06
Iter: 112 loss: 7.40623545e-06
Iter: 113 loss: 7.41851727e-06
Iter: 114 loss: 7.38650033e-06
Iter: 115 loss: 7.37055507e-06
Iter: 116 loss: 7.31764e-06
Iter: 117 loss: 7.31407181e-06
Iter: 118 loss: 7.2612e-06
Iter: 119 loss: 7.18827823e-06
Iter: 120 loss: 7.99429927e-06
Iter: 121 loss: 7.18680803e-06
Iter: 122 loss: 7.13931468e-06
Iter: 123 loss: 7.16811746e-06
Iter: 124 loss: 7.10911945e-06
Iter: 125 loss: 7.0407541e-06
Iter: 126 loss: 7.33384786e-06
Iter: 127 loss: 7.02692796e-06
Iter: 128 loss: 6.98248095e-06
Iter: 129 loss: 6.98665e-06
Iter: 130 loss: 6.94832852e-06
Iter: 131 loss: 6.88111595e-06
Iter: 132 loss: 7.46636852e-06
Iter: 133 loss: 6.87790089e-06
Iter: 134 loss: 6.83822145e-06
Iter: 135 loss: 6.89060289e-06
Iter: 136 loss: 6.81832944e-06
Iter: 137 loss: 6.76640775e-06
Iter: 138 loss: 6.77616572e-06
Iter: 139 loss: 6.72743681e-06
Iter: 140 loss: 6.67665654e-06
Iter: 141 loss: 6.93459151e-06
Iter: 142 loss: 6.66808955e-06
Iter: 143 loss: 6.61178092e-06
Iter: 144 loss: 6.59516809e-06
Iter: 145 loss: 6.56141901e-06
Iter: 146 loss: 6.52779954e-06
Iter: 147 loss: 6.52121525e-06
Iter: 148 loss: 6.51108212e-06
Iter: 149 loss: 6.50660741e-06
Iter: 150 loss: 6.49531876e-06
Iter: 151 loss: 6.4596511e-06
Iter: 152 loss: 6.50491756e-06
Iter: 153 loss: 6.43319208e-06
Iter: 154 loss: 6.4023593e-06
Iter: 155 loss: 6.79691811e-06
Iter: 156 loss: 6.40232065e-06
Iter: 157 loss: 6.37707853e-06
Iter: 158 loss: 6.33365471e-06
Iter: 159 loss: 6.33348463e-06
Iter: 160 loss: 6.30422437e-06
Iter: 161 loss: 6.30022441e-06
Iter: 162 loss: 6.28101952e-06
Iter: 163 loss: 6.24413133e-06
Iter: 164 loss: 7.02340094e-06
Iter: 165 loss: 6.24391441e-06
Iter: 166 loss: 6.21669824e-06
Iter: 167 loss: 6.213907e-06
Iter: 168 loss: 6.19351795e-06
Iter: 169 loss: 6.23006417e-06
Iter: 170 loss: 6.18459762e-06
Iter: 171 loss: 6.15690715e-06
Iter: 172 loss: 6.13834118e-06
Iter: 173 loss: 6.12783242e-06
Iter: 174 loss: 6.09491963e-06
Iter: 175 loss: 6.15742647e-06
Iter: 176 loss: 6.08099845e-06
Iter: 177 loss: 6.0387074e-06
Iter: 178 loss: 6.31426246e-06
Iter: 179 loss: 6.03404123e-06
Iter: 180 loss: 6.02170485e-06
Iter: 181 loss: 6.0210341e-06
Iter: 182 loss: 6.00647672e-06
Iter: 183 loss: 6.03447916e-06
Iter: 184 loss: 6.00047497e-06
Iter: 185 loss: 5.98830047e-06
Iter: 186 loss: 5.95777465e-06
Iter: 187 loss: 6.22348125e-06
Iter: 188 loss: 5.95285837e-06
Iter: 189 loss: 5.92583228e-06
Iter: 190 loss: 6.11259111e-06
Iter: 191 loss: 5.9232284e-06
Iter: 192 loss: 5.8975188e-06
Iter: 193 loss: 5.89675437e-06
Iter: 194 loss: 5.87667637e-06
Iter: 195 loss: 5.8554906e-06
Iter: 196 loss: 5.8552655e-06
Iter: 197 loss: 5.83889687e-06
Iter: 198 loss: 5.82423399e-06
Iter: 199 loss: 5.81986478e-06
Iter: 200 loss: 5.79979769e-06
Iter: 201 loss: 6.08796108e-06
Iter: 202 loss: 5.7997845e-06
Iter: 203 loss: 5.78368235e-06
Iter: 204 loss: 5.7918237e-06
Iter: 205 loss: 5.77279116e-06
Iter: 206 loss: 5.75041577e-06
Iter: 207 loss: 5.78274557e-06
Iter: 208 loss: 5.73951411e-06
Iter: 209 loss: 5.71852706e-06
Iter: 210 loss: 5.73060561e-06
Iter: 211 loss: 5.70508382e-06
Iter: 212 loss: 5.67748066e-06
Iter: 213 loss: 5.88577268e-06
Iter: 214 loss: 5.67533152e-06
Iter: 215 loss: 5.68031146e-06
Iter: 216 loss: 5.66914878e-06
Iter: 217 loss: 5.66353083e-06
Iter: 218 loss: 5.64957e-06
Iter: 219 loss: 5.80165261e-06
Iter: 220 loss: 5.6478957e-06
Iter: 221 loss: 5.63008234e-06
Iter: 222 loss: 5.61372508e-06
Iter: 223 loss: 5.60923763e-06
Iter: 224 loss: 5.58816646e-06
Iter: 225 loss: 5.66496647e-06
Iter: 226 loss: 5.58300098e-06
Iter: 227 loss: 5.55733141e-06
Iter: 228 loss: 5.59679847e-06
Iter: 229 loss: 5.54486e-06
Iter: 230 loss: 5.52980282e-06
Iter: 231 loss: 5.75706235e-06
Iter: 232 loss: 5.5297478e-06
Iter: 233 loss: 5.51684388e-06
Iter: 234 loss: 5.49576589e-06
Iter: 235 loss: 5.49558445e-06
Iter: 236 loss: 5.4824668e-06
Iter: 237 loss: 5.48093203e-06
Iter: 238 loss: 5.47031959e-06
Iter: 239 loss: 5.46536785e-06
Iter: 240 loss: 5.46010506e-06
Iter: 241 loss: 5.43946499e-06
Iter: 242 loss: 5.46802039e-06
Iter: 243 loss: 5.42900625e-06
Iter: 244 loss: 5.41468489e-06
Iter: 245 loss: 5.4860684e-06
Iter: 246 loss: 5.41238569e-06
Iter: 247 loss: 5.39570192e-06
Iter: 248 loss: 5.40978635e-06
Iter: 249 loss: 5.38604218e-06
Iter: 250 loss: 5.3940189e-06
Iter: 251 loss: 5.3787644e-06
Iter: 252 loss: 5.37489632e-06
Iter: 253 loss: 5.36369544e-06
Iter: 254 loss: 5.41107875e-06
Iter: 255 loss: 5.35938489e-06
Iter: 256 loss: 5.34445826e-06
Iter: 257 loss: 5.34757237e-06
Iter: 258 loss: 5.33355251e-06
Iter: 259 loss: 5.31691285e-06
Iter: 260 loss: 5.37728465e-06
Iter: 261 loss: 5.31282e-06
Iter: 262 loss: 5.29538147e-06
Iter: 263 loss: 5.38246877e-06
Iter: 264 loss: 5.29234148e-06
Iter: 265 loss: 5.2817195e-06
Iter: 266 loss: 5.32384684e-06
Iter: 267 loss: 5.27921475e-06
Iter: 268 loss: 5.26597432e-06
Iter: 269 loss: 5.25858559e-06
Iter: 270 loss: 5.25296946e-06
Iter: 271 loss: 5.24171901e-06
Iter: 272 loss: 5.24146071e-06
Iter: 273 loss: 5.2337291e-06
Iter: 274 loss: 5.23876633e-06
Iter: 275 loss: 5.22859773e-06
Iter: 276 loss: 5.2163864e-06
Iter: 277 loss: 5.22568098e-06
Iter: 278 loss: 5.20894628e-06
Iter: 279 loss: 5.19946252e-06
Iter: 280 loss: 5.23506606e-06
Iter: 281 loss: 5.19700461e-06
Iter: 282 loss: 5.18573597e-06
Iter: 283 loss: 5.24163534e-06
Iter: 284 loss: 5.18390971e-06
Iter: 285 loss: 5.17098306e-06
Iter: 286 loss: 5.27365046e-06
Iter: 287 loss: 5.17027092e-06
Iter: 288 loss: 5.16602722e-06
Iter: 289 loss: 5.15466627e-06
Iter: 290 loss: 5.22488654e-06
Iter: 291 loss: 5.15154215e-06
Iter: 292 loss: 5.13664691e-06
Iter: 293 loss: 5.19602054e-06
Iter: 294 loss: 5.13338728e-06
Iter: 295 loss: 5.12406905e-06
Iter: 296 loss: 5.13259965e-06
Iter: 297 loss: 5.11864664e-06
Iter: 298 loss: 5.10508653e-06
Iter: 299 loss: 5.17482e-06
Iter: 300 loss: 5.10287555e-06
Iter: 301 loss: 5.09431447e-06
Iter: 302 loss: 5.12735e-06
Iter: 303 loss: 5.09237088e-06
Iter: 304 loss: 5.08112225e-06
Iter: 305 loss: 5.06823653e-06
Iter: 306 loss: 5.06672268e-06
Iter: 307 loss: 5.05722801e-06
Iter: 308 loss: 5.05627759e-06
Iter: 309 loss: 5.04878972e-06
Iter: 310 loss: 5.04791205e-06
Iter: 311 loss: 5.04253057e-06
Iter: 312 loss: 5.03202227e-06
Iter: 313 loss: 5.08342418e-06
Iter: 314 loss: 5.03021647e-06
Iter: 315 loss: 5.02272496e-06
Iter: 316 loss: 5.0274175e-06
Iter: 317 loss: 5.01794239e-06
Iter: 318 loss: 5.01247905e-06
Iter: 319 loss: 5.01170325e-06
Iter: 320 loss: 5.00446367e-06
Iter: 321 loss: 4.99559064e-06
Iter: 322 loss: 4.99492216e-06
Iter: 323 loss: 4.98875579e-06
Iter: 324 loss: 4.98350846e-06
Iter: 325 loss: 4.98178451e-06
Iter: 326 loss: 4.97081965e-06
Iter: 327 loss: 4.9981536e-06
Iter: 328 loss: 4.96694793e-06
Iter: 329 loss: 4.95885206e-06
Iter: 330 loss: 4.96634357e-06
Iter: 331 loss: 4.95421455e-06
Iter: 332 loss: 4.94302731e-06
Iter: 333 loss: 5.02229614e-06
Iter: 334 loss: 4.94189226e-06
Iter: 335 loss: 4.9339269e-06
Iter: 336 loss: 4.93519565e-06
Iter: 337 loss: 4.92805293e-06
Iter: 338 loss: 4.91649462e-06
Iter: 339 loss: 4.95814766e-06
Iter: 340 loss: 4.91362698e-06
Iter: 341 loss: 4.90609546e-06
Iter: 342 loss: 4.96830398e-06
Iter: 343 loss: 4.9057594e-06
Iter: 344 loss: 4.89765762e-06
Iter: 345 loss: 4.8974448e-06
Iter: 346 loss: 4.89111653e-06
Iter: 347 loss: 4.88375e-06
Iter: 348 loss: 4.96650591e-06
Iter: 349 loss: 4.88363503e-06
Iter: 350 loss: 4.87855505e-06
Iter: 351 loss: 4.87634406e-06
Iter: 352 loss: 4.87360148e-06
Iter: 353 loss: 4.86838508e-06
Iter: 354 loss: 4.86762747e-06
Iter: 355 loss: 4.86310637e-06
Iter: 356 loss: 4.85603869e-06
Iter: 357 loss: 4.85586679e-06
Iter: 358 loss: 4.85137298e-06
Iter: 359 loss: 4.84200473e-06
Iter: 360 loss: 4.9942546e-06
Iter: 361 loss: 4.84164775e-06
Iter: 362 loss: 4.83231543e-06
Iter: 363 loss: 4.97506244e-06
Iter: 364 loss: 4.83232088e-06
Iter: 365 loss: 4.82712039e-06
Iter: 366 loss: 4.81822963e-06
Iter: 367 loss: 4.81826737e-06
Iter: 368 loss: 4.80659946e-06
Iter: 369 loss: 4.95299355e-06
Iter: 370 loss: 4.80650169e-06
Iter: 371 loss: 4.80069457e-06
Iter: 372 loss: 4.81112238e-06
Iter: 373 loss: 4.79807932e-06
Iter: 374 loss: 4.78976108e-06
Iter: 375 loss: 4.79503933e-06
Iter: 376 loss: 4.78460652e-06
Iter: 377 loss: 4.77861295e-06
Iter: 378 loss: 4.82497489e-06
Iter: 379 loss: 4.77802223e-06
Iter: 380 loss: 4.77164076e-06
Iter: 381 loss: 4.78128823e-06
Iter: 382 loss: 4.76853711e-06
Iter: 383 loss: 4.76232799e-06
Iter: 384 loss: 4.7990552e-06
Iter: 385 loss: 4.7617159e-06
Iter: 386 loss: 4.75760771e-06
Iter: 387 loss: 4.79912e-06
Iter: 388 loss: 4.75739444e-06
Iter: 389 loss: 4.75244678e-06
Iter: 390 loss: 4.7569356e-06
Iter: 391 loss: 4.74973695e-06
Iter: 392 loss: 4.74540138e-06
Iter: 393 loss: 4.73964792e-06
Iter: 394 loss: 4.73939053e-06
Iter: 395 loss: 4.73367254e-06
Iter: 396 loss: 4.73086857e-06
Iter: 397 loss: 4.72806914e-06
Iter: 398 loss: 4.71885051e-06
Iter: 399 loss: 4.80526114e-06
Iter: 400 loss: 4.71843168e-06
Iter: 401 loss: 4.71366684e-06
Iter: 402 loss: 4.72182546e-06
Iter: 403 loss: 4.71166277e-06
Iter: 404 loss: 4.70514624e-06
Iter: 405 loss: 4.70707892e-06
Iter: 406 loss: 4.70062787e-06
Iter: 407 loss: 4.69458973e-06
Iter: 408 loss: 4.71690237e-06
Iter: 409 loss: 4.69307724e-06
Iter: 410 loss: 4.68473081e-06
Iter: 411 loss: 4.69209681e-06
Iter: 412 loss: 4.67982545e-06
Iter: 413 loss: 4.67447217e-06
Iter: 414 loss: 4.754168e-06
Iter: 415 loss: 4.67443488e-06
Iter: 416 loss: 4.66910024e-06
Iter: 417 loss: 4.66183337e-06
Iter: 418 loss: 4.66154097e-06
Iter: 419 loss: 4.6572768e-06
Iter: 420 loss: 4.65626272e-06
Iter: 421 loss: 4.65443372e-06
Iter: 422 loss: 4.65431549e-06
Iter: 423 loss: 4.65297e-06
Iter: 424 loss: 4.6486e-06
Iter: 425 loss: 4.65734502e-06
Iter: 426 loss: 4.64582e-06
Iter: 427 loss: 4.63869947e-06
Iter: 428 loss: 4.65739413e-06
Iter: 429 loss: 4.63643391e-06
Iter: 430 loss: 4.63157403e-06
Iter: 431 loss: 4.6554751e-06
Iter: 432 loss: 4.63077731e-06
Iter: 433 loss: 4.62482103e-06
Iter: 434 loss: 4.62346543e-06
Iter: 435 loss: 4.61979744e-06
Iter: 436 loss: 4.61416585e-06
Iter: 437 loss: 4.64284494e-06
Iter: 438 loss: 4.61329091e-06
Iter: 439 loss: 4.60732463e-06
Iter: 440 loss: 4.6108371e-06
Iter: 441 loss: 4.60337742e-06
Iter: 442 loss: 4.59770445e-06
Iter: 443 loss: 4.65897074e-06
Iter: 444 loss: 4.59752209e-06
Iter: 445 loss: 4.59253442e-06
Iter: 446 loss: 4.5849556e-06
Iter: 447 loss: 4.58471e-06
Iter: 448 loss: 4.57806436e-06
Iter: 449 loss: 4.67558948e-06
Iter: 450 loss: 4.57797159e-06
Iter: 451 loss: 4.57229908e-06
Iter: 452 loss: 4.57775059e-06
Iter: 453 loss: 4.56907219e-06
Iter: 454 loss: 4.56593443e-06
Iter: 455 loss: 4.5651168e-06
Iter: 456 loss: 4.56197768e-06
Iter: 457 loss: 4.57147144e-06
Iter: 458 loss: 4.56115868e-06
Iter: 459 loss: 4.55937607e-06
Iter: 460 loss: 4.55467307e-06
Iter: 461 loss: 4.57644182e-06
Iter: 462 loss: 4.553086e-06
Iter: 463 loss: 4.54616e-06
Iter: 464 loss: 4.58296472e-06
Iter: 465 loss: 4.54535621e-06
Iter: 466 loss: 4.53981556e-06
Iter: 467 loss: 4.55472127e-06
Iter: 468 loss: 4.53806297e-06
Iter: 469 loss: 4.53127723e-06
Iter: 470 loss: 4.53647135e-06
Iter: 471 loss: 4.52700806e-06
Iter: 472 loss: 4.5216434e-06
Iter: 473 loss: 4.52795393e-06
Iter: 474 loss: 4.51875621e-06
Iter: 475 loss: 4.51178403e-06
Iter: 476 loss: 4.56198131e-06
Iter: 477 loss: 4.51113556e-06
Iter: 478 loss: 4.50675634e-06
Iter: 479 loss: 4.51425694e-06
Iter: 480 loss: 4.50461084e-06
Iter: 481 loss: 4.49825848e-06
Iter: 482 loss: 4.50663174e-06
Iter: 483 loss: 4.4948747e-06
Iter: 484 loss: 4.4891749e-06
Iter: 485 loss: 4.50634025e-06
Iter: 486 loss: 4.48731316e-06
Iter: 487 loss: 4.48224e-06
Iter: 488 loss: 4.5452739e-06
Iter: 489 loss: 4.4821918e-06
Iter: 490 loss: 4.47893854e-06
Iter: 491 loss: 4.47874845e-06
Iter: 492 loss: 4.47742786e-06
Iter: 493 loss: 4.4749022e-06
Iter: 494 loss: 4.47494813e-06
Iter: 495 loss: 4.4717317e-06
Iter: 496 loss: 4.46670083e-06
Iter: 497 loss: 4.46663171e-06
Iter: 498 loss: 4.46223203e-06
Iter: 499 loss: 4.52135828e-06
Iter: 500 loss: 4.46225567e-06
Iter: 501 loss: 4.45878095e-06
Iter: 502 loss: 4.45570367e-06
Iter: 503 loss: 4.45475462e-06
Iter: 504 loss: 4.44971647e-06
Iter: 505 loss: 4.49305298e-06
Iter: 506 loss: 4.44943635e-06
Iter: 507 loss: 4.44624584e-06
Iter: 508 loss: 4.44564e-06
Iter: 509 loss: 4.44341367e-06
Iter: 510 loss: 4.43758336e-06
Iter: 511 loss: 4.46798686e-06
Iter: 512 loss: 4.43661611e-06
Iter: 513 loss: 4.43357385e-06
Iter: 514 loss: 4.43432782e-06
Iter: 515 loss: 4.43145927e-06
Iter: 516 loss: 4.42552391e-06
Iter: 517 loss: 4.4271319e-06
Iter: 518 loss: 4.4212461e-06
Iter: 519 loss: 4.41597103e-06
Iter: 520 loss: 4.42367809e-06
Iter: 521 loss: 4.413444e-06
Iter: 522 loss: 4.41071779e-06
Iter: 523 loss: 4.40989243e-06
Iter: 524 loss: 4.4066046e-06
Iter: 525 loss: 4.43156387e-06
Iter: 526 loss: 4.40635813e-06
Iter: 527 loss: 4.40518852e-06
Iter: 528 loss: 4.40263466e-06
Iter: 529 loss: 4.44131592e-06
Iter: 530 loss: 4.40259555e-06
Iter: 531 loss: 4.39905625e-06
Iter: 532 loss: 4.40977283e-06
Iter: 533 loss: 4.39803625e-06
Iter: 534 loss: 4.39563973e-06
Iter: 535 loss: 4.39245468e-06
Iter: 536 loss: 4.39223231e-06
Iter: 537 loss: 4.38658344e-06
Iter: 538 loss: 4.41959673e-06
Iter: 539 loss: 4.38576762e-06
Iter: 540 loss: 4.3824748e-06
Iter: 541 loss: 4.39370615e-06
Iter: 542 loss: 4.38161624e-06
Iter: 543 loss: 4.37762128e-06
Iter: 544 loss: 4.38113966e-06
Iter: 545 loss: 4.37517565e-06
Iter: 546 loss: 4.37127164e-06
Iter: 547 loss: 4.38298457e-06
Iter: 548 loss: 4.37017206e-06
Iter: 549 loss: 4.36518167e-06
Iter: 550 loss: 4.3694472e-06
Iter: 551 loss: 4.36225946e-06
Iter: 552 loss: 4.3582113e-06
Iter: 553 loss: 4.4135063e-06
Iter: 554 loss: 4.35820584e-06
Iter: 555 loss: 4.35499942e-06
Iter: 556 loss: 4.3492837e-06
Iter: 557 loss: 4.48194851e-06
Iter: 558 loss: 4.3494e-06
Iter: 559 loss: 4.35023e-06
Iter: 560 loss: 4.34712638e-06
Iter: 561 loss: 4.34462618e-06
Iter: 562 loss: 4.34783669e-06
Iter: 563 loss: 4.34337e-06
Iter: 564 loss: 4.34098911e-06
Iter: 565 loss: 4.33621744e-06
Iter: 566 loss: 4.42368946e-06
Iter: 567 loss: 4.33617106e-06
Iter: 568 loss: 4.33228251e-06
Iter: 569 loss: 4.36384744e-06
Iter: 570 loss: 4.33203149e-06
Iter: 571 loss: 4.32899287e-06
Iter: 572 loss: 4.3339046e-06
Iter: 573 loss: 4.32767138e-06
Iter: 574 loss: 4.32407705e-06
Iter: 575 loss: 4.33697915e-06
Iter: 576 loss: 4.32323759e-06
Iter: 577 loss: 4.32047727e-06
Iter: 578 loss: 4.31616309e-06
Iter: 579 loss: 4.31606395e-06
Iter: 580 loss: 4.31085755e-06
Iter: 581 loss: 4.37337621e-06
Iter: 582 loss: 4.31077751e-06
Iter: 583 loss: 4.30837827e-06
Iter: 584 loss: 4.30609089e-06
Iter: 585 loss: 4.30538057e-06
Iter: 586 loss: 4.30079945e-06
Iter: 587 loss: 4.33388777e-06
Iter: 588 loss: 4.30030286e-06
Iter: 589 loss: 4.2969873e-06
Iter: 590 loss: 4.30480395e-06
Iter: 591 loss: 4.29595821e-06
Iter: 592 loss: 4.29129068e-06
Iter: 593 loss: 4.29264765e-06
Iter: 594 loss: 4.28804e-06
Iter: 595 loss: 4.29526699e-06
Iter: 596 loss: 4.28695057e-06
Iter: 597 loss: 4.28600106e-06
Iter: 598 loss: 4.28359863e-06
Iter: 599 loss: 4.29597912e-06
Iter: 600 loss: 4.28283784e-06
Iter: 601 loss: 4.27923896e-06
Iter: 602 loss: 4.28274461e-06
Iter: 603 loss: 4.27736404e-06
Iter: 604 loss: 4.27397936e-06
Iter: 605 loss: 4.2864167e-06
Iter: 606 loss: 4.27313e-06
Iter: 607 loss: 4.26923e-06
Iter: 608 loss: 4.27448504e-06
Iter: 609 loss: 4.26734823e-06
Iter: 610 loss: 4.26417728e-06
Iter: 611 loss: 4.29803731e-06
Iter: 612 loss: 4.26411634e-06
Iter: 613 loss: 4.2615643e-06
Iter: 614 loss: 4.25718281e-06
Iter: 615 loss: 4.25725784e-06
Iter: 616 loss: 4.25432972e-06
Iter: 617 loss: 4.25417829e-06
Iter: 618 loss: 4.25214239e-06
Iter: 619 loss: 4.24845302e-06
Iter: 620 loss: 4.32496108e-06
Iter: 621 loss: 4.24841346e-06
Iter: 622 loss: 4.24339487e-06
Iter: 623 loss: 4.28171825e-06
Iter: 624 loss: 4.24301561e-06
Iter: 625 loss: 4.23998381e-06
Iter: 626 loss: 4.23639312e-06
Iter: 627 loss: 4.23605707e-06
Iter: 628 loss: 4.23533493e-06
Iter: 629 loss: 4.23367919e-06
Iter: 630 loss: 4.23145229e-06
Iter: 631 loss: 4.23443726e-06
Iter: 632 loss: 4.23039e-06
Iter: 633 loss: 4.22877838e-06
Iter: 634 loss: 4.22719404e-06
Iter: 635 loss: 4.22688436e-06
Iter: 636 loss: 4.22363382e-06
Iter: 637 loss: 4.22306948e-06
Iter: 638 loss: 4.22087442e-06
Iter: 639 loss: 4.21804361e-06
Iter: 640 loss: 4.22564699e-06
Iter: 641 loss: 4.21683399e-06
Iter: 642 loss: 4.21278264e-06
Iter: 643 loss: 4.2223146e-06
Iter: 644 loss: 4.21132972e-06
Iter: 645 loss: 4.20829292e-06
Iter: 646 loss: 4.23994288e-06
Iter: 647 loss: 4.20827928e-06
Iter: 648 loss: 4.20564174e-06
Iter: 649 loss: 4.2032907e-06
Iter: 650 loss: 4.20272318e-06
Iter: 651 loss: 4.19917797e-06
Iter: 652 loss: 4.23540223e-06
Iter: 653 loss: 4.19912e-06
Iter: 654 loss: 4.19674188e-06
Iter: 655 loss: 4.19527623e-06
Iter: 656 loss: 4.19426215e-06
Iter: 657 loss: 4.19015578e-06
Iter: 658 loss: 4.20925608e-06
Iter: 659 loss: 4.18931859e-06
Iter: 660 loss: 4.18613536e-06
Iter: 661 loss: 4.19047228e-06
Iter: 662 loss: 4.18455102e-06
Iter: 663 loss: 4.18323816e-06
Iter: 664 loss: 4.18246964e-06
Iter: 665 loss: 4.18041827e-06
Iter: 666 loss: 4.17888396e-06
Iter: 667 loss: 4.17822e-06
Iter: 668 loss: 4.17665024e-06
Iter: 669 loss: 4.1740459e-06
Iter: 670 loss: 4.2414722e-06
Iter: 671 loss: 4.17394494e-06
Iter: 672 loss: 4.16974535e-06
Iter: 673 loss: 4.18734498e-06
Iter: 674 loss: 4.16890589e-06
Iter: 675 loss: 4.16603234e-06
Iter: 676 loss: 4.17823458e-06
Iter: 677 loss: 4.16542116e-06
Iter: 678 loss: 4.16289458e-06
Iter: 679 loss: 4.16405328e-06
Iter: 680 loss: 4.16107059e-06
Iter: 681 loss: 4.15831209e-06
Iter: 682 loss: 4.18718719e-06
Iter: 683 loss: 4.15825343e-06
Iter: 684 loss: 4.15573504e-06
Iter: 685 loss: 4.15124032e-06
Iter: 686 loss: 4.15127e-06
Iter: 687 loss: 4.14785518e-06
Iter: 688 loss: 4.14771102e-06
Iter: 689 loss: 4.14533542e-06
Iter: 690 loss: 4.14279293e-06
Iter: 691 loss: 4.1423209e-06
Iter: 692 loss: 4.13891803e-06
Iter: 693 loss: 4.17641922e-06
Iter: 694 loss: 4.13870384e-06
Iter: 695 loss: 4.13663383e-06
Iter: 696 loss: 4.13552425e-06
Iter: 697 loss: 4.13461657e-06
Iter: 698 loss: 4.13452108e-06
Iter: 699 loss: 4.13326325e-06
Iter: 700 loss: 4.13175258e-06
Iter: 701 loss: 4.13128782e-06
Iter: 702 loss: 4.13037969e-06
Iter: 703 loss: 4.12901e-06
Iter: 704 loss: 4.12620102e-06
Iter: 705 loss: 4.17226966e-06
Iter: 706 loss: 4.12608324e-06
Iter: 707 loss: 4.1223484e-06
Iter: 708 loss: 4.14610395e-06
Iter: 709 loss: 4.12192639e-06
Iter: 710 loss: 4.1197045e-06
Iter: 711 loss: 4.1295043e-06
Iter: 712 loss: 4.1192834e-06
Iter: 713 loss: 4.11700785e-06
Iter: 714 loss: 4.11995e-06
Iter: 715 loss: 4.11587189e-06
Iter: 716 loss: 4.11358087e-06
Iter: 717 loss: 4.12559075e-06
Iter: 718 loss: 4.11320298e-06
Iter: 719 loss: 4.11078281e-06
Iter: 720 loss: 4.11162182e-06
Iter: 721 loss: 4.10905e-06
Iter: 722 loss: 4.10594657e-06
Iter: 723 loss: 4.1145131e-06
Iter: 724 loss: 4.10487064e-06
Iter: 725 loss: 4.1017438e-06
Iter: 726 loss: 4.10849043e-06
Iter: 727 loss: 4.10043913e-06
Iter: 728 loss: 4.09786935e-06
Iter: 729 loss: 4.11939891e-06
Iter: 730 loss: 4.09769473e-06
Iter: 731 loss: 4.0955033e-06
Iter: 732 loss: 4.09351378e-06
Iter: 733 loss: 4.09308768e-06
Iter: 734 loss: 4.09394261e-06
Iter: 735 loss: 4.09166341e-06
Iter: 736 loss: 4.09029872e-06
Iter: 737 loss: 4.08871574e-06
Iter: 738 loss: 4.08857068e-06
Iter: 739 loss: 4.08717551e-06
Iter: 740 loss: 4.08444475e-06
Iter: 741 loss: 4.14063197e-06
Iter: 742 loss: 4.08446067e-06
Iter: 743 loss: 4.08074629e-06
Iter: 744 loss: 4.1052981e-06
Iter: 745 loss: 4.0805503e-06
Iter: 746 loss: 4.0783134e-06
Iter: 747 loss: 4.08528058e-06
Iter: 748 loss: 4.07781545e-06
Iter: 749 loss: 4.07527159e-06
Iter: 750 loss: 4.07889729e-06
Iter: 751 loss: 4.07392599e-06
Iter: 752 loss: 4.07173047e-06
Iter: 753 loss: 4.08437836e-06
Iter: 754 loss: 4.07152493e-06
Iter: 755 loss: 4.06943764e-06
Iter: 756 loss: 4.06931213e-06
Iter: 757 loss: 4.06761501e-06
Iter: 758 loss: 4.06489471e-06
Iter: 759 loss: 4.08348023e-06
Iter: 760 loss: 4.06471463e-06
Iter: 761 loss: 4.06256686e-06
Iter: 762 loss: 4.06155596e-06
Iter: 763 loss: 4.06048412e-06
Iter: 764 loss: 4.057566e-06
Iter: 765 loss: 4.08147025e-06
Iter: 766 loss: 4.05748324e-06
Iter: 767 loss: 4.05559831e-06
Iter: 768 loss: 4.06825529e-06
Iter: 769 loss: 4.0553382e-06
Iter: 770 loss: 4.05324226e-06
Iter: 771 loss: 4.06981599e-06
Iter: 772 loss: 4.05318406e-06
Iter: 773 loss: 4.05214087e-06
Iter: 774 loss: 4.04972616e-06
Iter: 775 loss: 4.08525693e-06
Iter: 776 loss: 4.0496061e-06
Iter: 777 loss: 4.04772e-06
Iter: 778 loss: 4.05020182e-06
Iter: 779 loss: 4.04658704e-06
Iter: 780 loss: 4.04356e-06
Iter: 781 loss: 4.04857292e-06
Iter: 782 loss: 4.04215234e-06
Iter: 783 loss: 4.03990089e-06
Iter: 784 loss: 4.05312903e-06
Iter: 785 loss: 4.03950298e-06
Iter: 786 loss: 4.03705326e-06
Iter: 787 loss: 4.04116054e-06
Iter: 788 loss: 4.03603053e-06
Iter: 789 loss: 4.03413151e-06
Iter: 790 loss: 4.04750381e-06
Iter: 791 loss: 4.03403192e-06
Iter: 792 loss: 4.03253307e-06
Iter: 793 loss: 4.03140803e-06
Iter: 794 loss: 4.03073227e-06
Iter: 795 loss: 4.02816522e-06
Iter: 796 loss: 4.04314505e-06
Iter: 797 loss: 4.0277755e-06
Iter: 798 loss: 4.02599562e-06
Iter: 799 loss: 4.02624164e-06
Iter: 800 loss: 4.02472051e-06
Iter: 801 loss: 4.02234855e-06
Iter: 802 loss: 4.03851118e-06
Iter: 803 loss: 4.02205205e-06
Iter: 804 loss: 4.02132582e-06
Iter: 805 loss: 4.02110618e-06
Iter: 806 loss: 4.02011619e-06
Iter: 807 loss: 4.01858779e-06
Iter: 808 loss: 4.01848729e-06
Iter: 809 loss: 4.01693023e-06
Iter: 810 loss: 4.01517173e-06
Iter: 811 loss: 4.01483885e-06
Iter: 812 loss: 4.01283069e-06
Iter: 813 loss: 4.02345e-06
Iter: 814 loss: 4.01258194e-06
Iter: 815 loss: 4.01042416e-06
Iter: 816 loss: 4.01122952e-06
Iter: 817 loss: 4.00889712e-06
Iter: 818 loss: 4.00675208e-06
Iter: 819 loss: 4.02183741e-06
Iter: 820 loss: 4.00646604e-06
Iter: 821 loss: 4.00419049e-06
Iter: 822 loss: 4.00607087e-06
Iter: 823 loss: 4.00296267e-06
Iter: 824 loss: 4.00088629e-06
Iter: 825 loss: 4.0138134e-06
Iter: 826 loss: 4.00063936e-06
Iter: 827 loss: 3.99847249e-06
Iter: 828 loss: 3.99748433e-06
Iter: 829 loss: 3.99639157e-06
Iter: 830 loss: 3.99396185e-06
Iter: 831 loss: 4.02460864e-06
Iter: 832 loss: 3.99390501e-06
Iter: 833 loss: 3.99253531e-06
Iter: 834 loss: 3.99193914e-06
Iter: 835 loss: 3.9911879e-06
Iter: 836 loss: 3.98867178e-06
Iter: 837 loss: 3.99768169e-06
Iter: 838 loss: 3.98812e-06
Iter: 839 loss: 3.98946759e-06
Iter: 840 loss: 3.9875e-06
Iter: 841 loss: 3.98698512e-06
Iter: 842 loss: 3.98514749e-06
Iter: 843 loss: 3.98467273e-06
Iter: 844 loss: 3.98304201e-06
Iter: 845 loss: 3.98047041e-06
Iter: 846 loss: 4.00705903e-06
Iter: 847 loss: 3.9803358e-06
Iter: 848 loss: 3.97873055e-06
Iter: 849 loss: 3.97914891e-06
Iter: 850 loss: 3.97745498e-06
Iter: 851 loss: 3.9748e-06
Iter: 852 loss: 3.98233442e-06
Iter: 853 loss: 3.9740039e-06
Iter: 854 loss: 3.97203e-06
Iter: 855 loss: 3.97528402e-06
Iter: 856 loss: 3.97114854e-06
Iter: 857 loss: 3.96862e-06
Iter: 858 loss: 3.97897611e-06
Iter: 859 loss: 3.9679926e-06
Iter: 860 loss: 3.96596079e-06
Iter: 861 loss: 3.97198301e-06
Iter: 862 loss: 3.96529958e-06
Iter: 863 loss: 3.96308133e-06
Iter: 864 loss: 3.97172607e-06
Iter: 865 loss: 3.96261e-06
Iter: 866 loss: 3.96095902e-06
Iter: 867 loss: 3.96607356e-06
Iter: 868 loss: 3.96049381e-06
Iter: 869 loss: 3.95883126e-06
Iter: 870 loss: 3.95875759e-06
Iter: 871 loss: 3.95749612e-06
Iter: 872 loss: 3.95572715e-06
Iter: 873 loss: 3.95575898e-06
Iter: 874 loss: 3.9540937e-06
Iter: 875 loss: 3.96619089e-06
Iter: 876 loss: 3.95402822e-06
Iter: 877 loss: 3.95316829e-06
Iter: 878 loss: 3.95048846e-06
Iter: 879 loss: 3.95620737e-06
Iter: 880 loss: 3.94885092e-06
Iter: 881 loss: 3.94552444e-06
Iter: 882 loss: 3.99193232e-06
Iter: 883 loss: 3.94553217e-06
Iter: 884 loss: 3.94367862e-06
Iter: 885 loss: 3.94224844e-06
Iter: 886 loss: 3.94171e-06
Iter: 887 loss: 3.9394713e-06
Iter: 888 loss: 3.97082067e-06
Iter: 889 loss: 3.93948721e-06
Iter: 890 loss: 3.93775099e-06
Iter: 891 loss: 3.93610208e-06
Iter: 892 loss: 3.93573282e-06
Iter: 893 loss: 3.9322058e-06
Iter: 894 loss: 3.94647031e-06
Iter: 895 loss: 3.93150367e-06
Iter: 896 loss: 3.92905167e-06
Iter: 897 loss: 3.93139771e-06
Iter: 898 loss: 3.92783977e-06
Iter: 899 loss: 3.924883e-06
Iter: 900 loss: 3.94004064e-06
Iter: 901 loss: 3.92456241e-06
Iter: 902 loss: 3.92288575e-06
Iter: 903 loss: 3.9313768e-06
Iter: 904 loss: 3.92265656e-06
Iter: 905 loss: 3.92101629e-06
Iter: 906 loss: 3.91941376e-06
Iter: 907 loss: 3.9191782e-06
Iter: 908 loss: 3.91799313e-06
Iter: 909 loss: 3.91772e-06
Iter: 910 loss: 3.91669437e-06
Iter: 911 loss: 3.92286256e-06
Iter: 912 loss: 3.91650519e-06
Iter: 913 loss: 3.91553476e-06
Iter: 914 loss: 3.91352751e-06
Iter: 915 loss: 3.92904303e-06
Iter: 916 loss: 3.91308777e-06
Iter: 917 loss: 3.91109643e-06
Iter: 918 loss: 3.92988932e-06
Iter: 919 loss: 3.9110746e-06
Iter: 920 loss: 3.90943887e-06
Iter: 921 loss: 3.90773039e-06
Iter: 922 loss: 3.90734294e-06
Iter: 923 loss: 3.90508e-06
Iter: 924 loss: 3.92694756e-06
Iter: 925 loss: 3.90486093e-06
Iter: 926 loss: 3.90306104e-06
Iter: 927 loss: 3.90169816e-06
Iter: 928 loss: 3.90101241e-06
Iter: 929 loss: 3.89849674e-06
Iter: 930 loss: 3.93340315e-06
Iter: 931 loss: 3.89849629e-06
Iter: 932 loss: 3.89692468e-06
Iter: 933 loss: 3.89496927e-06
Iter: 934 loss: 3.89471e-06
Iter: 935 loss: 3.8921944e-06
Iter: 936 loss: 3.92277889e-06
Iter: 937 loss: 3.89220395e-06
Iter: 938 loss: 3.89074421e-06
Iter: 939 loss: 3.89194e-06
Iter: 940 loss: 3.88982653e-06
Iter: 941 loss: 3.88819717e-06
Iter: 942 loss: 3.89661409e-06
Iter: 943 loss: 3.88794069e-06
Iter: 944 loss: 3.88722174e-06
Iter: 945 loss: 3.88714443e-06
Iter: 946 loss: 3.88653643e-06
Iter: 947 loss: 3.88545232e-06
Iter: 948 loss: 3.9095421e-06
Iter: 949 loss: 3.8854314e-06
Iter: 950 loss: 3.88420358e-06
Iter: 951 loss: 3.88295939e-06
Iter: 952 loss: 3.88248191e-06
Iter: 953 loss: 3.88063427e-06
Iter: 954 loss: 3.90196419e-06
Iter: 955 loss: 3.88066383e-06
Iter: 956 loss: 3.87909768e-06
Iter: 957 loss: 3.87656974e-06
Iter: 958 loss: 3.9400993e-06
Iter: 959 loss: 3.87659e-06
Iter: 960 loss: 3.87421733e-06
Iter: 961 loss: 3.87409546e-06
Iter: 962 loss: 3.87257933e-06
Iter: 963 loss: 3.87195405e-06
Iter: 964 loss: 3.8711396e-06
Iter: 965 loss: 3.86877218e-06
Iter: 966 loss: 3.88074932e-06
Iter: 967 loss: 3.8685007e-06
Iter: 968 loss: 3.86680495e-06
Iter: 969 loss: 3.86846114e-06
Iter: 970 loss: 3.86598731e-06
Iter: 971 loss: 3.86352167e-06
Iter: 972 loss: 3.87371892e-06
Iter: 973 loss: 3.86309966e-06
Iter: 974 loss: 3.86138436e-06
Iter: 975 loss: 3.86153442e-06
Iter: 976 loss: 3.8602011e-06
Iter: 977 loss: 3.85930161e-06
Iter: 978 loss: 3.8589078e-06
Iter: 979 loss: 3.85778458e-06
Iter: 980 loss: 3.85739577e-06
Iter: 981 loss: 3.85673593e-06
Iter: 982 loss: 3.85583462e-06
Iter: 983 loss: 3.85410658e-06
Iter: 984 loss: 3.85414296e-06
Iter: 985 loss: 3.85177418e-06
Iter: 986 loss: 3.86389365e-06
Iter: 987 loss: 3.85142675e-06
Iter: 988 loss: 3.8496828e-06
Iter: 989 loss: 3.86068314e-06
Iter: 990 loss: 3.84949772e-06
Iter: 991 loss: 3.84788837e-06
Iter: 992 loss: 3.84654595e-06
Iter: 993 loss: 3.84603482e-06
Iter: 994 loss: 3.8443759e-06
Iter: 995 loss: 3.87158343e-06
Iter: 996 loss: 3.84436e-06
Iter: 997 loss: 3.84296072e-06
Iter: 998 loss: 3.84105078e-06
Iter: 999 loss: 3.84103669e-06
Iter: 1000 loss: 3.83868837e-06
Iter: 1001 loss: 3.86120337e-06
Iter: 1002 loss: 3.83856377e-06
Iter: 1003 loss: 3.83728866e-06
Iter: 1004 loss: 3.83575616e-06
Iter: 1005 loss: 3.8353478e-06
Iter: 1006 loss: 3.83331144e-06
Iter: 1007 loss: 3.86591637e-06
Iter: 1008 loss: 3.83330735e-06
Iter: 1009 loss: 3.83211727e-06
Iter: 1010 loss: 3.83893e-06
Iter: 1011 loss: 3.83190672e-06
Iter: 1012 loss: 3.83019596e-06
Iter: 1013 loss: 3.83752831e-06
Iter: 1014 loss: 3.82987037e-06
Iter: 1015 loss: 3.82904454e-06
Iter: 1016 loss: 3.82753524e-06
Iter: 1017 loss: 3.86262309e-06
Iter: 1018 loss: 3.82743656e-06
Iter: 1019 loss: 3.82577264e-06
Iter: 1020 loss: 3.82551252e-06
Iter: 1021 loss: 3.82439157e-06
Iter: 1022 loss: 3.82244252e-06
Iter: 1023 loss: 3.84981e-06
Iter: 1024 loss: 3.82242342e-06
Iter: 1025 loss: 3.82122789e-06
Iter: 1026 loss: 3.82259805e-06
Iter: 1027 loss: 3.82053895e-06
Iter: 1028 loss: 3.8185226e-06
Iter: 1029 loss: 3.81805194e-06
Iter: 1030 loss: 3.81685686e-06
Iter: 1031 loss: 3.81464179e-06
Iter: 1032 loss: 3.83493852e-06
Iter: 1033 loss: 3.81447717e-06
Iter: 1034 loss: 3.81290783e-06
Iter: 1035 loss: 3.81209566e-06
Iter: 1036 loss: 3.81116956e-06
Iter: 1037 loss: 3.8095518e-06
Iter: 1038 loss: 3.83035513e-06
Iter: 1039 loss: 3.80941128e-06
Iter: 1040 loss: 3.80811298e-06
Iter: 1041 loss: 3.80797792e-06
Iter: 1042 loss: 3.806824e-06
Iter: 1043 loss: 3.8051046e-06
Iter: 1044 loss: 3.81908103e-06
Iter: 1045 loss: 3.80497158e-06
Iter: 1046 loss: 3.80469419e-06
Iter: 1047 loss: 3.80442634e-06
Iter: 1048 loss: 3.8040057e-06
Iter: 1049 loss: 3.80265919e-06
Iter: 1050 loss: 3.80351344e-06
Iter: 1051 loss: 3.80159668e-06
Iter: 1052 loss: 3.79904554e-06
Iter: 1053 loss: 3.8104256e-06
Iter: 1054 loss: 3.79849484e-06
Iter: 1055 loss: 3.79704124e-06
Iter: 1056 loss: 3.79914218e-06
Iter: 1057 loss: 3.79626454e-06
Iter: 1058 loss: 3.7939767e-06
Iter: 1059 loss: 3.79745e-06
Iter: 1060 loss: 3.79286257e-06
Iter: 1061 loss: 3.79095445e-06
Iter: 1062 loss: 3.80580673e-06
Iter: 1063 loss: 3.79085031e-06
Iter: 1064 loss: 3.78890149e-06
Iter: 1065 loss: 3.79033168e-06
Iter: 1066 loss: 3.78779805e-06
Iter: 1067 loss: 3.78605637e-06
Iter: 1068 loss: 3.79181097e-06
Iter: 1069 loss: 3.78555114e-06
Iter: 1070 loss: 3.78360346e-06
Iter: 1071 loss: 3.78311711e-06
Iter: 1072 loss: 3.7820023e-06
Iter: 1073 loss: 3.78035588e-06
Iter: 1074 loss: 3.80441702e-06
Iter: 1075 loss: 3.78031814e-06
Iter: 1076 loss: 3.7785494e-06
Iter: 1077 loss: 3.77816309e-06
Iter: 1078 loss: 3.77705874e-06
Iter: 1079 loss: 3.7791674e-06
Iter: 1080 loss: 3.77648735e-06
Iter: 1081 loss: 3.77585957e-06
Iter: 1082 loss: 3.7745649e-06
Iter: 1083 loss: 3.78878735e-06
Iter: 1084 loss: 3.77438778e-06
Iter: 1085 loss: 3.77286028e-06
Iter: 1086 loss: 3.77259425e-06
Iter: 1087 loss: 3.7715879e-06
Iter: 1088 loss: 3.76969524e-06
Iter: 1089 loss: 3.77984e-06
Iter: 1090 loss: 3.76945172e-06
Iter: 1091 loss: 3.76767343e-06
Iter: 1092 loss: 3.76559592e-06
Iter: 1093 loss: 3.76541038e-06
Iter: 1094 loss: 3.76299568e-06
Iter: 1095 loss: 3.76300841e-06
Iter: 1096 loss: 3.7616453e-06
Iter: 1097 loss: 3.76582739e-06
Iter: 1098 loss: 3.76127105e-06
Iter: 1099 loss: 3.75955346e-06
Iter: 1100 loss: 3.76012258e-06
Iter: 1101 loss: 3.75824561e-06
Iter: 1102 loss: 3.75650961e-06
Iter: 1103 loss: 3.76382218e-06
Iter: 1104 loss: 3.75611717e-06
Iter: 1105 loss: 3.75426839e-06
Iter: 1106 loss: 3.75452146e-06
Iter: 1107 loss: 3.7528398e-06
Iter: 1108 loss: 3.750622e-06
Iter: 1109 loss: 3.76900039e-06
Iter: 1110 loss: 3.75058335e-06
Iter: 1111 loss: 3.74886395e-06
Iter: 1112 loss: 3.7525e-06
Iter: 1113 loss: 3.74825959e-06
Iter: 1114 loss: 3.74706178e-06
Iter: 1115 loss: 3.74693013e-06
Iter: 1116 loss: 3.74643059e-06
Iter: 1117 loss: 3.74542105e-06
Iter: 1118 loss: 3.7516686e-06
Iter: 1119 loss: 3.74507295e-06
Iter: 1120 loss: 3.74333831e-06
Iter: 1121 loss: 3.74378192e-06
Iter: 1122 loss: 3.74203887e-06
Iter: 1123 loss: 3.74017122e-06
Iter: 1124 loss: 3.75195305e-06
Iter: 1125 loss: 3.73997796e-06
Iter: 1126 loss: 3.73806142e-06
Iter: 1127 loss: 3.73770126e-06
Iter: 1128 loss: 3.73635635e-06
Iter: 1129 loss: 3.73456851e-06
Iter: 1130 loss: 3.76323192e-06
Iter: 1131 loss: 3.73457942e-06
Iter: 1132 loss: 3.73298121e-06
Iter: 1133 loss: 3.73306943e-06
Iter: 1134 loss: 3.73173e-06
Iter: 1135 loss: 3.72973454e-06
Iter: 1136 loss: 3.74146839e-06
Iter: 1137 loss: 3.72953764e-06
Iter: 1138 loss: 3.72827594e-06
Iter: 1139 loss: 3.73124931e-06
Iter: 1140 loss: 3.72788827e-06
Iter: 1141 loss: 3.7260188e-06
Iter: 1142 loss: 3.72442491e-06
Iter: 1143 loss: 3.72398949e-06
Iter: 1144 loss: 3.72188333e-06
Iter: 1145 loss: 3.73328203e-06
Iter: 1146 loss: 3.72143836e-06
Iter: 1147 loss: 3.7209079e-06
Iter: 1148 loss: 3.72040358e-06
Iter: 1149 loss: 3.71928149e-06
Iter: 1150 loss: 3.71727333e-06
Iter: 1151 loss: 3.76481921e-06
Iter: 1152 loss: 3.71720762e-06
Iter: 1153 loss: 3.71616215e-06
Iter: 1154 loss: 3.71576e-06
Iter: 1155 loss: 3.71516398e-06
Iter: 1156 loss: 3.71304168e-06
Iter: 1157 loss: 3.71477654e-06
Iter: 1158 loss: 3.71189662e-06
Iter: 1159 loss: 3.70984e-06
Iter: 1160 loss: 3.72042041e-06
Iter: 1161 loss: 3.7095781e-06
Iter: 1162 loss: 3.70764678e-06
Iter: 1163 loss: 3.70958287e-06
Iter: 1164 loss: 3.70642e-06
Iter: 1165 loss: 3.70499333e-06
Iter: 1166 loss: 3.72161321e-06
Iter: 1167 loss: 3.70498174e-06
Iter: 1168 loss: 3.70366956e-06
Iter: 1169 loss: 3.70437056e-06
Iter: 1170 loss: 3.7027944e-06
Iter: 1171 loss: 3.70136354e-06
Iter: 1172 loss: 3.71076067e-06
Iter: 1173 loss: 3.70120824e-06
Iter: 1174 loss: 3.69991358e-06
Iter: 1175 loss: 3.69909731e-06
Iter: 1176 loss: 3.69859094e-06
Iter: 1177 loss: 3.69649752e-06
Iter: 1178 loss: 3.70424414e-06
Iter: 1179 loss: 3.69590202e-06
Iter: 1180 loss: 3.69449117e-06
Iter: 1181 loss: 3.70329099e-06
Iter: 1182 loss: 3.69430018e-06
Iter: 1183 loss: 3.69254462e-06
Iter: 1184 loss: 3.70606494e-06
Iter: 1185 loss: 3.69238819e-06
Iter: 1186 loss: 3.69167742e-06
Iter: 1187 loss: 3.69077907e-06
Iter: 1188 loss: 3.69059762e-06
Iter: 1189 loss: 3.68941164e-06
Iter: 1190 loss: 3.68769633e-06
Iter: 1191 loss: 3.68766518e-06
Iter: 1192 loss: 3.68543533e-06
Iter: 1193 loss: 3.70540693e-06
Iter: 1194 loss: 3.68525843e-06
Iter: 1195 loss: 3.68393603e-06
Iter: 1196 loss: 3.68192877e-06
Iter: 1197 loss: 3.68192923e-06
Iter: 1198 loss: 3.67984421e-06
Iter: 1199 loss: 3.67984239e-06
Iter: 1200 loss: 3.67857797e-06
Iter: 1201 loss: 3.67844359e-06
Iter: 1202 loss: 3.67751318e-06
Iter: 1203 loss: 3.6757765e-06
Iter: 1204 loss: 3.69033978e-06
Iter: 1205 loss: 3.67558096e-06
Iter: 1206 loss: 3.67418988e-06
Iter: 1207 loss: 3.67842904e-06
Iter: 1208 loss: 3.673827e-06
Iter: 1209 loss: 3.67246639e-06
Iter: 1210 loss: 3.67158259e-06
Iter: 1211 loss: 3.67094526e-06
Iter: 1212 loss: 3.66958216e-06
Iter: 1213 loss: 3.67487974e-06
Iter: 1214 loss: 3.66930817e-06
Iter: 1215 loss: 3.6679005e-06
Iter: 1216 loss: 3.67744269e-06
Iter: 1217 loss: 3.66775203e-06
Iter: 1218 loss: 3.66691e-06
Iter: 1219 loss: 3.66694894e-06
Iter: 1220 loss: 3.66656e-06
Iter: 1221 loss: 3.66564655e-06
Iter: 1222 loss: 3.67029e-06
Iter: 1223 loss: 3.66528911e-06
Iter: 1224 loss: 3.6639567e-06
Iter: 1225 loss: 3.66490167e-06
Iter: 1226 loss: 3.66301583e-06
Iter: 1227 loss: 3.66139e-06
Iter: 1228 loss: 3.66571498e-06
Iter: 1229 loss: 3.66093e-06
Iter: 1230 loss: 3.65901224e-06
Iter: 1231 loss: 3.66335303e-06
Iter: 1232 loss: 3.65846699e-06
Iter: 1233 loss: 3.65701544e-06
Iter: 1234 loss: 3.67002394e-06
Iter: 1235 loss: 3.65704204e-06
Iter: 1236 loss: 3.65536334e-06
Iter: 1237 loss: 3.65238429e-06
Iter: 1238 loss: 3.715611e-06
Iter: 1239 loss: 3.65246069e-06
Iter: 1240 loss: 3.65066558e-06
Iter: 1241 loss: 3.65064125e-06
Iter: 1242 loss: 3.64905804e-06
Iter: 1243 loss: 3.65146934e-06
Iter: 1244 loss: 3.64827702e-06
Iter: 1245 loss: 3.64698144e-06
Iter: 1246 loss: 3.65267215e-06
Iter: 1247 loss: 3.64669631e-06
Iter: 1248 loss: 3.64522521e-06
Iter: 1249 loss: 3.64622474e-06
Iter: 1250 loss: 3.64411335e-06
Iter: 1251 loss: 3.64451284e-06
Iter: 1252 loss: 3.6435481e-06
Iter: 1253 loss: 3.64290963e-06
Iter: 1254 loss: 3.64167181e-06
Iter: 1255 loss: 3.64169409e-06
Iter: 1256 loss: 3.64028415e-06
Iter: 1257 loss: 3.64040852e-06
Iter: 1258 loss: 3.63924823e-06
Iter: 1259 loss: 3.63790423e-06
Iter: 1260 loss: 3.64424045e-06
Iter: 1261 loss: 3.63774302e-06
Iter: 1262 loss: 3.63653953e-06
Iter: 1263 loss: 3.63591062e-06
Iter: 1264 loss: 3.63542335e-06
Iter: 1265 loss: 3.63392974e-06
Iter: 1266 loss: 3.64950711e-06
Iter: 1267 loss: 3.63393292e-06
Iter: 1268 loss: 3.63279969e-06
Iter: 1269 loss: 3.63145659e-06
Iter: 1270 loss: 3.63140271e-06
Iter: 1271 loss: 3.62984952e-06
Iter: 1272 loss: 3.64776452e-06
Iter: 1273 loss: 3.62985952e-06
Iter: 1274 loss: 3.62851279e-06
Iter: 1275 loss: 3.62872538e-06
Iter: 1276 loss: 3.62749506e-06
Iter: 1277 loss: 3.62542255e-06
Iter: 1278 loss: 3.63514755e-06
Iter: 1279 loss: 3.62517403e-06
Iter: 1280 loss: 3.62356582e-06
Iter: 1281 loss: 3.62359879e-06
Iter: 1282 loss: 3.62253058e-06
Iter: 1283 loss: 3.62048263e-06
Iter: 1284 loss: 3.63290337e-06
Iter: 1285 loss: 3.62039373e-06
Iter: 1286 loss: 3.61931734e-06
Iter: 1287 loss: 3.63056597e-06
Iter: 1288 loss: 3.61921775e-06
Iter: 1289 loss: 3.61789034e-06
Iter: 1290 loss: 3.61905268e-06
Iter: 1291 loss: 3.61706043e-06
Iter: 1292 loss: 3.61606703e-06
Iter: 1293 loss: 3.61525099e-06
Iter: 1294 loss: 3.61496609e-06
Iter: 1295 loss: 3.61380353e-06
Iter: 1296 loss: 3.61782986e-06
Iter: 1297 loss: 3.61358207e-06
Iter: 1298 loss: 3.61230377e-06
Iter: 1299 loss: 3.613643e-06
Iter: 1300 loss: 3.61169077e-06
Iter: 1301 loss: 3.61045113e-06
Iter: 1302 loss: 3.61881848e-06
Iter: 1303 loss: 3.6102897e-06
Iter: 1304 loss: 3.60911554e-06
Iter: 1305 loss: 3.60721629e-06
Iter: 1306 loss: 3.60725517e-06
Iter: 1307 loss: 3.60596459e-06
Iter: 1308 loss: 3.60591093e-06
Iter: 1309 loss: 3.60471404e-06
Iter: 1310 loss: 3.60376544e-06
Iter: 1311 loss: 3.60335844e-06
Iter: 1312 loss: 3.60161516e-06
Iter: 1313 loss: 3.61014554e-06
Iter: 1314 loss: 3.60129707e-06
Iter: 1315 loss: 3.59966589e-06
Iter: 1316 loss: 3.60243962e-06
Iter: 1317 loss: 3.59900423e-06
Iter: 1318 loss: 3.59765136e-06
Iter: 1319 loss: 3.61099706e-06
Iter: 1320 loss: 3.59743763e-06
Iter: 1321 loss: 3.59684077e-06
Iter: 1322 loss: 3.60841432e-06
Iter: 1323 loss: 3.59682144e-06
Iter: 1324 loss: 3.59588194e-06
Iter: 1325 loss: 3.59491514e-06
Iter: 1326 loss: 3.59471483e-06
Iter: 1327 loss: 3.59385308e-06
Iter: 1328 loss: 3.59341584e-06
Iter: 1329 loss: 3.59295973e-06
Iter: 1330 loss: 3.59162709e-06
Iter: 1331 loss: 3.58982561e-06
Iter: 1332 loss: 3.58976149e-06
Iter: 1333 loss: 3.58791658e-06
Iter: 1334 loss: 3.58798729e-06
Iter: 1335 loss: 3.58643e-06
Iter: 1336 loss: 3.58897e-06
Iter: 1337 loss: 3.58570696e-06
Iter: 1338 loss: 3.58402804e-06
Iter: 1339 loss: 3.58995203e-06
Iter: 1340 loss: 3.58354464e-06
Iter: 1341 loss: 3.5823623e-06
Iter: 1342 loss: 3.58130114e-06
Iter: 1343 loss: 3.58088255e-06
Iter: 1344 loss: 3.57907084e-06
Iter: 1345 loss: 3.595203e-06
Iter: 1346 loss: 3.57889485e-06
Iter: 1347 loss: 3.57738872e-06
Iter: 1348 loss: 3.57835279e-06
Iter: 1349 loss: 3.57628096e-06
Iter: 1350 loss: 3.57412318e-06
Iter: 1351 loss: 3.5821256e-06
Iter: 1352 loss: 3.57358886e-06
Iter: 1353 loss: 3.57199315e-06
Iter: 1354 loss: 3.57520025e-06
Iter: 1355 loss: 3.57133467e-06
Iter: 1356 loss: 3.56967871e-06
Iter: 1357 loss: 3.5800872e-06
Iter: 1358 loss: 3.56941814e-06
Iter: 1359 loss: 3.56928649e-06
Iter: 1360 loss: 3.56862211e-06
Iter: 1361 loss: 3.56841315e-06
Iter: 1362 loss: 3.56747591e-06
Iter: 1363 loss: 3.56981786e-06
Iter: 1364 loss: 3.56680448e-06
Iter: 1365 loss: 3.56515329e-06
Iter: 1366 loss: 3.56846135e-06
Iter: 1367 loss: 3.56443297e-06
Iter: 1368 loss: 3.56307828e-06
Iter: 1369 loss: 3.56367627e-06
Iter: 1370 loss: 3.56209466e-06
Iter: 1371 loss: 3.5601513e-06
Iter: 1372 loss: 3.57684121e-06
Iter: 1373 loss: 3.56007786e-06
Iter: 1374 loss: 3.55898737e-06
Iter: 1375 loss: 3.56299e-06
Iter: 1376 loss: 3.55867314e-06
Iter: 1377 loss: 3.55729503e-06
Iter: 1378 loss: 3.55770908e-06
Iter: 1379 loss: 3.55638736e-06
Iter: 1380 loss: 3.55484053e-06
Iter: 1381 loss: 3.56644932e-06
Iter: 1382 loss: 3.55473821e-06
Iter: 1383 loss: 3.55362681e-06
Iter: 1384 loss: 3.55167231e-06
Iter: 1385 loss: 3.55157681e-06
Iter: 1386 loss: 3.5491762e-06
Iter: 1387 loss: 3.57067529e-06
Iter: 1388 loss: 3.54914096e-06
Iter: 1389 loss: 3.54777922e-06
Iter: 1390 loss: 3.54777808e-06
Iter: 1391 loss: 3.5467192e-06
Iter: 1392 loss: 3.5454018e-06
Iter: 1393 loss: 3.54541635e-06
Iter: 1394 loss: 3.54512645e-06
Iter: 1395 loss: 3.54489748e-06
Iter: 1396 loss: 3.54458371e-06
Iter: 1397 loss: 3.54344365e-06
Iter: 1398 loss: 3.54746749e-06
Iter: 1399 loss: 3.54293161e-06
Iter: 1400 loss: 3.54121744e-06
Iter: 1401 loss: 3.54428107e-06
Iter: 1402 loss: 3.54047279e-06
Iter: 1403 loss: 3.53911446e-06
Iter: 1404 loss: 3.54067765e-06
Iter: 1405 loss: 3.53836276e-06
Iter: 1406 loss: 3.53673863e-06
Iter: 1407 loss: 3.54652775e-06
Iter: 1408 loss: 3.53664154e-06
Iter: 1409 loss: 3.53532323e-06
Iter: 1410 loss: 3.53559199e-06
Iter: 1411 loss: 3.53439168e-06
Iter: 1412 loss: 3.53248015e-06
Iter: 1413 loss: 3.54540725e-06
Iter: 1414 loss: 3.53230189e-06
Iter: 1415 loss: 3.5311798e-06
Iter: 1416 loss: 3.53210839e-06
Iter: 1417 loss: 3.53042583e-06
Iter: 1418 loss: 3.52853385e-06
Iter: 1419 loss: 3.53582027e-06
Iter: 1420 loss: 3.52818097e-06
Iter: 1421 loss: 3.52685583e-06
Iter: 1422 loss: 3.53368796e-06
Iter: 1423 loss: 3.52662664e-06
Iter: 1424 loss: 3.52575216e-06
Iter: 1425 loss: 3.5240696e-06
Iter: 1426 loss: 3.56306805e-06
Iter: 1427 loss: 3.52404277e-06
Iter: 1428 loss: 3.52272559e-06
Iter: 1429 loss: 3.5226426e-06
Iter: 1430 loss: 3.52169604e-06
Iter: 1431 loss: 3.52791358e-06
Iter: 1432 loss: 3.52156758e-06
Iter: 1433 loss: 3.52010557e-06
Iter: 1434 loss: 3.52036636e-06
Iter: 1435 loss: 3.51909125e-06
Iter: 1436 loss: 3.51812969e-06
Iter: 1437 loss: 3.51691642e-06
Iter: 1438 loss: 3.51672543e-06
Iter: 1439 loss: 3.51500694e-06
Iter: 1440 loss: 3.51537096e-06
Iter: 1441 loss: 3.51362769e-06
Iter: 1442 loss: 3.51229846e-06
Iter: 1443 loss: 3.51230301e-06
Iter: 1444 loss: 3.51103654e-06
Iter: 1445 loss: 3.50933351e-06
Iter: 1446 loss: 3.50927257e-06
Iter: 1447 loss: 3.50750543e-06
Iter: 1448 loss: 3.52646066e-06
Iter: 1449 loss: 3.50749042e-06
Iter: 1450 loss: 3.50603113e-06
Iter: 1451 loss: 3.50456435e-06
Iter: 1452 loss: 3.50423124e-06
Iter: 1453 loss: 3.50267192e-06
Iter: 1454 loss: 3.50257369e-06
Iter: 1455 loss: 3.50155e-06
Iter: 1456 loss: 3.50218056e-06
Iter: 1457 loss: 3.50088294e-06
Iter: 1458 loss: 3.49936431e-06
Iter: 1459 loss: 3.50435948e-06
Iter: 1460 loss: 3.49909556e-06
Iter: 1461 loss: 3.49780976e-06
Iter: 1462 loss: 3.50266509e-06
Iter: 1463 loss: 3.49753645e-06
Iter: 1464 loss: 3.49659945e-06
Iter: 1465 loss: 3.5048165e-06
Iter: 1466 loss: 3.49650531e-06
Iter: 1467 loss: 3.4954337e-06
Iter: 1468 loss: 3.49830498e-06
Iter: 1469 loss: 3.49506217e-06
Iter: 1470 loss: 3.49464517e-06
Iter: 1471 loss: 3.49335869e-06
Iter: 1472 loss: 3.50669438e-06
Iter: 1473 loss: 3.49325705e-06
Iter: 1474 loss: 3.49163156e-06
Iter: 1475 loss: 3.49529932e-06
Iter: 1476 loss: 3.49085849e-06
Iter: 1477 loss: 3.48986464e-06
Iter: 1478 loss: 3.4955458e-06
Iter: 1479 loss: 3.48960543e-06
Iter: 1480 loss: 3.48825984e-06
Iter: 1481 loss: 3.48653884e-06
Iter: 1482 loss: 3.4864288e-06
Iter: 1483 loss: 3.48441722e-06
Iter: 1484 loss: 3.49286847e-06
Iter: 1485 loss: 3.4840964e-06
Iter: 1486 loss: 3.48194658e-06
Iter: 1487 loss: 3.49236916e-06
Iter: 1488 loss: 3.48165759e-06
Iter: 1489 loss: 3.48016056e-06
Iter: 1490 loss: 3.48057256e-06
Iter: 1491 loss: 3.47901823e-06
Iter: 1492 loss: 3.47700916e-06
Iter: 1493 loss: 3.49843572e-06
Iter: 1494 loss: 3.47688388e-06
Iter: 1495 loss: 3.47588389e-06
Iter: 1496 loss: 3.47840205e-06
Iter: 1497 loss: 3.47553896e-06
Iter: 1498 loss: 3.47416062e-06
Iter: 1499 loss: 3.47331411e-06
Iter: 1500 loss: 3.47270134e-06
Iter: 1501 loss: 3.47454397e-06
Iter: 1502 loss: 3.47219247e-06
Iter: 1503 loss: 3.4718305e-06
Iter: 1504 loss: 3.47186779e-06
Iter: 1505 loss: 3.4715049e-06
Iter: 1506 loss: 3.47089804e-06
Iter: 1507 loss: 3.46919842e-06
Iter: 1508 loss: 3.48315029e-06
Iter: 1509 loss: 3.46893944e-06
Iter: 1510 loss: 3.4674e-06
Iter: 1511 loss: 3.47338323e-06
Iter: 1512 loss: 3.46708794e-06
Iter: 1513 loss: 3.46554361e-06
Iter: 1514 loss: 3.47237165e-06
Iter: 1515 loss: 3.46534353e-06
Iter: 1516 loss: 3.46422576e-06
Iter: 1517 loss: 3.46695379e-06
Iter: 1518 loss: 3.46386651e-06
Iter: 1519 loss: 3.46239244e-06
Iter: 1520 loss: 3.46141064e-06
Iter: 1521 loss: 3.46106208e-06
Iter: 1522 loss: 3.45953163e-06
Iter: 1523 loss: 3.48043932e-06
Iter: 1524 loss: 3.45942158e-06
Iter: 1525 loss: 3.45818353e-06
Iter: 1526 loss: 3.45790659e-06
Iter: 1527 loss: 3.45711578e-06
Iter: 1528 loss: 3.45539343e-06
Iter: 1529 loss: 3.46642082e-06
Iter: 1530 loss: 3.45529497e-06
Iter: 1531 loss: 3.4540094e-06
Iter: 1532 loss: 3.4531688e-06
Iter: 1533 loss: 3.45257604e-06
Iter: 1534 loss: 3.45205854e-06
Iter: 1535 loss: 3.45168655e-06
Iter: 1536 loss: 3.4510781e-06
Iter: 1537 loss: 3.45904391e-06
Iter: 1538 loss: 3.45108492e-06
Iter: 1539 loss: 3.45058697e-06
Iter: 1540 loss: 3.44950604e-06
Iter: 1541 loss: 3.4495697e-06
Iter: 1542 loss: 3.44833961e-06
Iter: 1543 loss: 3.4483237e-06
Iter: 1544 loss: 3.44731279e-06
Iter: 1545 loss: 3.44614e-06
Iter: 1546 loss: 3.4509344e-06
Iter: 1547 loss: 3.4458933e-06
Iter: 1548 loss: 3.44443e-06
Iter: 1549 loss: 3.44432647e-06
Iter: 1550 loss: 3.44325304e-06
Iter: 1551 loss: 3.44207047e-06
Iter: 1552 loss: 3.44815908e-06
Iter: 1553 loss: 3.44175737e-06
Iter: 1554 loss: 3.44033015e-06
Iter: 1555 loss: 3.44153887e-06
Iter: 1556 loss: 3.4394302e-06
Iter: 1557 loss: 3.43808233e-06
Iter: 1558 loss: 3.44963701e-06
Iter: 1559 loss: 3.43799297e-06
Iter: 1560 loss: 3.4366567e-06
Iter: 1561 loss: 3.43664442e-06
Iter: 1562 loss: 3.43542456e-06
Iter: 1563 loss: 3.43419174e-06
Iter: 1564 loss: 3.4434147e-06
Iter: 1565 loss: 3.43394186e-06
Iter: 1566 loss: 3.43272018e-06
Iter: 1567 loss: 3.43247984e-06
Iter: 1568 loss: 3.43148622e-06
Iter: 1569 loss: 3.4300333e-06
Iter: 1570 loss: 3.4371958e-06
Iter: 1571 loss: 3.4298821e-06
Iter: 1572 loss: 3.42836211e-06
Iter: 1573 loss: 3.43619172e-06
Iter: 1574 loss: 3.42815656e-06
Iter: 1575 loss: 3.4282873e-06
Iter: 1576 loss: 3.4276145e-06
Iter: 1577 loss: 3.42742328e-06
Iter: 1578 loss: 3.42659473e-06
Iter: 1579 loss: 3.42948488e-06
Iter: 1580 loss: 3.42621161e-06
Iter: 1581 loss: 3.42475937e-06
Iter: 1582 loss: 3.42509134e-06
Iter: 1583 loss: 3.42368276e-06
Iter: 1584 loss: 3.42236035e-06
Iter: 1585 loss: 3.44051296e-06
Iter: 1586 loss: 3.42246221e-06
Iter: 1587 loss: 3.42133944e-06
Iter: 1588 loss: 3.4202385e-06
Iter: 1589 loss: 3.42010321e-06
Iter: 1590 loss: 3.41838859e-06
Iter: 1591 loss: 3.42870658e-06
Iter: 1592 loss: 3.4182076e-06
Iter: 1593 loss: 3.41714667e-06
Iter: 1594 loss: 3.41849227e-06
Iter: 1595 loss: 3.41665327e-06
Iter: 1596 loss: 3.41492819e-06
Iter: 1597 loss: 3.41753093e-06
Iter: 1598 loss: 3.41416717e-06
Iter: 1599 loss: 3.41286295e-06
Iter: 1600 loss: 3.41667646e-06
Iter: 1601 loss: 3.41254417e-06
Iter: 1602 loss: 3.41084456e-06
Iter: 1603 loss: 3.41175291e-06
Iter: 1604 loss: 3.40981865e-06
Iter: 1605 loss: 3.40841643e-06
Iter: 1606 loss: 3.41693203e-06
Iter: 1607 loss: 3.40823e-06
Iter: 1608 loss: 3.40679821e-06
Iter: 1609 loss: 3.41260852e-06
Iter: 1610 loss: 3.40636439e-06
Iter: 1611 loss: 3.40639644e-06
Iter: 1612 loss: 3.4059658e-06
Iter: 1613 loss: 3.40550832e-06
Iter: 1614 loss: 3.4046252e-06
Iter: 1615 loss: 3.40466431e-06
Iter: 1616 loss: 3.40374436e-06
Iter: 1617 loss: 3.40217616e-06
Iter: 1618 loss: 3.40221482e-06
Iter: 1619 loss: 3.40069801e-06
Iter: 1620 loss: 3.4057698e-06
Iter: 1621 loss: 3.40036308e-06
Iter: 1622 loss: 3.39856138e-06
Iter: 1623 loss: 3.40649376e-06
Iter: 1624 loss: 3.39815483e-06
Iter: 1625 loss: 3.39707299e-06
Iter: 1626 loss: 3.40692804e-06
Iter: 1627 loss: 3.39698045e-06
Iter: 1628 loss: 3.39613075e-06
Iter: 1629 loss: 3.3943677e-06
Iter: 1630 loss: 3.42638623e-06
Iter: 1631 loss: 3.3943345e-06
Iter: 1632 loss: 3.39277562e-06
Iter: 1633 loss: 3.41005398e-06
Iter: 1634 loss: 3.39280132e-06
Iter: 1635 loss: 3.3911781e-06
Iter: 1636 loss: 3.39225244e-06
Iter: 1637 loss: 3.39015423e-06
Iter: 1638 loss: 3.38880864e-06
Iter: 1639 loss: 3.39476492e-06
Iter: 1640 loss: 3.38847758e-06
Iter: 1641 loss: 3.38685913e-06
Iter: 1642 loss: 3.38650216e-06
Iter: 1643 loss: 3.3855556e-06
Iter: 1644 loss: 3.38449854e-06
Iter: 1645 loss: 3.38437394e-06
Iter: 1646 loss: 3.38408472e-06
Iter: 1647 loss: 3.38405403e-06
Iter: 1648 loss: 3.38339169e-06
Iter: 1649 loss: 3.38198561e-06
Iter: 1650 loss: 3.39538747e-06
Iter: 1651 loss: 3.38188102e-06
Iter: 1652 loss: 3.38060272e-06
Iter: 1653 loss: 3.38175369e-06
Iter: 1654 loss: 3.37982146e-06
Iter: 1655 loss: 3.37809274e-06
Iter: 1656 loss: 3.37954361e-06
Iter: 1657 loss: 3.37706888e-06
Iter: 1658 loss: 3.37580605e-06
Iter: 1659 loss: 3.39119561e-06
Iter: 1660 loss: 3.37571282e-06
Iter: 1661 loss: 3.37456686e-06
Iter: 1662 loss: 3.37415599e-06
Iter: 1663 loss: 3.3737349e-06
Iter: 1664 loss: 3.37240726e-06
Iter: 1665 loss: 3.38288737e-06
Iter: 1666 loss: 3.3722481e-06
Iter: 1667 loss: 3.37117331e-06
Iter: 1668 loss: 3.3745664e-06
Iter: 1669 loss: 3.37073584e-06
Iter: 1670 loss: 3.36958078e-06
Iter: 1671 loss: 3.37203255e-06
Iter: 1672 loss: 3.3690842e-06
Iter: 1673 loss: 3.36785752e-06
Iter: 1674 loss: 3.36830226e-06
Iter: 1675 loss: 3.36685639e-06
Iter: 1676 loss: 3.36544599e-06
Iter: 1677 loss: 3.37982965e-06
Iter: 1678 loss: 3.3653987e-06
Iter: 1679 loss: 3.36460084e-06
Iter: 1680 loss: 3.36293124e-06
Iter: 1681 loss: 3.39873941e-06
Iter: 1682 loss: 3.36290395e-06
Iter: 1683 loss: 3.36295898e-06
Iter: 1684 loss: 3.36201015e-06
Iter: 1685 loss: 3.36103176e-06
Iter: 1686 loss: 3.361577e-06
Iter: 1687 loss: 3.36058247e-06
Iter: 1688 loss: 3.36010748e-06
Iter: 1689 loss: 3.35891309e-06
Iter: 1690 loss: 3.37656502e-06
Iter: 1691 loss: 3.35891582e-06
Iter: 1692 loss: 3.35731988e-06
Iter: 1693 loss: 3.36343192e-06
Iter: 1694 loss: 3.35691675e-06
Iter: 1695 loss: 3.35580557e-06
Iter: 1696 loss: 3.35682398e-06
Iter: 1697 loss: 3.35523168e-06
Iter: 1698 loss: 3.35374807e-06
Iter: 1699 loss: 3.36046196e-06
Iter: 1700 loss: 3.35339655e-06
Iter: 1701 loss: 3.35220147e-06
Iter: 1702 loss: 3.3526735e-06
Iter: 1703 loss: 3.35127129e-06
Iter: 1704 loss: 3.34962851e-06
Iter: 1705 loss: 3.36377275e-06
Iter: 1706 loss: 3.34950983e-06
Iter: 1707 loss: 3.34851211e-06
Iter: 1708 loss: 3.35071309e-06
Iter: 1709 loss: 3.34815741e-06
Iter: 1710 loss: 3.34682022e-06
Iter: 1711 loss: 3.34812376e-06
Iter: 1712 loss: 3.34607603e-06
Iter: 1713 loss: 3.34459173e-06
Iter: 1714 loss: 3.34833089e-06
Iter: 1715 loss: 3.34421679e-06
Iter: 1716 loss: 3.34306787e-06
Iter: 1717 loss: 3.35911591e-06
Iter: 1718 loss: 3.34301649e-06
Iter: 1719 loss: 3.34247125e-06
Iter: 1720 loss: 3.3425008e-06
Iter: 1721 loss: 3.34201786e-06
Iter: 1722 loss: 3.34083234e-06
Iter: 1723 loss: 3.35413597e-06
Iter: 1724 loss: 3.34072183e-06
Iter: 1725 loss: 3.33947833e-06
Iter: 1726 loss: 3.34163724e-06
Iter: 1727 loss: 3.33893604e-06
Iter: 1728 loss: 3.33792286e-06
Iter: 1729 loss: 3.34336755e-06
Iter: 1730 loss: 3.33779167e-06
Iter: 1731 loss: 3.33675143e-06
Iter: 1732 loss: 3.3360252e-06
Iter: 1733 loss: 3.33558478e-06
Iter: 1734 loss: 3.3345093e-06
Iter: 1735 loss: 3.34940614e-06
Iter: 1736 loss: 3.33448679e-06
Iter: 1737 loss: 3.33359048e-06
Iter: 1738 loss: 3.33326466e-06
Iter: 1739 loss: 3.33285061e-06
Iter: 1740 loss: 3.33156299e-06
Iter: 1741 loss: 3.33698972e-06
Iter: 1742 loss: 3.3311685e-06
Iter: 1743 loss: 3.3301344e-06
Iter: 1744 loss: 3.32952823e-06
Iter: 1745 loss: 3.32909121e-06
Iter: 1746 loss: 3.32755826e-06
Iter: 1747 loss: 3.34279434e-06
Iter: 1748 loss: 3.3274498e-06
Iter: 1749 loss: 3.32627496e-06
Iter: 1750 loss: 3.32798118e-06
Iter: 1751 loss: 3.32570789e-06
Iter: 1752 loss: 3.32474679e-06
Iter: 1753 loss: 3.33843718e-06
Iter: 1754 loss: 3.32475383e-06
Iter: 1755 loss: 3.32431159e-06
Iter: 1756 loss: 3.32430545e-06
Iter: 1757 loss: 3.32393483e-06
Iter: 1758 loss: 3.32294121e-06
Iter: 1759 loss: 3.32196851e-06
Iter: 1760 loss: 3.32162153e-06
Iter: 1761 loss: 3.31995284e-06
Iter: 1762 loss: 3.33876756e-06
Iter: 1763 loss: 3.31992305e-06
Iter: 1764 loss: 3.31858405e-06
Iter: 1765 loss: 3.32257423e-06
Iter: 1766 loss: 3.31817478e-06
Iter: 1767 loss: 3.31696128e-06
Iter: 1768 loss: 3.32038894e-06
Iter: 1769 loss: 3.31654041e-06
Iter: 1770 loss: 3.31565138e-06
Iter: 1771 loss: 3.3180072e-06
Iter: 1772 loss: 3.31521824e-06
Iter: 1773 loss: 3.31395404e-06
Iter: 1774 loss: 3.31621459e-06
Iter: 1775 loss: 3.31338606e-06
Iter: 1776 loss: 3.31249794e-06
Iter: 1777 loss: 3.31240108e-06
Iter: 1778 loss: 3.31167826e-06
Iter: 1779 loss: 3.31006504e-06
Iter: 1780 loss: 3.31562228e-06
Iter: 1781 loss: 3.30953162e-06
Iter: 1782 loss: 3.30833018e-06
Iter: 1783 loss: 3.31243746e-06
Iter: 1784 loss: 3.30803368e-06
Iter: 1785 loss: 3.30662533e-06
Iter: 1786 loss: 3.31123351e-06
Iter: 1787 loss: 3.30630178e-06
Iter: 1788 loss: 3.30695889e-06
Iter: 1789 loss: 3.30590547e-06
Iter: 1790 loss: 3.30561693e-06
Iter: 1791 loss: 3.30467128e-06
Iter: 1792 loss: 3.31263163e-06
Iter: 1793 loss: 3.30455737e-06
Iter: 1794 loss: 3.30358739e-06
Iter: 1795 loss: 3.30367766e-06
Iter: 1796 loss: 3.30289186e-06
Iter: 1797 loss: 3.30182365e-06
Iter: 1798 loss: 3.30436023e-06
Iter: 1799 loss: 3.30144144e-06
Iter: 1800 loss: 3.30000876e-06
Iter: 1801 loss: 3.30772855e-06
Iter: 1802 loss: 3.29989689e-06
Iter: 1803 loss: 3.29882732e-06
Iter: 1804 loss: 3.30090711e-06
Iter: 1805 loss: 3.29836075e-06
Iter: 1806 loss: 3.2971584e-06
Iter: 1807 loss: 3.2979342e-06
Iter: 1808 loss: 3.29643512e-06
Iter: 1809 loss: 3.2953144e-06
Iter: 1810 loss: 3.30701459e-06
Iter: 1811 loss: 3.29532941e-06
Iter: 1812 loss: 3.29435625e-06
Iter: 1813 loss: 3.29304612e-06
Iter: 1814 loss: 3.29292197e-06
Iter: 1815 loss: 3.29168279e-06
Iter: 1816 loss: 3.31095293e-06
Iter: 1817 loss: 3.29165323e-06
Iter: 1818 loss: 3.29069485e-06
Iter: 1819 loss: 3.2894884e-06
Iter: 1820 loss: 3.28937426e-06
Iter: 1821 loss: 3.28793976e-06
Iter: 1822 loss: 3.30876264e-06
Iter: 1823 loss: 3.28784063e-06
Iter: 1824 loss: 3.28807891e-06
Iter: 1825 loss: 3.28757142e-06
Iter: 1826 loss: 3.28729357e-06
Iter: 1827 loss: 3.28648412e-06
Iter: 1828 loss: 3.28768283e-06
Iter: 1829 loss: 3.28593387e-06
Iter: 1830 loss: 3.28476926e-06
Iter: 1831 loss: 3.28691158e-06
Iter: 1832 loss: 3.28433293e-06
Iter: 1833 loss: 3.28312012e-06
Iter: 1834 loss: 3.28470242e-06
Iter: 1835 loss: 3.28251645e-06
Iter: 1836 loss: 3.28122906e-06
Iter: 1837 loss: 3.29212298e-06
Iter: 1838 loss: 3.2811281e-06
Iter: 1839 loss: 3.2800549e-06
Iter: 1840 loss: 3.27900648e-06
Iter: 1841 loss: 3.27874977e-06
Iter: 1842 loss: 3.27725706e-06
Iter: 1843 loss: 3.29574823e-06
Iter: 1844 loss: 3.27724683e-06
Iter: 1845 loss: 3.27618272e-06
Iter: 1846 loss: 3.2786993e-06
Iter: 1847 loss: 3.27576572e-06
Iter: 1848 loss: 3.27486168e-06
Iter: 1849 loss: 3.27816542e-06
Iter: 1850 loss: 3.27457656e-06
Iter: 1851 loss: 3.27384851e-06
Iter: 1852 loss: 3.2733476e-06
Iter: 1853 loss: 3.27306907e-06
Iter: 1854 loss: 3.27177941e-06
Iter: 1855 loss: 3.27665248e-06
Iter: 1856 loss: 3.27144403e-06
Iter: 1857 loss: 3.27117505e-06
Iter: 1858 loss: 3.27091129e-06
Iter: 1859 loss: 3.27037833e-06
Iter: 1860 loss: 3.27048474e-06
Iter: 1861 loss: 3.26999429e-06
Iter: 1862 loss: 3.2692792e-06
Iter: 1863 loss: 3.26800864e-06
Iter: 1864 loss: 3.2891212e-06
Iter: 1865 loss: 3.26790769e-06
Iter: 1866 loss: 3.26682903e-06
Iter: 1867 loss: 3.27392968e-06
Iter: 1868 loss: 3.26672284e-06
Iter: 1869 loss: 3.26574423e-06
Iter: 1870 loss: 3.26673626e-06
Iter: 1871 loss: 3.2650928e-06
Iter: 1872 loss: 3.26405848e-06
Iter: 1873 loss: 3.26897953e-06
Iter: 1874 loss: 3.26392046e-06
Iter: 1875 loss: 3.26278632e-06
Iter: 1876 loss: 3.26312124e-06
Iter: 1877 loss: 3.26198347e-06
Iter: 1878 loss: 3.26095915e-06
Iter: 1879 loss: 3.27257385e-06
Iter: 1880 loss: 3.26097233e-06
Iter: 1881 loss: 3.26015333e-06
Iter: 1882 loss: 3.2604471e-06
Iter: 1883 loss: 3.25945848e-06
Iter: 1884 loss: 3.25848941e-06
Iter: 1885 loss: 3.2638668e-06
Iter: 1886 loss: 3.25828432e-06
Iter: 1887 loss: 3.25733072e-06
Iter: 1888 loss: 3.26043573e-06
Iter: 1889 loss: 3.25710789e-06
Iter: 1890 loss: 3.25613814e-06
Iter: 1891 loss: 3.25722817e-06
Iter: 1892 loss: 3.25561973e-06
Iter: 1893 loss: 3.25544261e-06
Iter: 1894 loss: 3.25524252e-06
Iter: 1895 loss: 3.2548428e-06
Iter: 1896 loss: 3.25367523e-06
Iter: 1897 loss: 3.26119834e-06
Iter: 1898 loss: 3.25341125e-06
Iter: 1899 loss: 3.25249312e-06
Iter: 1900 loss: 3.2561602e-06
Iter: 1901 loss: 3.25219753e-06
Iter: 1902 loss: 3.25129804e-06
Iter: 1903 loss: 3.25199267e-06
Iter: 1904 loss: 3.25089741e-06
Iter: 1905 loss: 3.24974417e-06
Iter: 1906 loss: 3.25388692e-06
Iter: 1907 loss: 3.24949133e-06
Iter: 1908 loss: 3.24870916e-06
Iter: 1909 loss: 3.24910661e-06
Iter: 1910 loss: 3.24829216e-06
Iter: 1911 loss: 3.247319e-06
Iter: 1912 loss: 3.25100859e-06
Iter: 1913 loss: 3.24685743e-06
Iter: 1914 loss: 3.24596158e-06
Iter: 1915 loss: 3.24857501e-06
Iter: 1916 loss: 3.24567736e-06
Iter: 1917 loss: 3.24461848e-06
Iter: 1918 loss: 3.24506664e-06
Iter: 1919 loss: 3.24372809e-06
Iter: 1920 loss: 3.24270604e-06
Iter: 1921 loss: 3.24882717e-06
Iter: 1922 loss: 3.24257189e-06
Iter: 1923 loss: 3.24145276e-06
Iter: 1924 loss: 3.24621669e-06
Iter: 1925 loss: 3.24135203e-06
Iter: 1926 loss: 3.24054622e-06
Iter: 1927 loss: 3.244749e-06
Iter: 1928 loss: 3.24055054e-06
Iter: 1929 loss: 3.23982044e-06
Iter: 1930 loss: 3.24726307e-06
Iter: 1931 loss: 3.23981317e-06
Iter: 1932 loss: 3.23930863e-06
Iter: 1933 loss: 3.23840527e-06
Iter: 1934 loss: 3.25898759e-06
Iter: 1935 loss: 3.23837071e-06
Iter: 1936 loss: 3.2377061e-06
Iter: 1937 loss: 3.23924678e-06
Iter: 1938 loss: 3.2373905e-06
Iter: 1939 loss: 3.23650738e-06
Iter: 1940 loss: 3.23557788e-06
Iter: 1941 loss: 3.23536779e-06
Iter: 1942 loss: 3.2343346e-06
Iter: 1943 loss: 3.23991617e-06
Iter: 1944 loss: 3.23416816e-06
Iter: 1945 loss: 3.23312179e-06
Iter: 1946 loss: 3.23550466e-06
Iter: 1947 loss: 3.23263521e-06
Iter: 1948 loss: 3.23176073e-06
Iter: 1949 loss: 3.23684935e-06
Iter: 1950 loss: 3.23158793e-06
Iter: 1951 loss: 3.23051222e-06
Iter: 1952 loss: 3.23159065e-06
Iter: 1953 loss: 3.22988217e-06
Iter: 1954 loss: 3.228997e-06
Iter: 1955 loss: 3.23435188e-06
Iter: 1956 loss: 3.22877213e-06
Iter: 1957 loss: 3.2279022e-06
Iter: 1958 loss: 3.22755795e-06
Iter: 1959 loss: 3.2271646e-06
Iter: 1960 loss: 3.2261446e-06
Iter: 1961 loss: 3.23487302e-06
Iter: 1962 loss: 3.22614051e-06
Iter: 1963 loss: 3.22573965e-06
Iter: 1964 loss: 3.22559708e-06
Iter: 1965 loss: 3.22519099e-06
Iter: 1966 loss: 3.22487904e-06
Iter: 1967 loss: 3.22462665e-06
Iter: 1968 loss: 3.22423602e-06
Iter: 1969 loss: 3.22384903e-06
Iter: 1970 loss: 3.22370306e-06
Iter: 1971 loss: 3.22283427e-06
Iter: 1972 loss: 3.22323967e-06
Iter: 1973 loss: 3.22226265e-06
Iter: 1974 loss: 3.22117012e-06
Iter: 1975 loss: 3.22911319e-06
Iter: 1976 loss: 3.22107371e-06
Iter: 1977 loss: 3.22033929e-06
Iter: 1978 loss: 3.2194489e-06
Iter: 1979 loss: 3.21939729e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi3
+ date
Mon Oct 26 15:55:44 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi3/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi3_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi3_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi3_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi3/300_300_300_1 --optimizer lbfgs --function f1 --psi -1 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi3_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2cdbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2cfe620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2c9cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2c9cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2d99510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2d99d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2c5c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2c098c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2c09730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2ba3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2bd4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2b7cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2b82378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2b82510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2b22a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2b22ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2ac31e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2a82ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2a867b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2a13ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2a14048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d2a12a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d29f9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d29c7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d29c7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d294fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d29587b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d28aa730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d28aa2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d290eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57d28fbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f579ebe5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f579ec186a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f579ebe50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f579eb9f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f579eb66620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.72775544e-05
Iter: 2 loss: 1.5059466e-05
Iter: 3 loss: 2.86282229e-05
Iter: 4 loss: 1.47905093e-05
Iter: 5 loss: 1.38576061e-05
Iter: 6 loss: 1.37919415e-05
Iter: 7 loss: 1.32253217e-05
Iter: 8 loss: 1.19810511e-05
Iter: 9 loss: 3.02417702e-05
Iter: 10 loss: 1.19251417e-05
Iter: 11 loss: 1.13043316e-05
Iter: 12 loss: 2.08334895e-05
Iter: 13 loss: 1.13039896e-05
Iter: 14 loss: 1.08252e-05
Iter: 15 loss: 1.01454643e-05
Iter: 16 loss: 1.01214118e-05
Iter: 17 loss: 9.3465369e-06
Iter: 18 loss: 1.91447925e-05
Iter: 19 loss: 9.34035415e-06
Iter: 20 loss: 8.87671467e-06
Iter: 21 loss: 9.53280323e-06
Iter: 22 loss: 8.6492355e-06
Iter: 23 loss: 8.03509101e-06
Iter: 24 loss: 8.22578295e-06
Iter: 25 loss: 7.59520026e-06
Iter: 26 loss: 7.1742511e-06
Iter: 27 loss: 8.14162e-06
Iter: 28 loss: 7.01730505e-06
Iter: 29 loss: 6.72105853e-06
Iter: 30 loss: 1.11294321e-05
Iter: 31 loss: 6.72081705e-06
Iter: 32 loss: 6.51162463e-06
Iter: 33 loss: 6.1525966e-06
Iter: 34 loss: 6.15196313e-06
Iter: 35 loss: 5.892221e-06
Iter: 36 loss: 5.89003412e-06
Iter: 37 loss: 5.69673193e-06
Iter: 38 loss: 6.1585979e-06
Iter: 39 loss: 5.62672267e-06
Iter: 40 loss: 5.41413419e-06
Iter: 41 loss: 8.56131555e-06
Iter: 42 loss: 5.41402642e-06
Iter: 43 loss: 5.35373147e-06
Iter: 44 loss: 5.24640927e-06
Iter: 45 loss: 5.24643383e-06
Iter: 46 loss: 5.08899848e-06
Iter: 47 loss: 5.1781517e-06
Iter: 48 loss: 4.98623604e-06
Iter: 49 loss: 4.83157237e-06
Iter: 50 loss: 7.27118822e-06
Iter: 51 loss: 4.83154372e-06
Iter: 52 loss: 4.75335673e-06
Iter: 53 loss: 4.74510125e-06
Iter: 54 loss: 4.68840926e-06
Iter: 55 loss: 4.54657493e-06
Iter: 56 loss: 4.84060456e-06
Iter: 57 loss: 4.49002346e-06
Iter: 58 loss: 4.40460735e-06
Iter: 59 loss: 4.70757e-06
Iter: 60 loss: 4.38279676e-06
Iter: 61 loss: 4.28510566e-06
Iter: 62 loss: 4.21345749e-06
Iter: 63 loss: 4.18021955e-06
Iter: 64 loss: 4.06251502e-06
Iter: 65 loss: 4.99597354e-06
Iter: 66 loss: 4.05450692e-06
Iter: 67 loss: 3.95029656e-06
Iter: 68 loss: 4.03208378e-06
Iter: 69 loss: 3.88722765e-06
Iter: 70 loss: 3.80152233e-06
Iter: 71 loss: 4.0605446e-06
Iter: 72 loss: 3.77567244e-06
Iter: 73 loss: 3.7157206e-06
Iter: 74 loss: 3.71536316e-06
Iter: 75 loss: 3.68049814e-06
Iter: 76 loss: 3.67973621e-06
Iter: 77 loss: 3.65599954e-06
Iter: 78 loss: 3.58381567e-06
Iter: 79 loss: 3.77300603e-06
Iter: 80 loss: 3.54424242e-06
Iter: 81 loss: 3.46595425e-06
Iter: 82 loss: 3.85698604e-06
Iter: 83 loss: 3.45273793e-06
Iter: 84 loss: 3.3875549e-06
Iter: 85 loss: 3.69163263e-06
Iter: 86 loss: 3.37550182e-06
Iter: 87 loss: 3.33953153e-06
Iter: 88 loss: 3.33939943e-06
Iter: 89 loss: 3.30768216e-06
Iter: 90 loss: 3.24905022e-06
Iter: 91 loss: 4.59509783e-06
Iter: 92 loss: 3.24886e-06
Iter: 93 loss: 3.20077743e-06
Iter: 94 loss: 3.70780845e-06
Iter: 95 loss: 3.19964715e-06
Iter: 96 loss: 3.16470391e-06
Iter: 97 loss: 3.16114756e-06
Iter: 98 loss: 3.13594478e-06
Iter: 99 loss: 3.10695145e-06
Iter: 100 loss: 3.10559426e-06
Iter: 101 loss: 3.08553626e-06
Iter: 102 loss: 3.06425977e-06
Iter: 103 loss: 3.06070706e-06
Iter: 104 loss: 3.01433829e-06
Iter: 105 loss: 3.05907088e-06
Iter: 106 loss: 2.98772693e-06
Iter: 107 loss: 2.94365555e-06
Iter: 108 loss: 3.12845305e-06
Iter: 109 loss: 2.93415724e-06
Iter: 110 loss: 2.91911419e-06
Iter: 111 loss: 2.91324795e-06
Iter: 112 loss: 2.89270088e-06
Iter: 113 loss: 2.90667958e-06
Iter: 114 loss: 2.8798836e-06
Iter: 115 loss: 2.86663135e-06
Iter: 116 loss: 2.83312579e-06
Iter: 117 loss: 3.13411647e-06
Iter: 118 loss: 2.82808242e-06
Iter: 119 loss: 2.79011374e-06
Iter: 120 loss: 3.11894792e-06
Iter: 121 loss: 2.78803327e-06
Iter: 122 loss: 2.76190349e-06
Iter: 123 loss: 2.8132863e-06
Iter: 124 loss: 2.75112779e-06
Iter: 125 loss: 2.72605121e-06
Iter: 126 loss: 3.084117e-06
Iter: 127 loss: 2.7259764e-06
Iter: 128 loss: 2.71203635e-06
Iter: 129 loss: 2.69402221e-06
Iter: 130 loss: 2.69274233e-06
Iter: 131 loss: 2.66742882e-06
Iter: 132 loss: 2.76799619e-06
Iter: 133 loss: 2.66175402e-06
Iter: 134 loss: 2.64131495e-06
Iter: 135 loss: 2.65420158e-06
Iter: 136 loss: 2.62816252e-06
Iter: 137 loss: 2.60272373e-06
Iter: 138 loss: 2.90236585e-06
Iter: 139 loss: 2.60236811e-06
Iter: 140 loss: 2.58864884e-06
Iter: 141 loss: 2.62098638e-06
Iter: 142 loss: 2.58379032e-06
Iter: 143 loss: 2.56730891e-06
Iter: 144 loss: 2.54474776e-06
Iter: 145 loss: 2.54363863e-06
Iter: 146 loss: 2.5215686e-06
Iter: 147 loss: 2.72348325e-06
Iter: 148 loss: 2.52051768e-06
Iter: 149 loss: 2.49887717e-06
Iter: 150 loss: 2.5125712e-06
Iter: 151 loss: 2.48501192e-06
Iter: 152 loss: 2.46458239e-06
Iter: 153 loss: 2.61000287e-06
Iter: 154 loss: 2.4627484e-06
Iter: 155 loss: 2.45488445e-06
Iter: 156 loss: 2.45225874e-06
Iter: 157 loss: 2.44448302e-06
Iter: 158 loss: 2.42751321e-06
Iter: 159 loss: 2.67345558e-06
Iter: 160 loss: 2.42672172e-06
Iter: 161 loss: 2.41611428e-06
Iter: 162 loss: 2.46586e-06
Iter: 163 loss: 2.41412636e-06
Iter: 164 loss: 2.40222198e-06
Iter: 165 loss: 2.43349859e-06
Iter: 166 loss: 2.39833048e-06
Iter: 167 loss: 2.38920734e-06
Iter: 168 loss: 2.39697e-06
Iter: 169 loss: 2.38390749e-06
Iter: 170 loss: 2.3719856e-06
Iter: 171 loss: 2.37202812e-06
Iter: 172 loss: 2.36248843e-06
Iter: 173 loss: 2.34758704e-06
Iter: 174 loss: 2.42543501e-06
Iter: 175 loss: 2.34525146e-06
Iter: 176 loss: 2.32875232e-06
Iter: 177 loss: 2.37773861e-06
Iter: 178 loss: 2.3237385e-06
Iter: 179 loss: 2.31074318e-06
Iter: 180 loss: 2.39555311e-06
Iter: 181 loss: 2.30938326e-06
Iter: 182 loss: 2.30115279e-06
Iter: 183 loss: 2.28703948e-06
Iter: 184 loss: 2.28702083e-06
Iter: 185 loss: 2.27101327e-06
Iter: 186 loss: 2.43312888e-06
Iter: 187 loss: 2.27052783e-06
Iter: 188 loss: 2.25740177e-06
Iter: 189 loss: 2.2535296e-06
Iter: 190 loss: 2.24561427e-06
Iter: 191 loss: 2.25183703e-06
Iter: 192 loss: 2.24027872e-06
Iter: 193 loss: 2.23479105e-06
Iter: 194 loss: 2.23527627e-06
Iter: 195 loss: 2.23050961e-06
Iter: 196 loss: 2.22436847e-06
Iter: 197 loss: 2.21028449e-06
Iter: 198 loss: 2.38005305e-06
Iter: 199 loss: 2.20903939e-06
Iter: 200 loss: 2.19874664e-06
Iter: 201 loss: 2.19871299e-06
Iter: 202 loss: 2.18945297e-06
Iter: 203 loss: 2.20824404e-06
Iter: 204 loss: 2.18577588e-06
Iter: 205 loss: 2.17710749e-06
Iter: 206 loss: 2.19676463e-06
Iter: 207 loss: 2.17376237e-06
Iter: 208 loss: 2.16654621e-06
Iter: 209 loss: 2.16433909e-06
Iter: 210 loss: 2.16000717e-06
Iter: 211 loss: 2.15084719e-06
Iter: 212 loss: 2.22473159e-06
Iter: 213 loss: 2.1502351e-06
Iter: 214 loss: 2.14205443e-06
Iter: 215 loss: 2.14382453e-06
Iter: 216 loss: 2.13611202e-06
Iter: 217 loss: 2.12644909e-06
Iter: 218 loss: 2.25277199e-06
Iter: 219 loss: 2.12641953e-06
Iter: 220 loss: 2.12011946e-06
Iter: 221 loss: 2.11559245e-06
Iter: 222 loss: 2.11339852e-06
Iter: 223 loss: 2.10371695e-06
Iter: 224 loss: 2.15319437e-06
Iter: 225 loss: 2.10205121e-06
Iter: 226 loss: 2.09580685e-06
Iter: 227 loss: 2.09469408e-06
Iter: 228 loss: 2.09045038e-06
Iter: 229 loss: 2.09089558e-06
Iter: 230 loss: 2.08616416e-06
Iter: 231 loss: 2.08218171e-06
Iter: 232 loss: 2.07616472e-06
Iter: 233 loss: 2.07605854e-06
Iter: 234 loss: 2.07084508e-06
Iter: 235 loss: 2.06070899e-06
Iter: 236 loss: 2.26691782e-06
Iter: 237 loss: 2.06058712e-06
Iter: 238 loss: 2.05287984e-06
Iter: 239 loss: 2.05278639e-06
Iter: 240 loss: 2.04698813e-06
Iter: 241 loss: 2.06936556e-06
Iter: 242 loss: 2.0456805e-06
Iter: 243 loss: 2.03960553e-06
Iter: 244 loss: 2.04837352e-06
Iter: 245 loss: 2.03671448e-06
Iter: 246 loss: 2.03156196e-06
Iter: 247 loss: 2.03168247e-06
Iter: 248 loss: 2.02749925e-06
Iter: 249 loss: 2.0205805e-06
Iter: 250 loss: 2.06283494e-06
Iter: 251 loss: 2.01982471e-06
Iter: 252 loss: 2.0147022e-06
Iter: 253 loss: 2.01738567e-06
Iter: 254 loss: 2.01124749e-06
Iter: 255 loss: 2.00445902e-06
Iter: 256 loss: 2.06902541e-06
Iter: 257 loss: 2.00421073e-06
Iter: 258 loss: 2.00001205e-06
Iter: 259 loss: 2.0055686e-06
Iter: 260 loss: 1.9980148e-06
Iter: 261 loss: 1.9925651e-06
Iter: 262 loss: 1.99253736e-06
Iter: 263 loss: 1.98824046e-06
Iter: 264 loss: 1.98242924e-06
Iter: 265 loss: 2.01527428e-06
Iter: 266 loss: 1.98162638e-06
Iter: 267 loss: 1.97860891e-06
Iter: 268 loss: 1.97844042e-06
Iter: 269 loss: 1.97463851e-06
Iter: 270 loss: 1.97189706e-06
Iter: 271 loss: 1.97058853e-06
Iter: 272 loss: 1.96785413e-06
Iter: 273 loss: 1.96283645e-06
Iter: 274 loss: 2.0789987e-06
Iter: 275 loss: 1.96281258e-06
Iter: 276 loss: 1.95622988e-06
Iter: 277 loss: 2.01001922e-06
Iter: 278 loss: 1.95588677e-06
Iter: 279 loss: 1.95155849e-06
Iter: 280 loss: 1.98332873e-06
Iter: 281 loss: 1.95119128e-06
Iter: 282 loss: 1.94780114e-06
Iter: 283 loss: 1.9432141e-06
Iter: 284 loss: 1.94292124e-06
Iter: 285 loss: 1.9377328e-06
Iter: 286 loss: 1.96417477e-06
Iter: 287 loss: 1.9368581e-06
Iter: 288 loss: 1.93183701e-06
Iter: 289 loss: 1.93891015e-06
Iter: 290 loss: 1.92937432e-06
Iter: 291 loss: 1.92528501e-06
Iter: 292 loss: 1.96305291e-06
Iter: 293 loss: 1.92510083e-06
Iter: 294 loss: 1.9212855e-06
Iter: 295 loss: 1.924657e-06
Iter: 296 loss: 1.91891786e-06
Iter: 297 loss: 1.91518097e-06
Iter: 298 loss: 1.93575534e-06
Iter: 299 loss: 1.91457343e-06
Iter: 300 loss: 1.91165373e-06
Iter: 301 loss: 1.90934e-06
Iter: 302 loss: 1.90846959e-06
Iter: 303 loss: 1.90435446e-06
Iter: 304 loss: 1.94326685e-06
Iter: 305 loss: 1.90414664e-06
Iter: 306 loss: 1.90362755e-06
Iter: 307 loss: 1.90278479e-06
Iter: 308 loss: 1.90143385e-06
Iter: 309 loss: 1.8971707e-06
Iter: 310 loss: 1.90751302e-06
Iter: 311 loss: 1.89464049e-06
Iter: 312 loss: 1.89062962e-06
Iter: 313 loss: 1.92068023e-06
Iter: 314 loss: 1.89033312e-06
Iter: 315 loss: 1.8866441e-06
Iter: 316 loss: 1.89563548e-06
Iter: 317 loss: 1.88531965e-06
Iter: 318 loss: 1.88133913e-06
Iter: 319 loss: 1.90695778e-06
Iter: 320 loss: 1.88089825e-06
Iter: 321 loss: 1.8785305e-06
Iter: 322 loss: 1.87441515e-06
Iter: 323 loss: 1.87438832e-06
Iter: 324 loss: 1.86952968e-06
Iter: 325 loss: 1.90617038e-06
Iter: 326 loss: 1.86924353e-06
Iter: 327 loss: 1.86546436e-06
Iter: 328 loss: 1.86890145e-06
Iter: 329 loss: 1.86307193e-06
Iter: 330 loss: 1.85985846e-06
Iter: 331 loss: 1.89980801e-06
Iter: 332 loss: 1.85974386e-06
Iter: 333 loss: 1.85700605e-06
Iter: 334 loss: 1.8586444e-06
Iter: 335 loss: 1.85519684e-06
Iter: 336 loss: 1.85144017e-06
Iter: 337 loss: 1.86323473e-06
Iter: 338 loss: 1.85029614e-06
Iter: 339 loss: 1.84760222e-06
Iter: 340 loss: 1.84941291e-06
Iter: 341 loss: 1.84600572e-06
Iter: 342 loss: 1.84422106e-06
Iter: 343 loss: 1.8439207e-06
Iter: 344 loss: 1.84178111e-06
Iter: 345 loss: 1.84318469e-06
Iter: 346 loss: 1.84043472e-06
Iter: 347 loss: 1.83905797e-06
Iter: 348 loss: 1.83514521e-06
Iter: 349 loss: 1.85192039e-06
Iter: 350 loss: 1.8335619e-06
Iter: 351 loss: 1.8290998e-06
Iter: 352 loss: 1.82910185e-06
Iter: 353 loss: 1.82631436e-06
Iter: 354 loss: 1.84163787e-06
Iter: 355 loss: 1.82594408e-06
Iter: 356 loss: 1.82287977e-06
Iter: 357 loss: 1.82063775e-06
Iter: 358 loss: 1.81972246e-06
Iter: 359 loss: 1.8167766e-06
Iter: 360 loss: 1.82420422e-06
Iter: 361 loss: 1.8156735e-06
Iter: 362 loss: 1.8120636e-06
Iter: 363 loss: 1.81954078e-06
Iter: 364 loss: 1.81052667e-06
Iter: 365 loss: 1.80699863e-06
Iter: 366 loss: 1.82117424e-06
Iter: 367 loss: 1.80623181e-06
Iter: 368 loss: 1.80286111e-06
Iter: 369 loss: 1.82148449e-06
Iter: 370 loss: 1.80238931e-06
Iter: 371 loss: 1.79997721e-06
Iter: 372 loss: 1.80714665e-06
Iter: 373 loss: 1.79917583e-06
Iter: 374 loss: 1.79660469e-06
Iter: 375 loss: 1.79460358e-06
Iter: 376 loss: 1.7936909e-06
Iter: 377 loss: 1.79289214e-06
Iter: 378 loss: 1.79219796e-06
Iter: 379 loss: 1.79038318e-06
Iter: 380 loss: 1.79290635e-06
Iter: 381 loss: 1.78956839e-06
Iter: 382 loss: 1.78808375e-06
Iter: 383 loss: 1.78502228e-06
Iter: 384 loss: 1.83577527e-06
Iter: 385 loss: 1.78488904e-06
Iter: 386 loss: 1.78184166e-06
Iter: 387 loss: 1.78450819e-06
Iter: 388 loss: 1.78011146e-06
Iter: 389 loss: 1.77634331e-06
Iter: 390 loss: 1.8134765e-06
Iter: 391 loss: 1.77622951e-06
Iter: 392 loss: 1.77471748e-06
Iter: 393 loss: 1.78949983e-06
Iter: 394 loss: 1.77465711e-06
Iter: 395 loss: 1.77282482e-06
Iter: 396 loss: 1.76952312e-06
Iter: 397 loss: 1.84686292e-06
Iter: 398 loss: 1.76955109e-06
Iter: 399 loss: 1.76643266e-06
Iter: 400 loss: 1.78572191e-06
Iter: 401 loss: 1.76602782e-06
Iter: 402 loss: 1.76331241e-06
Iter: 403 loss: 1.7639004e-06
Iter: 404 loss: 1.76129151e-06
Iter: 405 loss: 1.75810624e-06
Iter: 406 loss: 1.78592791e-06
Iter: 407 loss: 1.75800369e-06
Iter: 408 loss: 1.75527862e-06
Iter: 409 loss: 1.76169692e-06
Iter: 410 loss: 1.75424327e-06
Iter: 411 loss: 1.75196965e-06
Iter: 412 loss: 1.76656624e-06
Iter: 413 loss: 1.75173022e-06
Iter: 414 loss: 1.75046284e-06
Iter: 415 loss: 1.76446736e-06
Iter: 416 loss: 1.75044124e-06
Iter: 417 loss: 1.74890977e-06
Iter: 418 loss: 1.74770821e-06
Iter: 419 loss: 1.74724482e-06
Iter: 420 loss: 1.74556089e-06
Iter: 421 loss: 1.7446755e-06
Iter: 422 loss: 1.74396598e-06
Iter: 423 loss: 1.74169145e-06
Iter: 424 loss: 1.7420424e-06
Iter: 425 loss: 1.73996227e-06
Iter: 426 loss: 1.73805824e-06
Iter: 427 loss: 1.76755316e-06
Iter: 428 loss: 1.73805188e-06
Iter: 429 loss: 1.73635544e-06
Iter: 430 loss: 1.7380255e-06
Iter: 431 loss: 1.73542753e-06
Iter: 432 loss: 1.73277681e-06
Iter: 433 loss: 1.74047045e-06
Iter: 434 loss: 1.73190904e-06
Iter: 435 loss: 1.73014155e-06
Iter: 436 loss: 1.72749287e-06
Iter: 437 loss: 1.72744626e-06
Iter: 438 loss: 1.7237985e-06
Iter: 439 loss: 1.75343189e-06
Iter: 440 loss: 1.72352179e-06
Iter: 441 loss: 1.72165483e-06
Iter: 442 loss: 1.72614693e-06
Iter: 443 loss: 1.72099135e-06
Iter: 444 loss: 1.71847796e-06
Iter: 445 loss: 1.72617752e-06
Iter: 446 loss: 1.71773092e-06
Iter: 447 loss: 1.71559373e-06
Iter: 448 loss: 1.72405612e-06
Iter: 449 loss: 1.71502597e-06
Iter: 450 loss: 1.71366628e-06
Iter: 451 loss: 1.71366355e-06
Iter: 452 loss: 1.71220358e-06
Iter: 453 loss: 1.71071281e-06
Iter: 454 loss: 1.71042245e-06
Iter: 455 loss: 1.70909482e-06
Iter: 456 loss: 1.70803582e-06
Iter: 457 loss: 1.7076627e-06
Iter: 458 loss: 1.70521753e-06
Iter: 459 loss: 1.71059128e-06
Iter: 460 loss: 1.70433805e-06
Iter: 461 loss: 1.70241299e-06
Iter: 462 loss: 1.70735007e-06
Iter: 463 loss: 1.70173632e-06
Iter: 464 loss: 1.69948328e-06
Iter: 465 loss: 1.71157853e-06
Iter: 466 loss: 1.69909777e-06
Iter: 467 loss: 1.69744612e-06
Iter: 468 loss: 1.71245017e-06
Iter: 469 loss: 1.69739872e-06
Iter: 470 loss: 1.69616965e-06
Iter: 471 loss: 1.69365353e-06
Iter: 472 loss: 1.7416952e-06
Iter: 473 loss: 1.69363693e-06
Iter: 474 loss: 1.69147654e-06
Iter: 475 loss: 1.71077966e-06
Iter: 476 loss: 1.69135819e-06
Iter: 477 loss: 1.6891704e-06
Iter: 478 loss: 1.68820566e-06
Iter: 479 loss: 1.68713291e-06
Iter: 480 loss: 1.68583165e-06
Iter: 481 loss: 1.68556289e-06
Iter: 482 loss: 1.6844939e-06
Iter: 483 loss: 1.68413681e-06
Iter: 484 loss: 1.68346787e-06
Iter: 485 loss: 1.68241195e-06
Iter: 486 loss: 1.68230054e-06
Iter: 487 loss: 1.68140855e-06
Iter: 488 loss: 1.68003749e-06
Iter: 489 loss: 1.6800393e-06
Iter: 490 loss: 1.67884173e-06
Iter: 491 loss: 1.67724352e-06
Iter: 492 loss: 1.67717485e-06
Iter: 493 loss: 1.67497865e-06
Iter: 494 loss: 1.68650365e-06
Iter: 495 loss: 1.67464009e-06
Iter: 496 loss: 1.67273492e-06
Iter: 497 loss: 1.67835492e-06
Iter: 498 loss: 1.67214398e-06
Iter: 499 loss: 1.67037092e-06
Iter: 500 loss: 1.67977259e-06
Iter: 501 loss: 1.6701083e-06
Iter: 502 loss: 1.6689163e-06
Iter: 503 loss: 1.68248766e-06
Iter: 504 loss: 1.66891118e-06
Iter: 505 loss: 1.6679586e-06
Iter: 506 loss: 1.66637574e-06
Iter: 507 loss: 1.66644418e-06
Iter: 508 loss: 1.66483812e-06
Iter: 509 loss: 1.6697918e-06
Iter: 510 loss: 1.66438508e-06
Iter: 511 loss: 1.66252039e-06
Iter: 512 loss: 1.66618929e-06
Iter: 513 loss: 1.66168184e-06
Iter: 514 loss: 1.66022483e-06
Iter: 515 loss: 1.67454573e-06
Iter: 516 loss: 1.66018981e-06
Iter: 517 loss: 1.65886206e-06
Iter: 518 loss: 1.66446387e-06
Iter: 519 loss: 1.65855045e-06
Iter: 520 loss: 1.65702488e-06
Iter: 521 loss: 1.66739324e-06
Iter: 522 loss: 1.65693984e-06
Iter: 523 loss: 1.65635447e-06
Iter: 524 loss: 1.65537176e-06
Iter: 525 loss: 1.6553422e-06
Iter: 526 loss: 1.65403253e-06
Iter: 527 loss: 1.65255528e-06
Iter: 528 loss: 1.65235542e-06
Iter: 529 loss: 1.65048789e-06
Iter: 530 loss: 1.66362224e-06
Iter: 531 loss: 1.65026881e-06
Iter: 532 loss: 1.64849394e-06
Iter: 533 loss: 1.65025926e-06
Iter: 534 loss: 1.64741948e-06
Iter: 535 loss: 1.64548646e-06
Iter: 536 loss: 1.66109271e-06
Iter: 537 loss: 1.64540643e-06
Iter: 538 loss: 1.64424159e-06
Iter: 539 loss: 1.65246956e-06
Iter: 540 loss: 1.64414928e-06
Iter: 541 loss: 1.64296944e-06
Iter: 542 loss: 1.64293283e-06
Iter: 543 loss: 1.64203755e-06
Iter: 544 loss: 1.64056542e-06
Iter: 545 loss: 1.63992536e-06
Iter: 546 loss: 1.63907544e-06
Iter: 547 loss: 1.63731818e-06
Iter: 548 loss: 1.65667507e-06
Iter: 549 loss: 1.63723712e-06
Iter: 550 loss: 1.63602613e-06
Iter: 551 loss: 1.639365e-06
Iter: 552 loss: 1.6356239e-06
Iter: 553 loss: 1.63453262e-06
Iter: 554 loss: 1.6345258e-06
Iter: 555 loss: 1.63364246e-06
Iter: 556 loss: 1.63603454e-06
Iter: 557 loss: 1.63334607e-06
Iter: 558 loss: 1.63274865e-06
Iter: 559 loss: 1.63129539e-06
Iter: 560 loss: 1.64412779e-06
Iter: 561 loss: 1.63106438e-06
Iter: 562 loss: 1.62935225e-06
Iter: 563 loss: 1.63952609e-06
Iter: 564 loss: 1.6290569e-06
Iter: 565 loss: 1.62785545e-06
Iter: 566 loss: 1.62685274e-06
Iter: 567 loss: 1.62636e-06
Iter: 568 loss: 1.62460219e-06
Iter: 569 loss: 1.64539381e-06
Iter: 570 loss: 1.62457911e-06
Iter: 571 loss: 1.62330218e-06
Iter: 572 loss: 1.62467404e-06
Iter: 573 loss: 1.62262745e-06
Iter: 574 loss: 1.62124e-06
Iter: 575 loss: 1.62948368e-06
Iter: 576 loss: 1.62105471e-06
Iter: 577 loss: 1.6198951e-06
Iter: 578 loss: 1.6291076e-06
Iter: 579 loss: 1.61982769e-06
Iter: 580 loss: 1.61876869e-06
Iter: 581 loss: 1.61736511e-06
Iter: 582 loss: 1.61731737e-06
Iter: 583 loss: 1.61582398e-06
Iter: 584 loss: 1.62445599e-06
Iter: 585 loss: 1.61565663e-06
Iter: 586 loss: 1.61444166e-06
Iter: 587 loss: 1.61727053e-06
Iter: 588 loss: 1.61398054e-06
Iter: 589 loss: 1.61291609e-06
Iter: 590 loss: 1.62470667e-06
Iter: 591 loss: 1.61288722e-06
Iter: 592 loss: 1.61219953e-06
Iter: 593 loss: 1.61939215e-06
Iter: 594 loss: 1.6121686e-06
Iter: 595 loss: 1.61139792e-06
Iter: 596 loss: 1.61026037e-06
Iter: 597 loss: 1.61021967e-06
Iter: 598 loss: 1.6093104e-06
Iter: 599 loss: 1.60904779e-06
Iter: 600 loss: 1.60852665e-06
Iter: 601 loss: 1.6069107e-06
Iter: 602 loss: 1.60830893e-06
Iter: 603 loss: 1.60592583e-06
Iter: 604 loss: 1.60445916e-06
Iter: 605 loss: 1.61037519e-06
Iter: 606 loss: 1.60407126e-06
Iter: 607 loss: 1.60252387e-06
Iter: 608 loss: 1.60653462e-06
Iter: 609 loss: 1.60195987e-06
Iter: 610 loss: 1.60049558e-06
Iter: 611 loss: 1.60661102e-06
Iter: 612 loss: 1.60018737e-06
Iter: 613 loss: 1.59883189e-06
Iter: 614 loss: 1.60623858e-06
Iter: 615 loss: 1.59864612e-06
Iter: 616 loss: 1.5974083e-06
Iter: 617 loss: 1.6016586e-06
Iter: 618 loss: 1.59700016e-06
Iter: 619 loss: 1.5961989e-06
Iter: 620 loss: 1.59543742e-06
Iter: 621 loss: 1.59520255e-06
Iter: 622 loss: 1.59372053e-06
Iter: 623 loss: 1.60061222e-06
Iter: 624 loss: 1.59341857e-06
Iter: 625 loss: 1.59219735e-06
Iter: 626 loss: 1.59703154e-06
Iter: 627 loss: 1.59191734e-06
Iter: 628 loss: 1.5910191e-06
Iter: 629 loss: 1.60381944e-06
Iter: 630 loss: 1.59103706e-06
Iter: 631 loss: 1.59022579e-06
Iter: 632 loss: 1.59640649e-06
Iter: 633 loss: 1.59016952e-06
Iter: 634 loss: 1.58975683e-06
Iter: 635 loss: 1.58857233e-06
Iter: 636 loss: 1.59451781e-06
Iter: 637 loss: 1.58814282e-06
Iter: 638 loss: 1.58665125e-06
Iter: 639 loss: 1.59440856e-06
Iter: 640 loss: 1.58641762e-06
Iter: 641 loss: 1.58517992e-06
Iter: 642 loss: 1.5847088e-06
Iter: 643 loss: 1.58398802e-06
Iter: 644 loss: 1.58246587e-06
Iter: 645 loss: 1.59953083e-06
Iter: 646 loss: 1.58249111e-06
Iter: 647 loss: 1.58128034e-06
Iter: 648 loss: 1.58206137e-06
Iter: 649 loss: 1.58056775e-06
Iter: 650 loss: 1.57922454e-06
Iter: 651 loss: 1.5911304e-06
Iter: 652 loss: 1.57912621e-06
Iter: 653 loss: 1.57816589e-06
Iter: 654 loss: 1.58418698e-06
Iter: 655 loss: 1.57807085e-06
Iter: 656 loss: 1.57725776e-06
Iter: 657 loss: 1.57582815e-06
Iter: 658 loss: 1.6073227e-06
Iter: 659 loss: 1.57579609e-06
Iter: 660 loss: 1.57424302e-06
Iter: 661 loss: 1.5852587e-06
Iter: 662 loss: 1.57411171e-06
Iter: 663 loss: 1.57280078e-06
Iter: 664 loss: 1.57615784e-06
Iter: 665 loss: 1.5723399e-06
Iter: 666 loss: 1.57157092e-06
Iter: 667 loss: 1.57151726e-06
Iter: 668 loss: 1.5708863e-06
Iter: 669 loss: 1.57343652e-06
Iter: 670 loss: 1.5707592e-06
Iter: 671 loss: 1.57026955e-06
Iter: 672 loss: 1.56894498e-06
Iter: 673 loss: 1.57968248e-06
Iter: 674 loss: 1.56876081e-06
Iter: 675 loss: 1.56729675e-06
Iter: 676 loss: 1.57128022e-06
Iter: 677 loss: 1.56685428e-06
Iter: 678 loss: 1.56518456e-06
Iter: 679 loss: 1.56772285e-06
Iter: 680 loss: 1.56440456e-06
Iter: 681 loss: 1.56309443e-06
Iter: 682 loss: 1.56970714e-06
Iter: 683 loss: 1.56288559e-06
Iter: 684 loss: 1.56149827e-06
Iter: 685 loss: 1.56480337e-06
Iter: 686 loss: 1.56107035e-06
Iter: 687 loss: 1.5599386e-06
Iter: 688 loss: 1.56687622e-06
Iter: 689 loss: 1.55981445e-06
Iter: 690 loss: 1.55876887e-06
Iter: 691 loss: 1.56280066e-06
Iter: 692 loss: 1.55847965e-06
Iter: 693 loss: 1.55742646e-06
Iter: 694 loss: 1.55859971e-06
Iter: 695 loss: 1.55684654e-06
Iter: 696 loss: 1.55598445e-06
Iter: 697 loss: 1.55622342e-06
Iter: 698 loss: 1.55540579e-06
Iter: 699 loss: 1.55421151e-06
Iter: 700 loss: 1.56105273e-06
Iter: 701 loss: 1.55407793e-06
Iter: 702 loss: 1.55331441e-06
Iter: 703 loss: 1.55334942e-06
Iter: 704 loss: 1.55269709e-06
Iter: 705 loss: 1.55475891e-06
Iter: 706 loss: 1.55249359e-06
Iter: 707 loss: 1.55186115e-06
Iter: 708 loss: 1.55086968e-06
Iter: 709 loss: 1.5508117e-06
Iter: 710 loss: 1.54985173e-06
Iter: 711 loss: 1.54985116e-06
Iter: 712 loss: 1.54913209e-06
Iter: 713 loss: 1.547804e-06
Iter: 714 loss: 1.55387227e-06
Iter: 715 loss: 1.54757549e-06
Iter: 716 loss: 1.54649092e-06
Iter: 717 loss: 1.54634779e-06
Iter: 718 loss: 1.54558927e-06
Iter: 719 loss: 1.54411509e-06
Iter: 720 loss: 1.55775297e-06
Iter: 721 loss: 1.5440379e-06
Iter: 722 loss: 1.54301631e-06
Iter: 723 loss: 1.54462384e-06
Iter: 724 loss: 1.54253075e-06
Iter: 725 loss: 1.54146699e-06
Iter: 726 loss: 1.55261716e-06
Iter: 727 loss: 1.54148324e-06
Iter: 728 loss: 1.54072825e-06
Iter: 729 loss: 1.54197028e-06
Iter: 730 loss: 1.54032898e-06
Iter: 731 loss: 1.53941846e-06
Iter: 732 loss: 1.53789256e-06
Iter: 733 loss: 1.53791177e-06
Iter: 734 loss: 1.53674728e-06
Iter: 735 loss: 1.53673079e-06
Iter: 736 loss: 1.53586143e-06
Iter: 737 loss: 1.53992301e-06
Iter: 738 loss: 1.53582982e-06
Iter: 739 loss: 1.53509882e-06
Iter: 740 loss: 1.54516465e-06
Iter: 741 loss: 1.53507335e-06
Iter: 742 loss: 1.5346352e-06
Iter: 743 loss: 1.53434394e-06
Iter: 744 loss: 1.53414658e-06
Iter: 745 loss: 1.53354063e-06
Iter: 746 loss: 1.53232418e-06
Iter: 747 loss: 1.55819953e-06
Iter: 748 loss: 1.53231099e-06
Iter: 749 loss: 1.5312105e-06
Iter: 750 loss: 1.53851909e-06
Iter: 751 loss: 1.53109022e-06
Iter: 752 loss: 1.52997791e-06
Iter: 753 loss: 1.53064263e-06
Iter: 754 loss: 1.52928897e-06
Iter: 755 loss: 1.5280591e-06
Iter: 756 loss: 1.5322812e-06
Iter: 757 loss: 1.5277144e-06
Iter: 758 loss: 1.52643202e-06
Iter: 759 loss: 1.53167798e-06
Iter: 760 loss: 1.5261063e-06
Iter: 761 loss: 1.52521397e-06
Iter: 762 loss: 1.52893926e-06
Iter: 763 loss: 1.52497591e-06
Iter: 764 loss: 1.52399468e-06
Iter: 765 loss: 1.52841483e-06
Iter: 766 loss: 1.52384723e-06
Iter: 767 loss: 1.52291705e-06
Iter: 768 loss: 1.5258255e-06
Iter: 769 loss: 1.5226193e-06
Iter: 770 loss: 1.52194571e-06
Iter: 771 loss: 1.52131804e-06
Iter: 772 loss: 1.52116604e-06
Iter: 773 loss: 1.52030771e-06
Iter: 774 loss: 1.53294991e-06
Iter: 775 loss: 1.520296e-06
Iter: 776 loss: 1.51980612e-06
Iter: 777 loss: 1.51976349e-06
Iter: 778 loss: 1.51944619e-06
Iter: 779 loss: 1.51853874e-06
Iter: 780 loss: 1.52247276e-06
Iter: 781 loss: 1.51825498e-06
Iter: 782 loss: 1.51708696e-06
Iter: 783 loss: 1.52910593e-06
Iter: 784 loss: 1.51712095e-06
Iter: 785 loss: 1.51639233e-06
Iter: 786 loss: 1.51553604e-06
Iter: 787 loss: 1.51542702e-06
Iter: 788 loss: 1.51428048e-06
Iter: 789 loss: 1.5204256e-06
Iter: 790 loss: 1.51409347e-06
Iter: 791 loss: 1.51312929e-06
Iter: 792 loss: 1.51340805e-06
Iter: 793 loss: 1.51242364e-06
Iter: 794 loss: 1.5113236e-06
Iter: 795 loss: 1.51889981e-06
Iter: 796 loss: 1.51124425e-06
Iter: 797 loss: 1.51037557e-06
Iter: 798 loss: 1.51111817e-06
Iter: 799 loss: 1.50980622e-06
Iter: 800 loss: 1.50876201e-06
Iter: 801 loss: 1.51593315e-06
Iter: 802 loss: 1.50859819e-06
Iter: 803 loss: 1.50789094e-06
Iter: 804 loss: 1.50758274e-06
Iter: 805 loss: 1.50720234e-06
Iter: 806 loss: 1.50637243e-06
Iter: 807 loss: 1.51636982e-06
Iter: 808 loss: 1.50636083e-06
Iter: 809 loss: 1.50576443e-06
Iter: 810 loss: 1.50574715e-06
Iter: 811 loss: 1.50522078e-06
Iter: 812 loss: 1.50562528e-06
Iter: 813 loss: 1.50490644e-06
Iter: 814 loss: 1.50431924e-06
Iter: 815 loss: 1.50410051e-06
Iter: 816 loss: 1.50373717e-06
Iter: 817 loss: 1.50303504e-06
Iter: 818 loss: 1.50250958e-06
Iter: 819 loss: 1.5022589e-06
Iter: 820 loss: 1.50149503e-06
Iter: 821 loss: 1.51304766e-06
Iter: 822 loss: 1.50148367e-06
Iter: 823 loss: 1.50081121e-06
Iter: 824 loss: 1.50116534e-06
Iter: 825 loss: 1.5003543e-06
Iter: 826 loss: 1.49937637e-06
Iter: 827 loss: 1.4999373e-06
Iter: 828 loss: 1.49867583e-06
Iter: 829 loss: 1.4975634e-06
Iter: 830 loss: 1.49898767e-06
Iter: 831 loss: 1.49705147e-06
Iter: 832 loss: 1.49604739e-06
Iter: 833 loss: 1.50423409e-06
Iter: 834 loss: 1.49592699e-06
Iter: 835 loss: 1.49513767e-06
Iter: 836 loss: 1.49493803e-06
Iter: 837 loss: 1.49441e-06
Iter: 838 loss: 1.49321488e-06
Iter: 839 loss: 1.50116284e-06
Iter: 840 loss: 1.4931104e-06
Iter: 841 loss: 1.49214543e-06
Iter: 842 loss: 1.4942774e-06
Iter: 843 loss: 1.4918171e-06
Iter: 844 loss: 1.49152038e-06
Iter: 845 loss: 1.49122627e-06
Iter: 846 loss: 1.4910006e-06
Iter: 847 loss: 1.49050095e-06
Iter: 848 loss: 1.49936398e-06
Iter: 849 loss: 1.49046252e-06
Iter: 850 loss: 1.48992194e-06
Iter: 851 loss: 1.4914898e-06
Iter: 852 loss: 1.48980041e-06
Iter: 853 loss: 1.48917968e-06
Iter: 854 loss: 1.49061123e-06
Iter: 855 loss: 1.48898039e-06
Iter: 856 loss: 1.48844231e-06
Iter: 857 loss: 1.48772017e-06
Iter: 858 loss: 1.48763468e-06
Iter: 859 loss: 1.48669778e-06
Iter: 860 loss: 1.49456787e-06
Iter: 861 loss: 1.48657273e-06
Iter: 862 loss: 1.48593449e-06
Iter: 863 loss: 1.48978211e-06
Iter: 864 loss: 1.48583717e-06
Iter: 865 loss: 1.48520587e-06
Iter: 866 loss: 1.48546428e-06
Iter: 867 loss: 1.48474464e-06
Iter: 868 loss: 1.48401489e-06
Iter: 869 loss: 1.48448305e-06
Iter: 870 loss: 1.48358595e-06
Iter: 871 loss: 1.48241384e-06
Iter: 872 loss: 1.48487527e-06
Iter: 873 loss: 1.48199604e-06
Iter: 874 loss: 1.48095501e-06
Iter: 875 loss: 1.48319486e-06
Iter: 876 loss: 1.48054983e-06
Iter: 877 loss: 1.48000117e-06
Iter: 878 loss: 1.47987362e-06
Iter: 879 loss: 1.47920912e-06
Iter: 880 loss: 1.48139543e-06
Iter: 881 loss: 1.4790171e-06
Iter: 882 loss: 1.47868855e-06
Iter: 883 loss: 1.47787614e-06
Iter: 884 loss: 1.48424181e-06
Iter: 885 loss: 1.47773062e-06
Iter: 886 loss: 1.47705953e-06
Iter: 887 loss: 1.47702781e-06
Iter: 888 loss: 1.47654077e-06
Iter: 889 loss: 1.47639082e-06
Iter: 890 loss: 1.47608989e-06
Iter: 891 loss: 1.47533831e-06
Iter: 892 loss: 1.47609603e-06
Iter: 893 loss: 1.47491028e-06
Iter: 894 loss: 1.47428455e-06
Iter: 895 loss: 1.47636752e-06
Iter: 896 loss: 1.47413061e-06
Iter: 897 loss: 1.4734934e-06
Iter: 898 loss: 1.47634194e-06
Iter: 899 loss: 1.47328046e-06
Iter: 900 loss: 1.47270089e-06
Iter: 901 loss: 1.47433104e-06
Iter: 902 loss: 1.47250375e-06
Iter: 903 loss: 1.47176729e-06
Iter: 904 loss: 1.47127503e-06
Iter: 905 loss: 1.4709774e-06
Iter: 906 loss: 1.47008109e-06
Iter: 907 loss: 1.47483604e-06
Iter: 908 loss: 1.4699458e-06
Iter: 909 loss: 1.46904358e-06
Iter: 910 loss: 1.47131959e-06
Iter: 911 loss: 1.46876016e-06
Iter: 912 loss: 1.46913089e-06
Iter: 913 loss: 1.46843809e-06
Iter: 914 loss: 1.4682862e-06
Iter: 915 loss: 1.46762613e-06
Iter: 916 loss: 1.46849425e-06
Iter: 917 loss: 1.4671599e-06
Iter: 918 loss: 1.46617924e-06
Iter: 919 loss: 1.47049707e-06
Iter: 920 loss: 1.46597199e-06
Iter: 921 loss: 1.46533444e-06
Iter: 922 loss: 1.47145954e-06
Iter: 923 loss: 1.46531897e-06
Iter: 924 loss: 1.46460798e-06
Iter: 925 loss: 1.46460707e-06
Iter: 926 loss: 1.46410355e-06
Iter: 927 loss: 1.46333241e-06
Iter: 928 loss: 1.46628577e-06
Iter: 929 loss: 1.46312459e-06
Iter: 930 loss: 1.46253444e-06
Iter: 931 loss: 1.46232242e-06
Iter: 932 loss: 1.46195646e-06
Iter: 933 loss: 1.46119055e-06
Iter: 934 loss: 1.46553475e-06
Iter: 935 loss: 1.4611403e-06
Iter: 936 loss: 1.46034336e-06
Iter: 937 loss: 1.46258731e-06
Iter: 938 loss: 1.4601269e-06
Iter: 939 loss: 1.45946342e-06
Iter: 940 loss: 1.4641148e-06
Iter: 941 loss: 1.45937418e-06
Iter: 942 loss: 1.45879312e-06
Iter: 943 loss: 1.45823765e-06
Iter: 944 loss: 1.45810441e-06
Iter: 945 loss: 1.45766126e-06
Iter: 946 loss: 1.45768263e-06
Iter: 947 loss: 1.45716615e-06
Iter: 948 loss: 1.45897275e-06
Iter: 949 loss: 1.45705883e-06
Iter: 950 loss: 1.45668969e-06
Iter: 951 loss: 1.45596937e-06
Iter: 952 loss: 1.46632487e-06
Iter: 953 loss: 1.45588126e-06
Iter: 954 loss: 1.455237e-06
Iter: 955 loss: 1.45735e-06
Iter: 956 loss: 1.45496142e-06
Iter: 957 loss: 1.45414174e-06
Iter: 958 loss: 1.45504e-06
Iter: 959 loss: 1.45368153e-06
Iter: 960 loss: 1.45319189e-06
Iter: 961 loss: 1.45321246e-06
Iter: 962 loss: 1.45274521e-06
Iter: 963 loss: 1.45286867e-06
Iter: 964 loss: 1.4524403e-06
Iter: 965 loss: 1.45188e-06
Iter: 966 loss: 1.45308627e-06
Iter: 967 loss: 1.45162494e-06
Iter: 968 loss: 1.45103672e-06
Iter: 969 loss: 1.45058971e-06
Iter: 970 loss: 1.45037575e-06
Iter: 971 loss: 1.44959631e-06
Iter: 972 loss: 1.45720469e-06
Iter: 973 loss: 1.44957369e-06
Iter: 974 loss: 1.44899218e-06
Iter: 975 loss: 1.45032857e-06
Iter: 976 loss: 1.4488005e-06
Iter: 977 loss: 1.4481916e-06
Iter: 978 loss: 1.45186561e-06
Iter: 979 loss: 1.44812793e-06
Iter: 980 loss: 1.44767955e-06
Iter: 981 loss: 1.44986495e-06
Iter: 982 loss: 1.44761577e-06
Iter: 983 loss: 1.44703972e-06
Iter: 984 loss: 1.44736123e-06
Iter: 985 loss: 1.44665432e-06
Iter: 986 loss: 1.44616604e-06
Iter: 987 loss: 1.44574449e-06
Iter: 988 loss: 1.44561591e-06
Iter: 989 loss: 1.44492287e-06
Iter: 990 loss: 1.44550938e-06
Iter: 991 loss: 1.44452326e-06
Iter: 992 loss: 1.4437469e-06
Iter: 993 loss: 1.44848821e-06
Iter: 994 loss: 1.44364071e-06
Iter: 995 loss: 1.4429844e-06
Iter: 996 loss: 1.44378396e-06
Iter: 997 loss: 1.44265664e-06
Iter: 998 loss: 1.44196042e-06
Iter: 999 loss: 1.45002753e-06
Iter: 1000 loss: 1.44189517e-06
Iter: 1001 loss: 1.44154319e-06
Iter: 1002 loss: 1.44193245e-06
Iter: 1003 loss: 1.44132014e-06
Iter: 1004 loss: 1.44078786e-06
Iter: 1005 loss: 1.44044043e-06
Iter: 1006 loss: 1.44023466e-06
Iter: 1007 loss: 1.43947125e-06
Iter: 1008 loss: 1.44278238e-06
Iter: 1009 loss: 1.43935313e-06
Iter: 1010 loss: 1.4384982e-06
Iter: 1011 loss: 1.43971965e-06
Iter: 1012 loss: 1.43804186e-06
Iter: 1013 loss: 1.4374391e-06
Iter: 1014 loss: 1.4471334e-06
Iter: 1015 loss: 1.43742795e-06
Iter: 1016 loss: 1.43694365e-06
Iter: 1017 loss: 1.44022533e-06
Iter: 1018 loss: 1.43688248e-06
Iter: 1019 loss: 1.4363693e-06
Iter: 1020 loss: 1.43670673e-06
Iter: 1021 loss: 1.43602904e-06
Iter: 1022 loss: 1.43569457e-06
Iter: 1023 loss: 1.43527973e-06
Iter: 1024 loss: 1.43520469e-06
Iter: 1025 loss: 1.43458055e-06
Iter: 1026 loss: 1.43532156e-06
Iter: 1027 loss: 1.43418856e-06
Iter: 1028 loss: 1.43353964e-06
Iter: 1029 loss: 1.43694433e-06
Iter: 1030 loss: 1.43347461e-06
Iter: 1031 loss: 1.43278135e-06
Iter: 1032 loss: 1.43371653e-06
Iter: 1033 loss: 1.43246643e-06
Iter: 1034 loss: 1.4319495e-06
Iter: 1035 loss: 1.43192972e-06
Iter: 1036 loss: 1.43165244e-06
Iter: 1037 loss: 1.43120087e-06
Iter: 1038 loss: 1.43115358e-06
Iter: 1039 loss: 1.43043576e-06
Iter: 1040 loss: 1.43177817e-06
Iter: 1041 loss: 1.43016769e-06
Iter: 1042 loss: 1.42960653e-06
Iter: 1043 loss: 1.43031912e-06
Iter: 1044 loss: 1.42923375e-06
Iter: 1045 loss: 1.42847796e-06
Iter: 1046 loss: 1.43289617e-06
Iter: 1047 loss: 1.4283579e-06
Iter: 1048 loss: 1.42783176e-06
Iter: 1049 loss: 1.43177806e-06
Iter: 1050 loss: 1.42778322e-06
Iter: 1051 loss: 1.42744966e-06
Iter: 1052 loss: 1.4320367e-06
Iter: 1053 loss: 1.42746103e-06
Iter: 1054 loss: 1.42709303e-06
Iter: 1055 loss: 1.42696e-06
Iter: 1056 loss: 1.42673048e-06
Iter: 1057 loss: 1.4264217e-06
Iter: 1058 loss: 1.42586077e-06
Iter: 1059 loss: 1.42586305e-06
Iter: 1060 loss: 1.42518434e-06
Iter: 1061 loss: 1.42780584e-06
Iter: 1062 loss: 1.42499459e-06
Iter: 1063 loss: 1.42437216e-06
Iter: 1064 loss: 1.42522822e-06
Iter: 1065 loss: 1.42408544e-06
Iter: 1066 loss: 1.42344163e-06
Iter: 1067 loss: 1.42780743e-06
Iter: 1068 loss: 1.42335944e-06
Iter: 1069 loss: 1.42290185e-06
Iter: 1070 loss: 1.42652084e-06
Iter: 1071 loss: 1.42281942e-06
Iter: 1072 loss: 1.42240606e-06
Iter: 1073 loss: 1.42167994e-06
Iter: 1074 loss: 1.42163697e-06
Iter: 1075 loss: 1.42082695e-06
Iter: 1076 loss: 1.42591352e-06
Iter: 1077 loss: 1.42070826e-06
Iter: 1078 loss: 1.42007116e-06
Iter: 1079 loss: 1.41962084e-06
Iter: 1080 loss: 1.41937971e-06
Iter: 1081 loss: 1.41870066e-06
Iter: 1082 loss: 1.42816566e-06
Iter: 1083 loss: 1.41870555e-06
Iter: 1084 loss: 1.41813064e-06
Iter: 1085 loss: 1.41984651e-06
Iter: 1086 loss: 1.41799023e-06
Iter: 1087 loss: 1.41758233e-06
Iter: 1088 loss: 1.41759688e-06
Iter: 1089 loss: 1.41721773e-06
Iter: 1090 loss: 1.41734654e-06
Iter: 1091 loss: 1.41696694e-06
Iter: 1092 loss: 1.41667169e-06
Iter: 1093 loss: 1.41598639e-06
Iter: 1094 loss: 1.42686963e-06
Iter: 1095 loss: 1.41596195e-06
Iter: 1096 loss: 1.41525766e-06
Iter: 1097 loss: 1.4211588e-06
Iter: 1098 loss: 1.41518422e-06
Iter: 1099 loss: 1.41464739e-06
Iter: 1100 loss: 1.41466899e-06
Iter: 1101 loss: 1.41421526e-06
Iter: 1102 loss: 1.41356918e-06
Iter: 1103 loss: 1.4161385e-06
Iter: 1104 loss: 1.41347596e-06
Iter: 1105 loss: 1.41269959e-06
Iter: 1106 loss: 1.41379917e-06
Iter: 1107 loss: 1.41232954e-06
Iter: 1108 loss: 1.4116813e-06
Iter: 1109 loss: 1.41511714e-06
Iter: 1110 loss: 1.41152304e-06
Iter: 1111 loss: 1.41083342e-06
Iter: 1112 loss: 1.41235569e-06
Iter: 1113 loss: 1.41057512e-06
Iter: 1114 loss: 1.4101513e-06
Iter: 1115 loss: 1.41018791e-06
Iter: 1116 loss: 1.40992267e-06
Iter: 1117 loss: 1.4093805e-06
Iter: 1118 loss: 1.40940597e-06
Iter: 1119 loss: 1.40878228e-06
Iter: 1120 loss: 1.41163014e-06
Iter: 1121 loss: 1.40866109e-06
Iter: 1122 loss: 1.40818122e-06
Iter: 1123 loss: 1.40815303e-06
Iter: 1124 loss: 1.40784653e-06
Iter: 1125 loss: 1.40969451e-06
Iter: 1126 loss: 1.40781435e-06
Iter: 1127 loss: 1.40751433e-06
Iter: 1128 loss: 1.4069908e-06
Iter: 1129 loss: 1.41896498e-06
Iter: 1130 loss: 1.40698944e-06
Iter: 1131 loss: 1.40652196e-06
Iter: 1132 loss: 1.40667328e-06
Iter: 1133 loss: 1.40620841e-06
Iter: 1134 loss: 1.40555267e-06
Iter: 1135 loss: 1.40847453e-06
Iter: 1136 loss: 1.40543216e-06
Iter: 1137 loss: 1.40490602e-06
Iter: 1138 loss: 1.40454563e-06
Iter: 1139 loss: 1.40428131e-06
Iter: 1140 loss: 1.40346299e-06
Iter: 1141 loss: 1.41073247e-06
Iter: 1142 loss: 1.40337181e-06
Iter: 1143 loss: 1.40272209e-06
Iter: 1144 loss: 1.40318593e-06
Iter: 1145 loss: 1.40232419e-06
Iter: 1146 loss: 1.40164559e-06
Iter: 1147 loss: 1.40753707e-06
Iter: 1148 loss: 1.40162433e-06
Iter: 1149 loss: 1.40114162e-06
Iter: 1150 loss: 1.40300608e-06
Iter: 1151 loss: 1.40106442e-06
Iter: 1152 loss: 1.40039458e-06
Iter: 1153 loss: 1.40039333e-06
Iter: 1154 loss: 1.39994006e-06
Iter: 1155 loss: 1.39944791e-06
Iter: 1156 loss: 1.40325847e-06
Iter: 1157 loss: 1.39941494e-06
Iter: 1158 loss: 1.39907024e-06
Iter: 1159 loss: 1.40373413e-06
Iter: 1160 loss: 1.39904569e-06
Iter: 1161 loss: 1.39864551e-06
Iter: 1162 loss: 1.39875976e-06
Iter: 1163 loss: 1.39839335e-06
Iter: 1164 loss: 1.39807594e-06
Iter: 1165 loss: 1.39815563e-06
Iter: 1166 loss: 1.39785823e-06
Iter: 1167 loss: 1.39744145e-06
Iter: 1168 loss: 1.39700614e-06
Iter: 1169 loss: 1.39691838e-06
Iter: 1170 loss: 1.39630379e-06
Iter: 1171 loss: 1.40082352e-06
Iter: 1172 loss: 1.39620477e-06
Iter: 1173 loss: 1.39562121e-06
Iter: 1174 loss: 1.39578901e-06
Iter: 1175 loss: 1.39519761e-06
Iter: 1176 loss: 1.39455017e-06
Iter: 1177 loss: 1.3983589e-06
Iter: 1178 loss: 1.39447263e-06
Iter: 1179 loss: 1.3938984e-06
Iter: 1180 loss: 1.3951659e-06
Iter: 1181 loss: 1.393704e-06
Iter: 1182 loss: 1.39318718e-06
Iter: 1183 loss: 1.39581232e-06
Iter: 1184 loss: 1.39312283e-06
Iter: 1185 loss: 1.39256872e-06
Iter: 1186 loss: 1.39415988e-06
Iter: 1187 loss: 1.39246345e-06
Iter: 1188 loss: 1.39190092e-06
Iter: 1189 loss: 1.39379267e-06
Iter: 1190 loss: 1.39175518e-06
Iter: 1191 loss: 1.39135602e-06
Iter: 1192 loss: 1.39083227e-06
Iter: 1193 loss: 1.3907752e-06
Iter: 1194 loss: 1.39115321e-06
Iter: 1195 loss: 1.39043209e-06
Iter: 1196 loss: 1.39032534e-06
Iter: 1197 loss: 1.3899305e-06
Iter: 1198 loss: 1.39511098e-06
Iter: 1199 loss: 1.38988605e-06
Iter: 1200 loss: 1.38947507e-06
Iter: 1201 loss: 1.38922849e-06
Iter: 1202 loss: 1.38903579e-06
Iter: 1203 loss: 1.38854182e-06
Iter: 1204 loss: 1.39075769e-06
Iter: 1205 loss: 1.38846872e-06
Iter: 1206 loss: 1.38789346e-06
Iter: 1207 loss: 1.39000849e-06
Iter: 1208 loss: 1.38768314e-06
Iter: 1209 loss: 1.38724249e-06
Iter: 1210 loss: 1.3892427e-06
Iter: 1211 loss: 1.38715143e-06
Iter: 1212 loss: 1.38671351e-06
Iter: 1213 loss: 1.38665087e-06
Iter: 1214 loss: 1.3863845e-06
Iter: 1215 loss: 1.38588075e-06
Iter: 1216 loss: 1.3883498e-06
Iter: 1217 loss: 1.38582652e-06
Iter: 1218 loss: 1.38538803e-06
Iter: 1219 loss: 1.38544874e-06
Iter: 1220 loss: 1.38502e-06
Iter: 1221 loss: 1.38437576e-06
Iter: 1222 loss: 1.38754217e-06
Iter: 1223 loss: 1.38429039e-06
Iter: 1224 loss: 1.38364487e-06
Iter: 1225 loss: 1.38476059e-06
Iter: 1226 loss: 1.38333394e-06
Iter: 1227 loss: 1.38312839e-06
Iter: 1228 loss: 1.3830836e-06
Iter: 1229 loss: 1.38280598e-06
Iter: 1230 loss: 1.3836991e-06
Iter: 1231 loss: 1.38274686e-06
Iter: 1232 loss: 1.38242376e-06
Iter: 1233 loss: 1.38188716e-06
Iter: 1234 loss: 1.38184987e-06
Iter: 1235 loss: 1.38134362e-06
Iter: 1236 loss: 1.38279393e-06
Iter: 1237 loss: 1.38112591e-06
Iter: 1238 loss: 1.38069777e-06
Iter: 1239 loss: 1.38019846e-06
Iter: 1240 loss: 1.38018163e-06
Iter: 1241 loss: 1.3796132e-06
Iter: 1242 loss: 1.37960683e-06
Iter: 1243 loss: 1.3792494e-06
Iter: 1244 loss: 1.37970073e-06
Iter: 1245 loss: 1.3790052e-06
Iter: 1246 loss: 1.37847462e-06
Iter: 1247 loss: 1.37907318e-06
Iter: 1248 loss: 1.37820643e-06
Iter: 1249 loss: 1.37776749e-06
Iter: 1250 loss: 1.37803477e-06
Iter: 1251 loss: 1.37742313e-06
Iter: 1252 loss: 1.3768248e-06
Iter: 1253 loss: 1.38041548e-06
Iter: 1254 loss: 1.37671e-06
Iter: 1255 loss: 1.37621976e-06
Iter: 1256 loss: 1.3765117e-06
Iter: 1257 loss: 1.37595453e-06
Iter: 1258 loss: 1.37535903e-06
Iter: 1259 loss: 1.38051473e-06
Iter: 1260 loss: 1.37534107e-06
Iter: 1261 loss: 1.37504958e-06
Iter: 1262 loss: 1.3750489e-06
Iter: 1263 loss: 1.37476241e-06
Iter: 1264 loss: 1.37522306e-06
Iter: 1265 loss: 1.37462041e-06
Iter: 1266 loss: 1.37428094e-06
Iter: 1267 loss: 1.37414622e-06
Iter: 1268 loss: 1.37402981e-06
Iter: 1269 loss: 1.37365635e-06
Iter: 1270 loss: 1.37379732e-06
Iter: 1271 loss: 1.37345523e-06
Iter: 1272 loss: 1.37290942e-06
Iter: 1273 loss: 1.37277925e-06
Iter: 1274 loss: 1.37240249e-06
Iter: 1275 loss: 1.37196571e-06
Iter: 1276 loss: 1.37196503e-06
Iter: 1277 loss: 1.3715503e-06
Iter: 1278 loss: 1.37132679e-06
Iter: 1279 loss: 1.37115751e-06
Iter: 1280 loss: 1.37050131e-06
Iter: 1281 loss: 1.37283837e-06
Iter: 1282 loss: 1.37036113e-06
Iter: 1283 loss: 1.36982e-06
Iter: 1284 loss: 1.36949916e-06
Iter: 1285 loss: 1.36926474e-06
Iter: 1286 loss: 1.36857489e-06
Iter: 1287 loss: 1.37536927e-06
Iter: 1288 loss: 1.36847359e-06
Iter: 1289 loss: 1.36800327e-06
Iter: 1290 loss: 1.3679836e-06
Iter: 1291 loss: 1.36765459e-06
Iter: 1292 loss: 1.36747281e-06
Iter: 1293 loss: 1.36734388e-06
Iter: 1294 loss: 1.36715505e-06
Iter: 1295 loss: 1.36736094e-06
Iter: 1296 loss: 1.36702124e-06
Iter: 1297 loss: 1.36673725e-06
Iter: 1298 loss: 1.36714539e-06
Iter: 1299 loss: 1.36661913e-06
Iter: 1300 loss: 1.36630103e-06
Iter: 1301 loss: 1.36615574e-06
Iter: 1302 loss: 1.36601318e-06
Iter: 1303 loss: 1.36561584e-06
Iter: 1304 loss: 1.36553672e-06
Iter: 1305 loss: 1.36527842e-06
Iter: 1306 loss: 1.36468338e-06
Iter: 1307 loss: 1.36775463e-06
Iter: 1308 loss: 1.36454628e-06
Iter: 1309 loss: 1.364117e-06
Iter: 1310 loss: 1.36488711e-06
Iter: 1311 loss: 1.36397489e-06
Iter: 1312 loss: 1.36338815e-06
Iter: 1313 loss: 1.3658514e-06
Iter: 1314 loss: 1.36323729e-06
Iter: 1315 loss: 1.36277083e-06
Iter: 1316 loss: 1.36360268e-06
Iter: 1317 loss: 1.3626186e-06
Iter: 1318 loss: 1.3620903e-06
Iter: 1319 loss: 1.36186486e-06
Iter: 1320 loss: 1.36155472e-06
Iter: 1321 loss: 1.36103927e-06
Iter: 1322 loss: 1.36678011e-06
Iter: 1323 loss: 1.36103222e-06
Iter: 1324 loss: 1.36059339e-06
Iter: 1325 loss: 1.36058952e-06
Iter: 1326 loss: 1.36021208e-06
Iter: 1327 loss: 1.36025506e-06
Iter: 1328 loss: 1.35997186e-06
Iter: 1329 loss: 1.35970743e-06
Iter: 1330 loss: 1.35923381e-06
Iter: 1331 loss: 1.35927303e-06
Iter: 1332 loss: 1.35873358e-06
Iter: 1333 loss: 1.36063477e-06
Iter: 1334 loss: 1.35858545e-06
Iter: 1335 loss: 1.35820244e-06
Iter: 1336 loss: 1.35849245e-06
Iter: 1337 loss: 1.35795472e-06
Iter: 1338 loss: 1.35749087e-06
Iter: 1339 loss: 1.35766663e-06
Iter: 1340 loss: 1.3571962e-06
Iter: 1341 loss: 1.3567759e-06
Iter: 1342 loss: 1.35812707e-06
Iter: 1343 loss: 1.35665562e-06
Iter: 1344 loss: 1.35610742e-06
Iter: 1345 loss: 1.35858591e-06
Iter: 1346 loss: 1.35602772e-06
Iter: 1347 loss: 1.35554546e-06
Iter: 1348 loss: 1.35639038e-06
Iter: 1349 loss: 1.35539449e-06
Iter: 1350 loss: 1.35481196e-06
Iter: 1351 loss: 1.35450455e-06
Iter: 1352 loss: 1.35423102e-06
Iter: 1353 loss: 1.35364803e-06
Iter: 1354 loss: 1.35645905e-06
Iter: 1355 loss: 1.35355583e-06
Iter: 1356 loss: 1.35282312e-06
Iter: 1357 loss: 1.35401774e-06
Iter: 1358 loss: 1.35250343e-06
Iter: 1359 loss: 1.3521726e-06
Iter: 1360 loss: 1.35217329e-06
Iter: 1361 loss: 1.35180051e-06
Iter: 1362 loss: 1.35300957e-06
Iter: 1363 loss: 1.35173036e-06
Iter: 1364 loss: 1.35137861e-06
Iter: 1365 loss: 1.35115056e-06
Iter: 1366 loss: 1.35106598e-06
Iter: 1367 loss: 1.35056189e-06
Iter: 1368 loss: 1.35286155e-06
Iter: 1369 loss: 1.35053426e-06
Iter: 1370 loss: 1.35021e-06
Iter: 1371 loss: 1.34968275e-06
Iter: 1372 loss: 1.34963466e-06
Iter: 1373 loss: 1.34910806e-06
Iter: 1374 loss: 1.35238145e-06
Iter: 1375 loss: 1.34909692e-06
Iter: 1376 loss: 1.34856737e-06
Iter: 1377 loss: 1.34873221e-06
Iter: 1378 loss: 1.3481822e-06
Iter: 1379 loss: 1.34770312e-06
Iter: 1380 loss: 1.35304163e-06
Iter: 1381 loss: 1.34770301e-06
Iter: 1382 loss: 1.34722529e-06
Iter: 1383 loss: 1.34736558e-06
Iter: 1384 loss: 1.34683592e-06
Iter: 1385 loss: 1.34635957e-06
Iter: 1386 loss: 1.34772699e-06
Iter: 1387 loss: 1.34623053e-06
Iter: 1388 loss: 1.34566812e-06
Iter: 1389 loss: 1.34586458e-06
Iter: 1390 loss: 1.3452418e-06
Iter: 1391 loss: 1.34477818e-06
Iter: 1392 loss: 1.34882032e-06
Iter: 1393 loss: 1.34476818e-06
Iter: 1394 loss: 1.34431502e-06
Iter: 1395 loss: 1.34971083e-06
Iter: 1396 loss: 1.3443231e-06
Iter: 1397 loss: 1.34403035e-06
Iter: 1398 loss: 1.34368463e-06
Iter: 1399 loss: 1.34364541e-06
Iter: 1400 loss: 1.34329764e-06
Iter: 1401 loss: 1.34585207e-06
Iter: 1402 loss: 1.34327433e-06
Iter: 1403 loss: 1.34292554e-06
Iter: 1404 loss: 1.34259471e-06
Iter: 1405 loss: 1.34252741e-06
Iter: 1406 loss: 1.3420962e-06
Iter: 1407 loss: 1.34411016e-06
Iter: 1408 loss: 1.34199377e-06
Iter: 1409 loss: 1.3415829e-06
Iter: 1410 loss: 1.34132085e-06
Iter: 1411 loss: 1.34111701e-06
Iter: 1412 loss: 1.34058678e-06
Iter: 1413 loss: 1.3461879e-06
Iter: 1414 loss: 1.34059667e-06
Iter: 1415 loss: 1.34023048e-06
Iter: 1416 loss: 1.34068773e-06
Iter: 1417 loss: 1.3399939e-06
Iter: 1418 loss: 1.33951141e-06
Iter: 1419 loss: 1.34239917e-06
Iter: 1420 loss: 1.33946867e-06
Iter: 1421 loss: 1.33910225e-06
Iter: 1422 loss: 1.33847084e-06
Iter: 1423 loss: 1.35376968e-06
Iter: 1424 loss: 1.33848744e-06
Iter: 1425 loss: 1.33786762e-06
Iter: 1426 loss: 1.342818e-06
Iter: 1427 loss: 1.33780009e-06
Iter: 1428 loss: 1.33754952e-06
Iter: 1429 loss: 1.33742515e-06
Iter: 1430 loss: 1.33709716e-06
Iter: 1431 loss: 1.33701042e-06
Iter: 1432 loss: 1.33683977e-06
Iter: 1433 loss: 1.33660751e-06
Iter: 1434 loss: 1.3372088e-06
Iter: 1435 loss: 1.33654953e-06
Iter: 1436 loss: 1.3361996e-06
Iter: 1437 loss: 1.33594108e-06
Iter: 1438 loss: 1.33588787e-06
Iter: 1439 loss: 1.3354371e-06
Iter: 1440 loss: 1.33729509e-06
Iter: 1441 loss: 1.33536423e-06
Iter: 1442 loss: 1.33500885e-06
Iter: 1443 loss: 1.33465755e-06
Iter: 1444 loss: 1.3345649e-06
Iter: 1445 loss: 1.33411356e-06
Iter: 1446 loss: 1.34030608e-06
Iter: 1447 loss: 1.33413289e-06
Iter: 1448 loss: 1.33376125e-06
Iter: 1449 loss: 1.33373385e-06
Iter: 1450 loss: 1.3335025e-06
Iter: 1451 loss: 1.33306446e-06
Iter: 1452 loss: 1.33833771e-06
Iter: 1453 loss: 1.33306048e-06
Iter: 1454 loss: 1.33278945e-06
Iter: 1455 loss: 1.33238154e-06
Iter: 1456 loss: 1.33238291e-06
Iter: 1457 loss: 1.33193043e-06
Iter: 1458 loss: 1.33426136e-06
Iter: 1459 loss: 1.33188416e-06
Iter: 1460 loss: 1.33150854e-06
Iter: 1461 loss: 1.33172659e-06
Iter: 1462 loss: 1.3312922e-06
Iter: 1463 loss: 1.33094022e-06
Iter: 1464 loss: 1.33094022e-06
Iter: 1465 loss: 1.33083563e-06
Iter: 1466 loss: 1.33058597e-06
Iter: 1467 loss: 1.33485696e-06
Iter: 1468 loss: 1.33058813e-06
Iter: 1469 loss: 1.33017102e-06
Iter: 1470 loss: 1.33060848e-06
Iter: 1471 loss: 1.32997957e-06
Iter: 1472 loss: 1.32953892e-06
Iter: 1473 loss: 1.33151116e-06
Iter: 1474 loss: 1.32951641e-06
Iter: 1475 loss: 1.32915454e-06
Iter: 1476 loss: 1.32870991e-06
Iter: 1477 loss: 1.32871924e-06
Iter: 1478 loss: 1.32823379e-06
Iter: 1479 loss: 1.33397612e-06
Iter: 1480 loss: 1.32817672e-06
Iter: 1481 loss: 1.3278725e-06
Iter: 1482 loss: 1.3277546e-06
Iter: 1483 loss: 1.32758259e-06
Iter: 1484 loss: 1.32714911e-06
Iter: 1485 loss: 1.32717332e-06
Iter: 1486 loss: 1.32690514e-06
Iter: 1487 loss: 1.32684738e-06
Iter: 1488 loss: 1.32666651e-06
Iter: 1489 loss: 1.32620551e-06
Iter: 1490 loss: 1.32736318e-06
Iter: 1491 loss: 1.32607954e-06
Iter: 1492 loss: 1.32561354e-06
Iter: 1493 loss: 1.32545767e-06
Iter: 1494 loss: 1.32520972e-06
Iter: 1495 loss: 1.32580612e-06
Iter: 1496 loss: 1.32499792e-06
Iter: 1497 loss: 1.32486889e-06
Iter: 1498 loss: 1.32458615e-06
Iter: 1499 loss: 1.32785181e-06
Iter: 1500 loss: 1.32456978e-06
Iter: 1501 loss: 1.32421076e-06
Iter: 1502 loss: 1.32447212e-06
Iter: 1503 loss: 1.32399055e-06
Iter: 1504 loss: 1.32360151e-06
Iter: 1505 loss: 1.32730406e-06
Iter: 1506 loss: 1.32354194e-06
Iter: 1507 loss: 1.32327739e-06
Iter: 1508 loss: 1.32284049e-06
Iter: 1509 loss: 1.33384378e-06
Iter: 1510 loss: 1.32277557e-06
Iter: 1511 loss: 1.32223363e-06
Iter: 1512 loss: 1.32605669e-06
Iter: 1513 loss: 1.3222085e-06
Iter: 1514 loss: 1.32170135e-06
Iter: 1515 loss: 1.32157447e-06
Iter: 1516 loss: 1.32123705e-06
Iter: 1517 loss: 1.32102e-06
Iter: 1518 loss: 1.32095158e-06
Iter: 1519 loss: 1.32066577e-06
Iter: 1520 loss: 1.32053356e-06
Iter: 1521 loss: 1.32043044e-06
Iter: 1522 loss: 1.31994602e-06
Iter: 1523 loss: 1.32072103e-06
Iter: 1524 loss: 1.31965771e-06
Iter: 1525 loss: 1.31925322e-06
Iter: 1526 loss: 1.31944807e-06
Iter: 1527 loss: 1.31894762e-06
Iter: 1528 loss: 1.31845991e-06
Iter: 1529 loss: 1.32449213e-06
Iter: 1530 loss: 1.3184474e-06
Iter: 1531 loss: 1.31828961e-06
Iter: 1532 loss: 1.31824527e-06
Iter: 1533 loss: 1.3181e-06
Iter: 1534 loss: 1.31776301e-06
Iter: 1535 loss: 1.3194491e-06
Iter: 1536 loss: 1.3175993e-06
Iter: 1537 loss: 1.31728711e-06
Iter: 1538 loss: 1.32148421e-06
Iter: 1539 loss: 1.31725665e-06
Iter: 1540 loss: 1.31699915e-06
Iter: 1541 loss: 1.31703837e-06
Iter: 1542 loss: 1.31676723e-06
Iter: 1543 loss: 1.31636602e-06
Iter: 1544 loss: 1.31721811e-06
Iter: 1545 loss: 1.31622278e-06
Iter: 1546 loss: 1.31596562e-06
Iter: 1547 loss: 1.31607555e-06
Iter: 1548 loss: 1.31576553e-06
Iter: 1549 loss: 1.3152777e-06
Iter: 1550 loss: 1.31699221e-06
Iter: 1551 loss: 1.31514844e-06
Iter: 1552 loss: 1.31473712e-06
Iter: 1553 loss: 1.31467232e-06
Iter: 1554 loss: 1.31439197e-06
Iter: 1555 loss: 1.31419563e-06
Iter: 1556 loss: 1.31415686e-06
Iter: 1557 loss: 1.31390993e-06
Iter: 1558 loss: 1.31374168e-06
Iter: 1559 loss: 1.31365141e-06
Iter: 1560 loss: 1.31317051e-06
Iter: 1561 loss: 1.31385559e-06
Iter: 1562 loss: 1.3129993e-06
Iter: 1563 loss: 1.31355705e-06
Iter: 1564 loss: 1.31287698e-06
Iter: 1565 loss: 1.31275158e-06
Iter: 1566 loss: 1.31245815e-06
Iter: 1567 loss: 1.31327988e-06
Iter: 1568 loss: 1.31225602e-06
Iter: 1569 loss: 1.31180946e-06
Iter: 1570 loss: 1.31373133e-06
Iter: 1571 loss: 1.31171726e-06
Iter: 1572 loss: 1.31138302e-06
Iter: 1573 loss: 1.31307149e-06
Iter: 1574 loss: 1.31137813e-06
Iter: 1575 loss: 1.31105594e-06
Iter: 1576 loss: 1.31135778e-06
Iter: 1577 loss: 1.31085801e-06
Iter: 1578 loss: 1.31055322e-06
Iter: 1579 loss: 1.31030424e-06
Iter: 1580 loss: 1.31024183e-06
Iter: 1581 loss: 1.30973274e-06
Iter: 1582 loss: 1.31379136e-06
Iter: 1583 loss: 1.30972933e-06
Iter: 1584 loss: 1.30941226e-06
Iter: 1585 loss: 1.30941976e-06
Iter: 1586 loss: 1.30918852e-06
Iter: 1587 loss: 1.30874469e-06
Iter: 1588 loss: 1.31204126e-06
Iter: 1589 loss: 1.30867579e-06
Iter: 1590 loss: 1.30830085e-06
Iter: 1591 loss: 1.30903345e-06
Iter: 1592 loss: 1.3081825e-06
Iter: 1593 loss: 1.30775709e-06
Iter: 1594 loss: 1.30917192e-06
Iter: 1595 loss: 1.30758667e-06
Iter: 1596 loss: 1.30729609e-06
Iter: 1597 loss: 1.30836315e-06
Iter: 1598 loss: 1.30723993e-06
Iter: 1599 loss: 1.3070387e-06
Iter: 1600 loss: 1.30700255e-06
Iter: 1601 loss: 1.30683816e-06
Iter: 1602 loss: 1.30641979e-06
Iter: 1603 loss: 1.31060426e-06
Iter: 1604 loss: 1.30638534e-06
Iter: 1605 loss: 1.3060685e-06
Iter: 1606 loss: 1.30609806e-06
Iter: 1607 loss: 1.30578348e-06
Iter: 1608 loss: 1.30522767e-06
Iter: 1609 loss: 1.30996625e-06
Iter: 1610 loss: 1.30516344e-06
Iter: 1611 loss: 1.3049023e-06
Iter: 1612 loss: 1.30665478e-06
Iter: 1613 loss: 1.3048965e-06
Iter: 1614 loss: 1.30470244e-06
Iter: 1615 loss: 1.30422166e-06
Iter: 1616 loss: 1.30670799e-06
Iter: 1617 loss: 1.3039778e-06
Iter: 1618 loss: 1.30369813e-06
Iter: 1619 loss: 1.30363742e-06
Iter: 1620 loss: 1.30328704e-06
Iter: 1621 loss: 1.30296974e-06
Iter: 1622 loss: 1.30292801e-06
Iter: 1623 loss: 1.30239141e-06
Iter: 1624 loss: 1.30510716e-06
Iter: 1625 loss: 1.30226022e-06
Iter: 1626 loss: 1.3018373e-06
Iter: 1627 loss: 1.30418971e-06
Iter: 1628 loss: 1.30181297e-06
Iter: 1629 loss: 1.30144872e-06
Iter: 1630 loss: 1.30137676e-06
Iter: 1631 loss: 1.3011088e-06
Iter: 1632 loss: 1.30086607e-06
Iter: 1633 loss: 1.30083254e-06
Iter: 1634 loss: 1.30058913e-06
Iter: 1635 loss: 1.30078718e-06
Iter: 1636 loss: 1.30043577e-06
Iter: 1637 loss: 1.30012143e-06
Iter: 1638 loss: 1.30286344e-06
Iter: 1639 loss: 1.30011222e-06
Iter: 1640 loss: 1.2998795e-06
Iter: 1641 loss: 1.29955515e-06
Iter: 1642 loss: 1.29952582e-06
Iter: 1643 loss: 1.29919272e-06
Iter: 1644 loss: 1.29888235e-06
Iter: 1645 loss: 1.29877617e-06
Iter: 1646 loss: 1.29836837e-06
Iter: 1647 loss: 1.29835462e-06
Iter: 1648 loss: 1.29808279e-06
Iter: 1649 loss: 1.2979956e-06
Iter: 1650 loss: 1.29782325e-06
Iter: 1651 loss: 1.29740408e-06
Iter: 1652 loss: 1.30258661e-06
Iter: 1653 loss: 1.29739965e-06
Iter: 1654 loss: 1.29717409e-06
Iter: 1655 loss: 1.29731302e-06
Iter: 1656 loss: 1.2969839e-06
Iter: 1657 loss: 1.29668013e-06
Iter: 1658 loss: 1.296351e-06
Iter: 1659 loss: 1.29629234e-06
Iter: 1660 loss: 1.29582384e-06
Iter: 1661 loss: 1.29616274e-06
Iter: 1662 loss: 1.29552234e-06
Iter: 1663 loss: 1.29509e-06
Iter: 1664 loss: 1.30141711e-06
Iter: 1665 loss: 1.29503792e-06
Iter: 1666 loss: 1.29467935e-06
Iter: 1667 loss: 1.29623436e-06
Iter: 1668 loss: 1.29459556e-06
Iter: 1669 loss: 1.2944065e-06
Iter: 1670 loss: 1.29438934e-06
Iter: 1671 loss: 1.29424711e-06
Iter: 1672 loss: 1.29411853e-06
Iter: 1673 loss: 1.29406067e-06
Iter: 1674 loss: 1.29386285e-06
Iter: 1675 loss: 1.29353407e-06
Iter: 1676 loss: 1.29355806e-06
Iter: 1677 loss: 1.29323018e-06
Iter: 1678 loss: 1.29457385e-06
Iter: 1679 loss: 1.29318141e-06
Iter: 1680 loss: 1.29279886e-06
Iter: 1681 loss: 1.29214857e-06
Iter: 1682 loss: 1.2921588e-06
Iter: 1683 loss: 1.29206296e-06
Iter: 1684 loss: 1.29188038e-06
Iter: 1685 loss: 1.29162845e-06
Iter: 1686 loss: 1.29153477e-06
Iter: 1687 loss: 1.29135469e-06
Iter: 1688 loss: 1.2909203e-06
Iter: 1689 loss: 1.29275566e-06
Iter: 1690 loss: 1.29084788e-06
Iter: 1691 loss: 1.29062448e-06
Iter: 1692 loss: 1.29026557e-06
Iter: 1693 loss: 1.29029706e-06
Iter: 1694 loss: 1.28974693e-06
Iter: 1695 loss: 1.29275281e-06
Iter: 1696 loss: 1.28971419e-06
Iter: 1697 loss: 1.28932174e-06
Iter: 1698 loss: 1.2891702e-06
Iter: 1699 loss: 1.28895272e-06
Iter: 1700 loss: 1.28886688e-06
Iter: 1701 loss: 1.28874751e-06
Iter: 1702 loss: 1.28846796e-06
Iter: 1703 loss: 1.28881243e-06
Iter: 1704 loss: 1.28840975e-06
Iter: 1705 loss: 1.28820898e-06
Iter: 1706 loss: 1.28803595e-06
Iter: 1707 loss: 1.28797478e-06
Iter: 1708 loss: 1.28766123e-06
Iter: 1709 loss: 1.28851104e-06
Iter: 1710 loss: 1.2875264e-06
Iter: 1711 loss: 1.28722036e-06
Iter: 1712 loss: 1.28731074e-06
Iter: 1713 loss: 1.28698184e-06
Iter: 1714 loss: 1.28656984e-06
Iter: 1715 loss: 1.28732472e-06
Iter: 1716 loss: 1.28638931e-06
Iter: 1717 loss: 1.28611123e-06
Iter: 1718 loss: 1.28612737e-06
Iter: 1719 loss: 1.28575e-06
Iter: 1720 loss: 1.285694e-06
Iter: 1721 loss: 1.28550323e-06
Iter: 1722 loss: 1.2852131e-06
Iter: 1723 loss: 1.28606257e-06
Iter: 1724 loss: 1.28517513e-06
Iter: 1725 loss: 1.28473653e-06
Iter: 1726 loss: 1.28456384e-06
Iter: 1727 loss: 1.28436125e-06
Iter: 1728 loss: 1.28394993e-06
Iter: 1729 loss: 1.28664919e-06
Iter: 1730 loss: 1.283904e-06
Iter: 1731 loss: 1.28347165e-06
Iter: 1732 loss: 1.28485203e-06
Iter: 1733 loss: 1.28328702e-06
Iter: 1734 loss: 1.28329793e-06
Iter: 1735 loss: 1.28309568e-06
Iter: 1736 loss: 1.28301645e-06
Iter: 1737 loss: 1.28281476e-06
Iter: 1738 loss: 1.28362615e-06
Iter: 1739 loss: 1.28268016e-06
Iter: 1740 loss: 1.28229044e-06
Iter: 1741 loss: 1.28454303e-06
Iter: 1742 loss: 1.28223269e-06
Iter: 1743 loss: 1.28191846e-06
Iter: 1744 loss: 1.28355896e-06
Iter: 1745 loss: 1.28188253e-06
Iter: 1746 loss: 1.28162469e-06
Iter: 1747 loss: 1.28143301e-06
Iter: 1748 loss: 1.28132865e-06
Iter: 1749 loss: 1.28093575e-06
Iter: 1750 loss: 1.28246597e-06
Iter: 1751 loss: 1.28087436e-06
Iter: 1752 loss: 1.28049487e-06
Iter: 1753 loss: 1.28285194e-06
Iter: 1754 loss: 1.28043791e-06
Iter: 1755 loss: 1.28016018e-06
Iter: 1756 loss: 1.28071304e-06
Iter: 1757 loss: 1.28001375e-06
Iter: 1758 loss: 1.27970065e-06
Iter: 1759 loss: 1.27915268e-06
Iter: 1760 loss: 1.27916e-06
Iter: 1761 loss: 1.27866e-06
Iter: 1762 loss: 1.28555507e-06
Iter: 1763 loss: 1.27866736e-06
Iter: 1764 loss: 1.27819089e-06
Iter: 1765 loss: 1.27797364e-06
Iter: 1766 loss: 1.27773353e-06
Iter: 1767 loss: 1.27832789e-06
Iter: 1768 loss: 1.27765588e-06
Iter: 1769 loss: 1.27747762e-06
Iter: 1770 loss: 1.27716157e-06
Iter: 1771 loss: 1.27990336e-06
Iter: 1772 loss: 1.27703174e-06
Iter: 1773 loss: 1.27667022e-06
Iter: 1774 loss: 1.27789713e-06
Iter: 1775 loss: 1.27653061e-06
Iter: 1776 loss: 1.27619069e-06
Iter: 1777 loss: 1.2786968e-06
Iter: 1778 loss: 1.27616283e-06
Iter: 1779 loss: 1.27583178e-06
Iter: 1780 loss: 1.27558587e-06
Iter: 1781 loss: 1.27550368e-06
Iter: 1782 loss: 1.2751666e-06
Iter: 1783 loss: 1.27727526e-06
Iter: 1784 loss: 1.27510009e-06
Iter: 1785 loss: 1.27476244e-06
Iter: 1786 loss: 1.27537203e-06
Iter: 1787 loss: 1.27458725e-06
Iter: 1788 loss: 1.27430303e-06
Iter: 1789 loss: 1.27774115e-06
Iter: 1790 loss: 1.27429496e-06
Iter: 1791 loss: 1.27406133e-06
Iter: 1792 loss: 1.27362341e-06
Iter: 1793 loss: 1.27359215e-06
Iter: 1794 loss: 1.27317037e-06
Iter: 1795 loss: 1.27682824e-06
Iter: 1796 loss: 1.273164e-06
Iter: 1797 loss: 1.27282124e-06
Iter: 1798 loss: 1.27242379e-06
Iter: 1799 loss: 1.27236422e-06
Iter: 1800 loss: 1.27268049e-06
Iter: 1801 loss: 1.272237e-06
Iter: 1802 loss: 1.27206351e-06
Iter: 1803 loss: 1.27184319e-06
Iter: 1804 loss: 1.27181829e-06
Iter: 1805 loss: 1.27150156e-06
Iter: 1806 loss: 1.27110343e-06
Iter: 1807 loss: 1.27105568e-06
Iter: 1808 loss: 1.27065664e-06
Iter: 1809 loss: 1.27254907e-06
Iter: 1810 loss: 1.27053852e-06
Iter: 1811 loss: 1.27012049e-06
Iter: 1812 loss: 1.27237581e-06
Iter: 1813 loss: 1.27002909e-06
Iter: 1814 loss: 1.26975306e-06
Iter: 1815 loss: 1.27106227e-06
Iter: 1816 loss: 1.26965892e-06
Iter: 1817 loss: 1.26942007e-06
Iter: 1818 loss: 1.26897226e-06
Iter: 1819 loss: 1.27806538e-06
Iter: 1820 loss: 1.26901e-06
Iter: 1821 loss: 1.26877205e-06
Iter: 1822 loss: 1.26870896e-06
Iter: 1823 loss: 1.2684709e-06
Iter: 1824 loss: 1.26861812e-06
Iter: 1825 loss: 1.26833891e-06
Iter: 1826 loss: 1.26808607e-06
Iter: 1827 loss: 1.2678704e-06
Iter: 1828 loss: 1.26775581e-06
Iter: 1829 loss: 1.26736813e-06
Iter: 1830 loss: 1.26807277e-06
Iter: 1831 loss: 1.26721193e-06
Iter: 1832 loss: 1.26689611e-06
Iter: 1833 loss: 1.2717428e-06
Iter: 1834 loss: 1.26689338e-06
Iter: 1835 loss: 1.26671398e-06
Iter: 1836 loss: 1.26670807e-06
Iter: 1837 loss: 1.26658063e-06
Iter: 1838 loss: 1.26630516e-06
Iter: 1839 loss: 1.26700866e-06
Iter: 1840 loss: 1.26607506e-06
Iter: 1841 loss: 1.26567807e-06
Iter: 1842 loss: 1.26861732e-06
Iter: 1843 loss: 1.26560747e-06
Iter: 1844 loss: 1.26524014e-06
Iter: 1845 loss: 1.26598775e-06
Iter: 1846 loss: 1.26503653e-06
Iter: 1847 loss: 1.26472537e-06
Iter: 1848 loss: 1.26868986e-06
Iter: 1849 loss: 1.26471457e-06
Iter: 1850 loss: 1.26448458e-06
Iter: 1851 loss: 1.26391524e-06
Iter: 1852 loss: 1.27322255e-06
Iter: 1853 loss: 1.26394025e-06
Iter: 1854 loss: 1.26347709e-06
Iter: 1855 loss: 1.26730401e-06
Iter: 1856 loss: 1.26348459e-06
Iter: 1857 loss: 1.26318287e-06
Iter: 1858 loss: 1.26603663e-06
Iter: 1859 loss: 1.26317263e-06
Iter: 1860 loss: 1.26292e-06
Iter: 1861 loss: 1.2630112e-06
Iter: 1862 loss: 1.26272266e-06
Iter: 1863 loss: 1.26246573e-06
Iter: 1864 loss: 1.26230395e-06
Iter: 1865 loss: 1.26217344e-06
Iter: 1866 loss: 1.26179657e-06
Iter: 1867 loss: 1.26497935e-06
Iter: 1868 loss: 1.26177247e-06
Iter: 1869 loss: 1.26167686e-06
Iter: 1870 loss: 1.26169061e-06
Iter: 1871 loss: 1.26151372e-06
Iter: 1872 loss: 1.26112809e-06
Iter: 1873 loss: 1.26296254e-06
Iter: 1874 loss: 1.26093846e-06
Iter: 1875 loss: 1.26058592e-06
Iter: 1876 loss: 1.26320322e-06
Iter: 1877 loss: 1.26052635e-06
Iter: 1878 loss: 1.26016721e-06
Iter: 1879 loss: 1.26041925e-06
Iter: 1880 loss: 1.25994325e-06
Iter: 1881 loss: 1.25963481e-06
Iter: 1882 loss: 1.2640802e-06
Iter: 1883 loss: 1.25964e-06
Iter: 1884 loss: 1.25940812e-06
Iter: 1885 loss: 1.25926476e-06
Iter: 1886 loss: 1.25920621e-06
Iter: 1887 loss: 1.25884367e-06
Iter: 1888 loss: 1.25988731e-06
Iter: 1889 loss: 1.25876761e-06
Iter: 1890 loss: 1.25844667e-06
Iter: 1891 loss: 1.25949691e-06
Iter: 1892 loss: 1.25833583e-06
Iter: 1893 loss: 1.25799841e-06
Iter: 1894 loss: 1.26020689e-06
Iter: 1895 loss: 1.25795327e-06
Iter: 1896 loss: 1.25772829e-06
Iter: 1897 loss: 1.25756765e-06
Iter: 1898 loss: 1.2575415e-06
Iter: 1899 loss: 1.25710949e-06
Iter: 1900 loss: 1.25783015e-06
Iter: 1901 loss: 1.25693055e-06
Iter: 1902 loss: 1.25697943e-06
Iter: 1903 loss: 1.25676706e-06
Iter: 1904 loss: 1.25664178e-06
Iter: 1905 loss: 1.2563662e-06
Iter: 1906 loss: 1.26213104e-06
Iter: 1907 loss: 1.25636302e-06
Iter: 1908 loss: 1.25607789e-06
Iter: 1909 loss: 1.25622068e-06
Iter: 1910 loss: 1.25589418e-06
Iter: 1911 loss: 1.25564884e-06
Iter: 1912 loss: 1.25603719e-06
Iter: 1913 loss: 1.25552651e-06
Iter: 1914 loss: 1.25518193e-06
Iter: 1915 loss: 1.25790802e-06
Iter: 1916 loss: 1.25514953e-06
Iter: 1917 loss: 1.25494557e-06
Iter: 1918 loss: 1.25548547e-06
Iter: 1919 loss: 1.25487509e-06
Iter: 1920 loss: 1.25461111e-06
Iter: 1921 loss: 1.25452664e-06
Iter: 1922 loss: 1.25436418e-06
Iter: 1923 loss: 1.2540595e-06
Iter: 1924 loss: 1.25525094e-06
Iter: 1925 loss: 1.25402767e-06
Iter: 1926 loss: 1.25374982e-06
Iter: 1927 loss: 1.25569329e-06
Iter: 1928 loss: 1.25367956e-06
Iter: 1929 loss: 1.25350709e-06
Iter: 1930 loss: 1.25406359e-06
Iter: 1931 loss: 1.25343445e-06
Iter: 1932 loss: 1.25317479e-06
Iter: 1933 loss: 1.25283418e-06
Iter: 1934 loss: 1.25283327e-06
Iter: 1935 loss: 1.25270958e-06
Iter: 1936 loss: 1.25265069e-06
Iter: 1937 loss: 1.25247163e-06
Iter: 1938 loss: 1.25352256e-06
Iter: 1939 loss: 1.25243855e-06
Iter: 1940 loss: 1.25231156e-06
Iter: 1941 loss: 1.25197926e-06
Iter: 1942 loss: 1.25501083e-06
Iter: 1943 loss: 1.25192491e-06
Iter: 1944 loss: 1.25169345e-06
Iter: 1945 loss: 1.25206964e-06
Iter: 1946 loss: 1.25151382e-06
Iter: 1947 loss: 1.25120118e-06
Iter: 1948 loss: 1.25406359e-06
Iter: 1949 loss: 1.25118e-06
Iter: 1950 loss: 1.25097699e-06
Iter: 1951 loss: 1.25149063e-06
Iter: 1952 loss: 1.2509322e-06
Iter: 1953 loss: 1.25061911e-06
Iter: 1954 loss: 1.25095607e-06
Iter: 1955 loss: 1.25047836e-06
Iter: 1956 loss: 1.25026077e-06
Iter: 1957 loss: 1.250742e-06
Iter: 1958 loss: 1.25014844e-06
Iter: 1959 loss: 1.24981773e-06
Iter: 1960 loss: 1.25110705e-06
Iter: 1961 loss: 1.24975168e-06
Iter: 1962 loss: 1.249506e-06
Iter: 1963 loss: 1.25144243e-06
Iter: 1964 loss: 1.24952044e-06
Iter: 1965 loss: 1.24929466e-06
Iter: 1966 loss: 1.24900748e-06
Iter: 1967 loss: 1.2572591e-06
Iter: 1968 loss: 1.24903499e-06
Iter: 1969 loss: 1.24869177e-06
Iter: 1970 loss: 1.25240581e-06
Iter: 1971 loss: 1.24869837e-06
Iter: 1972 loss: 1.24859412e-06
Iter: 1973 loss: 1.24858661e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi3/300_300_300_1
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0
+ date
Mon Oct 26 16:07:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -1 --phi 0 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b06f1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0636730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0690ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b069e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0750a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b069e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0629488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0750f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b07509d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b05c7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b05c7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b059ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b04f8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b04f8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b04e4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b04f5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0482488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0433950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b04827b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b04137b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0413378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b03be950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0413c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b03380d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0355620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0355f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b030a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b035ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b035ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b028bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b0252950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b02521e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b024dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b02286a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b01c8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38b01b5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.90408695e-06
Iter: 2 loss: 2.94805659e-06
Iter: 3 loss: 1.208514e-05
Iter: 4 loss: 2.91193237e-06
Iter: 5 loss: 2.45777142e-06
Iter: 6 loss: 5.96235395e-06
Iter: 7 loss: 2.42456463e-06
Iter: 8 loss: 2.30233809e-06
Iter: 9 loss: 2.11128508e-06
Iter: 10 loss: 2.1089545e-06
Iter: 11 loss: 2.0449263e-06
Iter: 12 loss: 1.99486794e-06
Iter: 13 loss: 1.94483141e-06
Iter: 14 loss: 1.95242228e-06
Iter: 15 loss: 1.90703395e-06
Iter: 16 loss: 1.87551802e-06
Iter: 17 loss: 1.82318831e-06
Iter: 18 loss: 1.82300505e-06
Iter: 19 loss: 1.82777637e-06
Iter: 20 loss: 1.79585709e-06
Iter: 21 loss: 1.77047127e-06
Iter: 22 loss: 1.70110525e-06
Iter: 23 loss: 2.12007672e-06
Iter: 24 loss: 1.68216525e-06
Iter: 25 loss: 1.60261402e-06
Iter: 26 loss: 1.98207681e-06
Iter: 27 loss: 1.58833507e-06
Iter: 28 loss: 1.51514655e-06
Iter: 29 loss: 2.21023947e-06
Iter: 30 loss: 1.51227687e-06
Iter: 31 loss: 1.49016023e-06
Iter: 32 loss: 1.45774584e-06
Iter: 33 loss: 1.45687545e-06
Iter: 34 loss: 1.41822193e-06
Iter: 35 loss: 1.51428299e-06
Iter: 36 loss: 1.40464363e-06
Iter: 37 loss: 1.39651547e-06
Iter: 38 loss: 1.40921e-06
Iter: 39 loss: 1.39269093e-06
Iter: 40 loss: 1.38356643e-06
Iter: 41 loss: 1.391144e-06
Iter: 42 loss: 1.37812879e-06
Iter: 43 loss: 1.36664028e-06
Iter: 44 loss: 1.44516912e-06
Iter: 45 loss: 1.3655158e-06
Iter: 46 loss: 1.35550238e-06
Iter: 47 loss: 1.41970486e-06
Iter: 48 loss: 1.35442451e-06
Iter: 49 loss: 1.3434427e-06
Iter: 50 loss: 1.31074034e-06
Iter: 51 loss: 1.4192758e-06
Iter: 52 loss: 1.29536443e-06
Iter: 53 loss: 1.26590407e-06
Iter: 54 loss: 1.5931937e-06
Iter: 55 loss: 1.26528539e-06
Iter: 56 loss: 1.24236317e-06
Iter: 57 loss: 1.5999301e-06
Iter: 58 loss: 1.24235225e-06
Iter: 59 loss: 1.22683036e-06
Iter: 60 loss: 1.20937864e-06
Iter: 61 loss: 1.2070534e-06
Iter: 62 loss: 1.20281845e-06
Iter: 63 loss: 1.20166669e-06
Iter: 64 loss: 1.19559104e-06
Iter: 65 loss: 1.18129128e-06
Iter: 66 loss: 1.35036726e-06
Iter: 67 loss: 1.18006267e-06
Iter: 68 loss: 1.16622925e-06
Iter: 69 loss: 1.30027843e-06
Iter: 70 loss: 1.16568071e-06
Iter: 71 loss: 1.15259661e-06
Iter: 72 loss: 1.18987793e-06
Iter: 73 loss: 1.14849604e-06
Iter: 74 loss: 1.13835699e-06
Iter: 75 loss: 1.11640793e-06
Iter: 76 loss: 1.45024808e-06
Iter: 77 loss: 1.11551708e-06
Iter: 78 loss: 1.10427175e-06
Iter: 79 loss: 1.10292808e-06
Iter: 80 loss: 1.09410735e-06
Iter: 81 loss: 1.1406363e-06
Iter: 82 loss: 1.09271036e-06
Iter: 83 loss: 1.08209781e-06
Iter: 84 loss: 1.07989217e-06
Iter: 85 loss: 1.07295523e-06
Iter: 86 loss: 1.06381617e-06
Iter: 87 loss: 1.07397932e-06
Iter: 88 loss: 1.05890081e-06
Iter: 89 loss: 1.04758988e-06
Iter: 90 loss: 1.04303774e-06
Iter: 91 loss: 1.03698574e-06
Iter: 92 loss: 1.00938e-06
Iter: 93 loss: 1.1199395e-06
Iter: 94 loss: 1.00314219e-06
Iter: 95 loss: 9.92620471e-07
Iter: 96 loss: 9.7962868e-07
Iter: 97 loss: 9.78481467e-07
Iter: 98 loss: 9.67346068e-07
Iter: 99 loss: 9.65779691e-07
Iter: 100 loss: 9.5825817e-07
Iter: 101 loss: 9.50986e-07
Iter: 102 loss: 9.49319315e-07
Iter: 103 loss: 9.4319546e-07
Iter: 104 loss: 1.0195738e-06
Iter: 105 loss: 9.43162775e-07
Iter: 106 loss: 9.36174331e-07
Iter: 107 loss: 9.2994361e-07
Iter: 108 loss: 9.28163331e-07
Iter: 109 loss: 9.21911237e-07
Iter: 110 loss: 9.31936427e-07
Iter: 111 loss: 9.18998808e-07
Iter: 112 loss: 9.11250254e-07
Iter: 113 loss: 9.30732426e-07
Iter: 114 loss: 9.08557752e-07
Iter: 115 loss: 8.96253823e-07
Iter: 116 loss: 9.24861524e-07
Iter: 117 loss: 8.91706122e-07
Iter: 118 loss: 8.83968767e-07
Iter: 119 loss: 9.04133117e-07
Iter: 120 loss: 8.81336575e-07
Iter: 121 loss: 8.74625755e-07
Iter: 122 loss: 8.64393769e-07
Iter: 123 loss: 8.64263711e-07
Iter: 124 loss: 8.7066519e-07
Iter: 125 loss: 8.60558828e-07
Iter: 126 loss: 8.5742488e-07
Iter: 127 loss: 8.51009418e-07
Iter: 128 loss: 9.66202833e-07
Iter: 129 loss: 8.50938818e-07
Iter: 130 loss: 8.43528539e-07
Iter: 131 loss: 8.51399136e-07
Iter: 132 loss: 8.39460483e-07
Iter: 133 loss: 8.30290105e-07
Iter: 134 loss: 9.58641294e-07
Iter: 135 loss: 8.30261797e-07
Iter: 136 loss: 8.2729423e-07
Iter: 137 loss: 8.19443642e-07
Iter: 138 loss: 8.76040303e-07
Iter: 139 loss: 8.17715545e-07
Iter: 140 loss: 8.07809442e-07
Iter: 141 loss: 8.07767776e-07
Iter: 142 loss: 8.04453066e-07
Iter: 143 loss: 8.04753597e-07
Iter: 144 loss: 8.01866861e-07
Iter: 145 loss: 7.98969381e-07
Iter: 146 loss: 7.96067297e-07
Iter: 147 loss: 7.95493406e-07
Iter: 148 loss: 7.95015126e-07
Iter: 149 loss: 7.92893616e-07
Iter: 150 loss: 7.91327579e-07
Iter: 151 loss: 7.88081195e-07
Iter: 152 loss: 8.44817237e-07
Iter: 153 loss: 7.88026114e-07
Iter: 154 loss: 7.83295604e-07
Iter: 155 loss: 7.82215693e-07
Iter: 156 loss: 7.79198785e-07
Iter: 157 loss: 7.73182421e-07
Iter: 158 loss: 8.34881405e-07
Iter: 159 loss: 7.73000693e-07
Iter: 160 loss: 7.67903373e-07
Iter: 161 loss: 8.06568494e-07
Iter: 162 loss: 7.67487165e-07
Iter: 163 loss: 7.63675757e-07
Iter: 164 loss: 7.54549774e-07
Iter: 165 loss: 8.58786507e-07
Iter: 166 loss: 7.53720713e-07
Iter: 167 loss: 7.63218679e-07
Iter: 168 loss: 7.52081746e-07
Iter: 169 loss: 7.51118137e-07
Iter: 170 loss: 7.48239188e-07
Iter: 171 loss: 7.57550538e-07
Iter: 172 loss: 7.4683112e-07
Iter: 173 loss: 7.45997568e-07
Iter: 174 loss: 7.44899808e-07
Iter: 175 loss: 7.4325294e-07
Iter: 176 loss: 7.42652e-07
Iter: 177 loss: 7.41739711e-07
Iter: 178 loss: 7.39706138e-07
Iter: 179 loss: 7.34394e-07
Iter: 180 loss: 7.79268248e-07
Iter: 181 loss: 7.3349571e-07
Iter: 182 loss: 7.33956767e-07
Iter: 183 loss: 7.30977604e-07
Iter: 184 loss: 7.2815908e-07
Iter: 185 loss: 7.25653308e-07
Iter: 186 loss: 7.24907068e-07
Iter: 187 loss: 7.20097034e-07
Iter: 188 loss: 7.20970831e-07
Iter: 189 loss: 7.16512545e-07
Iter: 190 loss: 7.13260476e-07
Iter: 191 loss: 7.13157192e-07
Iter: 192 loss: 7.1056229e-07
Iter: 193 loss: 7.23463359e-07
Iter: 194 loss: 7.10132156e-07
Iter: 195 loss: 7.0697547e-07
Iter: 196 loss: 7.05199568e-07
Iter: 197 loss: 7.0383669e-07
Iter: 198 loss: 7.01001397e-07
Iter: 199 loss: 7.11476787e-07
Iter: 200 loss: 7.00324e-07
Iter: 201 loss: 6.9688582e-07
Iter: 202 loss: 7.14689804e-07
Iter: 203 loss: 6.96350185e-07
Iter: 204 loss: 6.94854634e-07
Iter: 205 loss: 6.94977587e-07
Iter: 206 loss: 6.93658762e-07
Iter: 207 loss: 6.91686694e-07
Iter: 208 loss: 7.12191309e-07
Iter: 209 loss: 6.91633318e-07
Iter: 210 loss: 6.90195634e-07
Iter: 211 loss: 6.87295e-07
Iter: 212 loss: 7.39881557e-07
Iter: 213 loss: 6.87249553e-07
Iter: 214 loss: 6.85862233e-07
Iter: 215 loss: 6.98653423e-07
Iter: 216 loss: 6.85793452e-07
Iter: 217 loss: 6.84314614e-07
Iter: 218 loss: 6.92749609e-07
Iter: 219 loss: 6.84113672e-07
Iter: 220 loss: 6.8276205e-07
Iter: 221 loss: 6.80217681e-07
Iter: 222 loss: 7.36438835e-07
Iter: 223 loss: 6.80215464e-07
Iter: 224 loss: 6.77424111e-07
Iter: 225 loss: 6.78169044e-07
Iter: 226 loss: 6.75387469e-07
Iter: 227 loss: 6.72237661e-07
Iter: 228 loss: 6.72195597e-07
Iter: 229 loss: 6.70207328e-07
Iter: 230 loss: 6.84885208e-07
Iter: 231 loss: 6.7003532e-07
Iter: 232 loss: 6.68932444e-07
Iter: 233 loss: 6.67002155e-07
Iter: 234 loss: 6.67017559e-07
Iter: 235 loss: 6.65187542e-07
Iter: 236 loss: 6.65194193e-07
Iter: 237 loss: 6.63596211e-07
Iter: 238 loss: 6.64435106e-07
Iter: 239 loss: 6.62549496e-07
Iter: 240 loss: 6.61907961e-07
Iter: 241 loss: 6.67629195e-07
Iter: 242 loss: 6.61869649e-07
Iter: 243 loss: 6.60974592e-07
Iter: 244 loss: 6.5850827e-07
Iter: 245 loss: 6.72343788e-07
Iter: 246 loss: 6.57749752e-07
Iter: 247 loss: 6.54842e-07
Iter: 248 loss: 6.68530163e-07
Iter: 249 loss: 6.54281052e-07
Iter: 250 loss: 6.52751396e-07
Iter: 251 loss: 6.52711833e-07
Iter: 252 loss: 6.50965148e-07
Iter: 253 loss: 6.49556455e-07
Iter: 254 loss: 6.49067829e-07
Iter: 255 loss: 6.47508671e-07
Iter: 256 loss: 6.47338538e-07
Iter: 257 loss: 6.46215597e-07
Iter: 258 loss: 6.43860403e-07
Iter: 259 loss: 6.55308e-07
Iter: 260 loss: 6.43454086e-07
Iter: 261 loss: 6.42263217e-07
Iter: 262 loss: 6.42112582e-07
Iter: 263 loss: 6.41388738e-07
Iter: 264 loss: 6.40272162e-07
Iter: 265 loss: 6.40249596e-07
Iter: 266 loss: 6.38301742e-07
Iter: 267 loss: 6.37329208e-07
Iter: 268 loss: 6.36422101e-07
Iter: 269 loss: 6.34870787e-07
Iter: 270 loss: 6.34542175e-07
Iter: 271 loss: 6.33978345e-07
Iter: 272 loss: 6.33116542e-07
Iter: 273 loss: 6.3312234e-07
Iter: 274 loss: 6.32098818e-07
Iter: 275 loss: 6.45412968e-07
Iter: 276 loss: 6.32089836e-07
Iter: 277 loss: 6.31485e-07
Iter: 278 loss: 6.30429e-07
Iter: 279 loss: 6.30411e-07
Iter: 280 loss: 6.29248177e-07
Iter: 281 loss: 6.27234726e-07
Iter: 282 loss: 6.27236886e-07
Iter: 283 loss: 6.28464761e-07
Iter: 284 loss: 6.26167036e-07
Iter: 285 loss: 6.25601729e-07
Iter: 286 loss: 6.24119764e-07
Iter: 287 loss: 6.34597541e-07
Iter: 288 loss: 6.23785127e-07
Iter: 289 loss: 6.21240758e-07
Iter: 290 loss: 6.21438858e-07
Iter: 291 loss: 6.19275283e-07
Iter: 292 loss: 6.17747958e-07
Iter: 293 loss: 6.17665933e-07
Iter: 294 loss: 6.16685497e-07
Iter: 295 loss: 6.32475462e-07
Iter: 296 loss: 6.16683792e-07
Iter: 297 loss: 6.15801127e-07
Iter: 298 loss: 6.15278168e-07
Iter: 299 loss: 6.14930059e-07
Iter: 300 loss: 6.14013913e-07
Iter: 301 loss: 6.17406329e-07
Iter: 302 loss: 6.13802115e-07
Iter: 303 loss: 6.12969359e-07
Iter: 304 loss: 6.15957902e-07
Iter: 305 loss: 6.12750796e-07
Iter: 306 loss: 6.11541e-07
Iter: 307 loss: 6.11665e-07
Iter: 308 loss: 6.10651227e-07
Iter: 309 loss: 6.09883841e-07
Iter: 310 loss: 6.21130312e-07
Iter: 311 loss: 6.09896574e-07
Iter: 312 loss: 6.0921451e-07
Iter: 313 loss: 6.08305243e-07
Iter: 314 loss: 6.08242885e-07
Iter: 315 loss: 6.07089532e-07
Iter: 316 loss: 6.07144102e-07
Iter: 317 loss: 6.06195727e-07
Iter: 318 loss: 6.061357e-07
Iter: 319 loss: 6.05753712e-07
Iter: 320 loss: 6.05230639e-07
Iter: 321 loss: 6.03692683e-07
Iter: 322 loss: 6.07764946e-07
Iter: 323 loss: 6.02850434e-07
Iter: 324 loss: 6.00613248e-07
Iter: 325 loss: 6.11272e-07
Iter: 326 loss: 6.00199769e-07
Iter: 327 loss: 5.98284771e-07
Iter: 328 loss: 5.98776e-07
Iter: 329 loss: 5.96910468e-07
Iter: 330 loss: 5.9771e-07
Iter: 331 loss: 5.95908659e-07
Iter: 332 loss: 5.9534068e-07
Iter: 333 loss: 5.94496e-07
Iter: 334 loss: 5.94524749e-07
Iter: 335 loss: 5.93584446e-07
Iter: 336 loss: 5.99288796e-07
Iter: 337 loss: 5.93463824e-07
Iter: 338 loss: 5.92787899e-07
Iter: 339 loss: 5.99336943e-07
Iter: 340 loss: 5.92767947e-07
Iter: 341 loss: 5.92337244e-07
Iter: 342 loss: 5.93089794e-07
Iter: 343 loss: 5.92126e-07
Iter: 344 loss: 5.9164779e-07
Iter: 345 loss: 5.93067739e-07
Iter: 346 loss: 5.91530238e-07
Iter: 347 loss: 5.90855393e-07
Iter: 348 loss: 5.89467163e-07
Iter: 349 loss: 6.11614155e-07
Iter: 350 loss: 5.89418676e-07
Iter: 351 loss: 5.88416e-07
Iter: 352 loss: 5.94922142e-07
Iter: 353 loss: 5.88303237e-07
Iter: 354 loss: 5.87675913e-07
Iter: 355 loss: 5.87666761e-07
Iter: 356 loss: 5.87138175e-07
Iter: 357 loss: 5.85831799e-07
Iter: 358 loss: 5.99922828e-07
Iter: 359 loss: 5.85723456e-07
Iter: 360 loss: 5.84611939e-07
Iter: 361 loss: 5.84090685e-07
Iter: 362 loss: 5.83559199e-07
Iter: 363 loss: 5.81988957e-07
Iter: 364 loss: 6.03652438e-07
Iter: 365 loss: 5.8198e-07
Iter: 366 loss: 5.81043878e-07
Iter: 367 loss: 5.81021141e-07
Iter: 368 loss: 5.80488063e-07
Iter: 369 loss: 5.79169637e-07
Iter: 370 loss: 5.92650281e-07
Iter: 371 loss: 5.79034236e-07
Iter: 372 loss: 5.78327672e-07
Iter: 373 loss: 5.78172262e-07
Iter: 374 loss: 5.77771857e-07
Iter: 375 loss: 5.80383187e-07
Iter: 376 loss: 5.77720698e-07
Iter: 377 loss: 5.77429e-07
Iter: 378 loss: 5.77518222e-07
Iter: 379 loss: 5.77228e-07
Iter: 380 loss: 5.76703e-07
Iter: 381 loss: 5.7681018e-07
Iter: 382 loss: 5.76323544e-07
Iter: 383 loss: 5.7571674e-07
Iter: 384 loss: 5.76264256e-07
Iter: 385 loss: 5.75338561e-07
Iter: 386 loss: 5.74727267e-07
Iter: 387 loss: 5.76044727e-07
Iter: 388 loss: 5.74505179e-07
Iter: 389 loss: 5.73340913e-07
Iter: 390 loss: 5.7334114e-07
Iter: 391 loss: 5.72403849e-07
Iter: 392 loss: 5.71389705e-07
Iter: 393 loss: 5.7123259e-07
Iter: 394 loss: 5.70517329e-07
Iter: 395 loss: 5.69205781e-07
Iter: 396 loss: 5.71010787e-07
Iter: 397 loss: 5.68570272e-07
Iter: 398 loss: 5.6861245e-07
Iter: 399 loss: 5.68160317e-07
Iter: 400 loss: 5.67675784e-07
Iter: 401 loss: 5.66779e-07
Iter: 402 loss: 5.86488056e-07
Iter: 403 loss: 5.66766971e-07
Iter: 404 loss: 5.66106905e-07
Iter: 405 loss: 5.72097633e-07
Iter: 406 loss: 5.66076778e-07
Iter: 407 loss: 5.65695927e-07
Iter: 408 loss: 5.70311499e-07
Iter: 409 loss: 5.65684843e-07
Iter: 410 loss: 5.65428309e-07
Iter: 411 loss: 5.65517e-07
Iter: 412 loss: 5.65218556e-07
Iter: 413 loss: 5.64787513e-07
Iter: 414 loss: 5.65684445e-07
Iter: 415 loss: 5.64589186e-07
Iter: 416 loss: 5.64145694e-07
Iter: 417 loss: 5.64761308e-07
Iter: 418 loss: 5.63936396e-07
Iter: 419 loss: 5.63608467e-07
Iter: 420 loss: 5.63220965e-07
Iter: 421 loss: 5.63190611e-07
Iter: 422 loss: 5.62692662e-07
Iter: 423 loss: 5.62671744e-07
Iter: 424 loss: 5.62265143e-07
Iter: 425 loss: 5.61584272e-07
Iter: 426 loss: 5.61574325e-07
Iter: 427 loss: 5.60757258e-07
Iter: 428 loss: 5.59215e-07
Iter: 429 loss: 5.94116727e-07
Iter: 430 loss: 5.59208956e-07
Iter: 431 loss: 5.58677584e-07
Iter: 432 loss: 5.58546617e-07
Iter: 433 loss: 5.57938051e-07
Iter: 434 loss: 5.63197e-07
Iter: 435 loss: 5.57896101e-07
Iter: 436 loss: 5.5752389e-07
Iter: 437 loss: 5.57054364e-07
Iter: 438 loss: 5.57015198e-07
Iter: 439 loss: 5.56791747e-07
Iter: 440 loss: 5.56752923e-07
Iter: 441 loss: 5.56464329e-07
Iter: 442 loss: 5.56077509e-07
Iter: 443 loss: 5.56055511e-07
Iter: 444 loss: 5.55550514e-07
Iter: 445 loss: 5.61796583e-07
Iter: 446 loss: 5.5554824e-07
Iter: 447 loss: 5.5524373e-07
Iter: 448 loss: 5.55171539e-07
Iter: 449 loss: 5.54972246e-07
Iter: 450 loss: 5.54500275e-07
Iter: 451 loss: 5.54294161e-07
Iter: 452 loss: 5.54075e-07
Iter: 453 loss: 5.53744599e-07
Iter: 454 loss: 5.53701852e-07
Iter: 455 loss: 5.53392056e-07
Iter: 456 loss: 5.53453674e-07
Iter: 457 loss: 5.53143821e-07
Iter: 458 loss: 5.52749e-07
Iter: 459 loss: 5.516182e-07
Iter: 460 loss: 5.56983878e-07
Iter: 461 loss: 5.5123212e-07
Iter: 462 loss: 5.50111508e-07
Iter: 463 loss: 5.50106222e-07
Iter: 464 loss: 5.49667448e-07
Iter: 465 loss: 5.49634819e-07
Iter: 466 loss: 5.49157505e-07
Iter: 467 loss: 5.48293883e-07
Iter: 468 loss: 5.68862561e-07
Iter: 469 loss: 5.48298829e-07
Iter: 470 loss: 5.47909735e-07
Iter: 471 loss: 5.53062421e-07
Iter: 472 loss: 5.47917807e-07
Iter: 473 loss: 5.47541958e-07
Iter: 474 loss: 5.48318042e-07
Iter: 475 loss: 5.47374725e-07
Iter: 476 loss: 5.47130526e-07
Iter: 477 loss: 5.49953882e-07
Iter: 478 loss: 5.47131037e-07
Iter: 479 loss: 5.46935951e-07
Iter: 480 loss: 5.46827152e-07
Iter: 481 loss: 5.46726e-07
Iter: 482 loss: 5.4641589e-07
Iter: 483 loss: 5.4600423e-07
Iter: 484 loss: 5.45994624e-07
Iter: 485 loss: 5.45448188e-07
Iter: 486 loss: 5.50706886e-07
Iter: 487 loss: 5.45407659e-07
Iter: 488 loss: 5.45017201e-07
Iter: 489 loss: 5.48594812e-07
Iter: 490 loss: 5.4498912e-07
Iter: 491 loss: 5.44766294e-07
Iter: 492 loss: 5.44102363e-07
Iter: 493 loss: 5.47246373e-07
Iter: 494 loss: 5.43869078e-07
Iter: 495 loss: 5.43031945e-07
Iter: 496 loss: 5.48772903e-07
Iter: 497 loss: 5.42945259e-07
Iter: 498 loss: 5.42449e-07
Iter: 499 loss: 5.50029597e-07
Iter: 500 loss: 5.42433156e-07
Iter: 501 loss: 5.41851136e-07
Iter: 502 loss: 5.42049634e-07
Iter: 503 loss: 5.41428392e-07
Iter: 504 loss: 5.41123654e-07
Iter: 505 loss: 5.41125871e-07
Iter: 506 loss: 5.40878091e-07
Iter: 507 loss: 5.40354335e-07
Iter: 508 loss: 5.45216835e-07
Iter: 509 loss: 5.40340125e-07
Iter: 510 loss: 5.40039537e-07
Iter: 511 loss: 5.41005477e-07
Iter: 512 loss: 5.39978828e-07
Iter: 513 loss: 5.39746225e-07
Iter: 514 loss: 5.41229042e-07
Iter: 515 loss: 5.39699613e-07
Iter: 516 loss: 5.39552616e-07
Iter: 517 loss: 5.39351731e-07
Iter: 518 loss: 5.393332e-07
Iter: 519 loss: 5.39076268e-07
Iter: 520 loss: 5.40369456e-07
Iter: 521 loss: 5.39033465e-07
Iter: 522 loss: 5.38798304e-07
Iter: 523 loss: 5.40958922e-07
Iter: 524 loss: 5.38784207e-07
Iter: 525 loss: 5.38564564e-07
Iter: 526 loss: 5.38002723e-07
Iter: 527 loss: 5.4026259e-07
Iter: 528 loss: 5.37767733e-07
Iter: 529 loss: 5.37057076e-07
Iter: 530 loss: 5.42401835e-07
Iter: 531 loss: 5.37009953e-07
Iter: 532 loss: 5.3648381e-07
Iter: 533 loss: 5.38566837e-07
Iter: 534 loss: 5.36368e-07
Iter: 535 loss: 5.35689367e-07
Iter: 536 loss: 5.40421524e-07
Iter: 537 loss: 5.3565816e-07
Iter: 538 loss: 5.35461538e-07
Iter: 539 loss: 5.35035952e-07
Iter: 540 loss: 5.42982548e-07
Iter: 541 loss: 5.35013953e-07
Iter: 542 loss: 5.34733715e-07
Iter: 543 loss: 5.34718936e-07
Iter: 544 loss: 5.34482922e-07
Iter: 545 loss: 5.34350306e-07
Iter: 546 loss: 5.34238836e-07
Iter: 547 loss: 5.33977072e-07
Iter: 548 loss: 5.38169331e-07
Iter: 549 loss: 5.33989e-07
Iter: 550 loss: 5.33756122e-07
Iter: 551 loss: 5.3332235e-07
Iter: 552 loss: 5.40180622e-07
Iter: 553 loss: 5.33294838e-07
Iter: 554 loss: 5.32900913e-07
Iter: 555 loss: 5.35756215e-07
Iter: 556 loss: 5.32884144e-07
Iter: 557 loss: 5.32632669e-07
Iter: 558 loss: 5.32869763e-07
Iter: 559 loss: 5.32505055e-07
Iter: 560 loss: 5.32127615e-07
Iter: 561 loss: 5.31561113e-07
Iter: 562 loss: 5.31544174e-07
Iter: 563 loss: 5.30849888e-07
Iter: 564 loss: 5.31120634e-07
Iter: 565 loss: 5.30366378e-07
Iter: 566 loss: 5.29410954e-07
Iter: 567 loss: 5.3235425e-07
Iter: 568 loss: 5.29111503e-07
Iter: 569 loss: 5.29531349e-07
Iter: 570 loss: 5.28837745e-07
Iter: 571 loss: 5.28659655e-07
Iter: 572 loss: 5.28103556e-07
Iter: 573 loss: 5.29213196e-07
Iter: 574 loss: 5.27747545e-07
Iter: 575 loss: 5.27400516e-07
Iter: 576 loss: 5.27315137e-07
Iter: 577 loss: 5.26949918e-07
Iter: 578 loss: 5.28893111e-07
Iter: 579 loss: 5.26883923e-07
Iter: 580 loss: 5.26666e-07
Iter: 581 loss: 5.26838903e-07
Iter: 582 loss: 5.2650563e-07
Iter: 583 loss: 5.26072427e-07
Iter: 584 loss: 5.26122165e-07
Iter: 585 loss: 5.2574012e-07
Iter: 586 loss: 5.2542805e-07
Iter: 587 loss: 5.26802296e-07
Iter: 588 loss: 5.25372172e-07
Iter: 589 loss: 5.25005817e-07
Iter: 590 loss: 5.25718747e-07
Iter: 591 loss: 5.24871723e-07
Iter: 592 loss: 5.24504912e-07
Iter: 593 loss: 5.25443852e-07
Iter: 594 loss: 5.24377469e-07
Iter: 595 loss: 5.2415e-07
Iter: 596 loss: 5.23956089e-07
Iter: 597 loss: 5.23917038e-07
Iter: 598 loss: 5.23492417e-07
Iter: 599 loss: 5.23847689e-07
Iter: 600 loss: 5.23262599e-07
Iter: 601 loss: 5.22940809e-07
Iter: 602 loss: 5.22933306e-07
Iter: 603 loss: 5.22505559e-07
Iter: 604 loss: 5.21558491e-07
Iter: 605 loss: 5.33663069e-07
Iter: 606 loss: 5.21491756e-07
Iter: 607 loss: 5.20981757e-07
Iter: 608 loss: 5.25596533e-07
Iter: 609 loss: 5.20946344e-07
Iter: 610 loss: 5.20515187e-07
Iter: 611 loss: 5.24029304e-07
Iter: 612 loss: 5.20481365e-07
Iter: 613 loss: 5.20162757e-07
Iter: 614 loss: 5.20386891e-07
Iter: 615 loss: 5.19953119e-07
Iter: 616 loss: 5.19626951e-07
Iter: 617 loss: 5.23877702e-07
Iter: 618 loss: 5.19635762e-07
Iter: 619 loss: 5.19506898e-07
Iter: 620 loss: 5.19228365e-07
Iter: 621 loss: 5.23732922e-07
Iter: 622 loss: 5.19211426e-07
Iter: 623 loss: 5.18836544e-07
Iter: 624 loss: 5.22232881e-07
Iter: 625 loss: 5.18815511e-07
Iter: 626 loss: 5.18571653e-07
Iter: 627 loss: 5.18887191e-07
Iter: 628 loss: 5.18439151e-07
Iter: 629 loss: 5.18151751e-07
Iter: 630 loss: 5.17707235e-07
Iter: 631 loss: 5.17694502e-07
Iter: 632 loss: 5.17136755e-07
Iter: 633 loss: 5.20271783e-07
Iter: 634 loss: 5.17057515e-07
Iter: 635 loss: 5.16817636e-07
Iter: 636 loss: 5.16788077e-07
Iter: 637 loss: 5.16490786e-07
Iter: 638 loss: 5.16301043e-07
Iter: 639 loss: 5.16151886e-07
Iter: 640 loss: 5.15855334e-07
Iter: 641 loss: 5.15616307e-07
Iter: 642 loss: 5.1551558e-07
Iter: 643 loss: 5.15091131e-07
Iter: 644 loss: 5.1509528e-07
Iter: 645 loss: 5.14732e-07
Iter: 646 loss: 5.14481712e-07
Iter: 647 loss: 5.14360238e-07
Iter: 648 loss: 5.14087e-07
Iter: 649 loss: 5.1406613e-07
Iter: 650 loss: 5.13845521e-07
Iter: 651 loss: 5.13326256e-07
Iter: 652 loss: 5.18796071e-07
Iter: 653 loss: 5.13256566e-07
Iter: 654 loss: 5.12847123e-07
Iter: 655 loss: 5.12810743e-07
Iter: 656 loss: 5.12678412e-07
Iter: 657 loss: 5.12534484e-07
Iter: 658 loss: 5.12490317e-07
Iter: 659 loss: 5.1219024e-07
Iter: 660 loss: 5.12001861e-07
Iter: 661 loss: 5.11885446e-07
Iter: 662 loss: 5.11368398e-07
Iter: 663 loss: 5.12052793e-07
Iter: 664 loss: 5.11106862e-07
Iter: 665 loss: 5.10605446e-07
Iter: 666 loss: 5.10989196e-07
Iter: 667 loss: 5.1030554e-07
Iter: 668 loss: 5.102047e-07
Iter: 669 loss: 5.09937593e-07
Iter: 670 loss: 5.09839765e-07
Iter: 671 loss: 5.09499159e-07
Iter: 672 loss: 5.11054907e-07
Iter: 673 loss: 5.09399399e-07
Iter: 674 loss: 5.09083861e-07
Iter: 675 loss: 5.09061238e-07
Iter: 676 loss: 5.08815447e-07
Iter: 677 loss: 5.08875871e-07
Iter: 678 loss: 5.08621724e-07
Iter: 679 loss: 5.08347568e-07
Iter: 680 loss: 5.0836627e-07
Iter: 681 loss: 5.08132757e-07
Iter: 682 loss: 5.07512141e-07
Iter: 683 loss: 5.0888093e-07
Iter: 684 loss: 5.07286e-07
Iter: 685 loss: 5.07020218e-07
Iter: 686 loss: 5.09981874e-07
Iter: 687 loss: 5.07019308e-07
Iter: 688 loss: 5.06788069e-07
Iter: 689 loss: 5.0673691e-07
Iter: 690 loss: 5.0659628e-07
Iter: 691 loss: 5.06286e-07
Iter: 692 loss: 5.06907554e-07
Iter: 693 loss: 5.06169613e-07
Iter: 694 loss: 5.05987884e-07
Iter: 695 loss: 5.06603556e-07
Iter: 696 loss: 5.05940079e-07
Iter: 697 loss: 5.05780463e-07
Iter: 698 loss: 5.05418029e-07
Iter: 699 loss: 5.10866585e-07
Iter: 700 loss: 5.05405865e-07
Iter: 701 loss: 5.05374373e-07
Iter: 702 loss: 5.05197477e-07
Iter: 703 loss: 5.04954642e-07
Iter: 704 loss: 5.04418381e-07
Iter: 705 loss: 5.12422957e-07
Iter: 706 loss: 5.04399736e-07
Iter: 707 loss: 5.038317e-07
Iter: 708 loss: 5.05131766e-07
Iter: 709 loss: 5.03596084e-07
Iter: 710 loss: 5.03437832e-07
Iter: 711 loss: 5.0330118e-07
Iter: 712 loss: 5.03177375e-07
Iter: 713 loss: 5.02763839e-07
Iter: 714 loss: 5.04342779e-07
Iter: 715 loss: 5.02618093e-07
Iter: 716 loss: 5.02891908e-07
Iter: 717 loss: 5.02509465e-07
Iter: 718 loss: 5.02398166e-07
Iter: 719 loss: 5.02343141e-07
Iter: 720 loss: 5.02304829e-07
Iter: 721 loss: 5.02128557e-07
Iter: 722 loss: 5.02099624e-07
Iter: 723 loss: 5.0196445e-07
Iter: 724 loss: 5.01684e-07
Iter: 725 loss: 5.05681e-07
Iter: 726 loss: 5.01682052e-07
Iter: 727 loss: 5.0156757e-07
Iter: 728 loss: 5.01339969e-07
Iter: 729 loss: 5.05912908e-07
Iter: 730 loss: 5.01335421e-07
Iter: 731 loss: 5.01040631e-07
Iter: 732 loss: 5.01557849e-07
Iter: 733 loss: 5.00892611e-07
Iter: 734 loss: 5.00814e-07
Iter: 735 loss: 5.00746239e-07
Iter: 736 loss: 5.00576959e-07
Iter: 737 loss: 5.00317867e-07
Iter: 738 loss: 5.00318151e-07
Iter: 739 loss: 4.99983344e-07
Iter: 740 loss: 4.9994577e-07
Iter: 741 loss: 4.99697535e-07
Iter: 742 loss: 4.99553096e-07
Iter: 743 loss: 4.9948892e-07
Iter: 744 loss: 4.99287239e-07
Iter: 745 loss: 4.98749671e-07
Iter: 746 loss: 5.02576313e-07
Iter: 747 loss: 4.98631266e-07
Iter: 748 loss: 4.98241207e-07
Iter: 749 loss: 5.034982e-07
Iter: 750 loss: 4.98237227e-07
Iter: 751 loss: 4.9793789e-07
Iter: 752 loss: 5.00693261e-07
Iter: 753 loss: 4.97912538e-07
Iter: 754 loss: 4.97747237e-07
Iter: 755 loss: 4.97430506e-07
Iter: 756 loss: 4.97428232e-07
Iter: 757 loss: 4.97443e-07
Iter: 758 loss: 4.97314318e-07
Iter: 759 loss: 4.97250767e-07
Iter: 760 loss: 4.9700634e-07
Iter: 761 loss: 4.98941e-07
Iter: 762 loss: 4.96981897e-07
Iter: 763 loss: 4.96692223e-07
Iter: 764 loss: 4.96708822e-07
Iter: 765 loss: 4.96486109e-07
Iter: 766 loss: 4.96197e-07
Iter: 767 loss: 4.96117593e-07
Iter: 768 loss: 4.95939446e-07
Iter: 769 loss: 4.9582161e-07
Iter: 770 loss: 4.95725601e-07
Iter: 771 loss: 4.95558766e-07
Iter: 772 loss: 4.95394715e-07
Iter: 773 loss: 4.9535447e-07
Iter: 774 loss: 4.95116751e-07
Iter: 775 loss: 4.95196787e-07
Iter: 776 loss: 4.9496839e-07
Iter: 777 loss: 4.94663e-07
Iter: 778 loss: 4.98912868e-07
Iter: 779 loss: 4.94663368e-07
Iter: 780 loss: 4.94551841e-07
Iter: 781 loss: 4.94282745e-07
Iter: 782 loss: 4.97184487e-07
Iter: 783 loss: 4.94260803e-07
Iter: 784 loss: 4.94002393e-07
Iter: 785 loss: 4.94002734e-07
Iter: 786 loss: 4.93730226e-07
Iter: 787 loss: 4.93621656e-07
Iter: 788 loss: 4.93496714e-07
Iter: 789 loss: 4.9338621e-07
Iter: 790 loss: 4.93367e-07
Iter: 791 loss: 4.93298785e-07
Iter: 792 loss: 4.93172138e-07
Iter: 793 loss: 4.95952e-07
Iter: 794 loss: 4.93170717e-07
Iter: 795 loss: 4.93028779e-07
Iter: 796 loss: 4.93122911e-07
Iter: 797 loss: 4.9292828e-07
Iter: 798 loss: 4.92745244e-07
Iter: 799 loss: 4.92823801e-07
Iter: 800 loss: 4.92617914e-07
Iter: 801 loss: 4.92478421e-07
Iter: 802 loss: 4.92462391e-07
Iter: 803 loss: 4.92359447e-07
Iter: 804 loss: 4.92277e-07
Iter: 805 loss: 4.92226548e-07
Iter: 806 loss: 4.92060735e-07
Iter: 807 loss: 4.91715582e-07
Iter: 808 loss: 4.97768497e-07
Iter: 809 loss: 4.9169455e-07
Iter: 810 loss: 4.91318076e-07
Iter: 811 loss: 4.95336621e-07
Iter: 812 loss: 4.91294088e-07
Iter: 813 loss: 4.91180174e-07
Iter: 814 loss: 4.91151354e-07
Iter: 815 loss: 4.91036e-07
Iter: 816 loss: 4.90865318e-07
Iter: 817 loss: 4.90859179e-07
Iter: 818 loss: 4.90690923e-07
Iter: 819 loss: 4.90519653e-07
Iter: 820 loss: 4.90475088e-07
Iter: 821 loss: 4.90269713e-07
Iter: 822 loss: 4.90250443e-07
Iter: 823 loss: 4.90126e-07
Iter: 824 loss: 4.89852e-07
Iter: 825 loss: 4.94310825e-07
Iter: 826 loss: 4.8984009e-07
Iter: 827 loss: 4.89630224e-07
Iter: 828 loss: 4.90192917e-07
Iter: 829 loss: 4.89563831e-07
Iter: 830 loss: 4.8933407e-07
Iter: 831 loss: 4.91730532e-07
Iter: 832 loss: 4.89335832e-07
Iter: 833 loss: 4.89188835e-07
Iter: 834 loss: 4.90376351e-07
Iter: 835 loss: 4.89193496e-07
Iter: 836 loss: 4.89098795e-07
Iter: 837 loss: 4.88936166e-07
Iter: 838 loss: 4.92798e-07
Iter: 839 loss: 4.88944693e-07
Iter: 840 loss: 4.88669855e-07
Iter: 841 loss: 4.89511137e-07
Iter: 842 loss: 4.88573903e-07
Iter: 843 loss: 4.88398769e-07
Iter: 844 loss: 4.8822551e-07
Iter: 845 loss: 4.88165256e-07
Iter: 846 loss: 4.87877969e-07
Iter: 847 loss: 4.90052571e-07
Iter: 848 loss: 4.87849547e-07
Iter: 849 loss: 4.87526e-07
Iter: 850 loss: 4.88587261e-07
Iter: 851 loss: 4.87434875e-07
Iter: 852 loss: 4.87274519e-07
Iter: 853 loss: 4.87191301e-07
Iter: 854 loss: 4.87118257e-07
Iter: 855 loss: 4.86994622e-07
Iter: 856 loss: 4.86986892e-07
Iter: 857 loss: 4.86848649e-07
Iter: 858 loss: 4.8662929e-07
Iter: 859 loss: 4.86626163e-07
Iter: 860 loss: 4.86425392e-07
Iter: 861 loss: 4.86209728e-07
Iter: 862 loss: 4.861771e-07
Iter: 863 loss: 4.86065971e-07
Iter: 864 loss: 4.86011e-07
Iter: 865 loss: 4.85921248e-07
Iter: 866 loss: 4.86135889e-07
Iter: 867 loss: 4.85877763e-07
Iter: 868 loss: 4.85758164e-07
Iter: 869 loss: 4.85896635e-07
Iter: 870 loss: 4.85724399e-07
Iter: 871 loss: 4.85616567e-07
Iter: 872 loss: 4.86127419e-07
Iter: 873 loss: 4.85619466e-07
Iter: 874 loss: 4.85521639e-07
Iter: 875 loss: 4.85553414e-07
Iter: 876 loss: 4.85466899e-07
Iter: 877 loss: 4.8535594e-07
Iter: 878 loss: 4.85478608e-07
Iter: 879 loss: 4.85292048e-07
Iter: 880 loss: 4.85124e-07
Iter: 881 loss: 4.85861165e-07
Iter: 882 loss: 4.85073e-07
Iter: 883 loss: 4.84925295e-07
Iter: 884 loss: 4.84651707e-07
Iter: 885 loss: 4.90388061e-07
Iter: 886 loss: 4.84640225e-07
Iter: 887 loss: 4.84388408e-07
Iter: 888 loss: 4.8439307e-07
Iter: 889 loss: 4.84131363e-07
Iter: 890 loss: 4.84712587e-07
Iter: 891 loss: 4.84053942e-07
Iter: 892 loss: 4.83937583e-07
Iter: 893 loss: 4.8370481e-07
Iter: 894 loss: 4.88300827e-07
Iter: 895 loss: 4.83685199e-07
Iter: 896 loss: 4.83643362e-07
Iter: 897 loss: 4.83587655e-07
Iter: 898 loss: 4.83500344e-07
Iter: 899 loss: 4.83439e-07
Iter: 900 loss: 4.83407462e-07
Iter: 901 loss: 4.83253e-07
Iter: 902 loss: 4.84208329e-07
Iter: 903 loss: 4.8323443e-07
Iter: 904 loss: 4.83113126e-07
Iter: 905 loss: 4.83153087e-07
Iter: 906 loss: 4.83019107e-07
Iter: 907 loss: 4.8283863e-07
Iter: 908 loss: 4.83349254e-07
Iter: 909 loss: 4.82812879e-07
Iter: 910 loss: 4.82669805e-07
Iter: 911 loss: 4.82428618e-07
Iter: 912 loss: 4.82410087e-07
Iter: 913 loss: 4.82243763e-07
Iter: 914 loss: 4.82215569e-07
Iter: 915 loss: 4.82120754e-07
Iter: 916 loss: 4.81930726e-07
Iter: 917 loss: 4.85770101e-07
Iter: 918 loss: 4.8192021e-07
Iter: 919 loss: 4.81723646e-07
Iter: 920 loss: 4.83529448e-07
Iter: 921 loss: 4.81706e-07
Iter: 922 loss: 4.81528332e-07
Iter: 923 loss: 4.82697374e-07
Iter: 924 loss: 4.8149866e-07
Iter: 925 loss: 4.81394409e-07
Iter: 926 loss: 4.81172947e-07
Iter: 927 loss: 4.85222472e-07
Iter: 928 loss: 4.81158793e-07
Iter: 929 loss: 4.80937729e-07
Iter: 930 loss: 4.8262e-07
Iter: 931 loss: 4.80934204e-07
Iter: 932 loss: 4.80805909e-07
Iter: 933 loss: 4.80793346e-07
Iter: 934 loss: 4.80747531e-07
Iter: 935 loss: 4.80735196e-07
Iter: 936 loss: 4.80690346e-07
Iter: 937 loss: 4.80556366e-07
Iter: 938 loss: 4.8077959e-07
Iter: 939 loss: 4.80487586e-07
Iter: 940 loss: 4.80418294e-07
Iter: 941 loss: 4.80909591e-07
Iter: 942 loss: 4.80383051e-07
Iter: 943 loss: 4.80323706e-07
Iter: 944 loss: 4.80175117e-07
Iter: 945 loss: 4.83332542e-07
Iter: 946 loss: 4.80183303e-07
Iter: 947 loss: 4.80058e-07
Iter: 948 loss: 4.8004415e-07
Iter: 949 loss: 4.79971277e-07
Iter: 950 loss: 4.79837468e-07
Iter: 951 loss: 4.79814275e-07
Iter: 952 loss: 4.79652499e-07
Iter: 953 loss: 4.79667733e-07
Iter: 954 loss: 4.79509481e-07
Iter: 955 loss: 4.79345e-07
Iter: 956 loss: 4.7934293e-07
Iter: 957 loss: 4.7924209e-07
Iter: 958 loss: 4.791018e-07
Iter: 959 loss: 4.82495921e-07
Iter: 960 loss: 4.79098389e-07
Iter: 961 loss: 4.78940194e-07
Iter: 962 loss: 4.78931e-07
Iter: 963 loss: 4.78791719e-07
Iter: 964 loss: 4.78696961e-07
Iter: 965 loss: 4.78679624e-07
Iter: 966 loss: 4.78566221e-07
Iter: 967 loss: 4.78579068e-07
Iter: 968 loss: 4.78495679e-07
Iter: 969 loss: 4.78323102e-07
Iter: 970 loss: 4.78721063e-07
Iter: 971 loss: 4.7824966e-07
Iter: 972 loss: 4.78114544e-07
Iter: 973 loss: 4.78560708e-07
Iter: 974 loss: 4.78081915e-07
Iter: 975 loss: 4.77946799e-07
Iter: 976 loss: 4.77974595e-07
Iter: 977 loss: 4.77860453e-07
Iter: 978 loss: 4.77714593e-07
Iter: 979 loss: 4.78912511e-07
Iter: 980 loss: 4.7771141e-07
Iter: 981 loss: 4.77595222e-07
Iter: 982 loss: 4.77776211e-07
Iter: 983 loss: 4.77524623e-07
Iter: 984 loss: 4.77432252e-07
Iter: 985 loss: 4.77354718e-07
Iter: 986 loss: 4.77314927e-07
Iter: 987 loss: 4.77190497e-07
Iter: 988 loss: 4.77192202e-07
Iter: 989 loss: 4.7707681e-07
Iter: 990 loss: 4.7691924e-07
Iter: 991 loss: 4.76915318e-07
Iter: 992 loss: 4.76727e-07
Iter: 993 loss: 4.76672142e-07
Iter: 994 loss: 4.76566754e-07
Iter: 995 loss: 4.7643357e-07
Iter: 996 loss: 4.76409468e-07
Iter: 997 loss: 4.76288761e-07
Iter: 998 loss: 4.7653964e-07
Iter: 999 loss: 4.76242974e-07
Iter: 1000 loss: 4.76097057e-07
Iter: 1001 loss: 4.76460116e-07
Iter: 1002 loss: 4.76059711e-07
Iter: 1003 loss: 4.75920814e-07
Iter: 1004 loss: 4.75990078e-07
Iter: 1005 loss: 4.75836487e-07
Iter: 1006 loss: 4.75689092e-07
Iter: 1007 loss: 4.76131163e-07
Iter: 1008 loss: 4.75629918e-07
Iter: 1009 loss: 4.75498695e-07
Iter: 1010 loss: 4.75575291e-07
Iter: 1011 loss: 4.75430767e-07
Iter: 1012 loss: 4.7522289e-07
Iter: 1013 loss: 4.7621046e-07
Iter: 1014 loss: 4.75184095e-07
Iter: 1015 loss: 4.7507865e-07
Iter: 1016 loss: 4.74958114e-07
Iter: 1017 loss: 4.74934609e-07
Iter: 1018 loss: 4.74830927e-07
Iter: 1019 loss: 4.74829022e-07
Iter: 1020 loss: 4.74722725e-07
Iter: 1021 loss: 4.74670117e-07
Iter: 1022 loss: 4.74635e-07
Iter: 1023 loss: 4.74508624e-07
Iter: 1024 loss: 4.74346308e-07
Iter: 1025 loss: 4.74331188e-07
Iter: 1026 loss: 4.74072067e-07
Iter: 1027 loss: 4.76495302e-07
Iter: 1028 loss: 4.74069651e-07
Iter: 1029 loss: 4.73835712e-07
Iter: 1030 loss: 4.75258929e-07
Iter: 1031 loss: 4.73777277e-07
Iter: 1032 loss: 4.73632724e-07
Iter: 1033 loss: 4.74507601e-07
Iter: 1034 loss: 4.73596e-07
Iter: 1035 loss: 4.73417231e-07
Iter: 1036 loss: 4.73467594e-07
Iter: 1037 loss: 4.73311928e-07
Iter: 1038 loss: 4.73167916e-07
Iter: 1039 loss: 4.74248793e-07
Iter: 1040 loss: 4.7316405e-07
Iter: 1041 loss: 4.73064233e-07
Iter: 1042 loss: 4.73096861e-07
Iter: 1043 loss: 4.7297965e-07
Iter: 1044 loss: 4.7284783e-07
Iter: 1045 loss: 4.73960483e-07
Iter: 1046 loss: 4.72844789e-07
Iter: 1047 loss: 4.72751481e-07
Iter: 1048 loss: 4.72689095e-07
Iter: 1049 loss: 4.7266272e-07
Iter: 1050 loss: 4.72562874e-07
Iter: 1051 loss: 4.72566796e-07
Iter: 1052 loss: 4.72497902e-07
Iter: 1053 loss: 4.72439183e-07
Iter: 1054 loss: 4.72402519e-07
Iter: 1055 loss: 4.72304265e-07
Iter: 1056 loss: 4.72236025e-07
Iter: 1057 loss: 4.72194017e-07
Iter: 1058 loss: 4.72045258e-07
Iter: 1059 loss: 4.73020521e-07
Iter: 1060 loss: 4.72031473e-07
Iter: 1061 loss: 4.71932196e-07
Iter: 1062 loss: 4.73060851e-07
Iter: 1063 loss: 4.71926569e-07
Iter: 1064 loss: 4.71833516e-07
Iter: 1065 loss: 4.72057536e-07
Iter: 1066 loss: 4.71805151e-07
Iter: 1067 loss: 4.71692942e-07
Iter: 1068 loss: 4.71634905e-07
Iter: 1069 loss: 4.71598185e-07
Iter: 1070 loss: 4.71470798e-07
Iter: 1071 loss: 4.71970765e-07
Iter: 1072 loss: 4.71430582e-07
Iter: 1073 loss: 4.71305555e-07
Iter: 1074 loss: 4.71268322e-07
Iter: 1075 loss: 4.71182403e-07
Iter: 1076 loss: 4.70984332e-07
Iter: 1077 loss: 4.73121133e-07
Iter: 1078 loss: 4.70975124e-07
Iter: 1079 loss: 4.70879485e-07
Iter: 1080 loss: 4.70811642e-07
Iter: 1081 loss: 4.70757982e-07
Iter: 1082 loss: 4.70649184e-07
Iter: 1083 loss: 4.72119439e-07
Iter: 1084 loss: 4.70645773e-07
Iter: 1085 loss: 4.70540442e-07
Iter: 1086 loss: 4.7058208e-07
Iter: 1087 loss: 4.70484338e-07
Iter: 1088 loss: 4.70372868e-07
Iter: 1089 loss: 4.70299142e-07
Iter: 1090 loss: 4.70265036e-07
Iter: 1091 loss: 4.70097859e-07
Iter: 1092 loss: 4.70350557e-07
Iter: 1093 loss: 4.7001754e-07
Iter: 1094 loss: 4.69877051e-07
Iter: 1095 loss: 4.69873783e-07
Iter: 1096 loss: 4.69780417e-07
Iter: 1097 loss: 4.69854712e-07
Iter: 1098 loss: 4.69711097e-07
Iter: 1099 loss: 4.6952735e-07
Iter: 1100 loss: 4.69751825e-07
Iter: 1101 loss: 4.69433189e-07
Iter: 1102 loss: 4.69300289e-07
Iter: 1103 loss: 4.69682135e-07
Iter: 1104 loss: 4.6925544e-07
Iter: 1105 loss: 4.69110176e-07
Iter: 1106 loss: 4.69107164e-07
Iter: 1107 loss: 4.69014878e-07
Iter: 1108 loss: 4.6879839e-07
Iter: 1109 loss: 4.70667175e-07
Iter: 1110 loss: 4.68784265e-07
Iter: 1111 loss: 4.68632692e-07
Iter: 1112 loss: 4.68455653e-07
Iter: 1113 loss: 4.68439794e-07
Iter: 1114 loss: 4.68259174e-07
Iter: 1115 loss: 4.70262762e-07
Iter: 1116 loss: 4.68252608e-07
Iter: 1117 loss: 4.68067725e-07
Iter: 1118 loss: 4.68239193e-07
Iter: 1119 loss: 4.67956312e-07
Iter: 1120 loss: 4.67805847e-07
Iter: 1121 loss: 4.67773646e-07
Iter: 1122 loss: 4.67665672e-07
Iter: 1123 loss: 4.67486103e-07
Iter: 1124 loss: 4.68019863e-07
Iter: 1125 loss: 4.67414225e-07
Iter: 1126 loss: 4.67301732e-07
Iter: 1127 loss: 4.67282803e-07
Iter: 1128 loss: 4.67199328e-07
Iter: 1129 loss: 4.67294399e-07
Iter: 1130 loss: 4.67121794e-07
Iter: 1131 loss: 4.66997506e-07
Iter: 1132 loss: 4.67336861e-07
Iter: 1133 loss: 4.66942453e-07
Iter: 1134 loss: 4.66829135e-07
Iter: 1135 loss: 4.67077513e-07
Iter: 1136 loss: 4.66797587e-07
Iter: 1137 loss: 4.66673555e-07
Iter: 1138 loss: 4.66823053e-07
Iter: 1139 loss: 4.66613727e-07
Iter: 1140 loss: 4.66484892e-07
Iter: 1141 loss: 4.67592827e-07
Iter: 1142 loss: 4.66470311e-07
Iter: 1143 loss: 4.66367567e-07
Iter: 1144 loss: 4.6625459e-07
Iter: 1145 loss: 4.66222076e-07
Iter: 1146 loss: 4.66112198e-07
Iter: 1147 loss: 4.67260065e-07
Iter: 1148 loss: 4.66095742e-07
Iter: 1149 loss: 4.65959829e-07
Iter: 1150 loss: 4.66141302e-07
Iter: 1151 loss: 4.6588417e-07
Iter: 1152 loss: 4.65773496e-07
Iter: 1153 loss: 4.65716965e-07
Iter: 1154 loss: 4.65670411e-07
Iter: 1155 loss: 4.65511135e-07
Iter: 1156 loss: 4.65670212e-07
Iter: 1157 loss: 4.65422517e-07
Iter: 1158 loss: 4.65308801e-07
Iter: 1159 loss: 4.65313349e-07
Iter: 1160 loss: 4.65223877e-07
Iter: 1161 loss: 4.65272478e-07
Iter: 1162 loss: 4.6514424e-07
Iter: 1163 loss: 4.64994514e-07
Iter: 1164 loss: 4.65603932e-07
Iter: 1165 loss: 4.64966035e-07
Iter: 1166 loss: 4.64869601e-07
Iter: 1167 loss: 4.64986e-07
Iter: 1168 loss: 4.64832e-07
Iter: 1169 loss: 4.64724963e-07
Iter: 1170 loss: 4.6481361e-07
Iter: 1171 loss: 4.64655187e-07
Iter: 1172 loss: 4.64528625e-07
Iter: 1173 loss: 4.65490189e-07
Iter: 1174 loss: 4.64519616e-07
Iter: 1175 loss: 4.64404138e-07
Iter: 1176 loss: 4.64286529e-07
Iter: 1177 loss: 4.64256317e-07
Iter: 1178 loss: 4.64138395e-07
Iter: 1179 loss: 4.65185082e-07
Iter: 1180 loss: 4.64130267e-07
Iter: 1181 loss: 4.63970821e-07
Iter: 1182 loss: 4.64278571e-07
Iter: 1183 loss: 4.63927279e-07
Iter: 1184 loss: 4.6383235e-07
Iter: 1185 loss: 4.63788069e-07
Iter: 1186 loss: 4.63729805e-07
Iter: 1187 loss: 4.63598951e-07
Iter: 1188 loss: 4.63818367e-07
Iter: 1189 loss: 4.63518234e-07
Iter: 1190 loss: 4.63390847e-07
Iter: 1191 loss: 4.65097969e-07
Iter: 1192 loss: 4.63388403e-07
Iter: 1193 loss: 4.6327574e-07
Iter: 1194 loss: 4.63305e-07
Iter: 1195 loss: 4.6318678e-07
Iter: 1196 loss: 4.63018921e-07
Iter: 1197 loss: 4.63947487e-07
Iter: 1198 loss: 4.62996354e-07
Iter: 1199 loss: 4.62858395e-07
Iter: 1200 loss: 4.62930245e-07
Iter: 1201 loss: 4.62777678e-07
Iter: 1202 loss: 4.62623973e-07
Iter: 1203 loss: 4.62908588e-07
Iter: 1204 loss: 4.62564088e-07
Iter: 1205 loss: 4.62397395e-07
Iter: 1206 loss: 4.63742822e-07
Iter: 1207 loss: 4.6238776e-07
Iter: 1208 loss: 4.62253865e-07
Iter: 1209 loss: 4.62163683e-07
Iter: 1210 loss: 4.62102832e-07
Iter: 1211 loss: 4.61986588e-07
Iter: 1212 loss: 4.62819855e-07
Iter: 1213 loss: 4.61970387e-07
Iter: 1214 loss: 4.61829586e-07
Iter: 1215 loss: 4.62291041e-07
Iter: 1216 loss: 4.61785191e-07
Iter: 1217 loss: 4.6170905e-07
Iter: 1218 loss: 4.61702712e-07
Iter: 1219 loss: 4.61661614e-07
Iter: 1220 loss: 4.61517374e-07
Iter: 1221 loss: 4.61639956e-07
Iter: 1222 loss: 4.61453e-07
Iter: 1223 loss: 4.61333116e-07
Iter: 1224 loss: 4.62911203e-07
Iter: 1225 loss: 4.61337606e-07
Iter: 1226 loss: 4.61226506e-07
Iter: 1227 loss: 4.61234521e-07
Iter: 1228 loss: 4.61158606e-07
Iter: 1229 loss: 4.60993903e-07
Iter: 1230 loss: 4.62008e-07
Iter: 1231 loss: 4.60963264e-07
Iter: 1232 loss: 4.60869671e-07
Iter: 1233 loss: 4.60929584e-07
Iter: 1234 loss: 4.60792336e-07
Iter: 1235 loss: 4.60672709e-07
Iter: 1236 loss: 4.60838891e-07
Iter: 1237 loss: 4.60604127e-07
Iter: 1238 loss: 4.60473444e-07
Iter: 1239 loss: 4.61269792e-07
Iter: 1240 loss: 4.60452554e-07
Iter: 1241 loss: 4.60310446e-07
Iter: 1242 loss: 4.6028498e-07
Iter: 1243 loss: 4.60185674e-07
Iter: 1244 loss: 4.60074943e-07
Iter: 1245 loss: 4.60701301e-07
Iter: 1246 loss: 4.60067582e-07
Iter: 1247 loss: 4.59937496e-07
Iter: 1248 loss: 4.60519061e-07
Iter: 1249 loss: 4.59910638e-07
Iter: 1250 loss: 4.59839498e-07
Iter: 1251 loss: 4.59805136e-07
Iter: 1252 loss: 4.59775094e-07
Iter: 1253 loss: 4.59649641e-07
Iter: 1254 loss: 4.59815453e-07
Iter: 1255 loss: 4.59622413e-07
Iter: 1256 loss: 4.59515377e-07
Iter: 1257 loss: 4.60481658e-07
Iter: 1258 loss: 4.59507817e-07
Iter: 1259 loss: 4.59413968e-07
Iter: 1260 loss: 4.59463791e-07
Iter: 1261 loss: 4.59362383e-07
Iter: 1262 loss: 4.59242898e-07
Iter: 1263 loss: 4.60332075e-07
Iter: 1264 loss: 4.59235196e-07
Iter: 1265 loss: 4.59153e-07
Iter: 1266 loss: 4.59145724e-07
Iter: 1267 loss: 4.59071316e-07
Iter: 1268 loss: 4.5897059e-07
Iter: 1269 loss: 4.59203193e-07
Iter: 1270 loss: 4.58940917e-07
Iter: 1271 loss: 4.58831323e-07
Iter: 1272 loss: 4.59393732e-07
Iter: 1273 loss: 4.58780221e-07
Iter: 1274 loss: 4.5868461e-07
Iter: 1275 loss: 4.58662583e-07
Iter: 1276 loss: 4.58588e-07
Iter: 1277 loss: 4.58490149e-07
Iter: 1278 loss: 4.58803811e-07
Iter: 1279 loss: 4.58441207e-07
Iter: 1280 loss: 4.58330589e-07
Iter: 1281 loss: 4.58949899e-07
Iter: 1282 loss: 4.58305e-07
Iter: 1283 loss: 4.58221336e-07
Iter: 1284 loss: 4.58139965e-07
Iter: 1285 loss: 4.58123168e-07
Iter: 1286 loss: 4.58007349e-07
Iter: 1287 loss: 4.58235e-07
Iter: 1288 loss: 4.57925921e-07
Iter: 1289 loss: 4.57787849e-07
Iter: 1290 loss: 4.59619514e-07
Iter: 1291 loss: 4.57786371e-07
Iter: 1292 loss: 4.57678738e-07
Iter: 1293 loss: 4.57863109e-07
Iter: 1294 loss: 4.57625532e-07
Iter: 1295 loss: 4.57543308e-07
Iter: 1296 loss: 4.58496174e-07
Iter: 1297 loss: 4.57528415e-07
Iter: 1298 loss: 4.57474044e-07
Iter: 1299 loss: 4.57458668e-07
Iter: 1300 loss: 4.57410692e-07
Iter: 1301 loss: 4.57307408e-07
Iter: 1302 loss: 4.57640596e-07
Iter: 1303 loss: 4.57275775e-07
Iter: 1304 loss: 4.57189628e-07
Iter: 1305 loss: 4.57665806e-07
Iter: 1306 loss: 4.57178658e-07
Iter: 1307 loss: 4.57093904e-07
Iter: 1308 loss: 4.57081825e-07
Iter: 1309 loss: 4.57034275e-07
Iter: 1310 loss: 4.56948783e-07
Iter: 1311 loss: 4.57162656e-07
Iter: 1312 loss: 4.56924965e-07
Iter: 1313 loss: 4.56815343e-07
Iter: 1314 loss: 4.57462e-07
Iter: 1315 loss: 4.5679468e-07
Iter: 1316 loss: 4.56730788e-07
Iter: 1317 loss: 4.56639441e-07
Iter: 1318 loss: 4.56644841e-07
Iter: 1319 loss: 4.56519388e-07
Iter: 1320 loss: 4.56692817e-07
Iter: 1321 loss: 4.56453563e-07
Iter: 1322 loss: 4.56323363e-07
Iter: 1323 loss: 4.58060157e-07
Iter: 1324 loss: 4.56329872e-07
Iter: 1325 loss: 4.56251797e-07
Iter: 1326 loss: 4.56400642e-07
Iter: 1327 loss: 4.56191344e-07
Iter: 1328 loss: 4.56115487e-07
Iter: 1329 loss: 4.56843907e-07
Iter: 1330 loss: 4.56116652e-07
Iter: 1331 loss: 4.56048269e-07
Iter: 1332 loss: 4.56029454e-07
Iter: 1333 loss: 4.55982786e-07
Iter: 1334 loss: 4.55906076e-07
Iter: 1335 loss: 4.56158887e-07
Iter: 1336 loss: 4.55903432e-07
Iter: 1337 loss: 4.5583613e-07
Iter: 1338 loss: 4.56154822e-07
Iter: 1339 loss: 4.55819304e-07
Iter: 1340 loss: 4.55774227e-07
Iter: 1341 loss: 4.55754e-07
Iter: 1342 loss: 4.55690866e-07
Iter: 1343 loss: 4.55632573e-07
Iter: 1344 loss: 4.55740235e-07
Iter: 1345 loss: 4.55611257e-07
Iter: 1346 loss: 4.55521558e-07
Iter: 1347 loss: 4.56125292e-07
Iter: 1348 loss: 4.55506097e-07
Iter: 1349 loss: 4.55448912e-07
Iter: 1350 loss: 4.55337329e-07
Iter: 1351 loss: 4.55332611e-07
Iter: 1352 loss: 4.55204713e-07
Iter: 1353 loss: 4.55515163e-07
Iter: 1354 loss: 4.55183738e-07
Iter: 1355 loss: 4.55050355e-07
Iter: 1356 loss: 4.56444639e-07
Iter: 1357 loss: 4.55054646e-07
Iter: 1358 loss: 4.54966539e-07
Iter: 1359 loss: 4.55114332e-07
Iter: 1360 loss: 4.54927829e-07
Iter: 1361 loss: 4.54840716e-07
Iter: 1362 loss: 4.55553447e-07
Iter: 1363 loss: 4.54823748e-07
Iter: 1364 loss: 4.54765598e-07
Iter: 1365 loss: 4.54767843e-07
Iter: 1366 loss: 4.54722056e-07
Iter: 1367 loss: 4.54635057e-07
Iter: 1368 loss: 4.54792342e-07
Iter: 1369 loss: 4.54591657e-07
Iter: 1370 loss: 4.54527623e-07
Iter: 1371 loss: 4.55039952e-07
Iter: 1372 loss: 4.54509e-07
Iter: 1373 loss: 4.54440681e-07
Iter: 1374 loss: 4.54384178e-07
Iter: 1375 loss: 4.54350356e-07
Iter: 1376 loss: 4.54255314e-07
Iter: 1377 loss: 4.54489225e-07
Iter: 1378 loss: 4.54217457e-07
Iter: 1379 loss: 4.54122414e-07
Iter: 1380 loss: 4.55083608e-07
Iter: 1381 loss: 4.5409675e-07
Iter: 1382 loss: 4.54040844e-07
Iter: 1383 loss: 4.53932557e-07
Iter: 1384 loss: 4.56605619e-07
Iter: 1385 loss: 4.53932614e-07
Iter: 1386 loss: 4.53791927e-07
Iter: 1387 loss: 4.54259037e-07
Iter: 1388 loss: 4.53751056e-07
Iter: 1389 loss: 4.53666189e-07
Iter: 1390 loss: 4.53673579e-07
Iter: 1391 loss: 4.53603661e-07
Iter: 1392 loss: 4.53740086e-07
Iter: 1393 loss: 4.53572511e-07
Iter: 1394 loss: 4.53496654e-07
Iter: 1395 loss: 4.53989742e-07
Iter: 1396 loss: 4.53517316e-07
Iter: 1397 loss: 4.53462292e-07
Iter: 1398 loss: 4.53427276e-07
Iter: 1399 loss: 4.53395785e-07
Iter: 1400 loss: 4.53322542e-07
Iter: 1401 loss: 4.53590133e-07
Iter: 1402 loss: 4.53306569e-07
Iter: 1403 loss: 4.53226e-07
Iter: 1404 loss: 4.53571715e-07
Iter: 1405 loss: 4.53213659e-07
Iter: 1406 loss: 4.53136948e-07
Iter: 1407 loss: 4.53119213e-07
Iter: 1408 loss: 4.53068168e-07
Iter: 1409 loss: 4.52990918e-07
Iter: 1410 loss: 4.53116598e-07
Iter: 1411 loss: 4.52953827e-07
Iter: 1412 loss: 4.52827607e-07
Iter: 1413 loss: 4.54004464e-07
Iter: 1414 loss: 4.52818597e-07
Iter: 1415 loss: 4.52776817e-07
Iter: 1416 loss: 4.5265196e-07
Iter: 1417 loss: 4.55319821e-07
Iter: 1418 loss: 4.52662135e-07
Iter: 1419 loss: 4.52543162e-07
Iter: 1420 loss: 4.52807626e-07
Iter: 1421 loss: 4.52498568e-07
Iter: 1422 loss: 4.52398751e-07
Iter: 1423 loss: 4.52397899e-07
Iter: 1424 loss: 4.52350719e-07
Iter: 1425 loss: 4.5241012e-07
Iter: 1426 loss: 4.5233665e-07
Iter: 1427 loss: 4.52271905e-07
Iter: 1428 loss: 4.52681462e-07
Iter: 1429 loss: 4.52268921e-07
Iter: 1430 loss: 4.52223219e-07
Iter: 1431 loss: 4.52198378e-07
Iter: 1432 loss: 4.52156769e-07
Iter: 1433 loss: 4.52102427e-07
Iter: 1434 loss: 4.52234332e-07
Iter: 1435 loss: 4.52073152e-07
Iter: 1436 loss: 4.5199485e-07
Iter: 1437 loss: 4.52338156e-07
Iter: 1438 loss: 4.5198e-07
Iter: 1439 loss: 4.51900291e-07
Iter: 1440 loss: 4.51906772e-07
Iter: 1441 loss: 4.51828072e-07
Iter: 1442 loss: 4.51748377e-07
Iter: 1443 loss: 4.51754e-07
Iter: 1444 loss: 4.51677863e-07
Iter: 1445 loss: 4.51567701e-07
Iter: 1446 loss: 4.53153405e-07
Iter: 1447 loss: 4.51578188e-07
Iter: 1448 loss: 4.51497044e-07
Iter: 1449 loss: 4.51376479e-07
Iter: 1450 loss: 4.54285157e-07
Iter: 1451 loss: 4.51360364e-07
Iter: 1452 loss: 4.51221808e-07
Iter: 1453 loss: 4.51553433e-07
Iter: 1454 loss: 4.51169853e-07
Iter: 1455 loss: 4.51049203e-07
Iter: 1456 loss: 4.5105196e-07
Iter: 1457 loss: 4.50968855e-07
Iter: 1458 loss: 4.51040364e-07
Iter: 1459 loss: 4.50909397e-07
Iter: 1460 loss: 4.50815605e-07
Iter: 1461 loss: 4.51709525e-07
Iter: 1462 loss: 4.50822029e-07
Iter: 1463 loss: 4.50731704e-07
Iter: 1464 loss: 4.50692824e-07
Iter: 1465 loss: 4.50647065e-07
Iter: 1466 loss: 4.50547191e-07
Iter: 1467 loss: 4.50845249e-07
Iter: 1468 loss: 4.50513653e-07
Iter: 1469 loss: 4.50421851e-07
Iter: 1470 loss: 4.50820096e-07
Iter: 1471 loss: 4.50385755e-07
Iter: 1472 loss: 4.5029023e-07
Iter: 1473 loss: 4.50427962e-07
Iter: 1474 loss: 4.50218778e-07
Iter: 1475 loss: 4.50133655e-07
Iter: 1476 loss: 4.50136099e-07
Iter: 1477 loss: 4.50067603e-07
Iter: 1478 loss: 4.4994772e-07
Iter: 1479 loss: 4.51756136e-07
Iter: 1480 loss: 4.49949084e-07
Iter: 1481 loss: 4.49895055e-07
Iter: 1482 loss: 4.49786711e-07
Iter: 1483 loss: 4.49788246e-07
Iter: 1484 loss: 4.4966913e-07
Iter: 1485 loss: 4.49761586e-07
Iter: 1486 loss: 4.49622462e-07
Iter: 1487 loss: 4.49537481e-07
Iter: 1488 loss: 4.49517955e-07
Iter: 1489 loss: 4.49451619e-07
Iter: 1490 loss: 4.49466256e-07
Iter: 1491 loss: 4.49401455e-07
Iter: 1492 loss: 4.49308516e-07
Iter: 1493 loss: 4.5009557e-07
Iter: 1494 loss: 4.49312665e-07
Iter: 1495 loss: 4.49215975e-07
Iter: 1496 loss: 4.4916365e-07
Iter: 1497 loss: 4.49138838e-07
Iter: 1498 loss: 4.48998804e-07
Iter: 1499 loss: 4.49268782e-07
Iter: 1500 loss: 4.48968308e-07
Iter: 1501 loss: 4.48845924e-07
Iter: 1502 loss: 4.49424022e-07
Iter: 1503 loss: 4.48805736e-07
Iter: 1504 loss: 4.48673347e-07
Iter: 1505 loss: 4.48901972e-07
Iter: 1506 loss: 4.48615822e-07
Iter: 1507 loss: 4.4851754e-07
Iter: 1508 loss: 4.48487924e-07
Iter: 1509 loss: 4.48431962e-07
Iter: 1510 loss: 4.4831711e-07
Iter: 1511 loss: 4.48302018e-07
Iter: 1512 loss: 4.48240939e-07
Iter: 1513 loss: 4.48123274e-07
Iter: 1514 loss: 4.48128333e-07
Iter: 1515 loss: 4.48012543e-07
Iter: 1516 loss: 4.48202627e-07
Iter: 1517 loss: 4.47928386e-07
Iter: 1518 loss: 4.47843433e-07
Iter: 1519 loss: 4.47851107e-07
Iter: 1520 loss: 4.47760158e-07
Iter: 1521 loss: 4.47724e-07
Iter: 1522 loss: 4.47691036e-07
Iter: 1523 loss: 4.47545887e-07
Iter: 1524 loss: 4.48553067e-07
Iter: 1525 loss: 4.4754222e-07
Iter: 1526 loss: 4.47416312e-07
Iter: 1527 loss: 4.47378142e-07
Iter: 1528 loss: 4.47275795e-07
Iter: 1529 loss: 4.47112711e-07
Iter: 1530 loss: 4.47517778e-07
Iter: 1531 loss: 4.47072864e-07
Iter: 1532 loss: 4.46915465e-07
Iter: 1533 loss: 4.47717184e-07
Iter: 1534 loss: 4.46880563e-07
Iter: 1535 loss: 4.46728961e-07
Iter: 1536 loss: 4.47012383e-07
Iter: 1537 loss: 4.46662341e-07
Iter: 1538 loss: 4.46541236e-07
Iter: 1539 loss: 4.46528873e-07
Iter: 1540 loss: 4.46448325e-07
Iter: 1541 loss: 4.4630292e-07
Iter: 1542 loss: 4.46305961e-07
Iter: 1543 loss: 4.46201682e-07
Iter: 1544 loss: 4.46110533e-07
Iter: 1545 loss: 4.46104309e-07
Iter: 1546 loss: 4.45966066e-07
Iter: 1547 loss: 4.46141712e-07
Iter: 1548 loss: 4.45900611e-07
Iter: 1549 loss: 4.45771263e-07
Iter: 1550 loss: 4.47194736e-07
Iter: 1551 loss: 4.45760662e-07
Iter: 1552 loss: 4.45639387e-07
Iter: 1553 loss: 4.45683838e-07
Iter: 1554 loss: 4.45556765e-07
Iter: 1555 loss: 4.45394903e-07
Iter: 1556 loss: 4.46751869e-07
Iter: 1557 loss: 4.45394221e-07
Iter: 1558 loss: 4.45278715e-07
Iter: 1559 loss: 4.4537893e-07
Iter: 1560 loss: 4.4521272e-07
Iter: 1561 loss: 4.45103609e-07
Iter: 1562 loss: 4.45347439e-07
Iter: 1563 loss: 4.45069588e-07
Iter: 1564 loss: 4.44989212e-07
Iter: 1565 loss: 4.45499751e-07
Iter: 1566 loss: 4.44977559e-07
Iter: 1567 loss: 4.44908267e-07
Iter: 1568 loss: 4.45073852e-07
Iter: 1569 loss: 4.44870409e-07
Iter: 1570 loss: 4.44816351e-07
Iter: 1571 loss: 4.44736884e-07
Iter: 1572 loss: 4.44738475e-07
Iter: 1573 loss: 4.44673901e-07
Iter: 1574 loss: 4.44669098e-07
Iter: 1575 loss: 4.44609952e-07
Iter: 1576 loss: 4.44546714e-07
Iter: 1577 loss: 4.44536681e-07
Iter: 1578 loss: 4.44433169e-07
Iter: 1579 loss: 4.44503712e-07
Iter: 1580 loss: 4.44382238e-07
Iter: 1581 loss: 4.44281198e-07
Iter: 1582 loss: 4.44283899e-07
Iter: 1583 loss: 4.44197951e-07
Iter: 1584 loss: 4.44291402e-07
Iter: 1585 loss: 4.44153528e-07
Iter: 1586 loss: 4.44056042e-07
Iter: 1587 loss: 4.44785e-07
Iter: 1588 loss: 4.44065392e-07
Iter: 1589 loss: 4.43986977e-07
Iter: 1590 loss: 4.4404365e-07
Iter: 1591 loss: 4.43954036e-07
Iter: 1592 loss: 4.4389418e-07
Iter: 1593 loss: 4.43993315e-07
Iter: 1594 loss: 4.43861495e-07
Iter: 1595 loss: 4.43798228e-07
Iter: 1596 loss: 4.44151283e-07
Iter: 1597 loss: 4.43776685e-07
Iter: 1598 loss: 4.43728879e-07
Iter: 1599 loss: 4.4386303e-07
Iter: 1600 loss: 4.43696933e-07
Iter: 1601 loss: 4.4364316e-07
Iter: 1602 loss: 4.43600641e-07
Iter: 1603 loss: 4.43590579e-07
Iter: 1604 loss: 4.43536067e-07
Iter: 1605 loss: 4.43530723e-07
Iter: 1606 loss: 4.4348468e-07
Iter: 1607 loss: 4.43411835e-07
Iter: 1608 loss: 4.44945073e-07
Iter: 1609 loss: 4.43408624e-07
Iter: 1610 loss: 4.43312302e-07
Iter: 1611 loss: 4.4341931e-07
Iter: 1612 loss: 4.43254123e-07
Iter: 1613 loss: 4.43161866e-07
Iter: 1614 loss: 4.444924e-07
Iter: 1615 loss: 4.43170677e-07
Iter: 1616 loss: 4.43089164e-07
Iter: 1617 loss: 4.43184376e-07
Iter: 1618 loss: 4.43046815e-07
Iter: 1619 loss: 4.4299631e-07
Iter: 1620 loss: 4.43402172e-07
Iter: 1621 loss: 4.42974397e-07
Iter: 1622 loss: 4.42896777e-07
Iter: 1623 loss: 4.42925256e-07
Iter: 1624 loss: 4.42846556e-07
Iter: 1625 loss: 4.42767771e-07
Iter: 1626 loss: 4.42848403e-07
Iter: 1627 loss: 4.42735342e-07
Iter: 1628 loss: 4.42649082e-07
Iter: 1629 loss: 4.43035731e-07
Iter: 1630 loss: 4.4262174e-07
Iter: 1631 loss: 4.42545399e-07
Iter: 1632 loss: 4.42824785e-07
Iter: 1633 loss: 4.42542728e-07
Iter: 1634 loss: 4.42469485e-07
Iter: 1635 loss: 4.42357191e-07
Iter: 1636 loss: 4.45182849e-07
Iter: 1637 loss: 4.42353723e-07
Iter: 1638 loss: 4.42278406e-07
Iter: 1639 loss: 4.42259363e-07
Iter: 1640 loss: 4.4219712e-07
Iter: 1641 loss: 4.42100827e-07
Iter: 1642 loss: 4.42102078e-07
Iter: 1643 loss: 4.42000101e-07
Iter: 1644 loss: 4.42043472e-07
Iter: 1645 loss: 4.41930126e-07
Iter: 1646 loss: 4.41839404e-07
Iter: 1647 loss: 4.43002392e-07
Iter: 1648 loss: 4.41837e-07
Iter: 1649 loss: 4.41750444e-07
Iter: 1650 loss: 4.41964573e-07
Iter: 1651 loss: 4.41723614e-07
Iter: 1652 loss: 4.41659694e-07
Iter: 1653 loss: 4.42020792e-07
Iter: 1654 loss: 4.41645909e-07
Iter: 1655 loss: 4.41572126e-07
Iter: 1656 loss: 4.41672597e-07
Iter: 1657 loss: 4.41529949e-07
Iter: 1658 loss: 4.41468103e-07
Iter: 1659 loss: 4.41540521e-07
Iter: 1660 loss: 4.41439227e-07
Iter: 1661 loss: 4.41379711e-07
Iter: 1662 loss: 4.41795493e-07
Iter: 1663 loss: 4.41359049e-07
Iter: 1664 loss: 4.41288762e-07
Iter: 1665 loss: 4.41509542e-07
Iter: 1666 loss: 4.4127529e-07
Iter: 1667 loss: 4.41197813e-07
Iter: 1668 loss: 4.41081653e-07
Iter: 1669 loss: 4.4327146e-07
Iter: 1670 loss: 4.41087934e-07
Iter: 1671 loss: 4.41043596e-07
Iter: 1672 loss: 4.41007018e-07
Iter: 1673 loss: 4.40946962e-07
Iter: 1674 loss: 4.40862493e-07
Iter: 1675 loss: 4.40877074e-07
Iter: 1676 loss: 4.40767451e-07
Iter: 1677 loss: 4.40826767e-07
Iter: 1678 loss: 4.40727547e-07
Iter: 1679 loss: 4.40629151e-07
Iter: 1680 loss: 4.41259715e-07
Iter: 1681 loss: 4.40622387e-07
Iter: 1682 loss: 4.40567732e-07
Iter: 1683 loss: 4.40721237e-07
Iter: 1684 loss: 4.40526804e-07
Iter: 1685 loss: 4.40469648e-07
Iter: 1686 loss: 4.4087119e-07
Iter: 1687 loss: 4.40455409e-07
Iter: 1688 loss: 4.4038137e-07
Iter: 1689 loss: 4.40479255e-07
Iter: 1690 loss: 4.40364687e-07
Iter: 1691 loss: 4.40300084e-07
Iter: 1692 loss: 4.4031259e-07
Iter: 1693 loss: 4.4026703e-07
Iter: 1694 loss: 4.40183953e-07
Iter: 1695 loss: 4.40871872e-07
Iter: 1696 loss: 4.40187961e-07
Iter: 1697 loss: 4.40109972e-07
Iter: 1698 loss: 4.40244548e-07
Iter: 1699 loss: 4.40081521e-07
Iter: 1700 loss: 4.40036246e-07
Iter: 1701 loss: 4.39931569e-07
Iter: 1702 loss: 4.39932421e-07
Iter: 1703 loss: 4.3986492e-07
Iter: 1704 loss: 4.39827772e-07
Iter: 1705 loss: 4.3979108e-07
Iter: 1706 loss: 4.3971022e-07
Iter: 1707 loss: 4.39696976e-07
Iter: 1708 loss: 4.39609835e-07
Iter: 1709 loss: 4.39685778e-07
Iter: 1710 loss: 4.39553617e-07
Iter: 1711 loss: 4.39460877e-07
Iter: 1712 loss: 4.40116594e-07
Iter: 1713 loss: 4.39466703e-07
Iter: 1714 loss: 4.39381381e-07
Iter: 1715 loss: 4.39545e-07
Iter: 1716 loss: 4.39355574e-07
Iter: 1717 loss: 4.39252659e-07
Iter: 1718 loss: 4.39570329e-07
Iter: 1719 loss: 4.39230803e-07
Iter: 1720 loss: 4.39151734e-07
Iter: 1721 loss: 4.39359781e-07
Iter: 1722 loss: 4.39081305e-07
Iter: 1723 loss: 4.390036e-07
Iter: 1724 loss: 4.38918732e-07
Iter: 1725 loss: 4.3892544e-07
Iter: 1726 loss: 4.38768495e-07
Iter: 1727 loss: 4.40283031e-07
Iter: 1728 loss: 4.3876048e-07
Iter: 1729 loss: 4.38665353e-07
Iter: 1730 loss: 4.38964889e-07
Iter: 1731 loss: 4.38627097e-07
Iter: 1732 loss: 4.38553741e-07
Iter: 1733 loss: 4.38409529e-07
Iter: 1734 loss: 4.38430675e-07
Iter: 1735 loss: 4.38397535e-07
Iter: 1736 loss: 4.38334894e-07
Iter: 1737 loss: 4.38283e-07
Iter: 1738 loss: 4.38221718e-07
Iter: 1739 loss: 4.38211657e-07
Iter: 1740 loss: 4.38132361e-07
Iter: 1741 loss: 4.3821413e-07
Iter: 1742 loss: 4.38077791e-07
Iter: 1743 loss: 4.3798957e-07
Iter: 1744 loss: 4.38664756e-07
Iter: 1745 loss: 4.38000825e-07
Iter: 1746 loss: 4.37910927e-07
Iter: 1747 loss: 4.38107463e-07
Iter: 1748 loss: 4.37867641e-07
Iter: 1749 loss: 4.37780443e-07
Iter: 1750 loss: 4.38008925e-07
Iter: 1751 loss: 4.37742301e-07
Iter: 1752 loss: 4.37622901e-07
Iter: 1753 loss: 4.37997357e-07
Iter: 1754 loss: 4.37597294e-07
Iter: 1755 loss: 4.37513506e-07
Iter: 1756 loss: 4.3744717e-07
Iter: 1757 loss: 4.37402377e-07
Iter: 1758 loss: 4.3729338e-07
Iter: 1759 loss: 4.38796292e-07
Iter: 1760 loss: 4.37303072e-07
Iter: 1761 loss: 4.37231108e-07
Iter: 1762 loss: 4.37522175e-07
Iter: 1763 loss: 4.37207461e-07
Iter: 1764 loss: 4.37131405e-07
Iter: 1765 loss: 4.37054496e-07
Iter: 1766 loss: 4.37039262e-07
Iter: 1767 loss: 4.37007259e-07
Iter: 1768 loss: 4.36989069e-07
Iter: 1769 loss: 4.36940724e-07
Iter: 1770 loss: 4.36897125e-07
Iter: 1771 loss: 4.36884648e-07
Iter: 1772 loss: 4.36827321e-07
Iter: 1773 loss: 4.36838803e-07
Iter: 1774 loss: 4.36774542e-07
Iter: 1775 loss: 4.36695302e-07
Iter: 1776 loss: 4.37328424e-07
Iter: 1777 loss: 4.36707666e-07
Iter: 1778 loss: 4.36639255e-07
Iter: 1779 loss: 4.36834739e-07
Iter: 1780 loss: 4.36615039e-07
Iter: 1781 loss: 4.36555268e-07
Iter: 1782 loss: 4.36671712e-07
Iter: 1783 loss: 4.36521731e-07
Iter: 1784 loss: 4.36479354e-07
Iter: 1785 loss: 4.36765617e-07
Iter: 1786 loss: 4.36433766e-07
Iter: 1787 loss: 4.36392668e-07
Iter: 1788 loss: 4.36363052e-07
Iter: 1789 loss: 4.36353e-07
Iter: 1790 loss: 4.3628674e-07
Iter: 1791 loss: 4.37010499e-07
Iter: 1792 loss: 4.36291941e-07
Iter: 1793 loss: 4.36237656e-07
Iter: 1794 loss: 4.36363564e-07
Iter: 1795 loss: 4.3621057e-07
Iter: 1796 loss: 4.36163816e-07
Iter: 1797 loss: 4.36083809e-07
Iter: 1798 loss: 4.38142706e-07
Iter: 1799 loss: 4.36095348e-07
Iter: 1800 loss: 4.36051408e-07
Iter: 1801 loss: 4.36036515e-07
Iter: 1802 loss: 4.35989733e-07
Iter: 1803 loss: 4.35907168e-07
Iter: 1804 loss: 4.35922e-07
Iter: 1805 loss: 4.35833897e-07
Iter: 1806 loss: 4.35867889e-07
Iter: 1807 loss: 4.35766651e-07
Iter: 1808 loss: 4.35685337e-07
Iter: 1809 loss: 4.36836274e-07
Iter: 1810 loss: 4.35680931e-07
Iter: 1811 loss: 4.35617892e-07
Iter: 1812 loss: 4.35798597e-07
Iter: 1813 loss: 4.3559163e-07
Iter: 1814 loss: 4.355351e-07
Iter: 1815 loss: 4.35601123e-07
Iter: 1816 loss: 4.35495508e-07
Iter: 1817 loss: 4.35421896e-07
Iter: 1818 loss: 4.35944628e-07
Iter: 1819 loss: 4.35408595e-07
Iter: 1820 loss: 4.35352717e-07
Iter: 1821 loss: 4.35292975e-07
Iter: 1822 loss: 4.35279219e-07
Iter: 1823 loss: 4.35205692e-07
Iter: 1824 loss: 4.36072042e-07
Iter: 1825 loss: 4.35202452e-07
Iter: 1826 loss: 4.35155641e-07
Iter: 1827 loss: 4.35351069e-07
Iter: 1828 loss: 4.35117528e-07
Iter: 1829 loss: 4.35064862e-07
Iter: 1830 loss: 4.3497721e-07
Iter: 1831 loss: 4.34990454e-07
Iter: 1832 loss: 4.34951232e-07
Iter: 1833 loss: 4.34928211e-07
Iter: 1834 loss: 4.34880121e-07
Iter: 1835 loss: 4.34852666e-07
Iter: 1836 loss: 4.34839336e-07
Iter: 1837 loss: 4.34752707e-07
Iter: 1838 loss: 4.34801819e-07
Iter: 1839 loss: 4.34715759e-07
Iter: 1840 loss: 4.34671733e-07
Iter: 1841 loss: 4.35274217e-07
Iter: 1842 loss: 4.34677133e-07
Iter: 1843 loss: 4.34625377e-07
Iter: 1844 loss: 4.34759528e-07
Iter: 1845 loss: 4.34591328e-07
Iter: 1846 loss: 4.34558e-07
Iter: 1847 loss: 4.3456032e-07
Iter: 1848 loss: 4.34515698e-07
Iter: 1849 loss: 4.34433275e-07
Iter: 1850 loss: 4.35019473e-07
Iter: 1851 loss: 4.34435179e-07
Iter: 1852 loss: 4.34392348e-07
Iter: 1853 loss: 4.34392661e-07
Iter: 1854 loss: 4.34354206e-07
Iter: 1855 loss: 4.34313961e-07
Iter: 1856 loss: 4.34811653e-07
Iter: 1857 loss: 4.34310721e-07
Iter: 1858 loss: 4.34272607e-07
Iter: 1859 loss: 4.34329081e-07
Iter: 1860 loss: 4.34249699e-07
Iter: 1861 loss: 4.3420124e-07
Iter: 1862 loss: 4.34137888e-07
Iter: 1863 loss: 4.34133852e-07
Iter: 1864 loss: 4.34095057e-07
Iter: 1865 loss: 4.34079823e-07
Iter: 1866 loss: 4.34054641e-07
Iter: 1867 loss: 4.34006211e-07
Iter: 1868 loss: 4.34016073e-07
Iter: 1869 loss: 4.33954938e-07
Iter: 1870 loss: 4.33945274e-07
Iter: 1871 loss: 4.33892353e-07
Iter: 1872 loss: 4.33823857e-07
Iter: 1873 loss: 4.34612787e-07
Iter: 1874 loss: 4.33829115e-07
Iter: 1875 loss: 4.33772186e-07
Iter: 1876 loss: 4.33943455e-07
Iter: 1877 loss: 4.33747573e-07
Iter: 1878 loss: 4.33699455e-07
Iter: 1879 loss: 4.33665775e-07
Iter: 1880 loss: 4.33633488e-07
Iter: 1881 loss: 4.33539583e-07
Iter: 1882 loss: 4.34742049e-07
Iter: 1883 loss: 4.33528783e-07
Iter: 1884 loss: 4.33506159e-07
Iter: 1885 loss: 4.33458723e-07
Iter: 1886 loss: 4.33439823e-07
Iter: 1887 loss: 4.33369593e-07
Iter: 1888 loss: 4.33952181e-07
Iter: 1889 loss: 4.3336226e-07
Iter: 1890 loss: 4.3329274e-07
Iter: 1891 loss: 4.33467193e-07
Iter: 1892 loss: 4.3327276e-07
Iter: 1893 loss: 4.33215035e-07
Iter: 1894 loss: 4.33152962e-07
Iter: 1895 loss: 4.33140599e-07
Iter: 1896 loss: 4.33079947e-07
Iter: 1897 loss: 4.33086484e-07
Iter: 1898 loss: 4.33036632e-07
Iter: 1899 loss: 4.33015572e-07
Iter: 1900 loss: 4.32993602e-07
Iter: 1901 loss: 4.32929085e-07
Iter: 1902 loss: 4.32934172e-07
Iter: 1903 loss: 4.32868489e-07
Iter: 1904 loss: 4.32806615e-07
Iter: 1905 loss: 4.33489561e-07
Iter: 1906 loss: 4.32790841e-07
Iter: 1907 loss: 4.32751392e-07
Iter: 1908 loss: 4.33002299e-07
Iter: 1909 loss: 4.32743946e-07
Iter: 1910 loss: 4.32688921e-07
Iter: 1911 loss: 4.32590895e-07
Iter: 1912 loss: 4.32578929e-07
Iter: 1913 loss: 4.32503299e-07
Iter: 1914 loss: 4.32511939e-07
Iter: 1915 loss: 4.32472945e-07
Iter: 1916 loss: 4.32420961e-07
Iter: 1917 loss: 4.32412179e-07
Iter: 1918 loss: 4.32353545e-07
Iter: 1919 loss: 4.3274224e-07
Iter: 1920 loss: 4.32353829e-07
Iter: 1921 loss: 4.3230537e-07
Iter: 1922 loss: 4.3246169e-07
Iter: 1923 loss: 4.32315517e-07
Iter: 1924 loss: 4.32267825e-07
Iter: 1925 loss: 4.32237329e-07
Iter: 1926 loss: 4.32223175e-07
Iter: 1927 loss: 4.32176876e-07
Iter: 1928 loss: 4.32760828e-07
Iter: 1929 loss: 4.3215411e-07
Iter: 1930 loss: 4.32109488e-07
Iter: 1931 loss: 4.32132026e-07
Iter: 1932 loss: 4.32067793e-07
Iter: 1933 loss: 4.32001229e-07
Iter: 1934 loss: 4.31945864e-07
Iter: 1935 loss: 4.3192648e-07
Iter: 1936 loss: 4.31834906e-07
Iter: 1937 loss: 4.32790273e-07
Iter: 1938 loss: 4.31836099e-07
Iter: 1939 loss: 4.3176567e-07
Iter: 1940 loss: 4.32054492e-07
Iter: 1941 loss: 4.31752028e-07
Iter: 1942 loss: 4.31684384e-07
Iter: 1943 loss: 4.3157317e-07
Iter: 1944 loss: 4.31593037e-07
Iter: 1945 loss: 4.31500496e-07
Iter: 1946 loss: 4.31487337e-07
Iter: 1947 loss: 4.31435524e-07
Iter: 1948 loss: 4.31381523e-07
Iter: 1949 loss: 4.31373167e-07
Iter: 1950 loss: 4.3130197e-07
Iter: 1951 loss: 4.3164016e-07
Iter: 1952 loss: 4.31284548e-07
Iter: 1953 loss: 4.3122111e-07
Iter: 1954 loss: 4.31672561e-07
Iter: 1955 loss: 4.31194792e-07
Iter: 1956 loss: 4.31137664e-07
Iter: 1957 loss: 4.31096851e-07
Iter: 1958 loss: 4.31074227e-07
Iter: 1959 loss: 4.30978048e-07
Iter: 1960 loss: 4.31529202e-07
Iter: 1961 loss: 4.30974922e-07
Iter: 1962 loss: 4.30884654e-07
Iter: 1963 loss: 4.31099522e-07
Iter: 1964 loss: 4.30843e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.4
+ date
Mon Oct 26 16:33:08 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.4/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.4_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -1 --phi 0.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.4_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83a80c0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83a814ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83a814ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83a8081bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83a806c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83900e40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83900c6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f839005d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83900547b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8390054c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8344649598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f834464cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8344637400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8344637620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83445aab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83445aabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8344571378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f834451dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8344571510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83444e4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83444de730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83444e5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83444ab950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83444deb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8344497488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8344497ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83443f19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f834439d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f834439d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8344360620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83443798c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f834434f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f834434f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f834434a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83443019d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83442a8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.11914152e-06
Iter: 2 loss: 3.08707558e-06
Iter: 3 loss: 3.04417904e-06
Iter: 4 loss: 2.63433662e-06
Iter: 5 loss: 3.09770235e-06
Iter: 6 loss: 2.41337966e-06
Iter: 7 loss: 2.24113273e-06
Iter: 8 loss: 2.12642817e-06
Iter: 9 loss: 2.0618636e-06
Iter: 10 loss: 2.03877e-06
Iter: 11 loss: 1.95323946e-06
Iter: 12 loss: 1.89878926e-06
Iter: 13 loss: 1.87216983e-06
Iter: 14 loss: 1.84606051e-06
Iter: 15 loss: 1.80377219e-06
Iter: 16 loss: 2.18792775e-06
Iter: 17 loss: 1.8018336e-06
Iter: 18 loss: 1.75415607e-06
Iter: 19 loss: 1.74764e-06
Iter: 20 loss: 1.7140336e-06
Iter: 21 loss: 1.66692485e-06
Iter: 22 loss: 1.64558253e-06
Iter: 23 loss: 1.62200604e-06
Iter: 24 loss: 1.55164594e-06
Iter: 25 loss: 1.85656052e-06
Iter: 26 loss: 1.53725114e-06
Iter: 27 loss: 1.45601234e-06
Iter: 28 loss: 2.03946638e-06
Iter: 29 loss: 1.44895262e-06
Iter: 30 loss: 1.42596832e-06
Iter: 31 loss: 1.39341353e-06
Iter: 32 loss: 1.39222539e-06
Iter: 33 loss: 1.35797472e-06
Iter: 34 loss: 1.48147126e-06
Iter: 35 loss: 1.34931406e-06
Iter: 36 loss: 1.34197433e-06
Iter: 37 loss: 1.33917706e-06
Iter: 38 loss: 1.32691139e-06
Iter: 39 loss: 1.3136505e-06
Iter: 40 loss: 1.31158515e-06
Iter: 41 loss: 1.291388e-06
Iter: 42 loss: 1.28550255e-06
Iter: 43 loss: 1.27331077e-06
Iter: 44 loss: 1.26521059e-06
Iter: 45 loss: 1.26156124e-06
Iter: 46 loss: 1.24856274e-06
Iter: 47 loss: 1.22041433e-06
Iter: 48 loss: 1.65261417e-06
Iter: 49 loss: 1.21933294e-06
Iter: 50 loss: 1.20870982e-06
Iter: 51 loss: 1.20704431e-06
Iter: 52 loss: 1.19660399e-06
Iter: 53 loss: 1.21347045e-06
Iter: 54 loss: 1.19183289e-06
Iter: 55 loss: 1.18142123e-06
Iter: 56 loss: 1.1587141e-06
Iter: 57 loss: 1.50315577e-06
Iter: 58 loss: 1.15782359e-06
Iter: 59 loss: 1.14135514e-06
Iter: 60 loss: 1.14113391e-06
Iter: 61 loss: 1.1288605e-06
Iter: 62 loss: 1.23854318e-06
Iter: 63 loss: 1.12827502e-06
Iter: 64 loss: 1.11937175e-06
Iter: 65 loss: 1.09234043e-06
Iter: 66 loss: 1.16311048e-06
Iter: 67 loss: 1.07752203e-06
Iter: 68 loss: 1.0531237e-06
Iter: 69 loss: 1.0527483e-06
Iter: 70 loss: 1.04469621e-06
Iter: 71 loss: 1.04412334e-06
Iter: 72 loss: 1.03515e-06
Iter: 73 loss: 1.02698698e-06
Iter: 74 loss: 1.02480067e-06
Iter: 75 loss: 1.01597288e-06
Iter: 76 loss: 1.01559795e-06
Iter: 77 loss: 1.0088155e-06
Iter: 78 loss: 9.9980366e-07
Iter: 79 loss: 9.99459189e-07
Iter: 80 loss: 9.90613671e-07
Iter: 81 loss: 9.71193231e-07
Iter: 82 loss: 1.25675854e-06
Iter: 83 loss: 9.7033012e-07
Iter: 84 loss: 9.6127178e-07
Iter: 85 loss: 9.60134e-07
Iter: 86 loss: 9.48897821e-07
Iter: 87 loss: 9.26589223e-07
Iter: 88 loss: 1.35694063e-06
Iter: 89 loss: 9.26375662e-07
Iter: 90 loss: 9.0026424e-07
Iter: 91 loss: 9.46796831e-07
Iter: 92 loss: 8.88848035e-07
Iter: 93 loss: 8.84853421e-07
Iter: 94 loss: 8.81855215e-07
Iter: 95 loss: 8.75664739e-07
Iter: 96 loss: 8.75169917e-07
Iter: 97 loss: 8.70596523e-07
Iter: 98 loss: 8.62897537e-07
Iter: 99 loss: 8.61349463e-07
Iter: 100 loss: 8.56235602e-07
Iter: 101 loss: 8.48673551e-07
Iter: 102 loss: 8.56646409e-07
Iter: 103 loss: 8.44454405e-07
Iter: 104 loss: 8.36822e-07
Iter: 105 loss: 8.36613935e-07
Iter: 106 loss: 8.29877763e-07
Iter: 107 loss: 8.15782585e-07
Iter: 108 loss: 1.05196375e-06
Iter: 109 loss: 8.15391729e-07
Iter: 110 loss: 8.02716727e-07
Iter: 111 loss: 8.23658752e-07
Iter: 112 loss: 7.9693956e-07
Iter: 113 loss: 7.84823101e-07
Iter: 114 loss: 7.84492e-07
Iter: 115 loss: 7.79501477e-07
Iter: 116 loss: 7.77050104e-07
Iter: 117 loss: 7.7466666e-07
Iter: 118 loss: 7.71769066e-07
Iter: 119 loss: 7.71474447e-07
Iter: 120 loss: 7.68705263e-07
Iter: 121 loss: 7.61736487e-07
Iter: 122 loss: 8.23683195e-07
Iter: 123 loss: 7.60667376e-07
Iter: 124 loss: 7.5496564e-07
Iter: 125 loss: 7.95465724e-07
Iter: 126 loss: 7.54472183e-07
Iter: 127 loss: 7.5078492e-07
Iter: 128 loss: 8.09063863e-07
Iter: 129 loss: 7.50790207e-07
Iter: 130 loss: 7.47006197e-07
Iter: 131 loss: 7.3803642e-07
Iter: 132 loss: 8.41282144e-07
Iter: 133 loss: 7.37238565e-07
Iter: 134 loss: 7.28209272e-07
Iter: 135 loss: 7.77522928e-07
Iter: 136 loss: 7.26876e-07
Iter: 137 loss: 7.20351863e-07
Iter: 138 loss: 7.36593734e-07
Iter: 139 loss: 7.18023216e-07
Iter: 140 loss: 7.10916083e-07
Iter: 141 loss: 8.07827405e-07
Iter: 142 loss: 7.10882318e-07
Iter: 143 loss: 7.08511152e-07
Iter: 144 loss: 7.04719e-07
Iter: 145 loss: 7.04699346e-07
Iter: 146 loss: 6.99902785e-07
Iter: 147 loss: 7.13628424e-07
Iter: 148 loss: 6.98354256e-07
Iter: 149 loss: 6.91032312e-07
Iter: 150 loss: 7.1535e-07
Iter: 151 loss: 6.89011415e-07
Iter: 152 loss: 6.85866e-07
Iter: 153 loss: 6.88968214e-07
Iter: 154 loss: 6.84136126e-07
Iter: 155 loss: 6.7997928e-07
Iter: 156 loss: 7.12592737e-07
Iter: 157 loss: 6.79660445e-07
Iter: 158 loss: 6.76824243e-07
Iter: 159 loss: 6.70254394e-07
Iter: 160 loss: 7.53406312e-07
Iter: 161 loss: 6.69786289e-07
Iter: 162 loss: 6.68225e-07
Iter: 163 loss: 6.6749044e-07
Iter: 164 loss: 6.65135e-07
Iter: 165 loss: 6.68696714e-07
Iter: 166 loss: 6.64e-07
Iter: 167 loss: 6.61370791e-07
Iter: 168 loss: 6.56934617e-07
Iter: 169 loss: 6.56927057e-07
Iter: 170 loss: 6.52072913e-07
Iter: 171 loss: 6.66627898e-07
Iter: 172 loss: 6.50623804e-07
Iter: 173 loss: 6.46043929e-07
Iter: 174 loss: 7.14144903e-07
Iter: 175 loss: 6.46045521e-07
Iter: 176 loss: 6.41596444e-07
Iter: 177 loss: 6.39967652e-07
Iter: 178 loss: 6.37528046e-07
Iter: 179 loss: 6.34542687e-07
Iter: 180 loss: 6.3823893e-07
Iter: 181 loss: 6.32954823e-07
Iter: 182 loss: 6.313021e-07
Iter: 183 loss: 6.30966042e-07
Iter: 184 loss: 6.29505735e-07
Iter: 185 loss: 6.27645e-07
Iter: 186 loss: 6.27518602e-07
Iter: 187 loss: 6.26531687e-07
Iter: 188 loss: 6.26507926e-07
Iter: 189 loss: 6.25334e-07
Iter: 190 loss: 6.22430377e-07
Iter: 191 loss: 6.52643905e-07
Iter: 192 loss: 6.2211484e-07
Iter: 193 loss: 6.18682861e-07
Iter: 194 loss: 6.22906441e-07
Iter: 195 loss: 6.16930322e-07
Iter: 196 loss: 6.13939619e-07
Iter: 197 loss: 6.13933253e-07
Iter: 198 loss: 6.10558516e-07
Iter: 199 loss: 6.04390493e-07
Iter: 200 loss: 7.50169079e-07
Iter: 201 loss: 6.04413856e-07
Iter: 202 loss: 6.00408839e-07
Iter: 203 loss: 6.21598e-07
Iter: 204 loss: 5.99791406e-07
Iter: 205 loss: 5.96898e-07
Iter: 206 loss: 6.11648488e-07
Iter: 207 loss: 5.96423547e-07
Iter: 208 loss: 5.93549657e-07
Iter: 209 loss: 6.19509422e-07
Iter: 210 loss: 5.93415962e-07
Iter: 211 loss: 5.92129595e-07
Iter: 212 loss: 5.91037349e-07
Iter: 213 loss: 5.9067537e-07
Iter: 214 loss: 5.88970238e-07
Iter: 215 loss: 5.92553306e-07
Iter: 216 loss: 5.88291641e-07
Iter: 217 loss: 5.85594876e-07
Iter: 218 loss: 5.9689944e-07
Iter: 219 loss: 5.85009388e-07
Iter: 220 loss: 5.83558574e-07
Iter: 221 loss: 5.82557561e-07
Iter: 222 loss: 5.8201374e-07
Iter: 223 loss: 5.79862444e-07
Iter: 224 loss: 6.09555855e-07
Iter: 225 loss: 5.79859659e-07
Iter: 226 loss: 5.78242464e-07
Iter: 227 loss: 5.75323497e-07
Iter: 228 loss: 6.45129944e-07
Iter: 229 loss: 5.75314971e-07
Iter: 230 loss: 5.73521106e-07
Iter: 231 loss: 5.78264576e-07
Iter: 232 loss: 5.72917884e-07
Iter: 233 loss: 5.70182124e-07
Iter: 234 loss: 5.83485416e-07
Iter: 235 loss: 5.69709243e-07
Iter: 236 loss: 5.67538677e-07
Iter: 237 loss: 5.64653533e-07
Iter: 238 loss: 5.64472771e-07
Iter: 239 loss: 5.62068465e-07
Iter: 240 loss: 5.68423275e-07
Iter: 241 loss: 5.61247475e-07
Iter: 242 loss: 5.59503178e-07
Iter: 243 loss: 5.59402906e-07
Iter: 244 loss: 5.57968463e-07
Iter: 245 loss: 5.58899671e-07
Iter: 246 loss: 5.57032422e-07
Iter: 247 loss: 5.563071e-07
Iter: 248 loss: 5.56255088e-07
Iter: 249 loss: 5.55725592e-07
Iter: 250 loss: 5.54924e-07
Iter: 251 loss: 5.54922281e-07
Iter: 252 loss: 5.54156e-07
Iter: 253 loss: 5.52892402e-07
Iter: 254 loss: 5.52884842e-07
Iter: 255 loss: 5.51736434e-07
Iter: 256 loss: 5.59233911e-07
Iter: 257 loss: 5.51617063e-07
Iter: 258 loss: 5.50043183e-07
Iter: 259 loss: 5.49656079e-07
Iter: 260 loss: 5.48683886e-07
Iter: 261 loss: 5.46733247e-07
Iter: 262 loss: 5.45535215e-07
Iter: 263 loss: 5.4472622e-07
Iter: 264 loss: 5.42992e-07
Iter: 265 loss: 5.68339658e-07
Iter: 266 loss: 5.42995281e-07
Iter: 267 loss: 5.4087559e-07
Iter: 268 loss: 5.42417524e-07
Iter: 269 loss: 5.39572397e-07
Iter: 270 loss: 5.3865233e-07
Iter: 271 loss: 5.39436087e-07
Iter: 272 loss: 5.38123913e-07
Iter: 273 loss: 5.36832e-07
Iter: 274 loss: 5.35610809e-07
Iter: 275 loss: 5.3532608e-07
Iter: 276 loss: 5.3412839e-07
Iter: 277 loss: 5.33752768e-07
Iter: 278 loss: 5.33092475e-07
Iter: 279 loss: 5.31553269e-07
Iter: 280 loss: 5.51455742e-07
Iter: 281 loss: 5.31447881e-07
Iter: 282 loss: 5.29400438e-07
Iter: 283 loss: 5.32451281e-07
Iter: 284 loss: 5.28429155e-07
Iter: 285 loss: 5.27043369e-07
Iter: 286 loss: 5.26838392e-07
Iter: 287 loss: 5.26076519e-07
Iter: 288 loss: 5.24865129e-07
Iter: 289 loss: 5.24833524e-07
Iter: 290 loss: 5.23697167e-07
Iter: 291 loss: 5.36746938e-07
Iter: 292 loss: 5.23673123e-07
Iter: 293 loss: 5.22472646e-07
Iter: 294 loss: 5.22773576e-07
Iter: 295 loss: 5.21618517e-07
Iter: 296 loss: 5.20778485e-07
Iter: 297 loss: 5.1878942e-07
Iter: 298 loss: 5.42715782e-07
Iter: 299 loss: 5.1864464e-07
Iter: 300 loss: 5.17209685e-07
Iter: 301 loss: 5.16987711e-07
Iter: 302 loss: 5.15464365e-07
Iter: 303 loss: 5.16689965e-07
Iter: 304 loss: 5.14581302e-07
Iter: 305 loss: 5.13763439e-07
Iter: 306 loss: 5.12770839e-07
Iter: 307 loss: 5.12671136e-07
Iter: 308 loss: 5.11329745e-07
Iter: 309 loss: 5.20488925e-07
Iter: 310 loss: 5.1119406e-07
Iter: 311 loss: 5.11092765e-07
Iter: 312 loss: 5.10748464e-07
Iter: 313 loss: 5.10428549e-07
Iter: 314 loss: 5.09381039e-07
Iter: 315 loss: 5.11273811e-07
Iter: 316 loss: 5.08703806e-07
Iter: 317 loss: 5.07339e-07
Iter: 318 loss: 5.17600597e-07
Iter: 319 loss: 5.07228378e-07
Iter: 320 loss: 5.0604865e-07
Iter: 321 loss: 5.19834941e-07
Iter: 322 loss: 5.06024264e-07
Iter: 323 loss: 5.0499807e-07
Iter: 324 loss: 5.02901798e-07
Iter: 325 loss: 5.40893552e-07
Iter: 326 loss: 5.02860075e-07
Iter: 327 loss: 5.01887882e-07
Iter: 328 loss: 5.0179915e-07
Iter: 329 loss: 5.00985152e-07
Iter: 330 loss: 5.04437878e-07
Iter: 331 loss: 5.0080331e-07
Iter: 332 loss: 5.00269095e-07
Iter: 333 loss: 4.99094e-07
Iter: 334 loss: 5.19131333e-07
Iter: 335 loss: 4.9906879e-07
Iter: 336 loss: 4.98296856e-07
Iter: 337 loss: 5.09226538e-07
Iter: 338 loss: 4.98301e-07
Iter: 339 loss: 4.97844439e-07
Iter: 340 loss: 5.0499068e-07
Iter: 341 loss: 4.97844894e-07
Iter: 342 loss: 4.97408223e-07
Iter: 343 loss: 4.96204393e-07
Iter: 344 loss: 5.03451815e-07
Iter: 345 loss: 4.95890902e-07
Iter: 346 loss: 4.9453007e-07
Iter: 347 loss: 4.96286702e-07
Iter: 348 loss: 4.93860739e-07
Iter: 349 loss: 4.92573804e-07
Iter: 350 loss: 4.92538049e-07
Iter: 351 loss: 4.91191884e-07
Iter: 352 loss: 4.90183766e-07
Iter: 353 loss: 4.89747322e-07
Iter: 354 loss: 4.89060199e-07
Iter: 355 loss: 4.89823037e-07
Iter: 356 loss: 4.88717205e-07
Iter: 357 loss: 4.88318335e-07
Iter: 358 loss: 4.88237902e-07
Iter: 359 loss: 4.87841646e-07
Iter: 360 loss: 4.87535e-07
Iter: 361 loss: 4.87409466e-07
Iter: 362 loss: 4.87051e-07
Iter: 363 loss: 4.87048737e-07
Iter: 364 loss: 4.86740475e-07
Iter: 365 loss: 4.85922669e-07
Iter: 366 loss: 4.88131491e-07
Iter: 367 loss: 4.85648684e-07
Iter: 368 loss: 4.84936436e-07
Iter: 369 loss: 4.84335374e-07
Iter: 370 loss: 4.8413159e-07
Iter: 371 loss: 4.82987502e-07
Iter: 372 loss: 4.8355389e-07
Iter: 373 loss: 4.8222762e-07
Iter: 374 loss: 4.81744337e-07
Iter: 375 loss: 4.8141186e-07
Iter: 376 loss: 4.80924427e-07
Iter: 377 loss: 4.80094855e-07
Iter: 378 loss: 4.80072572e-07
Iter: 379 loss: 4.79257153e-07
Iter: 380 loss: 4.79173536e-07
Iter: 381 loss: 4.78613856e-07
Iter: 382 loss: 4.78771938e-07
Iter: 383 loss: 4.78239826e-07
Iter: 384 loss: 4.7793543e-07
Iter: 385 loss: 4.78002448e-07
Iter: 386 loss: 4.77725962e-07
Iter: 387 loss: 4.77363415e-07
Iter: 388 loss: 4.76406797e-07
Iter: 389 loss: 4.81210122e-07
Iter: 390 loss: 4.76067783e-07
Iter: 391 loss: 4.76236721e-07
Iter: 392 loss: 4.75614058e-07
Iter: 393 loss: 4.75107583e-07
Iter: 394 loss: 4.75212858e-07
Iter: 395 loss: 4.74739522e-07
Iter: 396 loss: 4.7414494e-07
Iter: 397 loss: 4.73786542e-07
Iter: 398 loss: 4.73570736e-07
Iter: 399 loss: 4.72939433e-07
Iter: 400 loss: 4.72900808e-07
Iter: 401 loss: 4.72569525e-07
Iter: 402 loss: 4.71762604e-07
Iter: 403 loss: 4.79735036e-07
Iter: 404 loss: 4.71677737e-07
Iter: 405 loss: 4.70738627e-07
Iter: 406 loss: 4.80613608e-07
Iter: 407 loss: 4.70733028e-07
Iter: 408 loss: 4.69966523e-07
Iter: 409 loss: 4.74322e-07
Iter: 410 loss: 4.69828677e-07
Iter: 411 loss: 4.69389192e-07
Iter: 412 loss: 4.68476429e-07
Iter: 413 loss: 4.85282783e-07
Iter: 414 loss: 4.68455198e-07
Iter: 415 loss: 4.67350532e-07
Iter: 416 loss: 4.7186083e-07
Iter: 417 loss: 4.67092747e-07
Iter: 418 loss: 4.67240966e-07
Iter: 419 loss: 4.66800344e-07
Iter: 420 loss: 4.66574193e-07
Iter: 421 loss: 4.66075306e-07
Iter: 422 loss: 4.72176964e-07
Iter: 423 loss: 4.66030258e-07
Iter: 424 loss: 4.65479218e-07
Iter: 425 loss: 4.65714777e-07
Iter: 426 loss: 4.6509075e-07
Iter: 427 loss: 4.64841264e-07
Iter: 428 loss: 4.64703106e-07
Iter: 429 loss: 4.64285932e-07
Iter: 430 loss: 4.63133404e-07
Iter: 431 loss: 4.68591935e-07
Iter: 432 loss: 4.62732402e-07
Iter: 433 loss: 4.6206415e-07
Iter: 434 loss: 4.6194026e-07
Iter: 435 loss: 4.61227302e-07
Iter: 436 loss: 4.61888732e-07
Iter: 437 loss: 4.60823514e-07
Iter: 438 loss: 4.60150346e-07
Iter: 439 loss: 4.59100477e-07
Iter: 440 loss: 4.59091069e-07
Iter: 441 loss: 4.59249321e-07
Iter: 442 loss: 4.58583912e-07
Iter: 443 loss: 4.5830771e-07
Iter: 444 loss: 4.58237821e-07
Iter: 445 loss: 4.58083832e-07
Iter: 446 loss: 4.57769318e-07
Iter: 447 loss: 4.57162827e-07
Iter: 448 loss: 4.70013219e-07
Iter: 449 loss: 4.57163196e-07
Iter: 450 loss: 4.56694494e-07
Iter: 451 loss: 4.56671955e-07
Iter: 452 loss: 4.56318958e-07
Iter: 453 loss: 4.56827109e-07
Iter: 454 loss: 4.56108637e-07
Iter: 455 loss: 4.55789404e-07
Iter: 456 loss: 4.54946075e-07
Iter: 457 loss: 4.63641896e-07
Iter: 458 loss: 4.54847196e-07
Iter: 459 loss: 4.54325857e-07
Iter: 460 loss: 4.54235106e-07
Iter: 461 loss: 4.53713255e-07
Iter: 462 loss: 4.55810266e-07
Iter: 463 loss: 4.5357686e-07
Iter: 464 loss: 4.53272e-07
Iter: 465 loss: 4.52781279e-07
Iter: 466 loss: 4.52788441e-07
Iter: 467 loss: 4.52335371e-07
Iter: 468 loss: 4.523244e-07
Iter: 469 loss: 4.52055872e-07
Iter: 470 loss: 4.5162713e-07
Iter: 471 loss: 4.61915249e-07
Iter: 472 loss: 4.51616756e-07
Iter: 473 loss: 4.51057844e-07
Iter: 474 loss: 4.52093701e-07
Iter: 475 loss: 4.5083965e-07
Iter: 476 loss: 4.49980661e-07
Iter: 477 loss: 4.54655975e-07
Iter: 478 loss: 4.49867457e-07
Iter: 479 loss: 4.49487516e-07
Iter: 480 loss: 4.4961223e-07
Iter: 481 loss: 4.49210916e-07
Iter: 482 loss: 4.48740195e-07
Iter: 483 loss: 4.50407072e-07
Iter: 484 loss: 4.48605221e-07
Iter: 485 loss: 4.48039884e-07
Iter: 486 loss: 4.5151063e-07
Iter: 487 loss: 4.479881e-07
Iter: 488 loss: 4.47683021e-07
Iter: 489 loss: 4.47533381e-07
Iter: 490 loss: 4.47387947e-07
Iter: 491 loss: 4.46977424e-07
Iter: 492 loss: 4.4674249e-07
Iter: 493 loss: 4.46574717e-07
Iter: 494 loss: 4.46093566e-07
Iter: 495 loss: 4.46073187e-07
Iter: 496 loss: 4.45732837e-07
Iter: 497 loss: 4.44988984e-07
Iter: 498 loss: 4.54523502e-07
Iter: 499 loss: 4.44946238e-07
Iter: 500 loss: 4.44271421e-07
Iter: 501 loss: 4.44269205e-07
Iter: 502 loss: 4.43573526e-07
Iter: 503 loss: 4.43130773e-07
Iter: 504 loss: 4.42857555e-07
Iter: 505 loss: 4.42408663e-07
Iter: 506 loss: 4.43567473e-07
Iter: 507 loss: 4.42240037e-07
Iter: 508 loss: 4.41901364e-07
Iter: 509 loss: 4.41888261e-07
Iter: 510 loss: 4.41610581e-07
Iter: 511 loss: 4.41173171e-07
Iter: 512 loss: 4.41176553e-07
Iter: 513 loss: 4.40830945e-07
Iter: 514 loss: 4.42345311e-07
Iter: 515 loss: 4.40770918e-07
Iter: 516 loss: 4.40467346e-07
Iter: 517 loss: 4.43794164e-07
Iter: 518 loss: 4.40441852e-07
Iter: 519 loss: 4.40130151e-07
Iter: 520 loss: 4.39401077e-07
Iter: 521 loss: 4.47600257e-07
Iter: 522 loss: 4.39328602e-07
Iter: 523 loss: 4.38608481e-07
Iter: 524 loss: 4.41970656e-07
Iter: 525 loss: 4.38483369e-07
Iter: 526 loss: 4.37933807e-07
Iter: 527 loss: 4.42631062e-07
Iter: 528 loss: 4.37915304e-07
Iter: 529 loss: 4.37308586e-07
Iter: 530 loss: 4.38665552e-07
Iter: 531 loss: 4.37064728e-07
Iter: 532 loss: 4.36813764e-07
Iter: 533 loss: 4.3692495e-07
Iter: 534 loss: 4.36637663e-07
Iter: 535 loss: 4.36231119e-07
Iter: 536 loss: 4.387187e-07
Iter: 537 loss: 4.36186951e-07
Iter: 538 loss: 4.35918309e-07
Iter: 539 loss: 4.35486641e-07
Iter: 540 loss: 4.35473083e-07
Iter: 541 loss: 4.35187644e-07
Iter: 542 loss: 4.39117713e-07
Iter: 543 loss: 4.35187985e-07
Iter: 544 loss: 4.3480776e-07
Iter: 545 loss: 4.34627509e-07
Iter: 546 loss: 4.34451522e-07
Iter: 547 loss: 4.33961873e-07
Iter: 548 loss: 4.34206243e-07
Iter: 549 loss: 4.33618169e-07
Iter: 550 loss: 4.33303569e-07
Iter: 551 loss: 4.33307264e-07
Iter: 552 loss: 4.32974161e-07
Iter: 553 loss: 4.33216087e-07
Iter: 554 loss: 4.32751449e-07
Iter: 555 loss: 4.3241306e-07
Iter: 556 loss: 4.3237776e-07
Iter: 557 loss: 4.32148909e-07
Iter: 558 loss: 4.31747338e-07
Iter: 559 loss: 4.31859e-07
Iter: 560 loss: 4.31463889e-07
Iter: 561 loss: 4.30978389e-07
Iter: 562 loss: 4.30970374e-07
Iter: 563 loss: 4.30625192e-07
Iter: 564 loss: 4.30129148e-07
Iter: 565 loss: 4.3010462e-07
Iter: 566 loss: 4.29852946e-07
Iter: 567 loss: 4.29843311e-07
Iter: 568 loss: 4.29537863e-07
Iter: 569 loss: 4.28942712e-07
Iter: 570 loss: 4.39971046e-07
Iter: 571 loss: 4.28914916e-07
Iter: 572 loss: 4.28554955e-07
Iter: 573 loss: 4.31584738e-07
Iter: 574 loss: 4.28528125e-07
Iter: 575 loss: 4.28405087e-07
Iter: 576 loss: 4.28389114e-07
Iter: 577 loss: 4.28273069e-07
Iter: 578 loss: 4.27994962e-07
Iter: 579 loss: 4.31314106e-07
Iter: 580 loss: 4.27968871e-07
Iter: 581 loss: 4.27666123e-07
Iter: 582 loss: 4.27924647e-07
Iter: 583 loss: 4.27514635e-07
Iter: 584 loss: 4.27159023e-07
Iter: 585 loss: 4.31778147e-07
Iter: 586 loss: 4.27152543e-07
Iter: 587 loss: 4.26749637e-07
Iter: 588 loss: 4.26111285e-07
Iter: 589 loss: 4.26114354e-07
Iter: 590 loss: 4.25609443e-07
Iter: 591 loss: 4.27202309e-07
Iter: 592 loss: 4.25459547e-07
Iter: 593 loss: 4.25042771e-07
Iter: 594 loss: 4.28881947e-07
Iter: 595 loss: 4.25039332e-07
Iter: 596 loss: 4.24584243e-07
Iter: 597 loss: 4.25900026e-07
Iter: 598 loss: 4.24449837e-07
Iter: 599 loss: 4.24315715e-07
Iter: 600 loss: 4.24592315e-07
Iter: 601 loss: 4.24246252e-07
Iter: 602 loss: 4.24080497e-07
Iter: 603 loss: 4.25279922e-07
Iter: 604 loss: 4.24048039e-07
Iter: 605 loss: 4.23889958e-07
Iter: 606 loss: 4.23549636e-07
Iter: 607 loss: 4.26923265e-07
Iter: 608 loss: 4.23497795e-07
Iter: 609 loss: 4.23201982e-07
Iter: 610 loss: 4.26623274e-07
Iter: 611 loss: 4.2319769e-07
Iter: 612 loss: 4.22821415e-07
Iter: 613 loss: 4.23436461e-07
Iter: 614 loss: 4.22654779e-07
Iter: 615 loss: 4.22306528e-07
Iter: 616 loss: 4.21837512e-07
Iter: 617 loss: 4.21809318e-07
Iter: 618 loss: 4.21438415e-07
Iter: 619 loss: 4.21437392e-07
Iter: 620 loss: 4.2108104e-07
Iter: 621 loss: 4.21986954e-07
Iter: 622 loss: 4.20924778e-07
Iter: 623 loss: 4.20668243e-07
Iter: 624 loss: 4.20755697e-07
Iter: 625 loss: 4.20491062e-07
Iter: 626 loss: 4.20284465e-07
Iter: 627 loss: 4.20756891e-07
Iter: 628 loss: 4.20196159e-07
Iter: 629 loss: 4.19896196e-07
Iter: 630 loss: 4.22277594e-07
Iter: 631 loss: 4.19860385e-07
Iter: 632 loss: 4.19658022e-07
Iter: 633 loss: 4.19486895e-07
Iter: 634 loss: 4.19395519e-07
Iter: 635 loss: 4.19246646e-07
Iter: 636 loss: 4.19265433e-07
Iter: 637 loss: 4.19077111e-07
Iter: 638 loss: 4.18686454e-07
Iter: 639 loss: 4.23877054e-07
Iter: 640 loss: 4.18663717e-07
Iter: 641 loss: 4.18301084e-07
Iter: 642 loss: 4.19647023e-07
Iter: 643 loss: 4.18204365e-07
Iter: 644 loss: 4.18066804e-07
Iter: 645 loss: 4.18045602e-07
Iter: 646 loss: 4.1785205e-07
Iter: 647 loss: 4.1745119e-07
Iter: 648 loss: 4.23067092e-07
Iter: 649 loss: 4.17433057e-07
Iter: 650 loss: 4.17142644e-07
Iter: 651 loss: 4.19320429e-07
Iter: 652 loss: 4.17132838e-07
Iter: 653 loss: 4.16916322e-07
Iter: 654 loss: 4.19430251e-07
Iter: 655 loss: 4.16919079e-07
Iter: 656 loss: 4.16727147e-07
Iter: 657 loss: 4.16235025e-07
Iter: 658 loss: 4.20945753e-07
Iter: 659 loss: 4.16174544e-07
Iter: 660 loss: 4.15903969e-07
Iter: 661 loss: 4.20131045e-07
Iter: 662 loss: 4.1592466e-07
Iter: 663 loss: 4.15766038e-07
Iter: 664 loss: 4.17755018e-07
Iter: 665 loss: 4.1574765e-07
Iter: 666 loss: 4.15611026e-07
Iter: 667 loss: 4.15381578e-07
Iter: 668 loss: 4.15372767e-07
Iter: 669 loss: 4.1520218e-07
Iter: 670 loss: 4.16384523e-07
Iter: 671 loss: 4.15185e-07
Iter: 672 loss: 4.14993281e-07
Iter: 673 loss: 4.15648969e-07
Iter: 674 loss: 4.14941525e-07
Iter: 675 loss: 4.14791032e-07
Iter: 676 loss: 4.1438318e-07
Iter: 677 loss: 4.18649677e-07
Iter: 678 loss: 4.14369595e-07
Iter: 679 loss: 4.14099418e-07
Iter: 680 loss: 4.14103852e-07
Iter: 681 loss: 4.13768959e-07
Iter: 682 loss: 4.14196592e-07
Iter: 683 loss: 4.13610053e-07
Iter: 684 loss: 4.13282e-07
Iter: 685 loss: 4.12995036e-07
Iter: 686 loss: 4.12939585e-07
Iter: 687 loss: 4.12924379e-07
Iter: 688 loss: 4.12808163e-07
Iter: 689 loss: 4.12659773e-07
Iter: 690 loss: 4.12445615e-07
Iter: 691 loss: 4.12431859e-07
Iter: 692 loss: 4.12205765e-07
Iter: 693 loss: 4.12310442e-07
Iter: 694 loss: 4.12048678e-07
Iter: 695 loss: 4.11897815e-07
Iter: 696 loss: 4.11907706e-07
Iter: 697 loss: 4.11756332e-07
Iter: 698 loss: 4.1172629e-07
Iter: 699 loss: 4.11614792e-07
Iter: 700 loss: 4.11386367e-07
Iter: 701 loss: 4.11139268e-07
Iter: 702 loss: 4.11094732e-07
Iter: 703 loss: 4.10992413e-07
Iter: 704 loss: 4.10900384e-07
Iter: 705 loss: 4.10799316e-07
Iter: 706 loss: 4.10543834e-07
Iter: 707 loss: 4.13796471e-07
Iter: 708 loss: 4.10523796e-07
Iter: 709 loss: 4.10206837e-07
Iter: 710 loss: 4.10794826e-07
Iter: 711 loss: 4.10070555e-07
Iter: 712 loss: 4.10033579e-07
Iter: 713 loss: 4.09926685e-07
Iter: 714 loss: 4.09844972e-07
Iter: 715 loss: 4.09563171e-07
Iter: 716 loss: 4.10429664e-07
Iter: 717 loss: 4.09397757e-07
Iter: 718 loss: 4.09063318e-07
Iter: 719 loss: 4.14125054e-07
Iter: 720 loss: 4.09065024e-07
Iter: 721 loss: 4.08791266e-07
Iter: 722 loss: 4.10939379e-07
Iter: 723 loss: 4.0877768e-07
Iter: 724 loss: 4.08602489e-07
Iter: 725 loss: 4.08246365e-07
Iter: 726 loss: 4.13653396e-07
Iter: 727 loss: 4.08212117e-07
Iter: 728 loss: 4.07905958e-07
Iter: 729 loss: 4.07913944e-07
Iter: 730 loss: 4.07713316e-07
Iter: 731 loss: 4.09560045e-07
Iter: 732 loss: 4.07719142e-07
Iter: 733 loss: 4.07622622e-07
Iter: 734 loss: 4.07477e-07
Iter: 735 loss: 4.07456071e-07
Iter: 736 loss: 4.07314587e-07
Iter: 737 loss: 4.09501354e-07
Iter: 738 loss: 4.0732138e-07
Iter: 739 loss: 4.07167875e-07
Iter: 740 loss: 4.07092358e-07
Iter: 741 loss: 4.0703253e-07
Iter: 742 loss: 4.06856259e-07
Iter: 743 loss: 4.06490557e-07
Iter: 744 loss: 4.12896156e-07
Iter: 745 loss: 4.06493541e-07
Iter: 746 loss: 4.06197216e-07
Iter: 747 loss: 4.0617897e-07
Iter: 748 loss: 4.05880712e-07
Iter: 749 loss: 4.0593369e-07
Iter: 750 loss: 4.05668658e-07
Iter: 751 loss: 4.05416216e-07
Iter: 752 loss: 4.05135182e-07
Iter: 753 loss: 4.05085757e-07
Iter: 754 loss: 4.04972354e-07
Iter: 755 loss: 4.048336e-07
Iter: 756 loss: 4.0472321e-07
Iter: 757 loss: 4.04585364e-07
Iter: 758 loss: 4.04563139e-07
Iter: 759 loss: 4.04405682e-07
Iter: 760 loss: 4.04189166e-07
Iter: 761 loss: 4.0417595e-07
Iter: 762 loss: 4.04167821e-07
Iter: 763 loss: 4.04023865e-07
Iter: 764 loss: 4.03926663e-07
Iter: 765 loss: 4.03685362e-07
Iter: 766 loss: 4.05952335e-07
Iter: 767 loss: 4.03672971e-07
Iter: 768 loss: 4.03300419e-07
Iter: 769 loss: 4.05202542e-07
Iter: 770 loss: 4.03254148e-07
Iter: 771 loss: 4.02916783e-07
Iter: 772 loss: 4.03287686e-07
Iter: 773 loss: 4.02729029e-07
Iter: 774 loss: 4.02415026e-07
Iter: 775 loss: 4.03223225e-07
Iter: 776 loss: 4.02325924e-07
Iter: 777 loss: 4.02042218e-07
Iter: 778 loss: 4.01909517e-07
Iter: 779 loss: 4.01789919e-07
Iter: 780 loss: 4.01918385e-07
Iter: 781 loss: 4.01665659e-07
Iter: 782 loss: 4.0155976e-07
Iter: 783 loss: 4.01374677e-07
Iter: 784 loss: 4.05042783e-07
Iter: 785 loss: 4.01375331e-07
Iter: 786 loss: 4.01175498e-07
Iter: 787 loss: 4.01678e-07
Iter: 788 loss: 4.01093928e-07
Iter: 789 loss: 4.00827162e-07
Iter: 790 loss: 4.03124886e-07
Iter: 791 loss: 4.00817044e-07
Iter: 792 loss: 4.00696592e-07
Iter: 793 loss: 4.0038833e-07
Iter: 794 loss: 4.02638818e-07
Iter: 795 loss: 4.00318839e-07
Iter: 796 loss: 4.00099054e-07
Iter: 797 loss: 4.00077738e-07
Iter: 798 loss: 3.99875262e-07
Iter: 799 loss: 4.00899353e-07
Iter: 800 loss: 3.99860312e-07
Iter: 801 loss: 3.99756175e-07
Iter: 802 loss: 3.99565835e-07
Iter: 803 loss: 4.0311869e-07
Iter: 804 loss: 3.99548e-07
Iter: 805 loss: 3.99344515e-07
Iter: 806 loss: 4.02773196e-07
Iter: 807 loss: 3.99343378e-07
Iter: 808 loss: 3.99217328e-07
Iter: 809 loss: 3.99124104e-07
Iter: 810 loss: 3.9908906e-07
Iter: 811 loss: 3.98875329e-07
Iter: 812 loss: 3.98562122e-07
Iter: 813 loss: 3.98553368e-07
Iter: 814 loss: 3.98227456e-07
Iter: 815 loss: 3.98234761e-07
Iter: 816 loss: 3.97982546e-07
Iter: 817 loss: 3.98839916e-07
Iter: 818 loss: 3.97897907e-07
Iter: 819 loss: 3.97745396e-07
Iter: 820 loss: 3.97748067e-07
Iter: 821 loss: 3.97606158e-07
Iter: 822 loss: 3.97536922e-07
Iter: 823 loss: 3.97513531e-07
Iter: 824 loss: 3.97443387e-07
Iter: 825 loss: 3.97281894e-07
Iter: 826 loss: 4.00408112e-07
Iter: 827 loss: 3.97283145e-07
Iter: 828 loss: 3.97158203e-07
Iter: 829 loss: 3.97596125e-07
Iter: 830 loss: 3.97133363e-07
Iter: 831 loss: 3.96967664e-07
Iter: 832 loss: 3.98189172e-07
Iter: 833 loss: 3.96953e-07
Iter: 834 loss: 3.96847241e-07
Iter: 835 loss: 3.96647152e-07
Iter: 836 loss: 4.01522783e-07
Iter: 837 loss: 3.96650449e-07
Iter: 838 loss: 3.96426344e-07
Iter: 839 loss: 3.97728797e-07
Iter: 840 loss: 3.9637888e-07
Iter: 841 loss: 3.96062e-07
Iter: 842 loss: 3.96118651e-07
Iter: 843 loss: 3.95817665e-07
Iter: 844 loss: 3.95616723e-07
Iter: 845 loss: 3.96072977e-07
Iter: 846 loss: 3.95538166e-07
Iter: 847 loss: 3.95358711e-07
Iter: 848 loss: 3.96085909e-07
Iter: 849 loss: 3.95312497e-07
Iter: 850 loss: 3.95197873e-07
Iter: 851 loss: 3.95196395e-07
Iter: 852 loss: 3.95133725e-07
Iter: 853 loss: 3.95047778e-07
Iter: 854 loss: 3.95040956e-07
Iter: 855 loss: 3.94897597e-07
Iter: 856 loss: 3.9519955e-07
Iter: 857 loss: 3.94851327e-07
Iter: 858 loss: 3.94661299e-07
Iter: 859 loss: 3.954151e-07
Iter: 860 loss: 3.94603831e-07
Iter: 861 loss: 3.94529479e-07
Iter: 862 loss: 3.94302106e-07
Iter: 863 loss: 3.96387378e-07
Iter: 864 loss: 3.94277322e-07
Iter: 865 loss: 3.94130979e-07
Iter: 866 loss: 3.94091046e-07
Iter: 867 loss: 3.93941093e-07
Iter: 868 loss: 3.93917333e-07
Iter: 869 loss: 3.93813593e-07
Iter: 870 loss: 3.93688026e-07
Iter: 871 loss: 3.93372289e-07
Iter: 872 loss: 3.97587428e-07
Iter: 873 loss: 3.93353e-07
Iter: 874 loss: 3.93040892e-07
Iter: 875 loss: 3.956157e-07
Iter: 876 loss: 3.93036373e-07
Iter: 877 loss: 3.92721688e-07
Iter: 878 loss: 3.93791225e-07
Iter: 879 loss: 3.92631193e-07
Iter: 880 loss: 3.92316252e-07
Iter: 881 loss: 3.96244275e-07
Iter: 882 loss: 3.92316849e-07
Iter: 883 loss: 3.92200576e-07
Iter: 884 loss: 3.92085951e-07
Iter: 885 loss: 3.92045735e-07
Iter: 886 loss: 3.91853348e-07
Iter: 887 loss: 3.91889301e-07
Iter: 888 loss: 3.91701519e-07
Iter: 889 loss: 3.91554295e-07
Iter: 890 loss: 3.91513936e-07
Iter: 891 loss: 3.91461981e-07
Iter: 892 loss: 3.9136097e-07
Iter: 893 loss: 3.91368076e-07
Iter: 894 loss: 3.912279e-07
Iter: 895 loss: 3.92079812e-07
Iter: 896 loss: 3.91201382e-07
Iter: 897 loss: 3.91066237e-07
Iter: 898 loss: 3.91004363e-07
Iter: 899 loss: 3.90915801e-07
Iter: 900 loss: 3.90755957e-07
Iter: 901 loss: 3.90574172e-07
Iter: 902 loss: 3.9055152e-07
Iter: 903 loss: 3.90398782e-07
Iter: 904 loss: 3.90378688e-07
Iter: 905 loss: 3.9020486e-07
Iter: 906 loss: 3.90367518e-07
Iter: 907 loss: 3.90114678e-07
Iter: 908 loss: 3.89996501e-07
Iter: 909 loss: 3.89859963e-07
Iter: 910 loss: 3.8984922e-07
Iter: 911 loss: 3.89759322e-07
Iter: 912 loss: 3.89750596e-07
Iter: 913 loss: 3.89651092e-07
Iter: 914 loss: 3.89644185e-07
Iter: 915 loss: 3.89575177e-07
Iter: 916 loss: 3.89473257e-07
Iter: 917 loss: 3.89217462e-07
Iter: 918 loss: 3.91325727e-07
Iter: 919 loss: 3.89177842e-07
Iter: 920 loss: 3.88916845e-07
Iter: 921 loss: 3.88916305e-07
Iter: 922 loss: 3.88712863e-07
Iter: 923 loss: 3.88862787e-07
Iter: 924 loss: 3.88594231e-07
Iter: 925 loss: 3.88429612e-07
Iter: 926 loss: 3.88391356e-07
Iter: 927 loss: 3.88302851e-07
Iter: 928 loss: 3.881959e-07
Iter: 929 loss: 3.88166598e-07
Iter: 930 loss: 3.88068486e-07
Iter: 931 loss: 3.88028354e-07
Iter: 932 loss: 3.87993111e-07
Iter: 933 loss: 3.87880561e-07
Iter: 934 loss: 3.87800867e-07
Iter: 935 loss: 3.87789612e-07
Iter: 936 loss: 3.87705683e-07
Iter: 937 loss: 3.87693831e-07
Iter: 938 loss: 3.87609191e-07
Iter: 939 loss: 3.87536488e-07
Iter: 940 loss: 3.8751574e-07
Iter: 941 loss: 3.87391111e-07
Iter: 942 loss: 3.87093934e-07
Iter: 943 loss: 3.90953289e-07
Iter: 944 loss: 3.8709959e-07
Iter: 945 loss: 3.87124601e-07
Iter: 946 loss: 3.86949779e-07
Iter: 947 loss: 3.86848399e-07
Iter: 948 loss: 3.86634923e-07
Iter: 949 loss: 3.9040907e-07
Iter: 950 loss: 3.86636373e-07
Iter: 951 loss: 3.86437705e-07
Iter: 952 loss: 3.86700691e-07
Iter: 953 loss: 3.86299803e-07
Iter: 954 loss: 3.86243983e-07
Iter: 955 loss: 3.86189924e-07
Iter: 956 loss: 3.8608249e-07
Iter: 957 loss: 3.85819192e-07
Iter: 958 loss: 3.89661921e-07
Iter: 959 loss: 3.85825217e-07
Iter: 960 loss: 3.85542592e-07
Iter: 961 loss: 3.86390127e-07
Iter: 962 loss: 3.85453518e-07
Iter: 963 loss: 3.85386784e-07
Iter: 964 loss: 3.85371322e-07
Iter: 965 loss: 3.85252577e-07
Iter: 966 loss: 3.84990727e-07
Iter: 967 loss: 3.88215028e-07
Iter: 968 loss: 3.8498564e-07
Iter: 969 loss: 3.84725695e-07
Iter: 970 loss: 3.86659281e-07
Iter: 971 loss: 3.84704151e-07
Iter: 972 loss: 3.84627356e-07
Iter: 973 loss: 3.84624485e-07
Iter: 974 loss: 3.84544592e-07
Iter: 975 loss: 3.84396e-07
Iter: 976 loss: 3.8685053e-07
Iter: 977 loss: 3.8439623e-07
Iter: 978 loss: 3.84206942e-07
Iter: 979 loss: 3.84578186e-07
Iter: 980 loss: 3.84125087e-07
Iter: 981 loss: 3.83921332e-07
Iter: 982 loss: 3.86180488e-07
Iter: 983 loss: 3.83925368e-07
Iter: 984 loss: 3.83810686e-07
Iter: 985 loss: 3.83526299e-07
Iter: 986 loss: 3.85340059e-07
Iter: 987 loss: 3.83453539e-07
Iter: 988 loss: 3.83136353e-07
Iter: 989 loss: 3.86856215e-07
Iter: 990 loss: 3.83143345e-07
Iter: 991 loss: 3.83049155e-07
Iter: 992 loss: 3.8302943e-07
Iter: 993 loss: 3.82954198e-07
Iter: 994 loss: 3.82786965e-07
Iter: 995 loss: 3.84444405e-07
Iter: 996 loss: 3.82762636e-07
Iter: 997 loss: 3.82594465e-07
Iter: 998 loss: 3.83377341e-07
Iter: 999 loss: 3.82569766e-07
Iter: 1000 loss: 3.82498087e-07
Iter: 1001 loss: 3.82479413e-07
Iter: 1002 loss: 3.82400543e-07
Iter: 1003 loss: 3.82149835e-07
Iter: 1004 loss: 3.82951498e-07
Iter: 1005 loss: 3.82016708e-07
Iter: 1006 loss: 3.81884831e-07
Iter: 1007 loss: 3.81828784e-07
Iter: 1008 loss: 3.81685709e-07
Iter: 1009 loss: 3.8190791e-07
Iter: 1010 loss: 3.81586744e-07
Iter: 1011 loss: 3.81426e-07
Iter: 1012 loss: 3.81363265e-07
Iter: 1013 loss: 3.81292296e-07
Iter: 1014 loss: 3.81160618e-07
Iter: 1015 loss: 3.81157463e-07
Iter: 1016 loss: 3.81058953e-07
Iter: 1017 loss: 3.80977895e-07
Iter: 1018 loss: 3.8096465e-07
Iter: 1019 loss: 3.80854431e-07
Iter: 1020 loss: 3.80718262e-07
Iter: 1021 loss: 3.80698879e-07
Iter: 1022 loss: 3.80540456e-07
Iter: 1023 loss: 3.81221923e-07
Iter: 1024 loss: 3.80514166e-07
Iter: 1025 loss: 3.80265703e-07
Iter: 1026 loss: 3.81237271e-07
Iter: 1027 loss: 3.80197349e-07
Iter: 1028 loss: 3.80006981e-07
Iter: 1029 loss: 3.79783614e-07
Iter: 1030 loss: 3.79734558e-07
Iter: 1031 loss: 3.79557491e-07
Iter: 1032 loss: 3.80582804e-07
Iter: 1033 loss: 3.79538449e-07
Iter: 1034 loss: 3.79385597e-07
Iter: 1035 loss: 3.81564519e-07
Iter: 1036 loss: 3.79381532e-07
Iter: 1037 loss: 3.79305476e-07
Iter: 1038 loss: 3.79181273e-07
Iter: 1039 loss: 3.79160866e-07
Iter: 1040 loss: 3.79129403e-07
Iter: 1041 loss: 3.79122241e-07
Iter: 1042 loss: 3.7905383e-07
Iter: 1043 loss: 3.78917662e-07
Iter: 1044 loss: 3.81431704e-07
Iter: 1045 loss: 3.78925904e-07
Iter: 1046 loss: 3.78769812e-07
Iter: 1047 loss: 3.79486806e-07
Iter: 1048 loss: 3.78741731e-07
Iter: 1049 loss: 3.78605023e-07
Iter: 1050 loss: 3.79773923e-07
Iter: 1051 loss: 3.78605193e-07
Iter: 1052 loss: 3.78500147e-07
Iter: 1053 loss: 3.78205954e-07
Iter: 1054 loss: 3.79369794e-07
Iter: 1055 loss: 3.78068933e-07
Iter: 1056 loss: 3.77799324e-07
Iter: 1057 loss: 3.82237317e-07
Iter: 1058 loss: 3.77784318e-07
Iter: 1059 loss: 3.77668556e-07
Iter: 1060 loss: 3.78814491e-07
Iter: 1061 loss: 3.77668925e-07
Iter: 1062 loss: 3.77526334e-07
Iter: 1063 loss: 3.77817599e-07
Iter: 1064 loss: 3.77476567e-07
Iter: 1065 loss: 3.77403097e-07
Iter: 1066 loss: 3.77351057e-07
Iter: 1067 loss: 3.77325676e-07
Iter: 1068 loss: 3.77236319e-07
Iter: 1069 loss: 3.78404962e-07
Iter: 1070 loss: 3.77239928e-07
Iter: 1071 loss: 3.77106403e-07
Iter: 1072 loss: 3.76926607e-07
Iter: 1073 loss: 3.76927431e-07
Iter: 1074 loss: 3.76786716e-07
Iter: 1075 loss: 3.7701983e-07
Iter: 1076 loss: 3.76730952e-07
Iter: 1077 loss: 3.76549565e-07
Iter: 1078 loss: 3.78836035e-07
Iter: 1079 loss: 3.7655235e-07
Iter: 1080 loss: 3.76448327e-07
Iter: 1081 loss: 3.76281093e-07
Iter: 1082 loss: 3.76266968e-07
Iter: 1083 loss: 3.76196226e-07
Iter: 1084 loss: 3.76178406e-07
Iter: 1085 loss: 3.76095613e-07
Iter: 1086 loss: 3.7620589e-07
Iter: 1087 loss: 3.76059404e-07
Iter: 1088 loss: 3.75971183e-07
Iter: 1089 loss: 3.75855706e-07
Iter: 1090 loss: 3.75873526e-07
Iter: 1091 loss: 3.75745458e-07
Iter: 1092 loss: 3.76197022e-07
Iter: 1093 loss: 3.75727296e-07
Iter: 1094 loss: 3.75621084e-07
Iter: 1095 loss: 3.76912453e-07
Iter: 1096 loss: 3.75616281e-07
Iter: 1097 loss: 3.75518937e-07
Iter: 1098 loss: 3.75341301e-07
Iter: 1099 loss: 3.79345408e-07
Iter: 1100 loss: 3.75353494e-07
Iter: 1101 loss: 3.75189245e-07
Iter: 1102 loss: 3.75952254e-07
Iter: 1103 loss: 3.75167986e-07
Iter: 1104 loss: 3.749835e-07
Iter: 1105 loss: 3.76065259e-07
Iter: 1106 loss: 3.7496585e-07
Iter: 1107 loss: 3.74868421e-07
Iter: 1108 loss: 3.74830876e-07
Iter: 1109 loss: 3.7478739e-07
Iter: 1110 loss: 3.74715228e-07
Iter: 1111 loss: 3.75543692e-07
Iter: 1112 loss: 3.74697436e-07
Iter: 1113 loss: 3.74598471e-07
Iter: 1114 loss: 3.74424161e-07
Iter: 1115 loss: 3.74418079e-07
Iter: 1116 loss: 3.74292881e-07
Iter: 1117 loss: 3.74405573e-07
Iter: 1118 loss: 3.74220292e-07
Iter: 1119 loss: 3.74046266e-07
Iter: 1120 loss: 3.75244298e-07
Iter: 1121 loss: 3.74035608e-07
Iter: 1122 loss: 3.7380758e-07
Iter: 1123 loss: 3.7361508e-07
Iter: 1124 loss: 3.73535215e-07
Iter: 1125 loss: 3.73381226e-07
Iter: 1126 loss: 3.74258491e-07
Iter: 1127 loss: 3.73353942e-07
Iter: 1128 loss: 3.73254835e-07
Iter: 1129 loss: 3.73921409e-07
Iter: 1130 loss: 3.73239175e-07
Iter: 1131 loss: 3.73098715e-07
Iter: 1132 loss: 3.73392766e-07
Iter: 1133 loss: 3.73036045e-07
Iter: 1134 loss: 3.72953536e-07
Iter: 1135 loss: 3.72940463e-07
Iter: 1136 loss: 3.72878532e-07
Iter: 1137 loss: 3.72826037e-07
Iter: 1138 loss: 3.72813929e-07
Iter: 1139 loss: 3.7274711e-07
Iter: 1140 loss: 3.72528035e-07
Iter: 1141 loss: 3.74351913e-07
Iter: 1142 loss: 3.72501205e-07
Iter: 1143 loss: 3.72296597e-07
Iter: 1144 loss: 3.7288271e-07
Iter: 1145 loss: 3.72204454e-07
Iter: 1146 loss: 3.72e-07
Iter: 1147 loss: 3.72012494e-07
Iter: 1148 loss: 3.71846795e-07
Iter: 1149 loss: 3.71522162e-07
Iter: 1150 loss: 3.76533308e-07
Iter: 1151 loss: 3.71489023e-07
Iter: 1152 loss: 3.71350836e-07
Iter: 1153 loss: 3.72231398e-07
Iter: 1154 loss: 3.71331339e-07
Iter: 1155 loss: 3.71246699e-07
Iter: 1156 loss: 3.71234137e-07
Iter: 1157 loss: 3.71169676e-07
Iter: 1158 loss: 3.71163509e-07
Iter: 1159 loss: 3.71088333e-07
Iter: 1160 loss: 3.71033792e-07
Iter: 1161 loss: 3.70959441e-07
Iter: 1162 loss: 3.70948953e-07
Iter: 1163 loss: 3.70811676e-07
Iter: 1164 loss: 3.72164038e-07
Iter: 1165 loss: 3.70798773e-07
Iter: 1166 loss: 3.70679288e-07
Iter: 1167 loss: 3.70582939e-07
Iter: 1168 loss: 3.70544512e-07
Iter: 1169 loss: 3.70376455e-07
Iter: 1170 loss: 3.70409197e-07
Iter: 1171 loss: 3.70249836e-07
Iter: 1172 loss: 3.70073678e-07
Iter: 1173 loss: 3.70064299e-07
Iter: 1174 loss: 3.6994382e-07
Iter: 1175 loss: 3.69783947e-07
Iter: 1176 loss: 3.69774227e-07
Iter: 1177 loss: 3.69664264e-07
Iter: 1178 loss: 3.70368099e-07
Iter: 1179 loss: 3.69676172e-07
Iter: 1180 loss: 3.69529062e-07
Iter: 1181 loss: 3.69681061e-07
Iter: 1182 loss: 3.69464715e-07
Iter: 1183 loss: 3.69349323e-07
Iter: 1184 loss: 3.6918118e-07
Iter: 1185 loss: 3.69156453e-07
Iter: 1186 loss: 3.68985553e-07
Iter: 1187 loss: 3.69473298e-07
Iter: 1188 loss: 3.68912652e-07
Iter: 1189 loss: 3.68725495e-07
Iter: 1190 loss: 3.71655972e-07
Iter: 1191 loss: 3.68723704e-07
Iter: 1192 loss: 3.68632925e-07
Iter: 1193 loss: 3.68543226e-07
Iter: 1194 loss: 3.68529271e-07
Iter: 1195 loss: 3.68396456e-07
Iter: 1196 loss: 3.68843246e-07
Iter: 1197 loss: 3.68359366e-07
Iter: 1198 loss: 3.68201e-07
Iter: 1199 loss: 3.6937297e-07
Iter: 1200 loss: 3.68189944e-07
Iter: 1201 loss: 3.68134323e-07
Iter: 1202 loss: 3.68042834e-07
Iter: 1203 loss: 3.68030612e-07
Iter: 1204 loss: 3.67877618e-07
Iter: 1205 loss: 3.68395831e-07
Iter: 1206 loss: 3.67849395e-07
Iter: 1207 loss: 3.67619862e-07
Iter: 1208 loss: 3.67786782e-07
Iter: 1209 loss: 3.67477469e-07
Iter: 1210 loss: 3.67331438e-07
Iter: 1211 loss: 3.67374241e-07
Iter: 1212 loss: 3.67235828e-07
Iter: 1213 loss: 3.67151046e-07
Iter: 1214 loss: 3.67136664e-07
Iter: 1215 loss: 3.6702437e-07
Iter: 1216 loss: 3.66964713e-07
Iter: 1217 loss: 3.66927708e-07
Iter: 1218 loss: 3.66862309e-07
Iter: 1219 loss: 3.66874872e-07
Iter: 1220 loss: 3.66825105e-07
Iter: 1221 loss: 3.6674885e-07
Iter: 1222 loss: 3.66747372e-07
Iter: 1223 loss: 3.66684e-07
Iter: 1224 loss: 3.6661379e-07
Iter: 1225 loss: 3.66602251e-07
Iter: 1226 loss: 3.66510307e-07
Iter: 1227 loss: 3.66412e-07
Iter: 1228 loss: 3.66402446e-07
Iter: 1229 loss: 3.6621185e-07
Iter: 1230 loss: 3.68126507e-07
Iter: 1231 loss: 3.66218558e-07
Iter: 1232 loss: 3.66062238e-07
Iter: 1233 loss: 3.65980611e-07
Iter: 1234 loss: 3.65914e-07
Iter: 1235 loss: 3.65778476e-07
Iter: 1236 loss: 3.65814515e-07
Iter: 1237 loss: 3.65674168e-07
Iter: 1238 loss: 3.65577904e-07
Iter: 1239 loss: 3.65588733e-07
Iter: 1240 loss: 3.65491985e-07
Iter: 1241 loss: 3.65547e-07
Iter: 1242 loss: 3.65433522e-07
Iter: 1243 loss: 3.65372784e-07
Iter: 1244 loss: 3.65428832e-07
Iter: 1245 loss: 3.65350076e-07
Iter: 1246 loss: 3.6527274e-07
Iter: 1247 loss: 3.66070594e-07
Iter: 1248 loss: 3.65256454e-07
Iter: 1249 loss: 3.65208052e-07
Iter: 1250 loss: 3.65073674e-07
Iter: 1251 loss: 3.65896597e-07
Iter: 1252 loss: 3.65063443e-07
Iter: 1253 loss: 3.64833937e-07
Iter: 1254 loss: 3.6494572e-07
Iter: 1255 loss: 3.64696973e-07
Iter: 1256 loss: 3.64618131e-07
Iter: 1257 loss: 3.64545372e-07
Iter: 1258 loss: 3.64467667e-07
Iter: 1259 loss: 3.64362e-07
Iter: 1260 loss: 3.64334738e-07
Iter: 1261 loss: 3.64195984e-07
Iter: 1262 loss: 3.64725281e-07
Iter: 1263 loss: 3.64173331e-07
Iter: 1264 loss: 3.64061e-07
Iter: 1265 loss: 3.65106786e-07
Iter: 1266 loss: 3.64059645e-07
Iter: 1267 loss: 3.63996236e-07
Iter: 1268 loss: 3.63924357e-07
Iter: 1269 loss: 3.63917707e-07
Iter: 1270 loss: 3.63799046e-07
Iter: 1271 loss: 3.64133371e-07
Iter: 1272 loss: 3.63775399e-07
Iter: 1273 loss: 3.63641391e-07
Iter: 1274 loss: 3.64667272e-07
Iter: 1275 loss: 3.63630619e-07
Iter: 1276 loss: 3.6356505e-07
Iter: 1277 loss: 3.63463698e-07
Iter: 1278 loss: 3.63463982e-07
Iter: 1279 loss: 3.63358566e-07
Iter: 1280 loss: 3.64621457e-07
Iter: 1281 loss: 3.63340632e-07
Iter: 1282 loss: 3.63239451e-07
Iter: 1283 loss: 3.63253207e-07
Iter: 1284 loss: 3.63150662e-07
Iter: 1285 loss: 3.63091942e-07
Iter: 1286 loss: 3.63125764e-07
Iter: 1287 loss: 3.63037401e-07
Iter: 1288 loss: 3.62950345e-07
Iter: 1289 loss: 3.62920673e-07
Iter: 1290 loss: 3.62853257e-07
Iter: 1291 loss: 3.62789478e-07
Iter: 1292 loss: 3.62779076e-07
Iter: 1293 loss: 3.62728827e-07
Iter: 1294 loss: 3.62605761e-07
Iter: 1295 loss: 3.64088777e-07
Iter: 1296 loss: 3.62592175e-07
Iter: 1297 loss: 3.62482155e-07
Iter: 1298 loss: 3.62494518e-07
Iter: 1299 loss: 3.62378273e-07
Iter: 1300 loss: 3.62275671e-07
Iter: 1301 loss: 3.62260323e-07
Iter: 1302 loss: 3.62161813e-07
Iter: 1303 loss: 3.62926926e-07
Iter: 1304 loss: 3.62159369e-07
Iter: 1305 loss: 3.62082602e-07
Iter: 1306 loss: 3.62393081e-07
Iter: 1307 loss: 3.62061485e-07
Iter: 1308 loss: 3.61980256e-07
Iter: 1309 loss: 3.6188132e-07
Iter: 1310 loss: 3.61875863e-07
Iter: 1311 loss: 3.61772152e-07
Iter: 1312 loss: 3.62000293e-07
Iter: 1313 loss: 3.61748732e-07
Iter: 1314 loss: 3.61657e-07
Iter: 1315 loss: 3.61640559e-07
Iter: 1316 loss: 3.61570642e-07
Iter: 1317 loss: 3.61379193e-07
Iter: 1318 loss: 3.63380536e-07
Iter: 1319 loss: 3.6136251e-07
Iter: 1320 loss: 3.61205565e-07
Iter: 1321 loss: 3.61696209e-07
Iter: 1322 loss: 3.61149489e-07
Iter: 1323 loss: 3.61042424e-07
Iter: 1324 loss: 3.6282205e-07
Iter: 1325 loss: 3.61031255e-07
Iter: 1326 loss: 3.60924162e-07
Iter: 1327 loss: 3.61497257e-07
Iter: 1328 loss: 3.60905346e-07
Iter: 1329 loss: 3.60865585e-07
Iter: 1330 loss: 3.607812e-07
Iter: 1331 loss: 3.60784924e-07
Iter: 1332 loss: 3.60676125e-07
Iter: 1333 loss: 3.61785965e-07
Iter: 1334 loss: 3.60672345e-07
Iter: 1335 loss: 3.60608908e-07
Iter: 1336 loss: 3.60606123e-07
Iter: 1337 loss: 3.60557067e-07
Iter: 1338 loss: 3.60484563e-07
Iter: 1339 loss: 3.61225318e-07
Iter: 1340 loss: 3.60491754e-07
Iter: 1341 loss: 3.60425673e-07
Iter: 1342 loss: 3.60344131e-07
Iter: 1343 loss: 3.60342199e-07
Iter: 1344 loss: 3.60251818e-07
Iter: 1345 loss: 3.60322304e-07
Iter: 1346 loss: 3.60153933e-07
Iter: 1347 loss: 3.60069407e-07
Iter: 1348 loss: 3.60309e-07
Iter: 1349 loss: 3.60028139e-07
Iter: 1350 loss: 3.59932756e-07
Iter: 1351 loss: 3.61291683e-07
Iter: 1352 loss: 3.59936706e-07
Iter: 1353 loss: 3.5986929e-07
Iter: 1354 loss: 3.59797809e-07
Iter: 1355 loss: 3.59792409e-07
Iter: 1356 loss: 3.5968128e-07
Iter: 1357 loss: 3.59870569e-07
Iter: 1358 loss: 3.59623584e-07
Iter: 1359 loss: 3.59503986e-07
Iter: 1360 loss: 3.61281536e-07
Iter: 1361 loss: 3.59512825e-07
Iter: 1362 loss: 3.59426195e-07
Iter: 1363 loss: 3.5929645e-07
Iter: 1364 loss: 3.59289658e-07
Iter: 1365 loss: 3.59196235e-07
Iter: 1366 loss: 3.59189386e-07
Iter: 1367 loss: 3.59106679e-07
Iter: 1368 loss: 3.59022e-07
Iter: 1369 loss: 3.58992281e-07
Iter: 1370 loss: 3.58933789e-07
Iter: 1371 loss: 3.58943282e-07
Iter: 1372 loss: 3.5888354e-07
Iter: 1373 loss: 3.58884677e-07
Iter: 1374 loss: 3.58833574e-07
Iter: 1375 loss: 3.587663e-07
Iter: 1376 loss: 3.5868004e-07
Iter: 1377 loss: 3.58665545e-07
Iter: 1378 loss: 3.58514455e-07
Iter: 1379 loss: 3.59125693e-07
Iter: 1380 loss: 3.58488592e-07
Iter: 1381 loss: 3.58361291e-07
Iter: 1382 loss: 3.5846324e-07
Iter: 1383 loss: 3.58282932e-07
Iter: 1384 loss: 3.5813855e-07
Iter: 1385 loss: 3.58150629e-07
Iter: 1386 loss: 3.58058514e-07
Iter: 1387 loss: 3.57899751e-07
Iter: 1388 loss: 3.57897477e-07
Iter: 1389 loss: 3.57830402e-07
Iter: 1390 loss: 3.5782449e-07
Iter: 1391 loss: 3.57744739e-07
Iter: 1392 loss: 3.57659928e-07
Iter: 1393 loss: 3.57640431e-07
Iter: 1394 loss: 3.5754e-07
Iter: 1395 loss: 3.5760857e-07
Iter: 1396 loss: 3.57489483e-07
Iter: 1397 loss: 3.57392793e-07
Iter: 1398 loss: 3.57396033e-07
Iter: 1399 loss: 3.57321255e-07
Iter: 1400 loss: 3.57226099e-07
Iter: 1401 loss: 3.57220671e-07
Iter: 1402 loss: 3.57172269e-07
Iter: 1403 loss: 3.57139584e-07
Iter: 1404 loss: 3.57092176e-07
Iter: 1405 loss: 3.56990768e-07
Iter: 1406 loss: 3.56989347e-07
Iter: 1407 loss: 3.56882737e-07
Iter: 1408 loss: 3.56731249e-07
Iter: 1409 loss: 3.56712235e-07
Iter: 1410 loss: 3.56544319e-07
Iter: 1411 loss: 3.57665328e-07
Iter: 1412 loss: 3.56540227e-07
Iter: 1413 loss: 3.56381889e-07
Iter: 1414 loss: 3.56349233e-07
Iter: 1415 loss: 3.56243675e-07
Iter: 1416 loss: 3.56316036e-07
Iter: 1417 loss: 3.56163952e-07
Iter: 1418 loss: 3.56121092e-07
Iter: 1419 loss: 3.55983843e-07
Iter: 1420 loss: 3.58136674e-07
Iter: 1421 loss: 3.55976198e-07
Iter: 1422 loss: 3.55861403e-07
Iter: 1423 loss: 3.57014642e-07
Iter: 1424 loss: 3.55870611e-07
Iter: 1425 loss: 3.55741093e-07
Iter: 1426 loss: 3.55887039e-07
Iter: 1427 loss: 3.55686979e-07
Iter: 1428 loss: 3.55625275e-07
Iter: 1429 loss: 3.56049668e-07
Iter: 1430 loss: 3.55612769e-07
Iter: 1431 loss: 3.55558825e-07
Iter: 1432 loss: 3.55489789e-07
Iter: 1433 loss: 3.55475322e-07
Iter: 1434 loss: 3.55371924e-07
Iter: 1435 loss: 3.55759312e-07
Iter: 1436 loss: 3.55353905e-07
Iter: 1437 loss: 3.55242122e-07
Iter: 1438 loss: 3.55709631e-07
Iter: 1439 loss: 3.55215491e-07
Iter: 1440 loss: 3.55152167e-07
Iter: 1441 loss: 3.5502012e-07
Iter: 1442 loss: 3.57546128e-07
Iter: 1443 loss: 3.55018813e-07
Iter: 1444 loss: 3.54860418e-07
Iter: 1445 loss: 3.55627748e-07
Iter: 1446 loss: 3.54838676e-07
Iter: 1447 loss: 3.54744429e-07
Iter: 1448 loss: 3.54829467e-07
Iter: 1449 loss: 3.54677951e-07
Iter: 1450 loss: 3.54552952e-07
Iter: 1451 loss: 3.55360697e-07
Iter: 1452 loss: 3.54540589e-07
Iter: 1453 loss: 3.54490709e-07
Iter: 1454 loss: 3.54495171e-07
Iter: 1455 loss: 3.54450606e-07
Iter: 1456 loss: 3.54334588e-07
Iter: 1457 loss: 3.55088616e-07
Iter: 1458 loss: 3.5430412e-07
Iter: 1459 loss: 3.54211068e-07
Iter: 1460 loss: 3.55563088e-07
Iter: 1461 loss: 3.54217349e-07
Iter: 1462 loss: 3.54133306e-07
Iter: 1463 loss: 3.54556789e-07
Iter: 1464 loss: 3.54100791e-07
Iter: 1465 loss: 3.54055288e-07
Iter: 1466 loss: 3.53924634e-07
Iter: 1467 loss: 3.5504064e-07
Iter: 1468 loss: 3.53900219e-07
Iter: 1469 loss: 3.53913066e-07
Iter: 1470 loss: 3.53831496e-07
Iter: 1471 loss: 3.5378045e-07
Iter: 1472 loss: 3.53718406e-07
Iter: 1473 loss: 3.55274949e-07
Iter: 1474 loss: 3.5373202e-07
Iter: 1475 loss: 3.53620123e-07
Iter: 1476 loss: 3.53547733e-07
Iter: 1477 loss: 3.53504106e-07
Iter: 1478 loss: 3.53421257e-07
Iter: 1479 loss: 3.53382404e-07
Iter: 1480 loss: 3.5335114e-07
Iter: 1481 loss: 3.53230291e-07
Iter: 1482 loss: 3.54834725e-07
Iter: 1483 loss: 3.5322779e-07
Iter: 1484 loss: 3.53066952e-07
Iter: 1485 loss: 3.53528975e-07
Iter: 1486 loss: 3.53008772e-07
Iter: 1487 loss: 3.52988252e-07
Iter: 1488 loss: 3.5294326e-07
Iter: 1489 loss: 3.52876327e-07
Iter: 1490 loss: 3.52912707e-07
Iter: 1491 loss: 3.52838555e-07
Iter: 1492 loss: 3.52808826e-07
Iter: 1493 loss: 3.52719155e-07
Iter: 1494 loss: 3.52736947e-07
Iter: 1495 loss: 3.52660408e-07
Iter: 1496 loss: 3.53012e-07
Iter: 1497 loss: 3.52642076e-07
Iter: 1498 loss: 3.52567412e-07
Iter: 1499 loss: 3.52563802e-07
Iter: 1500 loss: 3.52513212e-07
Iter: 1501 loss: 3.52398928e-07
Iter: 1502 loss: 3.54307417e-07
Iter: 1503 loss: 3.52396739e-07
Iter: 1504 loss: 3.52293938e-07
Iter: 1505 loss: 3.52673396e-07
Iter: 1506 loss: 3.52266085e-07
Iter: 1507 loss: 3.52133924e-07
Iter: 1508 loss: 3.53186e-07
Iter: 1509 loss: 3.52115421e-07
Iter: 1510 loss: 3.52039734e-07
Iter: 1511 loss: 3.51936933e-07
Iter: 1512 loss: 3.51932954e-07
Iter: 1513 loss: 3.51834501e-07
Iter: 1514 loss: 3.5228328e-07
Iter: 1515 loss: 3.51816368e-07
Iter: 1516 loss: 3.51712146e-07
Iter: 1517 loss: 3.52691046e-07
Iter: 1518 loss: 3.51699043e-07
Iter: 1519 loss: 3.51664085e-07
Iter: 1520 loss: 3.51579189e-07
Iter: 1521 loss: 3.51571146e-07
Iter: 1522 loss: 3.5149975e-07
Iter: 1523 loss: 3.52139296e-07
Iter: 1524 loss: 3.51499551e-07
Iter: 1525 loss: 3.51393879e-07
Iter: 1526 loss: 3.51498983e-07
Iter: 1527 loss: 3.51342067e-07
Iter: 1528 loss: 3.51271e-07
Iter: 1529 loss: 3.51191431e-07
Iter: 1530 loss: 3.51188305e-07
Iter: 1531 loss: 3.5109133e-07
Iter: 1532 loss: 3.51097555e-07
Iter: 1533 loss: 3.51017547e-07
Iter: 1534 loss: 3.51150931e-07
Iter: 1535 loss: 3.50985545e-07
Iter: 1536 loss: 3.50925234e-07
Iter: 1537 loss: 3.50839969e-07
Iter: 1538 loss: 3.50836217e-07
Iter: 1539 loss: 3.50728442e-07
Iter: 1540 loss: 3.52205348e-07
Iter: 1541 loss: 3.50732392e-07
Iter: 1542 loss: 3.50618279e-07
Iter: 1543 loss: 3.50568257e-07
Iter: 1544 loss: 3.50522498e-07
Iter: 1545 loss: 3.50436778e-07
Iter: 1546 loss: 3.50329685e-07
Iter: 1547 loss: 3.50342532e-07
Iter: 1548 loss: 3.5029538e-07
Iter: 1549 loss: 3.50270255e-07
Iter: 1550 loss: 3.50191897e-07
Iter: 1551 loss: 3.50197439e-07
Iter: 1552 loss: 3.50136816e-07
Iter: 1553 loss: 3.50096e-07
Iter: 1554 loss: 3.50138407e-07
Iter: 1555 loss: 3.50045184e-07
Iter: 1556 loss: 3.50023129e-07
Iter: 1557 loss: 3.50001358e-07
Iter: 1558 loss: 3.49966967e-07
Iter: 1559 loss: 3.49886534e-07
Iter: 1560 loss: 3.50331163e-07
Iter: 1561 loss: 3.49858055e-07
Iter: 1562 loss: 3.49687866e-07
Iter: 1563 loss: 3.4983691e-07
Iter: 1564 loss: 3.49579466e-07
Iter: 1565 loss: 3.49583161e-07
Iter: 1566 loss: 3.49521486e-07
Iter: 1567 loss: 3.49457821e-07
Iter: 1568 loss: 3.49359539e-07
Iter: 1569 loss: 3.49341377e-07
Iter: 1570 loss: 3.4923346e-07
Iter: 1571 loss: 3.49199553e-07
Iter: 1572 loss: 3.49141715e-07
Iter: 1573 loss: 3.49114487e-07
Iter: 1574 loss: 3.49061452e-07
Iter: 1575 loss: 3.49015238e-07
Iter: 1576 loss: 3.48921276e-07
Iter: 1577 loss: 3.48936396e-07
Iter: 1578 loss: 3.48844964e-07
Iter: 1579 loss: 3.48978745e-07
Iter: 1580 loss: 3.48811312e-07
Iter: 1581 loss: 3.48750376e-07
Iter: 1582 loss: 3.48747e-07
Iter: 1583 loss: 3.4869592e-07
Iter: 1584 loss: 3.48620915e-07
Iter: 1585 loss: 3.48603209e-07
Iter: 1586 loss: 3.4853278e-07
Iter: 1587 loss: 3.48625264e-07
Iter: 1588 loss: 3.4846164e-07
Iter: 1589 loss: 3.48397236e-07
Iter: 1590 loss: 3.48378876e-07
Iter: 1591 loss: 3.48338972e-07
Iter: 1592 loss: 3.48228127e-07
Iter: 1593 loss: 3.49177725e-07
Iter: 1594 loss: 3.48212325e-07
Iter: 1595 loss: 3.48092897e-07
Iter: 1596 loss: 3.48725791e-07
Iter: 1597 loss: 3.48077151e-07
Iter: 1598 loss: 3.47990067e-07
Iter: 1599 loss: 3.49386909e-07
Iter: 1600 loss: 3.47985e-07
Iter: 1601 loss: 3.4789997e-07
Iter: 1602 loss: 3.47788557e-07
Iter: 1603 loss: 3.47801489e-07
Iter: 1604 loss: 3.47674018e-07
Iter: 1605 loss: 3.47894115e-07
Iter: 1606 loss: 3.47625985e-07
Iter: 1607 loss: 3.47582272e-07
Iter: 1608 loss: 3.47574e-07
Iter: 1609 loss: 3.47504056e-07
Iter: 1610 loss: 3.47381899e-07
Iter: 1611 loss: 3.50318658e-07
Iter: 1612 loss: 3.47374e-07
Iter: 1613 loss: 3.47309424e-07
Iter: 1614 loss: 3.47504567e-07
Iter: 1615 loss: 3.47284129e-07
Iter: 1616 loss: 3.47231207e-07
Iter: 1617 loss: 3.4722774e-07
Iter: 1618 loss: 3.47170953e-07
Iter: 1619 loss: 3.47092652e-07
Iter: 1620 loss: 3.47077844e-07
Iter: 1621 loss: 3.47018755e-07
Iter: 1622 loss: 3.47316842e-07
Iter: 1623 loss: 3.4700318e-07
Iter: 1624 loss: 3.46894552e-07
Iter: 1625 loss: 3.47170612e-07
Iter: 1626 loss: 3.46868717e-07
Iter: 1627 loss: 3.46785839e-07
Iter: 1628 loss: 3.4666229e-07
Iter: 1629 loss: 3.46675677e-07
Iter: 1630 loss: 3.46566878e-07
Iter: 1631 loss: 3.47766274e-07
Iter: 1632 loss: 3.4657586e-07
Iter: 1633 loss: 3.46474025e-07
Iter: 1634 loss: 3.46756536e-07
Iter: 1635 loss: 3.46439265e-07
Iter: 1636 loss: 3.46345928e-07
Iter: 1637 loss: 3.46321144e-07
Iter: 1638 loss: 3.46275783e-07
Iter: 1639 loss: 3.46213398e-07
Iter: 1640 loss: 3.46868433e-07
Iter: 1641 loss: 3.46203336e-07
Iter: 1642 loss: 3.46107242e-07
Iter: 1643 loss: 3.46376567e-07
Iter: 1644 loss: 3.46093657e-07
Iter: 1645 loss: 3.46031641e-07
Iter: 1646 loss: 3.45889674e-07
Iter: 1647 loss: 3.49115112e-07
Iter: 1648 loss: 3.45887457e-07
Iter: 1649 loss: 3.4577954e-07
Iter: 1650 loss: 3.46309605e-07
Iter: 1651 loss: 3.45785907e-07
Iter: 1652 loss: 3.45701778e-07
Iter: 1653 loss: 3.45705644e-07
Iter: 1654 loss: 3.45648203e-07
Iter: 1655 loss: 3.45546e-07
Iter: 1656 loss: 3.47448633e-07
Iter: 1657 loss: 3.4554219e-07
Iter: 1658 loss: 3.45503e-07
Iter: 1659 loss: 3.4547989e-07
Iter: 1660 loss: 3.45426827e-07
Iter: 1661 loss: 3.45261952e-07
Iter: 1662 loss: 3.46155616e-07
Iter: 1663 loss: 3.45221679e-07
Iter: 1664 loss: 3.45018464e-07
Iter: 1665 loss: 3.45648431e-07
Iter: 1666 loss: 3.44967077e-07
Iter: 1667 loss: 3.44846455e-07
Iter: 1668 loss: 3.46329159e-07
Iter: 1669 loss: 3.44847962e-07
Iter: 1670 loss: 3.44715687e-07
Iter: 1671 loss: 3.45083095e-07
Iter: 1672 loss: 3.44682235e-07
Iter: 1673 loss: 3.44619707e-07
Iter: 1674 loss: 3.44623572e-07
Iter: 1675 loss: 3.44580485e-07
Iter: 1676 loss: 3.44508209e-07
Iter: 1677 loss: 3.44618286e-07
Iter: 1678 loss: 3.44486e-07
Iter: 1679 loss: 3.44410182e-07
Iter: 1680 loss: 3.45257092e-07
Iter: 1681 loss: 3.44403531e-07
Iter: 1682 loss: 3.44384574e-07
Iter: 1683 loss: 3.44325258e-07
Iter: 1684 loss: 3.45442515e-07
Iter: 1685 loss: 3.44328669e-07
Iter: 1686 loss: 3.44214897e-07
Iter: 1687 loss: 3.44081712e-07
Iter: 1688 loss: 3.44078956e-07
Iter: 1689 loss: 3.44213731e-07
Iter: 1690 loss: 3.44026745e-07
Iter: 1691 loss: 3.43977888e-07
Iter: 1692 loss: 3.4385107e-07
Iter: 1693 loss: 3.44818062e-07
Iter: 1694 loss: 3.43826116e-07
Iter: 1695 loss: 3.43696854e-07
Iter: 1696 loss: 3.44855152e-07
Iter: 1697 loss: 3.43679147e-07
Iter: 1698 loss: 3.43641545e-07
Iter: 1699 loss: 3.43626908e-07
Iter: 1700 loss: 3.4360886e-07
Iter: 1701 loss: 3.43553552e-07
Iter: 1702 loss: 3.43925933e-07
Iter: 1703 loss: 3.43503672e-07
Iter: 1704 loss: 3.43424091e-07
Iter: 1705 loss: 3.44157968e-07
Iter: 1706 loss: 3.43411472e-07
Iter: 1707 loss: 3.43358579e-07
Iter: 1708 loss: 3.44258495e-07
Iter: 1709 loss: 3.43365656e-07
Iter: 1710 loss: 3.43333397e-07
Iter: 1711 loss: 3.43233239e-07
Iter: 1712 loss: 3.43783142e-07
Iter: 1713 loss: 3.43208853e-07
Iter: 1714 loss: 3.43081894e-07
Iter: 1715 loss: 3.43534225e-07
Iter: 1716 loss: 3.43043951e-07
Iter: 1717 loss: 3.43030081e-07
Iter: 1718 loss: 3.42991171e-07
Iter: 1719 loss: 3.42952518e-07
Iter: 1720 loss: 3.42847557e-07
Iter: 1721 loss: 3.44233541e-07
Iter: 1722 loss: 3.42851166e-07
Iter: 1723 loss: 3.42776445e-07
Iter: 1724 loss: 3.43092211e-07
Iter: 1725 loss: 3.42759733e-07
Iter: 1726 loss: 3.42725258e-07
Iter: 1727 loss: 3.42727731e-07
Iter: 1728 loss: 3.42687684e-07
Iter: 1729 loss: 3.42569308e-07
Iter: 1730 loss: 3.43115772e-07
Iter: 1731 loss: 3.4253074e-07
Iter: 1732 loss: 3.42480689e-07
Iter: 1733 loss: 3.42474493e-07
Iter: 1734 loss: 3.42416712e-07
Iter: 1735 loss: 3.42843435e-07
Iter: 1736 loss: 3.42419753e-07
Iter: 1737 loss: 3.42377348e-07
Iter: 1738 loss: 3.42308397e-07
Iter: 1739 loss: 3.42824421e-07
Iter: 1740 loss: 3.42267469e-07
Iter: 1741 loss: 3.42270056e-07
Iter: 1742 loss: 3.42240355e-07
Iter: 1743 loss: 3.42212559e-07
Iter: 1744 loss: 3.42233591e-07
Iter: 1745 loss: 3.42189253e-07
Iter: 1746 loss: 3.4215509e-07
Iter: 1747 loss: 3.42054534e-07
Iter: 1748 loss: 3.43757e-07
Iter: 1749 loss: 3.42082501e-07
Iter: 1750 loss: 3.41964437e-07
Iter: 1751 loss: 3.42152191e-07
Iter: 1752 loss: 3.41939568e-07
Iter: 1753 loss: 3.41828809e-07
Iter: 1754 loss: 3.42887716e-07
Iter: 1755 loss: 3.41823863e-07
Iter: 1756 loss: 3.4171839e-07
Iter: 1757 loss: 3.4177009e-07
Iter: 1758 loss: 3.41634831e-07
Iter: 1759 loss: 3.41561872e-07
Iter: 1760 loss: 3.41575856e-07
Iter: 1761 loss: 3.41519637e-07
Iter: 1762 loss: 3.41444832e-07
Iter: 1763 loss: 3.41926579e-07
Iter: 1764 loss: 3.41438749e-07
Iter: 1765 loss: 3.41361158e-07
Iter: 1766 loss: 3.42116778e-07
Iter: 1767 loss: 3.41351608e-07
Iter: 1768 loss: 3.41318867e-07
Iter: 1769 loss: 3.41248722e-07
Iter: 1770 loss: 3.41819941e-07
Iter: 1771 loss: 3.41246562e-07
Iter: 1772 loss: 3.41124263e-07
Iter: 1773 loss: 3.41610843e-07
Iter: 1774 loss: 3.4110991e-07
Iter: 1775 loss: 3.41046814e-07
Iter: 1776 loss: 3.41040447e-07
Iter: 1777 loss: 3.41012026e-07
Iter: 1778 loss: 3.40944581e-07
Iter: 1779 loss: 3.41268276e-07
Iter: 1780 loss: 3.40887425e-07
Iter: 1781 loss: 3.40895468e-07
Iter: 1782 loss: 3.40841495e-07
Iter: 1783 loss: 3.40802956e-07
Iter: 1784 loss: 3.4075137e-07
Iter: 1785 loss: 3.40751313e-07
Iter: 1786 loss: 3.40656214e-07
Iter: 1787 loss: 3.40706322e-07
Iter: 1788 loss: 3.40590674e-07
Iter: 1789 loss: 3.40497252e-07
Iter: 1790 loss: 3.40501657e-07
Iter: 1791 loss: 3.4044794e-07
Iter: 1792 loss: 3.4039698e-07
Iter: 1793 loss: 3.40374811e-07
Iter: 1794 loss: 3.40286107e-07
Iter: 1795 loss: 3.40256292e-07
Iter: 1796 loss: 3.4019061e-07
Iter: 1797 loss: 3.40159886e-07
Iter: 1798 loss: 3.40118845e-07
Iter: 1799 loss: 3.40074052e-07
Iter: 1800 loss: 3.40022069e-07
Iter: 1801 loss: 3.4001215e-07
Iter: 1802 loss: 3.39960309e-07
Iter: 1803 loss: 3.39918699e-07
Iter: 1804 loss: 3.39910713e-07
Iter: 1805 loss: 3.39889e-07
Iter: 1806 loss: 3.3986862e-07
Iter: 1807 loss: 3.39805041e-07
Iter: 1808 loss: 3.39775e-07
Iter: 1809 loss: 3.39763716e-07
Iter: 1810 loss: 3.39687659e-07
Iter: 1811 loss: 3.39606913e-07
Iter: 1812 loss: 3.39609016e-07
Iter: 1813 loss: 3.39590144e-07
Iter: 1814 loss: 3.39545068e-07
Iter: 1815 loss: 3.39505874e-07
Iter: 1816 loss: 3.39423252e-07
Iter: 1817 loss: 3.40541476e-07
Iter: 1818 loss: 3.39415749e-07
Iter: 1819 loss: 3.39328835e-07
Iter: 1820 loss: 3.39681264e-07
Iter: 1821 loss: 3.39308372e-07
Iter: 1822 loss: 3.39243684e-07
Iter: 1823 loss: 3.39243684e-07
Iter: 1824 loss: 3.3921657e-07
Iter: 1825 loss: 3.3917928e-07
Iter: 1826 loss: 3.39177575e-07
Iter: 1827 loss: 3.39129826e-07
Iter: 1828 loss: 3.39264602e-07
Iter: 1829 loss: 3.39111921e-07
Iter: 1830 loss: 3.39081168e-07
Iter: 1831 loss: 3.39081794e-07
Iter: 1832 loss: 3.39068635e-07
Iter: 1833 loss: 3.38980215e-07
Iter: 1834 loss: 3.39234163e-07
Iter: 1835 loss: 3.38934058e-07
Iter: 1836 loss: 3.38832933e-07
Iter: 1837 loss: 3.39567976e-07
Iter: 1838 loss: 3.38818666e-07
Iter: 1839 loss: 3.38788226e-07
Iter: 1840 loss: 3.38769667e-07
Iter: 1841 loss: 3.38741188e-07
Iter: 1842 loss: 3.38625455e-07
Iter: 1843 loss: 3.39624336e-07
Iter: 1844 loss: 3.38627956e-07
Iter: 1845 loss: 3.38546272e-07
Iter: 1846 loss: 3.3897004e-07
Iter: 1847 loss: 3.38518305e-07
Iter: 1848 loss: 3.38490167e-07
Iter: 1849 loss: 3.39026286e-07
Iter: 1850 loss: 3.38489428e-07
Iter: 1851 loss: 3.38447194e-07
Iter: 1852 loss: 3.38382222e-07
Iter: 1853 loss: 3.38398763e-07
Iter: 1854 loss: 3.38302243e-07
Iter: 1855 loss: 3.38308439e-07
Iter: 1856 loss: 3.38256655e-07
Iter: 1857 loss: 3.38153484e-07
Iter: 1858 loss: 3.38137255e-07
Iter: 1859 loss: 3.38061056e-07
Iter: 1860 loss: 3.38010778e-07
Iter: 1861 loss: 3.37987501e-07
Iter: 1862 loss: 3.37921733e-07
Iter: 1863 loss: 3.37980907e-07
Iter: 1864 loss: 3.3788794e-07
Iter: 1865 loss: 3.37833967e-07
Iter: 1866 loss: 3.3794214e-07
Iter: 1867 loss: 3.37802874e-07
Iter: 1868 loss: 3.37726561e-07
Iter: 1869 loss: 3.37922529e-07
Iter: 1870 loss: 3.37711157e-07
Iter: 1871 loss: 3.37641069e-07
Iter: 1872 loss: 3.37639676e-07
Iter: 1873 loss: 3.37610913e-07
Iter: 1874 loss: 3.37546112e-07
Iter: 1875 loss: 3.37828482e-07
Iter: 1876 loss: 3.3753139e-07
Iter: 1877 loss: 3.37435381e-07
Iter: 1878 loss: 3.3739272e-07
Iter: 1879 loss: 3.37351793e-07
Iter: 1880 loss: 3.37270734e-07
Iter: 1881 loss: 3.37314475e-07
Iter: 1882 loss: 3.37222758e-07
Iter: 1883 loss: 3.37149e-07
Iter: 1884 loss: 3.37141159e-07
Iter: 1885 loss: 3.37081644e-07
Iter: 1886 loss: 3.36981486e-07
Iter: 1887 loss: 3.36963467e-07
Iter: 1888 loss: 3.36911171e-07
Iter: 1889 loss: 3.37016502e-07
Iter: 1890 loss: 3.36858932e-07
Iter: 1891 loss: 3.36784211e-07
Iter: 1892 loss: 3.36826872e-07
Iter: 1893 loss: 3.36723758e-07
Iter: 1894 loss: 3.3668303e-07
Iter: 1895 loss: 3.36654409e-07
Iter: 1896 loss: 3.36635139e-07
Iter: 1897 loss: 3.36551579e-07
Iter: 1898 loss: 3.37272468e-07
Iter: 1899 loss: 3.3651952e-07
Iter: 1900 loss: 3.36445794e-07
Iter: 1901 loss: 3.36436642e-07
Iter: 1902 loss: 3.36375706e-07
Iter: 1903 loss: 3.36357431e-07
Iter: 1904 loss: 3.36293397e-07
Iter: 1905 loss: 3.36248377e-07
Iter: 1906 loss: 3.36461937e-07
Iter: 1907 loss: 3.36224616e-07
Iter: 1908 loss: 3.3618e-07
Iter: 1909 loss: 3.36695621e-07
Iter: 1910 loss: 3.36176299e-07
Iter: 1911 loss: 3.36127243e-07
Iter: 1912 loss: 3.36079182e-07
Iter: 1913 loss: 3.36083275e-07
Iter: 1914 loss: 3.36037544e-07
Iter: 1915 loss: 3.36035896e-07
Iter: 1916 loss: 3.36017962e-07
Iter: 1917 loss: 3.35949665e-07
Iter: 1918 loss: 3.35954695e-07
Iter: 1919 loss: 3.35879719e-07
Iter: 1920 loss: 3.35962284e-07
Iter: 1921 loss: 3.35843083e-07
Iter: 1922 loss: 3.35750656e-07
Iter: 1923 loss: 3.35819095e-07
Iter: 1924 loss: 3.35693869e-07
Iter: 1925 loss: 3.35680852e-07
Iter: 1926 loss: 3.35640181e-07
Iter: 1927 loss: 3.35604909e-07
Iter: 1928 loss: 3.35607865e-07
Iter: 1929 loss: 3.35576942e-07
Iter: 1930 loss: 3.35527233e-07
Iter: 1931 loss: 3.35749689e-07
Iter: 1932 loss: 3.35519559e-07
Iter: 1933 loss: 3.35479513e-07
Iter: 1934 loss: 3.35590471e-07
Iter: 1935 loss: 3.35449045e-07
Iter: 1936 loss: 3.35433668e-07
Iter: 1937 loss: 3.35432958e-07
Iter: 1938 loss: 3.35406781e-07
Iter: 1939 loss: 3.35383163e-07
Iter: 1940 loss: 3.35755715e-07
Iter: 1941 loss: 3.35366565e-07
Iter: 1942 loss: 3.35320692e-07
Iter: 1943 loss: 3.35306538e-07
Iter: 1944 loss: 3.35281499e-07
Iter: 1945 loss: 3.35227639e-07
Iter: 1946 loss: 3.35342037e-07
Iter: 1947 loss: 3.35199957e-07
Iter: 1948 loss: 3.35133393e-07
Iter: 1949 loss: 3.35378445e-07
Iter: 1950 loss: 3.35120689e-07
Iter: 1951 loss: 3.35056342e-07
Iter: 1952 loss: 3.34925e-07
Iter: 1953 loss: 3.36852963e-07
Iter: 1954 loss: 3.34910624e-07
Iter: 1955 loss: 3.34818964e-07
Iter: 1956 loss: 3.35986357e-07
Iter: 1957 loss: 3.34822516e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.8
+ date
Mon Oct 26 16:59:26 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.8/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.8_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -1 --phi 0.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi0.8_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4746f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4746b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe46fcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7030051950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7030051158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe46740d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4666730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4620840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe45f3048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe45f39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe45f31e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4591510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4591a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe455b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe455b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe44d1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe44b7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4478950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4464730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4443f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe44298c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe441b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe43da840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe43e2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4377620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe43882f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe43468c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe42ed2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4377730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe42dc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe42ab950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4297488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4297378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe42966a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe42500d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fe4215378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.37301787e-06
Iter: 2 loss: 3.50292703e-06
Iter: 3 loss: 3.32440345e-06
Iter: 4 loss: 2.98913938e-06
Iter: 5 loss: 2.74868648e-06
Iter: 6 loss: 2.63237916e-06
Iter: 7 loss: 2.35190259e-06
Iter: 8 loss: 5.3814133e-06
Iter: 9 loss: 2.34550862e-06
Iter: 10 loss: 2.10664257e-06
Iter: 11 loss: 2.23086818e-06
Iter: 12 loss: 1.94839276e-06
Iter: 13 loss: 1.85775241e-06
Iter: 14 loss: 2.27594319e-06
Iter: 15 loss: 1.84066903e-06
Iter: 16 loss: 1.80792199e-06
Iter: 17 loss: 1.80263601e-06
Iter: 18 loss: 1.77646086e-06
Iter: 19 loss: 1.70968224e-06
Iter: 20 loss: 2.28737917e-06
Iter: 21 loss: 1.69880491e-06
Iter: 22 loss: 1.68041743e-06
Iter: 23 loss: 1.66344523e-06
Iter: 24 loss: 1.63578488e-06
Iter: 25 loss: 1.60255854e-06
Iter: 26 loss: 1.59913941e-06
Iter: 27 loss: 1.55262546e-06
Iter: 28 loss: 1.5412038e-06
Iter: 29 loss: 1.51177596e-06
Iter: 30 loss: 1.45348042e-06
Iter: 31 loss: 1.70496492e-06
Iter: 32 loss: 1.4415067e-06
Iter: 33 loss: 1.40088844e-06
Iter: 34 loss: 1.61958155e-06
Iter: 35 loss: 1.39485076e-06
Iter: 36 loss: 1.37155223e-06
Iter: 37 loss: 1.37119855e-06
Iter: 38 loss: 1.34875745e-06
Iter: 39 loss: 1.33645187e-06
Iter: 40 loss: 1.32654941e-06
Iter: 41 loss: 1.30811873e-06
Iter: 42 loss: 1.44965986e-06
Iter: 43 loss: 1.30671515e-06
Iter: 44 loss: 1.28413308e-06
Iter: 45 loss: 1.26430723e-06
Iter: 46 loss: 1.25833981e-06
Iter: 47 loss: 1.24063445e-06
Iter: 48 loss: 1.27017631e-06
Iter: 49 loss: 1.23264817e-06
Iter: 50 loss: 1.21290304e-06
Iter: 51 loss: 1.47215542e-06
Iter: 52 loss: 1.21276412e-06
Iter: 53 loss: 1.196109e-06
Iter: 54 loss: 1.17255047e-06
Iter: 55 loss: 1.1716885e-06
Iter: 56 loss: 1.15851708e-06
Iter: 57 loss: 1.15787134e-06
Iter: 58 loss: 1.14464228e-06
Iter: 59 loss: 1.13369242e-06
Iter: 60 loss: 1.12985049e-06
Iter: 61 loss: 1.11207555e-06
Iter: 62 loss: 1.09990583e-06
Iter: 63 loss: 1.09341408e-06
Iter: 64 loss: 1.07427195e-06
Iter: 65 loss: 1.24985195e-06
Iter: 66 loss: 1.07346318e-06
Iter: 67 loss: 1.05694426e-06
Iter: 68 loss: 1.06666539e-06
Iter: 69 loss: 1.046276e-06
Iter: 70 loss: 1.02967829e-06
Iter: 71 loss: 1.08131621e-06
Iter: 72 loss: 1.02485535e-06
Iter: 73 loss: 1.01748446e-06
Iter: 74 loss: 1.01540832e-06
Iter: 75 loss: 1.00568434e-06
Iter: 76 loss: 9.86681584e-07
Iter: 77 loss: 1.37116047e-06
Iter: 78 loss: 9.86550162e-07
Iter: 79 loss: 9.68355153e-07
Iter: 80 loss: 1.04580658e-06
Iter: 81 loss: 9.64546416e-07
Iter: 82 loss: 9.49298055e-07
Iter: 83 loss: 1.12591647e-06
Iter: 84 loss: 9.49073524e-07
Iter: 85 loss: 9.37592233e-07
Iter: 86 loss: 9.09802679e-07
Iter: 87 loss: 1.20401148e-06
Iter: 88 loss: 9.0665867e-07
Iter: 89 loss: 9.10644246e-07
Iter: 90 loss: 8.96647464e-07
Iter: 91 loss: 8.88023e-07
Iter: 92 loss: 8.76134e-07
Iter: 93 loss: 8.75602893e-07
Iter: 94 loss: 8.6518537e-07
Iter: 95 loss: 9.032766e-07
Iter: 96 loss: 8.62576258e-07
Iter: 97 loss: 8.509287e-07
Iter: 98 loss: 9.38884625e-07
Iter: 99 loss: 8.50069057e-07
Iter: 100 loss: 8.45178818e-07
Iter: 101 loss: 8.36040101e-07
Iter: 102 loss: 1.03989828e-06
Iter: 103 loss: 8.36007416e-07
Iter: 104 loss: 8.241297e-07
Iter: 105 loss: 8.57351779e-07
Iter: 106 loss: 8.20324772e-07
Iter: 107 loss: 8.0983034e-07
Iter: 108 loss: 8.22753236e-07
Iter: 109 loss: 8.04370927e-07
Iter: 110 loss: 7.92791639e-07
Iter: 111 loss: 9.06640821e-07
Iter: 112 loss: 7.92379069e-07
Iter: 113 loss: 7.84690201e-07
Iter: 114 loss: 9.03510454e-07
Iter: 115 loss: 7.84660585e-07
Iter: 116 loss: 7.79693664e-07
Iter: 117 loss: 7.69597364e-07
Iter: 118 loss: 9.50757681e-07
Iter: 119 loss: 7.69430358e-07
Iter: 120 loss: 7.61140882e-07
Iter: 121 loss: 8.66232e-07
Iter: 122 loss: 7.61060846e-07
Iter: 123 loss: 7.52994538e-07
Iter: 124 loss: 7.6909248e-07
Iter: 125 loss: 7.49663457e-07
Iter: 126 loss: 7.44352633e-07
Iter: 127 loss: 7.3913418e-07
Iter: 128 loss: 7.38035226e-07
Iter: 129 loss: 7.30039858e-07
Iter: 130 loss: 7.29983071e-07
Iter: 131 loss: 7.25370228e-07
Iter: 132 loss: 7.19253535e-07
Iter: 133 loss: 7.18869728e-07
Iter: 134 loss: 7.12454664e-07
Iter: 135 loss: 7.66357061e-07
Iter: 136 loss: 7.12084955e-07
Iter: 137 loss: 7.04478964e-07
Iter: 138 loss: 7.03467776e-07
Iter: 139 loss: 6.98076633e-07
Iter: 140 loss: 6.92079084e-07
Iter: 141 loss: 6.90202114e-07
Iter: 142 loss: 6.86668784e-07
Iter: 143 loss: 6.79691482e-07
Iter: 144 loss: 7.0037089e-07
Iter: 145 loss: 6.775511e-07
Iter: 146 loss: 6.71158375e-07
Iter: 147 loss: 7.23210405e-07
Iter: 148 loss: 6.70764393e-07
Iter: 149 loss: 6.66602489e-07
Iter: 150 loss: 6.79788343e-07
Iter: 151 loss: 6.65415371e-07
Iter: 152 loss: 6.58905151e-07
Iter: 153 loss: 6.74812327e-07
Iter: 154 loss: 6.5660339e-07
Iter: 155 loss: 6.51507776e-07
Iter: 156 loss: 6.50374886e-07
Iter: 157 loss: 6.47021352e-07
Iter: 158 loss: 6.42226041e-07
Iter: 159 loss: 7.04423258e-07
Iter: 160 loss: 6.42195801e-07
Iter: 161 loss: 6.37274e-07
Iter: 162 loss: 6.35925403e-07
Iter: 163 loss: 6.32914123e-07
Iter: 164 loss: 6.2760671e-07
Iter: 165 loss: 6.39084078e-07
Iter: 166 loss: 6.25559665e-07
Iter: 167 loss: 6.22890354e-07
Iter: 168 loss: 6.22411164e-07
Iter: 169 loss: 6.20843878e-07
Iter: 170 loss: 6.16584884e-07
Iter: 171 loss: 6.43534349e-07
Iter: 172 loss: 6.15486272e-07
Iter: 173 loss: 6.13438374e-07
Iter: 174 loss: 6.12564122e-07
Iter: 175 loss: 6.10106156e-07
Iter: 176 loss: 6.06082381e-07
Iter: 177 loss: 6.06042818e-07
Iter: 178 loss: 6.01094882e-07
Iter: 179 loss: 6.01928605e-07
Iter: 180 loss: 5.97355e-07
Iter: 181 loss: 5.91453e-07
Iter: 182 loss: 5.99599957e-07
Iter: 183 loss: 5.88515945e-07
Iter: 184 loss: 5.82107134e-07
Iter: 185 loss: 6.04455067e-07
Iter: 186 loss: 5.80386541e-07
Iter: 187 loss: 5.76159e-07
Iter: 188 loss: 6.38929464e-07
Iter: 189 loss: 5.76165e-07
Iter: 190 loss: 5.73347393e-07
Iter: 191 loss: 5.7843863e-07
Iter: 192 loss: 5.72150668e-07
Iter: 193 loss: 5.68636381e-07
Iter: 194 loss: 6.04214961e-07
Iter: 195 loss: 5.68513769e-07
Iter: 196 loss: 5.66492531e-07
Iter: 197 loss: 5.65218556e-07
Iter: 198 loss: 5.64408367e-07
Iter: 199 loss: 5.61730644e-07
Iter: 200 loss: 5.93681932e-07
Iter: 201 loss: 5.6168426e-07
Iter: 202 loss: 5.59434511e-07
Iter: 203 loss: 5.54329404e-07
Iter: 204 loss: 6.24259769e-07
Iter: 205 loss: 5.54042231e-07
Iter: 206 loss: 5.4937459e-07
Iter: 207 loss: 5.92658466e-07
Iter: 208 loss: 5.49196557e-07
Iter: 209 loss: 5.47127968e-07
Iter: 210 loss: 5.47025479e-07
Iter: 211 loss: 5.45833132e-07
Iter: 212 loss: 5.42429802e-07
Iter: 213 loss: 5.6072156e-07
Iter: 214 loss: 5.41388e-07
Iter: 215 loss: 5.4155953e-07
Iter: 216 loss: 5.39810628e-07
Iter: 217 loss: 5.38388576e-07
Iter: 218 loss: 5.37536948e-07
Iter: 219 loss: 5.36943844e-07
Iter: 220 loss: 5.35068125e-07
Iter: 221 loss: 5.32525462e-07
Iter: 222 loss: 5.32392733e-07
Iter: 223 loss: 5.28923067e-07
Iter: 224 loss: 5.40262477e-07
Iter: 225 loss: 5.27898237e-07
Iter: 226 loss: 5.24275038e-07
Iter: 227 loss: 5.24972052e-07
Iter: 228 loss: 5.21537459e-07
Iter: 229 loss: 5.24321081e-07
Iter: 230 loss: 5.20024855e-07
Iter: 231 loss: 5.18998888e-07
Iter: 232 loss: 5.17653632e-07
Iter: 233 loss: 5.17597925e-07
Iter: 234 loss: 5.15785132e-07
Iter: 235 loss: 5.16258297e-07
Iter: 236 loss: 5.14502119e-07
Iter: 237 loss: 5.12751342e-07
Iter: 238 loss: 5.12742247e-07
Iter: 239 loss: 5.10944346e-07
Iter: 240 loss: 5.08658673e-07
Iter: 241 loss: 5.08488483e-07
Iter: 242 loss: 5.05953722e-07
Iter: 243 loss: 5.07665561e-07
Iter: 244 loss: 5.04391096e-07
Iter: 245 loss: 5.02381397e-07
Iter: 246 loss: 5.02258672e-07
Iter: 247 loss: 5.00368856e-07
Iter: 248 loss: 4.98792929e-07
Iter: 249 loss: 4.98230179e-07
Iter: 250 loss: 4.95927338e-07
Iter: 251 loss: 5.04474883e-07
Iter: 252 loss: 4.953705e-07
Iter: 253 loss: 4.93120638e-07
Iter: 254 loss: 5.16262503e-07
Iter: 255 loss: 4.93054813e-07
Iter: 256 loss: 4.92288e-07
Iter: 257 loss: 4.90736568e-07
Iter: 258 loss: 5.16571617e-07
Iter: 259 loss: 4.90698085e-07
Iter: 260 loss: 4.88684464e-07
Iter: 261 loss: 4.90286197e-07
Iter: 262 loss: 4.87509226e-07
Iter: 263 loss: 4.86842e-07
Iter: 264 loss: 4.86393503e-07
Iter: 265 loss: 4.8526266e-07
Iter: 266 loss: 4.85301484e-07
Iter: 267 loss: 4.84343957e-07
Iter: 268 loss: 4.82702887e-07
Iter: 269 loss: 4.79345204e-07
Iter: 270 loss: 5.39844109e-07
Iter: 271 loss: 4.79285234e-07
Iter: 272 loss: 4.79560299e-07
Iter: 273 loss: 4.7785818e-07
Iter: 274 loss: 4.76748738e-07
Iter: 275 loss: 4.75217803e-07
Iter: 276 loss: 4.7516528e-07
Iter: 277 loss: 4.73227203e-07
Iter: 278 loss: 4.77014908e-07
Iter: 279 loss: 4.72435261e-07
Iter: 280 loss: 4.71064283e-07
Iter: 281 loss: 4.77731191e-07
Iter: 282 loss: 4.70828468e-07
Iter: 283 loss: 4.69367876e-07
Iter: 284 loss: 4.80926e-07
Iter: 285 loss: 4.69246402e-07
Iter: 286 loss: 4.68542396e-07
Iter: 287 loss: 4.67375571e-07
Iter: 288 loss: 4.67384041e-07
Iter: 289 loss: 4.66052086e-07
Iter: 290 loss: 4.66048277e-07
Iter: 291 loss: 4.65117267e-07
Iter: 292 loss: 4.62841115e-07
Iter: 293 loss: 4.83300425e-07
Iter: 294 loss: 4.62481069e-07
Iter: 295 loss: 4.60086028e-07
Iter: 296 loss: 4.79111122e-07
Iter: 297 loss: 4.59910638e-07
Iter: 298 loss: 4.58911529e-07
Iter: 299 loss: 4.58792613e-07
Iter: 300 loss: 4.57896562e-07
Iter: 301 loss: 4.56202145e-07
Iter: 302 loss: 4.93112054e-07
Iter: 303 loss: 4.5620078e-07
Iter: 304 loss: 4.54811584e-07
Iter: 305 loss: 4.62367922e-07
Iter: 306 loss: 4.54594925e-07
Iter: 307 loss: 4.53925566e-07
Iter: 308 loss: 4.5387344e-07
Iter: 309 loss: 4.53460245e-07
Iter: 310 loss: 4.52741745e-07
Iter: 311 loss: 4.52751436e-07
Iter: 312 loss: 4.5170006e-07
Iter: 313 loss: 4.51031383e-07
Iter: 314 loss: 4.50617847e-07
Iter: 315 loss: 4.49692038e-07
Iter: 316 loss: 4.4961223e-07
Iter: 317 loss: 4.48503584e-07
Iter: 318 loss: 4.47456472e-07
Iter: 319 loss: 4.47231372e-07
Iter: 320 loss: 4.4536759e-07
Iter: 321 loss: 4.48395696e-07
Iter: 322 loss: 4.44511613e-07
Iter: 323 loss: 4.43114345e-07
Iter: 324 loss: 4.43077028e-07
Iter: 325 loss: 4.42339569e-07
Iter: 326 loss: 4.40181537e-07
Iter: 327 loss: 4.49972958e-07
Iter: 328 loss: 4.39423e-07
Iter: 329 loss: 4.38468135e-07
Iter: 330 loss: 4.38246417e-07
Iter: 331 loss: 4.37528115e-07
Iter: 332 loss: 4.48272147e-07
Iter: 333 loss: 4.37530389e-07
Iter: 334 loss: 4.3713851e-07
Iter: 335 loss: 4.36095632e-07
Iter: 336 loss: 4.43593876e-07
Iter: 337 loss: 4.3587761e-07
Iter: 338 loss: 4.34650644e-07
Iter: 339 loss: 4.46962133e-07
Iter: 340 loss: 4.34616965e-07
Iter: 341 loss: 4.33596625e-07
Iter: 342 loss: 4.41264177e-07
Iter: 343 loss: 4.33540208e-07
Iter: 344 loss: 4.33008211e-07
Iter: 345 loss: 4.31975053e-07
Iter: 346 loss: 4.53186885e-07
Iter: 347 loss: 4.31967521e-07
Iter: 348 loss: 4.30624311e-07
Iter: 349 loss: 4.33413703e-07
Iter: 350 loss: 4.30099e-07
Iter: 351 loss: 4.29088487e-07
Iter: 352 loss: 4.29034174e-07
Iter: 353 loss: 4.28331532e-07
Iter: 354 loss: 4.28127521e-07
Iter: 355 loss: 4.27684142e-07
Iter: 356 loss: 4.26854371e-07
Iter: 357 loss: 4.29473744e-07
Iter: 358 loss: 4.26620829e-07
Iter: 359 loss: 4.2552108e-07
Iter: 360 loss: 4.28728868e-07
Iter: 361 loss: 4.25181e-07
Iter: 362 loss: 4.24659277e-07
Iter: 363 loss: 4.23549466e-07
Iter: 364 loss: 4.40629265e-07
Iter: 365 loss: 4.23545089e-07
Iter: 366 loss: 4.22533958e-07
Iter: 367 loss: 4.2253555e-07
Iter: 368 loss: 4.21566483e-07
Iter: 369 loss: 4.24561563e-07
Iter: 370 loss: 4.2128579e-07
Iter: 371 loss: 4.20644085e-07
Iter: 372 loss: 4.19415187e-07
Iter: 373 loss: 4.45395926e-07
Iter: 374 loss: 4.19414675e-07
Iter: 375 loss: 4.18626115e-07
Iter: 376 loss: 4.18546904e-07
Iter: 377 loss: 4.17734782e-07
Iter: 378 loss: 4.17217251e-07
Iter: 379 loss: 4.16903276e-07
Iter: 380 loss: 4.16008021e-07
Iter: 381 loss: 4.15347955e-07
Iter: 382 loss: 4.15026534e-07
Iter: 383 loss: 4.14078073e-07
Iter: 384 loss: 4.14065909e-07
Iter: 385 loss: 4.13361192e-07
Iter: 386 loss: 4.19034819e-07
Iter: 387 loss: 4.13312108e-07
Iter: 388 loss: 4.12819759e-07
Iter: 389 loss: 4.11914698e-07
Iter: 390 loss: 4.33627804e-07
Iter: 391 loss: 4.11897076e-07
Iter: 392 loss: 4.11395717e-07
Iter: 393 loss: 4.11294963e-07
Iter: 394 loss: 4.10823304e-07
Iter: 395 loss: 4.10098e-07
Iter: 396 loss: 4.10081753e-07
Iter: 397 loss: 4.09334433e-07
Iter: 398 loss: 4.09956527e-07
Iter: 399 loss: 4.08904384e-07
Iter: 400 loss: 4.08406663e-07
Iter: 401 loss: 4.08361529e-07
Iter: 402 loss: 4.07895016e-07
Iter: 403 loss: 4.07334426e-07
Iter: 404 loss: 4.07270022e-07
Iter: 405 loss: 4.06539186e-07
Iter: 406 loss: 4.05930649e-07
Iter: 407 loss: 4.05715213e-07
Iter: 408 loss: 4.04795657e-07
Iter: 409 loss: 4.04743446e-07
Iter: 410 loss: 4.04091111e-07
Iter: 411 loss: 4.02932869e-07
Iter: 412 loss: 4.02934347e-07
Iter: 413 loss: 4.01982675e-07
Iter: 414 loss: 4.04868501e-07
Iter: 415 loss: 4.0171517e-07
Iter: 416 loss: 4.00886734e-07
Iter: 417 loss: 4.09149067e-07
Iter: 418 loss: 4.00870931e-07
Iter: 419 loss: 4.00115624e-07
Iter: 420 loss: 4.02351475e-07
Iter: 421 loss: 3.9987782e-07
Iter: 422 loss: 3.99522946e-07
Iter: 423 loss: 3.99943758e-07
Iter: 424 loss: 3.99296169e-07
Iter: 425 loss: 3.9885083e-07
Iter: 426 loss: 4.02488212e-07
Iter: 427 loss: 3.98806264e-07
Iter: 428 loss: 3.98491522e-07
Iter: 429 loss: 3.98128151e-07
Iter: 430 loss: 3.98098e-07
Iter: 431 loss: 3.97497161e-07
Iter: 432 loss: 3.97348458e-07
Iter: 433 loss: 3.96980624e-07
Iter: 434 loss: 3.96200392e-07
Iter: 435 loss: 3.96192576e-07
Iter: 436 loss: 3.95778386e-07
Iter: 437 loss: 3.94818244e-07
Iter: 438 loss: 4.07975847e-07
Iter: 439 loss: 3.94775356e-07
Iter: 440 loss: 3.93941065e-07
Iter: 441 loss: 4.02462035e-07
Iter: 442 loss: 3.93924324e-07
Iter: 443 loss: 3.9307605e-07
Iter: 444 loss: 3.96253427e-07
Iter: 445 loss: 3.92877837e-07
Iter: 446 loss: 3.92362892e-07
Iter: 447 loss: 3.91907918e-07
Iter: 448 loss: 3.91808101e-07
Iter: 449 loss: 3.90980688e-07
Iter: 450 loss: 3.9259649e-07
Iter: 451 loss: 3.90658272e-07
Iter: 452 loss: 3.90049252e-07
Iter: 453 loss: 3.90015202e-07
Iter: 454 loss: 3.89625399e-07
Iter: 455 loss: 3.89011063e-07
Iter: 456 loss: 3.89012428e-07
Iter: 457 loss: 3.88456897e-07
Iter: 458 loss: 3.88454055e-07
Iter: 459 loss: 3.87912621e-07
Iter: 460 loss: 3.87370875e-07
Iter: 461 loss: 3.87272024e-07
Iter: 462 loss: 3.86439751e-07
Iter: 463 loss: 3.87433488e-07
Iter: 464 loss: 3.85994156e-07
Iter: 465 loss: 3.85814047e-07
Iter: 466 loss: 3.8570289e-07
Iter: 467 loss: 3.85399289e-07
Iter: 468 loss: 3.84800273e-07
Iter: 469 loss: 3.96274174e-07
Iter: 470 loss: 3.84797801e-07
Iter: 471 loss: 3.84142879e-07
Iter: 472 loss: 3.8534057e-07
Iter: 473 loss: 3.8387077e-07
Iter: 474 loss: 3.83623217e-07
Iter: 475 loss: 3.83542272e-07
Iter: 476 loss: 3.8327596e-07
Iter: 477 loss: 3.82666599e-07
Iter: 478 loss: 3.91263768e-07
Iter: 479 loss: 3.82641872e-07
Iter: 480 loss: 3.82050189e-07
Iter: 481 loss: 3.8429917e-07
Iter: 482 loss: 3.81908535e-07
Iter: 483 loss: 3.81627558e-07
Iter: 484 loss: 3.81626535e-07
Iter: 485 loss: 3.81253557e-07
Iter: 486 loss: 3.80459312e-07
Iter: 487 loss: 3.9187654e-07
Iter: 488 loss: 3.80429128e-07
Iter: 489 loss: 3.7973723e-07
Iter: 490 loss: 3.89043635e-07
Iter: 491 loss: 3.79738793e-07
Iter: 492 loss: 3.79203072e-07
Iter: 493 loss: 3.81876816e-07
Iter: 494 loss: 3.79110958e-07
Iter: 495 loss: 3.78759182e-07
Iter: 496 loss: 3.78110258e-07
Iter: 497 loss: 3.94181257e-07
Iter: 498 loss: 3.7811148e-07
Iter: 499 loss: 3.77352308e-07
Iter: 500 loss: 3.82319e-07
Iter: 501 loss: 3.77250984e-07
Iter: 502 loss: 3.76515572e-07
Iter: 503 loss: 3.81907512e-07
Iter: 504 loss: 3.76441449e-07
Iter: 505 loss: 3.76119857e-07
Iter: 506 loss: 3.75953221e-07
Iter: 507 loss: 3.75800028e-07
Iter: 508 loss: 3.75423639e-07
Iter: 509 loss: 3.76899749e-07
Iter: 510 loss: 3.75339653e-07
Iter: 511 loss: 3.74823799e-07
Iter: 512 loss: 3.76225813e-07
Iter: 513 loss: 3.74658413e-07
Iter: 514 loss: 3.74319058e-07
Iter: 515 loss: 3.73860701e-07
Iter: 516 loss: 3.7383e-07
Iter: 517 loss: 3.7333902e-07
Iter: 518 loss: 3.7711564e-07
Iter: 519 loss: 3.73335695e-07
Iter: 520 loss: 3.72853435e-07
Iter: 521 loss: 3.75973912e-07
Iter: 522 loss: 3.72784967e-07
Iter: 523 loss: 3.72466957e-07
Iter: 524 loss: 3.72002205e-07
Iter: 525 loss: 3.7199473e-07
Iter: 526 loss: 3.7134555e-07
Iter: 527 loss: 3.78339109e-07
Iter: 528 loss: 3.71332817e-07
Iter: 529 loss: 3.70874176e-07
Iter: 530 loss: 3.70585411e-07
Iter: 531 loss: 3.70394332e-07
Iter: 532 loss: 3.69956979e-07
Iter: 533 loss: 3.7116672e-07
Iter: 534 loss: 3.69814984e-07
Iter: 535 loss: 3.69450277e-07
Iter: 536 loss: 3.69448856e-07
Iter: 537 loss: 3.69143322e-07
Iter: 538 loss: 3.6863787e-07
Iter: 539 loss: 3.68635796e-07
Iter: 540 loss: 3.68160272e-07
Iter: 541 loss: 3.69337954e-07
Iter: 542 loss: 3.6796115e-07
Iter: 543 loss: 3.67515554e-07
Iter: 544 loss: 3.73345074e-07
Iter: 545 loss: 3.67518425e-07
Iter: 546 loss: 3.67063365e-07
Iter: 547 loss: 3.66181695e-07
Iter: 548 loss: 3.82729951e-07
Iter: 549 loss: 3.66154921e-07
Iter: 550 loss: 3.65556332e-07
Iter: 551 loss: 3.68717139e-07
Iter: 552 loss: 3.6543949e-07
Iter: 553 loss: 3.65203846e-07
Iter: 554 loss: 3.65146434e-07
Iter: 555 loss: 3.64900131e-07
Iter: 556 loss: 3.64415143e-07
Iter: 557 loss: 3.73957391e-07
Iter: 558 loss: 3.64408294e-07
Iter: 559 loss: 3.64153635e-07
Iter: 560 loss: 3.64120325e-07
Iter: 561 loss: 3.63918673e-07
Iter: 562 loss: 3.63949312e-07
Iter: 563 loss: 3.63755589e-07
Iter: 564 loss: 3.63481234e-07
Iter: 565 loss: 3.62963902e-07
Iter: 566 loss: 3.75210192e-07
Iter: 567 loss: 3.62954694e-07
Iter: 568 loss: 3.62616817e-07
Iter: 569 loss: 3.62564037e-07
Iter: 570 loss: 3.6220132e-07
Iter: 571 loss: 3.62033632e-07
Iter: 572 loss: 3.61863641e-07
Iter: 573 loss: 3.61331274e-07
Iter: 574 loss: 3.60715802e-07
Iter: 575 loss: 3.60629713e-07
Iter: 576 loss: 3.60000115e-07
Iter: 577 loss: 3.59995113e-07
Iter: 578 loss: 3.59468231e-07
Iter: 579 loss: 3.60917369e-07
Iter: 580 loss: 3.59292756e-07
Iter: 581 loss: 3.58985289e-07
Iter: 582 loss: 3.58428565e-07
Iter: 583 loss: 3.70471241e-07
Iter: 584 loss: 3.58425552e-07
Iter: 585 loss: 3.58248116e-07
Iter: 586 loss: 3.58152562e-07
Iter: 587 loss: 3.57878548e-07
Iter: 588 loss: 3.58385705e-07
Iter: 589 loss: 3.57773445e-07
Iter: 590 loss: 3.57555109e-07
Iter: 591 loss: 3.57370766e-07
Iter: 592 loss: 3.57310967e-07
Iter: 593 loss: 3.56833056e-07
Iter: 594 loss: 3.59345904e-07
Iter: 595 loss: 3.56776582e-07
Iter: 596 loss: 3.56453597e-07
Iter: 597 loss: 3.56172734e-07
Iter: 598 loss: 3.56108728e-07
Iter: 599 loss: 3.55707812e-07
Iter: 600 loss: 3.58164129e-07
Iter: 601 loss: 3.55669556e-07
Iter: 602 loss: 3.55238683e-07
Iter: 603 loss: 3.57349279e-07
Iter: 604 loss: 3.55166264e-07
Iter: 605 loss: 3.54930648e-07
Iter: 606 loss: 3.54658312e-07
Iter: 607 loss: 3.54617242e-07
Iter: 608 loss: 3.54267513e-07
Iter: 609 loss: 3.56134194e-07
Iter: 610 loss: 3.5421732e-07
Iter: 611 loss: 3.53963372e-07
Iter: 612 loss: 3.57631194e-07
Iter: 613 loss: 3.5395712e-07
Iter: 614 loss: 3.53790057e-07
Iter: 615 loss: 3.53340226e-07
Iter: 616 loss: 3.56869833e-07
Iter: 617 loss: 3.53263943e-07
Iter: 618 loss: 3.52723418e-07
Iter: 619 loss: 3.55255452e-07
Iter: 620 loss: 3.5262633e-07
Iter: 621 loss: 3.52203017e-07
Iter: 622 loss: 3.52199777e-07
Iter: 623 loss: 3.51965753e-07
Iter: 624 loss: 3.51613778e-07
Iter: 625 loss: 3.51582855e-07
Iter: 626 loss: 3.51267374e-07
Iter: 627 loss: 3.5127556e-07
Iter: 628 loss: 3.50971362e-07
Iter: 629 loss: 3.50435585e-07
Iter: 630 loss: 3.6314043e-07
Iter: 631 loss: 3.50436949e-07
Iter: 632 loss: 3.50078977e-07
Iter: 633 loss: 3.52851714e-07
Iter: 634 loss: 3.50038846e-07
Iter: 635 loss: 3.49826848e-07
Iter: 636 loss: 3.49811359e-07
Iter: 637 loss: 3.49660127e-07
Iter: 638 loss: 3.49351296e-07
Iter: 639 loss: 3.54901715e-07
Iter: 640 loss: 3.49360477e-07
Iter: 641 loss: 3.490415e-07
Iter: 642 loss: 3.50473101e-07
Iter: 643 loss: 3.48957656e-07
Iter: 644 loss: 3.4876976e-07
Iter: 645 loss: 3.5129e-07
Iter: 646 loss: 3.48749e-07
Iter: 647 loss: 3.48521979e-07
Iter: 648 loss: 3.48108415e-07
Iter: 649 loss: 3.57791805e-07
Iter: 650 loss: 3.48109836e-07
Iter: 651 loss: 3.47715542e-07
Iter: 652 loss: 3.48522093e-07
Iter: 653 loss: 3.47545e-07
Iter: 654 loss: 3.47370019e-07
Iter: 655 loss: 3.47336027e-07
Iter: 656 loss: 3.47137927e-07
Iter: 657 loss: 3.46853909e-07
Iter: 658 loss: 3.46845e-07
Iter: 659 loss: 3.46509296e-07
Iter: 660 loss: 3.48150081e-07
Iter: 661 loss: 3.46455408e-07
Iter: 662 loss: 3.46041105e-07
Iter: 663 loss: 3.46706116e-07
Iter: 664 loss: 3.45858183e-07
Iter: 665 loss: 3.45537103e-07
Iter: 666 loss: 3.45199e-07
Iter: 667 loss: 3.45135277e-07
Iter: 668 loss: 3.44856431e-07
Iter: 669 loss: 3.44814339e-07
Iter: 670 loss: 3.44500933e-07
Iter: 671 loss: 3.44190653e-07
Iter: 672 loss: 3.44132957e-07
Iter: 673 loss: 3.43706574e-07
Iter: 674 loss: 3.44019185e-07
Iter: 675 loss: 3.43456179e-07
Iter: 676 loss: 3.43089027e-07
Iter: 677 loss: 3.48019427e-07
Iter: 678 loss: 3.43099202e-07
Iter: 679 loss: 3.42821295e-07
Iter: 680 loss: 3.44601517e-07
Iter: 681 loss: 3.42782016e-07
Iter: 682 loss: 3.42640192e-07
Iter: 683 loss: 3.42283101e-07
Iter: 684 loss: 3.45323372e-07
Iter: 685 loss: 3.42203293e-07
Iter: 686 loss: 3.41869452e-07
Iter: 687 loss: 3.4186229e-07
Iter: 688 loss: 3.41578897e-07
Iter: 689 loss: 3.43091187e-07
Iter: 690 loss: 3.41539334e-07
Iter: 691 loss: 3.41378041e-07
Iter: 692 loss: 3.41241474e-07
Iter: 693 loss: 3.41183323e-07
Iter: 694 loss: 3.40917722e-07
Iter: 695 loss: 3.43950092e-07
Iter: 696 loss: 3.40911811e-07
Iter: 697 loss: 3.4073031e-07
Iter: 698 loss: 3.40419433e-07
Iter: 699 loss: 3.40422815e-07
Iter: 700 loss: 3.40095511e-07
Iter: 701 loss: 3.42458577e-07
Iter: 702 loss: 3.40073683e-07
Iter: 703 loss: 3.3971935e-07
Iter: 704 loss: 3.40892086e-07
Iter: 705 loss: 3.39607624e-07
Iter: 706 loss: 3.39410349e-07
Iter: 707 loss: 3.39313772e-07
Iter: 708 loss: 3.3918883e-07
Iter: 709 loss: 3.38917943e-07
Iter: 710 loss: 3.3985981e-07
Iter: 711 loss: 3.38844814e-07
Iter: 712 loss: 3.3855369e-07
Iter: 713 loss: 3.40767258e-07
Iter: 714 loss: 3.3852757e-07
Iter: 715 loss: 3.38264243e-07
Iter: 716 loss: 3.37905647e-07
Iter: 717 loss: 3.37874894e-07
Iter: 718 loss: 3.37543554e-07
Iter: 719 loss: 3.38661749e-07
Iter: 720 loss: 3.3745647e-07
Iter: 721 loss: 3.37147668e-07
Iter: 722 loss: 3.41979614e-07
Iter: 723 loss: 3.37154574e-07
Iter: 724 loss: 3.36951075e-07
Iter: 725 loss: 3.36622236e-07
Iter: 726 loss: 3.36629739e-07
Iter: 727 loss: 3.36432151e-07
Iter: 728 loss: 3.3640805e-07
Iter: 729 loss: 3.3626219e-07
Iter: 730 loss: 3.36010459e-07
Iter: 731 loss: 3.3601961e-07
Iter: 732 loss: 3.3572681e-07
Iter: 733 loss: 3.36227913e-07
Iter: 734 loss: 3.35613549e-07
Iter: 735 loss: 3.35445208e-07
Iter: 736 loss: 3.35396123e-07
Iter: 737 loss: 3.35317736e-07
Iter: 738 loss: 3.35079335e-07
Iter: 739 loss: 3.37836923e-07
Iter: 740 loss: 3.35078937e-07
Iter: 741 loss: 3.34794663e-07
Iter: 742 loss: 3.35407947e-07
Iter: 743 loss: 3.34671881e-07
Iter: 744 loss: 3.34528067e-07
Iter: 745 loss: 3.34494729e-07
Iter: 746 loss: 3.34365723e-07
Iter: 747 loss: 3.34198717e-07
Iter: 748 loss: 3.34188655e-07
Iter: 749 loss: 3.33915182e-07
Iter: 750 loss: 3.33639235e-07
Iter: 751 loss: 3.33611979e-07
Iter: 752 loss: 3.33461571e-07
Iter: 753 loss: 3.33368519e-07
Iter: 754 loss: 3.33176729e-07
Iter: 755 loss: 3.32943557e-07
Iter: 756 loss: 3.32917637e-07
Iter: 757 loss: 3.32680372e-07
Iter: 758 loss: 3.35274251e-07
Iter: 759 loss: 3.32678212e-07
Iter: 760 loss: 3.32397605e-07
Iter: 761 loss: 3.32189103e-07
Iter: 762 loss: 3.32111568e-07
Iter: 763 loss: 3.31799072e-07
Iter: 764 loss: 3.32554322e-07
Iter: 765 loss: 3.31698857e-07
Iter: 766 loss: 3.31577098e-07
Iter: 767 loss: 3.31536597e-07
Iter: 768 loss: 3.3142004e-07
Iter: 769 loss: 3.31167087e-07
Iter: 770 loss: 3.35373045e-07
Iter: 771 loss: 3.31154126e-07
Iter: 772 loss: 3.3094534e-07
Iter: 773 loss: 3.31762465e-07
Iter: 774 loss: 3.30889804e-07
Iter: 775 loss: 3.30731439e-07
Iter: 776 loss: 3.32754695e-07
Iter: 777 loss: 3.30722173e-07
Iter: 778 loss: 3.30544708e-07
Iter: 779 loss: 3.30507248e-07
Iter: 780 loss: 3.30392709e-07
Iter: 781 loss: 3.30150755e-07
Iter: 782 loss: 3.30273053e-07
Iter: 783 loss: 3.29986733e-07
Iter: 784 loss: 3.29793977e-07
Iter: 785 loss: 3.31654576e-07
Iter: 786 loss: 3.29787781e-07
Iter: 787 loss: 3.29553018e-07
Iter: 788 loss: 3.29653574e-07
Iter: 789 loss: 3.29396215e-07
Iter: 790 loss: 3.2918598e-07
Iter: 791 loss: 3.30238038e-07
Iter: 792 loss: 3.29144598e-07
Iter: 793 loss: 3.28905656e-07
Iter: 794 loss: 3.29360063e-07
Iter: 795 loss: 3.28815304e-07
Iter: 796 loss: 3.28620246e-07
Iter: 797 loss: 3.2848439e-07
Iter: 798 loss: 3.28415808e-07
Iter: 799 loss: 3.28324376e-07
Iter: 800 loss: 3.2829962e-07
Iter: 801 loss: 3.28145575e-07
Iter: 802 loss: 3.28052153e-07
Iter: 803 loss: 3.27995792e-07
Iter: 804 loss: 3.27812501e-07
Iter: 805 loss: 3.27537236e-07
Iter: 806 loss: 3.27521462e-07
Iter: 807 loss: 3.27226701e-07
Iter: 808 loss: 3.31060562e-07
Iter: 809 loss: 3.27220363e-07
Iter: 810 loss: 3.26974657e-07
Iter: 811 loss: 3.28517956e-07
Iter: 812 loss: 3.26933048e-07
Iter: 813 loss: 3.26769737e-07
Iter: 814 loss: 3.2667117e-07
Iter: 815 loss: 3.2659085e-07
Iter: 816 loss: 3.2636089e-07
Iter: 817 loss: 3.26804582e-07
Iter: 818 loss: 3.26261585e-07
Iter: 819 loss: 3.26037537e-07
Iter: 820 loss: 3.26043676e-07
Iter: 821 loss: 3.2594761e-07
Iter: 822 loss: 3.25849641e-07
Iter: 823 loss: 3.25826079e-07
Iter: 824 loss: 3.25640315e-07
Iter: 825 loss: 3.26766667e-07
Iter: 826 loss: 3.25605924e-07
Iter: 827 loss: 3.25461741e-07
Iter: 828 loss: 3.25288624e-07
Iter: 829 loss: 3.25286805e-07
Iter: 830 loss: 3.25122613e-07
Iter: 831 loss: 3.2719251e-07
Iter: 832 loss: 3.25112694e-07
Iter: 833 loss: 3.24943755e-07
Iter: 834 loss: 3.25072051e-07
Iter: 835 loss: 3.24821826e-07
Iter: 836 loss: 3.24617929e-07
Iter: 837 loss: 3.24366709e-07
Iter: 838 loss: 3.2433627e-07
Iter: 839 loss: 3.24072857e-07
Iter: 840 loss: 3.26680436e-07
Iter: 841 loss: 3.24071124e-07
Iter: 842 loss: 3.23854636e-07
Iter: 843 loss: 3.25202762e-07
Iter: 844 loss: 3.23832239e-07
Iter: 845 loss: 3.23651534e-07
Iter: 846 loss: 3.23478218e-07
Iter: 847 loss: 3.23416089e-07
Iter: 848 loss: 3.23152477e-07
Iter: 849 loss: 3.24091332e-07
Iter: 850 loss: 3.23069372e-07
Iter: 851 loss: 3.22929111e-07
Iter: 852 loss: 3.22920727e-07
Iter: 853 loss: 3.22803e-07
Iter: 854 loss: 3.22576199e-07
Iter: 855 loss: 3.27649502e-07
Iter: 856 loss: 3.22581968e-07
Iter: 857 loss: 3.22406294e-07
Iter: 858 loss: 3.22385858e-07
Iter: 859 loss: 3.22297183e-07
Iter: 860 loss: 3.22153085e-07
Iter: 861 loss: 3.22126482e-07
Iter: 862 loss: 3.21965899e-07
Iter: 863 loss: 3.22321398e-07
Iter: 864 loss: 3.21894731e-07
Iter: 865 loss: 3.21704476e-07
Iter: 866 loss: 3.23750783e-07
Iter: 867 loss: 3.21704476e-07
Iter: 868 loss: 3.21584395e-07
Iter: 869 loss: 3.21478e-07
Iter: 870 loss: 3.21456923e-07
Iter: 871 loss: 3.21309471e-07
Iter: 872 loss: 3.2143538e-07
Iter: 873 loss: 3.21225514e-07
Iter: 874 loss: 3.21015108e-07
Iter: 875 loss: 3.22881078e-07
Iter: 876 loss: 3.20983474e-07
Iter: 877 loss: 3.20789923e-07
Iter: 878 loss: 3.20662537e-07
Iter: 879 loss: 3.20577e-07
Iter: 880 loss: 3.20327985e-07
Iter: 881 loss: 3.20757351e-07
Iter: 882 loss: 3.20221119e-07
Iter: 883 loss: 3.19967313e-07
Iter: 884 loss: 3.22672236e-07
Iter: 885 loss: 3.19980472e-07
Iter: 886 loss: 3.19748722e-07
Iter: 887 loss: 3.19616106e-07
Iter: 888 loss: 3.19513333e-07
Iter: 889 loss: 3.19374e-07
Iter: 890 loss: 3.19357866e-07
Iter: 891 loss: 3.19237444e-07
Iter: 892 loss: 3.19067738e-07
Iter: 893 loss: 3.19058586e-07
Iter: 894 loss: 3.18841359e-07
Iter: 895 loss: 3.19035053e-07
Iter: 896 loss: 3.18708544e-07
Iter: 897 loss: 3.18560751e-07
Iter: 898 loss: 3.18545943e-07
Iter: 899 loss: 3.18419723e-07
Iter: 900 loss: 3.18177854e-07
Iter: 901 loss: 3.22287178e-07
Iter: 902 loss: 3.18168816e-07
Iter: 903 loss: 3.17925696e-07
Iter: 904 loss: 3.18140565e-07
Iter: 905 loss: 3.17777307e-07
Iter: 906 loss: 3.17583755e-07
Iter: 907 loss: 3.17554537e-07
Iter: 908 loss: 3.1739367e-07
Iter: 909 loss: 3.17517276e-07
Iter: 910 loss: 3.17290016e-07
Iter: 911 loss: 3.17149102e-07
Iter: 912 loss: 3.17146657e-07
Iter: 913 loss: 3.17018078e-07
Iter: 914 loss: 3.16859314e-07
Iter: 915 loss: 3.19229741e-07
Iter: 916 loss: 3.1686767e-07
Iter: 917 loss: 3.16734372e-07
Iter: 918 loss: 3.17055651e-07
Iter: 919 loss: 3.1669822e-07
Iter: 920 loss: 3.16589251e-07
Iter: 921 loss: 3.16681223e-07
Iter: 922 loss: 3.16534681e-07
Iter: 923 loss: 3.16352896e-07
Iter: 924 loss: 3.16402236e-07
Iter: 925 loss: 3.16227784e-07
Iter: 926 loss: 3.16027865e-07
Iter: 927 loss: 3.1600149e-07
Iter: 928 loss: 3.15851253e-07
Iter: 929 loss: 3.15688538e-07
Iter: 930 loss: 3.15696e-07
Iter: 931 loss: 3.15518918e-07
Iter: 932 loss: 3.15197e-07
Iter: 933 loss: 3.21522577e-07
Iter: 934 loss: 3.15186298e-07
Iter: 935 loss: 3.1486411e-07
Iter: 936 loss: 3.1579242e-07
Iter: 937 loss: 3.14769977e-07
Iter: 938 loss: 3.14606893e-07
Iter: 939 loss: 3.14614738e-07
Iter: 940 loss: 3.14456173e-07
Iter: 941 loss: 3.14740532e-07
Iter: 942 loss: 3.14386654e-07
Iter: 943 loss: 3.14246591e-07
Iter: 944 loss: 3.14210666e-07
Iter: 945 loss: 3.14150554e-07
Iter: 946 loss: 3.13961181e-07
Iter: 947 loss: 3.15112e-07
Iter: 948 loss: 3.13946657e-07
Iter: 949 loss: 3.13764929e-07
Iter: 950 loss: 3.1461957e-07
Iter: 951 loss: 3.13736052e-07
Iter: 952 loss: 3.13611423e-07
Iter: 953 loss: 3.1363686e-07
Iter: 954 loss: 3.13537328e-07
Iter: 955 loss: 3.13346959e-07
Iter: 956 loss: 3.14539193e-07
Iter: 957 loss: 3.13315809e-07
Iter: 958 loss: 3.13206e-07
Iter: 959 loss: 3.12988902e-07
Iter: 960 loss: 3.12979466e-07
Iter: 961 loss: 3.12865353e-07
Iter: 962 loss: 3.12863165e-07
Iter: 963 loss: 3.12729924e-07
Iter: 964 loss: 3.12772841e-07
Iter: 965 loss: 3.12650599e-07
Iter: 966 loss: 3.12533359e-07
Iter: 967 loss: 3.12391535e-07
Iter: 968 loss: 3.12378063e-07
Iter: 969 loss: 3.1221623e-07
Iter: 970 loss: 3.13767572e-07
Iter: 971 loss: 3.12201109e-07
Iter: 972 loss: 3.12061275e-07
Iter: 973 loss: 3.13408e-07
Iter: 974 loss: 3.12052094e-07
Iter: 975 loss: 3.11951055e-07
Iter: 976 loss: 3.11797805e-07
Iter: 977 loss: 3.11798715e-07
Iter: 978 loss: 3.11594931e-07
Iter: 979 loss: 3.12327444e-07
Iter: 980 loss: 3.1153661e-07
Iter: 981 loss: 3.11326971e-07
Iter: 982 loss: 3.13325074e-07
Iter: 983 loss: 3.11314096e-07
Iter: 984 loss: 3.11185147e-07
Iter: 985 loss: 3.11031641e-07
Iter: 986 loss: 3.11009302e-07
Iter: 987 loss: 3.10795429e-07
Iter: 988 loss: 3.13836239e-07
Iter: 989 loss: 3.10796679e-07
Iter: 990 loss: 3.10669492e-07
Iter: 991 loss: 3.10476764e-07
Iter: 992 loss: 3.10476906e-07
Iter: 993 loss: 3.10351055e-07
Iter: 994 loss: 3.10356825e-07
Iter: 995 loss: 3.10224891e-07
Iter: 996 loss: 3.10507943e-07
Iter: 997 loss: 3.10183339e-07
Iter: 998 loss: 3.10076416e-07
Iter: 999 loss: 3.09889117e-07
Iter: 1000 loss: 3.14097321e-07
Iter: 1001 loss: 3.09887952e-07
Iter: 1002 loss: 3.09706138e-07
Iter: 1003 loss: 3.10754245e-07
Iter: 1004 loss: 3.09680416e-07
Iter: 1005 loss: 3.09531572e-07
Iter: 1006 loss: 3.11653679e-07
Iter: 1007 loss: 3.09532368e-07
Iter: 1008 loss: 3.09445568e-07
Iter: 1009 loss: 3.09236327e-07
Iter: 1010 loss: 3.09233e-07
Iter: 1011 loss: 3.09044594e-07
Iter: 1012 loss: 3.10633197e-07
Iter: 1013 loss: 3.09036238e-07
Iter: 1014 loss: 3.08897455e-07
Iter: 1015 loss: 3.10220969e-07
Iter: 1016 loss: 3.0888981e-07
Iter: 1017 loss: 3.08781495e-07
Iter: 1018 loss: 3.08613238e-07
Iter: 1019 loss: 3.08607582e-07
Iter: 1020 loss: 3.08470248e-07
Iter: 1021 loss: 3.08469822e-07
Iter: 1022 loss: 3.08324644e-07
Iter: 1023 loss: 3.08149822e-07
Iter: 1024 loss: 3.08135952e-07
Iter: 1025 loss: 3.07958743e-07
Iter: 1026 loss: 3.08345022e-07
Iter: 1027 loss: 3.07879304e-07
Iter: 1028 loss: 3.0767697e-07
Iter: 1029 loss: 3.10108419e-07
Iter: 1030 loss: 3.07668273e-07
Iter: 1031 loss: 3.0754569e-07
Iter: 1032 loss: 3.07400228e-07
Iter: 1033 loss: 3.07390309e-07
Iter: 1034 loss: 3.07224809e-07
Iter: 1035 loss: 3.07165323e-07
Iter: 1036 loss: 3.07066159e-07
Iter: 1037 loss: 3.06985811e-07
Iter: 1038 loss: 3.06927291e-07
Iter: 1039 loss: 3.0681619e-07
Iter: 1040 loss: 3.06727941e-07
Iter: 1041 loss: 3.06671268e-07
Iter: 1042 loss: 3.06534673e-07
Iter: 1043 loss: 3.06748859e-07
Iter: 1044 loss: 3.06455263e-07
Iter: 1045 loss: 3.06354309e-07
Iter: 1046 loss: 3.06348454e-07
Iter: 1047 loss: 3.06256879e-07
Iter: 1048 loss: 3.06263502e-07
Iter: 1049 loss: 3.06190543e-07
Iter: 1050 loss: 3.06092204e-07
Iter: 1051 loss: 3.06471122e-07
Iter: 1052 loss: 3.06082967e-07
Iter: 1053 loss: 3.05959077e-07
Iter: 1054 loss: 3.0607481e-07
Iter: 1055 loss: 3.05892343e-07
Iter: 1056 loss: 3.05799404e-07
Iter: 1057 loss: 3.05791474e-07
Iter: 1058 loss: 3.05727923e-07
Iter: 1059 loss: 3.05629072e-07
Iter: 1060 loss: 3.05619608e-07
Iter: 1061 loss: 3.05544347e-07
Iter: 1062 loss: 3.05337892e-07
Iter: 1063 loss: 3.07472391e-07
Iter: 1064 loss: 3.05331355e-07
Iter: 1065 loss: 3.05089742e-07
Iter: 1066 loss: 3.05044239e-07
Iter: 1067 loss: 3.04882917e-07
Iter: 1068 loss: 3.0461544e-07
Iter: 1069 loss: 3.08452059e-07
Iter: 1070 loss: 3.04621551e-07
Iter: 1071 loss: 3.04357116e-07
Iter: 1072 loss: 3.05559752e-07
Iter: 1073 loss: 3.04293962e-07
Iter: 1074 loss: 3.04156032e-07
Iter: 1075 loss: 3.04212222e-07
Iter: 1076 loss: 3.04034756e-07
Iter: 1077 loss: 3.0390504e-07
Iter: 1078 loss: 3.05907861e-07
Iter: 1079 loss: 3.03903278e-07
Iter: 1080 loss: 3.03810168e-07
Iter: 1081 loss: 3.03974787e-07
Iter: 1082 loss: 3.03763244e-07
Iter: 1083 loss: 3.03674852e-07
Iter: 1084 loss: 3.03778222e-07
Iter: 1085 loss: 3.03611387e-07
Iter: 1086 loss: 3.0351498e-07
Iter: 1087 loss: 3.04535263e-07
Iter: 1088 loss: 3.03512053e-07
Iter: 1089 loss: 3.03443159e-07
Iter: 1090 loss: 3.03316256e-07
Iter: 1091 loss: 3.05275023e-07
Iter: 1092 loss: 3.03312618e-07
Iter: 1093 loss: 3.03193474e-07
Iter: 1094 loss: 3.03196828e-07
Iter: 1095 loss: 3.03059153e-07
Iter: 1096 loss: 3.03141235e-07
Iter: 1097 loss: 3.02974456e-07
Iter: 1098 loss: 3.02865686e-07
Iter: 1099 loss: 3.02634305e-07
Iter: 1100 loss: 3.06618e-07
Iter: 1101 loss: 3.02619156e-07
Iter: 1102 loss: 3.02384422e-07
Iter: 1103 loss: 3.03915385e-07
Iter: 1104 loss: 3.0237635e-07
Iter: 1105 loss: 3.02316e-07
Iter: 1106 loss: 3.02274117e-07
Iter: 1107 loss: 3.02199055e-07
Iter: 1108 loss: 3.02068e-07
Iter: 1109 loss: 3.02069623e-07
Iter: 1110 loss: 3.01942435e-07
Iter: 1111 loss: 3.02907893e-07
Iter: 1112 loss: 3.01945477e-07
Iter: 1113 loss: 3.01838213e-07
Iter: 1114 loss: 3.01997915e-07
Iter: 1115 loss: 3.01790351e-07
Iter: 1116 loss: 3.01639659e-07
Iter: 1117 loss: 3.01688573e-07
Iter: 1118 loss: 3.01547061e-07
Iter: 1119 loss: 3.01397336e-07
Iter: 1120 loss: 3.02510671e-07
Iter: 1121 loss: 3.01384603e-07
Iter: 1122 loss: 3.01252101e-07
Iter: 1123 loss: 3.01208502e-07
Iter: 1124 loss: 3.01139607e-07
Iter: 1125 loss: 3.00981e-07
Iter: 1126 loss: 3.01342794e-07
Iter: 1127 loss: 3.00937927e-07
Iter: 1128 loss: 3.00826514e-07
Iter: 1129 loss: 3.00820034e-07
Iter: 1130 loss: 3.00767226e-07
Iter: 1131 loss: 3.00664624e-07
Iter: 1132 loss: 3.00661e-07
Iter: 1133 loss: 3.00544286e-07
Iter: 1134 loss: 3.00450665e-07
Iter: 1135 loss: 3.00426734e-07
Iter: 1136 loss: 3.00295198e-07
Iter: 1137 loss: 3.02183537e-07
Iter: 1138 loss: 3.0030526e-07
Iter: 1139 loss: 3.00198678e-07
Iter: 1140 loss: 3.01381192e-07
Iter: 1141 loss: 3.00198565e-07
Iter: 1142 loss: 3.00148059e-07
Iter: 1143 loss: 3.00026642e-07
Iter: 1144 loss: 3.01659639e-07
Iter: 1145 loss: 3.00025533e-07
Iter: 1146 loss: 2.9986964e-07
Iter: 1147 loss: 3.01517616e-07
Iter: 1148 loss: 2.99861455e-07
Iter: 1149 loss: 2.99720767e-07
Iter: 1150 loss: 2.9989468e-07
Iter: 1151 loss: 2.99676515e-07
Iter: 1152 loss: 2.99568427e-07
Iter: 1153 loss: 3.00123645e-07
Iter: 1154 loss: 2.99557144e-07
Iter: 1155 loss: 2.99460652e-07
Iter: 1156 loss: 2.99490665e-07
Iter: 1157 loss: 2.99387438e-07
Iter: 1158 loss: 2.99278724e-07
Iter: 1159 loss: 2.99382066e-07
Iter: 1160 loss: 2.9919471e-07
Iter: 1161 loss: 2.99119279e-07
Iter: 1162 loss: 2.99112855e-07
Iter: 1163 loss: 2.99039414e-07
Iter: 1164 loss: 2.98905661e-07
Iter: 1165 loss: 3.02314703e-07
Iter: 1166 loss: 2.98913335e-07
Iter: 1167 loss: 2.98743885e-07
Iter: 1168 loss: 2.98801638e-07
Iter: 1169 loss: 2.98632557e-07
Iter: 1170 loss: 2.98452505e-07
Iter: 1171 loss: 2.98695028e-07
Iter: 1172 loss: 2.98383782e-07
Iter: 1173 loss: 2.98305252e-07
Iter: 1174 loss: 2.98257902e-07
Iter: 1175 loss: 2.98188894e-07
Iter: 1176 loss: 2.98090299e-07
Iter: 1177 loss: 2.98067846e-07
Iter: 1178 loss: 2.98002732e-07
Iter: 1179 loss: 2.98979955e-07
Iter: 1180 loss: 2.97999264e-07
Iter: 1181 loss: 2.97934065e-07
Iter: 1182 loss: 2.98051788e-07
Iter: 1183 loss: 2.97912152e-07
Iter: 1184 loss: 2.9783331e-07
Iter: 1185 loss: 2.97941511e-07
Iter: 1186 loss: 2.97806849e-07
Iter: 1187 loss: 2.9772832e-07
Iter: 1188 loss: 2.98231129e-07
Iter: 1189 loss: 2.97725364e-07
Iter: 1190 loss: 2.97665395e-07
Iter: 1191 loss: 2.976102e-07
Iter: 1192 loss: 2.9760696e-07
Iter: 1193 loss: 2.97497252e-07
Iter: 1194 loss: 2.97815859e-07
Iter: 1195 loss: 2.97475e-07
Iter: 1196 loss: 2.973226e-07
Iter: 1197 loss: 2.97635864e-07
Iter: 1198 loss: 2.97254758e-07
Iter: 1199 loss: 2.97152042e-07
Iter: 1200 loss: 2.96974775e-07
Iter: 1201 loss: 2.96958575e-07
Iter: 1202 loss: 2.96740325e-07
Iter: 1203 loss: 2.9697506e-07
Iter: 1204 loss: 2.96603787e-07
Iter: 1205 loss: 2.96528128e-07
Iter: 1206 loss: 2.96504425e-07
Iter: 1207 loss: 2.9640168e-07
Iter: 1208 loss: 2.96602252e-07
Iter: 1209 loss: 2.96349185e-07
Iter: 1210 loss: 2.96264204e-07
Iter: 1211 loss: 2.96241438e-07
Iter: 1212 loss: 2.96198266e-07
Iter: 1213 loss: 2.96119595e-07
Iter: 1214 loss: 2.96117179e-07
Iter: 1215 loss: 2.96065764e-07
Iter: 1216 loss: 2.95979675e-07
Iter: 1217 loss: 2.97543465e-07
Iter: 1218 loss: 2.95985956e-07
Iter: 1219 loss: 2.95875395e-07
Iter: 1220 loss: 2.97314557e-07
Iter: 1221 loss: 2.9587676e-07
Iter: 1222 loss: 2.95792177e-07
Iter: 1223 loss: 2.95731411e-07
Iter: 1224 loss: 2.95701454e-07
Iter: 1225 loss: 2.95597459e-07
Iter: 1226 loss: 2.95883751e-07
Iter: 1227 loss: 2.95547721e-07
Iter: 1228 loss: 2.95492015e-07
Iter: 1229 loss: 2.95471807e-07
Iter: 1230 loss: 2.95409251e-07
Iter: 1231 loss: 2.9526376e-07
Iter: 1232 loss: 2.96938452e-07
Iter: 1233 loss: 2.95244405e-07
Iter: 1234 loss: 2.95120401e-07
Iter: 1235 loss: 2.95503753e-07
Iter: 1236 loss: 2.95079673e-07
Iter: 1237 loss: 2.94953253e-07
Iter: 1238 loss: 2.95133873e-07
Iter: 1239 loss: 2.94890185e-07
Iter: 1240 loss: 2.94755523e-07
Iter: 1241 loss: 2.94768199e-07
Iter: 1242 loss: 2.94674635e-07
Iter: 1243 loss: 2.94574704e-07
Iter: 1244 loss: 2.94561573e-07
Iter: 1245 loss: 2.94441719e-07
Iter: 1246 loss: 2.95982119e-07
Iter: 1247 loss: 2.94444703e-07
Iter: 1248 loss: 2.94318738e-07
Iter: 1249 loss: 2.94265533e-07
Iter: 1250 loss: 2.94217159e-07
Iter: 1251 loss: 2.94119388e-07
Iter: 1252 loss: 2.95429487e-07
Iter: 1253 loss: 2.94123708e-07
Iter: 1254 loss: 2.94031565e-07
Iter: 1255 loss: 2.94103245e-07
Iter: 1256 loss: 2.93985067e-07
Iter: 1257 loss: 2.93892981e-07
Iter: 1258 loss: 2.93850746e-07
Iter: 1259 loss: 2.93812946e-07
Iter: 1260 loss: 2.93774775e-07
Iter: 1261 loss: 2.93750503e-07
Iter: 1262 loss: 2.936996e-07
Iter: 1263 loss: 2.93673793e-07
Iter: 1264 loss: 2.93656797e-07
Iter: 1265 loss: 2.93588243e-07
Iter: 1266 loss: 2.93458925e-07
Iter: 1267 loss: 2.95535187e-07
Iter: 1268 loss: 2.93449034e-07
Iter: 1269 loss: 2.93318209e-07
Iter: 1270 loss: 2.94127375e-07
Iter: 1271 loss: 2.93314542e-07
Iter: 1272 loss: 2.93197616e-07
Iter: 1273 loss: 2.94090484e-07
Iter: 1274 loss: 2.93178744e-07
Iter: 1275 loss: 2.93073924e-07
Iter: 1276 loss: 2.93249911e-07
Iter: 1277 loss: 2.93020889e-07
Iter: 1278 loss: 2.92935226e-07
Iter: 1279 loss: 2.93179255e-07
Iter: 1280 loss: 2.92918855e-07
Iter: 1281 loss: 2.9282171e-07
Iter: 1282 loss: 2.93515711e-07
Iter: 1283 loss: 2.92814576e-07
Iter: 1284 loss: 2.92765122e-07
Iter: 1285 loss: 2.92718539e-07
Iter: 1286 loss: 2.92712542e-07
Iter: 1287 loss: 2.92590386e-07
Iter: 1288 loss: 2.93176242e-07
Iter: 1289 loss: 2.92588282e-07
Iter: 1290 loss: 2.92493667e-07
Iter: 1291 loss: 2.9244444e-07
Iter: 1292 loss: 2.92413745e-07
Iter: 1293 loss: 2.9231353e-07
Iter: 1294 loss: 2.93265742e-07
Iter: 1295 loss: 2.92306197e-07
Iter: 1296 loss: 2.92186769e-07
Iter: 1297 loss: 2.92461635e-07
Iter: 1298 loss: 2.92163918e-07
Iter: 1299 loss: 2.92071604e-07
Iter: 1300 loss: 2.91937454e-07
Iter: 1301 loss: 2.91940751e-07
Iter: 1302 loss: 2.9178986e-07
Iter: 1303 loss: 2.92156415e-07
Iter: 1304 loss: 2.91729805e-07
Iter: 1305 loss: 2.916602e-07
Iter: 1306 loss: 2.91652697e-07
Iter: 1307 loss: 2.91594944e-07
Iter: 1308 loss: 2.92127339e-07
Iter: 1309 loss: 2.91587639e-07
Iter: 1310 loss: 2.91554443e-07
Iter: 1311 loss: 2.91478841e-07
Iter: 1312 loss: 2.93203328e-07
Iter: 1313 loss: 2.91474635e-07
Iter: 1314 loss: 2.91406479e-07
Iter: 1315 loss: 2.9140719e-07
Iter: 1316 loss: 2.91372572e-07
Iter: 1317 loss: 2.91315217e-07
Iter: 1318 loss: 2.91314791e-07
Iter: 1319 loss: 2.91220488e-07
Iter: 1320 loss: 2.91733613e-07
Iter: 1321 loss: 2.91220516e-07
Iter: 1322 loss: 2.91136132e-07
Iter: 1323 loss: 2.90975407e-07
Iter: 1324 loss: 2.90980324e-07
Iter: 1325 loss: 2.90842365e-07
Iter: 1326 loss: 2.92121797e-07
Iter: 1327 loss: 2.90829888e-07
Iter: 1328 loss: 2.90703895e-07
Iter: 1329 loss: 2.92020587e-07
Iter: 1330 loss: 2.90723449e-07
Iter: 1331 loss: 2.90657368e-07
Iter: 1332 loss: 2.90498861e-07
Iter: 1333 loss: 2.92348631e-07
Iter: 1334 loss: 2.90496871e-07
Iter: 1335 loss: 2.90353e-07
Iter: 1336 loss: 2.91190418e-07
Iter: 1337 loss: 2.90341944e-07
Iter: 1338 loss: 2.90239257e-07
Iter: 1339 loss: 2.90460321e-07
Iter: 1340 loss: 2.90225131e-07
Iter: 1341 loss: 2.90151092e-07
Iter: 1342 loss: 2.90136313e-07
Iter: 1343 loss: 2.90098797e-07
Iter: 1344 loss: 2.90020523e-07
Iter: 1345 loss: 2.9001e-07
Iter: 1346 loss: 2.89927073e-07
Iter: 1347 loss: 2.90770458e-07
Iter: 1348 loss: 2.89929517e-07
Iter: 1349 loss: 2.89857951e-07
Iter: 1350 loss: 2.89872588e-07
Iter: 1351 loss: 2.89790648e-07
Iter: 1352 loss: 2.89740626e-07
Iter: 1353 loss: 2.90303603e-07
Iter: 1354 loss: 2.89732213e-07
Iter: 1355 loss: 2.89658885e-07
Iter: 1356 loss: 2.89663632e-07
Iter: 1357 loss: 2.89629952e-07
Iter: 1358 loss: 2.89549973e-07
Iter: 1359 loss: 2.89579162e-07
Iter: 1360 loss: 2.89499667e-07
Iter: 1361 loss: 2.8943748e-07
Iter: 1362 loss: 2.89442312e-07
Iter: 1363 loss: 2.89379841e-07
Iter: 1364 loss: 2.89288295e-07
Iter: 1365 loss: 2.89301852e-07
Iter: 1366 loss: 2.89196123e-07
Iter: 1367 loss: 2.89123705e-07
Iter: 1368 loss: 2.89084085e-07
Iter: 1369 loss: 2.88938082e-07
Iter: 1370 loss: 2.89831547e-07
Iter: 1371 loss: 2.88906762e-07
Iter: 1372 loss: 2.88840226e-07
Iter: 1373 loss: 2.88830364e-07
Iter: 1374 loss: 2.88750357e-07
Iter: 1375 loss: 2.88656e-07
Iter: 1376 loss: 2.88641161e-07
Iter: 1377 loss: 2.88534e-07
Iter: 1378 loss: 2.88743593e-07
Iter: 1379 loss: 2.88481147e-07
Iter: 1380 loss: 2.88415833e-07
Iter: 1381 loss: 2.88404692e-07
Iter: 1382 loss: 2.88353874e-07
Iter: 1383 loss: 2.88260537e-07
Iter: 1384 loss: 2.89772458e-07
Iter: 1385 loss: 2.88258377e-07
Iter: 1386 loss: 2.88156571e-07
Iter: 1387 loss: 2.89380978e-07
Iter: 1388 loss: 2.88157452e-07
Iter: 1389 loss: 2.88097965e-07
Iter: 1390 loss: 2.88062097e-07
Iter: 1391 loss: 2.88017475e-07
Iter: 1392 loss: 2.87935677e-07
Iter: 1393 loss: 2.88315846e-07
Iter: 1394 loss: 2.87922148e-07
Iter: 1395 loss: 2.87821933e-07
Iter: 1396 loss: 2.88487911e-07
Iter: 1397 loss: 2.8780363e-07
Iter: 1398 loss: 2.87763783e-07
Iter: 1399 loss: 2.8765453e-07
Iter: 1400 loss: 2.87659617e-07
Iter: 1401 loss: 2.87552609e-07
Iter: 1402 loss: 2.87620139e-07
Iter: 1403 loss: 2.87499688e-07
Iter: 1404 loss: 2.87379976e-07
Iter: 1405 loss: 2.88545266e-07
Iter: 1406 loss: 2.8737864e-07
Iter: 1407 loss: 2.87291925e-07
Iter: 1408 loss: 2.88369506e-07
Iter: 1409 loss: 2.87294711e-07
Iter: 1410 loss: 2.87221781e-07
Iter: 1411 loss: 2.87114403e-07
Iter: 1412 loss: 2.88334547e-07
Iter: 1413 loss: 2.87080695e-07
Iter: 1414 loss: 2.87030986e-07
Iter: 1415 loss: 2.87009073e-07
Iter: 1416 loss: 2.86914229e-07
Iter: 1417 loss: 2.86827543e-07
Iter: 1418 loss: 2.86795455e-07
Iter: 1419 loss: 2.86694132e-07
Iter: 1420 loss: 2.87621873e-07
Iter: 1421 loss: 2.86682905e-07
Iter: 1422 loss: 2.86598123e-07
Iter: 1423 loss: 2.86752851e-07
Iter: 1424 loss: 2.8654776e-07
Iter: 1425 loss: 2.86486284e-07
Iter: 1426 loss: 2.86445953e-07
Iter: 1427 loss: 2.86407243e-07
Iter: 1428 loss: 2.86334341e-07
Iter: 1429 loss: 2.87607691e-07
Iter: 1430 loss: 2.86331158e-07
Iter: 1431 loss: 2.8625044e-07
Iter: 1432 loss: 2.86341191e-07
Iter: 1433 loss: 2.86202521e-07
Iter: 1434 loss: 2.86153636e-07
Iter: 1435 loss: 2.86054899e-07
Iter: 1436 loss: 2.86051318e-07
Iter: 1437 loss: 2.85940928e-07
Iter: 1438 loss: 2.86288639e-07
Iter: 1439 loss: 2.85901933e-07
Iter: 1440 loss: 2.85787223e-07
Iter: 1441 loss: 2.86418526e-07
Iter: 1442 loss: 2.85793789e-07
Iter: 1443 loss: 2.85728959e-07
Iter: 1444 loss: 2.8572839e-07
Iter: 1445 loss: 2.85689111e-07
Iter: 1446 loss: 2.8558577e-07
Iter: 1447 loss: 2.87130376e-07
Iter: 1448 loss: 2.85574401e-07
Iter: 1449 loss: 2.85553227e-07
Iter: 1450 loss: 2.85538874e-07
Iter: 1451 loss: 2.85490046e-07
Iter: 1452 loss: 2.85378178e-07
Iter: 1453 loss: 2.8693168e-07
Iter: 1454 loss: 2.85363626e-07
Iter: 1455 loss: 2.85243317e-07
Iter: 1456 loss: 2.8653389e-07
Iter: 1457 loss: 2.85246841e-07
Iter: 1458 loss: 2.85145234e-07
Iter: 1459 loss: 2.85167403e-07
Iter: 1460 loss: 2.85070684e-07
Iter: 1461 loss: 2.84971861e-07
Iter: 1462 loss: 2.84926728e-07
Iter: 1463 loss: 2.84877245e-07
Iter: 1464 loss: 2.84822846e-07
Iter: 1465 loss: 2.84807442e-07
Iter: 1466 loss: 2.84734483e-07
Iter: 1467 loss: 2.84863262e-07
Iter: 1468 loss: 2.84705607e-07
Iter: 1469 loss: 2.84658313e-07
Iter: 1470 loss: 2.84601896e-07
Iter: 1471 loss: 2.84594876e-07
Iter: 1472 loss: 2.84503869e-07
Iter: 1473 loss: 2.84633416e-07
Iter: 1474 loss: 2.84453222e-07
Iter: 1475 loss: 2.84375346e-07
Iter: 1476 loss: 2.84840496e-07
Iter: 1477 loss: 2.84369719e-07
Iter: 1478 loss: 2.84313728e-07
Iter: 1479 loss: 2.84321061e-07
Iter: 1480 loss: 2.84262399e-07
Iter: 1481 loss: 2.84173638e-07
Iter: 1482 loss: 2.86065756e-07
Iter: 1483 loss: 2.84174718e-07
Iter: 1484 loss: 2.84080699e-07
Iter: 1485 loss: 2.84817702e-07
Iter: 1486 loss: 2.84074076e-07
Iter: 1487 loss: 2.83978693e-07
Iter: 1488 loss: 2.84184296e-07
Iter: 1489 loss: 2.83944644e-07
Iter: 1490 loss: 2.83873248e-07
Iter: 1491 loss: 2.83880297e-07
Iter: 1492 loss: 2.83821748e-07
Iter: 1493 loss: 2.83755753e-07
Iter: 1494 loss: 2.84867411e-07
Iter: 1495 loss: 2.83752058e-07
Iter: 1496 loss: 2.83700132e-07
Iter: 1497 loss: 2.83586928e-07
Iter: 1498 loss: 2.85290525e-07
Iter: 1499 loss: 2.83578828e-07
Iter: 1500 loss: 2.83489783e-07
Iter: 1501 loss: 2.84411357e-07
Iter: 1502 loss: 2.8349541e-07
Iter: 1503 loss: 2.83391927e-07
Iter: 1504 loss: 2.83904313e-07
Iter: 1505 loss: 2.83372742e-07
Iter: 1506 loss: 2.83323288e-07
Iter: 1507 loss: 2.83218469e-07
Iter: 1508 loss: 2.85500363e-07
Iter: 1509 loss: 2.83221652e-07
Iter: 1510 loss: 2.83119277e-07
Iter: 1511 loss: 2.83312147e-07
Iter: 1512 loss: 2.83063e-07
Iter: 1513 loss: 2.82946701e-07
Iter: 1514 loss: 2.83844855e-07
Iter: 1515 loss: 2.82928681e-07
Iter: 1516 loss: 2.82830456e-07
Iter: 1517 loss: 2.83712438e-07
Iter: 1518 loss: 2.82827102e-07
Iter: 1519 loss: 2.82767076e-07
Iter: 1520 loss: 2.82682862e-07
Iter: 1521 loss: 2.82686699e-07
Iter: 1522 loss: 2.82597483e-07
Iter: 1523 loss: 2.83319139e-07
Iter: 1524 loss: 2.82594783e-07
Iter: 1525 loss: 2.82523416e-07
Iter: 1526 loss: 2.82764631e-07
Iter: 1527 loss: 2.82489736e-07
Iter: 1528 loss: 2.82441022e-07
Iter: 1529 loss: 2.82392875e-07
Iter: 1530 loss: 2.82378267e-07
Iter: 1531 loss: 2.82330149e-07
Iter: 1532 loss: 2.82325658e-07
Iter: 1533 loss: 2.82288966e-07
Iter: 1534 loss: 2.82174483e-07
Iter: 1535 loss: 2.83405086e-07
Iter: 1536 loss: 2.82167974e-07
Iter: 1537 loss: 2.82054486e-07
Iter: 1538 loss: 2.82747294e-07
Iter: 1539 loss: 2.8204542e-07
Iter: 1540 loss: 2.81971438e-07
Iter: 1541 loss: 2.8317757e-07
Iter: 1542 loss: 2.81970614e-07
Iter: 1543 loss: 2.81917295e-07
Iter: 1544 loss: 2.81785503e-07
Iter: 1545 loss: 2.82784299e-07
Iter: 1546 loss: 2.81737357e-07
Iter: 1547 loss: 2.81616167e-07
Iter: 1548 loss: 2.82646852e-07
Iter: 1549 loss: 2.8160872e-07
Iter: 1550 loss: 2.81534284e-07
Iter: 1551 loss: 2.82569658e-07
Iter: 1552 loss: 2.81525303e-07
Iter: 1553 loss: 2.81432108e-07
Iter: 1554 loss: 2.81516151e-07
Iter: 1555 loss: 2.81380267e-07
Iter: 1556 loss: 2.81330642e-07
Iter: 1557 loss: 2.81322286e-07
Iter: 1558 loss: 2.81267489e-07
Iter: 1559 loss: 2.81214398e-07
Iter: 1560 loss: 2.8120661e-07
Iter: 1561 loss: 2.81160681e-07
Iter: 1562 loss: 2.81098551e-07
Iter: 1563 loss: 2.81092241e-07
Iter: 1564 loss: 2.81018401e-07
Iter: 1565 loss: 2.81329648e-07
Iter: 1566 loss: 2.81016241e-07
Iter: 1567 loss: 2.80947404e-07
Iter: 1568 loss: 2.8121508e-07
Iter: 1569 loss: 2.80928361e-07
Iter: 1570 loss: 2.80877799e-07
Iter: 1571 loss: 2.80840084e-07
Iter: 1572 loss: 2.80802112e-07
Iter: 1573 loss: 2.80764425e-07
Iter: 1574 loss: 2.81602382e-07
Iter: 1575 loss: 2.80766812e-07
Iter: 1576 loss: 2.80699709e-07
Iter: 1577 loss: 2.80705535e-07
Iter: 1578 loss: 2.80652813e-07
Iter: 1579 loss: 2.80599863e-07
Iter: 1580 loss: 2.80451e-07
Iter: 1581 loss: 2.8329552e-07
Iter: 1582 loss: 2.80446159e-07
Iter: 1583 loss: 2.80293904e-07
Iter: 1584 loss: 2.81280393e-07
Iter: 1585 loss: 2.80262327e-07
Iter: 1586 loss: 2.8019997e-07
Iter: 1587 loss: 2.80192779e-07
Iter: 1588 loss: 2.80120815e-07
Iter: 1589 loss: 2.80044731e-07
Iter: 1590 loss: 2.80040467e-07
Iter: 1591 loss: 2.79967594e-07
Iter: 1592 loss: 2.80337332e-07
Iter: 1593 loss: 2.79944146e-07
Iter: 1594 loss: 2.79850838e-07
Iter: 1595 loss: 2.80462189e-07
Iter: 1596 loss: 2.79835746e-07
Iter: 1597 loss: 2.79805647e-07
Iter: 1598 loss: 2.79753181e-07
Iter: 1599 loss: 2.79736696e-07
Iter: 1600 loss: 2.79710662e-07
Iter: 1601 loss: 2.79702647e-07
Iter: 1602 loss: 2.79661577e-07
Iter: 1603 loss: 2.79592e-07
Iter: 1604 loss: 2.81359064e-07
Iter: 1605 loss: 2.79582537e-07
Iter: 1606 loss: 2.79494515e-07
Iter: 1607 loss: 2.79534305e-07
Iter: 1608 loss: 2.79427184e-07
Iter: 1609 loss: 2.79401462e-07
Iter: 1610 loss: 2.79379151e-07
Iter: 1611 loss: 2.79326031e-07
Iter: 1612 loss: 2.79278311e-07
Iter: 1613 loss: 2.79277742e-07
Iter: 1614 loss: 2.79199497e-07
Iter: 1615 loss: 2.79155813e-07
Iter: 1616 loss: 2.7912472e-07
Iter: 1617 loss: 2.79027148e-07
Iter: 1618 loss: 2.79563096e-07
Iter: 1619 loss: 2.7902243e-07
Iter: 1620 loss: 2.78963711e-07
Iter: 1621 loss: 2.78962375e-07
Iter: 1622 loss: 2.78898511e-07
Iter: 1623 loss: 2.78806397e-07
Iter: 1624 loss: 2.81079792e-07
Iter: 1625 loss: 2.78819329e-07
Iter: 1626 loss: 2.78719369e-07
Iter: 1627 loss: 2.79630314e-07
Iter: 1628 loss: 2.78731193e-07
Iter: 1629 loss: 2.78634673e-07
Iter: 1630 loss: 2.78672132e-07
Iter: 1631 loss: 2.78566205e-07
Iter: 1632 loss: 2.78458515e-07
Iter: 1633 loss: 2.78290884e-07
Iter: 1634 loss: 2.78283665e-07
Iter: 1635 loss: 2.78274769e-07
Iter: 1636 loss: 2.7820019e-07
Iter: 1637 loss: 2.78131665e-07
Iter: 1638 loss: 2.78031251e-07
Iter: 1639 loss: 2.78024e-07
Iter: 1640 loss: 2.7791927e-07
Iter: 1641 loss: 2.78570667e-07
Iter: 1642 loss: 2.77913102e-07
Iter: 1643 loss: 2.77817776e-07
Iter: 1644 loss: 2.7848796e-07
Iter: 1645 loss: 2.77823943e-07
Iter: 1646 loss: 2.77779066e-07
Iter: 1647 loss: 2.77704203e-07
Iter: 1648 loss: 2.77696529e-07
Iter: 1649 loss: 2.77606546e-07
Iter: 1650 loss: 2.7757207e-07
Iter: 1651 loss: 2.77527562e-07
Iter: 1652 loss: 2.77482286e-07
Iter: 1653 loss: 2.77459549e-07
Iter: 1654 loss: 2.77416518e-07
Iter: 1655 loss: 2.77525146e-07
Iter: 1656 loss: 2.77393923e-07
Iter: 1657 loss: 2.77328581e-07
Iter: 1658 loss: 2.77213e-07
Iter: 1659 loss: 2.79322478e-07
Iter: 1660 loss: 2.77212848e-07
Iter: 1661 loss: 2.77162314e-07
Iter: 1662 loss: 2.77147109e-07
Iter: 1663 loss: 2.77105755e-07
Iter: 1664 loss: 2.77011537e-07
Iter: 1665 loss: 2.7700213e-07
Iter: 1666 loss: 2.76950232e-07
Iter: 1667 loss: 2.77094671e-07
Iter: 1668 loss: 2.76903336e-07
Iter: 1669 loss: 2.76801302e-07
Iter: 1670 loss: 2.77299137e-07
Iter: 1671 loss: 2.76775467e-07
Iter: 1672 loss: 2.76711575e-07
Iter: 1673 loss: 2.76626764e-07
Iter: 1674 loss: 2.76601895e-07
Iter: 1675 loss: 2.76518335e-07
Iter: 1676 loss: 2.77236637e-07
Iter: 1677 loss: 2.76499492e-07
Iter: 1678 loss: 2.76380177e-07
Iter: 1679 loss: 2.7669418e-07
Iter: 1680 loss: 2.76331917e-07
Iter: 1681 loss: 2.76261517e-07
Iter: 1682 loss: 2.761941e-07
Iter: 1683 loss: 2.76179065e-07
Iter: 1684 loss: 2.76071603e-07
Iter: 1685 loss: 2.76146352e-07
Iter: 1686 loss: 2.7599674e-07
Iter: 1687 loss: 2.75929068e-07
Iter: 1688 loss: 2.75915397e-07
Iter: 1689 loss: 2.75858099e-07
Iter: 1690 loss: 2.76193958e-07
Iter: 1691 loss: 2.758444e-07
Iter: 1692 loss: 2.75801767e-07
Iter: 1693 loss: 2.75716985e-07
Iter: 1694 loss: 2.77652674e-07
Iter: 1695 loss: 2.75712921e-07
Iter: 1696 loss: 2.75640161e-07
Iter: 1697 loss: 2.76847544e-07
Iter: 1698 loss: 2.7564451e-07
Iter: 1699 loss: 2.75570187e-07
Iter: 1700 loss: 2.75737449e-07
Iter: 1701 loss: 2.75542618e-07
Iter: 1702 loss: 2.75491175e-07
Iter: 1703 loss: 2.75349578e-07
Iter: 1704 loss: 2.76970724e-07
Iter: 1705 loss: 2.75338152e-07
Iter: 1706 loss: 2.75258174e-07
Iter: 1707 loss: 2.75234754e-07
Iter: 1708 loss: 2.75163671e-07
Iter: 1709 loss: 2.75061211e-07
Iter: 1710 loss: 2.75064508e-07
Iter: 1711 loss: 2.74934592e-07
Iter: 1712 loss: 2.74885565e-07
Iter: 1713 loss: 2.74817069e-07
Iter: 1714 loss: 2.74805558e-07
Iter: 1715 loss: 2.74749937e-07
Iter: 1716 loss: 2.7468883e-07
Iter: 1717 loss: 2.74604304e-07
Iter: 1718 loss: 2.74602485e-07
Iter: 1719 loss: 2.74534131e-07
Iter: 1720 loss: 2.7449056e-07
Iter: 1721 loss: 2.74450286e-07
Iter: 1722 loss: 2.74387503e-07
Iter: 1723 loss: 2.74386935e-07
Iter: 1724 loss: 2.74326936e-07
Iter: 1725 loss: 2.74307581e-07
Iter: 1726 loss: 2.74278307e-07
Iter: 1727 loss: 2.74193212e-07
Iter: 1728 loss: 2.74121817e-07
Iter: 1729 loss: 2.74089416e-07
Iter: 1730 loss: 2.73985734e-07
Iter: 1731 loss: 2.74893239e-07
Iter: 1732 loss: 2.7398346e-07
Iter: 1733 loss: 2.73903112e-07
Iter: 1734 loss: 2.73892738e-07
Iter: 1735 loss: 2.73854113e-07
Iter: 1736 loss: 2.73718882e-07
Iter: 1737 loss: 2.75525394e-07
Iter: 1738 loss: 2.73719508e-07
Iter: 1739 loss: 2.73588796e-07
Iter: 1740 loss: 2.7416587e-07
Iter: 1741 loss: 2.73579076e-07
Iter: 1742 loss: 2.73517685e-07
Iter: 1743 loss: 2.73517969e-07
Iter: 1744 loss: 2.73454191e-07
Iter: 1745 loss: 2.73350167e-07
Iter: 1746 loss: 2.73966776e-07
Iter: 1747 loss: 2.73283092e-07
Iter: 1748 loss: 2.73174976e-07
Iter: 1749 loss: 2.74096692e-07
Iter: 1750 loss: 2.73158776e-07
Iter: 1751 loss: 2.73068594e-07
Iter: 1752 loss: 2.73428043e-07
Iter: 1753 loss: 2.73045885e-07
Iter: 1754 loss: 2.72944931e-07
Iter: 1755 loss: 2.7294567e-07
Iter: 1756 loss: 2.72880499e-07
Iter: 1757 loss: 2.72787275e-07
Iter: 1758 loss: 2.72789805e-07
Iter: 1759 loss: 2.72729125e-07
Iter: 1760 loss: 2.72729778e-07
Iter: 1761 loss: 2.72659179e-07
Iter: 1762 loss: 2.72519912e-07
Iter: 1763 loss: 2.74581225e-07
Iter: 1764 loss: 2.72521731e-07
Iter: 1765 loss: 2.72453775e-07
Iter: 1766 loss: 2.72431947e-07
Iter: 1767 loss: 2.72379509e-07
Iter: 1768 loss: 2.72341708e-07
Iter: 1769 loss: 2.72328577e-07
Iter: 1770 loss: 2.72269e-07
Iter: 1771 loss: 2.7223129e-07
Iter: 1772 loss: 2.72183087e-07
Iter: 1773 loss: 2.72089125e-07
Iter: 1774 loss: 2.72698202e-07
Iter: 1775 loss: 2.72090546e-07
Iter: 1776 loss: 2.7203842e-07
Iter: 1777 loss: 2.72029695e-07
Iter: 1778 loss: 2.71984277e-07
Iter: 1779 loss: 2.71940536e-07
Iter: 1780 loss: 2.71914217e-07
Iter: 1781 loss: 2.71833414e-07
Iter: 1782 loss: 2.71821591e-07
Iter: 1783 loss: 2.71773274e-07
Iter: 1784 loss: 2.71659417e-07
Iter: 1785 loss: 2.71972823e-07
Iter: 1786 loss: 2.7162389e-07
Iter: 1787 loss: 2.71515603e-07
Iter: 1788 loss: 2.73126545e-07
Iter: 1789 loss: 2.71513755e-07
Iter: 1790 loss: 2.71435709e-07
Iter: 1791 loss: 2.71328815e-07
Iter: 1792 loss: 2.7132694e-07
Iter: 1793 loss: 2.71249348e-07
Iter: 1794 loss: 2.71250371e-07
Iter: 1795 loss: 2.71167664e-07
Iter: 1796 loss: 2.71055171e-07
Iter: 1797 loss: 2.71046957e-07
Iter: 1798 loss: 2.70961095e-07
Iter: 1799 loss: 2.72119621e-07
Iter: 1800 loss: 2.70970901e-07
Iter: 1801 loss: 2.70881344e-07
Iter: 1802 loss: 2.7117477e-07
Iter: 1803 loss: 2.70862188e-07
Iter: 1804 loss: 2.70822056e-07
Iter: 1805 loss: 2.70731618e-07
Iter: 1806 loss: 2.71260831e-07
Iter: 1807 loss: 2.70698848e-07
Iter: 1808 loss: 2.70676452e-07
Iter: 1809 loss: 2.70636463e-07
Iter: 1810 loss: 2.70561429e-07
Iter: 1811 loss: 2.70786188e-07
Iter: 1812 loss: 2.70532553e-07
Iter: 1813 loss: 2.70472952e-07
Iter: 1814 loss: 2.70437681e-07
Iter: 1815 loss: 2.70428046e-07
Iter: 1816 loss: 2.70335e-07
Iter: 1817 loss: 2.70662099e-07
Iter: 1818 loss: 2.70309613e-07
Iter: 1819 loss: 2.70263342e-07
Iter: 1820 loss: 2.70250979e-07
Iter: 1821 loss: 2.70221904e-07
Iter: 1822 loss: 2.70184756e-07
Iter: 1823 loss: 2.70173018e-07
Iter: 1824 loss: 2.70103214e-07
Iter: 1825 loss: 2.69997258e-07
Iter: 1826 loss: 2.70005756e-07
Iter: 1827 loss: 2.69941438e-07
Iter: 1828 loss: 2.69916455e-07
Iter: 1829 loss: 2.69866234e-07
Iter: 1830 loss: 2.69772e-07
Iter: 1831 loss: 2.71022174e-07
Iter: 1832 loss: 2.69765e-07
Iter: 1833 loss: 2.69666543e-07
Iter: 1834 loss: 2.69664184e-07
Iter: 1835 loss: 2.69579743e-07
Iter: 1836 loss: 2.69478079e-07
Iter: 1837 loss: 2.69480665e-07
Iter: 1838 loss: 2.69365898e-07
Iter: 1839 loss: 2.69508632e-07
Iter: 1840 loss: 2.69288932e-07
Iter: 1841 loss: 2.6921839e-07
Iter: 1842 loss: 2.69212421e-07
Iter: 1843 loss: 2.69134546e-07
Iter: 1844 loss: 2.69267417e-07
Iter: 1845 loss: 2.69088247e-07
Iter: 1846 loss: 2.69037059e-07
Iter: 1847 loss: 2.68968421e-07
Iter: 1848 loss: 2.68963845e-07
Iter: 1849 loss: 2.68860049e-07
Iter: 1850 loss: 2.69239735e-07
Iter: 1851 loss: 2.68840751e-07
Iter: 1852 loss: 2.68740706e-07
Iter: 1853 loss: 2.69343815e-07
Iter: 1854 loss: 2.68755429e-07
Iter: 1855 loss: 2.68654446e-07
Iter: 1856 loss: 2.69075827e-07
Iter: 1857 loss: 2.68633755e-07
Iter: 1858 loss: 2.68580038e-07
Iter: 1859 loss: 2.68509496e-07
Iter: 1860 loss: 2.68496649e-07
Iter: 1861 loss: 2.68398196e-07
Iter: 1862 loss: 2.69352824e-07
Iter: 1863 loss: 2.68395638e-07
Iter: 1864 loss: 2.68313727e-07
Iter: 1865 loss: 2.68339392e-07
Iter: 1866 loss: 2.68259612e-07
Iter: 1867 loss: 2.68189154e-07
Iter: 1868 loss: 2.69035496e-07
Iter: 1869 loss: 2.68189808e-07
Iter: 1870 loss: 2.68116025e-07
Iter: 1871 loss: 2.6802968e-07
Iter: 1872 loss: 2.68012286e-07
Iter: 1873 loss: 2.67925572e-07
Iter: 1874 loss: 2.67879102e-07
Iter: 1875 loss: 2.67838232e-07
Iter: 1876 loss: 2.67716445e-07
Iter: 1877 loss: 2.68920644e-07
Iter: 1878 loss: 2.67717894e-07
Iter: 1879 loss: 2.67626064e-07
Iter: 1880 loss: 2.68644385e-07
Iter: 1881 loss: 2.6761677e-07
Iter: 1882 loss: 2.67553389e-07
Iter: 1883 loss: 2.67497228e-07
Iter: 1884 loss: 2.67485234e-07
Iter: 1885 loss: 2.67367312e-07
Iter: 1886 loss: 2.6753014e-07
Iter: 1887 loss: 2.67328915e-07
Iter: 1888 loss: 2.67254e-07
Iter: 1889 loss: 2.67254393e-07
Iter: 1890 loss: 2.67189193e-07
Iter: 1891 loss: 2.67113847e-07
Iter: 1892 loss: 2.67120669e-07
Iter: 1893 loss: 2.67040662e-07
Iter: 1894 loss: 2.6743038e-07
Iter: 1895 loss: 2.6703276e-07
Iter: 1896 loss: 2.66950451e-07
Iter: 1897 loss: 2.67359042e-07
Iter: 1898 loss: 2.66953634e-07
Iter: 1899 loss: 2.66888378e-07
Iter: 1900 loss: 2.6685592e-07
Iter: 1901 loss: 2.66829801e-07
Iter: 1902 loss: 2.66734105e-07
Iter: 1903 loss: 2.67662159e-07
Iter: 1904 loss: 2.66750675e-07
Iter: 1905 loss: 2.66690222e-07
Iter: 1906 loss: 2.66639972e-07
Iter: 1907 loss: 2.66629939e-07
Iter: 1908 loss: 2.66533959e-07
Iter: 1909 loss: 2.66584777e-07
Iter: 1910 loss: 2.66477713e-07
Iter: 1911 loss: 2.66356437e-07
Iter: 1912 loss: 2.66602939e-07
Iter: 1913 loss: 2.6631443e-07
Iter: 1914 loss: 2.66217171e-07
Iter: 1915 loss: 2.67459768e-07
Iter: 1916 loss: 2.66228341e-07
Iter: 1917 loss: 2.66133782e-07
Iter: 1918 loss: 2.66259661e-07
Iter: 1919 loss: 2.66092144e-07
Iter: 1920 loss: 2.6598849e-07
Iter: 1921 loss: 2.65944067e-07
Iter: 1922 loss: 2.65915787e-07
Iter: 1923 loss: 2.65823815e-07
Iter: 1924 loss: 2.65815657e-07
Iter: 1925 loss: 2.65737725e-07
Iter: 1926 loss: 2.65645838e-07
Iter: 1927 loss: 2.65645951e-07
Iter: 1928 loss: 2.65556878e-07
Iter: 1929 loss: 2.65913201e-07
Iter: 1930 loss: 2.65537722e-07
Iter: 1931 loss: 2.65469254e-07
Iter: 1932 loss: 2.66170076e-07
Iter: 1933 loss: 2.65448477e-07
Iter: 1934 loss: 2.65407607e-07
Iter: 1935 loss: 2.65405447e-07
Iter: 1936 loss: 2.65376798e-07
Iter: 1937 loss: 2.65306113e-07
Iter: 1938 loss: 2.65792096e-07
Iter: 1939 loss: 2.65310774e-07
Iter: 1940 loss: 2.65261349e-07
Iter: 1941 loss: 2.65171252e-07
Iter: 1942 loss: 2.67104866e-07
Iter: 1943 loss: 2.65176794e-07
Iter: 1944 loss: 2.65077574e-07
Iter: 1945 loss: 2.65164033e-07
Iter: 1946 loss: 2.6502056e-07
Iter: 1947 loss: 2.64889138e-07
Iter: 1948 loss: 2.65382226e-07
Iter: 1949 loss: 2.64853242e-07
Iter: 1950 loss: 2.64730176e-07
Iter: 1951 loss: 2.64907015e-07
Iter: 1952 loss: 2.64666028e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.2
+ date
Mon Oct 26 17:25:45 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.2/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.2_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -1 --phi 1.2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.2_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700afa8e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700b023378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700b029d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700b0adbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700af50510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700af507b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700af39400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700aed5510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700aece7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700aecea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ae42bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ae70d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ae4f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ae4f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700adcfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700adb8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ada2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ad73158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ad379d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ad1ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ad11620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ad111e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700acb26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700acb2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ac796a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ac79268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ac02840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700abea7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700abea378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700ab86bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f700abad950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ff46f6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ff46f6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ff469dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ff46cbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ff46be0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.51889127e-06
Iter: 2 loss: 3.55265956e-06
Iter: 3 loss: 3.29450131e-06
Iter: 4 loss: 2.96244434e-06
Iter: 5 loss: 3.6408685e-06
Iter: 6 loss: 2.82843121e-06
Iter: 7 loss: 2.63650418e-06
Iter: 8 loss: 4.02540491e-06
Iter: 9 loss: 2.62006688e-06
Iter: 10 loss: 2.49124378e-06
Iter: 11 loss: 2.39004066e-06
Iter: 12 loss: 2.35007633e-06
Iter: 13 loss: 2.25454392e-06
Iter: 14 loss: 3.42356793e-06
Iter: 15 loss: 2.25347276e-06
Iter: 16 loss: 2.16670628e-06
Iter: 17 loss: 2.45552428e-06
Iter: 18 loss: 2.14292095e-06
Iter: 19 loss: 2.10430881e-06
Iter: 20 loss: 2.18549712e-06
Iter: 21 loss: 2.08904567e-06
Iter: 22 loss: 2.04035541e-06
Iter: 23 loss: 2.18811056e-06
Iter: 24 loss: 2.0256573e-06
Iter: 25 loss: 1.975239e-06
Iter: 26 loss: 1.93624419e-06
Iter: 27 loss: 1.92025755e-06
Iter: 28 loss: 1.84794658e-06
Iter: 29 loss: 1.87749788e-06
Iter: 30 loss: 1.79817084e-06
Iter: 31 loss: 1.7212094e-06
Iter: 32 loss: 2.02872047e-06
Iter: 33 loss: 1.70388637e-06
Iter: 34 loss: 1.65275424e-06
Iter: 35 loss: 2.20468837e-06
Iter: 36 loss: 1.65157462e-06
Iter: 37 loss: 1.61521484e-06
Iter: 38 loss: 1.60943227e-06
Iter: 39 loss: 1.58434102e-06
Iter: 40 loss: 1.54708539e-06
Iter: 41 loss: 2.09656719e-06
Iter: 42 loss: 1.54702434e-06
Iter: 43 loss: 1.50419601e-06
Iter: 44 loss: 1.62010303e-06
Iter: 45 loss: 1.4900927e-06
Iter: 46 loss: 1.46770674e-06
Iter: 47 loss: 1.5325511e-06
Iter: 48 loss: 1.46077184e-06
Iter: 49 loss: 1.43674902e-06
Iter: 50 loss: 1.50864764e-06
Iter: 51 loss: 1.42943884e-06
Iter: 52 loss: 1.40828161e-06
Iter: 53 loss: 1.38918597e-06
Iter: 54 loss: 1.38383257e-06
Iter: 55 loss: 1.3602853e-06
Iter: 56 loss: 1.35947607e-06
Iter: 57 loss: 1.34011088e-06
Iter: 58 loss: 1.30489343e-06
Iter: 59 loss: 2.14618422e-06
Iter: 60 loss: 1.30488752e-06
Iter: 61 loss: 1.28994702e-06
Iter: 62 loss: 1.28832312e-06
Iter: 63 loss: 1.27371459e-06
Iter: 64 loss: 1.27089902e-06
Iter: 65 loss: 1.26107989e-06
Iter: 66 loss: 1.24629094e-06
Iter: 67 loss: 1.22743143e-06
Iter: 68 loss: 1.22598317e-06
Iter: 69 loss: 1.20103766e-06
Iter: 70 loss: 1.43994794e-06
Iter: 71 loss: 1.20013203e-06
Iter: 72 loss: 1.18220396e-06
Iter: 73 loss: 1.14108275e-06
Iter: 74 loss: 1.6673755e-06
Iter: 75 loss: 1.13821579e-06
Iter: 76 loss: 1.12472583e-06
Iter: 77 loss: 1.11501413e-06
Iter: 78 loss: 1.0890675e-06
Iter: 79 loss: 1.17665809e-06
Iter: 80 loss: 1.08197651e-06
Iter: 81 loss: 1.06231482e-06
Iter: 82 loss: 1.06353468e-06
Iter: 83 loss: 1.04688263e-06
Iter: 84 loss: 1.02887691e-06
Iter: 85 loss: 1.26855389e-06
Iter: 86 loss: 1.02881722e-06
Iter: 87 loss: 1.01597448e-06
Iter: 88 loss: 1.00994873e-06
Iter: 89 loss: 1.003688e-06
Iter: 90 loss: 9.9305e-07
Iter: 91 loss: 9.9302008e-07
Iter: 92 loss: 9.82746542e-07
Iter: 93 loss: 9.72376483e-07
Iter: 94 loss: 9.70318297e-07
Iter: 95 loss: 9.59941644e-07
Iter: 96 loss: 1.01369801e-06
Iter: 97 loss: 9.58259875e-07
Iter: 98 loss: 9.47443084e-07
Iter: 99 loss: 9.90581498e-07
Iter: 100 loss: 9.4493538e-07
Iter: 101 loss: 9.35095159e-07
Iter: 102 loss: 9.31116176e-07
Iter: 103 loss: 9.25887377e-07
Iter: 104 loss: 9.12849373e-07
Iter: 105 loss: 9.01753879e-07
Iter: 106 loss: 8.98142332e-07
Iter: 107 loss: 8.8296764e-07
Iter: 108 loss: 1.00355123e-06
Iter: 109 loss: 8.81906431e-07
Iter: 110 loss: 8.66057519e-07
Iter: 111 loss: 8.99926476e-07
Iter: 112 loss: 8.59847148e-07
Iter: 113 loss: 8.63977561e-07
Iter: 114 loss: 8.55998337e-07
Iter: 115 loss: 8.52415326e-07
Iter: 116 loss: 8.42140139e-07
Iter: 117 loss: 8.8572051e-07
Iter: 118 loss: 8.38191966e-07
Iter: 119 loss: 8.2697494e-07
Iter: 120 loss: 9.14628572e-07
Iter: 121 loss: 8.26163159e-07
Iter: 122 loss: 8.15403382e-07
Iter: 123 loss: 8.88021759e-07
Iter: 124 loss: 8.14330463e-07
Iter: 125 loss: 8.08384243e-07
Iter: 126 loss: 8.04347337e-07
Iter: 127 loss: 8.0218615e-07
Iter: 128 loss: 7.88302259e-07
Iter: 129 loss: 8.23863331e-07
Iter: 130 loss: 7.83467954e-07
Iter: 131 loss: 7.70336442e-07
Iter: 132 loss: 7.73867896e-07
Iter: 133 loss: 7.60849673e-07
Iter: 134 loss: 7.53237487e-07
Iter: 135 loss: 8.62477862e-07
Iter: 136 loss: 7.53235668e-07
Iter: 137 loss: 7.45275599e-07
Iter: 138 loss: 7.43059161e-07
Iter: 139 loss: 7.38226163e-07
Iter: 140 loss: 7.32130331e-07
Iter: 141 loss: 7.45992907e-07
Iter: 142 loss: 7.29836131e-07
Iter: 143 loss: 7.23809421e-07
Iter: 144 loss: 7.3795519e-07
Iter: 145 loss: 7.21558308e-07
Iter: 146 loss: 7.15746182e-07
Iter: 147 loss: 7.14745056e-07
Iter: 148 loss: 7.10778295e-07
Iter: 149 loss: 7.05364414e-07
Iter: 150 loss: 7.05278239e-07
Iter: 151 loss: 7.0185996e-07
Iter: 152 loss: 7.47138415e-07
Iter: 153 loss: 7.01831937e-07
Iter: 154 loss: 6.98174688e-07
Iter: 155 loss: 6.89792387e-07
Iter: 156 loss: 7.93172262e-07
Iter: 157 loss: 6.89115609e-07
Iter: 158 loss: 6.81683446e-07
Iter: 159 loss: 6.93602033e-07
Iter: 160 loss: 6.78286e-07
Iter: 161 loss: 6.74866442e-07
Iter: 162 loss: 6.7346889e-07
Iter: 163 loss: 6.70558734e-07
Iter: 164 loss: 6.64881554e-07
Iter: 165 loss: 7.79618574e-07
Iter: 166 loss: 6.64882236e-07
Iter: 167 loss: 6.59495413e-07
Iter: 168 loss: 6.59461875e-07
Iter: 169 loss: 6.56221118e-07
Iter: 170 loss: 6.52692393e-07
Iter: 171 loss: 6.52146184e-07
Iter: 172 loss: 6.47591207e-07
Iter: 173 loss: 6.68085477e-07
Iter: 174 loss: 6.46735543e-07
Iter: 175 loss: 6.41838653e-07
Iter: 176 loss: 6.64572781e-07
Iter: 177 loss: 6.40923815e-07
Iter: 178 loss: 6.37097173e-07
Iter: 179 loss: 6.31701823e-07
Iter: 180 loss: 6.3148525e-07
Iter: 181 loss: 6.26452277e-07
Iter: 182 loss: 6.3051931e-07
Iter: 183 loss: 6.23402798e-07
Iter: 184 loss: 6.17284741e-07
Iter: 185 loss: 6.77587423e-07
Iter: 186 loss: 6.17095168e-07
Iter: 187 loss: 6.14661815e-07
Iter: 188 loss: 6.14652379e-07
Iter: 189 loss: 6.11957148e-07
Iter: 190 loss: 6.14678e-07
Iter: 191 loss: 6.10453299e-07
Iter: 192 loss: 6.08204687e-07
Iter: 193 loss: 6.03422848e-07
Iter: 194 loss: 6.79668801e-07
Iter: 195 loss: 6.03236117e-07
Iter: 196 loss: 5.9904238e-07
Iter: 197 loss: 5.9904346e-07
Iter: 198 loss: 5.94816925e-07
Iter: 199 loss: 6.06069591e-07
Iter: 200 loss: 5.93387881e-07
Iter: 201 loss: 5.89724664e-07
Iter: 202 loss: 5.97689223e-07
Iter: 203 loss: 5.88326202e-07
Iter: 204 loss: 5.8608191e-07
Iter: 205 loss: 6.19142327e-07
Iter: 206 loss: 5.8607327e-07
Iter: 207 loss: 5.84283157e-07
Iter: 208 loss: 5.7941304e-07
Iter: 209 loss: 6.11711812e-07
Iter: 210 loss: 5.78198637e-07
Iter: 211 loss: 5.76484467e-07
Iter: 212 loss: 5.75823435e-07
Iter: 213 loss: 5.73308625e-07
Iter: 214 loss: 5.72509066e-07
Iter: 215 loss: 5.71017608e-07
Iter: 216 loss: 5.68319479e-07
Iter: 217 loss: 5.72388217e-07
Iter: 218 loss: 5.67024131e-07
Iter: 219 loss: 5.639493e-07
Iter: 220 loss: 5.6373085e-07
Iter: 221 loss: 5.61474792e-07
Iter: 222 loss: 5.57709768e-07
Iter: 223 loss: 5.74904789e-07
Iter: 224 loss: 5.5696114e-07
Iter: 225 loss: 5.5534e-07
Iter: 226 loss: 5.54794951e-07
Iter: 227 loss: 5.53261202e-07
Iter: 228 loss: 5.50377536e-07
Iter: 229 loss: 6.12602378e-07
Iter: 230 loss: 5.50339394e-07
Iter: 231 loss: 5.47227728e-07
Iter: 232 loss: 5.48555363e-07
Iter: 233 loss: 5.45092803e-07
Iter: 234 loss: 5.41472218e-07
Iter: 235 loss: 5.58387e-07
Iter: 236 loss: 5.40799192e-07
Iter: 237 loss: 5.36694643e-07
Iter: 238 loss: 5.70381417e-07
Iter: 239 loss: 5.36470452e-07
Iter: 240 loss: 5.35162e-07
Iter: 241 loss: 5.34970354e-07
Iter: 242 loss: 5.34047444e-07
Iter: 243 loss: 5.31281898e-07
Iter: 244 loss: 5.3501833e-07
Iter: 245 loss: 5.29889803e-07
Iter: 246 loss: 5.27857196e-07
Iter: 247 loss: 5.28921362e-07
Iter: 248 loss: 5.26501196e-07
Iter: 249 loss: 5.23787e-07
Iter: 250 loss: 5.38051154e-07
Iter: 251 loss: 5.23369579e-07
Iter: 252 loss: 5.20214712e-07
Iter: 253 loss: 5.23993322e-07
Iter: 254 loss: 5.18570744e-07
Iter: 255 loss: 5.15562249e-07
Iter: 256 loss: 5.13908503e-07
Iter: 257 loss: 5.12600536e-07
Iter: 258 loss: 5.10050654e-07
Iter: 259 loss: 5.32061847e-07
Iter: 260 loss: 5.09900303e-07
Iter: 261 loss: 5.07497248e-07
Iter: 262 loss: 5.08447329e-07
Iter: 263 loss: 5.05833668e-07
Iter: 264 loss: 5.06943252e-07
Iter: 265 loss: 5.04802301e-07
Iter: 266 loss: 5.04201353e-07
Iter: 267 loss: 5.02592627e-07
Iter: 268 loss: 5.16289674e-07
Iter: 269 loss: 5.02332966e-07
Iter: 270 loss: 5.00432634e-07
Iter: 271 loss: 4.99051339e-07
Iter: 272 loss: 4.98394343e-07
Iter: 273 loss: 4.96953646e-07
Iter: 274 loss: 4.96571715e-07
Iter: 275 loss: 4.95131303e-07
Iter: 276 loss: 4.98938334e-07
Iter: 277 loss: 4.9467269e-07
Iter: 278 loss: 4.93452262e-07
Iter: 279 loss: 4.91159767e-07
Iter: 280 loss: 5.42343628e-07
Iter: 281 loss: 4.9113703e-07
Iter: 282 loss: 4.87931175e-07
Iter: 283 loss: 5.2031055e-07
Iter: 284 loss: 4.87805e-07
Iter: 285 loss: 4.86255544e-07
Iter: 286 loss: 4.83575491e-07
Iter: 287 loss: 4.8358072e-07
Iter: 288 loss: 4.82491714e-07
Iter: 289 loss: 4.82232565e-07
Iter: 290 loss: 4.80871279e-07
Iter: 291 loss: 4.80150391e-07
Iter: 292 loss: 4.79541882e-07
Iter: 293 loss: 4.77779849e-07
Iter: 294 loss: 4.79618166e-07
Iter: 295 loss: 4.76778382e-07
Iter: 296 loss: 4.75200522e-07
Iter: 297 loss: 4.81795325e-07
Iter: 298 loss: 4.74904766e-07
Iter: 299 loss: 4.73490417e-07
Iter: 300 loss: 4.75774357e-07
Iter: 301 loss: 4.72848058e-07
Iter: 302 loss: 4.70612918e-07
Iter: 303 loss: 4.85309499e-07
Iter: 304 loss: 4.703694e-07
Iter: 305 loss: 4.69476049e-07
Iter: 306 loss: 4.69077e-07
Iter: 307 loss: 4.68653809e-07
Iter: 308 loss: 4.67100904e-07
Iter: 309 loss: 4.64064442e-07
Iter: 310 loss: 5.21814627e-07
Iter: 311 loss: 4.6402053e-07
Iter: 312 loss: 4.65050846e-07
Iter: 313 loss: 4.62479278e-07
Iter: 314 loss: 4.61829984e-07
Iter: 315 loss: 4.60515594e-07
Iter: 316 loss: 4.85943133e-07
Iter: 317 loss: 4.60527986e-07
Iter: 318 loss: 4.58947881e-07
Iter: 319 loss: 4.68304449e-07
Iter: 320 loss: 4.58760269e-07
Iter: 321 loss: 4.57143813e-07
Iter: 322 loss: 4.60572352e-07
Iter: 323 loss: 4.56523566e-07
Iter: 324 loss: 4.55338977e-07
Iter: 325 loss: 4.5441e-07
Iter: 326 loss: 4.54022086e-07
Iter: 327 loss: 4.5262459e-07
Iter: 328 loss: 4.52619133e-07
Iter: 329 loss: 4.51410671e-07
Iter: 330 loss: 4.48732862e-07
Iter: 331 loss: 4.84931604e-07
Iter: 332 loss: 4.48579442e-07
Iter: 333 loss: 4.46340721e-07
Iter: 334 loss: 4.62364255e-07
Iter: 335 loss: 4.46136596e-07
Iter: 336 loss: 4.4463809e-07
Iter: 337 loss: 4.67454072e-07
Iter: 338 loss: 4.44651505e-07
Iter: 339 loss: 4.43108377e-07
Iter: 340 loss: 4.44828089e-07
Iter: 341 loss: 4.42308931e-07
Iter: 342 loss: 4.41462646e-07
Iter: 343 loss: 4.40673517e-07
Iter: 344 loss: 4.40462628e-07
Iter: 345 loss: 4.39572943e-07
Iter: 346 loss: 4.50245892e-07
Iter: 347 loss: 4.39574308e-07
Iter: 348 loss: 4.38652393e-07
Iter: 349 loss: 4.4278832e-07
Iter: 350 loss: 4.38481862e-07
Iter: 351 loss: 4.3767011e-07
Iter: 352 loss: 4.37053473e-07
Iter: 353 loss: 4.36785399e-07
Iter: 354 loss: 4.35884374e-07
Iter: 355 loss: 4.42897289e-07
Iter: 356 loss: 4.35811756e-07
Iter: 357 loss: 4.34722097e-07
Iter: 358 loss: 4.33835254e-07
Iter: 359 loss: 4.33517982e-07
Iter: 360 loss: 4.32053e-07
Iter: 361 loss: 4.33197101e-07
Iter: 362 loss: 4.31161084e-07
Iter: 363 loss: 4.29599822e-07
Iter: 364 loss: 4.29592092e-07
Iter: 365 loss: 4.28634593e-07
Iter: 366 loss: 4.27510287e-07
Iter: 367 loss: 4.27391313e-07
Iter: 368 loss: 4.2579029e-07
Iter: 369 loss: 4.2891287e-07
Iter: 370 loss: 4.25123034e-07
Iter: 371 loss: 4.24921893e-07
Iter: 372 loss: 4.24484654e-07
Iter: 373 loss: 4.23898967e-07
Iter: 374 loss: 4.23176175e-07
Iter: 375 loss: 4.23093127e-07
Iter: 376 loss: 4.22493741e-07
Iter: 377 loss: 4.21675225e-07
Iter: 378 loss: 4.21606245e-07
Iter: 379 loss: 4.20600031e-07
Iter: 380 loss: 4.36752572e-07
Iter: 381 loss: 4.20597132e-07
Iter: 382 loss: 4.19761932e-07
Iter: 383 loss: 4.24948439e-07
Iter: 384 loss: 4.19691531e-07
Iter: 385 loss: 4.19026378e-07
Iter: 386 loss: 4.17347962e-07
Iter: 387 loss: 4.30711111e-07
Iter: 388 loss: 4.17040752e-07
Iter: 389 loss: 4.16693553e-07
Iter: 390 loss: 4.16095475e-07
Iter: 391 loss: 4.15434414e-07
Iter: 392 loss: 4.14047406e-07
Iter: 393 loss: 4.40928147e-07
Iter: 394 loss: 4.14033366e-07
Iter: 395 loss: 4.12484695e-07
Iter: 396 loss: 4.20606227e-07
Iter: 397 loss: 4.12276364e-07
Iter: 398 loss: 4.11456824e-07
Iter: 399 loss: 4.1145455e-07
Iter: 400 loss: 4.10798691e-07
Iter: 401 loss: 4.10086329e-07
Iter: 402 loss: 4.09958403e-07
Iter: 403 loss: 4.09020288e-07
Iter: 404 loss: 4.0892354e-07
Iter: 405 loss: 4.08227265e-07
Iter: 406 loss: 4.08267795e-07
Iter: 407 loss: 4.07660821e-07
Iter: 408 loss: 4.07221023e-07
Iter: 409 loss: 4.06275262e-07
Iter: 410 loss: 4.21556535e-07
Iter: 411 loss: 4.06242634e-07
Iter: 412 loss: 4.05149933e-07
Iter: 413 loss: 4.05257083e-07
Iter: 414 loss: 4.04289892e-07
Iter: 415 loss: 4.03853733e-07
Iter: 416 loss: 4.03688659e-07
Iter: 417 loss: 4.03052837e-07
Iter: 418 loss: 4.01883796e-07
Iter: 419 loss: 4.01888201e-07
Iter: 420 loss: 4.00971373e-07
Iter: 421 loss: 4.08678773e-07
Iter: 422 loss: 4.00940849e-07
Iter: 423 loss: 4.00320715e-07
Iter: 424 loss: 4.04474633e-07
Iter: 425 loss: 4.00258699e-07
Iter: 426 loss: 3.99618671e-07
Iter: 427 loss: 3.9874152e-07
Iter: 428 loss: 3.98699228e-07
Iter: 429 loss: 3.9789353e-07
Iter: 430 loss: 4.00574436e-07
Iter: 431 loss: 3.97664621e-07
Iter: 432 loss: 3.96847554e-07
Iter: 433 loss: 4.06671091e-07
Iter: 434 loss: 3.96830075e-07
Iter: 435 loss: 3.96451156e-07
Iter: 436 loss: 3.96233418e-07
Iter: 437 loss: 3.96064195e-07
Iter: 438 loss: 3.95374173e-07
Iter: 439 loss: 3.95252243e-07
Iter: 440 loss: 3.94794938e-07
Iter: 441 loss: 3.94019821e-07
Iter: 442 loss: 3.93968435e-07
Iter: 443 loss: 3.93516473e-07
Iter: 444 loss: 3.92419111e-07
Iter: 445 loss: 4.04868672e-07
Iter: 446 loss: 3.92309744e-07
Iter: 447 loss: 3.91311517e-07
Iter: 448 loss: 3.94605081e-07
Iter: 449 loss: 3.91058961e-07
Iter: 450 loss: 3.8997706e-07
Iter: 451 loss: 4.01596708e-07
Iter: 452 loss: 3.89931e-07
Iter: 453 loss: 3.89126541e-07
Iter: 454 loss: 3.88819842e-07
Iter: 455 loss: 3.88375327e-07
Iter: 456 loss: 3.87687578e-07
Iter: 457 loss: 3.89537888e-07
Iter: 458 loss: 3.87465633e-07
Iter: 459 loss: 3.86657632e-07
Iter: 460 loss: 3.92439858e-07
Iter: 461 loss: 3.86592745e-07
Iter: 462 loss: 3.8607476e-07
Iter: 463 loss: 3.85274e-07
Iter: 464 loss: 3.85242117e-07
Iter: 465 loss: 3.84516511e-07
Iter: 466 loss: 3.95209469e-07
Iter: 467 loss: 3.84511537e-07
Iter: 468 loss: 3.84008928e-07
Iter: 469 loss: 3.87581395e-07
Iter: 470 loss: 3.83961435e-07
Iter: 471 loss: 3.83564583e-07
Iter: 472 loss: 3.82604924e-07
Iter: 473 loss: 3.95370591e-07
Iter: 474 loss: 3.82564338e-07
Iter: 475 loss: 3.82269945e-07
Iter: 476 loss: 3.82119907e-07
Iter: 477 loss: 3.81668769e-07
Iter: 478 loss: 3.81968675e-07
Iter: 479 loss: 3.81377959e-07
Iter: 480 loss: 3.80855113e-07
Iter: 481 loss: 3.79821643e-07
Iter: 482 loss: 4.0214789e-07
Iter: 483 loss: 3.79824e-07
Iter: 484 loss: 3.7853863e-07
Iter: 485 loss: 3.82517499e-07
Iter: 486 loss: 3.78174974e-07
Iter: 487 loss: 3.77337614e-07
Iter: 488 loss: 3.77246522e-07
Iter: 489 loss: 3.76830343e-07
Iter: 490 loss: 3.75878869e-07
Iter: 491 loss: 3.8824146e-07
Iter: 492 loss: 3.75809861e-07
Iter: 493 loss: 3.74912787e-07
Iter: 494 loss: 3.84242981e-07
Iter: 495 loss: 3.74887236e-07
Iter: 496 loss: 3.74071391e-07
Iter: 497 loss: 3.77748734e-07
Iter: 498 loss: 3.73906943e-07
Iter: 499 loss: 3.73340697e-07
Iter: 500 loss: 3.73511114e-07
Iter: 501 loss: 3.72956094e-07
Iter: 502 loss: 3.72418697e-07
Iter: 503 loss: 3.73674197e-07
Iter: 504 loss: 3.7219985e-07
Iter: 505 loss: 3.71503091e-07
Iter: 506 loss: 3.765511e-07
Iter: 507 loss: 3.71433e-07
Iter: 508 loss: 3.71123292e-07
Iter: 509 loss: 3.70515693e-07
Iter: 510 loss: 3.81983881e-07
Iter: 511 loss: 3.70498043e-07
Iter: 512 loss: 3.69931627e-07
Iter: 513 loss: 3.69899169e-07
Iter: 514 loss: 3.69432485e-07
Iter: 515 loss: 3.69066242e-07
Iter: 516 loss: 3.68898498e-07
Iter: 517 loss: 3.68380569e-07
Iter: 518 loss: 3.6822567e-07
Iter: 519 loss: 3.67924599e-07
Iter: 520 loss: 3.67374895e-07
Iter: 521 loss: 3.75690121e-07
Iter: 522 loss: 3.67366908e-07
Iter: 523 loss: 3.66787901e-07
Iter: 524 loss: 3.66897552e-07
Iter: 525 loss: 3.6632548e-07
Iter: 526 loss: 3.65927121e-07
Iter: 527 loss: 3.66615666e-07
Iter: 528 loss: 3.65761707e-07
Iter: 529 loss: 3.65268477e-07
Iter: 530 loss: 3.66311951e-07
Iter: 531 loss: 3.65060771e-07
Iter: 532 loss: 3.64356282e-07
Iter: 533 loss: 3.6689795e-07
Iter: 534 loss: 3.64199536e-07
Iter: 535 loss: 3.63794499e-07
Iter: 536 loss: 3.62817445e-07
Iter: 537 loss: 3.732751e-07
Iter: 538 loss: 3.62709045e-07
Iter: 539 loss: 3.62125775e-07
Iter: 540 loss: 3.61966329e-07
Iter: 541 loss: 3.61390505e-07
Iter: 542 loss: 3.60326595e-07
Iter: 543 loss: 3.85056836e-07
Iter: 544 loss: 3.60334326e-07
Iter: 545 loss: 3.59549375e-07
Iter: 546 loss: 3.59556566e-07
Iter: 547 loss: 3.5903247e-07
Iter: 548 loss: 3.65892845e-07
Iter: 549 loss: 3.59051597e-07
Iter: 550 loss: 3.58830846e-07
Iter: 551 loss: 3.58326417e-07
Iter: 552 loss: 3.64507116e-07
Iter: 553 loss: 3.5826676e-07
Iter: 554 loss: 3.57866867e-07
Iter: 555 loss: 3.61946462e-07
Iter: 556 loss: 3.57866412e-07
Iter: 557 loss: 3.57584696e-07
Iter: 558 loss: 3.60943574e-07
Iter: 559 loss: 3.57581882e-07
Iter: 560 loss: 3.57393731e-07
Iter: 561 loss: 3.57023453e-07
Iter: 562 loss: 3.64874836e-07
Iter: 563 loss: 3.57013619e-07
Iter: 564 loss: 3.565589e-07
Iter: 565 loss: 3.57184433e-07
Iter: 566 loss: 3.56303076e-07
Iter: 567 loss: 3.55908298e-07
Iter: 568 loss: 3.55898578e-07
Iter: 569 loss: 3.5557224e-07
Iter: 570 loss: 3.54801102e-07
Iter: 571 loss: 3.63791457e-07
Iter: 572 loss: 3.54729906e-07
Iter: 573 loss: 3.53915169e-07
Iter: 574 loss: 3.59600904e-07
Iter: 575 loss: 3.53843916e-07
Iter: 576 loss: 3.53306461e-07
Iter: 577 loss: 3.5944413e-07
Iter: 578 loss: 3.53304188e-07
Iter: 579 loss: 3.52781825e-07
Iter: 580 loss: 3.527052e-07
Iter: 581 loss: 3.52372751e-07
Iter: 582 loss: 3.51945147e-07
Iter: 583 loss: 3.53644054e-07
Iter: 584 loss: 3.51810627e-07
Iter: 585 loss: 3.51290737e-07
Iter: 586 loss: 3.55042459e-07
Iter: 587 loss: 3.51251344e-07
Iter: 588 loss: 3.51013966e-07
Iter: 589 loss: 3.50412563e-07
Iter: 590 loss: 3.53567316e-07
Iter: 591 loss: 3.50203806e-07
Iter: 592 loss: 3.49703754e-07
Iter: 593 loss: 3.49692641e-07
Iter: 594 loss: 3.49164765e-07
Iter: 595 loss: 3.50450705e-07
Iter: 596 loss: 3.48961976e-07
Iter: 597 loss: 3.48499611e-07
Iter: 598 loss: 3.47949481e-07
Iter: 599 loss: 3.47919666e-07
Iter: 600 loss: 3.47309623e-07
Iter: 601 loss: 3.51541303e-07
Iter: 602 loss: 3.47272078e-07
Iter: 603 loss: 3.4669884e-07
Iter: 604 loss: 3.50672281e-07
Iter: 605 loss: 3.46642906e-07
Iter: 606 loss: 3.4637813e-07
Iter: 607 loss: 3.46013508e-07
Iter: 608 loss: 3.45996625e-07
Iter: 609 loss: 3.45556202e-07
Iter: 610 loss: 3.49404701e-07
Iter: 611 loss: 3.4552437e-07
Iter: 612 loss: 3.45346706e-07
Iter: 613 loss: 3.45336957e-07
Iter: 614 loss: 3.45152557e-07
Iter: 615 loss: 3.44585033e-07
Iter: 616 loss: 3.47231037e-07
Iter: 617 loss: 3.44382642e-07
Iter: 618 loss: 3.44553371e-07
Iter: 619 loss: 3.44103256e-07
Iter: 620 loss: 3.43850331e-07
Iter: 621 loss: 3.43340417e-07
Iter: 622 loss: 3.51555258e-07
Iter: 623 loss: 3.43334563e-07
Iter: 624 loss: 3.42634081e-07
Iter: 625 loss: 3.42531962e-07
Iter: 626 loss: 3.42041062e-07
Iter: 627 loss: 3.41714411e-07
Iter: 628 loss: 3.41532427e-07
Iter: 629 loss: 3.41065942e-07
Iter: 630 loss: 3.40655e-07
Iter: 631 loss: 3.40534086e-07
Iter: 632 loss: 3.40111342e-07
Iter: 633 loss: 3.3986791e-07
Iter: 634 loss: 3.39684874e-07
Iter: 635 loss: 3.39233736e-07
Iter: 636 loss: 3.3921421e-07
Iter: 637 loss: 3.38812129e-07
Iter: 638 loss: 3.38655042e-07
Iter: 639 loss: 3.38447478e-07
Iter: 640 loss: 3.38060858e-07
Iter: 641 loss: 3.3951406e-07
Iter: 642 loss: 3.37967379e-07
Iter: 643 loss: 3.37692939e-07
Iter: 644 loss: 3.37884273e-07
Iter: 645 loss: 3.37513086e-07
Iter: 646 loss: 3.36996152e-07
Iter: 647 loss: 3.38762675e-07
Iter: 648 loss: 3.36862882e-07
Iter: 649 loss: 3.36575056e-07
Iter: 650 loss: 3.37090967e-07
Iter: 651 loss: 3.36439541e-07
Iter: 652 loss: 3.36218619e-07
Iter: 653 loss: 3.39601456e-07
Iter: 654 loss: 3.36224957e-07
Iter: 655 loss: 3.35979848e-07
Iter: 656 loss: 3.35580779e-07
Iter: 657 loss: 3.35588027e-07
Iter: 658 loss: 3.35144478e-07
Iter: 659 loss: 3.3519774e-07
Iter: 660 loss: 3.34821266e-07
Iter: 661 loss: 3.34760728e-07
Iter: 662 loss: 3.34584e-07
Iter: 663 loss: 3.3439926e-07
Iter: 664 loss: 3.33912169e-07
Iter: 665 loss: 3.39148556e-07
Iter: 666 loss: 3.33854e-07
Iter: 667 loss: 3.33310851e-07
Iter: 668 loss: 3.3533226e-07
Iter: 669 loss: 3.33181788e-07
Iter: 670 loss: 3.32842e-07
Iter: 671 loss: 3.37670571e-07
Iter: 672 loss: 3.32860452e-07
Iter: 673 loss: 3.32545653e-07
Iter: 674 loss: 3.32478237e-07
Iter: 675 loss: 3.3225831e-07
Iter: 676 loss: 3.31912645e-07
Iter: 677 loss: 3.31517299e-07
Iter: 678 loss: 3.31476258e-07
Iter: 679 loss: 3.31127353e-07
Iter: 680 loss: 3.31084493e-07
Iter: 681 loss: 3.30778022e-07
Iter: 682 loss: 3.31179592e-07
Iter: 683 loss: 3.30635146e-07
Iter: 684 loss: 3.3034064e-07
Iter: 685 loss: 3.30584555e-07
Iter: 686 loss: 3.30150442e-07
Iter: 687 loss: 3.29822541e-07
Iter: 688 loss: 3.34780424e-07
Iter: 689 loss: 3.29825042e-07
Iter: 690 loss: 3.29687765e-07
Iter: 691 loss: 3.2924828e-07
Iter: 692 loss: 3.32334082e-07
Iter: 693 loss: 3.29135617e-07
Iter: 694 loss: 3.28788815e-07
Iter: 695 loss: 3.2877773e-07
Iter: 696 loss: 3.28499169e-07
Iter: 697 loss: 3.29965019e-07
Iter: 698 loss: 3.28440137e-07
Iter: 699 loss: 3.28206596e-07
Iter: 700 loss: 3.27677299e-07
Iter: 701 loss: 3.36000539e-07
Iter: 702 loss: 3.2765945e-07
Iter: 703 loss: 3.27239889e-07
Iter: 704 loss: 3.29317061e-07
Iter: 705 loss: 3.27172359e-07
Iter: 706 loss: 3.26799665e-07
Iter: 707 loss: 3.26811062e-07
Iter: 708 loss: 3.26606397e-07
Iter: 709 loss: 3.26226541e-07
Iter: 710 loss: 3.34144289e-07
Iter: 711 loss: 3.26241889e-07
Iter: 712 loss: 3.25746527e-07
Iter: 713 loss: 3.27064583e-07
Iter: 714 loss: 3.25580828e-07
Iter: 715 loss: 3.25200489e-07
Iter: 716 loss: 3.31098363e-07
Iter: 717 loss: 3.25184317e-07
Iter: 718 loss: 3.24834076e-07
Iter: 719 loss: 3.24541702e-07
Iter: 720 loss: 3.24447484e-07
Iter: 721 loss: 3.24223151e-07
Iter: 722 loss: 3.24191433e-07
Iter: 723 loss: 3.23916481e-07
Iter: 724 loss: 3.23329658e-07
Iter: 725 loss: 3.32330188e-07
Iter: 726 loss: 3.23312e-07
Iter: 727 loss: 3.22826338e-07
Iter: 728 loss: 3.25909525e-07
Iter: 729 loss: 3.22778305e-07
Iter: 730 loss: 3.22500512e-07
Iter: 731 loss: 3.25954204e-07
Iter: 732 loss: 3.22488347e-07
Iter: 733 loss: 3.22201458e-07
Iter: 734 loss: 3.22028427e-07
Iter: 735 loss: 3.21903087e-07
Iter: 736 loss: 3.21624498e-07
Iter: 737 loss: 3.22043263e-07
Iter: 738 loss: 3.21502938e-07
Iter: 739 loss: 3.21272466e-07
Iter: 740 loss: 3.23545294e-07
Iter: 741 loss: 3.21272239e-07
Iter: 742 loss: 3.21011441e-07
Iter: 743 loss: 3.21308903e-07
Iter: 744 loss: 3.20905627e-07
Iter: 745 loss: 3.20645029e-07
Iter: 746 loss: 3.20712957e-07
Iter: 747 loss: 3.20472736e-07
Iter: 748 loss: 3.20255168e-07
Iter: 749 loss: 3.2098086e-07
Iter: 750 loss: 3.20189031e-07
Iter: 751 loss: 3.19839899e-07
Iter: 752 loss: 3.20798506e-07
Iter: 753 loss: 3.19735307e-07
Iter: 754 loss: 3.19479796e-07
Iter: 755 loss: 3.20007842e-07
Iter: 756 loss: 3.1939453e-07
Iter: 757 loss: 3.19123387e-07
Iter: 758 loss: 3.21080108e-07
Iter: 759 loss: 3.19086041e-07
Iter: 760 loss: 3.18888198e-07
Iter: 761 loss: 3.18532358e-07
Iter: 762 loss: 3.26918212e-07
Iter: 763 loss: 3.18525679e-07
Iter: 764 loss: 3.18120101e-07
Iter: 765 loss: 3.19650894e-07
Iter: 766 loss: 3.18043703e-07
Iter: 767 loss: 3.17653274e-07
Iter: 768 loss: 3.21851019e-07
Iter: 769 loss: 3.17624938e-07
Iter: 770 loss: 3.1748516e-07
Iter: 771 loss: 3.17286947e-07
Iter: 772 loss: 3.17262106e-07
Iter: 773 loss: 3.16917408e-07
Iter: 774 loss: 3.17131111e-07
Iter: 775 loss: 3.16712288e-07
Iter: 776 loss: 3.16420625e-07
Iter: 777 loss: 3.16416958e-07
Iter: 778 loss: 3.161785e-07
Iter: 779 loss: 3.15897239e-07
Iter: 780 loss: 3.15860632e-07
Iter: 781 loss: 3.15575846e-07
Iter: 782 loss: 3.15770535e-07
Iter: 783 loss: 3.15378543e-07
Iter: 784 loss: 3.15088869e-07
Iter: 785 loss: 3.15094e-07
Iter: 786 loss: 3.14861296e-07
Iter: 787 loss: 3.15007298e-07
Iter: 788 loss: 3.14723707e-07
Iter: 789 loss: 3.14502898e-07
Iter: 790 loss: 3.16289089e-07
Iter: 791 loss: 3.14504234e-07
Iter: 792 loss: 3.14292521e-07
Iter: 793 loss: 3.14150725e-07
Iter: 794 loss: 3.14076203e-07
Iter: 795 loss: 3.13830526e-07
Iter: 796 loss: 3.13723035e-07
Iter: 797 loss: 3.13612077e-07
Iter: 798 loss: 3.13424664e-07
Iter: 799 loss: 3.13394679e-07
Iter: 800 loss: 3.1326104e-07
Iter: 801 loss: 3.12964119e-07
Iter: 802 loss: 3.18440868e-07
Iter: 803 loss: 3.12943484e-07
Iter: 804 loss: 3.12590515e-07
Iter: 805 loss: 3.13074281e-07
Iter: 806 loss: 3.12405859e-07
Iter: 807 loss: 3.12190025e-07
Iter: 808 loss: 3.12178742e-07
Iter: 809 loss: 3.11940198e-07
Iter: 810 loss: 3.11887163e-07
Iter: 811 loss: 3.11743122e-07
Iter: 812 loss: 3.11421502e-07
Iter: 813 loss: 3.11158033e-07
Iter: 814 loss: 3.11073904e-07
Iter: 815 loss: 3.10899452e-07
Iter: 816 loss: 3.10852e-07
Iter: 817 loss: 3.10644566e-07
Iter: 818 loss: 3.10804865e-07
Iter: 819 loss: 3.10517805e-07
Iter: 820 loss: 3.10263033e-07
Iter: 821 loss: 3.10935036e-07
Iter: 822 loss: 3.10164523e-07
Iter: 823 loss: 3.09914867e-07
Iter: 824 loss: 3.12165099e-07
Iter: 825 loss: 3.09891846e-07
Iter: 826 loss: 3.09782422e-07
Iter: 827 loss: 3.09542287e-07
Iter: 828 loss: 3.12937232e-07
Iter: 829 loss: 3.0954061e-07
Iter: 830 loss: 3.09396682e-07
Iter: 831 loss: 3.09370506e-07
Iter: 832 loss: 3.09225754e-07
Iter: 833 loss: 3.0903135e-07
Iter: 834 loss: 3.09021971e-07
Iter: 835 loss: 3.08774702e-07
Iter: 836 loss: 3.0910121e-07
Iter: 837 loss: 3.08661583e-07
Iter: 838 loss: 3.08419033e-07
Iter: 839 loss: 3.08977576e-07
Iter: 840 loss: 3.08310348e-07
Iter: 841 loss: 3.08088943e-07
Iter: 842 loss: 3.08080814e-07
Iter: 843 loss: 3.07941519e-07
Iter: 844 loss: 3.07578603e-07
Iter: 845 loss: 3.1126666e-07
Iter: 846 loss: 3.07545292e-07
Iter: 847 loss: 3.07198576e-07
Iter: 848 loss: 3.1100052e-07
Iter: 849 loss: 3.07194369e-07
Iter: 850 loss: 3.06966371e-07
Iter: 851 loss: 3.07284097e-07
Iter: 852 loss: 3.06845124e-07
Iter: 853 loss: 3.06544e-07
Iter: 854 loss: 3.06729504e-07
Iter: 855 loss: 3.06347943e-07
Iter: 856 loss: 3.06056506e-07
Iter: 857 loss: 3.08416901e-07
Iter: 858 loss: 3.06018279e-07
Iter: 859 loss: 3.05802814e-07
Iter: 860 loss: 3.05564868e-07
Iter: 861 loss: 3.05500265e-07
Iter: 862 loss: 3.05248818e-07
Iter: 863 loss: 3.07467701e-07
Iter: 864 loss: 3.05232106e-07
Iter: 865 loss: 3.05059643e-07
Iter: 866 loss: 3.06822756e-07
Iter: 867 loss: 3.05058535e-07
Iter: 868 loss: 3.04923134e-07
Iter: 869 loss: 3.04651905e-07
Iter: 870 loss: 3.07572e-07
Iter: 871 loss: 3.04605805e-07
Iter: 872 loss: 3.04311754e-07
Iter: 873 loss: 3.0629721e-07
Iter: 874 loss: 3.04290609e-07
Iter: 875 loss: 3.04085688e-07
Iter: 876 loss: 3.05779281e-07
Iter: 877 loss: 3.0405289e-07
Iter: 878 loss: 3.03827278e-07
Iter: 879 loss: 3.04209493e-07
Iter: 880 loss: 3.03731e-07
Iter: 881 loss: 3.03514042e-07
Iter: 882 loss: 3.03162977e-07
Iter: 883 loss: 3.03141064e-07
Iter: 884 loss: 3.03030731e-07
Iter: 885 loss: 3.02926878e-07
Iter: 886 loss: 3.02810292e-07
Iter: 887 loss: 3.02794348e-07
Iter: 888 loss: 3.02686487e-07
Iter: 889 loss: 3.02503963e-07
Iter: 890 loss: 3.03510831e-07
Iter: 891 loss: 3.02453515e-07
Iter: 892 loss: 3.02260588e-07
Iter: 893 loss: 3.02310099e-07
Iter: 894 loss: 3.0210586e-07
Iter: 895 loss: 3.019e-07
Iter: 896 loss: 3.02322121e-07
Iter: 897 loss: 3.01835371e-07
Iter: 898 loss: 3.01622208e-07
Iter: 899 loss: 3.03082061e-07
Iter: 900 loss: 3.0160183e-07
Iter: 901 loss: 3.0137528e-07
Iter: 902 loss: 3.01213305e-07
Iter: 903 loss: 3.01121474e-07
Iter: 904 loss: 3.00855334e-07
Iter: 905 loss: 3.01153648e-07
Iter: 906 loss: 3.00692875e-07
Iter: 907 loss: 3.00404253e-07
Iter: 908 loss: 3.01180535e-07
Iter: 909 loss: 3.00296932e-07
Iter: 910 loss: 2.99901245e-07
Iter: 911 loss: 3.02564843e-07
Iter: 912 loss: 2.99877058e-07
Iter: 913 loss: 2.99634e-07
Iter: 914 loss: 2.99843578e-07
Iter: 915 loss: 2.99489841e-07
Iter: 916 loss: 2.99291798e-07
Iter: 917 loss: 3.00247564e-07
Iter: 918 loss: 2.99248171e-07
Iter: 919 loss: 2.99041943e-07
Iter: 920 loss: 3.00350052e-07
Iter: 921 loss: 2.99036969e-07
Iter: 922 loss: 2.98911175e-07
Iter: 923 loss: 2.99316895e-07
Iter: 924 loss: 2.9887957e-07
Iter: 925 loss: 2.98735046e-07
Iter: 926 loss: 2.98915211e-07
Iter: 927 loss: 2.98630368e-07
Iter: 928 loss: 2.98545046e-07
Iter: 929 loss: 2.98739764e-07
Iter: 930 loss: 2.98478028e-07
Iter: 931 loss: 2.9835212e-07
Iter: 932 loss: 2.98929706e-07
Iter: 933 loss: 2.98343593e-07
Iter: 934 loss: 2.9820751e-07
Iter: 935 loss: 2.98191395e-07
Iter: 936 loss: 2.98080096e-07
Iter: 937 loss: 2.97907434e-07
Iter: 938 loss: 2.97885e-07
Iter: 939 loss: 2.97777319e-07
Iter: 940 loss: 2.97482075e-07
Iter: 941 loss: 2.97682902e-07
Iter: 942 loss: 2.9732206e-07
Iter: 943 loss: 2.97054299e-07
Iter: 944 loss: 2.97056431e-07
Iter: 945 loss: 2.96786851e-07
Iter: 946 loss: 2.96576786e-07
Iter: 947 loss: 2.96493113e-07
Iter: 948 loss: 2.96249482e-07
Iter: 949 loss: 2.96942801e-07
Iter: 950 loss: 2.96157509e-07
Iter: 951 loss: 2.95847059e-07
Iter: 952 loss: 2.98645261e-07
Iter: 953 loss: 2.95826624e-07
Iter: 954 loss: 2.95613518e-07
Iter: 955 loss: 2.95938321e-07
Iter: 956 loss: 2.95504151e-07
Iter: 957 loss: 2.95328732e-07
Iter: 958 loss: 2.97621625e-07
Iter: 959 loss: 2.95321399e-07
Iter: 960 loss: 2.95215671e-07
Iter: 961 loss: 2.95096413e-07
Iter: 962 loss: 2.95077143e-07
Iter: 963 loss: 2.94929237e-07
Iter: 964 loss: 2.95980897e-07
Iter: 965 loss: 2.94918038e-07
Iter: 966 loss: 2.94752653e-07
Iter: 967 loss: 2.94925655e-07
Iter: 968 loss: 2.94663778e-07
Iter: 969 loss: 2.9448637e-07
Iter: 970 loss: 2.94435893e-07
Iter: 971 loss: 2.94337667e-07
Iter: 972 loss: 2.94089773e-07
Iter: 973 loss: 2.93957982e-07
Iter: 974 loss: 2.93815e-07
Iter: 975 loss: 2.93598589e-07
Iter: 976 loss: 2.93575397e-07
Iter: 977 loss: 2.93392304e-07
Iter: 978 loss: 2.94216193e-07
Iter: 979 loss: 2.93370476e-07
Iter: 980 loss: 2.93240049e-07
Iter: 981 loss: 2.93035384e-07
Iter: 982 loss: 2.93052352e-07
Iter: 983 loss: 2.92906236e-07
Iter: 984 loss: 2.9289339e-07
Iter: 985 loss: 2.92754862e-07
Iter: 986 loss: 2.92749462e-07
Iter: 987 loss: 2.9263353e-07
Iter: 988 loss: 2.92505433e-07
Iter: 989 loss: 2.94413496e-07
Iter: 990 loss: 2.92498555e-07
Iter: 991 loss: 2.92393736e-07
Iter: 992 loss: 2.92194613e-07
Iter: 993 loss: 2.97071722e-07
Iter: 994 loss: 2.92193675e-07
Iter: 995 loss: 2.91943536e-07
Iter: 996 loss: 2.9240482e-07
Iter: 997 loss: 2.91807822e-07
Iter: 998 loss: 2.91557768e-07
Iter: 999 loss: 2.91559587e-07
Iter: 1000 loss: 2.91373283e-07
Iter: 1001 loss: 2.91055812e-07
Iter: 1002 loss: 2.97989487e-07
Iter: 1003 loss: 2.9105496e-07
Iter: 1004 loss: 2.9072271e-07
Iter: 1005 loss: 2.92077772e-07
Iter: 1006 loss: 2.90651542e-07
Iter: 1007 loss: 2.90404159e-07
Iter: 1008 loss: 2.91120102e-07
Iter: 1009 loss: 2.90308378e-07
Iter: 1010 loss: 2.90168458e-07
Iter: 1011 loss: 2.90149018e-07
Iter: 1012 loss: 2.9004994e-07
Iter: 1013 loss: 2.8985977e-07
Iter: 1014 loss: 2.94124447e-07
Iter: 1015 loss: 2.89840699e-07
Iter: 1016 loss: 2.89637825e-07
Iter: 1017 loss: 2.90787369e-07
Iter: 1018 loss: 2.89624325e-07
Iter: 1019 loss: 2.89473263e-07
Iter: 1020 loss: 2.9103046e-07
Iter: 1021 loss: 2.894663e-07
Iter: 1022 loss: 2.89349771e-07
Iter: 1023 loss: 2.89341244e-07
Iter: 1024 loss: 2.89230599e-07
Iter: 1025 loss: 2.89048501e-07
Iter: 1026 loss: 2.90406433e-07
Iter: 1027 loss: 2.89035853e-07
Iter: 1028 loss: 2.8893902e-07
Iter: 1029 loss: 2.88710396e-07
Iter: 1030 loss: 2.92019706e-07
Iter: 1031 loss: 2.88712272e-07
Iter: 1032 loss: 2.88596027e-07
Iter: 1033 loss: 2.88567975e-07
Iter: 1034 loss: 2.88436e-07
Iter: 1035 loss: 2.88312322e-07
Iter: 1036 loss: 2.88271536e-07
Iter: 1037 loss: 2.88053656e-07
Iter: 1038 loss: 2.88189511e-07
Iter: 1039 loss: 2.87943607e-07
Iter: 1040 loss: 2.87708133e-07
Iter: 1041 loss: 2.87430936e-07
Iter: 1042 loss: 2.87399416e-07
Iter: 1043 loss: 2.87187e-07
Iter: 1044 loss: 2.87156752e-07
Iter: 1045 loss: 2.86953593e-07
Iter: 1046 loss: 2.87890373e-07
Iter: 1047 loss: 2.86919658e-07
Iter: 1048 loss: 2.86762543e-07
Iter: 1049 loss: 2.86517889e-07
Iter: 1050 loss: 2.86505724e-07
Iter: 1051 loss: 2.86224292e-07
Iter: 1052 loss: 2.89197544e-07
Iter: 1053 loss: 2.86216675e-07
Iter: 1054 loss: 2.85949255e-07
Iter: 1055 loss: 2.87185685e-07
Iter: 1056 loss: 2.85903582e-07
Iter: 1057 loss: 2.85798535e-07
Iter: 1058 loss: 2.86647378e-07
Iter: 1059 loss: 2.8579035e-07
Iter: 1060 loss: 2.85665237e-07
Iter: 1061 loss: 2.85423312e-07
Iter: 1062 loss: 2.88398553e-07
Iter: 1063 loss: 2.85402223e-07
Iter: 1064 loss: 2.85185195e-07
Iter: 1065 loss: 2.86984857e-07
Iter: 1066 loss: 2.85198439e-07
Iter: 1067 loss: 2.85041665e-07
Iter: 1068 loss: 2.86638624e-07
Iter: 1069 loss: 2.85035071e-07
Iter: 1070 loss: 2.84928348e-07
Iter: 1071 loss: 2.84818299e-07
Iter: 1072 loss: 2.84793828e-07
Iter: 1073 loss: 2.84646489e-07
Iter: 1074 loss: 2.8441923e-07
Iter: 1075 loss: 2.84393508e-07
Iter: 1076 loss: 2.84149394e-07
Iter: 1077 loss: 2.87263163e-07
Iter: 1078 loss: 2.84131715e-07
Iter: 1079 loss: 2.83924095e-07
Iter: 1080 loss: 2.85464921e-07
Iter: 1081 loss: 2.83898544e-07
Iter: 1082 loss: 2.8370016e-07
Iter: 1083 loss: 2.83636837e-07
Iter: 1084 loss: 2.83528379e-07
Iter: 1085 loss: 2.83297652e-07
Iter: 1086 loss: 2.8358906e-07
Iter: 1087 loss: 2.83209744e-07
Iter: 1088 loss: 2.83093982e-07
Iter: 1089 loss: 2.83039014e-07
Iter: 1090 loss: 2.8294707e-07
Iter: 1091 loss: 2.82752694e-07
Iter: 1092 loss: 2.87356045e-07
Iter: 1093 loss: 2.8275403e-07
Iter: 1094 loss: 2.82528333e-07
Iter: 1095 loss: 2.85404553e-07
Iter: 1096 loss: 2.82541436e-07
Iter: 1097 loss: 2.82455289e-07
Iter: 1098 loss: 2.8228564e-07
Iter: 1099 loss: 2.85955764e-07
Iter: 1100 loss: 2.82286294e-07
Iter: 1101 loss: 2.82098142e-07
Iter: 1102 loss: 2.83506807e-07
Iter: 1103 loss: 2.82086262e-07
Iter: 1104 loss: 2.81877789e-07
Iter: 1105 loss: 2.82569403e-07
Iter: 1106 loss: 2.81827823e-07
Iter: 1107 loss: 2.81714136e-07
Iter: 1108 loss: 2.81473092e-07
Iter: 1109 loss: 2.86525164e-07
Iter: 1110 loss: 2.81492731e-07
Iter: 1111 loss: 2.81270559e-07
Iter: 1112 loss: 2.82148619e-07
Iter: 1113 loss: 2.81233383e-07
Iter: 1114 loss: 2.81025734e-07
Iter: 1115 loss: 2.82065741e-07
Iter: 1116 loss: 2.80975598e-07
Iter: 1117 loss: 2.80851793e-07
Iter: 1118 loss: 2.82809765e-07
Iter: 1119 loss: 2.80842357e-07
Iter: 1120 loss: 2.80747059e-07
Iter: 1121 loss: 2.80587614e-07
Iter: 1122 loss: 2.80596055e-07
Iter: 1123 loss: 2.80531737e-07
Iter: 1124 loss: 2.8052e-07
Iter: 1125 loss: 2.80438655e-07
Iter: 1126 loss: 2.80395824e-07
Iter: 1127 loss: 2.80338554e-07
Iter: 1128 loss: 2.80240442e-07
Iter: 1129 loss: 2.80903407e-07
Iter: 1130 loss: 2.80220746e-07
Iter: 1131 loss: 2.80106036e-07
Iter: 1132 loss: 2.79914047e-07
Iter: 1133 loss: 2.79913081e-07
Iter: 1134 loss: 2.79666835e-07
Iter: 1135 loss: 2.79835e-07
Iter: 1136 loss: 2.79528507e-07
Iter: 1137 loss: 2.79330436e-07
Iter: 1138 loss: 2.79325121e-07
Iter: 1139 loss: 2.79172468e-07
Iter: 1140 loss: 2.78883135e-07
Iter: 1141 loss: 2.84936647e-07
Iter: 1142 loss: 2.78887597e-07
Iter: 1143 loss: 2.78629642e-07
Iter: 1144 loss: 2.79092e-07
Iter: 1145 loss: 2.78529825e-07
Iter: 1146 loss: 2.78322659e-07
Iter: 1147 loss: 2.81204962e-07
Iter: 1148 loss: 2.78312086e-07
Iter: 1149 loss: 2.78185325e-07
Iter: 1150 loss: 2.79406777e-07
Iter: 1151 loss: 2.78171171e-07
Iter: 1152 loss: 2.78056689e-07
Iter: 1153 loss: 2.77977108e-07
Iter: 1154 loss: 2.77942206e-07
Iter: 1155 loss: 2.77795209e-07
Iter: 1156 loss: 2.78614607e-07
Iter: 1157 loss: 2.77781282e-07
Iter: 1158 loss: 2.77668278e-07
Iter: 1159 loss: 2.78625294e-07
Iter: 1160 loss: 2.77665748e-07
Iter: 1161 loss: 2.77556751e-07
Iter: 1162 loss: 2.77529e-07
Iter: 1163 loss: 2.7748095e-07
Iter: 1164 loss: 2.77314967e-07
Iter: 1165 loss: 2.78296795e-07
Iter: 1166 loss: 2.77338188e-07
Iter: 1167 loss: 2.77233312e-07
Iter: 1168 loss: 2.77100355e-07
Iter: 1169 loss: 2.7709882e-07
Iter: 1170 loss: 2.76983315e-07
Iter: 1171 loss: 2.76981297e-07
Iter: 1172 loss: 2.76855872e-07
Iter: 1173 loss: 2.76795959e-07
Iter: 1174 loss: 2.76728827e-07
Iter: 1175 loss: 2.76582824e-07
Iter: 1176 loss: 2.76613264e-07
Iter: 1177 loss: 2.76455239e-07
Iter: 1178 loss: 2.76233635e-07
Iter: 1179 loss: 2.76093971e-07
Iter: 1180 loss: 2.76013225e-07
Iter: 1181 loss: 2.76008e-07
Iter: 1182 loss: 2.75876801e-07
Iter: 1183 loss: 2.7577147e-07
Iter: 1184 loss: 2.75700131e-07
Iter: 1185 loss: 2.7564829e-07
Iter: 1186 loss: 2.75494898e-07
Iter: 1187 loss: 2.7554e-07
Iter: 1188 loss: 2.75377e-07
Iter: 1189 loss: 2.75141701e-07
Iter: 1190 loss: 2.77391791e-07
Iter: 1191 loss: 2.75136983e-07
Iter: 1192 loss: 2.74940476e-07
Iter: 1193 loss: 2.75334287e-07
Iter: 1194 loss: 2.7485379e-07
Iter: 1195 loss: 2.74743854e-07
Iter: 1196 loss: 2.75942767e-07
Iter: 1197 loss: 2.74737459e-07
Iter: 1198 loss: 2.74642275e-07
Iter: 1199 loss: 2.74511819e-07
Iter: 1200 loss: 2.74505538e-07
Iter: 1201 loss: 2.74374656e-07
Iter: 1202 loss: 2.75344973e-07
Iter: 1203 loss: 2.74369825e-07
Iter: 1204 loss: 2.74238658e-07
Iter: 1205 loss: 2.75197181e-07
Iter: 1206 loss: 2.74235845e-07
Iter: 1207 loss: 2.74189063e-07
Iter: 1208 loss: 2.74090524e-07
Iter: 1209 loss: 2.74076285e-07
Iter: 1210 loss: 2.73959188e-07
Iter: 1211 loss: 2.73869659e-07
Iter: 1212 loss: 2.73824867e-07
Iter: 1213 loss: 2.73676079e-07
Iter: 1214 loss: 2.75708857e-07
Iter: 1215 loss: 2.73683582e-07
Iter: 1216 loss: 2.73559806e-07
Iter: 1217 loss: 2.74500081e-07
Iter: 1218 loss: 2.73559465e-07
Iter: 1219 loss: 2.73416731e-07
Iter: 1220 loss: 2.73188249e-07
Iter: 1221 loss: 2.73192796e-07
Iter: 1222 loss: 2.72969487e-07
Iter: 1223 loss: 2.74547659e-07
Iter: 1224 loss: 2.72963121e-07
Iter: 1225 loss: 2.72718864e-07
Iter: 1226 loss: 2.74266881e-07
Iter: 1227 loss: 2.72701072e-07
Iter: 1228 loss: 2.72568428e-07
Iter: 1229 loss: 2.72847331e-07
Iter: 1230 loss: 2.72515933e-07
Iter: 1231 loss: 2.7240165e-07
Iter: 1232 loss: 2.72681234e-07
Iter: 1233 loss: 2.72339804e-07
Iter: 1234 loss: 2.72186838e-07
Iter: 1235 loss: 2.72016308e-07
Iter: 1236 loss: 2.72009459e-07
Iter: 1237 loss: 2.71987346e-07
Iter: 1238 loss: 2.71919362e-07
Iter: 1239 loss: 2.71870334e-07
Iter: 1240 loss: 2.71753208e-07
Iter: 1241 loss: 2.71754232e-07
Iter: 1242 loss: 2.71613715e-07
Iter: 1243 loss: 2.71755425e-07
Iter: 1244 loss: 2.71545446e-07
Iter: 1245 loss: 2.7141192e-07
Iter: 1246 loss: 2.71553546e-07
Iter: 1247 loss: 2.71315656e-07
Iter: 1248 loss: 2.71143676e-07
Iter: 1249 loss: 2.72031684e-07
Iter: 1250 loss: 2.71115283e-07
Iter: 1251 loss: 2.7092716e-07
Iter: 1252 loss: 2.72064597e-07
Iter: 1253 loss: 2.70898965e-07
Iter: 1254 loss: 2.70813018e-07
Iter: 1255 loss: 2.707786e-07
Iter: 1256 loss: 2.70739065e-07
Iter: 1257 loss: 2.70618045e-07
Iter: 1258 loss: 2.72451615e-07
Iter: 1259 loss: 2.70622138e-07
Iter: 1260 loss: 2.70550174e-07
Iter: 1261 loss: 2.70481593e-07
Iter: 1262 loss: 2.70457377e-07
Iter: 1263 loss: 2.70326836e-07
Iter: 1264 loss: 2.70897544e-07
Iter: 1265 loss: 2.70312938e-07
Iter: 1266 loss: 2.70174496e-07
Iter: 1267 loss: 2.70203628e-07
Iter: 1268 loss: 2.70084229e-07
Iter: 1269 loss: 2.6996247e-07
Iter: 1270 loss: 2.70067233e-07
Iter: 1271 loss: 2.69883856e-07
Iter: 1272 loss: 2.69704714e-07
Iter: 1273 loss: 2.7156284e-07
Iter: 1274 loss: 2.69687348e-07
Iter: 1275 loss: 2.69608364e-07
Iter: 1276 loss: 2.69446275e-07
Iter: 1277 loss: 2.73407295e-07
Iter: 1278 loss: 2.69436413e-07
Iter: 1279 loss: 2.69246073e-07
Iter: 1280 loss: 2.69865097e-07
Iter: 1281 loss: 2.69195624e-07
Iter: 1282 loss: 2.69035638e-07
Iter: 1283 loss: 2.69124683e-07
Iter: 1284 loss: 2.68924907e-07
Iter: 1285 loss: 2.68748806e-07
Iter: 1286 loss: 2.70463602e-07
Iter: 1287 loss: 2.68752046e-07
Iter: 1288 loss: 2.68614087e-07
Iter: 1289 loss: 2.70095654e-07
Iter: 1290 loss: 2.686117e-07
Iter: 1291 loss: 2.68535075e-07
Iter: 1292 loss: 2.68489543e-07
Iter: 1293 loss: 2.6843972e-07
Iter: 1294 loss: 2.68358178e-07
Iter: 1295 loss: 2.68364744e-07
Iter: 1296 loss: 2.68288375e-07
Iter: 1297 loss: 2.68187449e-07
Iter: 1298 loss: 2.68178667e-07
Iter: 1299 loss: 2.68097466e-07
Iter: 1300 loss: 2.69262159e-07
Iter: 1301 loss: 2.68097324e-07
Iter: 1302 loss: 2.680286e-07
Iter: 1303 loss: 2.67867108e-07
Iter: 1304 loss: 2.70436658e-07
Iter: 1305 loss: 2.67867733e-07
Iter: 1306 loss: 2.67705161e-07
Iter: 1307 loss: 2.68395183e-07
Iter: 1308 loss: 2.67689643e-07
Iter: 1309 loss: 2.6754131e-07
Iter: 1310 loss: 2.69064373e-07
Iter: 1311 loss: 2.6754816e-07
Iter: 1312 loss: 2.67416226e-07
Iter: 1313 loss: 2.67305126e-07
Iter: 1314 loss: 2.67258486e-07
Iter: 1315 loss: 2.67134624e-07
Iter: 1316 loss: 2.67125586e-07
Iter: 1317 loss: 2.67033272e-07
Iter: 1318 loss: 2.66853903e-07
Iter: 1319 loss: 2.68180344e-07
Iter: 1320 loss: 2.66838896e-07
Iter: 1321 loss: 2.66725635e-07
Iter: 1322 loss: 2.67149346e-07
Iter: 1323 loss: 2.66683514e-07
Iter: 1324 loss: 2.66571874e-07
Iter: 1325 loss: 2.67820951e-07
Iter: 1326 loss: 2.66574432e-07
Iter: 1327 loss: 2.6648911e-07
Iter: 1328 loss: 2.66358313e-07
Iter: 1329 loss: 2.69716963e-07
Iter: 1330 loss: 2.66341118e-07
Iter: 1331 loss: 2.66262958e-07
Iter: 1332 loss: 2.6623789e-07
Iter: 1333 loss: 2.66177381e-07
Iter: 1334 loss: 2.6608916e-07
Iter: 1335 loss: 2.6760577e-07
Iter: 1336 loss: 2.66084186e-07
Iter: 1337 loss: 2.65964701e-07
Iter: 1338 loss: 2.66891618e-07
Iter: 1339 loss: 2.65956629e-07
Iter: 1340 loss: 2.65876167e-07
Iter: 1341 loss: 2.65955805e-07
Iter: 1342 loss: 2.65832597e-07
Iter: 1343 loss: 2.65746621e-07
Iter: 1344 loss: 2.65799883e-07
Iter: 1345 loss: 2.65697707e-07
Iter: 1346 loss: 2.65594025e-07
Iter: 1347 loss: 2.66400264e-07
Iter: 1348 loss: 2.65580837e-07
Iter: 1349 loss: 2.65500603e-07
Iter: 1350 loss: 2.65384301e-07
Iter: 1351 loss: 2.65377281e-07
Iter: 1352 loss: 2.65221843e-07
Iter: 1353 loss: 2.65572737e-07
Iter: 1354 loss: 2.65142688e-07
Iter: 1355 loss: 2.64993133e-07
Iter: 1356 loss: 2.65628103e-07
Iter: 1357 loss: 2.64959084e-07
Iter: 1358 loss: 2.6480285e-07
Iter: 1359 loss: 2.65822905e-07
Iter: 1360 loss: 2.64799354e-07
Iter: 1361 loss: 2.64661537e-07
Iter: 1362 loss: 2.64781875e-07
Iter: 1363 loss: 2.64604239e-07
Iter: 1364 loss: 2.64493536e-07
Iter: 1365 loss: 2.65536926e-07
Iter: 1366 loss: 2.64475091e-07
Iter: 1367 loss: 2.64367884e-07
Iter: 1368 loss: 2.64495384e-07
Iter: 1369 loss: 2.64303253e-07
Iter: 1370 loss: 2.64190817e-07
Iter: 1371 loss: 2.64101146e-07
Iter: 1372 loss: 2.64053824e-07
Iter: 1373 loss: 2.63946504e-07
Iter: 1374 loss: 2.63930019e-07
Iter: 1375 loss: 2.63877894e-07
Iter: 1376 loss: 2.63764605e-07
Iter: 1377 loss: 2.66044253e-07
Iter: 1378 loss: 2.63771653e-07
Iter: 1379 loss: 2.63714327e-07
Iter: 1380 loss: 2.63691959e-07
Iter: 1381 loss: 2.63642306e-07
Iter: 1382 loss: 2.63551499e-07
Iter: 1383 loss: 2.63541e-07
Iter: 1384 loss: 2.63421128e-07
Iter: 1385 loss: 2.63457792e-07
Iter: 1386 loss: 2.63337654e-07
Iter: 1387 loss: 2.63175053e-07
Iter: 1388 loss: 2.63245738e-07
Iter: 1389 loss: 2.63095785e-07
Iter: 1390 loss: 2.62861363e-07
Iter: 1391 loss: 2.63955542e-07
Iter: 1392 loss: 2.62822311e-07
Iter: 1393 loss: 2.6271158e-07
Iter: 1394 loss: 2.6270834e-07
Iter: 1395 loss: 2.62607301e-07
Iter: 1396 loss: 2.62576464e-07
Iter: 1397 loss: 2.62528459e-07
Iter: 1398 loss: 2.62436629e-07
Iter: 1399 loss: 2.63852087e-07
Iter: 1400 loss: 2.62427307e-07
Iter: 1401 loss: 2.62332264e-07
Iter: 1402 loss: 2.62170204e-07
Iter: 1403 loss: 2.62166225e-07
Iter: 1404 loss: 2.62039293e-07
Iter: 1405 loss: 2.63346664e-07
Iter: 1406 loss: 2.62058307e-07
Iter: 1407 loss: 2.61934815e-07
Iter: 1408 loss: 2.62001038e-07
Iter: 1409 loss: 2.6187098e-07
Iter: 1410 loss: 2.61728985e-07
Iter: 1411 loss: 2.61905711e-07
Iter: 1412 loss: 2.61668731e-07
Iter: 1413 loss: 2.61583125e-07
Iter: 1414 loss: 2.6277516e-07
Iter: 1415 loss: 2.61560274e-07
Iter: 1416 loss: 2.61483024e-07
Iter: 1417 loss: 2.61391477e-07
Iter: 1418 loss: 2.61367688e-07
Iter: 1419 loss: 2.61218474e-07
Iter: 1420 loss: 2.61390966e-07
Iter: 1421 loss: 2.6115751e-07
Iter: 1422 loss: 2.60997922e-07
Iter: 1423 loss: 2.61390596e-07
Iter: 1424 loss: 2.60942272e-07
Iter: 1425 loss: 2.60814033e-07
Iter: 1426 loss: 2.6135703e-07
Iter: 1427 loss: 2.60789591e-07
Iter: 1428 loss: 2.60664848e-07
Iter: 1429 loss: 2.62103981e-07
Iter: 1430 loss: 2.60669623e-07
Iter: 1431 loss: 2.605899e-07
Iter: 1432 loss: 2.60604622e-07
Iter: 1433 loss: 2.60540162e-07
Iter: 1434 loss: 2.60459387e-07
Iter: 1435 loss: 2.6142223e-07
Iter: 1436 loss: 2.60457341e-07
Iter: 1437 loss: 2.60381228e-07
Iter: 1438 loss: 2.60298179e-07
Iter: 1439 loss: 2.60292978e-07
Iter: 1440 loss: 2.60197083e-07
Iter: 1441 loss: 2.60271889e-07
Iter: 1442 loss: 2.60139245e-07
Iter: 1443 loss: 2.60034739e-07
Iter: 1444 loss: 2.61518068e-07
Iter: 1445 loss: 2.6003525e-07
Iter: 1446 loss: 2.59938957e-07
Iter: 1447 loss: 2.59781928e-07
Iter: 1448 loss: 2.61794526e-07
Iter: 1449 loss: 2.59763794e-07
Iter: 1450 loss: 2.59590962e-07
Iter: 1451 loss: 2.59593264e-07
Iter: 1452 loss: 2.59461871e-07
Iter: 1453 loss: 2.59729575e-07
Iter: 1454 loss: 2.59411422e-07
Iter: 1455 loss: 2.59318824e-07
Iter: 1456 loss: 2.59170292e-07
Iter: 1457 loss: 2.59175692e-07
Iter: 1458 loss: 2.59030315e-07
Iter: 1459 loss: 2.60259583e-07
Iter: 1460 loss: 2.59030969e-07
Iter: 1461 loss: 2.58906823e-07
Iter: 1462 loss: 2.59086505e-07
Iter: 1463 loss: 2.58846399e-07
Iter: 1464 loss: 2.58790294e-07
Iter: 1465 loss: 2.5877759e-07
Iter: 1466 loss: 2.58726772e-07
Iter: 1467 loss: 2.58608509e-07
Iter: 1468 loss: 2.60746788e-07
Iter: 1469 loss: 2.58599243e-07
Iter: 1470 loss: 2.58570537e-07
Iter: 1471 loss: 2.58565279e-07
Iter: 1472 loss: 2.58500165e-07
Iter: 1473 loss: 2.58392447e-07
Iter: 1474 loss: 2.60014247e-07
Iter: 1475 loss: 2.58384944e-07
Iter: 1476 loss: 2.58277566e-07
Iter: 1477 loss: 2.59004082e-07
Iter: 1478 loss: 2.58268244e-07
Iter: 1479 loss: 2.58145036e-07
Iter: 1480 loss: 2.58426724e-07
Iter: 1481 loss: 2.58106809e-07
Iter: 1482 loss: 2.57976666e-07
Iter: 1483 loss: 2.57946937e-07
Iter: 1484 loss: 2.5785792e-07
Iter: 1485 loss: 2.57774019e-07
Iter: 1486 loss: 2.5776535e-07
Iter: 1487 loss: 2.57680938e-07
Iter: 1488 loss: 2.57541444e-07
Iter: 1489 loss: 2.61213131e-07
Iter: 1490 loss: 2.57541558e-07
Iter: 1491 loss: 2.57367645e-07
Iter: 1492 loss: 2.58170303e-07
Iter: 1493 loss: 2.57320238e-07
Iter: 1494 loss: 2.5723773e-07
Iter: 1495 loss: 2.57407464e-07
Iter: 1496 loss: 2.57206182e-07
Iter: 1497 loss: 2.57101249e-07
Iter: 1498 loss: 2.57481048e-07
Iter: 1499 loss: 2.57082718e-07
Iter: 1500 loss: 2.56969145e-07
Iter: 1501 loss: 2.57550738e-07
Iter: 1502 loss: 2.5694834e-07
Iter: 1503 loss: 2.56874301e-07
Iter: 1504 loss: 2.56968519e-07
Iter: 1505 loss: 2.56824052e-07
Iter: 1506 loss: 2.56762632e-07
Iter: 1507 loss: 2.57478405e-07
Iter: 1508 loss: 2.56752287e-07
Iter: 1509 loss: 2.56689447e-07
Iter: 1510 loss: 2.56532104e-07
Iter: 1511 loss: 2.57649731e-07
Iter: 1512 loss: 2.56498424e-07
Iter: 1513 loss: 2.5643061e-07
Iter: 1514 loss: 2.56402018e-07
Iter: 1515 loss: 2.56317662e-07
Iter: 1516 loss: 2.56254566e-07
Iter: 1517 loss: 2.56244164e-07
Iter: 1518 loss: 2.56147871e-07
Iter: 1519 loss: 2.56711928e-07
Iter: 1520 loss: 2.56127635e-07
Iter: 1521 loss: 2.56049816e-07
Iter: 1522 loss: 2.56293845e-07
Iter: 1523 loss: 2.56022815e-07
Iter: 1524 loss: 2.55949089e-07
Iter: 1525 loss: 2.55874056e-07
Iter: 1526 loss: 2.55853763e-07
Iter: 1527 loss: 2.55750734e-07
Iter: 1528 loss: 2.56275371e-07
Iter: 1529 loss: 2.55720579e-07
Iter: 1530 loss: 2.55638355e-07
Iter: 1531 loss: 2.55638668e-07
Iter: 1532 loss: 2.55563066e-07
Iter: 1533 loss: 2.55450914e-07
Iter: 1534 loss: 2.55434571e-07
Iter: 1535 loss: 2.55351779e-07
Iter: 1536 loss: 2.55440398e-07
Iter: 1537 loss: 2.55313e-07
Iter: 1538 loss: 2.5522607e-07
Iter: 1539 loss: 2.55759829e-07
Iter: 1540 loss: 2.55214388e-07
Iter: 1541 loss: 2.55093823e-07
Iter: 1542 loss: 2.55026578e-07
Iter: 1543 loss: 2.54992159e-07
Iter: 1544 loss: 2.54883673e-07
Iter: 1545 loss: 2.55272028e-07
Iter: 1546 loss: 2.54856417e-07
Iter: 1547 loss: 2.54753843e-07
Iter: 1548 loss: 2.55700456e-07
Iter: 1549 loss: 2.5475947e-07
Iter: 1550 loss: 2.54696175e-07
Iter: 1551 loss: 2.54695749e-07
Iter: 1552 loss: 2.54660677e-07
Iter: 1553 loss: 2.54581295e-07
Iter: 1554 loss: 2.55119801e-07
Iter: 1555 loss: 2.54575355e-07
Iter: 1556 loss: 2.54515527e-07
Iter: 1557 loss: 2.54595591e-07
Iter: 1558 loss: 2.54500037e-07
Iter: 1559 loss: 2.54448821e-07
Iter: 1560 loss: 2.54403e-07
Iter: 1561 loss: 2.54387885e-07
Iter: 1562 loss: 2.54285538e-07
Iter: 1563 loss: 2.5459e-07
Iter: 1564 loss: 2.54268883e-07
Iter: 1565 loss: 2.54175404e-07
Iter: 1566 loss: 2.54655504e-07
Iter: 1567 loss: 2.54168583e-07
Iter: 1568 loss: 2.54077293e-07
Iter: 1569 loss: 2.54373845e-07
Iter: 1570 loss: 2.540624e-07
Iter: 1571 loss: 2.53992795e-07
Iter: 1572 loss: 2.54236e-07
Iter: 1573 loss: 2.53978783e-07
Iter: 1574 loss: 2.53882234e-07
Iter: 1575 loss: 2.53970654e-07
Iter: 1576 loss: 2.53840142e-07
Iter: 1577 loss: 2.53743252e-07
Iter: 1578 loss: 2.53803535e-07
Iter: 1579 loss: 2.53702353e-07
Iter: 1580 loss: 2.53640593e-07
Iter: 1581 loss: 2.53633175e-07
Iter: 1582 loss: 2.53587075e-07
Iter: 1583 loss: 2.5349928e-07
Iter: 1584 loss: 2.55049457e-07
Iter: 1585 loss: 2.53500872e-07
Iter: 1586 loss: 2.53417369e-07
Iter: 1587 loss: 2.534224e-07
Iter: 1588 loss: 2.53371496e-07
Iter: 1589 loss: 2.53371297e-07
Iter: 1590 loss: 2.53333383e-07
Iter: 1591 loss: 2.53272162e-07
Iter: 1592 loss: 2.53237886e-07
Iter: 1593 loss: 2.53200199e-07
Iter: 1594 loss: 2.53090604e-07
Iter: 1595 loss: 2.53482199e-07
Iter: 1596 loss: 2.53061273e-07
Iter: 1597 loss: 2.52958273e-07
Iter: 1598 loss: 2.53397786e-07
Iter: 1599 loss: 2.52930704e-07
Iter: 1600 loss: 2.52847e-07
Iter: 1601 loss: 2.53697266e-07
Iter: 1602 loss: 2.52847144e-07
Iter: 1603 loss: 2.52770946e-07
Iter: 1604 loss: 2.52803716e-07
Iter: 1605 loss: 2.5272476e-07
Iter: 1606 loss: 2.52645407e-07
Iter: 1607 loss: 2.53147221e-07
Iter: 1608 loss: 2.5264228e-07
Iter: 1609 loss: 2.52580719e-07
Iter: 1610 loss: 2.52542634e-07
Iter: 1611 loss: 2.52520636e-07
Iter: 1612 loss: 2.52454925e-07
Iter: 1613 loss: 2.52943096e-07
Iter: 1614 loss: 2.52458392e-07
Iter: 1615 loss: 2.52406323e-07
Iter: 1616 loss: 2.52450747e-07
Iter: 1617 loss: 2.523646e-07
Iter: 1618 loss: 2.52325151e-07
Iter: 1619 loss: 2.52519783e-07
Iter: 1620 loss: 2.52301049e-07
Iter: 1621 loss: 2.52242614e-07
Iter: 1622 loss: 2.52298946e-07
Iter: 1623 loss: 2.52197481e-07
Iter: 1624 loss: 2.5213842e-07
Iter: 1625 loss: 2.52219166e-07
Iter: 1626 loss: 2.52118269e-07
Iter: 1627 loss: 2.52029082e-07
Iter: 1628 loss: 2.51970562e-07
Iter: 1629 loss: 2.51933926e-07
Iter: 1630 loss: 2.51830159e-07
Iter: 1631 loss: 2.53560529e-07
Iter: 1632 loss: 2.51832432e-07
Iter: 1633 loss: 2.51782552e-07
Iter: 1634 loss: 2.52075296e-07
Iter: 1635 loss: 2.51762458e-07
Iter: 1636 loss: 2.5170479e-07
Iter: 1637 loss: 2.5174316e-07
Iter: 1638 loss: 2.51672702e-07
Iter: 1639 loss: 2.51602728e-07
Iter: 1640 loss: 2.52312816e-07
Iter: 1641 loss: 2.51604888e-07
Iter: 1642 loss: 2.51570157e-07
Iter: 1643 loss: 2.51558646e-07
Iter: 1644 loss: 2.5154236e-07
Iter: 1645 loss: 2.51488075e-07
Iter: 1646 loss: 2.51650135e-07
Iter: 1647 loss: 2.51477786e-07
Iter: 1648 loss: 2.51443169e-07
Iter: 1649 loss: 2.51553615e-07
Iter: 1650 loss: 2.5140298e-07
Iter: 1651 loss: 2.51378253e-07
Iter: 1652 loss: 2.51361e-07
Iter: 1653 loss: 2.51344375e-07
Iter: 1654 loss: 2.51274912e-07
Iter: 1655 loss: 2.51871057e-07
Iter: 1656 loss: 2.51271729e-07
Iter: 1657 loss: 2.51233075e-07
Iter: 1658 loss: 2.5116e-07
Iter: 1659 loss: 2.51154688e-07
Iter: 1660 loss: 2.51056292e-07
Iter: 1661 loss: 2.51323257e-07
Iter: 1662 loss: 2.51017525e-07
Iter: 1663 loss: 2.509517e-07
Iter: 1664 loss: 2.51696918e-07
Iter: 1665 loss: 2.50932857e-07
Iter: 1666 loss: 2.50867856e-07
Iter: 1667 loss: 2.51091024e-07
Iter: 1668 loss: 2.50845289e-07
Iter: 1669 loss: 2.50789242e-07
Iter: 1670 loss: 2.51132121e-07
Iter: 1671 loss: 2.50774718e-07
Iter: 1672 loss: 2.50735894e-07
Iter: 1673 loss: 2.50961023e-07
Iter: 1674 loss: 2.50728078e-07
Iter: 1675 loss: 2.50674077e-07
Iter: 1676 loss: 2.50664556e-07
Iter: 1677 loss: 2.50635622e-07
Iter: 1678 loss: 2.50601488e-07
Iter: 1679 loss: 2.50823859e-07
Iter: 1680 loss: 2.50580541e-07
Iter: 1681 loss: 2.50533191e-07
Iter: 1682 loss: 2.50758745e-07
Iter: 1683 loss: 2.50525829e-07
Iter: 1684 loss: 2.50471146e-07
Iter: 1685 loss: 2.50442667e-07
Iter: 1686 loss: 2.50434226e-07
Iter: 1687 loss: 2.50375706e-07
Iter: 1688 loss: 2.50377241e-07
Iter: 1689 loss: 2.50324064e-07
Iter: 1690 loss: 2.50222854e-07
Iter: 1691 loss: 2.51495123e-07
Iter: 1692 loss: 2.50218847e-07
Iter: 1693 loss: 2.50110645e-07
Iter: 1694 loss: 2.50635878e-07
Iter: 1695 loss: 2.50097571e-07
Iter: 1696 loss: 2.5001691e-07
Iter: 1697 loss: 2.50314372e-07
Iter: 1698 loss: 2.50005968e-07
Iter: 1699 loss: 2.49931645e-07
Iter: 1700 loss: 2.50450711e-07
Iter: 1701 loss: 2.49934033e-07
Iter: 1702 loss: 2.49868066e-07
Iter: 1703 loss: 2.50023504e-07
Iter: 1704 loss: 2.49859738e-07
Iter: 1705 loss: 2.49795278e-07
Iter: 1706 loss: 2.50071366e-07
Iter: 1707 loss: 2.49801758e-07
Iter: 1708 loss: 2.49739628e-07
Iter: 1709 loss: 2.49835637e-07
Iter: 1710 loss: 2.49723428e-07
Iter: 1711 loss: 2.49694892e-07
Iter: 1712 loss: 2.49647883e-07
Iter: 1713 loss: 2.49641346e-07
Iter: 1714 loss: 2.49571485e-07
Iter: 1715 loss: 2.50329435e-07
Iter: 1716 loss: 2.49566085e-07
Iter: 1717 loss: 2.49535958e-07
Iter: 1718 loss: 2.49493951e-07
Iter: 1719 loss: 2.49474539e-07
Iter: 1720 loss: 2.49414484e-07
Iter: 1721 loss: 2.499809e-07
Iter: 1722 loss: 2.49423266e-07
Iter: 1723 loss: 2.49360141e-07
Iter: 1724 loss: 2.49310631e-07
Iter: 1725 loss: 2.49305486e-07
Iter: 1726 loss: 2.49225934e-07
Iter: 1727 loss: 2.49232386e-07
Iter: 1728 loss: 2.49166305e-07
Iter: 1729 loss: 2.49087e-07
Iter: 1730 loss: 2.49950801e-07
Iter: 1731 loss: 2.49066034e-07
Iter: 1732 loss: 2.49020673e-07
Iter: 1733 loss: 2.49138424e-07
Iter: 1734 loss: 2.49013169e-07
Iter: 1735 loss: 2.48923698e-07
Iter: 1736 loss: 2.49149025e-07
Iter: 1737 loss: 2.48905792e-07
Iter: 1738 loss: 2.48854349e-07
Iter: 1739 loss: 2.49283687e-07
Iter: 1740 loss: 2.48847414e-07
Iter: 1741 loss: 2.48793043e-07
Iter: 1742 loss: 2.48883396e-07
Iter: 1743 loss: 2.4876806e-07
Iter: 1744 loss: 2.48734e-07
Iter: 1745 loss: 2.48642692e-07
Iter: 1746 loss: 2.48646643e-07
Iter: 1747 loss: 2.48589856e-07
Iter: 1748 loss: 2.48593466e-07
Iter: 1749 loss: 2.48526902e-07
Iter: 1750 loss: 2.48543984e-07
Iter: 1751 loss: 2.48490949e-07
Iter: 1752 loss: 2.48433025e-07
Iter: 1753 loss: 2.48714827e-07
Iter: 1754 loss: 2.48429302e-07
Iter: 1755 loss: 2.48363676e-07
Iter: 1756 loss: 2.48479068e-07
Iter: 1757 loss: 2.48349863e-07
Iter: 1758 loss: 2.48281168e-07
Iter: 1759 loss: 2.48207897e-07
Iter: 1760 loss: 2.48211734e-07
Iter: 1761 loss: 2.48094182e-07
Iter: 1762 loss: 2.48529233e-07
Iter: 1763 loss: 2.48067181e-07
Iter: 1764 loss: 2.47968558e-07
Iter: 1765 loss: 2.48768259e-07
Iter: 1766 loss: 2.47958042e-07
Iter: 1767 loss: 2.4787721e-07
Iter: 1768 loss: 2.48070819e-07
Iter: 1769 loss: 2.47847879e-07
Iter: 1770 loss: 2.47761136e-07
Iter: 1771 loss: 2.47973929e-07
Iter: 1772 loss: 2.47719157e-07
Iter: 1773 loss: 2.47651485e-07
Iter: 1774 loss: 2.48204714e-07
Iter: 1775 loss: 2.47637104e-07
Iter: 1776 loss: 2.47570284e-07
Iter: 1777 loss: 2.47558603e-07
Iter: 1778 loss: 2.47537145e-07
Iter: 1779 loss: 2.47443467e-07
Iter: 1780 loss: 2.47602514e-07
Iter: 1781 loss: 2.4742252e-07
Iter: 1782 loss: 2.47331343e-07
Iter: 1783 loss: 2.48143692e-07
Iter: 1784 loss: 2.47329e-07
Iter: 1785 loss: 2.47287204e-07
Iter: 1786 loss: 2.47288426e-07
Iter: 1787 loss: 2.47250171e-07
Iter: 1788 loss: 2.4718625e-07
Iter: 1789 loss: 2.47456541e-07
Iter: 1790 loss: 2.47172977e-07
Iter: 1791 loss: 2.47102179e-07
Iter: 1792 loss: 2.47018363e-07
Iter: 1793 loss: 2.46997672e-07
Iter: 1794 loss: 2.46891261e-07
Iter: 1795 loss: 2.47156834e-07
Iter: 1796 loss: 2.46850618e-07
Iter: 1797 loss: 2.46750886e-07
Iter: 1798 loss: 2.48006103e-07
Iter: 1799 loss: 2.46760322e-07
Iter: 1800 loss: 2.46685431e-07
Iter: 1801 loss: 2.46888277e-07
Iter: 1802 loss: 2.46655731e-07
Iter: 1803 loss: 2.46581664e-07
Iter: 1804 loss: 2.46865795e-07
Iter: 1805 loss: 2.46578281e-07
Iter: 1806 loss: 2.46516208e-07
Iter: 1807 loss: 2.46960042e-07
Iter: 1808 loss: 2.46519335e-07
Iter: 1809 loss: 2.46482273e-07
Iter: 1810 loss: 2.464472e-07
Iter: 1811 loss: 2.46437679e-07
Iter: 1812 loss: 2.4637194e-07
Iter: 1813 loss: 2.4645297e-07
Iter: 1814 loss: 2.46335389e-07
Iter: 1815 loss: 2.46257883e-07
Iter: 1816 loss: 2.46256718e-07
Iter: 1817 loss: 2.46237846e-07
Iter: 1818 loss: 2.46155679e-07
Iter: 1819 loss: 2.47673682e-07
Iter: 1820 loss: 2.46144396e-07
Iter: 1821 loss: 2.46076e-07
Iter: 1822 loss: 2.4607462e-07
Iter: 1823 loss: 2.46024968e-07
Iter: 1824 loss: 2.45904516e-07
Iter: 1825 loss: 2.48655113e-07
Iter: 1826 loss: 2.45900878e-07
Iter: 1827 loss: 2.45771844e-07
Iter: 1828 loss: 2.46041111e-07
Iter: 1829 loss: 2.45721452e-07
Iter: 1830 loss: 2.45634396e-07
Iter: 1831 loss: 2.46381148e-07
Iter: 1832 loss: 2.45623653e-07
Iter: 1833 loss: 2.45524433e-07
Iter: 1834 loss: 2.45849947e-07
Iter: 1835 loss: 2.45519118e-07
Iter: 1836 loss: 2.45428112e-07
Iter: 1837 loss: 2.45790119e-07
Iter: 1838 loss: 2.45408813e-07
Iter: 1839 loss: 2.45357171e-07
Iter: 1840 loss: 2.45620186e-07
Iter: 1841 loss: 2.45350407e-07
Iter: 1842 loss: 2.45286287e-07
Iter: 1843 loss: 2.45285776e-07
Iter: 1844 loss: 2.45267671e-07
Iter: 1845 loss: 2.45185845e-07
Iter: 1846 loss: 2.45306268e-07
Iter: 1847 loss: 2.4516541e-07
Iter: 1848 loss: 2.45087762e-07
Iter: 1849 loss: 2.45731343e-07
Iter: 1850 loss: 2.45084578e-07
Iter: 1851 loss: 2.45027e-07
Iter: 1852 loss: 2.44996272e-07
Iter: 1853 loss: 2.44969215e-07
Iter: 1854 loss: 2.4489222e-07
Iter: 1855 loss: 2.45737681e-07
Iter: 1856 loss: 2.44901344e-07
Iter: 1857 loss: 2.44825372e-07
Iter: 1858 loss: 2.44672094e-07
Iter: 1859 loss: 2.48097251e-07
Iter: 1860 loss: 2.4468045e-07
Iter: 1861 loss: 2.44529531e-07
Iter: 1862 loss: 2.45473927e-07
Iter: 1863 loss: 2.44538796e-07
Iter: 1864 loss: 2.44436194e-07
Iter: 1865 loss: 2.44445886e-07
Iter: 1866 loss: 2.44390122e-07
Iter: 1867 loss: 2.44280955e-07
Iter: 1868 loss: 2.44275839e-07
Iter: 1869 loss: 2.44206319e-07
Iter: 1870 loss: 2.44302839e-07
Iter: 1871 loss: 2.44186111e-07
Iter: 1872 loss: 2.4411105e-07
Iter: 1873 loss: 2.44616558e-07
Iter: 1874 loss: 2.44109e-07
Iter: 1875 loss: 2.44043861e-07
Iter: 1876 loss: 2.44065973e-07
Iter: 1877 loss: 2.44018395e-07
Iter: 1878 loss: 2.43948875e-07
Iter: 1879 loss: 2.43941969e-07
Iter: 1880 loss: 2.43884188e-07
Iter: 1881 loss: 2.43774195e-07
Iter: 1882 loss: 2.45420921e-07
Iter: 1883 loss: 2.43771211e-07
Iter: 1884 loss: 2.43678357e-07
Iter: 1885 loss: 2.43843346e-07
Iter: 1886 loss: 2.43646184e-07
Iter: 1887 loss: 2.43570923e-07
Iter: 1888 loss: 2.43642518e-07
Iter: 1889 loss: 2.43516126e-07
Iter: 1890 loss: 2.43413126e-07
Iter: 1891 loss: 2.44039597e-07
Iter: 1892 loss: 2.43402667e-07
Iter: 1893 loss: 2.43339e-07
Iter: 1894 loss: 2.43263429e-07
Iter: 1895 loss: 2.43266072e-07
Iter: 1896 loss: 2.4317012e-07
Iter: 1897 loss: 2.43539773e-07
Iter: 1898 loss: 2.43130955e-07
Iter: 1899 loss: 2.43054842e-07
Iter: 1900 loss: 2.43650675e-07
Iter: 1901 loss: 2.43058139e-07
Iter: 1902 loss: 2.42984527e-07
Iter: 1903 loss: 2.4328682e-07
Iter: 1904 loss: 2.42968866e-07
Iter: 1905 loss: 2.42918929e-07
Iter: 1906 loss: 2.4316688e-07
Iter: 1907 loss: 2.42919839e-07
Iter: 1908 loss: 2.42875217e-07
Iter: 1909 loss: 2.4287732e-07
Iter: 1910 loss: 2.42839803e-07
Iter: 1911 loss: 2.42771733e-07
Iter: 1912 loss: 2.42706335e-07
Iter: 1913 loss: 2.42691556e-07
Iter: 1914 loss: 2.42593558e-07
Iter: 1915 loss: 2.43228612e-07
Iter: 1916 loss: 2.42593046e-07
Iter: 1917 loss: 2.42512073e-07
Iter: 1918 loss: 2.43018206e-07
Iter: 1919 loss: 2.42503035e-07
Iter: 1920 loss: 2.42433629e-07
Iter: 1921 loss: 2.42340889e-07
Iter: 1922 loss: 2.42333215e-07
Iter: 1923 loss: 2.42216231e-07
Iter: 1924 loss: 2.43572231e-07
Iter: 1925 loss: 2.4222004e-07
Iter: 1926 loss: 2.42122667e-07
Iter: 1927 loss: 2.42121189e-07
Iter: 1928 loss: 2.42032172e-07
Iter: 1929 loss: 2.41959725e-07
Iter: 1930 loss: 2.41904331e-07
Iter: 1931 loss: 2.41865791e-07
Iter: 1932 loss: 2.41774842e-07
Iter: 1933 loss: 2.4335597e-07
Iter: 1934 loss: 2.41759437e-07
Iter: 1935 loss: 2.41688667e-07
Iter: 1936 loss: 2.42157171e-07
Iter: 1937 loss: 2.41688497e-07
Iter: 1938 loss: 2.41633018e-07
Iter: 1939 loss: 2.41624775e-07
Iter: 1940 loss: 2.41582939e-07
Iter: 1941 loss: 2.41538146e-07
Iter: 1942 loss: 2.41530785e-07
Iter: 1943 loss: 2.41500487e-07
Iter: 1944 loss: 2.41439494e-07
Iter: 1945 loss: 2.4246259e-07
Iter: 1946 loss: 2.41421816e-07
Iter: 1947 loss: 2.41359714e-07
Iter: 1948 loss: 2.41693641e-07
Iter: 1949 loss: 2.41324898e-07
Iter: 1950 loss: 2.41296533e-07
Iter: 1951 loss: 2.41276581e-07
Iter: 1952 loss: 2.41246482e-07
Iter: 1953 loss: 2.41221954e-07
Iter: 1954 loss: 2.41208227e-07
Iter: 1955 loss: 2.41165196e-07
Iter: 1956 loss: 2.41397544e-07
Iter: 1957 loss: 2.41151e-07
Iter: 1958 loss: 2.41091215e-07
Iter: 1959 loss: 2.41178896e-07
Iter: 1960 loss: 2.4107149e-07
Iter: 1961 loss: 2.40999441e-07
Iter: 1962 loss: 2.40970735e-07
Iter: 1963 loss: 2.40929126e-07
Iter: 1964 loss: 2.40862477e-07
Iter: 1965 loss: 2.40963203e-07
Iter: 1966 loss: 2.40803786e-07
Iter: 1967 loss: 2.40761807e-07
Iter: 1968 loss: 2.40759618e-07
Iter: 1969 loss: 2.4069368e-07
Iter: 1970 loss: 2.40676911e-07
Iter: 1971 loss: 2.40650337e-07
Iter: 1972 loss: 2.40580334e-07
Iter: 1973 loss: 2.41023201e-07
Iter: 1974 loss: 2.40559928e-07
Iter: 1975 loss: 2.405028e-07
Iter: 1976 loss: 2.40675519e-07
Iter: 1977 loss: 2.4046696e-07
Iter: 1978 loss: 2.40432513e-07
Iter: 1979 loss: 2.40361771e-07
Iter: 1980 loss: 2.40350971e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.6
+ date
Mon Oct 26 17:51:38 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.6/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.6_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.6_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.6_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.6/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -1 --phi 1.6 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi1.6_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3808d0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3808d0e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe38094f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3809ee2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe380882488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe380882510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe380875620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe380882620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3808752f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3807d6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3807a2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3807a3b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3807a3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe38076f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe38076f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3806fc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3806e9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3806afc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3806ab488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe38063bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe38064d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe38064d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3681606a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe36817d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe36817d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe368122ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3680f38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3680927b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe368092378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe368092048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe36807d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3407da7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3407dad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe34077fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3407acbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3407acb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.09911604e-06
Iter: 2 loss: 6.14755481e-06
Iter: 3 loss: 4.17281e-06
Iter: 4 loss: 3.88354283e-06
Iter: 5 loss: 4.81840334e-06
Iter: 6 loss: 3.80205529e-06
Iter: 7 loss: 3.53458427e-06
Iter: 8 loss: 3.52611073e-06
Iter: 9 loss: 3.31797582e-06
Iter: 10 loss: 3.16264845e-06
Iter: 11 loss: 5.18527031e-06
Iter: 12 loss: 3.16176965e-06
Iter: 13 loss: 3.05627668e-06
Iter: 14 loss: 3.00374813e-06
Iter: 15 loss: 2.95359405e-06
Iter: 16 loss: 2.84195335e-06
Iter: 17 loss: 3.240772e-06
Iter: 18 loss: 2.8135114e-06
Iter: 19 loss: 2.73549654e-06
Iter: 20 loss: 2.7621245e-06
Iter: 21 loss: 2.68042095e-06
Iter: 22 loss: 2.57302327e-06
Iter: 23 loss: 3.93440268e-06
Iter: 24 loss: 2.57204874e-06
Iter: 25 loss: 2.52189216e-06
Iter: 26 loss: 2.44297689e-06
Iter: 27 loss: 2.4421272e-06
Iter: 28 loss: 2.32019102e-06
Iter: 29 loss: 2.57582178e-06
Iter: 30 loss: 2.27181636e-06
Iter: 31 loss: 2.16134504e-06
Iter: 32 loss: 2.41175235e-06
Iter: 33 loss: 2.11989072e-06
Iter: 34 loss: 2.01504236e-06
Iter: 35 loss: 2.33956553e-06
Iter: 36 loss: 1.9843983e-06
Iter: 37 loss: 1.92210587e-06
Iter: 38 loss: 1.92087191e-06
Iter: 39 loss: 1.88912884e-06
Iter: 40 loss: 1.88862032e-06
Iter: 41 loss: 1.86930538e-06
Iter: 42 loss: 1.84267947e-06
Iter: 43 loss: 1.84148098e-06
Iter: 44 loss: 1.79629535e-06
Iter: 45 loss: 1.97993859e-06
Iter: 46 loss: 1.78636014e-06
Iter: 47 loss: 1.7674538e-06
Iter: 48 loss: 1.78980736e-06
Iter: 49 loss: 1.75738751e-06
Iter: 50 loss: 1.72649015e-06
Iter: 51 loss: 1.76053686e-06
Iter: 52 loss: 1.70976216e-06
Iter: 53 loss: 1.68342058e-06
Iter: 54 loss: 1.6668896e-06
Iter: 55 loss: 1.65651034e-06
Iter: 56 loss: 1.62274546e-06
Iter: 57 loss: 1.94669656e-06
Iter: 58 loss: 1.62146534e-06
Iter: 59 loss: 1.60216473e-06
Iter: 60 loss: 1.6964849e-06
Iter: 61 loss: 1.59894694e-06
Iter: 62 loss: 1.57205523e-06
Iter: 63 loss: 1.54375687e-06
Iter: 64 loss: 1.5387651e-06
Iter: 65 loss: 1.50853441e-06
Iter: 66 loss: 1.47916774e-06
Iter: 67 loss: 1.47259425e-06
Iter: 68 loss: 1.44868068e-06
Iter: 69 loss: 1.445002e-06
Iter: 70 loss: 1.42456315e-06
Iter: 71 loss: 1.40554289e-06
Iter: 72 loss: 1.40071847e-06
Iter: 73 loss: 1.38082601e-06
Iter: 74 loss: 1.38036103e-06
Iter: 75 loss: 1.35445134e-06
Iter: 76 loss: 1.37778659e-06
Iter: 77 loss: 1.33941205e-06
Iter: 78 loss: 1.32397031e-06
Iter: 79 loss: 1.40607858e-06
Iter: 80 loss: 1.32157652e-06
Iter: 81 loss: 1.30497972e-06
Iter: 82 loss: 1.30189392e-06
Iter: 83 loss: 1.29071532e-06
Iter: 84 loss: 1.2692218e-06
Iter: 85 loss: 1.31790796e-06
Iter: 86 loss: 1.26117561e-06
Iter: 87 loss: 1.24284543e-06
Iter: 88 loss: 1.39991289e-06
Iter: 89 loss: 1.24182679e-06
Iter: 90 loss: 1.22607412e-06
Iter: 91 loss: 1.19157778e-06
Iter: 92 loss: 1.70686405e-06
Iter: 93 loss: 1.19018137e-06
Iter: 94 loss: 1.16525234e-06
Iter: 95 loss: 1.25181714e-06
Iter: 96 loss: 1.15879732e-06
Iter: 97 loss: 1.13242982e-06
Iter: 98 loss: 1.4036915e-06
Iter: 99 loss: 1.13178351e-06
Iter: 100 loss: 1.11669669e-06
Iter: 101 loss: 1.23533118e-06
Iter: 102 loss: 1.11569852e-06
Iter: 103 loss: 1.10624535e-06
Iter: 104 loss: 1.09393648e-06
Iter: 105 loss: 1.09312066e-06
Iter: 106 loss: 1.07850019e-06
Iter: 107 loss: 1.10608971e-06
Iter: 108 loss: 1.07235553e-06
Iter: 109 loss: 1.05916456e-06
Iter: 110 loss: 1.11653253e-06
Iter: 111 loss: 1.05649576e-06
Iter: 112 loss: 1.04608739e-06
Iter: 113 loss: 1.04602282e-06
Iter: 114 loss: 1.03467016e-06
Iter: 115 loss: 1.04921298e-06
Iter: 116 loss: 1.02886531e-06
Iter: 117 loss: 1.02231684e-06
Iter: 118 loss: 1.01613739e-06
Iter: 119 loss: 1.01467469e-06
Iter: 120 loss: 1.00175396e-06
Iter: 121 loss: 1.11048575e-06
Iter: 122 loss: 1.00102613e-06
Iter: 123 loss: 9.93161166e-07
Iter: 124 loss: 9.77121545e-07
Iter: 125 loss: 1.26505461e-06
Iter: 126 loss: 9.76820161e-07
Iter: 127 loss: 9.73098167e-07
Iter: 128 loss: 9.68774e-07
Iter: 129 loss: 9.63887828e-07
Iter: 130 loss: 9.54765142e-07
Iter: 131 loss: 1.16420733e-06
Iter: 132 loss: 9.54773896e-07
Iter: 133 loss: 9.40479879e-07
Iter: 134 loss: 9.48519812e-07
Iter: 135 loss: 9.31062573e-07
Iter: 136 loss: 9.20429557e-07
Iter: 137 loss: 9.5569726e-07
Iter: 138 loss: 9.17575449e-07
Iter: 139 loss: 9.06746436e-07
Iter: 140 loss: 1.05270897e-06
Iter: 141 loss: 9.06649e-07
Iter: 142 loss: 9.01497856e-07
Iter: 143 loss: 8.92596063e-07
Iter: 144 loss: 8.92580886e-07
Iter: 145 loss: 8.81035078e-07
Iter: 146 loss: 9.46314231e-07
Iter: 147 loss: 8.79453751e-07
Iter: 148 loss: 8.75117507e-07
Iter: 149 loss: 8.75068167e-07
Iter: 150 loss: 8.69435553e-07
Iter: 151 loss: 8.61983722e-07
Iter: 152 loss: 8.61512149e-07
Iter: 153 loss: 8.5558213e-07
Iter: 154 loss: 8.62482693e-07
Iter: 155 loss: 8.52480582e-07
Iter: 156 loss: 8.44637782e-07
Iter: 157 loss: 9.18362673e-07
Iter: 158 loss: 8.44401598e-07
Iter: 159 loss: 8.40563416e-07
Iter: 160 loss: 8.37227731e-07
Iter: 161 loss: 8.36248773e-07
Iter: 162 loss: 8.2910276e-07
Iter: 163 loss: 8.44896135e-07
Iter: 164 loss: 8.26295036e-07
Iter: 165 loss: 8.18205308e-07
Iter: 166 loss: 8.71300358e-07
Iter: 167 loss: 8.17332875e-07
Iter: 168 loss: 8.14113378e-07
Iter: 169 loss: 8.08448362e-07
Iter: 170 loss: 8.08420225e-07
Iter: 171 loss: 8.00063845e-07
Iter: 172 loss: 8.26089718e-07
Iter: 173 loss: 7.97647601e-07
Iter: 174 loss: 7.90206911e-07
Iter: 175 loss: 8.51752816e-07
Iter: 176 loss: 7.89720275e-07
Iter: 177 loss: 7.82504685e-07
Iter: 178 loss: 7.98238148e-07
Iter: 179 loss: 7.79792686e-07
Iter: 180 loss: 7.72290832e-07
Iter: 181 loss: 7.59899876e-07
Iter: 182 loss: 7.59847467e-07
Iter: 183 loss: 7.62421e-07
Iter: 184 loss: 7.55864335e-07
Iter: 185 loss: 7.51179e-07
Iter: 186 loss: 7.45431066e-07
Iter: 187 loss: 7.44875535e-07
Iter: 188 loss: 7.38248218e-07
Iter: 189 loss: 7.45511e-07
Iter: 190 loss: 7.34625928e-07
Iter: 191 loss: 7.30265583e-07
Iter: 192 loss: 7.30249212e-07
Iter: 193 loss: 7.26367716e-07
Iter: 194 loss: 7.23307039e-07
Iter: 195 loss: 7.22087066e-07
Iter: 196 loss: 7.18004e-07
Iter: 197 loss: 7.17095759e-07
Iter: 198 loss: 7.14426392e-07
Iter: 199 loss: 7.10221912e-07
Iter: 200 loss: 7.099718e-07
Iter: 201 loss: 7.07979666e-07
Iter: 202 loss: 7.04623233e-07
Iter: 203 loss: 7.04661602e-07
Iter: 204 loss: 6.99166e-07
Iter: 205 loss: 7.02554587e-07
Iter: 206 loss: 6.9563157e-07
Iter: 207 loss: 6.90704269e-07
Iter: 208 loss: 6.90779927e-07
Iter: 209 loss: 6.86794465e-07
Iter: 210 loss: 6.84848374e-07
Iter: 211 loss: 6.82759946e-07
Iter: 212 loss: 6.81082042e-07
Iter: 213 loss: 6.77447929e-07
Iter: 214 loss: 7.36216521e-07
Iter: 215 loss: 6.77325602e-07
Iter: 216 loss: 6.7484649e-07
Iter: 217 loss: 6.74558521e-07
Iter: 218 loss: 6.71497901e-07
Iter: 219 loss: 6.69162091e-07
Iter: 220 loss: 6.68176767e-07
Iter: 221 loss: 6.64565619e-07
Iter: 222 loss: 6.63379751e-07
Iter: 223 loss: 6.61272281e-07
Iter: 224 loss: 6.57958708e-07
Iter: 225 loss: 6.78259e-07
Iter: 226 loss: 6.57525845e-07
Iter: 227 loss: 6.53020038e-07
Iter: 228 loss: 6.600402e-07
Iter: 229 loss: 6.50837308e-07
Iter: 230 loss: 6.47334502e-07
Iter: 231 loss: 6.54470966e-07
Iter: 232 loss: 6.45909665e-07
Iter: 233 loss: 6.4278197e-07
Iter: 234 loss: 6.44555598e-07
Iter: 235 loss: 6.40729752e-07
Iter: 236 loss: 6.37067728e-07
Iter: 237 loss: 6.88336684e-07
Iter: 238 loss: 6.37059145e-07
Iter: 239 loss: 6.34903245e-07
Iter: 240 loss: 6.31478315e-07
Iter: 241 loss: 6.31459216e-07
Iter: 242 loss: 6.27621944e-07
Iter: 243 loss: 6.30990428e-07
Iter: 244 loss: 6.25434723e-07
Iter: 245 loss: 6.20806532e-07
Iter: 246 loss: 6.62954449e-07
Iter: 247 loss: 6.20591209e-07
Iter: 248 loss: 6.18229706e-07
Iter: 249 loss: 6.2630653e-07
Iter: 250 loss: 6.17637227e-07
Iter: 251 loss: 6.14253281e-07
Iter: 252 loss: 6.22179698e-07
Iter: 253 loss: 6.13050588e-07
Iter: 254 loss: 6.11106771e-07
Iter: 255 loss: 6.22232108e-07
Iter: 256 loss: 6.10853931e-07
Iter: 257 loss: 6.08371408e-07
Iter: 258 loss: 6.14411931e-07
Iter: 259 loss: 6.07511254e-07
Iter: 260 loss: 6.05656737e-07
Iter: 261 loss: 6.02757495e-07
Iter: 262 loss: 6.02706677e-07
Iter: 263 loss: 5.99168743e-07
Iter: 264 loss: 6.00013095e-07
Iter: 265 loss: 5.965789e-07
Iter: 266 loss: 5.92105721e-07
Iter: 267 loss: 5.91959406e-07
Iter: 268 loss: 5.88434489e-07
Iter: 269 loss: 5.87564216e-07
Iter: 270 loss: 5.85505347e-07
Iter: 271 loss: 5.8364509e-07
Iter: 272 loss: 5.91227547e-07
Iter: 273 loss: 5.83259521e-07
Iter: 274 loss: 5.81012955e-07
Iter: 275 loss: 5.77499e-07
Iter: 276 loss: 5.77480932e-07
Iter: 277 loss: 5.74950093e-07
Iter: 278 loss: 5.7469606e-07
Iter: 279 loss: 5.73255193e-07
Iter: 280 loss: 5.69660187e-07
Iter: 281 loss: 6.00988415e-07
Iter: 282 loss: 5.69053896e-07
Iter: 283 loss: 5.65883624e-07
Iter: 284 loss: 5.91396088e-07
Iter: 285 loss: 5.65653181e-07
Iter: 286 loss: 5.62841763e-07
Iter: 287 loss: 5.68760299e-07
Iter: 288 loss: 5.61772936e-07
Iter: 289 loss: 5.59369255e-07
Iter: 290 loss: 5.6652334e-07
Iter: 291 loss: 5.58625857e-07
Iter: 292 loss: 5.56107409e-07
Iter: 293 loss: 5.56133216e-07
Iter: 294 loss: 5.55449049e-07
Iter: 295 loss: 5.53594191e-07
Iter: 296 loss: 5.66915332e-07
Iter: 297 loss: 5.53178e-07
Iter: 298 loss: 5.5013885e-07
Iter: 299 loss: 5.65641812e-07
Iter: 300 loss: 5.49623394e-07
Iter: 301 loss: 5.47474315e-07
Iter: 302 loss: 5.52243e-07
Iter: 303 loss: 5.46729041e-07
Iter: 304 loss: 5.44706609e-07
Iter: 305 loss: 5.42002056e-07
Iter: 306 loss: 5.41854206e-07
Iter: 307 loss: 5.38710481e-07
Iter: 308 loss: 5.53468738e-07
Iter: 309 loss: 5.38142558e-07
Iter: 310 loss: 5.35303e-07
Iter: 311 loss: 5.60231911e-07
Iter: 312 loss: 5.3518022e-07
Iter: 313 loss: 5.33217928e-07
Iter: 314 loss: 5.58408033e-07
Iter: 315 loss: 5.33189e-07
Iter: 316 loss: 5.3189251e-07
Iter: 317 loss: 5.29961881e-07
Iter: 318 loss: 5.29923341e-07
Iter: 319 loss: 5.28154828e-07
Iter: 320 loss: 5.37135122e-07
Iter: 321 loss: 5.27891245e-07
Iter: 322 loss: 5.25612563e-07
Iter: 323 loss: 5.30345119e-07
Iter: 324 loss: 5.24754455e-07
Iter: 325 loss: 5.23351275e-07
Iter: 326 loss: 5.21984305e-07
Iter: 327 loss: 5.21637503e-07
Iter: 328 loss: 5.19476941e-07
Iter: 329 loss: 5.44420232e-07
Iter: 330 loss: 5.19452328e-07
Iter: 331 loss: 5.174e-07
Iter: 332 loss: 5.3265029e-07
Iter: 333 loss: 5.17279432e-07
Iter: 334 loss: 5.16174737e-07
Iter: 335 loss: 5.14161115e-07
Iter: 336 loss: 5.61671641e-07
Iter: 337 loss: 5.14169756e-07
Iter: 338 loss: 5.12543807e-07
Iter: 339 loss: 5.13673911e-07
Iter: 340 loss: 5.1152773e-07
Iter: 341 loss: 5.08783955e-07
Iter: 342 loss: 5.27422515e-07
Iter: 343 loss: 5.08522362e-07
Iter: 344 loss: 5.07360824e-07
Iter: 345 loss: 5.09621145e-07
Iter: 346 loss: 5.06893684e-07
Iter: 347 loss: 5.05495564e-07
Iter: 348 loss: 5.03393096e-07
Iter: 349 loss: 5.0333233e-07
Iter: 350 loss: 5.01556428e-07
Iter: 351 loss: 5.25841e-07
Iter: 352 loss: 5.01550289e-07
Iter: 353 loss: 4.99438897e-07
Iter: 354 loss: 5.06029266e-07
Iter: 355 loss: 4.98846816e-07
Iter: 356 loss: 4.97836e-07
Iter: 357 loss: 4.97067845e-07
Iter: 358 loss: 4.96724169e-07
Iter: 359 loss: 4.9497703e-07
Iter: 360 loss: 5.02709895e-07
Iter: 361 loss: 4.94601181e-07
Iter: 362 loss: 4.92859499e-07
Iter: 363 loss: 5.02361331e-07
Iter: 364 loss: 4.92612571e-07
Iter: 365 loss: 4.91025503e-07
Iter: 366 loss: 4.88905869e-07
Iter: 367 loss: 4.88782234e-07
Iter: 368 loss: 4.88629553e-07
Iter: 369 loss: 4.88102557e-07
Iter: 370 loss: 4.87156456e-07
Iter: 371 loss: 4.84775057e-07
Iter: 372 loss: 5.05690593e-07
Iter: 373 loss: 4.84385055e-07
Iter: 374 loss: 4.82811231e-07
Iter: 375 loss: 4.95150175e-07
Iter: 376 loss: 4.82751034e-07
Iter: 377 loss: 4.81580514e-07
Iter: 378 loss: 4.83995109e-07
Iter: 379 loss: 4.81120424e-07
Iter: 380 loss: 4.79822518e-07
Iter: 381 loss: 4.89425929e-07
Iter: 382 loss: 4.79699565e-07
Iter: 383 loss: 4.79077698e-07
Iter: 384 loss: 4.7782197e-07
Iter: 385 loss: 5.06471565e-07
Iter: 386 loss: 4.77799858e-07
Iter: 387 loss: 4.76302887e-07
Iter: 388 loss: 4.89225727e-07
Iter: 389 loss: 4.76237119e-07
Iter: 390 loss: 4.7526089e-07
Iter: 391 loss: 4.88616081e-07
Iter: 392 loss: 4.75255945e-07
Iter: 393 loss: 4.74464116e-07
Iter: 394 loss: 4.72991189e-07
Iter: 395 loss: 5.06622e-07
Iter: 396 loss: 4.7300432e-07
Iter: 397 loss: 4.71525766e-07
Iter: 398 loss: 4.73890566e-07
Iter: 399 loss: 4.70845237e-07
Iter: 400 loss: 4.69375181e-07
Iter: 401 loss: 4.69374385e-07
Iter: 402 loss: 4.68552315e-07
Iter: 403 loss: 4.66742392e-07
Iter: 404 loss: 4.94057076e-07
Iter: 405 loss: 4.66683559e-07
Iter: 406 loss: 4.66954134e-07
Iter: 407 loss: 4.65818516e-07
Iter: 408 loss: 4.65159133e-07
Iter: 409 loss: 4.64446828e-07
Iter: 410 loss: 4.64329901e-07
Iter: 411 loss: 4.63558763e-07
Iter: 412 loss: 4.62861863e-07
Iter: 413 loss: 4.62664843e-07
Iter: 414 loss: 4.61419944e-07
Iter: 415 loss: 4.6464308e-07
Iter: 416 loss: 4.61009762e-07
Iter: 417 loss: 4.59847769e-07
Iter: 418 loss: 4.778434e-07
Iter: 419 loss: 4.5982614e-07
Iter: 420 loss: 4.59226413e-07
Iter: 421 loss: 4.5826971e-07
Iter: 422 loss: 4.58251151e-07
Iter: 423 loss: 4.56654e-07
Iter: 424 loss: 4.58547674e-07
Iter: 425 loss: 4.55755838e-07
Iter: 426 loss: 4.55705674e-07
Iter: 427 loss: 4.5514011e-07
Iter: 428 loss: 4.54652508e-07
Iter: 429 loss: 4.53274026e-07
Iter: 430 loss: 4.62449577e-07
Iter: 431 loss: 4.52974859e-07
Iter: 432 loss: 4.51682183e-07
Iter: 433 loss: 4.5979121e-07
Iter: 434 loss: 4.51544764e-07
Iter: 435 loss: 4.50525278e-07
Iter: 436 loss: 4.6013551e-07
Iter: 437 loss: 4.50496941e-07
Iter: 438 loss: 4.49691385e-07
Iter: 439 loss: 4.51784956e-07
Iter: 440 loss: 4.49448407e-07
Iter: 441 loss: 4.48805167e-07
Iter: 442 loss: 4.50542188e-07
Iter: 443 loss: 4.4859388e-07
Iter: 444 loss: 4.47594743e-07
Iter: 445 loss: 4.49479359e-07
Iter: 446 loss: 4.47224636e-07
Iter: 447 loss: 4.46647903e-07
Iter: 448 loss: 4.45154114e-07
Iter: 449 loss: 4.53800851e-07
Iter: 450 loss: 4.44730404e-07
Iter: 451 loss: 4.42983236e-07
Iter: 452 loss: 4.66254562e-07
Iter: 453 loss: 4.42968201e-07
Iter: 454 loss: 4.41803763e-07
Iter: 455 loss: 4.4208241e-07
Iter: 456 loss: 4.40932041e-07
Iter: 457 loss: 4.39096169e-07
Iter: 458 loss: 4.55811403e-07
Iter: 459 loss: 4.3899729e-07
Iter: 460 loss: 4.38111812e-07
Iter: 461 loss: 4.36729266e-07
Iter: 462 loss: 4.36723496e-07
Iter: 463 loss: 4.35596604e-07
Iter: 464 loss: 4.35590778e-07
Iter: 465 loss: 4.34453852e-07
Iter: 466 loss: 4.36789037e-07
Iter: 467 loss: 4.33982478e-07
Iter: 468 loss: 4.33137e-07
Iter: 469 loss: 4.33560558e-07
Iter: 470 loss: 4.32545448e-07
Iter: 471 loss: 4.31763397e-07
Iter: 472 loss: 4.34163439e-07
Iter: 473 loss: 4.31540855e-07
Iter: 474 loss: 4.30870159e-07
Iter: 475 loss: 4.30891248e-07
Iter: 476 loss: 4.30544844e-07
Iter: 477 loss: 4.30898496e-07
Iter: 478 loss: 4.30375e-07
Iter: 479 loss: 4.29694836e-07
Iter: 480 loss: 4.2916821e-07
Iter: 481 loss: 4.28938108e-07
Iter: 482 loss: 4.28207386e-07
Iter: 483 loss: 4.28345942e-07
Iter: 484 loss: 4.27683801e-07
Iter: 485 loss: 4.26614719e-07
Iter: 486 loss: 4.26223721e-07
Iter: 487 loss: 4.25627661e-07
Iter: 488 loss: 4.238955e-07
Iter: 489 loss: 4.26206213e-07
Iter: 490 loss: 4.23001552e-07
Iter: 491 loss: 4.22862e-07
Iter: 492 loss: 4.22105188e-07
Iter: 493 loss: 4.2166397e-07
Iter: 494 loss: 4.20867593e-07
Iter: 495 loss: 4.40178582e-07
Iter: 496 loss: 4.2083883e-07
Iter: 497 loss: 4.19536292e-07
Iter: 498 loss: 4.20848863e-07
Iter: 499 loss: 4.18790933e-07
Iter: 500 loss: 4.18462434e-07
Iter: 501 loss: 4.18097784e-07
Iter: 502 loss: 4.1769087e-07
Iter: 503 loss: 4.16712027e-07
Iter: 504 loss: 4.25916937e-07
Iter: 505 loss: 4.16567275e-07
Iter: 506 loss: 4.157734e-07
Iter: 507 loss: 4.24280188e-07
Iter: 508 loss: 4.157489e-07
Iter: 509 loss: 4.15227703e-07
Iter: 510 loss: 4.20622712e-07
Iter: 511 loss: 4.15213549e-07
Iter: 512 loss: 4.14730323e-07
Iter: 513 loss: 4.15812622e-07
Iter: 514 loss: 4.14530888e-07
Iter: 515 loss: 4.13963e-07
Iter: 516 loss: 4.15386637e-07
Iter: 517 loss: 4.13729026e-07
Iter: 518 loss: 4.13162439e-07
Iter: 519 loss: 4.11959547e-07
Iter: 520 loss: 4.31004935e-07
Iter: 521 loss: 4.11927317e-07
Iter: 522 loss: 4.1088046e-07
Iter: 523 loss: 4.1173854e-07
Iter: 524 loss: 4.10303983e-07
Iter: 525 loss: 4.09278755e-07
Iter: 526 loss: 4.2511968e-07
Iter: 527 loss: 4.09304107e-07
Iter: 528 loss: 4.08625e-07
Iter: 529 loss: 4.0893454e-07
Iter: 530 loss: 4.0813535e-07
Iter: 531 loss: 4.07363444e-07
Iter: 532 loss: 4.19784158e-07
Iter: 533 loss: 4.07380355e-07
Iter: 534 loss: 4.06829884e-07
Iter: 535 loss: 4.0614384e-07
Iter: 536 loss: 4.06096945e-07
Iter: 537 loss: 4.05477778e-07
Iter: 538 loss: 4.05477692e-07
Iter: 539 loss: 4.04801824e-07
Iter: 540 loss: 4.05039572e-07
Iter: 541 loss: 4.04342728e-07
Iter: 542 loss: 4.03645487e-07
Iter: 543 loss: 4.04325419e-07
Iter: 544 loss: 4.03233628e-07
Iter: 545 loss: 4.02488723e-07
Iter: 546 loss: 4.01539438e-07
Iter: 547 loss: 4.01505361e-07
Iter: 548 loss: 4.02291619e-07
Iter: 549 loss: 4.01000761e-07
Iter: 550 loss: 4.0064171e-07
Iter: 551 loss: 3.99865655e-07
Iter: 552 loss: 4.12273351e-07
Iter: 553 loss: 3.99850677e-07
Iter: 554 loss: 3.99054e-07
Iter: 555 loss: 4.07071752e-07
Iter: 556 loss: 3.99044154e-07
Iter: 557 loss: 3.98625389e-07
Iter: 558 loss: 3.98355098e-07
Iter: 559 loss: 3.98168368e-07
Iter: 560 loss: 3.97420024e-07
Iter: 561 loss: 3.97586234e-07
Iter: 562 loss: 3.96859946e-07
Iter: 563 loss: 3.96053792e-07
Iter: 564 loss: 3.96989037e-07
Iter: 565 loss: 3.95634117e-07
Iter: 566 loss: 3.95345467e-07
Iter: 567 loss: 3.95136027e-07
Iter: 568 loss: 3.9482984e-07
Iter: 569 loss: 3.94304465e-07
Iter: 570 loss: 3.94283518e-07
Iter: 571 loss: 3.93550408e-07
Iter: 572 loss: 3.9545381e-07
Iter: 573 loss: 3.9328259e-07
Iter: 574 loss: 3.92831822e-07
Iter: 575 loss: 3.99474857e-07
Iter: 576 loss: 3.9282196e-07
Iter: 577 loss: 3.92256368e-07
Iter: 578 loss: 3.9166855e-07
Iter: 579 loss: 3.91574247e-07
Iter: 580 loss: 3.9082795e-07
Iter: 581 loss: 3.90580112e-07
Iter: 582 loss: 3.90168651e-07
Iter: 583 loss: 3.90147392e-07
Iter: 584 loss: 3.89635659e-07
Iter: 585 loss: 3.89296702e-07
Iter: 586 loss: 3.88872081e-07
Iter: 587 loss: 3.88829164e-07
Iter: 588 loss: 3.88239556e-07
Iter: 589 loss: 3.88435723e-07
Iter: 590 loss: 3.87822126e-07
Iter: 591 loss: 3.86777458e-07
Iter: 592 loss: 3.90425896e-07
Iter: 593 loss: 3.86526409e-07
Iter: 594 loss: 3.86044235e-07
Iter: 595 loss: 3.85172257e-07
Iter: 596 loss: 4.05062679e-07
Iter: 597 loss: 3.85165606e-07
Iter: 598 loss: 3.8424497e-07
Iter: 599 loss: 3.94933494e-07
Iter: 600 loss: 3.84218652e-07
Iter: 601 loss: 3.83619408e-07
Iter: 602 loss: 3.85425068e-07
Iter: 603 loss: 3.83467324e-07
Iter: 604 loss: 3.82855603e-07
Iter: 605 loss: 3.87465633e-07
Iter: 606 loss: 3.82774886e-07
Iter: 607 loss: 3.82309338e-07
Iter: 608 loss: 3.8173846e-07
Iter: 609 loss: 3.81700403e-07
Iter: 610 loss: 3.81104201e-07
Iter: 611 loss: 3.88257888e-07
Iter: 612 loss: 3.81093372e-07
Iter: 613 loss: 3.80654399e-07
Iter: 614 loss: 3.84471974e-07
Iter: 615 loss: 3.80598209e-07
Iter: 616 loss: 3.80182598e-07
Iter: 617 loss: 3.7955482e-07
Iter: 618 loss: 3.79534981e-07
Iter: 619 loss: 3.79373631e-07
Iter: 620 loss: 3.79242948e-07
Iter: 621 loss: 3.78945515e-07
Iter: 622 loss: 3.7850549e-07
Iter: 623 loss: 3.78488267e-07
Iter: 624 loss: 3.77975169e-07
Iter: 625 loss: 3.7807871e-07
Iter: 626 loss: 3.7762976e-07
Iter: 627 loss: 3.77017614e-07
Iter: 628 loss: 3.81338623e-07
Iter: 629 loss: 3.76984758e-07
Iter: 630 loss: 3.76422e-07
Iter: 631 loss: 3.78564721e-07
Iter: 632 loss: 3.76317843e-07
Iter: 633 loss: 3.75957882e-07
Iter: 634 loss: 3.75262061e-07
Iter: 635 loss: 3.8950526e-07
Iter: 636 loss: 3.75242621e-07
Iter: 637 loss: 3.74495443e-07
Iter: 638 loss: 3.82689052e-07
Iter: 639 loss: 3.74441441e-07
Iter: 640 loss: 3.73952389e-07
Iter: 641 loss: 3.76931268e-07
Iter: 642 loss: 3.73901599e-07
Iter: 643 loss: 3.73308978e-07
Iter: 644 loss: 3.72967207e-07
Iter: 645 loss: 3.72740203e-07
Iter: 646 loss: 3.72015563e-07
Iter: 647 loss: 3.72373e-07
Iter: 648 loss: 3.71513238e-07
Iter: 649 loss: 3.70758471e-07
Iter: 650 loss: 3.78754066e-07
Iter: 651 loss: 3.70727349e-07
Iter: 652 loss: 3.70154225e-07
Iter: 653 loss: 3.76161495e-07
Iter: 654 loss: 3.70164543e-07
Iter: 655 loss: 3.69748136e-07
Iter: 656 loss: 3.69375243e-07
Iter: 657 loss: 3.6927446e-07
Iter: 658 loss: 3.68846656e-07
Iter: 659 loss: 3.68987486e-07
Iter: 660 loss: 3.68565054e-07
Iter: 661 loss: 3.68078162e-07
Iter: 662 loss: 3.74342193e-07
Iter: 663 loss: 3.68075291e-07
Iter: 664 loss: 3.67758389e-07
Iter: 665 loss: 3.70883384e-07
Iter: 666 loss: 3.67759213e-07
Iter: 667 loss: 3.67503901e-07
Iter: 668 loss: 3.67942619e-07
Iter: 669 loss: 3.67391806e-07
Iter: 670 loss: 3.670078e-07
Iter: 671 loss: 3.66900622e-07
Iter: 672 loss: 3.66692575e-07
Iter: 673 loss: 3.66284439e-07
Iter: 674 loss: 3.65677181e-07
Iter: 675 loss: 3.65641228e-07
Iter: 676 loss: 3.65050369e-07
Iter: 677 loss: 3.73125772e-07
Iter: 678 loss: 3.65039739e-07
Iter: 679 loss: 3.64514278e-07
Iter: 680 loss: 3.65655922e-07
Iter: 681 loss: 3.64292703e-07
Iter: 682 loss: 3.63562833e-07
Iter: 683 loss: 3.63634683e-07
Iter: 684 loss: 3.63012163e-07
Iter: 685 loss: 3.62460241e-07
Iter: 686 loss: 3.62812273e-07
Iter: 687 loss: 3.62109517e-07
Iter: 688 loss: 3.61478783e-07
Iter: 689 loss: 3.66831102e-07
Iter: 690 loss: 3.61448087e-07
Iter: 691 loss: 3.61013406e-07
Iter: 692 loss: 3.64530365e-07
Iter: 693 loss: 3.60991464e-07
Iter: 694 loss: 3.6048894e-07
Iter: 695 loss: 3.60813658e-07
Iter: 696 loss: 3.60158367e-07
Iter: 697 loss: 3.59816056e-07
Iter: 698 loss: 3.59478406e-07
Iter: 699 loss: 3.5938416e-07
Iter: 700 loss: 3.59049636e-07
Iter: 701 loss: 3.59026046e-07
Iter: 702 loss: 3.58686179e-07
Iter: 703 loss: 3.60279671e-07
Iter: 704 loss: 3.58636157e-07
Iter: 705 loss: 3.5830351e-07
Iter: 706 loss: 3.57942213e-07
Iter: 707 loss: 3.57923909e-07
Iter: 708 loss: 3.57569e-07
Iter: 709 loss: 3.57551812e-07
Iter: 710 loss: 3.57363547e-07
Iter: 711 loss: 3.56899335e-07
Iter: 712 loss: 3.60002105e-07
Iter: 713 loss: 3.56779083e-07
Iter: 714 loss: 3.56288695e-07
Iter: 715 loss: 3.63533019e-07
Iter: 716 loss: 3.56283124e-07
Iter: 717 loss: 3.5594141e-07
Iter: 718 loss: 3.57928e-07
Iter: 719 loss: 3.55878456e-07
Iter: 720 loss: 3.55527192e-07
Iter: 721 loss: 3.5539648e-07
Iter: 722 loss: 3.552002e-07
Iter: 723 loss: 3.54733231e-07
Iter: 724 loss: 3.55014805e-07
Iter: 725 loss: 3.54402232e-07
Iter: 726 loss: 3.54139416e-07
Iter: 727 loss: 3.54109773e-07
Iter: 728 loss: 3.53798498e-07
Iter: 729 loss: 3.53376919e-07
Iter: 730 loss: 3.53377743e-07
Iter: 731 loss: 3.52751584e-07
Iter: 732 loss: 3.52814368e-07
Iter: 733 loss: 3.5226239e-07
Iter: 734 loss: 3.51555315e-07
Iter: 735 loss: 3.52557265e-07
Iter: 736 loss: 3.51201663e-07
Iter: 737 loss: 3.51195126e-07
Iter: 738 loss: 3.50909772e-07
Iter: 739 loss: 3.5056911e-07
Iter: 740 loss: 3.50176379e-07
Iter: 741 loss: 3.50147246e-07
Iter: 742 loss: 3.49758608e-07
Iter: 743 loss: 3.52652251e-07
Iter: 744 loss: 3.49736069e-07
Iter: 745 loss: 3.49430422e-07
Iter: 746 loss: 3.49601464e-07
Iter: 747 loss: 3.49254151e-07
Iter: 748 loss: 3.48806935e-07
Iter: 749 loss: 3.48441631e-07
Iter: 750 loss: 3.48335675e-07
Iter: 751 loss: 3.47829712e-07
Iter: 752 loss: 3.48449561e-07
Iter: 753 loss: 3.4756988e-07
Iter: 754 loss: 3.47144322e-07
Iter: 755 loss: 3.5398881e-07
Iter: 756 loss: 3.47118345e-07
Iter: 757 loss: 3.46798572e-07
Iter: 758 loss: 3.46762306e-07
Iter: 759 loss: 3.46492868e-07
Iter: 760 loss: 3.45908177e-07
Iter: 761 loss: 3.51248559e-07
Iter: 762 loss: 3.45895e-07
Iter: 763 loss: 3.45559101e-07
Iter: 764 loss: 3.45655707e-07
Iter: 765 loss: 3.45342499e-07
Iter: 766 loss: 3.44897103e-07
Iter: 767 loss: 3.49103033e-07
Iter: 768 loss: 3.44876639e-07
Iter: 769 loss: 3.44715261e-07
Iter: 770 loss: 3.44366811e-07
Iter: 771 loss: 3.44363968e-07
Iter: 772 loss: 3.44055763e-07
Iter: 773 loss: 3.44046e-07
Iter: 774 loss: 3.43751594e-07
Iter: 775 loss: 3.43658286e-07
Iter: 776 loss: 3.43473459e-07
Iter: 777 loss: 3.43230056e-07
Iter: 778 loss: 3.42775564e-07
Iter: 779 loss: 3.53198317e-07
Iter: 780 loss: 3.42784119e-07
Iter: 781 loss: 3.42418218e-07
Iter: 782 loss: 3.42382151e-07
Iter: 783 loss: 3.42130136e-07
Iter: 784 loss: 3.42100606e-07
Iter: 785 loss: 3.41938e-07
Iter: 786 loss: 3.41546524e-07
Iter: 787 loss: 3.41654697e-07
Iter: 788 loss: 3.41291042e-07
Iter: 789 loss: 3.40912067e-07
Iter: 790 loss: 3.41436476e-07
Iter: 791 loss: 3.40714081e-07
Iter: 792 loss: 3.40374356e-07
Iter: 793 loss: 3.45686431e-07
Iter: 794 loss: 3.40369411e-07
Iter: 795 loss: 3.40072063e-07
Iter: 796 loss: 3.42232084e-07
Iter: 797 loss: 3.40033068e-07
Iter: 798 loss: 3.39845599e-07
Iter: 799 loss: 3.40166906e-07
Iter: 800 loss: 3.3974635e-07
Iter: 801 loss: 3.39505675e-07
Iter: 802 loss: 3.40343178e-07
Iter: 803 loss: 3.39437463e-07
Iter: 804 loss: 3.39186442e-07
Iter: 805 loss: 3.38730104e-07
Iter: 806 loss: 3.49792629e-07
Iter: 807 loss: 3.38736868e-07
Iter: 808 loss: 3.38741813e-07
Iter: 809 loss: 3.38504833e-07
Iter: 810 loss: 3.38313299e-07
Iter: 811 loss: 3.37914742e-07
Iter: 812 loss: 3.4649031e-07
Iter: 813 loss: 3.3792719e-07
Iter: 814 loss: 3.37559328e-07
Iter: 815 loss: 3.37824929e-07
Iter: 816 loss: 3.37355971e-07
Iter: 817 loss: 3.36945703e-07
Iter: 818 loss: 3.38866926e-07
Iter: 819 loss: 3.36874052e-07
Iter: 820 loss: 3.36425813e-07
Iter: 821 loss: 3.37930089e-07
Iter: 822 loss: 3.36303e-07
Iter: 823 loss: 3.36002699e-07
Iter: 824 loss: 3.35703817e-07
Iter: 825 loss: 3.35613038e-07
Iter: 826 loss: 3.35255265e-07
Iter: 827 loss: 3.40671619e-07
Iter: 828 loss: 3.35274734e-07
Iter: 829 loss: 3.35054722e-07
Iter: 830 loss: 3.35425966e-07
Iter: 831 loss: 3.34958088e-07
Iter: 832 loss: 3.34578431e-07
Iter: 833 loss: 3.35211212e-07
Iter: 834 loss: 3.34409151e-07
Iter: 835 loss: 3.34100605e-07
Iter: 836 loss: 3.36257614e-07
Iter: 837 loss: 3.34080084e-07
Iter: 838 loss: 3.33791093e-07
Iter: 839 loss: 3.33545415e-07
Iter: 840 loss: 3.33471235e-07
Iter: 841 loss: 3.33233572e-07
Iter: 842 loss: 3.36684479e-07
Iter: 843 loss: 3.33232975e-07
Iter: 844 loss: 3.32923889e-07
Iter: 845 loss: 3.33253212e-07
Iter: 846 loss: 3.32792126e-07
Iter: 847 loss: 3.32562536e-07
Iter: 848 loss: 3.32451179e-07
Iter: 849 loss: 3.32326e-07
Iter: 850 loss: 3.32017066e-07
Iter: 851 loss: 3.31884507e-07
Iter: 852 loss: 3.31717104e-07
Iter: 853 loss: 3.3161092e-07
Iter: 854 loss: 3.31466424e-07
Iter: 855 loss: 3.31330597e-07
Iter: 856 loss: 3.30985699e-07
Iter: 857 loss: 3.35345533e-07
Iter: 858 loss: 3.30969812e-07
Iter: 859 loss: 3.30635601e-07
Iter: 860 loss: 3.32353693e-07
Iter: 861 loss: 3.30567275e-07
Iter: 862 loss: 3.30240823e-07
Iter: 863 loss: 3.3139554e-07
Iter: 864 loss: 3.30140267e-07
Iter: 865 loss: 3.29862303e-07
Iter: 866 loss: 3.33712592e-07
Iter: 867 loss: 3.29870119e-07
Iter: 868 loss: 3.29694728e-07
Iter: 869 loss: 3.29549891e-07
Iter: 870 loss: 3.29483527e-07
Iter: 871 loss: 3.29086959e-07
Iter: 872 loss: 3.29230659e-07
Iter: 873 loss: 3.28780601e-07
Iter: 874 loss: 3.2850869e-07
Iter: 875 loss: 3.30993544e-07
Iter: 876 loss: 3.28484333e-07
Iter: 877 loss: 3.2824309e-07
Iter: 878 loss: 3.29610884e-07
Iter: 879 loss: 3.28207079e-07
Iter: 880 loss: 3.28003807e-07
Iter: 881 loss: 3.27600389e-07
Iter: 882 loss: 3.27598457e-07
Iter: 883 loss: 3.27280418e-07
Iter: 884 loss: 3.2784169e-07
Iter: 885 loss: 3.27140924e-07
Iter: 886 loss: 3.26853183e-07
Iter: 887 loss: 3.30282404e-07
Iter: 888 loss: 3.26859833e-07
Iter: 889 loss: 3.26600741e-07
Iter: 890 loss: 3.27174234e-07
Iter: 891 loss: 3.26515703e-07
Iter: 892 loss: 3.26229809e-07
Iter: 893 loss: 3.26390932e-07
Iter: 894 loss: 3.26067493e-07
Iter: 895 loss: 3.25810902e-07
Iter: 896 loss: 3.25746441e-07
Iter: 897 loss: 3.25607289e-07
Iter: 898 loss: 3.25392534e-07
Iter: 899 loss: 3.25373435e-07
Iter: 900 loss: 3.2517039e-07
Iter: 901 loss: 3.25139553e-07
Iter: 902 loss: 3.25012707e-07
Iter: 903 loss: 3.24727921e-07
Iter: 904 loss: 3.25873714e-07
Iter: 905 loss: 3.24646919e-07
Iter: 906 loss: 3.2439084e-07
Iter: 907 loss: 3.2435986e-07
Iter: 908 loss: 3.2418933e-07
Iter: 909 loss: 3.23965253e-07
Iter: 910 loss: 3.23951497e-07
Iter: 911 loss: 3.23726283e-07
Iter: 912 loss: 3.23509767e-07
Iter: 913 loss: 3.23449967e-07
Iter: 914 loss: 3.23234644e-07
Iter: 915 loss: 3.23030861e-07
Iter: 916 loss: 3.22978451e-07
Iter: 917 loss: 3.22672491e-07
Iter: 918 loss: 3.25196226e-07
Iter: 919 loss: 3.22639295e-07
Iter: 920 loss: 3.22382817e-07
Iter: 921 loss: 3.23991941e-07
Iter: 922 loss: 3.22335381e-07
Iter: 923 loss: 3.2206097e-07
Iter: 924 loss: 3.2263921e-07
Iter: 925 loss: 3.21946573e-07
Iter: 926 loss: 3.21738241e-07
Iter: 927 loss: 3.21407526e-07
Iter: 928 loss: 3.21398829e-07
Iter: 929 loss: 3.21050834e-07
Iter: 930 loss: 3.25766251e-07
Iter: 931 loss: 3.21056802e-07
Iter: 932 loss: 3.2077719e-07
Iter: 933 loss: 3.21708256e-07
Iter: 934 loss: 3.20669926e-07
Iter: 935 loss: 3.20343e-07
Iter: 936 loss: 3.2102912e-07
Iter: 937 loss: 3.20221773e-07
Iter: 938 loss: 3.19962709e-07
Iter: 939 loss: 3.21581552e-07
Iter: 940 loss: 3.19919479e-07
Iter: 941 loss: 3.19731413e-07
Iter: 942 loss: 3.1942028e-07
Iter: 943 loss: 3.1941434e-07
Iter: 944 loss: 3.1932754e-07
Iter: 945 loss: 3.19196431e-07
Iter: 946 loss: 3.19126059e-07
Iter: 947 loss: 3.18984632e-07
Iter: 948 loss: 3.2053677e-07
Iter: 949 loss: 3.18960247e-07
Iter: 950 loss: 3.18730429e-07
Iter: 951 loss: 3.18813477e-07
Iter: 952 loss: 3.1858778e-07
Iter: 953 loss: 3.18305922e-07
Iter: 954 loss: 3.18569107e-07
Iter: 955 loss: 3.18107169e-07
Iter: 956 loss: 3.17948377e-07
Iter: 957 loss: 3.17925753e-07
Iter: 958 loss: 3.17727142e-07
Iter: 959 loss: 3.17414674e-07
Iter: 960 loss: 3.17413367e-07
Iter: 961 loss: 3.170708e-07
Iter: 962 loss: 3.19218714e-07
Iter: 963 loss: 3.17046613e-07
Iter: 964 loss: 3.16789112e-07
Iter: 965 loss: 3.17504941e-07
Iter: 966 loss: 3.16731558e-07
Iter: 967 loss: 3.16446517e-07
Iter: 968 loss: 3.18157902e-07
Iter: 969 loss: 3.16419232e-07
Iter: 970 loss: 3.16221985e-07
Iter: 971 loss: 3.16598403e-07
Iter: 972 loss: 3.16104206e-07
Iter: 973 loss: 3.15867794e-07
Iter: 974 loss: 3.16433045e-07
Iter: 975 loss: 3.15764396e-07
Iter: 976 loss: 3.15618138e-07
Iter: 977 loss: 3.17111869e-07
Iter: 978 loss: 3.15611317e-07
Iter: 979 loss: 3.15449313e-07
Iter: 980 loss: 3.15749162e-07
Iter: 981 loss: 3.15353674e-07
Iter: 982 loss: 3.15178966e-07
Iter: 983 loss: 3.1507625e-07
Iter: 984 loss: 3.1503356e-07
Iter: 985 loss: 3.14831595e-07
Iter: 986 loss: 3.14578301e-07
Iter: 987 loss: 3.14549709e-07
Iter: 988 loss: 3.14289935e-07
Iter: 989 loss: 3.18583091e-07
Iter: 990 loss: 3.14278708e-07
Iter: 991 loss: 3.1404528e-07
Iter: 992 loss: 3.14254237e-07
Iter: 993 loss: 3.1385386e-07
Iter: 994 loss: 3.13502881e-07
Iter: 995 loss: 3.16515781e-07
Iter: 996 loss: 3.13484e-07
Iter: 997 loss: 3.13306032e-07
Iter: 998 loss: 3.1293655e-07
Iter: 999 loss: 3.19282947e-07
Iter: 1000 loss: 3.12940102e-07
Iter: 1001 loss: 3.12711393e-07
Iter: 1002 loss: 3.12674103e-07
Iter: 1003 loss: 3.12449117e-07
Iter: 1004 loss: 3.12691924e-07
Iter: 1005 loss: 3.1234606e-07
Iter: 1006 loss: 3.12063065e-07
Iter: 1007 loss: 3.12863961e-07
Iter: 1008 loss: 3.1198465e-07
Iter: 1009 loss: 3.11742951e-07
Iter: 1010 loss: 3.12598814e-07
Iter: 1011 loss: 3.11689831e-07
Iter: 1012 loss: 3.11449355e-07
Iter: 1013 loss: 3.11256485e-07
Iter: 1014 loss: 3.11213626e-07
Iter: 1015 loss: 3.11316825e-07
Iter: 1016 loss: 3.11089536e-07
Iter: 1017 loss: 3.11010439e-07
Iter: 1018 loss: 3.1081089e-07
Iter: 1019 loss: 3.13285938e-07
Iter: 1020 loss: 3.108062e-07
Iter: 1021 loss: 3.10543157e-07
Iter: 1022 loss: 3.10618134e-07
Iter: 1023 loss: 3.10375412e-07
Iter: 1024 loss: 3.10127291e-07
Iter: 1025 loss: 3.10042594e-07
Iter: 1026 loss: 3.09891504e-07
Iter: 1027 loss: 3.09500905e-07
Iter: 1028 loss: 3.13327092e-07
Iter: 1029 loss: 3.0947956e-07
Iter: 1030 loss: 3.09311531e-07
Iter: 1031 loss: 3.11438896e-07
Iter: 1032 loss: 3.09325685e-07
Iter: 1033 loss: 3.091248e-07
Iter: 1034 loss: 3.088835e-07
Iter: 1035 loss: 3.08874263e-07
Iter: 1036 loss: 3.08603973e-07
Iter: 1037 loss: 3.0890024e-07
Iter: 1038 loss: 3.08454958e-07
Iter: 1039 loss: 3.08241425e-07
Iter: 1040 loss: 3.08239692e-07
Iter: 1041 loss: 3.08087408e-07
Iter: 1042 loss: 3.08079677e-07
Iter: 1043 loss: 3.07979576e-07
Iter: 1044 loss: 3.07749872e-07
Iter: 1045 loss: 3.08190806e-07
Iter: 1046 loss: 3.0766796e-07
Iter: 1047 loss: 3.07489302e-07
Iter: 1048 loss: 3.08436938e-07
Iter: 1049 loss: 3.07455082e-07
Iter: 1050 loss: 3.07391304e-07
Iter: 1051 loss: 3.07372346e-07
Iter: 1052 loss: 3.07299842e-07
Iter: 1053 loss: 3.07041603e-07
Iter: 1054 loss: 3.08547669e-07
Iter: 1055 loss: 3.06978961e-07
Iter: 1056 loss: 3.06723905e-07
Iter: 1057 loss: 3.0832993e-07
Iter: 1058 loss: 3.0670364e-07
Iter: 1059 loss: 3.06520349e-07
Iter: 1060 loss: 3.06411607e-07
Iter: 1061 loss: 3.06328047e-07
Iter: 1062 loss: 3.05981587e-07
Iter: 1063 loss: 3.06234085e-07
Iter: 1064 loss: 3.05755e-07
Iter: 1065 loss: 3.05416165e-07
Iter: 1066 loss: 3.09307723e-07
Iter: 1067 loss: 3.05403205e-07
Iter: 1068 loss: 3.05168129e-07
Iter: 1069 loss: 3.06516114e-07
Iter: 1070 loss: 3.05139309e-07
Iter: 1071 loss: 3.04892581e-07
Iter: 1072 loss: 3.05141043e-07
Iter: 1073 loss: 3.04759396e-07
Iter: 1074 loss: 3.04580595e-07
Iter: 1075 loss: 3.04711193e-07
Iter: 1076 loss: 3.04482398e-07
Iter: 1077 loss: 3.04300073e-07
Iter: 1078 loss: 3.07014e-07
Iter: 1079 loss: 3.04310873e-07
Iter: 1080 loss: 3.04167884e-07
Iter: 1081 loss: 3.04202047e-07
Iter: 1082 loss: 3.04076025e-07
Iter: 1083 loss: 3.03875197e-07
Iter: 1084 loss: 3.04273186e-07
Iter: 1085 loss: 3.03818211e-07
Iter: 1086 loss: 3.03630259e-07
Iter: 1087 loss: 3.03878835e-07
Iter: 1088 loss: 3.03529873e-07
Iter: 1089 loss: 3.03350419e-07
Iter: 1090 loss: 3.04814421e-07
Iter: 1091 loss: 3.03351555e-07
Iter: 1092 loss: 3.03123556e-07
Iter: 1093 loss: 3.02955129e-07
Iter: 1094 loss: 3.02885951e-07
Iter: 1095 loss: 3.02648232e-07
Iter: 1096 loss: 3.02718661e-07
Iter: 1097 loss: 3.02470852e-07
Iter: 1098 loss: 3.0211865e-07
Iter: 1099 loss: 3.02440753e-07
Iter: 1100 loss: 3.01965684e-07
Iter: 1101 loss: 3.01668223e-07
Iter: 1102 loss: 3.021824e-07
Iter: 1103 loss: 3.01573266e-07
Iter: 1104 loss: 3.01293198e-07
Iter: 1105 loss: 3.03742723e-07
Iter: 1106 loss: 3.01254971e-07
Iter: 1107 loss: 3.01112379e-07
Iter: 1108 loss: 3.02528917e-07
Iter: 1109 loss: 3.01083276e-07
Iter: 1110 loss: 3.00928548e-07
Iter: 1111 loss: 3.00906777e-07
Iter: 1112 loss: 3.0080875e-07
Iter: 1113 loss: 3.00641204e-07
Iter: 1114 loss: 3.01206796e-07
Iter: 1115 loss: 3.00577113e-07
Iter: 1116 loss: 3.0036469e-07
Iter: 1117 loss: 3.01480839e-07
Iter: 1118 loss: 3.00324245e-07
Iter: 1119 loss: 3.00206807e-07
Iter: 1120 loss: 3.00013596e-07
Iter: 1121 loss: 3.00011607e-07
Iter: 1122 loss: 2.99820357e-07
Iter: 1123 loss: 3.02665313e-07
Iter: 1124 loss: 2.99834e-07
Iter: 1125 loss: 2.99639737e-07
Iter: 1126 loss: 3.00223292e-07
Iter: 1127 loss: 2.99580734e-07
Iter: 1128 loss: 2.99472788e-07
Iter: 1129 loss: 2.99602164e-07
Iter: 1130 loss: 2.99380417e-07
Iter: 1131 loss: 2.99204714e-07
Iter: 1132 loss: 2.99240412e-07
Iter: 1133 loss: 2.99092562e-07
Iter: 1134 loss: 2.98923624e-07
Iter: 1135 loss: 2.9880789e-07
Iter: 1136 loss: 2.98752923e-07
Iter: 1137 loss: 2.98543398e-07
Iter: 1138 loss: 3.00620741e-07
Iter: 1139 loss: 2.98545245e-07
Iter: 1140 loss: 2.98369116e-07
Iter: 1141 loss: 2.98408821e-07
Iter: 1142 loss: 2.98212115e-07
Iter: 1143 loss: 2.98207169e-07
Iter: 1144 loss: 2.98147597e-07
Iter: 1145 loss: 2.98056193e-07
Iter: 1146 loss: 2.9789004e-07
Iter: 1147 loss: 2.99702208e-07
Iter: 1148 loss: 2.97867672e-07
Iter: 1149 loss: 2.97735284e-07
Iter: 1150 loss: 2.97717207e-07
Iter: 1151 loss: 2.97641634e-07
Iter: 1152 loss: 2.9748594e-07
Iter: 1153 loss: 2.97477357e-07
Iter: 1154 loss: 2.97313846e-07
Iter: 1155 loss: 2.99124054e-07
Iter: 1156 loss: 2.9731649e-07
Iter: 1157 loss: 2.97138683e-07
Iter: 1158 loss: 2.98021519e-07
Iter: 1159 loss: 2.97120835e-07
Iter: 1160 loss: 2.96987537e-07
Iter: 1161 loss: 2.96903238e-07
Iter: 1162 loss: 2.96840739e-07
Iter: 1163 loss: 2.96554447e-07
Iter: 1164 loss: 2.97346361e-07
Iter: 1165 loss: 2.9644707e-07
Iter: 1166 loss: 2.96241268e-07
Iter: 1167 loss: 2.96177404e-07
Iter: 1168 loss: 2.96081225e-07
Iter: 1169 loss: 2.95765176e-07
Iter: 1170 loss: 2.96758969e-07
Iter: 1171 loss: 2.95688096e-07
Iter: 1172 loss: 2.95453191e-07
Iter: 1173 loss: 2.95450974e-07
Iter: 1174 loss: 2.95249833e-07
Iter: 1175 loss: 2.95128473e-07
Iter: 1176 loss: 2.95097522e-07
Iter: 1177 loss: 2.94945437e-07
Iter: 1178 loss: 2.94974939e-07
Iter: 1179 loss: 2.94817028e-07
Iter: 1180 loss: 2.9466517e-07
Iter: 1181 loss: 2.95238635e-07
Iter: 1182 loss: 2.94611652e-07
Iter: 1183 loss: 2.94431516e-07
Iter: 1184 loss: 2.95530299e-07
Iter: 1185 loss: 2.94399968e-07
Iter: 1186 loss: 2.94240721e-07
Iter: 1187 loss: 2.94049471e-07
Iter: 1188 loss: 2.94038273e-07
Iter: 1189 loss: 2.93968895e-07
Iter: 1190 loss: 2.93910546e-07
Iter: 1191 loss: 2.93819426e-07
Iter: 1192 loss: 2.93582104e-07
Iter: 1193 loss: 2.97509132e-07
Iter: 1194 loss: 2.93592e-07
Iter: 1195 loss: 2.93399694e-07
Iter: 1196 loss: 2.93405435e-07
Iter: 1197 loss: 2.93296267e-07
Iter: 1198 loss: 2.93168824e-07
Iter: 1199 loss: 2.93158479e-07
Iter: 1200 loss: 2.92961801e-07
Iter: 1201 loss: 2.92847e-07
Iter: 1202 loss: 2.92731215e-07
Iter: 1203 loss: 2.9251558e-07
Iter: 1204 loss: 2.92805396e-07
Iter: 1205 loss: 2.92417951e-07
Iter: 1206 loss: 2.92156386e-07
Iter: 1207 loss: 2.94213407e-07
Iter: 1208 loss: 2.92149593e-07
Iter: 1209 loss: 2.91921083e-07
Iter: 1210 loss: 2.93834717e-07
Iter: 1211 loss: 2.9191898e-07
Iter: 1212 loss: 2.91739383e-07
Iter: 1213 loss: 2.91763286e-07
Iter: 1214 loss: 2.91588378e-07
Iter: 1215 loss: 2.91407275e-07
Iter: 1216 loss: 2.92636628e-07
Iter: 1217 loss: 2.91387209e-07
Iter: 1218 loss: 2.91135e-07
Iter: 1219 loss: 2.91194e-07
Iter: 1220 loss: 2.90978392e-07
Iter: 1221 loss: 2.9082193e-07
Iter: 1222 loss: 2.91389938e-07
Iter: 1223 loss: 2.90776597e-07
Iter: 1224 loss: 2.90553658e-07
Iter: 1225 loss: 2.91810181e-07
Iter: 1226 loss: 2.90530096e-07
Iter: 1227 loss: 2.90445655e-07
Iter: 1228 loss: 2.90471689e-07
Iter: 1229 loss: 2.90381081e-07
Iter: 1230 loss: 2.90197391e-07
Iter: 1231 loss: 2.90266712e-07
Iter: 1232 loss: 2.90109313e-07
Iter: 1233 loss: 2.89925708e-07
Iter: 1234 loss: 2.90362095e-07
Iter: 1235 loss: 2.89866477e-07
Iter: 1236 loss: 2.89730565e-07
Iter: 1237 loss: 2.89672016e-07
Iter: 1238 loss: 2.89577827e-07
Iter: 1239 loss: 2.8938075e-07
Iter: 1240 loss: 2.8960352e-07
Iter: 1241 loss: 2.89246117e-07
Iter: 1242 loss: 2.8903429e-07
Iter: 1243 loss: 2.89049041e-07
Iter: 1244 loss: 2.88890078e-07
Iter: 1245 loss: 2.89329307e-07
Iter: 1246 loss: 2.88829682e-07
Iter: 1247 loss: 2.88707952e-07
Iter: 1248 loss: 2.88677086e-07
Iter: 1249 loss: 2.88575279e-07
Iter: 1250 loss: 2.88356489e-07
Iter: 1251 loss: 2.90079839e-07
Iter: 1252 loss: 2.88337617e-07
Iter: 1253 loss: 2.88190449e-07
Iter: 1254 loss: 2.88004117e-07
Iter: 1255 loss: 2.87984875e-07
Iter: 1256 loss: 2.88003662e-07
Iter: 1257 loss: 2.87887474e-07
Iter: 1258 loss: 2.87824662e-07
Iter: 1259 loss: 2.87622441e-07
Iter: 1260 loss: 2.89496512e-07
Iter: 1261 loss: 2.87593252e-07
Iter: 1262 loss: 2.87344875e-07
Iter: 1263 loss: 2.89030879e-07
Iter: 1264 loss: 2.87337912e-07
Iter: 1265 loss: 2.87163147e-07
Iter: 1266 loss: 2.8718847e-07
Iter: 1267 loss: 2.87052472e-07
Iter: 1268 loss: 2.86845e-07
Iter: 1269 loss: 2.86883903e-07
Iter: 1270 loss: 2.86680745e-07
Iter: 1271 loss: 2.86460136e-07
Iter: 1272 loss: 2.87641569e-07
Iter: 1273 loss: 2.86434783e-07
Iter: 1274 loss: 2.86218722e-07
Iter: 1275 loss: 2.86865941e-07
Iter: 1276 loss: 2.86175748e-07
Iter: 1277 loss: 2.85994361e-07
Iter: 1278 loss: 2.87855016e-07
Iter: 1279 loss: 2.86008202e-07
Iter: 1280 loss: 2.85849723e-07
Iter: 1281 loss: 2.85693432e-07
Iter: 1282 loss: 2.85667511e-07
Iter: 1283 loss: 2.85527562e-07
Iter: 1284 loss: 2.85523072e-07
Iter: 1285 loss: 2.85414984e-07
Iter: 1286 loss: 2.85235785e-07
Iter: 1287 loss: 2.85240162e-07
Iter: 1288 loss: 2.85031035e-07
Iter: 1289 loss: 2.87426474e-07
Iter: 1290 loss: 2.85008497e-07
Iter: 1291 loss: 2.84871277e-07
Iter: 1292 loss: 2.85340803e-07
Iter: 1293 loss: 2.8484763e-07
Iter: 1294 loss: 2.84723285e-07
Iter: 1295 loss: 2.84506541e-07
Iter: 1296 loss: 2.84510179e-07
Iter: 1297 loss: 2.84275814e-07
Iter: 1298 loss: 2.86944868e-07
Iter: 1299 loss: 2.84271607e-07
Iter: 1300 loss: 2.84152e-07
Iter: 1301 loss: 2.83973577e-07
Iter: 1302 loss: 2.83973577e-07
Iter: 1303 loss: 2.83732476e-07
Iter: 1304 loss: 2.84607665e-07
Iter: 1305 loss: 2.83667589e-07
Iter: 1306 loss: 2.83523605e-07
Iter: 1307 loss: 2.83822203e-07
Iter: 1308 loss: 2.83455023e-07
Iter: 1309 loss: 2.83293843e-07
Iter: 1310 loss: 2.84659961e-07
Iter: 1311 loss: 2.8325934e-07
Iter: 1312 loss: 2.83141219e-07
Iter: 1313 loss: 2.83865432e-07
Iter: 1314 loss: 2.83109046e-07
Iter: 1315 loss: 2.82971939e-07
Iter: 1316 loss: 2.82821588e-07
Iter: 1317 loss: 2.82773982e-07
Iter: 1318 loss: 2.82588843e-07
Iter: 1319 loss: 2.85741635e-07
Iter: 1320 loss: 2.825862e-07
Iter: 1321 loss: 2.82478169e-07
Iter: 1322 loss: 2.82329751e-07
Iter: 1323 loss: 2.82316506e-07
Iter: 1324 loss: 2.82247072e-07
Iter: 1325 loss: 2.82204439e-07
Iter: 1326 loss: 2.82146175e-07
Iter: 1327 loss: 2.81994176e-07
Iter: 1328 loss: 2.83445e-07
Iter: 1329 loss: 2.81984455e-07
Iter: 1330 loss: 2.81810827e-07
Iter: 1331 loss: 2.83258572e-07
Iter: 1332 loss: 2.81812163e-07
Iter: 1333 loss: 2.81697453e-07
Iter: 1334 loss: 2.81785702e-07
Iter: 1335 loss: 2.81629752e-07
Iter: 1336 loss: 2.81493044e-07
Iter: 1337 loss: 2.81363185e-07
Iter: 1338 loss: 2.81337236e-07
Iter: 1339 loss: 2.81128735e-07
Iter: 1340 loss: 2.82378e-07
Iter: 1341 loss: 2.81118e-07
Iter: 1342 loss: 2.80940753e-07
Iter: 1343 loss: 2.81908285e-07
Iter: 1344 loss: 2.80884024e-07
Iter: 1345 loss: 2.8075911e-07
Iter: 1346 loss: 2.8144575e-07
Iter: 1347 loss: 2.80750953e-07
Iter: 1348 loss: 2.80608504e-07
Iter: 1349 loss: 2.8063937e-07
Iter: 1350 loss: 2.80500444e-07
Iter: 1351 loss: 2.80332443e-07
Iter: 1352 loss: 2.81663489e-07
Iter: 1353 loss: 2.80332301e-07
Iter: 1354 loss: 2.80204233e-07
Iter: 1355 loss: 2.80589205e-07
Iter: 1356 loss: 2.80161572e-07
Iter: 1357 loss: 2.80039444e-07
Iter: 1358 loss: 2.80020572e-07
Iter: 1359 loss: 2.79941673e-07
Iter: 1360 loss: 2.7975986e-07
Iter: 1361 loss: 2.82097773e-07
Iter: 1362 loss: 2.79761451e-07
Iter: 1363 loss: 2.79667347e-07
Iter: 1364 loss: 2.79440144e-07
Iter: 1365 loss: 2.81572255e-07
Iter: 1366 loss: 2.79423887e-07
Iter: 1367 loss: 2.79254095e-07
Iter: 1368 loss: 2.79234058e-07
Iter: 1369 loss: 2.79170536e-07
Iter: 1370 loss: 2.79111873e-07
Iter: 1371 loss: 2.79072083e-07
Iter: 1372 loss: 2.78943901e-07
Iter: 1373 loss: 2.78904281e-07
Iter: 1374 loss: 2.78819925e-07
Iter: 1375 loss: 2.78585674e-07
Iter: 1376 loss: 2.78958282e-07
Iter: 1377 loss: 2.78496657e-07
Iter: 1378 loss: 2.78296739e-07
Iter: 1379 loss: 2.80092308e-07
Iter: 1380 loss: 2.78280652e-07
Iter: 1381 loss: 2.78113447e-07
Iter: 1382 loss: 2.79280869e-07
Iter: 1383 loss: 2.78093154e-07
Iter: 1384 loss: 2.77954769e-07
Iter: 1385 loss: 2.78111713e-07
Iter: 1386 loss: 2.77909976e-07
Iter: 1387 loss: 2.77772045e-07
Iter: 1388 loss: 2.78778344e-07
Iter: 1389 loss: 2.77769914e-07
Iter: 1390 loss: 2.77617062e-07
Iter: 1391 loss: 2.77566e-07
Iter: 1392 loss: 2.77493427e-07
Iter: 1393 loss: 2.77419531e-07
Iter: 1394 loss: 2.77413136e-07
Iter: 1395 loss: 2.77320282e-07
Iter: 1396 loss: 2.77117067e-07
Iter: 1397 loss: 2.79489598e-07
Iter: 1398 loss: 2.77112349e-07
Iter: 1399 loss: 2.76959099e-07
Iter: 1400 loss: 2.78532696e-07
Iter: 1401 loss: 2.76961032e-07
Iter: 1402 loss: 2.76875426e-07
Iter: 1403 loss: 2.77063975e-07
Iter: 1404 loss: 2.76849789e-07
Iter: 1405 loss: 2.76693953e-07
Iter: 1406 loss: 2.76568528e-07
Iter: 1407 loss: 2.76561821e-07
Iter: 1408 loss: 2.76353489e-07
Iter: 1409 loss: 2.76604197e-07
Iter: 1410 loss: 2.76216895e-07
Iter: 1411 loss: 2.76000605e-07
Iter: 1412 loss: 2.77530035e-07
Iter: 1413 loss: 2.75956893e-07
Iter: 1414 loss: 2.75789262e-07
Iter: 1415 loss: 2.75746572e-07
Iter: 1416 loss: 2.75622085e-07
Iter: 1417 loss: 2.75478271e-07
Iter: 1418 loss: 2.77074861e-07
Iter: 1419 loss: 2.75472559e-07
Iter: 1420 loss: 2.75307912e-07
Iter: 1421 loss: 2.7671976e-07
Iter: 1422 loss: 2.75312857e-07
Iter: 1423 loss: 2.75212471e-07
Iter: 1424 loss: 2.75432598e-07
Iter: 1425 loss: 2.75163785e-07
Iter: 1426 loss: 2.75056891e-07
Iter: 1427 loss: 2.75529089e-07
Iter: 1428 loss: 2.75039042e-07
Iter: 1429 loss: 2.74935473e-07
Iter: 1430 loss: 2.74958325e-07
Iter: 1431 loss: 2.7488133e-07
Iter: 1432 loss: 2.74751415e-07
Iter: 1433 loss: 2.74758747e-07
Iter: 1434 loss: 2.74708697e-07
Iter: 1435 loss: 2.74516935e-07
Iter: 1436 loss: 2.75722641e-07
Iter: 1437 loss: 2.74489622e-07
Iter: 1438 loss: 2.74340437e-07
Iter: 1439 loss: 2.7558562e-07
Iter: 1440 loss: 2.74322787e-07
Iter: 1441 loss: 2.74177069e-07
Iter: 1442 loss: 2.74648499e-07
Iter: 1443 loss: 2.74122556e-07
Iter: 1444 loss: 2.73978856e-07
Iter: 1445 loss: 2.73904845e-07
Iter: 1446 loss: 2.7383993e-07
Iter: 1447 loss: 2.7360187e-07
Iter: 1448 loss: 2.73697111e-07
Iter: 1449 loss: 2.73406954e-07
Iter: 1450 loss: 2.73140984e-07
Iter: 1451 loss: 2.73467606e-07
Iter: 1452 loss: 2.73000353e-07
Iter: 1453 loss: 2.72818568e-07
Iter: 1454 loss: 2.72802538e-07
Iter: 1455 loss: 2.7264781e-07
Iter: 1456 loss: 2.73110402e-07
Iter: 1457 loss: 2.72610635e-07
Iter: 1458 loss: 2.72461875e-07
Iter: 1459 loss: 2.72641e-07
Iter: 1460 loss: 2.7238616e-07
Iter: 1461 loss: 2.72281909e-07
Iter: 1462 loss: 2.73730905e-07
Iter: 1463 loss: 2.72281e-07
Iter: 1464 loss: 2.72223701e-07
Iter: 1465 loss: 2.72290606e-07
Iter: 1466 loss: 2.72173764e-07
Iter: 1467 loss: 2.72055047e-07
Iter: 1468 loss: 2.72117404e-07
Iter: 1469 loss: 2.71980241e-07
Iter: 1470 loss: 2.7188679e-07
Iter: 1471 loss: 2.71817669e-07
Iter: 1472 loss: 2.7180522e-07
Iter: 1473 loss: 2.71644524e-07
Iter: 1474 loss: 2.72068746e-07
Iter: 1475 loss: 2.71608485e-07
Iter: 1476 loss: 2.7147405e-07
Iter: 1477 loss: 2.73069247e-07
Iter: 1478 loss: 2.71474505e-07
Iter: 1479 loss: 2.71408737e-07
Iter: 1480 loss: 2.71188014e-07
Iter: 1481 loss: 2.7283204e-07
Iter: 1482 loss: 2.71144842e-07
Iter: 1483 loss: 2.70984401e-07
Iter: 1484 loss: 2.73763362e-07
Iter: 1485 loss: 2.709819e-07
Iter: 1486 loss: 2.70836949e-07
Iter: 1487 loss: 2.70686826e-07
Iter: 1488 loss: 2.7065127e-07
Iter: 1489 loss: 2.70434839e-07
Iter: 1490 loss: 2.72201135e-07
Iter: 1491 loss: 2.70414318e-07
Iter: 1492 loss: 2.70202406e-07
Iter: 1493 loss: 2.72294841e-07
Iter: 1494 loss: 2.70204282e-07
Iter: 1495 loss: 2.70112423e-07
Iter: 1496 loss: 2.7017029e-07
Iter: 1497 loss: 2.70067886e-07
Iter: 1498 loss: 2.69937061e-07
Iter: 1499 loss: 2.70326268e-07
Iter: 1500 loss: 2.69880218e-07
Iter: 1501 loss: 2.69784465e-07
Iter: 1502 loss: 2.70624298e-07
Iter: 1503 loss: 2.69771903e-07
Iter: 1504 loss: 2.697019e-07
Iter: 1505 loss: 2.69633631e-07
Iter: 1506 loss: 2.69605152e-07
Iter: 1507 loss: 2.69509201e-07
Iter: 1508 loss: 2.69452926e-07
Iter: 1509 loss: 2.69387527e-07
Iter: 1510 loss: 2.69276768e-07
Iter: 1511 loss: 2.70227702e-07
Iter: 1512 loss: 2.69271595e-07
Iter: 1513 loss: 2.69093562e-07
Iter: 1514 loss: 2.69329632e-07
Iter: 1515 loss: 2.69021285e-07
Iter: 1516 loss: 2.68882843e-07
Iter: 1517 loss: 2.68844985e-07
Iter: 1518 loss: 2.68779928e-07
Iter: 1519 loss: 2.68618635e-07
Iter: 1520 loss: 2.69633063e-07
Iter: 1521 loss: 2.68581573e-07
Iter: 1522 loss: 2.68455835e-07
Iter: 1523 loss: 2.68377534e-07
Iter: 1524 loss: 2.6829855e-07
Iter: 1525 loss: 2.68213199e-07
Iter: 1526 loss: 2.68207714e-07
Iter: 1527 loss: 2.68087064e-07
Iter: 1528 loss: 2.68232384e-07
Iter: 1529 loss: 2.68023143e-07
Iter: 1530 loss: 2.67903886e-07
Iter: 1531 loss: 2.6836409e-07
Iter: 1532 loss: 2.6789192e-07
Iter: 1533 loss: 2.6781737e-07
Iter: 1534 loss: 2.68122221e-07
Iter: 1535 loss: 2.6778892e-07
Iter: 1536 loss: 2.67696691e-07
Iter: 1537 loss: 2.6771923e-07
Iter: 1538 loss: 2.67634789e-07
Iter: 1539 loss: 2.67460592e-07
Iter: 1540 loss: 2.67680036e-07
Iter: 1541 loss: 2.67358899e-07
Iter: 1542 loss: 2.67266955e-07
Iter: 1543 loss: 2.670713e-07
Iter: 1544 loss: 2.71271631e-07
Iter: 1545 loss: 2.67071783e-07
Iter: 1546 loss: 2.66956135e-07
Iter: 1547 loss: 2.66922683e-07
Iter: 1548 loss: 2.66802772e-07
Iter: 1549 loss: 2.66706706e-07
Iter: 1550 loss: 2.66653444e-07
Iter: 1551 loss: 2.66504173e-07
Iter: 1552 loss: 2.67042e-07
Iter: 1553 loss: 2.66423825e-07
Iter: 1554 loss: 2.66273076e-07
Iter: 1555 loss: 2.66200857e-07
Iter: 1556 loss: 2.66118093e-07
Iter: 1557 loss: 2.6596706e-07
Iter: 1558 loss: 2.67338748e-07
Iter: 1559 loss: 2.65957482e-07
Iter: 1560 loss: 2.65859e-07
Iter: 1561 loss: 2.66939509e-07
Iter: 1562 loss: 2.65861047e-07
Iter: 1563 loss: 2.65763475e-07
Iter: 1564 loss: 2.65876054e-07
Iter: 1565 loss: 2.65745484e-07
Iter: 1566 loss: 2.65673577e-07
Iter: 1567 loss: 2.66192501e-07
Iter: 1568 loss: 2.656584e-07
Iter: 1569 loss: 2.65585243e-07
Iter: 1570 loss: 2.65652716e-07
Iter: 1571 loss: 2.65551137e-07
Iter: 1572 loss: 2.65492588e-07
Iter: 1573 loss: 2.65887962e-07
Iter: 1574 loss: 2.65472778e-07
Iter: 1575 loss: 2.65405305e-07
Iter: 1576 loss: 2.65254045e-07
Iter: 1577 loss: 2.67185897e-07
Iter: 1578 loss: 2.65247934e-07
Iter: 1579 loss: 2.65099175e-07
Iter: 1580 loss: 2.65391748e-07
Iter: 1581 loss: 2.65015387e-07
Iter: 1582 loss: 2.64857931e-07
Iter: 1583 loss: 2.66941697e-07
Iter: 1584 loss: 2.64848381e-07
Iter: 1585 loss: 2.64723099e-07
Iter: 1586 loss: 2.65260439e-07
Iter: 1587 loss: 2.6469985e-07
Iter: 1588 loss: 2.64612709e-07
Iter: 1589 loss: 2.64371693e-07
Iter: 1590 loss: 2.66797798e-07
Iter: 1591 loss: 2.64349524e-07
Iter: 1592 loss: 2.64164868e-07
Iter: 1593 loss: 2.66683315e-07
Iter: 1594 loss: 2.64156341e-07
Iter: 1595 loss: 2.63995958e-07
Iter: 1596 loss: 2.64650225e-07
Iter: 1597 loss: 2.63977199e-07
Iter: 1598 loss: 2.63854304e-07
Iter: 1599 loss: 2.65478661e-07
Iter: 1600 loss: 2.63866809e-07
Iter: 1601 loss: 2.63784528e-07
Iter: 1602 loss: 2.63800302e-07
Iter: 1603 loss: 2.63724218e-07
Iter: 1604 loss: 2.63630398e-07
Iter: 1605 loss: 2.64240668e-07
Iter: 1606 loss: 2.63605671e-07
Iter: 1607 loss: 2.63566164e-07
Iter: 1608 loss: 2.63587594e-07
Iter: 1609 loss: 2.63515034e-07
Iter: 1610 loss: 2.63432355e-07
Iter: 1611 loss: 2.63384209e-07
Iter: 1612 loss: 2.63331458e-07
Iter: 1613 loss: 2.63211945e-07
Iter: 1614 loss: 2.63111531e-07
Iter: 1615 loss: 2.63072366e-07
Iter: 1616 loss: 2.62911271e-07
Iter: 1617 loss: 2.64334744e-07
Iter: 1618 loss: 2.62867047e-07
Iter: 1619 loss: 2.62727326e-07
Iter: 1620 loss: 2.63622496e-07
Iter: 1621 loss: 2.62705043e-07
Iter: 1622 loss: 2.62570836e-07
Iter: 1623 loss: 2.62690435e-07
Iter: 1624 loss: 2.62497394e-07
Iter: 1625 loss: 2.62377e-07
Iter: 1626 loss: 2.62301683e-07
Iter: 1627 loss: 2.62257799e-07
Iter: 1628 loss: 2.62135103e-07
Iter: 1629 loss: 2.62134847e-07
Iter: 1630 loss: 2.62059359e-07
Iter: 1631 loss: 2.62673666e-07
Iter: 1632 loss: 2.62063423e-07
Iter: 1633 loss: 2.61948401e-07
Iter: 1634 loss: 2.61948429e-07
Iter: 1635 loss: 2.61886072e-07
Iter: 1636 loss: 2.61813e-07
Iter: 1637 loss: 2.61789637e-07
Iter: 1638 loss: 2.61754622e-07
Iter: 1639 loss: 2.61688854e-07
Iter: 1640 loss: 2.63339928e-07
Iter: 1641 loss: 2.616747e-07
Iter: 1642 loss: 2.61580169e-07
Iter: 1643 loss: 2.62219146e-07
Iter: 1644 loss: 2.61545807e-07
Iter: 1645 loss: 2.6146364e-07
Iter: 1646 loss: 2.61315733e-07
Iter: 1647 loss: 2.61323919e-07
Iter: 1648 loss: 2.61141025e-07
Iter: 1649 loss: 2.61808736e-07
Iter: 1650 loss: 2.61094982e-07
Iter: 1651 loss: 2.60967738e-07
Iter: 1652 loss: 2.6211552e-07
Iter: 1653 loss: 2.60966658e-07
Iter: 1654 loss: 2.60830291e-07
Iter: 1655 loss: 2.61061075e-07
Iter: 1656 loss: 2.60767962e-07
Iter: 1657 loss: 2.6064194e-07
Iter: 1658 loss: 2.6058359e-07
Iter: 1659 loss: 2.60528168e-07
Iter: 1660 loss: 2.60417096e-07
Iter: 1661 loss: 2.61419075e-07
Iter: 1662 loss: 2.60385093e-07
Iter: 1663 loss: 2.6031131e-07
Iter: 1664 loss: 2.60496563e-07
Iter: 1665 loss: 2.60263363e-07
Iter: 1666 loss: 2.60165507e-07
Iter: 1667 loss: 2.61672227e-07
Iter: 1668 loss: 2.60166246e-07
Iter: 1669 loss: 2.60124295e-07
Iter: 1670 loss: 2.6017841e-07
Iter: 1671 loss: 2.60098659e-07
Iter: 1672 loss: 2.59994408e-07
Iter: 1673 loss: 2.59999297e-07
Iter: 1674 loss: 2.59935405e-07
Iter: 1675 loss: 2.598332e-07
Iter: 1676 loss: 2.6074224e-07
Iter: 1677 loss: 2.59833598e-07
Iter: 1678 loss: 2.59779512e-07
Iter: 1679 loss: 2.59660169e-07
Iter: 1680 loss: 2.59660396e-07
Iter: 1681 loss: 2.59512404e-07
Iter: 1682 loss: 2.59585363e-07
Iter: 1683 loss: 2.59421938e-07
Iter: 1684 loss: 2.59264482e-07
Iter: 1685 loss: 2.59882682e-07
Iter: 1686 loss: 2.5924075e-07
Iter: 1687 loss: 2.59112767e-07
Iter: 1688 loss: 2.59114159e-07
Iter: 1689 loss: 2.59020737e-07
Iter: 1690 loss: 2.58860865e-07
Iter: 1691 loss: 2.62018176e-07
Iter: 1692 loss: 2.5885177e-07
Iter: 1693 loss: 2.58653245e-07
Iter: 1694 loss: 2.59881062e-07
Iter: 1695 loss: 2.58629655e-07
Iter: 1696 loss: 2.58504429e-07
Iter: 1697 loss: 2.589492e-07
Iter: 1698 loss: 2.58497209e-07
Iter: 1699 loss: 2.5840086e-07
Iter: 1700 loss: 2.58400206e-07
Iter: 1701 loss: 2.58307182e-07
Iter: 1702 loss: 2.58309626e-07
Iter: 1703 loss: 2.58249429e-07
Iter: 1704 loss: 2.58155296e-07
Iter: 1705 loss: 2.5914494e-07
Iter: 1706 loss: 2.58150862e-07
Iter: 1707 loss: 2.58099021e-07
Iter: 1708 loss: 2.5810408e-07
Iter: 1709 loss: 2.58066621e-07
Iter: 1710 loss: 2.57952649e-07
Iter: 1711 loss: 2.58028081e-07
Iter: 1712 loss: 2.57907857e-07
Iter: 1713 loss: 2.57796046e-07
Iter: 1714 loss: 2.57812673e-07
Iter: 1715 loss: 2.57692591e-07
Iter: 1716 loss: 2.57571457e-07
Iter: 1717 loss: 2.57718824e-07
Iter: 1718 loss: 2.57526239e-07
Iter: 1719 loss: 2.57442878e-07
Iter: 1720 loss: 2.57453252e-07
Iter: 1721 loss: 2.57362728e-07
Iter: 1722 loss: 2.57357584e-07
Iter: 1723 loss: 2.57306141e-07
Iter: 1724 loss: 2.57199986e-07
Iter: 1725 loss: 2.5726132e-07
Iter: 1726 loss: 2.57117733e-07
Iter: 1727 loss: 2.56991541e-07
Iter: 1728 loss: 2.57238696e-07
Iter: 1729 loss: 2.56953371e-07
Iter: 1730 loss: 2.56878025e-07
Iter: 1731 loss: 2.56870152e-07
Iter: 1732 loss: 2.5679833e-07
Iter: 1733 loss: 2.56805379e-07
Iter: 1734 loss: 2.56742112e-07
Iter: 1735 loss: 2.5666597e-07
Iter: 1736 loss: 2.57370658e-07
Iter: 1737 loss: 2.56658637e-07
Iter: 1738 loss: 2.56579028e-07
Iter: 1739 loss: 2.56579966e-07
Iter: 1740 loss: 2.56534292e-07
Iter: 1741 loss: 2.56417707e-07
Iter: 1742 loss: 2.56681176e-07
Iter: 1743 loss: 2.56381185e-07
Iter: 1744 loss: 2.56299018e-07
Iter: 1745 loss: 2.56326359e-07
Iter: 1746 loss: 2.56219437e-07
Iter: 1747 loss: 2.56108706e-07
Iter: 1748 loss: 2.56066073e-07
Iter: 1749 loss: 2.55988e-07
Iter: 1750 loss: 2.55890029e-07
Iter: 1751 loss: 2.57183217e-07
Iter: 1752 loss: 2.55890484e-07
Iter: 1753 loss: 2.55761506e-07
Iter: 1754 loss: 2.56107597e-07
Iter: 1755 loss: 2.55701195e-07
Iter: 1756 loss: 2.55593079e-07
Iter: 1757 loss: 2.55788e-07
Iter: 1758 loss: 2.55535895e-07
Iter: 1759 loss: 2.55432184e-07
Iter: 1760 loss: 2.55388841e-07
Iter: 1761 loss: 2.55335749e-07
Iter: 1762 loss: 2.5520248e-07
Iter: 1763 loss: 2.56818538e-07
Iter: 1764 loss: 2.55182499e-07
Iter: 1765 loss: 2.55102577e-07
Iter: 1766 loss: 2.5576793e-07
Iter: 1767 loss: 2.55094221e-07
Iter: 1768 loss: 2.5502078e-07
Iter: 1769 loss: 2.55261341e-07
Iter: 1770 loss: 2.55025384e-07
Iter: 1771 loss: 2.54950862e-07
Iter: 1772 loss: 2.5499736e-07
Iter: 1773 loss: 2.54918035e-07
Iter: 1774 loss: 2.5484411e-07
Iter: 1775 loss: 2.55008445e-07
Iter: 1776 loss: 2.54824613e-07
Iter: 1777 loss: 2.54737415e-07
Iter: 1778 loss: 2.54801023e-07
Iter: 1779 loss: 2.54686199e-07
Iter: 1780 loss: 2.54599655e-07
Iter: 1781 loss: 2.54523627e-07
Iter: 1782 loss: 2.54489748e-07
Iter: 1783 loss: 2.54362959e-07
Iter: 1784 loss: 2.54664428e-07
Iter: 1785 loss: 2.5428912e-07
Iter: 1786 loss: 2.54190951e-07
Iter: 1787 loss: 2.54193424e-07
Iter: 1788 loss: 2.54087752e-07
Iter: 1789 loss: 2.54251432e-07
Iter: 1790 loss: 2.54064389e-07
Iter: 1791 loss: 2.539756e-07
Iter: 1792 loss: 2.53802909e-07
Iter: 1793 loss: 2.53793132e-07
Iter: 1794 loss: 2.53646419e-07
Iter: 1795 loss: 2.55002817e-07
Iter: 1796 loss: 2.53623568e-07
Iter: 1797 loss: 2.53580481e-07
Iter: 1798 loss: 2.53564252e-07
Iter: 1799 loss: 2.53501241e-07
Iter: 1800 loss: 2.53521819e-07
Iter: 1801 loss: 2.53468443e-07
Iter: 1802 loss: 2.53374935e-07
Iter: 1803 loss: 2.53846821e-07
Iter: 1804 loss: 2.53398213e-07
Iter: 1805 loss: 2.53314738e-07
Iter: 1806 loss: 2.53249e-07
Iter: 1807 loss: 2.53235726e-07
Iter: 1808 loss: 2.53154326e-07
Iter: 1809 loss: 2.54182254e-07
Iter: 1810 loss: 2.53158561e-07
Iter: 1811 loss: 2.53097369e-07
Iter: 1812 loss: 2.53024723e-07
Iter: 1813 loss: 2.53002156e-07
Iter: 1814 loss: 2.5290737e-07
Iter: 1815 loss: 2.52884064e-07
Iter: 1816 loss: 2.52841858e-07
Iter: 1817 loss: 2.52721577e-07
Iter: 1818 loss: 2.53616719e-07
Iter: 1819 loss: 2.52717967e-07
Iter: 1820 loss: 2.52614456e-07
Iter: 1821 loss: 2.53507693e-07
Iter: 1822 loss: 2.52606611e-07
Iter: 1823 loss: 2.52557925e-07
Iter: 1824 loss: 2.5251336e-07
Iter: 1825 loss: 2.5249642e-07
Iter: 1826 loss: 2.52378697e-07
Iter: 1827 loss: 2.52416527e-07
Iter: 1828 loss: 2.52307444e-07
Iter: 1829 loss: 2.52229256e-07
Iter: 1830 loss: 2.53216655e-07
Iter: 1831 loss: 2.52210555e-07
Iter: 1832 loss: 2.52170793e-07
Iter: 1833 loss: 2.53040582e-07
Iter: 1834 loss: 2.52164227e-07
Iter: 1835 loss: 2.52127194e-07
Iter: 1836 loss: 2.52120913e-07
Iter: 1837 loss: 2.52077371e-07
Iter: 1838 loss: 2.52002962e-07
Iter: 1839 loss: 2.52300765e-07
Iter: 1840 loss: 2.51967492e-07
Iter: 1841 loss: 2.51942538e-07
Iter: 1842 loss: 2.51949359e-07
Iter: 1843 loss: 2.51905902e-07
Iter: 1844 loss: 2.51829022e-07
Iter: 1845 loss: 2.51845023e-07
Iter: 1846 loss: 2.51756489e-07
Iter: 1847 loss: 2.51669547e-07
Iter: 1848 loss: 2.5172065e-07
Iter: 1849 loss: 2.51638824e-07
Iter: 1850 loss: 2.51481794e-07
Iter: 1851 loss: 2.51484551e-07
Iter: 1852 loss: 2.51374388e-07
Iter: 1853 loss: 2.51228641e-07
Iter: 1854 loss: 2.53067981e-07
Iter: 1855 loss: 2.51222019e-07
Iter: 1856 loss: 2.5112945e-07
Iter: 1857 loss: 2.52091894e-07
Iter: 1858 loss: 2.51121747e-07
Iter: 1859 loss: 2.510684e-07
Iter: 1860 loss: 2.50980463e-07
Iter: 1861 loss: 2.52955147e-07
Iter: 1862 loss: 2.50969123e-07
Iter: 1863 loss: 2.50843812e-07
Iter: 1864 loss: 2.5195348e-07
Iter: 1865 loss: 2.50849439e-07
Iter: 1866 loss: 2.50795154e-07
Iter: 1867 loss: 2.5080314e-07
Iter: 1868 loss: 2.50747803e-07
Iter: 1869 loss: 2.50752066e-07
Iter: 1870 loss: 2.50715061e-07
Iter: 1871 loss: 2.50663135e-07
Iter: 1872 loss: 2.51154091e-07
Iter: 1873 loss: 2.5066754e-07
Iter: 1874 loss: 2.50630734e-07
Iter: 1875 loss: 2.50510936e-07
Iter: 1876 loss: 2.51453031e-07
Iter: 1877 loss: 2.50503859e-07
Iter: 1878 loss: 2.50391025e-07
Iter: 1879 loss: 2.50395942e-07
Iter: 1880 loss: 2.503171e-07
Iter: 1881 loss: 2.50250764e-07
Iter: 1882 loss: 2.50253265e-07
Iter: 1883 loss: 2.50130626e-07
Iter: 1884 loss: 2.50072418e-07
Iter: 1885 loss: 2.50009663e-07
Iter: 1886 loss: 2.49838763e-07
Iter: 1887 loss: 2.50143103e-07
Iter: 1888 loss: 2.49762053e-07
Iter: 1889 loss: 2.49750656e-07
Iter: 1890 loss: 2.49678948e-07
Iter: 1891 loss: 2.49633672e-07
Iter: 1892 loss: 2.49614232e-07
Iter: 1893 loss: 2.49599452e-07
Iter: 1894 loss: 2.49518934e-07
Iter: 1895 loss: 2.49520269e-07
Iter: 1896 loss: 2.49453251e-07
Iter: 1897 loss: 2.49366622e-07
Iter: 1898 loss: 2.49642369e-07
Iter: 1899 loss: 2.49347e-07
Iter: 1900 loss: 2.49319044e-07
Iter: 1901 loss: 2.49286842e-07
Iter: 1902 loss: 2.4926058e-07
Iter: 1903 loss: 2.49216384e-07
Iter: 1904 loss: 2.50206313e-07
Iter: 1905 loss: 2.49222126e-07
Iter: 1906 loss: 2.49116056e-07
Iter: 1907 loss: 2.49528568e-07
Iter: 1908 loss: 2.49102527e-07
Iter: 1909 loss: 2.49052079e-07
Iter: 1910 loss: 2.49062282e-07
Iter: 1911 loss: 2.4901783e-07
Iter: 1912 loss: 2.48923413e-07
Iter: 1913 loss: 2.49429576e-07
Iter: 1914 loss: 2.48906389e-07
Iter: 1915 loss: 2.4885523e-07
Iter: 1916 loss: 2.48922902e-07
Iter: 1917 loss: 2.48811403e-07
Iter: 1918 loss: 2.48749188e-07
Iter: 1919 loss: 2.48716617e-07
Iter: 1920 loss: 2.48680976e-07
Iter: 1921 loss: 2.48592897e-07
Iter: 1922 loss: 2.48724035e-07
Iter: 1923 loss: 2.48546883e-07
Iter: 1924 loss: 2.48462982e-07
Iter: 1925 loss: 2.48463721e-07
Iter: 1926 loss: 2.48391757e-07
Iter: 1927 loss: 2.48466847e-07
Iter: 1928 loss: 2.48366973e-07
Iter: 1929 loss: 2.48285829e-07
Iter: 1930 loss: 2.48221198e-07
Iter: 1931 loss: 2.48201047e-07
Iter: 1932 loss: 2.48100775e-07
Iter: 1933 loss: 2.48762319e-07
Iter: 1934 loss: 2.48103788e-07
Iter: 1935 loss: 2.47990386e-07
Iter: 1936 loss: 2.48613105e-07
Iter: 1937 loss: 2.47983849e-07
Iter: 1938 loss: 2.47911373e-07
Iter: 1939 loss: 2.47928739e-07
Iter: 1940 loss: 2.47845293e-07
Iter: 1941 loss: 2.4777998e-07
Iter: 1942 loss: 2.48619699e-07
Iter: 1943 loss: 2.47784811e-07
Iter: 1944 loss: 2.47735102e-07
Iter: 1945 loss: 2.47622495e-07
Iter: 1946 loss: 2.4869189e-07
Iter: 1947 loss: 2.47629714e-07
Iter: 1948 loss: 2.4750085e-07
Iter: 1949 loss: 2.48731453e-07
Iter: 1950 loss: 2.47510343e-07
Iter: 1951 loss: 2.47393785e-07
Iter: 1952 loss: 2.47506563e-07
Iter: 1953 loss: 2.47313722e-07
Iter: 1954 loss: 2.47249318e-07
Iter: 1955 loss: 2.47339756e-07
Iter: 1956 loss: 2.47212483e-07
Iter: 1957 loss: 2.47130259e-07
Iter: 1958 loss: 2.47282571e-07
Iter: 1959 loss: 2.47100331e-07
Iter: 1960 loss: 2.47015294e-07
Iter: 1961 loss: 2.47189689e-07
Iter: 1962 loss: 2.46998354e-07
Iter: 1963 loss: 2.46933268e-07
Iter: 1964 loss: 2.47187302e-07
Iter: 1965 loss: 2.4688768e-07
Iter: 1966 loss: 2.46824868e-07
Iter: 1967 loss: 2.47621188e-07
Iter: 1968 loss: 2.46811283e-07
Iter: 1969 loss: 2.46767797e-07
Iter: 1970 loss: 2.46735965e-07
Iter: 1971 loss: 2.46707884e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi1.6/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2
+ date
Mon Oct 26 18:18:08 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -1 --phi 2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef67bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef53c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef53cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef53cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef61a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef61ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef49ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef4579d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef45fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef3bad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef3f69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef3f6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef418730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef3f61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef2f6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef2f6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef354510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef2d7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef2bf9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef26cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef272620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef2949d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef205378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef205bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef1eb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef205598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ef1d47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bd4da400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bd4da950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bd4c2378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bd4c2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bd4621e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bd44e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bd462510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bd4267b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87984c8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.31465058e-05
Iter: 2 loss: 1.50327751e-05
Iter: 3 loss: 1.06673915e-05
Iter: 4 loss: 9.62754e-06
Iter: 5 loss: 1.36671988e-05
Iter: 6 loss: 9.38747417e-06
Iter: 7 loss: 8.91134e-06
Iter: 8 loss: 9.4138577e-06
Iter: 9 loss: 8.64809e-06
Iter: 10 loss: 8.1604112e-06
Iter: 11 loss: 8.21895264e-06
Iter: 12 loss: 7.78693629e-06
Iter: 13 loss: 7.47130935e-06
Iter: 14 loss: 7.47123931e-06
Iter: 15 loss: 7.28346868e-06
Iter: 16 loss: 7.01444787e-06
Iter: 17 loss: 7.00562941e-06
Iter: 18 loss: 6.70592681e-06
Iter: 19 loss: 7.48154025e-06
Iter: 20 loss: 6.60416026e-06
Iter: 21 loss: 6.23300275e-06
Iter: 22 loss: 7.34638525e-06
Iter: 23 loss: 6.12102849e-06
Iter: 24 loss: 5.83666269e-06
Iter: 25 loss: 8.73191129e-06
Iter: 26 loss: 5.8280657e-06
Iter: 27 loss: 5.62521836e-06
Iter: 28 loss: 5.60940407e-06
Iter: 29 loss: 5.45876901e-06
Iter: 30 loss: 5.21438142e-06
Iter: 31 loss: 5.0084418e-06
Iter: 32 loss: 4.94030382e-06
Iter: 33 loss: 4.63408333e-06
Iter: 34 loss: 4.63094284e-06
Iter: 35 loss: 4.53092343e-06
Iter: 36 loss: 5.15811553e-06
Iter: 37 loss: 4.51939e-06
Iter: 38 loss: 4.39400856e-06
Iter: 39 loss: 4.98609643e-06
Iter: 40 loss: 4.37118797e-06
Iter: 41 loss: 4.30474938e-06
Iter: 42 loss: 4.40076155e-06
Iter: 43 loss: 4.27268697e-06
Iter: 44 loss: 4.21650384e-06
Iter: 45 loss: 4.39386076e-06
Iter: 46 loss: 4.20024753e-06
Iter: 47 loss: 4.14570059e-06
Iter: 48 loss: 4.08012966e-06
Iter: 49 loss: 4.07329026e-06
Iter: 50 loss: 3.99669216e-06
Iter: 51 loss: 5.10747395e-06
Iter: 52 loss: 3.99652708e-06
Iter: 53 loss: 3.94148356e-06
Iter: 54 loss: 3.80261281e-06
Iter: 55 loss: 5.0834069e-06
Iter: 56 loss: 3.78235404e-06
Iter: 57 loss: 3.65150527e-06
Iter: 58 loss: 5.45944658e-06
Iter: 59 loss: 3.65115511e-06
Iter: 60 loss: 3.56113401e-06
Iter: 61 loss: 3.9264587e-06
Iter: 62 loss: 3.54131726e-06
Iter: 63 loss: 3.45887156e-06
Iter: 64 loss: 3.82789494e-06
Iter: 65 loss: 3.44281034e-06
Iter: 66 loss: 3.37179176e-06
Iter: 67 loss: 3.35288337e-06
Iter: 68 loss: 3.30876969e-06
Iter: 69 loss: 3.24117445e-06
Iter: 70 loss: 3.48352023e-06
Iter: 71 loss: 3.22393316e-06
Iter: 72 loss: 3.19587934e-06
Iter: 73 loss: 3.18495222e-06
Iter: 74 loss: 3.14594536e-06
Iter: 75 loss: 3.12567499e-06
Iter: 76 loss: 3.10776886e-06
Iter: 77 loss: 3.07485516e-06
Iter: 78 loss: 3.14131648e-06
Iter: 79 loss: 3.0612382e-06
Iter: 80 loss: 3.01476507e-06
Iter: 81 loss: 3.04160267e-06
Iter: 82 loss: 2.98462146e-06
Iter: 83 loss: 2.94781739e-06
Iter: 84 loss: 3.10879864e-06
Iter: 85 loss: 2.94050869e-06
Iter: 86 loss: 2.89821219e-06
Iter: 87 loss: 2.86458794e-06
Iter: 88 loss: 2.85180136e-06
Iter: 89 loss: 2.76599894e-06
Iter: 90 loss: 3.03506067e-06
Iter: 91 loss: 2.74085914e-06
Iter: 92 loss: 2.69739394e-06
Iter: 93 loss: 2.67744099e-06
Iter: 94 loss: 2.65574681e-06
Iter: 95 loss: 2.58716636e-06
Iter: 96 loss: 3.07102891e-06
Iter: 97 loss: 2.58089813e-06
Iter: 98 loss: 2.51784672e-06
Iter: 99 loss: 2.78315042e-06
Iter: 100 loss: 2.50456173e-06
Iter: 101 loss: 2.46677359e-06
Iter: 102 loss: 2.60595334e-06
Iter: 103 loss: 2.45756542e-06
Iter: 104 loss: 2.42012766e-06
Iter: 105 loss: 2.37492441e-06
Iter: 106 loss: 2.37056383e-06
Iter: 107 loss: 2.46704099e-06
Iter: 108 loss: 2.35182802e-06
Iter: 109 loss: 2.34427171e-06
Iter: 110 loss: 2.32262255e-06
Iter: 111 loss: 2.40937652e-06
Iter: 112 loss: 2.31362924e-06
Iter: 113 loss: 2.28278077e-06
Iter: 114 loss: 2.48385231e-06
Iter: 115 loss: 2.27949386e-06
Iter: 116 loss: 2.25207759e-06
Iter: 117 loss: 2.32208458e-06
Iter: 118 loss: 2.24251517e-06
Iter: 119 loss: 2.21729942e-06
Iter: 120 loss: 2.18016248e-06
Iter: 121 loss: 2.17922025e-06
Iter: 122 loss: 2.15211503e-06
Iter: 123 loss: 2.1488313e-06
Iter: 124 loss: 2.13459271e-06
Iter: 125 loss: 2.10428198e-06
Iter: 126 loss: 2.5785464e-06
Iter: 127 loss: 2.10324788e-06
Iter: 128 loss: 2.06064874e-06
Iter: 129 loss: 2.27750797e-06
Iter: 130 loss: 2.05375909e-06
Iter: 131 loss: 2.02856472e-06
Iter: 132 loss: 2.0430582e-06
Iter: 133 loss: 2.01209536e-06
Iter: 134 loss: 1.97557983e-06
Iter: 135 loss: 2.26588691e-06
Iter: 136 loss: 1.97311465e-06
Iter: 137 loss: 1.94850509e-06
Iter: 138 loss: 1.98989301e-06
Iter: 139 loss: 1.93728602e-06
Iter: 140 loss: 1.91738877e-06
Iter: 141 loss: 2.06862887e-06
Iter: 142 loss: 1.91590425e-06
Iter: 143 loss: 1.90671869e-06
Iter: 144 loss: 1.90541778e-06
Iter: 145 loss: 1.89860839e-06
Iter: 146 loss: 1.87668525e-06
Iter: 147 loss: 1.90662126e-06
Iter: 148 loss: 1.86056616e-06
Iter: 149 loss: 1.84840985e-06
Iter: 150 loss: 1.84571797e-06
Iter: 151 loss: 1.83068403e-06
Iter: 152 loss: 1.82101178e-06
Iter: 153 loss: 1.81513678e-06
Iter: 154 loss: 1.79551307e-06
Iter: 155 loss: 1.86759871e-06
Iter: 156 loss: 1.79066103e-06
Iter: 157 loss: 1.77508718e-06
Iter: 158 loss: 1.80817551e-06
Iter: 159 loss: 1.76892e-06
Iter: 160 loss: 1.74545426e-06
Iter: 161 loss: 1.76912772e-06
Iter: 162 loss: 1.73216381e-06
Iter: 163 loss: 1.71514125e-06
Iter: 164 loss: 1.69378302e-06
Iter: 165 loss: 1.69200268e-06
Iter: 166 loss: 1.66816551e-06
Iter: 167 loss: 1.66819007e-06
Iter: 168 loss: 1.6496507e-06
Iter: 169 loss: 1.69041266e-06
Iter: 170 loss: 1.64250059e-06
Iter: 171 loss: 1.62264803e-06
Iter: 172 loss: 1.71008674e-06
Iter: 173 loss: 1.61888511e-06
Iter: 174 loss: 1.60763443e-06
Iter: 175 loss: 1.73424507e-06
Iter: 176 loss: 1.60746265e-06
Iter: 177 loss: 1.5939936e-06
Iter: 178 loss: 1.60502509e-06
Iter: 179 loss: 1.58593411e-06
Iter: 180 loss: 1.57822626e-06
Iter: 181 loss: 1.56553199e-06
Iter: 182 loss: 1.56552733e-06
Iter: 183 loss: 1.55011423e-06
Iter: 184 loss: 1.67964708e-06
Iter: 185 loss: 1.54920781e-06
Iter: 186 loss: 1.53816416e-06
Iter: 187 loss: 1.60515879e-06
Iter: 188 loss: 1.53676228e-06
Iter: 189 loss: 1.52948451e-06
Iter: 190 loss: 1.51004224e-06
Iter: 191 loss: 1.64285041e-06
Iter: 192 loss: 1.50542201e-06
Iter: 193 loss: 1.50097401e-06
Iter: 194 loss: 1.49383709e-06
Iter: 195 loss: 1.48691583e-06
Iter: 196 loss: 1.48237064e-06
Iter: 197 loss: 1.47965159e-06
Iter: 198 loss: 1.46523394e-06
Iter: 199 loss: 1.46331433e-06
Iter: 200 loss: 1.4531513e-06
Iter: 201 loss: 1.43952957e-06
Iter: 202 loss: 1.47741969e-06
Iter: 203 loss: 1.43520947e-06
Iter: 204 loss: 1.42110071e-06
Iter: 205 loss: 1.53781775e-06
Iter: 206 loss: 1.42034514e-06
Iter: 207 loss: 1.41114162e-06
Iter: 208 loss: 1.44826618e-06
Iter: 209 loss: 1.40911072e-06
Iter: 210 loss: 1.40173972e-06
Iter: 211 loss: 1.50977485e-06
Iter: 212 loss: 1.40172403e-06
Iter: 213 loss: 1.39427061e-06
Iter: 214 loss: 1.39150188e-06
Iter: 215 loss: 1.38756207e-06
Iter: 216 loss: 1.38045516e-06
Iter: 217 loss: 1.36606195e-06
Iter: 218 loss: 1.6320655e-06
Iter: 219 loss: 1.36592257e-06
Iter: 220 loss: 1.36206677e-06
Iter: 221 loss: 1.35969492e-06
Iter: 222 loss: 1.35304333e-06
Iter: 223 loss: 1.34467984e-06
Iter: 224 loss: 1.34406628e-06
Iter: 225 loss: 1.33607909e-06
Iter: 226 loss: 1.38854693e-06
Iter: 227 loss: 1.33527283e-06
Iter: 228 loss: 1.32853688e-06
Iter: 229 loss: 1.32680452e-06
Iter: 230 loss: 1.32242178e-06
Iter: 231 loss: 1.31154161e-06
Iter: 232 loss: 1.39845599e-06
Iter: 233 loss: 1.31076831e-06
Iter: 234 loss: 1.30422359e-06
Iter: 235 loss: 1.29590808e-06
Iter: 236 loss: 1.29531622e-06
Iter: 237 loss: 1.28348154e-06
Iter: 238 loss: 1.377535e-06
Iter: 239 loss: 1.2826049e-06
Iter: 240 loss: 1.27687508e-06
Iter: 241 loss: 1.27462283e-06
Iter: 242 loss: 1.27150827e-06
Iter: 243 loss: 1.26404916e-06
Iter: 244 loss: 1.26389239e-06
Iter: 245 loss: 1.25886413e-06
Iter: 246 loss: 1.30277454e-06
Iter: 247 loss: 1.25858287e-06
Iter: 248 loss: 1.25614179e-06
Iter: 249 loss: 1.25035797e-06
Iter: 250 loss: 1.31355e-06
Iter: 251 loss: 1.2497926e-06
Iter: 252 loss: 1.24146709e-06
Iter: 253 loss: 1.24282133e-06
Iter: 254 loss: 1.23518123e-06
Iter: 255 loss: 1.22998222e-06
Iter: 256 loss: 1.22955134e-06
Iter: 257 loss: 1.22404958e-06
Iter: 258 loss: 1.22151118e-06
Iter: 259 loss: 1.21881567e-06
Iter: 260 loss: 1.21321466e-06
Iter: 261 loss: 1.21664016e-06
Iter: 262 loss: 1.20956054e-06
Iter: 263 loss: 1.20088851e-06
Iter: 264 loss: 1.2262318e-06
Iter: 265 loss: 1.19834908e-06
Iter: 266 loss: 1.19020206e-06
Iter: 267 loss: 1.24089297e-06
Iter: 268 loss: 1.18926732e-06
Iter: 269 loss: 1.1819867e-06
Iter: 270 loss: 1.17661796e-06
Iter: 271 loss: 1.17424895e-06
Iter: 272 loss: 1.1659339e-06
Iter: 273 loss: 1.18781725e-06
Iter: 274 loss: 1.16312185e-06
Iter: 275 loss: 1.15462899e-06
Iter: 276 loss: 1.22808638e-06
Iter: 277 loss: 1.15421926e-06
Iter: 278 loss: 1.15544094e-06
Iter: 279 loss: 1.15270495e-06
Iter: 280 loss: 1.15113164e-06
Iter: 281 loss: 1.14701038e-06
Iter: 282 loss: 1.15992691e-06
Iter: 283 loss: 1.14500085e-06
Iter: 284 loss: 1.13569797e-06
Iter: 285 loss: 1.13643102e-06
Iter: 286 loss: 1.12849375e-06
Iter: 287 loss: 1.12288399e-06
Iter: 288 loss: 1.15701278e-06
Iter: 289 loss: 1.12217049e-06
Iter: 290 loss: 1.11743088e-06
Iter: 291 loss: 1.15168314e-06
Iter: 292 loss: 1.11697739e-06
Iter: 293 loss: 1.1129107e-06
Iter: 294 loss: 1.12281668e-06
Iter: 295 loss: 1.11133772e-06
Iter: 296 loss: 1.10787903e-06
Iter: 297 loss: 1.10117674e-06
Iter: 298 loss: 1.24617713e-06
Iter: 299 loss: 1.10112512e-06
Iter: 300 loss: 1.09463144e-06
Iter: 301 loss: 1.15389855e-06
Iter: 302 loss: 1.09429084e-06
Iter: 303 loss: 1.0876588e-06
Iter: 304 loss: 1.11455631e-06
Iter: 305 loss: 1.08631525e-06
Iter: 306 loss: 1.08197537e-06
Iter: 307 loss: 1.08074482e-06
Iter: 308 loss: 1.07816129e-06
Iter: 309 loss: 1.07205e-06
Iter: 310 loss: 1.10612871e-06
Iter: 311 loss: 1.07121525e-06
Iter: 312 loss: 1.0678167e-06
Iter: 313 loss: 1.08695508e-06
Iter: 314 loss: 1.06739549e-06
Iter: 315 loss: 1.06287234e-06
Iter: 316 loss: 1.08174322e-06
Iter: 317 loss: 1.0619151e-06
Iter: 318 loss: 1.05925324e-06
Iter: 319 loss: 1.05794493e-06
Iter: 320 loss: 1.05662139e-06
Iter: 321 loss: 1.05312756e-06
Iter: 322 loss: 1.05414551e-06
Iter: 323 loss: 1.05072104e-06
Iter: 324 loss: 1.04479773e-06
Iter: 325 loss: 1.05168192e-06
Iter: 326 loss: 1.04170135e-06
Iter: 327 loss: 1.03887737e-06
Iter: 328 loss: 1.03882326e-06
Iter: 329 loss: 1.03568038e-06
Iter: 330 loss: 1.0286285e-06
Iter: 331 loss: 1.12509292e-06
Iter: 332 loss: 1.02817808e-06
Iter: 333 loss: 1.0233739e-06
Iter: 334 loss: 1.09501457e-06
Iter: 335 loss: 1.02342176e-06
Iter: 336 loss: 1.02005595e-06
Iter: 337 loss: 1.01572118e-06
Iter: 338 loss: 1.01544788e-06
Iter: 339 loss: 1.01017861e-06
Iter: 340 loss: 1.01012108e-06
Iter: 341 loss: 1.00782972e-06
Iter: 342 loss: 1.00572765e-06
Iter: 343 loss: 1.00512352e-06
Iter: 344 loss: 1.00189186e-06
Iter: 345 loss: 1.04162325e-06
Iter: 346 loss: 1.00184229e-06
Iter: 347 loss: 1.00094417e-06
Iter: 348 loss: 1.0003555e-06
Iter: 349 loss: 9.99398253e-07
Iter: 350 loss: 9.97110533e-07
Iter: 351 loss: 1.01330011e-06
Iter: 352 loss: 9.96658741e-07
Iter: 353 loss: 9.93583853e-07
Iter: 354 loss: 1.00548334e-06
Iter: 355 loss: 9.92931859e-07
Iter: 356 loss: 9.90013405e-07
Iter: 357 loss: 9.89761361e-07
Iter: 358 loss: 9.87605176e-07
Iter: 359 loss: 9.8454791e-07
Iter: 360 loss: 1.02106378e-06
Iter: 361 loss: 9.84517555e-07
Iter: 362 loss: 9.81773383e-07
Iter: 363 loss: 9.84048256e-07
Iter: 364 loss: 9.80109121e-07
Iter: 365 loss: 9.76218871e-07
Iter: 366 loss: 9.77011496e-07
Iter: 367 loss: 9.73343276e-07
Iter: 368 loss: 9.69485654e-07
Iter: 369 loss: 9.68379595e-07
Iter: 370 loss: 9.66061862e-07
Iter: 371 loss: 9.61034e-07
Iter: 372 loss: 1.03573757e-06
Iter: 373 loss: 9.61074761e-07
Iter: 374 loss: 9.57713382e-07
Iter: 375 loss: 9.66898824e-07
Iter: 376 loss: 9.56622898e-07
Iter: 377 loss: 9.52629193e-07
Iter: 378 loss: 9.47771468e-07
Iter: 379 loss: 9.47336957e-07
Iter: 380 loss: 9.50424464e-07
Iter: 381 loss: 9.46098339e-07
Iter: 382 loss: 9.44612339e-07
Iter: 383 loss: 9.40660868e-07
Iter: 384 loss: 9.69720304e-07
Iter: 385 loss: 9.39764959e-07
Iter: 386 loss: 9.36238393e-07
Iter: 387 loss: 9.64638e-07
Iter: 388 loss: 9.36095148e-07
Iter: 389 loss: 9.33686806e-07
Iter: 390 loss: 9.35759e-07
Iter: 391 loss: 9.32391e-07
Iter: 392 loss: 9.29578789e-07
Iter: 393 loss: 9.38122241e-07
Iter: 394 loss: 9.28804411e-07
Iter: 395 loss: 9.26074051e-07
Iter: 396 loss: 9.39827e-07
Iter: 397 loss: 9.25716108e-07
Iter: 398 loss: 9.22845913e-07
Iter: 399 loss: 9.26068083e-07
Iter: 400 loss: 9.21259584e-07
Iter: 401 loss: 9.18980561e-07
Iter: 402 loss: 9.1580182e-07
Iter: 403 loss: 9.15665737e-07
Iter: 404 loss: 9.11329039e-07
Iter: 405 loss: 9.53324218e-07
Iter: 406 loss: 9.11171298e-07
Iter: 407 loss: 9.08548429e-07
Iter: 408 loss: 9.16787599e-07
Iter: 409 loss: 9.07703395e-07
Iter: 410 loss: 9.04342528e-07
Iter: 411 loss: 9.10305e-07
Iter: 412 loss: 9.02916156e-07
Iter: 413 loss: 9.00872237e-07
Iter: 414 loss: 9.03051159e-07
Iter: 415 loss: 8.99707402e-07
Iter: 416 loss: 8.98120447e-07
Iter: 417 loss: 8.97865277e-07
Iter: 418 loss: 8.96991253e-07
Iter: 419 loss: 8.95076482e-07
Iter: 420 loss: 9.23116204e-07
Iter: 421 loss: 8.95026687e-07
Iter: 422 loss: 8.93106176e-07
Iter: 423 loss: 8.9306252e-07
Iter: 424 loss: 8.91512911e-07
Iter: 425 loss: 8.8890738e-07
Iter: 426 loss: 9.11589325e-07
Iter: 427 loss: 8.8870155e-07
Iter: 428 loss: 8.86364433e-07
Iter: 429 loss: 8.87608678e-07
Iter: 430 loss: 8.84835345e-07
Iter: 431 loss: 8.82566496e-07
Iter: 432 loss: 8.82544384e-07
Iter: 433 loss: 8.81366759e-07
Iter: 434 loss: 8.78709443e-07
Iter: 435 loss: 9.20002e-07
Iter: 436 loss: 8.78735932e-07
Iter: 437 loss: 8.75102273e-07
Iter: 438 loss: 8.86647626e-07
Iter: 439 loss: 8.74043e-07
Iter: 440 loss: 8.71118289e-07
Iter: 441 loss: 8.79946811e-07
Iter: 442 loss: 8.70283259e-07
Iter: 443 loss: 8.66654602e-07
Iter: 444 loss: 8.83463485e-07
Iter: 445 loss: 8.65962079e-07
Iter: 446 loss: 8.63672369e-07
Iter: 447 loss: 8.70038662e-07
Iter: 448 loss: 8.63013952e-07
Iter: 449 loss: 8.61516071e-07
Iter: 450 loss: 8.61492e-07
Iter: 451 loss: 8.59606e-07
Iter: 452 loss: 8.57315058e-07
Iter: 453 loss: 8.57112695e-07
Iter: 454 loss: 8.55277108e-07
Iter: 455 loss: 8.53591757e-07
Iter: 456 loss: 8.53191864e-07
Iter: 457 loss: 8.50731e-07
Iter: 458 loss: 8.73847625e-07
Iter: 459 loss: 8.50715651e-07
Iter: 460 loss: 8.48543323e-07
Iter: 461 loss: 8.51736104e-07
Iter: 462 loss: 8.47501269e-07
Iter: 463 loss: 8.45346e-07
Iter: 464 loss: 8.62925049e-07
Iter: 465 loss: 8.45145848e-07
Iter: 466 loss: 8.43484713e-07
Iter: 467 loss: 8.48044806e-07
Iter: 468 loss: 8.43003e-07
Iter: 469 loss: 8.41116332e-07
Iter: 470 loss: 8.37322773e-07
Iter: 471 loss: 9.03650914e-07
Iter: 472 loss: 8.37326525e-07
Iter: 473 loss: 8.33794672e-07
Iter: 474 loss: 8.50064112e-07
Iter: 475 loss: 8.33053377e-07
Iter: 476 loss: 8.29863438e-07
Iter: 477 loss: 8.66434902e-07
Iter: 478 loss: 8.29896351e-07
Iter: 479 loss: 8.27965039e-07
Iter: 480 loss: 8.33698891e-07
Iter: 481 loss: 8.2743e-07
Iter: 482 loss: 8.25464099e-07
Iter: 483 loss: 8.28316331e-07
Iter: 484 loss: 8.24533856e-07
Iter: 485 loss: 8.22034394e-07
Iter: 486 loss: 8.52741039e-07
Iter: 487 loss: 8.21951403e-07
Iter: 488 loss: 8.21189474e-07
Iter: 489 loss: 8.1900248e-07
Iter: 490 loss: 8.33248123e-07
Iter: 491 loss: 8.18466333e-07
Iter: 492 loss: 8.16297813e-07
Iter: 493 loss: 8.29887597e-07
Iter: 494 loss: 8.1603639e-07
Iter: 495 loss: 8.13855195e-07
Iter: 496 loss: 8.18485091e-07
Iter: 497 loss: 8.13004419e-07
Iter: 498 loss: 8.11329869e-07
Iter: 499 loss: 8.36010258e-07
Iter: 500 loss: 8.11300538e-07
Iter: 501 loss: 8.1044243e-07
Iter: 502 loss: 8.1107396e-07
Iter: 503 loss: 8.09842504e-07
Iter: 504 loss: 8.0810787e-07
Iter: 505 loss: 8.05434638e-07
Iter: 506 loss: 8.05438731e-07
Iter: 507 loss: 8.02703312e-07
Iter: 508 loss: 8.1272384e-07
Iter: 509 loss: 8.02042791e-07
Iter: 510 loss: 7.99435838e-07
Iter: 511 loss: 8.07137155e-07
Iter: 512 loss: 7.98656174e-07
Iter: 513 loss: 7.96730205e-07
Iter: 514 loss: 8.15886779e-07
Iter: 515 loss: 7.96681775e-07
Iter: 516 loss: 7.94941457e-07
Iter: 517 loss: 7.98724727e-07
Iter: 518 loss: 7.94308107e-07
Iter: 519 loss: 7.94137804e-07
Iter: 520 loss: 7.93640311e-07
Iter: 521 loss: 7.93216714e-07
Iter: 522 loss: 7.91657556e-07
Iter: 523 loss: 7.9501109e-07
Iter: 524 loss: 7.90752438e-07
Iter: 525 loss: 7.8848791e-07
Iter: 526 loss: 7.97961206e-07
Iter: 527 loss: 7.88046179e-07
Iter: 528 loss: 7.86089686e-07
Iter: 529 loss: 7.85783811e-07
Iter: 530 loss: 7.84472604e-07
Iter: 531 loss: 7.82956135e-07
Iter: 532 loss: 7.82746213e-07
Iter: 533 loss: 7.81739232e-07
Iter: 534 loss: 7.80877599e-07
Iter: 535 loss: 7.80627829e-07
Iter: 536 loss: 7.78631431e-07
Iter: 537 loss: 7.88106433e-07
Iter: 538 loss: 7.78329706e-07
Iter: 539 loss: 7.7702e-07
Iter: 540 loss: 7.81024369e-07
Iter: 541 loss: 7.76537036e-07
Iter: 542 loss: 7.7527676e-07
Iter: 543 loss: 7.73311058e-07
Iter: 544 loss: 7.73273541e-07
Iter: 545 loss: 7.70864858e-07
Iter: 546 loss: 7.81457175e-07
Iter: 547 loss: 7.70405791e-07
Iter: 548 loss: 7.68763e-07
Iter: 549 loss: 7.94771836e-07
Iter: 550 loss: 7.68742439e-07
Iter: 551 loss: 7.6772676e-07
Iter: 552 loss: 7.75186777e-07
Iter: 553 loss: 7.67634106e-07
Iter: 554 loss: 7.66521566e-07
Iter: 555 loss: 7.70849283e-07
Iter: 556 loss: 7.66294249e-07
Iter: 557 loss: 7.65467e-07
Iter: 558 loss: 7.64382e-07
Iter: 559 loss: 7.6434884e-07
Iter: 560 loss: 7.63233515e-07
Iter: 561 loss: 7.60726039e-07
Iter: 562 loss: 7.9415247e-07
Iter: 563 loss: 7.60589671e-07
Iter: 564 loss: 7.59633394e-07
Iter: 565 loss: 7.59035e-07
Iter: 566 loss: 7.58049509e-07
Iter: 567 loss: 7.61476485e-07
Iter: 568 loss: 7.57733062e-07
Iter: 569 loss: 7.56581471e-07
Iter: 570 loss: 7.58130568e-07
Iter: 571 loss: 7.55979158e-07
Iter: 572 loss: 7.54836265e-07
Iter: 573 loss: 7.6053874e-07
Iter: 574 loss: 7.54674659e-07
Iter: 575 loss: 7.5358048e-07
Iter: 576 loss: 7.53332699e-07
Iter: 577 loss: 7.52700942e-07
Iter: 578 loss: 7.51443963e-07
Iter: 579 loss: 7.54376401e-07
Iter: 580 loss: 7.51006951e-07
Iter: 581 loss: 7.49323249e-07
Iter: 582 loss: 7.50592676e-07
Iter: 583 loss: 7.48379534e-07
Iter: 584 loss: 7.47183549e-07
Iter: 585 loss: 7.47236072e-07
Iter: 586 loss: 7.46189585e-07
Iter: 587 loss: 7.54829102e-07
Iter: 588 loss: 7.46099204e-07
Iter: 589 loss: 7.45256102e-07
Iter: 590 loss: 7.44700799e-07
Iter: 591 loss: 7.44315344e-07
Iter: 592 loss: 7.43440637e-07
Iter: 593 loss: 7.42642101e-07
Iter: 594 loss: 7.42427233e-07
Iter: 595 loss: 7.40438509e-07
Iter: 596 loss: 7.40129622e-07
Iter: 597 loss: 7.38772542e-07
Iter: 598 loss: 7.37178368e-07
Iter: 599 loss: 7.48270509e-07
Iter: 600 loss: 7.36991865e-07
Iter: 601 loss: 7.35075844e-07
Iter: 602 loss: 7.44163458e-07
Iter: 603 loss: 7.34759169e-07
Iter: 604 loss: 7.33759236e-07
Iter: 605 loss: 7.39157429e-07
Iter: 606 loss: 7.33582624e-07
Iter: 607 loss: 7.32765102e-07
Iter: 608 loss: 7.33274646e-07
Iter: 609 loss: 7.3213215e-07
Iter: 610 loss: 7.31066166e-07
Iter: 611 loss: 7.31799901e-07
Iter: 612 loss: 7.30362672e-07
Iter: 613 loss: 7.2868977e-07
Iter: 614 loss: 7.29144404e-07
Iter: 615 loss: 7.27541078e-07
Iter: 616 loss: 7.25911832e-07
Iter: 617 loss: 7.40663381e-07
Iter: 618 loss: 7.25926498e-07
Iter: 619 loss: 7.24887e-07
Iter: 620 loss: 7.2489803e-07
Iter: 621 loss: 7.23849e-07
Iter: 622 loss: 7.24630809e-07
Iter: 623 loss: 7.23333073e-07
Iter: 624 loss: 7.22603829e-07
Iter: 625 loss: 7.22840923e-07
Iter: 626 loss: 7.22035281e-07
Iter: 627 loss: 7.21040806e-07
Iter: 628 loss: 7.1938814e-07
Iter: 629 loss: 7.19382683e-07
Iter: 630 loss: 7.17997693e-07
Iter: 631 loss: 7.26132498e-07
Iter: 632 loss: 7.17810735e-07
Iter: 633 loss: 7.16385898e-07
Iter: 634 loss: 7.26541543e-07
Iter: 635 loss: 7.1628574e-07
Iter: 636 loss: 7.15263354e-07
Iter: 637 loss: 7.23222684e-07
Iter: 638 loss: 7.15127101e-07
Iter: 639 loss: 7.14465557e-07
Iter: 640 loss: 7.13923157e-07
Iter: 641 loss: 7.13689815e-07
Iter: 642 loss: 7.12307553e-07
Iter: 643 loss: 7.17406692e-07
Iter: 644 loss: 7.11994062e-07
Iter: 645 loss: 7.1085708e-07
Iter: 646 loss: 7.09492838e-07
Iter: 647 loss: 7.09387109e-07
Iter: 648 loss: 7.07550953e-07
Iter: 649 loss: 7.22342463e-07
Iter: 650 loss: 7.07424e-07
Iter: 651 loss: 7.06454841e-07
Iter: 652 loss: 7.06378501e-07
Iter: 653 loss: 7.05364869e-07
Iter: 654 loss: 7.08730909e-07
Iter: 655 loss: 7.05093271e-07
Iter: 656 loss: 7.04432694e-07
Iter: 657 loss: 7.04593958e-07
Iter: 658 loss: 7.04026547e-07
Iter: 659 loss: 7.03063108e-07
Iter: 660 loss: 7.01834324e-07
Iter: 661 loss: 7.01845579e-07
Iter: 662 loss: 7.00815463e-07
Iter: 663 loss: 7.12577332e-07
Iter: 664 loss: 7.0080074e-07
Iter: 665 loss: 6.99922566e-07
Iter: 666 loss: 6.98341296e-07
Iter: 667 loss: 7.36048548e-07
Iter: 668 loss: 6.98354597e-07
Iter: 669 loss: 6.97925e-07
Iter: 670 loss: 6.97371775e-07
Iter: 671 loss: 6.9671006e-07
Iter: 672 loss: 6.95717176e-07
Iter: 673 loss: 6.95628273e-07
Iter: 674 loss: 6.94640562e-07
Iter: 675 loss: 7.03017918e-07
Iter: 676 loss: 6.94522782e-07
Iter: 677 loss: 6.93592256e-07
Iter: 678 loss: 6.93352263e-07
Iter: 679 loss: 6.92779849e-07
Iter: 680 loss: 6.91652758e-07
Iter: 681 loss: 6.97102905e-07
Iter: 682 loss: 6.91400658e-07
Iter: 683 loss: 6.90439947e-07
Iter: 684 loss: 6.89549211e-07
Iter: 685 loss: 6.89360434e-07
Iter: 686 loss: 6.90044203e-07
Iter: 687 loss: 6.88680814e-07
Iter: 688 loss: 6.88345267e-07
Iter: 689 loss: 6.87611475e-07
Iter: 690 loss: 7.01260035e-07
Iter: 691 loss: 6.87670251e-07
Iter: 692 loss: 6.86174587e-07
Iter: 693 loss: 6.86029352e-07
Iter: 694 loss: 6.84936708e-07
Iter: 695 loss: 6.83712301e-07
Iter: 696 loss: 6.8367433e-07
Iter: 697 loss: 6.82948382e-07
Iter: 698 loss: 6.81938104e-07
Iter: 699 loss: 6.81910194e-07
Iter: 700 loss: 6.80622861e-07
Iter: 701 loss: 6.92807362e-07
Iter: 702 loss: 6.80545043e-07
Iter: 703 loss: 6.79786808e-07
Iter: 704 loss: 6.86549527e-07
Iter: 705 loss: 6.79706659e-07
Iter: 706 loss: 6.78981621e-07
Iter: 707 loss: 6.77643243e-07
Iter: 708 loss: 6.77644493e-07
Iter: 709 loss: 6.76789568e-07
Iter: 710 loss: 6.76787806e-07
Iter: 711 loss: 6.75958177e-07
Iter: 712 loss: 6.74762816e-07
Iter: 713 loss: 6.74807723e-07
Iter: 714 loss: 6.73515217e-07
Iter: 715 loss: 6.86812598e-07
Iter: 716 loss: 6.7357837e-07
Iter: 717 loss: 6.73174952e-07
Iter: 718 loss: 6.73114869e-07
Iter: 719 loss: 6.72577585e-07
Iter: 720 loss: 6.71302814e-07
Iter: 721 loss: 6.86473697e-07
Iter: 722 loss: 6.71191628e-07
Iter: 723 loss: 6.70311522e-07
Iter: 724 loss: 6.69994051e-07
Iter: 725 loss: 6.69528049e-07
Iter: 726 loss: 6.683739e-07
Iter: 727 loss: 6.84647489e-07
Iter: 728 loss: 6.68302278e-07
Iter: 729 loss: 6.67518748e-07
Iter: 730 loss: 6.68171083e-07
Iter: 731 loss: 6.66991355e-07
Iter: 732 loss: 6.65972095e-07
Iter: 733 loss: 6.67491236e-07
Iter: 734 loss: 6.65516268e-07
Iter: 735 loss: 6.64560389e-07
Iter: 736 loss: 6.66950712e-07
Iter: 737 loss: 6.64237064e-07
Iter: 738 loss: 6.63212631e-07
Iter: 739 loss: 6.72064743e-07
Iter: 740 loss: 6.63146238e-07
Iter: 741 loss: 6.6261714e-07
Iter: 742 loss: 6.62071955e-07
Iter: 743 loss: 6.61964e-07
Iter: 744 loss: 6.6084e-07
Iter: 745 loss: 6.62730201e-07
Iter: 746 loss: 6.60329704e-07
Iter: 747 loss: 6.59244165e-07
Iter: 748 loss: 6.69373549e-07
Iter: 749 loss: 6.59213583e-07
Iter: 750 loss: 6.58480928e-07
Iter: 751 loss: 6.58816816e-07
Iter: 752 loss: 6.58003e-07
Iter: 753 loss: 6.56834288e-07
Iter: 754 loss: 6.70140707e-07
Iter: 755 loss: 6.56818827e-07
Iter: 756 loss: 6.56472821e-07
Iter: 757 loss: 6.55478232e-07
Iter: 758 loss: 6.59029126e-07
Iter: 759 loss: 6.55023825e-07
Iter: 760 loss: 6.53527877e-07
Iter: 761 loss: 6.64057779e-07
Iter: 762 loss: 6.53433631e-07
Iter: 763 loss: 6.5265931e-07
Iter: 764 loss: 6.57982127e-07
Iter: 765 loss: 6.52560061e-07
Iter: 766 loss: 6.51765276e-07
Iter: 767 loss: 6.50486641e-07
Iter: 768 loss: 6.50477716e-07
Iter: 769 loss: 6.49361652e-07
Iter: 770 loss: 6.60515809e-07
Iter: 771 loss: 6.4938456e-07
Iter: 772 loss: 6.48500873e-07
Iter: 773 loss: 6.523303e-07
Iter: 774 loss: 6.48351488e-07
Iter: 775 loss: 6.47644185e-07
Iter: 776 loss: 6.47628553e-07
Iter: 777 loss: 6.47081038e-07
Iter: 778 loss: 6.46039211e-07
Iter: 779 loss: 6.48219e-07
Iter: 780 loss: 6.45714181e-07
Iter: 781 loss: 6.44981469e-07
Iter: 782 loss: 6.48362743e-07
Iter: 783 loss: 6.44833108e-07
Iter: 784 loss: 6.43979888e-07
Iter: 785 loss: 6.44388194e-07
Iter: 786 loss: 6.43370186e-07
Iter: 787 loss: 6.43633484e-07
Iter: 788 loss: 6.42993029e-07
Iter: 789 loss: 6.42798454e-07
Iter: 790 loss: 6.4219563e-07
Iter: 791 loss: 6.44026386e-07
Iter: 792 loss: 6.41877648e-07
Iter: 793 loss: 6.4071304e-07
Iter: 794 loss: 6.40848384e-07
Iter: 795 loss: 6.397878e-07
Iter: 796 loss: 6.38946062e-07
Iter: 797 loss: 6.4787622e-07
Iter: 798 loss: 6.38902748e-07
Iter: 799 loss: 6.38053734e-07
Iter: 800 loss: 6.39375685e-07
Iter: 801 loss: 6.376481e-07
Iter: 802 loss: 6.36866616e-07
Iter: 803 loss: 6.39926839e-07
Iter: 804 loss: 6.36768277e-07
Iter: 805 loss: 6.36010782e-07
Iter: 806 loss: 6.36832397e-07
Iter: 807 loss: 6.3563283e-07
Iter: 808 loss: 6.34623404e-07
Iter: 809 loss: 6.4128875e-07
Iter: 810 loss: 6.34481e-07
Iter: 811 loss: 6.34046671e-07
Iter: 812 loss: 6.33282696e-07
Iter: 813 loss: 6.33246145e-07
Iter: 814 loss: 6.32107117e-07
Iter: 815 loss: 6.39456175e-07
Iter: 816 loss: 6.31994453e-07
Iter: 817 loss: 6.31211094e-07
Iter: 818 loss: 6.33741593e-07
Iter: 819 loss: 6.31024818e-07
Iter: 820 loss: 6.30727357e-07
Iter: 821 loss: 6.306758e-07
Iter: 822 loss: 6.30316322e-07
Iter: 823 loss: 6.29959459e-07
Iter: 824 loss: 6.29868794e-07
Iter: 825 loss: 6.2930485e-07
Iter: 826 loss: 6.28056e-07
Iter: 827 loss: 6.44365173e-07
Iter: 828 loss: 6.27884333e-07
Iter: 829 loss: 6.27128e-07
Iter: 830 loss: 6.27120926e-07
Iter: 831 loss: 6.2637514e-07
Iter: 832 loss: 6.27303052e-07
Iter: 833 loss: 6.26046244e-07
Iter: 834 loss: 6.252003e-07
Iter: 835 loss: 6.30759473e-07
Iter: 836 loss: 6.25116968e-07
Iter: 837 loss: 6.24545748e-07
Iter: 838 loss: 6.23846631e-07
Iter: 839 loss: 6.23801611e-07
Iter: 840 loss: 6.22905873e-07
Iter: 841 loss: 6.22891207e-07
Iter: 842 loss: 6.22399739e-07
Iter: 843 loss: 6.22011214e-07
Iter: 844 loss: 6.21883146e-07
Iter: 845 loss: 6.20935168e-07
Iter: 846 loss: 6.21393e-07
Iter: 847 loss: 6.20364119e-07
Iter: 848 loss: 6.1967188e-07
Iter: 849 loss: 6.29543933e-07
Iter: 850 loss: 6.19682112e-07
Iter: 851 loss: 6.19194452e-07
Iter: 852 loss: 6.20940625e-07
Iter: 853 loss: 6.19019545e-07
Iter: 854 loss: 6.18335548e-07
Iter: 855 loss: 6.21728475e-07
Iter: 856 loss: 6.18220724e-07
Iter: 857 loss: 6.17829755e-07
Iter: 858 loss: 6.16870125e-07
Iter: 859 loss: 6.24514655e-07
Iter: 860 loss: 6.16675266e-07
Iter: 861 loss: 6.15438694e-07
Iter: 862 loss: 6.22228299e-07
Iter: 863 loss: 6.15261456e-07
Iter: 864 loss: 6.14417615e-07
Iter: 865 loss: 6.15433862e-07
Iter: 866 loss: 6.13985435e-07
Iter: 867 loss: 6.1313051e-07
Iter: 868 loss: 6.25968482e-07
Iter: 869 loss: 6.13121927e-07
Iter: 870 loss: 6.12664678e-07
Iter: 871 loss: 6.11987673e-07
Iter: 872 loss: 6.11956125e-07
Iter: 873 loss: 6.10856205e-07
Iter: 874 loss: 6.15074327e-07
Iter: 875 loss: 6.1062633e-07
Iter: 876 loss: 6.10048346e-07
Iter: 877 loss: 6.10030611e-07
Iter: 878 loss: 6.09575181e-07
Iter: 879 loss: 6.08554956e-07
Iter: 880 loss: 6.19653065e-07
Iter: 881 loss: 6.08373455e-07
Iter: 882 loss: 6.07560764e-07
Iter: 883 loss: 6.07573497e-07
Iter: 884 loss: 6.07092318e-07
Iter: 885 loss: 6.08094808e-07
Iter: 886 loss: 6.06894901e-07
Iter: 887 loss: 6.06575497e-07
Iter: 888 loss: 6.06524452e-07
Iter: 889 loss: 6.0627417e-07
Iter: 890 loss: 6.05745868e-07
Iter: 891 loss: 6.15842e-07
Iter: 892 loss: 6.05738705e-07
Iter: 893 loss: 6.05090804e-07
Iter: 894 loss: 6.04589218e-07
Iter: 895 loss: 6.04435172e-07
Iter: 896 loss: 6.03564786e-07
Iter: 897 loss: 6.06704589e-07
Iter: 898 loss: 6.03375042e-07
Iter: 899 loss: 6.02651653e-07
Iter: 900 loss: 6.10853476e-07
Iter: 901 loss: 6.02638352e-07
Iter: 902 loss: 6.02146088e-07
Iter: 903 loss: 6.02878799e-07
Iter: 904 loss: 6.01952e-07
Iter: 905 loss: 6.01166676e-07
Iter: 906 loss: 6.01472721e-07
Iter: 907 loss: 6.00652e-07
Iter: 908 loss: 6.00051635e-07
Iter: 909 loss: 6.06421338e-07
Iter: 910 loss: 6.00003716e-07
Iter: 911 loss: 5.99327109e-07
Iter: 912 loss: 5.99904524e-07
Iter: 913 loss: 5.98908855e-07
Iter: 914 loss: 5.98274653e-07
Iter: 915 loss: 5.98660563e-07
Iter: 916 loss: 5.97841222e-07
Iter: 917 loss: 5.97110443e-07
Iter: 918 loss: 6.02153e-07
Iter: 919 loss: 5.97014036e-07
Iter: 920 loss: 5.96743e-07
Iter: 921 loss: 5.96702421e-07
Iter: 922 loss: 5.96468624e-07
Iter: 923 loss: 5.96253585e-07
Iter: 924 loss: 5.96203506e-07
Iter: 925 loss: 5.95795882e-07
Iter: 926 loss: 5.94916287e-07
Iter: 927 loss: 6.06898766e-07
Iter: 928 loss: 5.94877861e-07
Iter: 929 loss: 5.94031349e-07
Iter: 930 loss: 5.9759725e-07
Iter: 931 loss: 5.93904531e-07
Iter: 932 loss: 5.93077402e-07
Iter: 933 loss: 5.96419454e-07
Iter: 934 loss: 5.92943593e-07
Iter: 935 loss: 5.92203605e-07
Iter: 936 loss: 5.95826634e-07
Iter: 937 loss: 5.9204126e-07
Iter: 938 loss: 5.91304229e-07
Iter: 939 loss: 5.93179323e-07
Iter: 940 loss: 5.91067646e-07
Iter: 941 loss: 5.90500122e-07
Iter: 942 loss: 5.91955654e-07
Iter: 943 loss: 5.90354148e-07
Iter: 944 loss: 5.89774118e-07
Iter: 945 loss: 5.9445631e-07
Iter: 946 loss: 5.89709941e-07
Iter: 947 loss: 5.89352339e-07
Iter: 948 loss: 5.89475803e-07
Iter: 949 loss: 5.89067099e-07
Iter: 950 loss: 5.88587511e-07
Iter: 951 loss: 5.88567104e-07
Iter: 952 loss: 5.8817534e-07
Iter: 953 loss: 5.88002308e-07
Iter: 954 loss: 5.87822228e-07
Iter: 955 loss: 5.87566376e-07
Iter: 956 loss: 5.8799975e-07
Iter: 957 loss: 5.87490717e-07
Iter: 958 loss: 5.87185127e-07
Iter: 959 loss: 5.8648925e-07
Iter: 960 loss: 5.93485765e-07
Iter: 961 loss: 5.86441615e-07
Iter: 962 loss: 5.85705607e-07
Iter: 963 loss: 5.88927946e-07
Iter: 964 loss: 5.85546104e-07
Iter: 965 loss: 5.8490491e-07
Iter: 966 loss: 5.8570788e-07
Iter: 967 loss: 5.84516954e-07
Iter: 968 loss: 5.83780093e-07
Iter: 969 loss: 5.87142779e-07
Iter: 970 loss: 5.83656174e-07
Iter: 971 loss: 5.8294637e-07
Iter: 972 loss: 5.87573595e-07
Iter: 973 loss: 5.82832513e-07
Iter: 974 loss: 5.82408518e-07
Iter: 975 loss: 5.82563644e-07
Iter: 976 loss: 5.82156304e-07
Iter: 977 loss: 5.81565928e-07
Iter: 978 loss: 5.85524617e-07
Iter: 979 loss: 5.81544839e-07
Iter: 980 loss: 5.81078098e-07
Iter: 981 loss: 5.82470648e-07
Iter: 982 loss: 5.80928e-07
Iter: 983 loss: 5.80487381e-07
Iter: 984 loss: 5.79828225e-07
Iter: 985 loss: 5.79832374e-07
Iter: 986 loss: 5.79630864e-07
Iter: 987 loss: 5.79479945e-07
Iter: 988 loss: 5.79099e-07
Iter: 989 loss: 5.80131882e-07
Iter: 990 loss: 5.78965512e-07
Iter: 991 loss: 5.78609672e-07
Iter: 992 loss: 5.7815123e-07
Iter: 993 loss: 5.78119398e-07
Iter: 994 loss: 5.77674712e-07
Iter: 995 loss: 5.77839046e-07
Iter: 996 loss: 5.77327455e-07
Iter: 997 loss: 5.76528805e-07
Iter: 998 loss: 5.77775324e-07
Iter: 999 loss: 5.76209231e-07
Iter: 1000 loss: 5.75521881e-07
Iter: 1001 loss: 5.77008e-07
Iter: 1002 loss: 5.75308036e-07
Iter: 1003 loss: 5.74505748e-07
Iter: 1004 loss: 5.80584356e-07
Iter: 1005 loss: 5.74450155e-07
Iter: 1006 loss: 5.73869841e-07
Iter: 1007 loss: 5.74268824e-07
Iter: 1008 loss: 5.73546458e-07
Iter: 1009 loss: 5.72854219e-07
Iter: 1010 loss: 5.77054038e-07
Iter: 1011 loss: 5.72775832e-07
Iter: 1012 loss: 5.72299541e-07
Iter: 1013 loss: 5.75208219e-07
Iter: 1014 loss: 5.72207227e-07
Iter: 1015 loss: 5.7173213e-07
Iter: 1016 loss: 5.70988959e-07
Iter: 1017 loss: 5.89691865e-07
Iter: 1018 loss: 5.7098822e-07
Iter: 1019 loss: 5.70590203e-07
Iter: 1020 loss: 5.7055513e-07
Iter: 1021 loss: 5.70109137e-07
Iter: 1022 loss: 5.73047373e-07
Iter: 1023 loss: 5.70098905e-07
Iter: 1024 loss: 5.69793599e-07
Iter: 1025 loss: 5.69307645e-07
Iter: 1026 loss: 5.69329586e-07
Iter: 1027 loss: 5.68896496e-07
Iter: 1028 loss: 5.69605163e-07
Iter: 1029 loss: 5.68712835e-07
Iter: 1030 loss: 5.6818908e-07
Iter: 1031 loss: 5.6831891e-07
Iter: 1032 loss: 5.67795723e-07
Iter: 1033 loss: 5.6725122e-07
Iter: 1034 loss: 5.68343864e-07
Iter: 1035 loss: 5.67014752e-07
Iter: 1036 loss: 5.66426138e-07
Iter: 1037 loss: 5.73649515e-07
Iter: 1038 loss: 5.66441599e-07
Iter: 1039 loss: 5.65921709e-07
Iter: 1040 loss: 5.65951609e-07
Iter: 1041 loss: 5.65543417e-07
Iter: 1042 loss: 5.64943321e-07
Iter: 1043 loss: 5.68614439e-07
Iter: 1044 loss: 5.64889319e-07
Iter: 1045 loss: 5.64502898e-07
Iter: 1046 loss: 5.67213647e-07
Iter: 1047 loss: 5.64471293e-07
Iter: 1048 loss: 5.64060883e-07
Iter: 1049 loss: 5.63383765e-07
Iter: 1050 loss: 5.80628807e-07
Iter: 1051 loss: 5.63401045e-07
Iter: 1052 loss: 5.62826244e-07
Iter: 1053 loss: 5.65782102e-07
Iter: 1054 loss: 5.62680043e-07
Iter: 1055 loss: 5.62374112e-07
Iter: 1056 loss: 5.62320565e-07
Iter: 1057 loss: 5.62006221e-07
Iter: 1058 loss: 5.61602405e-07
Iter: 1059 loss: 5.61615423e-07
Iter: 1060 loss: 5.61207912e-07
Iter: 1061 loss: 5.61410729e-07
Iter: 1062 loss: 5.60904141e-07
Iter: 1063 loss: 5.60385502e-07
Iter: 1064 loss: 5.61612296e-07
Iter: 1065 loss: 5.60208264e-07
Iter: 1066 loss: 5.59775685e-07
Iter: 1067 loss: 5.59831847e-07
Iter: 1068 loss: 5.59440764e-07
Iter: 1069 loss: 5.58991132e-07
Iter: 1070 loss: 5.64836114e-07
Iter: 1071 loss: 5.5896669e-07
Iter: 1072 loss: 5.58578336e-07
Iter: 1073 loss: 5.59469072e-07
Iter: 1074 loss: 5.58456691e-07
Iter: 1075 loss: 5.57928672e-07
Iter: 1076 loss: 5.58117904e-07
Iter: 1077 loss: 5.57561748e-07
Iter: 1078 loss: 5.57194767e-07
Iter: 1079 loss: 5.63080107e-07
Iter: 1080 loss: 5.57199e-07
Iter: 1081 loss: 5.56847169e-07
Iter: 1082 loss: 5.5673695e-07
Iter: 1083 loss: 5.56536861e-07
Iter: 1084 loss: 5.56044938e-07
Iter: 1085 loss: 5.56136058e-07
Iter: 1086 loss: 5.55729741e-07
Iter: 1087 loss: 5.55299266e-07
Iter: 1088 loss: 5.55282554e-07
Iter: 1089 loss: 5.54775e-07
Iter: 1090 loss: 5.55879637e-07
Iter: 1091 loss: 5.54617543e-07
Iter: 1092 loss: 5.54372832e-07
Iter: 1093 loss: 5.54144094e-07
Iter: 1094 loss: 5.54113569e-07
Iter: 1095 loss: 5.53603456e-07
Iter: 1096 loss: 5.54085091e-07
Iter: 1097 loss: 5.53390691e-07
Iter: 1098 loss: 5.53042128e-07
Iter: 1099 loss: 5.53753e-07
Iter: 1100 loss: 5.52866823e-07
Iter: 1101 loss: 5.52455163e-07
Iter: 1102 loss: 5.53771429e-07
Iter: 1103 loss: 5.52312827e-07
Iter: 1104 loss: 5.51973699e-07
Iter: 1105 loss: 5.53017458e-07
Iter: 1106 loss: 5.51825622e-07
Iter: 1107 loss: 5.51406174e-07
Iter: 1108 loss: 5.52554809e-07
Iter: 1109 loss: 5.51251446e-07
Iter: 1110 loss: 5.50877814e-07
Iter: 1111 loss: 5.52298275e-07
Iter: 1112 loss: 5.50793175e-07
Iter: 1113 loss: 5.50485879e-07
Iter: 1114 loss: 5.52358244e-07
Iter: 1115 loss: 5.50413233e-07
Iter: 1116 loss: 5.50165339e-07
Iter: 1117 loss: 5.49913125e-07
Iter: 1118 loss: 5.49859351e-07
Iter: 1119 loss: 5.49380559e-07
Iter: 1120 loss: 5.5093733e-07
Iter: 1121 loss: 5.49267611e-07
Iter: 1122 loss: 5.48876699e-07
Iter: 1123 loss: 5.48860442e-07
Iter: 1124 loss: 5.48712933e-07
Iter: 1125 loss: 5.48431785e-07
Iter: 1126 loss: 5.51710343e-07
Iter: 1127 loss: 5.48388584e-07
Iter: 1128 loss: 5.47898424e-07
Iter: 1129 loss: 5.48009666e-07
Iter: 1130 loss: 5.47546392e-07
Iter: 1131 loss: 5.46945103e-07
Iter: 1132 loss: 5.5007132e-07
Iter: 1133 loss: 5.46884621e-07
Iter: 1134 loss: 5.46426463e-07
Iter: 1135 loss: 5.47089485e-07
Iter: 1136 loss: 5.46231149e-07
Iter: 1137 loss: 5.45756961e-07
Iter: 1138 loss: 5.46588581e-07
Iter: 1139 loss: 5.45601893e-07
Iter: 1140 loss: 5.45106843e-07
Iter: 1141 loss: 5.50011407e-07
Iter: 1142 loss: 5.45100079e-07
Iter: 1143 loss: 5.44779937e-07
Iter: 1144 loss: 5.45501052e-07
Iter: 1145 loss: 5.4466193e-07
Iter: 1146 loss: 5.44351678e-07
Iter: 1147 loss: 5.4481859e-07
Iter: 1148 loss: 5.44160343e-07
Iter: 1149 loss: 5.436317e-07
Iter: 1150 loss: 5.44474e-07
Iter: 1151 loss: 5.43448436e-07
Iter: 1152 loss: 5.43076794e-07
Iter: 1153 loss: 5.43060651e-07
Iter: 1154 loss: 5.42817759e-07
Iter: 1155 loss: 5.42686223e-07
Iter: 1156 loss: 5.42540533e-07
Iter: 1157 loss: 5.42332373e-07
Iter: 1158 loss: 5.42003363e-07
Iter: 1159 loss: 5.47348236e-07
Iter: 1160 loss: 5.4198847e-07
Iter: 1161 loss: 5.41528038e-07
Iter: 1162 loss: 5.4180407e-07
Iter: 1163 loss: 5.41242684e-07
Iter: 1164 loss: 5.40659187e-07
Iter: 1165 loss: 5.43791828e-07
Iter: 1166 loss: 5.40597512e-07
Iter: 1167 loss: 5.40144242e-07
Iter: 1168 loss: 5.40113547e-07
Iter: 1169 loss: 5.39786356e-07
Iter: 1170 loss: 5.39370717e-07
Iter: 1171 loss: 5.42169346e-07
Iter: 1172 loss: 5.39325583e-07
Iter: 1173 loss: 5.38953771e-07
Iter: 1174 loss: 5.39895211e-07
Iter: 1175 loss: 5.38775566e-07
Iter: 1176 loss: 5.38443203e-07
Iter: 1177 loss: 5.41519682e-07
Iter: 1178 loss: 5.38394886e-07
Iter: 1179 loss: 5.3816234e-07
Iter: 1180 loss: 5.37758069e-07
Iter: 1181 loss: 5.37761707e-07
Iter: 1182 loss: 5.37354254e-07
Iter: 1183 loss: 5.37394953e-07
Iter: 1184 loss: 5.37065262e-07
Iter: 1185 loss: 5.36645075e-07
Iter: 1186 loss: 5.36620746e-07
Iter: 1187 loss: 5.36507287e-07
Iter: 1188 loss: 5.36357334e-07
Iter: 1189 loss: 5.36213065e-07
Iter: 1190 loss: 5.35928507e-07
Iter: 1191 loss: 5.35999277e-07
Iter: 1192 loss: 5.35451647e-07
Iter: 1193 loss: 5.35050276e-07
Iter: 1194 loss: 5.34895662e-07
Iter: 1195 loss: 5.34402375e-07
Iter: 1196 loss: 5.39035e-07
Iter: 1197 loss: 5.34316143e-07
Iter: 1198 loss: 5.33937737e-07
Iter: 1199 loss: 5.3357104e-07
Iter: 1200 loss: 5.33447121e-07
Iter: 1201 loss: 5.33080652e-07
Iter: 1202 loss: 5.33075308e-07
Iter: 1203 loss: 5.32759714e-07
Iter: 1204 loss: 5.32444915e-07
Iter: 1205 loss: 5.32373292e-07
Iter: 1206 loss: 5.32049285e-07
Iter: 1207 loss: 5.36560151e-07
Iter: 1208 loss: 5.32022113e-07
Iter: 1209 loss: 5.31674743e-07
Iter: 1210 loss: 5.32665069e-07
Iter: 1211 loss: 5.31604542e-07
Iter: 1212 loss: 5.31280875e-07
Iter: 1213 loss: 5.31601188e-07
Iter: 1214 loss: 5.31034402e-07
Iter: 1215 loss: 5.30649515e-07
Iter: 1216 loss: 5.32995386e-07
Iter: 1217 loss: 5.30608304e-07
Iter: 1218 loss: 5.302212e-07
Iter: 1219 loss: 5.31165654e-07
Iter: 1220 loss: 5.30159582e-07
Iter: 1221 loss: 5.30036573e-07
Iter: 1222 loss: 5.30013381e-07
Iter: 1223 loss: 5.29848592e-07
Iter: 1224 loss: 5.29400609e-07
Iter: 1225 loss: 5.30841e-07
Iter: 1226 loss: 5.29132876e-07
Iter: 1227 loss: 5.28703424e-07
Iter: 1228 loss: 5.33071841e-07
Iter: 1229 loss: 5.28656074e-07
Iter: 1230 loss: 5.28273176e-07
Iter: 1231 loss: 5.28136297e-07
Iter: 1232 loss: 5.27937175e-07
Iter: 1233 loss: 5.27417228e-07
Iter: 1234 loss: 5.30750299e-07
Iter: 1235 loss: 5.27397958e-07
Iter: 1236 loss: 5.26904671e-07
Iter: 1237 loss: 5.26453732e-07
Iter: 1238 loss: 5.26358463e-07
Iter: 1239 loss: 5.25848179e-07
Iter: 1240 loss: 5.25853352e-07
Iter: 1241 loss: 5.25515702e-07
Iter: 1242 loss: 5.25413157e-07
Iter: 1243 loss: 5.2522887e-07
Iter: 1244 loss: 5.24824145e-07
Iter: 1245 loss: 5.30550551e-07
Iter: 1246 loss: 5.24827215e-07
Iter: 1247 loss: 5.24541406e-07
Iter: 1248 loss: 5.24840289e-07
Iter: 1249 loss: 5.24379061e-07
Iter: 1250 loss: 5.2404539e-07
Iter: 1251 loss: 5.25104156e-07
Iter: 1252 loss: 5.23932215e-07
Iter: 1253 loss: 5.23702852e-07
Iter: 1254 loss: 5.23718882e-07
Iter: 1255 loss: 5.23505832e-07
Iter: 1256 loss: 5.23496624e-07
Iter: 1257 loss: 5.23333824e-07
Iter: 1258 loss: 5.23098834e-07
Iter: 1259 loss: 5.22688083e-07
Iter: 1260 loss: 5.22732762e-07
Iter: 1261 loss: 5.22274036e-07
Iter: 1262 loss: 5.22356231e-07
Iter: 1263 loss: 5.21974e-07
Iter: 1264 loss: 5.21363404e-07
Iter: 1265 loss: 5.28083e-07
Iter: 1266 loss: 5.2138455e-07
Iter: 1267 loss: 5.20999492e-07
Iter: 1268 loss: 5.20751314e-07
Iter: 1269 loss: 5.20573508e-07
Iter: 1270 loss: 5.20075673e-07
Iter: 1271 loss: 5.2243e-07
Iter: 1272 loss: 5.19989044e-07
Iter: 1273 loss: 5.1944221e-07
Iter: 1274 loss: 5.21027687e-07
Iter: 1275 loss: 5.19345122e-07
Iter: 1276 loss: 5.19052605e-07
Iter: 1277 loss: 5.22577579e-07
Iter: 1278 loss: 5.19043738e-07
Iter: 1279 loss: 5.18754575e-07
Iter: 1280 loss: 5.18865193e-07
Iter: 1281 loss: 5.18554657e-07
Iter: 1282 loss: 5.1810224e-07
Iter: 1283 loss: 5.18707793e-07
Iter: 1284 loss: 5.17901526e-07
Iter: 1285 loss: 5.17577405e-07
Iter: 1286 loss: 5.22318373e-07
Iter: 1287 loss: 5.17607305e-07
Iter: 1288 loss: 5.1733906e-07
Iter: 1289 loss: 5.19158561e-07
Iter: 1290 loss: 5.1732178e-07
Iter: 1291 loss: 5.17107765e-07
Iter: 1292 loss: 5.16622208e-07
Iter: 1293 loss: 5.20790877e-07
Iter: 1294 loss: 5.16529781e-07
Iter: 1295 loss: 5.16001819e-07
Iter: 1296 loss: 5.17729802e-07
Iter: 1297 loss: 5.15901434e-07
Iter: 1298 loss: 5.15423608e-07
Iter: 1299 loss: 5.1650926e-07
Iter: 1300 loss: 5.15246313e-07
Iter: 1301 loss: 5.14729663e-07
Iter: 1302 loss: 5.15935255e-07
Iter: 1303 loss: 5.1449e-07
Iter: 1304 loss: 5.13872919e-07
Iter: 1305 loss: 5.15985562e-07
Iter: 1306 loss: 5.13688065e-07
Iter: 1307 loss: 5.13253667e-07
Iter: 1308 loss: 5.13215809e-07
Iter: 1309 loss: 5.12847578e-07
Iter: 1310 loss: 5.12542e-07
Iter: 1311 loss: 5.12550741e-07
Iter: 1312 loss: 5.12233896e-07
Iter: 1313 loss: 5.12844281e-07
Iter: 1314 loss: 5.12139309e-07
Iter: 1315 loss: 5.11756298e-07
Iter: 1316 loss: 5.12521638e-07
Iter: 1317 loss: 5.11608619e-07
Iter: 1318 loss: 5.11285293e-07
Iter: 1319 loss: 5.12297333e-07
Iter: 1320 loss: 5.11263806e-07
Iter: 1321 loss: 5.11043197e-07
Iter: 1322 loss: 5.14092505e-07
Iter: 1323 loss: 5.11031885e-07
Iter: 1324 loss: 5.10849247e-07
Iter: 1325 loss: 5.11073779e-07
Iter: 1326 loss: 5.10789164e-07
Iter: 1327 loss: 5.10606299e-07
Iter: 1328 loss: 5.10401605e-07
Iter: 1329 loss: 5.10353118e-07
Iter: 1330 loss: 5.10063387e-07
Iter: 1331 loss: 5.10632788e-07
Iter: 1332 loss: 5.09894903e-07
Iter: 1333 loss: 5.095809e-07
Iter: 1334 loss: 5.09825213e-07
Iter: 1335 loss: 5.09373e-07
Iter: 1336 loss: 5.08903099e-07
Iter: 1337 loss: 5.10873122e-07
Iter: 1338 loss: 5.08828748e-07
Iter: 1339 loss: 5.08487574e-07
Iter: 1340 loss: 5.10030759e-07
Iter: 1341 loss: 5.08419532e-07
Iter: 1342 loss: 5.08026915e-07
Iter: 1343 loss: 5.07616505e-07
Iter: 1344 loss: 5.07561708e-07
Iter: 1345 loss: 5.07229515e-07
Iter: 1346 loss: 5.0726203e-07
Iter: 1347 loss: 5.06893116e-07
Iter: 1348 loss: 5.07371908e-07
Iter: 1349 loss: 5.06759193e-07
Iter: 1350 loss: 5.0631354e-07
Iter: 1351 loss: 5.08294818e-07
Iter: 1352 loss: 5.06237257e-07
Iter: 1353 loss: 5.05955541e-07
Iter: 1354 loss: 5.06578147e-07
Iter: 1355 loss: 5.05863e-07
Iter: 1356 loss: 5.05695937e-07
Iter: 1357 loss: 5.08459038e-07
Iter: 1358 loss: 5.05667344e-07
Iter: 1359 loss: 5.0544628e-07
Iter: 1360 loss: 5.05271259e-07
Iter: 1361 loss: 5.05256935e-07
Iter: 1362 loss: 5.04951686e-07
Iter: 1363 loss: 5.05123865e-07
Iter: 1364 loss: 5.04791899e-07
Iter: 1365 loss: 5.04540708e-07
Iter: 1366 loss: 5.06087758e-07
Iter: 1367 loss: 5.04468517e-07
Iter: 1368 loss: 5.04299692e-07
Iter: 1369 loss: 5.03865294e-07
Iter: 1370 loss: 5.10431903e-07
Iter: 1371 loss: 5.03864044e-07
Iter: 1372 loss: 5.03511615e-07
Iter: 1373 loss: 5.03490355e-07
Iter: 1374 loss: 5.03300782e-07
Iter: 1375 loss: 5.03072215e-07
Iter: 1376 loss: 5.03058118e-07
Iter: 1377 loss: 5.02635089e-07
Iter: 1378 loss: 5.04904904e-07
Iter: 1379 loss: 5.02593934e-07
Iter: 1380 loss: 5.02336491e-07
Iter: 1381 loss: 5.02418629e-07
Iter: 1382 loss: 5.02125772e-07
Iter: 1383 loss: 5.01750606e-07
Iter: 1384 loss: 5.06068e-07
Iter: 1385 loss: 5.01722184e-07
Iter: 1386 loss: 5.01519821e-07
Iter: 1387 loss: 5.02752414e-07
Iter: 1388 loss: 5.01448767e-07
Iter: 1389 loss: 5.01307511e-07
Iter: 1390 loss: 5.01599061e-07
Iter: 1391 loss: 5.0124504e-07
Iter: 1392 loss: 5.00916826e-07
Iter: 1393 loss: 5.01012096e-07
Iter: 1394 loss: 5.00667284e-07
Iter: 1395 loss: 5.00428882e-07
Iter: 1396 loss: 5.00605552e-07
Iter: 1397 loss: 5.00290582e-07
Iter: 1398 loss: 4.99999828e-07
Iter: 1399 loss: 5.00194233e-07
Iter: 1400 loss: 4.99829298e-07
Iter: 1401 loss: 4.9953394e-07
Iter: 1402 loss: 5.00266765e-07
Iter: 1403 loss: 4.99410419e-07
Iter: 1404 loss: 4.99123246e-07
Iter: 1405 loss: 5.00175872e-07
Iter: 1406 loss: 4.99006433e-07
Iter: 1407 loss: 4.98731538e-07
Iter: 1408 loss: 4.9959408e-07
Iter: 1409 loss: 4.98676741e-07
Iter: 1410 loss: 4.98293502e-07
Iter: 1411 loss: 4.98309646e-07
Iter: 1412 loss: 4.97954e-07
Iter: 1413 loss: 4.97555e-07
Iter: 1414 loss: 4.99223916e-07
Iter: 1415 loss: 4.97479732e-07
Iter: 1416 loss: 4.97087e-07
Iter: 1417 loss: 4.98953284e-07
Iter: 1418 loss: 4.9699679e-07
Iter: 1419 loss: 4.96718485e-07
Iter: 1420 loss: 5.00253691e-07
Iter: 1421 loss: 4.96750033e-07
Iter: 1422 loss: 4.96595305e-07
Iter: 1423 loss: 4.96533517e-07
Iter: 1424 loss: 4.96461041e-07
Iter: 1425 loss: 4.96195071e-07
Iter: 1426 loss: 4.98837949e-07
Iter: 1427 loss: 4.96179211e-07
Iter: 1428 loss: 4.95973723e-07
Iter: 1429 loss: 4.9608127e-07
Iter: 1430 loss: 4.95843437e-07
Iter: 1431 loss: 4.95696668e-07
Iter: 1432 loss: 4.95375161e-07
Iter: 1433 loss: 5.01867817e-07
Iter: 1434 loss: 4.95325253e-07
Iter: 1435 loss: 4.94876417e-07
Iter: 1436 loss: 4.97459837e-07
Iter: 1437 loss: 4.9479263e-07
Iter: 1438 loss: 4.94561505e-07
Iter: 1439 loss: 4.94337542e-07
Iter: 1440 loss: 4.94277856e-07
Iter: 1441 loss: 4.93815264e-07
Iter: 1442 loss: 4.97486099e-07
Iter: 1443 loss: 4.93756261e-07
Iter: 1444 loss: 4.93576692e-07
Iter: 1445 loss: 4.94376934e-07
Iter: 1446 loss: 4.93494554e-07
Iter: 1447 loss: 4.93207494e-07
Iter: 1448 loss: 4.92619e-07
Iter: 1449 loss: 4.926099e-07
Iter: 1450 loss: 4.92257527e-07
Iter: 1451 loss: 4.92237632e-07
Iter: 1452 loss: 4.9191442e-07
Iter: 1453 loss: 4.92744732e-07
Iter: 1454 loss: 4.9176731e-07
Iter: 1455 loss: 4.91401352e-07
Iter: 1456 loss: 4.92471827e-07
Iter: 1457 loss: 4.91308e-07
Iter: 1458 loss: 4.91155447e-07
Iter: 1459 loss: 4.91143624e-07
Iter: 1460 loss: 4.90989066e-07
Iter: 1461 loss: 4.90894934e-07
Iter: 1462 loss: 4.90770447e-07
Iter: 1463 loss: 4.90550235e-07
Iter: 1464 loss: 4.90601678e-07
Iter: 1465 loss: 4.90351113e-07
Iter: 1466 loss: 4.90102366e-07
Iter: 1467 loss: 4.90149773e-07
Iter: 1468 loss: 4.89911827e-07
Iter: 1469 loss: 4.8952711e-07
Iter: 1470 loss: 4.90706384e-07
Iter: 1471 loss: 4.89409842e-07
Iter: 1472 loss: 4.89140518e-07
Iter: 1473 loss: 4.89445256e-07
Iter: 1474 loss: 4.88956971e-07
Iter: 1475 loss: 4.88672e-07
Iter: 1476 loss: 4.91336664e-07
Iter: 1477 loss: 4.88638705e-07
Iter: 1478 loss: 4.8843674e-07
Iter: 1479 loss: 4.88584419e-07
Iter: 1480 loss: 4.88262231e-07
Iter: 1481 loss: 4.87883312e-07
Iter: 1482 loss: 4.88514388e-07
Iter: 1483 loss: 4.87697662e-07
Iter: 1484 loss: 4.87417879e-07
Iter: 1485 loss: 4.8773552e-07
Iter: 1486 loss: 4.87203124e-07
Iter: 1487 loss: 4.86870135e-07
Iter: 1488 loss: 4.92177946e-07
Iter: 1489 loss: 4.86826536e-07
Iter: 1490 loss: 4.8672257e-07
Iter: 1491 loss: 4.87332841e-07
Iter: 1492 loss: 4.86648332e-07
Iter: 1493 loss: 4.86529416e-07
Iter: 1494 loss: 4.87997397e-07
Iter: 1495 loss: 4.86512477e-07
Iter: 1496 loss: 4.86357e-07
Iter: 1497 loss: 4.85953308e-07
Iter: 1498 loss: 4.89727142e-07
Iter: 1499 loss: 4.85894191e-07
Iter: 1500 loss: 4.85593603e-07
Iter: 1501 loss: 4.89545414e-07
Iter: 1502 loss: 4.85636633e-07
Iter: 1503 loss: 4.85450073e-07
Iter: 1504 loss: 4.85152214e-07
Iter: 1505 loss: 4.85116516e-07
Iter: 1506 loss: 4.84685188e-07
Iter: 1507 loss: 4.87178681e-07
Iter: 1508 loss: 4.84686609e-07
Iter: 1509 loss: 4.84429506e-07
Iter: 1510 loss: 4.8421748e-07
Iter: 1511 loss: 4.84146824e-07
Iter: 1512 loss: 4.83755628e-07
Iter: 1513 loss: 4.8534622e-07
Iter: 1514 loss: 4.8363114e-07
Iter: 1515 loss: 4.83267058e-07
Iter: 1516 loss: 4.84647785e-07
Iter: 1517 loss: 4.83160875e-07
Iter: 1518 loss: 4.82873872e-07
Iter: 1519 loss: 4.83530641e-07
Iter: 1520 loss: 4.82825726e-07
Iter: 1521 loss: 4.82400765e-07
Iter: 1522 loss: 4.83001372e-07
Iter: 1523 loss: 4.82243422e-07
Iter: 1524 loss: 4.81990696e-07
Iter: 1525 loss: 4.8404371e-07
Iter: 1526 loss: 4.8198e-07
Iter: 1527 loss: 4.81764346e-07
Iter: 1528 loss: 4.83791496e-07
Iter: 1529 loss: 4.81727284e-07
Iter: 1530 loss: 4.81530151e-07
Iter: 1531 loss: 4.82205849e-07
Iter: 1532 loss: 4.8154e-07
Iter: 1533 loss: 4.81379629e-07
Iter: 1534 loss: 4.81500933e-07
Iter: 1535 loss: 4.81244172e-07
Iter: 1536 loss: 4.81126222e-07
Iter: 1537 loss: 4.80696201e-07
Iter: 1538 loss: 4.84801149e-07
Iter: 1539 loss: 4.80657036e-07
Iter: 1540 loss: 4.80342919e-07
Iter: 1541 loss: 4.80329163e-07
Iter: 1542 loss: 4.8013419e-07
Iter: 1543 loss: 4.80301708e-07
Iter: 1544 loss: 4.79991797e-07
Iter: 1545 loss: 4.79652726e-07
Iter: 1546 loss: 4.80318818e-07
Iter: 1547 loss: 4.79507776e-07
Iter: 1548 loss: 4.79286427e-07
Iter: 1549 loss: 4.79955929e-07
Iter: 1550 loss: 4.79186156e-07
Iter: 1551 loss: 4.78910692e-07
Iter: 1552 loss: 4.78876416e-07
Iter: 1553 loss: 4.78668937e-07
Iter: 1554 loss: 4.78403535e-07
Iter: 1555 loss: 4.81524808e-07
Iter: 1556 loss: 4.78352774e-07
Iter: 1557 loss: 4.781852e-07
Iter: 1558 loss: 4.78777224e-07
Iter: 1559 loss: 4.78099707e-07
Iter: 1560 loss: 4.77895583e-07
Iter: 1561 loss: 4.78699747e-07
Iter: 1562 loss: 4.77804122e-07
Iter: 1563 loss: 4.77748e-07
Iter: 1564 loss: 4.77727099e-07
Iter: 1565 loss: 4.77629499e-07
Iter: 1566 loss: 4.77604942e-07
Iter: 1567 loss: 4.77523599e-07
Iter: 1568 loss: 4.77435947e-07
Iter: 1569 loss: 4.7720971e-07
Iter: 1570 loss: 4.82011615e-07
Iter: 1571 loss: 4.77240121e-07
Iter: 1572 loss: 4.76960622e-07
Iter: 1573 loss: 4.80319386e-07
Iter: 1574 loss: 4.76988475e-07
Iter: 1575 loss: 4.76859839e-07
Iter: 1576 loss: 4.76918558e-07
Iter: 1577 loss: 4.76789751e-07
Iter: 1578 loss: 4.76626923e-07
Iter: 1579 loss: 4.76491266e-07
Iter: 1580 loss: 4.7643897e-07
Iter: 1581 loss: 4.76230383e-07
Iter: 1582 loss: 4.79271591e-07
Iter: 1583 loss: 4.76209266e-07
Iter: 1584 loss: 4.76073126e-07
Iter: 1585 loss: 4.75867779e-07
Iter: 1586 loss: 4.75827164e-07
Iter: 1587 loss: 4.75613746e-07
Iter: 1588 loss: 4.78101697e-07
Iter: 1589 loss: 4.75644867e-07
Iter: 1590 loss: 4.75477577e-07
Iter: 1591 loss: 4.75259014e-07
Iter: 1592 loss: 4.75237584e-07
Iter: 1593 loss: 4.74979259e-07
Iter: 1594 loss: 4.77734091e-07
Iter: 1595 loss: 4.7498628e-07
Iter: 1596 loss: 4.74788891e-07
Iter: 1597 loss: 4.75299601e-07
Iter: 1598 loss: 4.74731621e-07
Iter: 1599 loss: 4.74633936e-07
Iter: 1600 loss: 4.74640217e-07
Iter: 1601 loss: 4.74523347e-07
Iter: 1602 loss: 4.74403123e-07
Iter: 1603 loss: 4.74410797e-07
Iter: 1604 loss: 4.74224549e-07
Iter: 1605 loss: 4.74080167e-07
Iter: 1606 loss: 4.73983732e-07
Iter: 1607 loss: 4.74041229e-07
Iter: 1608 loss: 4.73954344e-07
Iter: 1609 loss: 4.73875474e-07
Iter: 1610 loss: 4.73625221e-07
Iter: 1611 loss: 4.7514348e-07
Iter: 1612 loss: 4.7353538e-07
Iter: 1613 loss: 4.73282e-07
Iter: 1614 loss: 4.75177728e-07
Iter: 1615 loss: 4.73270688e-07
Iter: 1616 loss: 4.73043087e-07
Iter: 1617 loss: 4.73936382e-07
Iter: 1618 loss: 4.73031207e-07
Iter: 1619 loss: 4.72790305e-07
Iter: 1620 loss: 4.73235e-07
Iter: 1621 loss: 4.72738634e-07
Iter: 1622 loss: 4.7251487e-07
Iter: 1623 loss: 4.72326747e-07
Iter: 1624 loss: 4.72237645e-07
Iter: 1625 loss: 4.71910539e-07
Iter: 1626 loss: 4.75409763e-07
Iter: 1627 loss: 4.71882032e-07
Iter: 1628 loss: 4.71666738e-07
Iter: 1629 loss: 4.72298439e-07
Iter: 1630 loss: 4.71572747e-07
Iter: 1631 loss: 4.7136831e-07
Iter: 1632 loss: 4.71545064e-07
Iter: 1633 loss: 4.71206874e-07
Iter: 1634 loss: 4.70935589e-07
Iter: 1635 loss: 4.71008377e-07
Iter: 1636 loss: 4.70742918e-07
Iter: 1637 loss: 4.70533877e-07
Iter: 1638 loss: 4.70548457e-07
Iter: 1639 loss: 4.70441591e-07
Iter: 1640 loss: 4.70439034e-07
Iter: 1641 loss: 4.70354479e-07
Iter: 1642 loss: 4.70251734e-07
Iter: 1643 loss: 4.7020572e-07
Iter: 1644 loss: 4.70130772e-07
Iter: 1645 loss: 4.70521456e-07
Iter: 1646 loss: 4.70079385e-07
Iter: 1647 loss: 4.69993836e-07
Iter: 1648 loss: 4.69745e-07
Iter: 1649 loss: 4.7381161e-07
Iter: 1650 loss: 4.69761915e-07
Iter: 1651 loss: 4.69503448e-07
Iter: 1652 loss: 4.6967682e-07
Iter: 1653 loss: 4.69311715e-07
Iter: 1654 loss: 4.69075303e-07
Iter: 1655 loss: 4.72181796e-07
Iter: 1656 loss: 4.69050178e-07
Iter: 1657 loss: 4.68847105e-07
Iter: 1658 loss: 4.69513679e-07
Iter: 1659 loss: 4.68766359e-07
Iter: 1660 loss: 4.68516049e-07
Iter: 1661 loss: 4.68689194e-07
Iter: 1662 loss: 4.68351402e-07
Iter: 1663 loss: 4.68119481e-07
Iter: 1664 loss: 4.68203183e-07
Iter: 1665 loss: 4.67993402e-07
Iter: 1666 loss: 4.67644668e-07
Iter: 1667 loss: 4.70136342e-07
Iter: 1668 loss: 4.67640035e-07
Iter: 1669 loss: 4.67427839e-07
Iter: 1670 loss: 4.67685481e-07
Iter: 1671 loss: 4.67364544e-07
Iter: 1672 loss: 4.67070208e-07
Iter: 1673 loss: 4.68453322e-07
Iter: 1674 loss: 4.67022062e-07
Iter: 1675 loss: 4.6690343e-07
Iter: 1676 loss: 4.66899195e-07
Iter: 1677 loss: 4.66823906e-07
Iter: 1678 loss: 4.66656e-07
Iter: 1679 loss: 4.66658491e-07
Iter: 1680 loss: 4.66488075e-07
Iter: 1681 loss: 4.66482277e-07
Iter: 1682 loss: 4.66370096e-07
Iter: 1683 loss: 4.66180524e-07
Iter: 1684 loss: 4.66233132e-07
Iter: 1685 loss: 4.66018378e-07
Iter: 1686 loss: 4.65795239e-07
Iter: 1687 loss: 4.65790492e-07
Iter: 1688 loss: 4.65619792e-07
Iter: 1689 loss: 4.65363911e-07
Iter: 1690 loss: 4.65357289e-07
Iter: 1691 loss: 4.65056473e-07
Iter: 1692 loss: 4.6711034e-07
Iter: 1693 loss: 4.65058946e-07
Iter: 1694 loss: 4.64848483e-07
Iter: 1695 loss: 4.65309881e-07
Iter: 1696 loss: 4.64741788e-07
Iter: 1697 loss: 4.64480735e-07
Iter: 1698 loss: 4.64632706e-07
Iter: 1699 loss: 4.64291929e-07
Iter: 1700 loss: 4.64072116e-07
Iter: 1701 loss: 4.64137543e-07
Iter: 1702 loss: 4.63968604e-07
Iter: 1703 loss: 4.63662332e-07
Iter: 1704 loss: 4.6499855e-07
Iter: 1705 loss: 4.63594176e-07
Iter: 1706 loss: 4.63368934e-07
Iter: 1707 loss: 4.65064431e-07
Iter: 1708 loss: 4.63366717e-07
Iter: 1709 loss: 4.6321594e-07
Iter: 1710 loss: 4.65156631e-07
Iter: 1711 loss: 4.63219067e-07
Iter: 1712 loss: 4.63046035e-07
Iter: 1713 loss: 4.62925925e-07
Iter: 1714 loss: 4.62895031e-07
Iter: 1715 loss: 4.62741468e-07
Iter: 1716 loss: 4.62728281e-07
Iter: 1717 loss: 4.6262727e-07
Iter: 1718 loss: 4.62375738e-07
Iter: 1719 loss: 4.63424e-07
Iter: 1720 loss: 4.62310197e-07
Iter: 1721 loss: 4.62133244e-07
Iter: 1722 loss: 4.64696399e-07
Iter: 1723 loss: 4.62149842e-07
Iter: 1724 loss: 4.62012054e-07
Iter: 1725 loss: 4.6181674e-07
Iter: 1726 loss: 4.6696465e-07
Iter: 1727 loss: 4.61836237e-07
Iter: 1728 loss: 4.61531556e-07
Iter: 1729 loss: 4.61947081e-07
Iter: 1730 loss: 4.61390101e-07
Iter: 1731 loss: 4.6110091e-07
Iter: 1732 loss: 4.63250956e-07
Iter: 1733 loss: 4.61060608e-07
Iter: 1734 loss: 4.60920944e-07
Iter: 1735 loss: 4.62569858e-07
Iter: 1736 loss: 4.60937798e-07
Iter: 1737 loss: 4.60808877e-07
Iter: 1738 loss: 4.60482511e-07
Iter: 1739 loss: 4.64194216e-07
Iter: 1740 loss: 4.60462843e-07
Iter: 1741 loss: 4.60207247e-07
Iter: 1742 loss: 4.63461873e-07
Iter: 1743 loss: 4.60225e-07
Iter: 1744 loss: 4.60068918e-07
Iter: 1745 loss: 4.61707202e-07
Iter: 1746 loss: 4.60085573e-07
Iter: 1747 loss: 4.5990032e-07
Iter: 1748 loss: 4.6004169e-07
Iter: 1749 loss: 4.59819773e-07
Iter: 1750 loss: 4.59681843e-07
Iter: 1751 loss: 4.59755455e-07
Iter: 1752 loss: 4.5961491e-07
Iter: 1753 loss: 4.59434062e-07
Iter: 1754 loss: 4.59414451e-07
Iter: 1755 loss: 4.59267341e-07
Iter: 1756 loss: 4.59146577e-07
Iter: 1757 loss: 4.60626239e-07
Iter: 1758 loss: 4.59124834e-07
Iter: 1759 loss: 4.58948136e-07
Iter: 1760 loss: 4.59122646e-07
Iter: 1761 loss: 4.58763623e-07
Iter: 1762 loss: 4.58548186e-07
Iter: 1763 loss: 4.58796109e-07
Iter: 1764 loss: 4.58429213e-07
Iter: 1765 loss: 4.58209342e-07
Iter: 1766 loss: 4.58181489e-07
Iter: 1767 loss: 4.58052938e-07
Iter: 1768 loss: 4.57848472e-07
Iter: 1769 loss: 4.60429476e-07
Iter: 1770 loss: 4.57848671e-07
Iter: 1771 loss: 4.57654949e-07
Iter: 1772 loss: 4.58122827e-07
Iter: 1773 loss: 4.5758253e-07
Iter: 1774 loss: 4.57378178e-07
Iter: 1775 loss: 4.57651083e-07
Iter: 1776 loss: 4.57317753e-07
Iter: 1777 loss: 4.57094643e-07
Iter: 1778 loss: 4.57771307e-07
Iter: 1779 loss: 4.57066847e-07
Iter: 1780 loss: 4.56871248e-07
Iter: 1781 loss: 4.56886369e-07
Iter: 1782 loss: 4.56806902e-07
Iter: 1783 loss: 4.56707596e-07
Iter: 1784 loss: 4.56691737e-07
Iter: 1785 loss: 4.56498952e-07
Iter: 1786 loss: 4.56462431e-07
Iter: 1787 loss: 4.56331691e-07
Iter: 1788 loss: 4.56122962e-07
Iter: 1789 loss: 4.56609314e-07
Iter: 1790 loss: 4.56007172e-07
Iter: 1791 loss: 4.55745123e-07
Iter: 1792 loss: 4.57274382e-07
Iter: 1793 loss: 4.55727957e-07
Iter: 1794 loss: 4.55440443e-07
Iter: 1795 loss: 4.55861937e-07
Iter: 1796 loss: 4.55318656e-07
Iter: 1797 loss: 4.5512968e-07
Iter: 1798 loss: 4.54893524e-07
Iter: 1799 loss: 4.54885367e-07
Iter: 1800 loss: 4.54453073e-07
Iter: 1801 loss: 4.56451971e-07
Iter: 1802 loss: 4.5434507e-07
Iter: 1803 loss: 4.54101354e-07
Iter: 1804 loss: 4.55745919e-07
Iter: 1805 loss: 4.54065031e-07
Iter: 1806 loss: 4.53813954e-07
Iter: 1807 loss: 4.55278098e-07
Iter: 1808 loss: 4.53769815e-07
Iter: 1809 loss: 4.53549148e-07
Iter: 1810 loss: 4.53448e-07
Iter: 1811 loss: 4.53339425e-07
Iter: 1812 loss: 4.53227187e-07
Iter: 1813 loss: 4.53176966e-07
Iter: 1814 loss: 4.53062455e-07
Iter: 1815 loss: 4.52891186e-07
Iter: 1816 loss: 4.52882318e-07
Iter: 1817 loss: 4.52688425e-07
Iter: 1818 loss: 4.52908665e-07
Iter: 1819 loss: 4.52649e-07
Iter: 1820 loss: 4.52377435e-07
Iter: 1821 loss: 4.52819592e-07
Iter: 1822 loss: 4.52263492e-07
Iter: 1823 loss: 4.52075824e-07
Iter: 1824 loss: 4.52803761e-07
Iter: 1825 loss: 4.5200369e-07
Iter: 1826 loss: 4.51695257e-07
Iter: 1827 loss: 4.52411086e-07
Iter: 1828 loss: 4.51580433e-07
Iter: 1829 loss: 4.51344363e-07
Iter: 1830 loss: 4.50991422e-07
Iter: 1831 loss: 4.50985112e-07
Iter: 1832 loss: 4.50603579e-07
Iter: 1833 loss: 4.54397878e-07
Iter: 1834 loss: 4.5057314e-07
Iter: 1835 loss: 4.50191777e-07
Iter: 1836 loss: 4.50294067e-07
Iter: 1837 loss: 4.49924073e-07
Iter: 1838 loss: 4.49605068e-07
Iter: 1839 loss: 4.49664867e-07
Iter: 1840 loss: 4.49448976e-07
Iter: 1841 loss: 4.49358282e-07
Iter: 1842 loss: 4.49210972e-07
Iter: 1843 loss: 4.49158676e-07
Iter: 1844 loss: 4.49091317e-07
Iter: 1845 loss: 4.48922663e-07
Iter: 1846 loss: 4.48800165e-07
Iter: 1847 loss: 4.48735619e-07
Iter: 1848 loss: 4.48524332e-07
Iter: 1849 loss: 4.4839507e-07
Iter: 1850 loss: 4.4830773e-07
Iter: 1851 loss: 4.48048382e-07
Iter: 1852 loss: 4.48030789e-07
Iter: 1853 loss: 4.47914687e-07
Iter: 1854 loss: 4.47890272e-07
Iter: 1855 loss: 4.47811544e-07
Iter: 1856 loss: 4.47600939e-07
Iter: 1857 loss: 4.49783386e-07
Iter: 1858 loss: 4.47566038e-07
Iter: 1859 loss: 4.47451384e-07
Iter: 1860 loss: 4.47146817e-07
Iter: 1861 loss: 4.47124421e-07
Iter: 1862 loss: 4.46804222e-07
Iter: 1863 loss: 4.48290763e-07
Iter: 1864 loss: 4.46724414e-07
Iter: 1865 loss: 4.4647004e-07
Iter: 1866 loss: 4.4620964e-07
Iter: 1867 loss: 4.46128098e-07
Iter: 1868 loss: 4.45885803e-07
Iter: 1869 loss: 4.45862497e-07
Iter: 1870 loss: 4.4567571e-07
Iter: 1871 loss: 4.45828277e-07
Iter: 1872 loss: 4.45554974e-07
Iter: 1873 loss: 4.45371768e-07
Iter: 1874 loss: 4.47358e-07
Iter: 1875 loss: 4.45368471e-07
Iter: 1876 loss: 4.45292699e-07
Iter: 1877 loss: 4.45445153e-07
Iter: 1878 loss: 4.45245149e-07
Iter: 1879 loss: 4.45164e-07
Iter: 1880 loss: 4.44919749e-07
Iter: 1881 loss: 4.44940667e-07
Iter: 1882 loss: 4.44731938e-07
Iter: 1883 loss: 4.45875457e-07
Iter: 1884 loss: 4.44708917e-07
Iter: 1885 loss: 4.44580195e-07
Iter: 1886 loss: 4.45318506e-07
Iter: 1887 loss: 4.44562403e-07
Iter: 1888 loss: 4.44399518e-07
Iter: 1889 loss: 4.44322893e-07
Iter: 1890 loss: 4.44255363e-07
Iter: 1891 loss: 4.44007867e-07
Iter: 1892 loss: 4.44705904e-07
Iter: 1893 loss: 4.43938291e-07
Iter: 1894 loss: 4.43681614e-07
Iter: 1895 loss: 4.43576397e-07
Iter: 1896 loss: 4.43464e-07
Iter: 1897 loss: 4.43151237e-07
Iter: 1898 loss: 4.44429617e-07
Iter: 1899 loss: 4.43086179e-07
Iter: 1900 loss: 4.42824671e-07
Iter: 1901 loss: 4.43443412e-07
Iter: 1902 loss: 4.4274293e-07
Iter: 1903 loss: 4.42602015e-07
Iter: 1904 loss: 4.42585929e-07
Iter: 1905 loss: 4.42484748e-07
Iter: 1906 loss: 4.42779736e-07
Iter: 1907 loss: 4.42436885e-07
Iter: 1908 loss: 4.42305407e-07
Iter: 1909 loss: 4.43326257e-07
Iter: 1910 loss: 4.42301598e-07
Iter: 1911 loss: 4.42203714e-07
Iter: 1912 loss: 4.42109581e-07
Iter: 1913 loss: 4.4209321e-07
Iter: 1914 loss: 4.41946838e-07
Iter: 1915 loss: 4.42094915e-07
Iter: 1916 loss: 4.41850602e-07
Iter: 1917 loss: 4.41737882e-07
Iter: 1918 loss: 4.42720278e-07
Iter: 1919 loss: 4.41715201e-07
Iter: 1920 loss: 4.41600662e-07
Iter: 1921 loss: 4.41758573e-07
Iter: 1922 loss: 4.41499822e-07
Iter: 1923 loss: 4.41381474e-07
Iter: 1924 loss: 4.42393514e-07
Iter: 1925 loss: 4.41383463e-07
Iter: 1926 loss: 4.41294731e-07
Iter: 1927 loss: 4.41241411e-07
Iter: 1928 loss: 4.41213786e-07
Iter: 1929 loss: 4.41040299e-07
Iter: 1930 loss: 4.41554647e-07
Iter: 1931 loss: 4.40973167e-07
Iter: 1932 loss: 4.40818212e-07
Iter: 1933 loss: 4.40644072e-07
Iter: 1934 loss: 4.40640179e-07
Iter: 1935 loss: 4.40426078e-07
Iter: 1936 loss: 4.40434349e-07
Iter: 1937 loss: 4.40281326e-07
Iter: 1938 loss: 4.42155169e-07
Iter: 1939 loss: 4.40288261e-07
Iter: 1940 loss: 4.40193787e-07
Iter: 1941 loss: 4.40761056e-07
Iter: 1942 loss: 4.40197823e-07
Iter: 1943 loss: 4.40118924e-07
Iter: 1944 loss: 4.40127849e-07
Iter: 1945 loss: 4.40060205e-07
Iter: 1946 loss: 4.39985058e-07
Iter: 1947 loss: 4.39863555e-07
Iter: 1948 loss: 4.39863641e-07
Iter: 1949 loss: 4.39720395e-07
Iter: 1950 loss: 4.4069111e-07
Iter: 1951 loss: 4.39704252e-07
Iter: 1952 loss: 4.39569732e-07
Iter: 1953 loss: 4.39810805e-07
Iter: 1954 loss: 4.39480118e-07
Iter: 1955 loss: 4.39418301e-07
Iter: 1956 loss: 4.40809345e-07
Iter: 1957 loss: 4.39408723e-07
Iter: 1958 loss: 4.39321525e-07
Iter: 1959 loss: 4.39201756e-07
Iter: 1960 loss: 4.39233276e-07
Iter: 1961 loss: 4.39088865e-07
Iter: 1962 loss: 4.39912156e-07
Iter: 1963 loss: 4.3903816e-07
Iter: 1964 loss: 4.38928282e-07
Iter: 1965 loss: 4.39040292e-07
Iter: 1966 loss: 4.38861576e-07
Iter: 1967 loss: 4.3874428e-07
Iter: 1968 loss: 4.39163102e-07
Iter: 1969 loss: 4.38768495e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.4
+ date
Mon Oct 26 18:44:38 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.4/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.4_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -1 --phi 2.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.4_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b2397b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b1ab9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b1abd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b28c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b28c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b174c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b117e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b0d87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b0e51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b0d8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b0abd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b0658c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b065a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b035378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314b035268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314afb3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314af90488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314af4ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314af8b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314af1df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314af10bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314aef8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3124242510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f312425e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3124242400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31241c51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31241f99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f312418b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f312418b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3124151730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f312412a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f30fc0b36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f30fc0b3378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f30fc0b8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f30fc071bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f30fc01b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000254194398
Iter: 2 loss: 0.00729740597
Iter: 3 loss: 0.000205939607
Iter: 4 loss: 0.000191994113
Iter: 5 loss: 0.000222705043
Iter: 6 loss: 0.000186569785
Iter: 7 loss: 0.000170840503
Iter: 8 loss: 0.000151746674
Iter: 9 loss: 0.000150006876
Iter: 10 loss: 0.000137437019
Iter: 11 loss: 0.000111976689
Iter: 12 loss: 0.000612768345
Iter: 13 loss: 0.000111614681
Iter: 14 loss: 9.67549859e-05
Iter: 15 loss: 0.000306906179
Iter: 16 loss: 9.67200176e-05
Iter: 17 loss: 9.09275e-05
Iter: 18 loss: 9.09084192e-05
Iter: 19 loss: 8.67654599e-05
Iter: 20 loss: 7.69447506e-05
Iter: 21 loss: 0.000188395628
Iter: 22 loss: 7.602392e-05
Iter: 23 loss: 6.62504463e-05
Iter: 24 loss: 0.000125112303
Iter: 25 loss: 6.48984933e-05
Iter: 26 loss: 6.0096856e-05
Iter: 27 loss: 7.73781503e-05
Iter: 28 loss: 5.88911062e-05
Iter: 29 loss: 5.42606431e-05
Iter: 30 loss: 7.075756e-05
Iter: 31 loss: 5.30541947e-05
Iter: 32 loss: 5.02605981e-05
Iter: 33 loss: 6.92321482e-05
Iter: 34 loss: 4.9991344e-05
Iter: 35 loss: 4.7762871e-05
Iter: 36 loss: 4.51847154e-05
Iter: 37 loss: 4.48810206e-05
Iter: 38 loss: 4.25200087e-05
Iter: 39 loss: 4.2367541e-05
Iter: 40 loss: 4.36914888e-05
Iter: 41 loss: 4.17883894e-05
Iter: 42 loss: 4.15550239e-05
Iter: 43 loss: 4.08926862e-05
Iter: 44 loss: 4.40928161e-05
Iter: 45 loss: 4.066365e-05
Iter: 46 loss: 3.94086892e-05
Iter: 47 loss: 3.83145125e-05
Iter: 48 loss: 3.7983631e-05
Iter: 49 loss: 3.71901515e-05
Iter: 50 loss: 3.69454428e-05
Iter: 51 loss: 3.61281745e-05
Iter: 52 loss: 4.18247946e-05
Iter: 53 loss: 3.60531121e-05
Iter: 54 loss: 3.54155163e-05
Iter: 55 loss: 3.42555541e-05
Iter: 56 loss: 6.20687788e-05
Iter: 57 loss: 3.42543026e-05
Iter: 58 loss: 3.26335248e-05
Iter: 59 loss: 3.80323436e-05
Iter: 60 loss: 3.21912e-05
Iter: 61 loss: 3.1291871e-05
Iter: 62 loss: 3.24164139e-05
Iter: 63 loss: 3.08263552e-05
Iter: 64 loss: 2.97048427e-05
Iter: 65 loss: 3.48121248e-05
Iter: 66 loss: 2.94932306e-05
Iter: 67 loss: 2.87451403e-05
Iter: 68 loss: 3.28146925e-05
Iter: 69 loss: 2.86318245e-05
Iter: 70 loss: 2.80268905e-05
Iter: 71 loss: 2.76875908e-05
Iter: 72 loss: 2.7424323e-05
Iter: 73 loss: 2.87566399e-05
Iter: 74 loss: 2.7224869e-05
Iter: 75 loss: 2.69901702e-05
Iter: 76 loss: 2.67262949e-05
Iter: 77 loss: 2.66913376e-05
Iter: 78 loss: 2.64715345e-05
Iter: 79 loss: 2.58931759e-05
Iter: 80 loss: 3.03567322e-05
Iter: 81 loss: 2.57781958e-05
Iter: 82 loss: 2.51844249e-05
Iter: 83 loss: 3.23964668e-05
Iter: 84 loss: 2.5178224e-05
Iter: 85 loss: 2.47564e-05
Iter: 86 loss: 2.71132521e-05
Iter: 87 loss: 2.46963791e-05
Iter: 88 loss: 2.43765899e-05
Iter: 89 loss: 2.43762188e-05
Iter: 90 loss: 2.41655071e-05
Iter: 91 loss: 2.43966078e-05
Iter: 92 loss: 2.40495738e-05
Iter: 93 loss: 2.37070944e-05
Iter: 94 loss: 2.34932704e-05
Iter: 95 loss: 2.33552746e-05
Iter: 96 loss: 2.29377838e-05
Iter: 97 loss: 2.29376274e-05
Iter: 98 loss: 2.26440243e-05
Iter: 99 loss: 2.31418635e-05
Iter: 100 loss: 2.25133736e-05
Iter: 101 loss: 2.22346571e-05
Iter: 102 loss: 2.36436463e-05
Iter: 103 loss: 2.21892569e-05
Iter: 104 loss: 2.19915455e-05
Iter: 105 loss: 2.20791935e-05
Iter: 106 loss: 2.18564965e-05
Iter: 107 loss: 2.2041062e-05
Iter: 108 loss: 2.1779806e-05
Iter: 109 loss: 2.17008019e-05
Iter: 110 loss: 2.14693791e-05
Iter: 111 loss: 2.23573152e-05
Iter: 112 loss: 2.13714611e-05
Iter: 113 loss: 2.11377846e-05
Iter: 114 loss: 2.11406714e-05
Iter: 115 loss: 2.09509344e-05
Iter: 116 loss: 2.06665572e-05
Iter: 117 loss: 2.18796649e-05
Iter: 118 loss: 2.06068071e-05
Iter: 119 loss: 2.03634863e-05
Iter: 120 loss: 2.21622722e-05
Iter: 121 loss: 2.03441632e-05
Iter: 122 loss: 2.02175161e-05
Iter: 123 loss: 2.02164665e-05
Iter: 124 loss: 2.01582297e-05
Iter: 125 loss: 2.00127706e-05
Iter: 126 loss: 2.13201383e-05
Iter: 127 loss: 1.99905062e-05
Iter: 128 loss: 1.97784138e-05
Iter: 129 loss: 2.00559516e-05
Iter: 130 loss: 1.96716028e-05
Iter: 131 loss: 1.94927925e-05
Iter: 132 loss: 2.06659224e-05
Iter: 133 loss: 1.94739969e-05
Iter: 134 loss: 1.93145315e-05
Iter: 135 loss: 1.97927548e-05
Iter: 136 loss: 1.92660555e-05
Iter: 137 loss: 1.91175095e-05
Iter: 138 loss: 2.04013213e-05
Iter: 139 loss: 1.91109903e-05
Iter: 140 loss: 1.90998653e-05
Iter: 141 loss: 1.90731662e-05
Iter: 142 loss: 1.90266837e-05
Iter: 143 loss: 1.89391794e-05
Iter: 144 loss: 2.09048267e-05
Iter: 145 loss: 1.89392558e-05
Iter: 146 loss: 1.88725389e-05
Iter: 147 loss: 1.87347214e-05
Iter: 148 loss: 2.11358183e-05
Iter: 149 loss: 1.87302685e-05
Iter: 150 loss: 1.85542849e-05
Iter: 151 loss: 1.94203712e-05
Iter: 152 loss: 1.85228928e-05
Iter: 153 loss: 1.83656775e-05
Iter: 154 loss: 1.92064454e-05
Iter: 155 loss: 1.83425109e-05
Iter: 156 loss: 1.82860422e-05
Iter: 157 loss: 1.82768e-05
Iter: 158 loss: 1.82129e-05
Iter: 159 loss: 1.80784336e-05
Iter: 160 loss: 2.0324951e-05
Iter: 161 loss: 1.80753159e-05
Iter: 162 loss: 1.78986302e-05
Iter: 163 loss: 1.81030518e-05
Iter: 164 loss: 1.78045448e-05
Iter: 165 loss: 1.7700293e-05
Iter: 166 loss: 1.83750017e-05
Iter: 167 loss: 1.76887424e-05
Iter: 168 loss: 1.75783625e-05
Iter: 169 loss: 1.7659293e-05
Iter: 170 loss: 1.75094319e-05
Iter: 171 loss: 1.7390048e-05
Iter: 172 loss: 1.79644267e-05
Iter: 173 loss: 1.73691205e-05
Iter: 174 loss: 1.73123226e-05
Iter: 175 loss: 1.73063327e-05
Iter: 176 loss: 1.72369655e-05
Iter: 177 loss: 1.74434972e-05
Iter: 178 loss: 1.72157324e-05
Iter: 179 loss: 1.71778738e-05
Iter: 180 loss: 1.70760959e-05
Iter: 181 loss: 1.7766115e-05
Iter: 182 loss: 1.70521744e-05
Iter: 183 loss: 1.69621162e-05
Iter: 184 loss: 1.75840742e-05
Iter: 185 loss: 1.69530813e-05
Iter: 186 loss: 1.68705574e-05
Iter: 187 loss: 1.68243168e-05
Iter: 188 loss: 1.67889884e-05
Iter: 189 loss: 1.68298502e-05
Iter: 190 loss: 1.67431135e-05
Iter: 191 loss: 1.66877962e-05
Iter: 192 loss: 1.67809339e-05
Iter: 193 loss: 1.66618065e-05
Iter: 194 loss: 1.66057907e-05
Iter: 195 loss: 1.65547772e-05
Iter: 196 loss: 1.65402562e-05
Iter: 197 loss: 1.64279409e-05
Iter: 198 loss: 1.70031126e-05
Iter: 199 loss: 1.64095545e-05
Iter: 200 loss: 1.63301302e-05
Iter: 201 loss: 1.6568205e-05
Iter: 202 loss: 1.63059285e-05
Iter: 203 loss: 1.62452307e-05
Iter: 204 loss: 1.62110173e-05
Iter: 205 loss: 1.61851676e-05
Iter: 206 loss: 1.61070639e-05
Iter: 207 loss: 1.62371616e-05
Iter: 208 loss: 1.60715063e-05
Iter: 209 loss: 1.60802811e-05
Iter: 210 loss: 1.60351738e-05
Iter: 211 loss: 1.60103209e-05
Iter: 212 loss: 1.59572664e-05
Iter: 213 loss: 1.67352373e-05
Iter: 214 loss: 1.595508e-05
Iter: 215 loss: 1.59178635e-05
Iter: 216 loss: 1.58709518e-05
Iter: 217 loss: 1.58676339e-05
Iter: 218 loss: 1.57844443e-05
Iter: 219 loss: 1.58964503e-05
Iter: 220 loss: 1.57424329e-05
Iter: 221 loss: 1.56594433e-05
Iter: 222 loss: 1.61828721e-05
Iter: 223 loss: 1.56494461e-05
Iter: 224 loss: 1.56141359e-05
Iter: 225 loss: 1.56082169e-05
Iter: 226 loss: 1.55823764e-05
Iter: 227 loss: 1.5542817e-05
Iter: 228 loss: 1.55425823e-05
Iter: 229 loss: 1.54792815e-05
Iter: 230 loss: 1.54743757e-05
Iter: 231 loss: 1.5427202e-05
Iter: 232 loss: 1.53669607e-05
Iter: 233 loss: 1.58402108e-05
Iter: 234 loss: 1.53627952e-05
Iter: 235 loss: 1.53086894e-05
Iter: 236 loss: 1.53826531e-05
Iter: 237 loss: 1.52816901e-05
Iter: 238 loss: 1.52538951e-05
Iter: 239 loss: 1.52511557e-05
Iter: 240 loss: 1.52402426e-05
Iter: 241 loss: 1.52379489e-05
Iter: 242 loss: 1.52258e-05
Iter: 243 loss: 1.51973964e-05
Iter: 244 loss: 1.55444577e-05
Iter: 245 loss: 1.51947243e-05
Iter: 246 loss: 1.51644972e-05
Iter: 247 loss: 1.51066988e-05
Iter: 248 loss: 1.62930755e-05
Iter: 249 loss: 1.51062895e-05
Iter: 250 loss: 1.50455453e-05
Iter: 251 loss: 1.53871697e-05
Iter: 252 loss: 1.50368924e-05
Iter: 253 loss: 1.49876387e-05
Iter: 254 loss: 1.51111035e-05
Iter: 255 loss: 1.49707012e-05
Iter: 256 loss: 1.49241159e-05
Iter: 257 loss: 1.53279889e-05
Iter: 258 loss: 1.49209282e-05
Iter: 259 loss: 1.48875442e-05
Iter: 260 loss: 1.53459259e-05
Iter: 261 loss: 1.48875069e-05
Iter: 262 loss: 1.48684248e-05
Iter: 263 loss: 1.48237032e-05
Iter: 264 loss: 1.54111058e-05
Iter: 265 loss: 1.48202034e-05
Iter: 266 loss: 1.47702958e-05
Iter: 267 loss: 1.49884117e-05
Iter: 268 loss: 1.47588107e-05
Iter: 269 loss: 1.47126302e-05
Iter: 270 loss: 1.47401124e-05
Iter: 271 loss: 1.46827151e-05
Iter: 272 loss: 1.46412485e-05
Iter: 273 loss: 1.46403509e-05
Iter: 274 loss: 1.46497932e-05
Iter: 275 loss: 1.46300745e-05
Iter: 276 loss: 1.46210077e-05
Iter: 277 loss: 1.46013808e-05
Iter: 278 loss: 1.49139014e-05
Iter: 279 loss: 1.46004422e-05
Iter: 280 loss: 1.4580899e-05
Iter: 281 loss: 1.45395734e-05
Iter: 282 loss: 1.52754619e-05
Iter: 283 loss: 1.45385739e-05
Iter: 284 loss: 1.44907353e-05
Iter: 285 loss: 1.46938373e-05
Iter: 286 loss: 1.44809546e-05
Iter: 287 loss: 1.44398209e-05
Iter: 288 loss: 1.45874383e-05
Iter: 289 loss: 1.44293954e-05
Iter: 290 loss: 1.43937377e-05
Iter: 291 loss: 1.45185568e-05
Iter: 292 loss: 1.43843754e-05
Iter: 293 loss: 1.43601228e-05
Iter: 294 loss: 1.43592888e-05
Iter: 295 loss: 1.43407797e-05
Iter: 296 loss: 1.43144525e-05
Iter: 297 loss: 1.43136876e-05
Iter: 298 loss: 1.42769313e-05
Iter: 299 loss: 1.42736299e-05
Iter: 300 loss: 1.42467852e-05
Iter: 301 loss: 1.42078197e-05
Iter: 302 loss: 1.43880152e-05
Iter: 303 loss: 1.42002873e-05
Iter: 304 loss: 1.41586734e-05
Iter: 305 loss: 1.42654635e-05
Iter: 306 loss: 1.41454093e-05
Iter: 307 loss: 1.41948858e-05
Iter: 308 loss: 1.41358441e-05
Iter: 309 loss: 1.41277051e-05
Iter: 310 loss: 1.41105284e-05
Iter: 311 loss: 1.44009518e-05
Iter: 312 loss: 1.41105993e-05
Iter: 313 loss: 1.40907769e-05
Iter: 314 loss: 1.40545562e-05
Iter: 315 loss: 1.49319312e-05
Iter: 316 loss: 1.40547063e-05
Iter: 317 loss: 1.40157636e-05
Iter: 318 loss: 1.41997843e-05
Iter: 319 loss: 1.40097964e-05
Iter: 320 loss: 1.39810445e-05
Iter: 321 loss: 1.39948052e-05
Iter: 322 loss: 1.39624299e-05
Iter: 323 loss: 1.39191434e-05
Iter: 324 loss: 1.40220054e-05
Iter: 325 loss: 1.3903209e-05
Iter: 326 loss: 1.38718387e-05
Iter: 327 loss: 1.38710484e-05
Iter: 328 loss: 1.38443183e-05
Iter: 329 loss: 1.38750438e-05
Iter: 330 loss: 1.38310379e-05
Iter: 331 loss: 1.3805382e-05
Iter: 332 loss: 1.37602256e-05
Iter: 333 loss: 1.37601164e-05
Iter: 334 loss: 1.37170664e-05
Iter: 335 loss: 1.40984848e-05
Iter: 336 loss: 1.37147508e-05
Iter: 337 loss: 1.36811022e-05
Iter: 338 loss: 1.37340267e-05
Iter: 339 loss: 1.3665438e-05
Iter: 340 loss: 1.36768158e-05
Iter: 341 loss: 1.3653751e-05
Iter: 342 loss: 1.36397939e-05
Iter: 343 loss: 1.36426006e-05
Iter: 344 loss: 1.3629392e-05
Iter: 345 loss: 1.36200088e-05
Iter: 346 loss: 1.36018916e-05
Iter: 347 loss: 1.39369167e-05
Iter: 348 loss: 1.36011959e-05
Iter: 349 loss: 1.35703531e-05
Iter: 350 loss: 1.35603605e-05
Iter: 351 loss: 1.35430473e-05
Iter: 352 loss: 1.35134023e-05
Iter: 353 loss: 1.38388614e-05
Iter: 354 loss: 1.35132668e-05
Iter: 355 loss: 1.34901375e-05
Iter: 356 loss: 1.34906131e-05
Iter: 357 loss: 1.34714282e-05
Iter: 358 loss: 1.34559468e-05
Iter: 359 loss: 1.34537131e-05
Iter: 360 loss: 1.34398961e-05
Iter: 361 loss: 1.34732345e-05
Iter: 362 loss: 1.34349712e-05
Iter: 363 loss: 1.34210277e-05
Iter: 364 loss: 1.33988215e-05
Iter: 365 loss: 1.33991298e-05
Iter: 366 loss: 1.33697158e-05
Iter: 367 loss: 1.34386237e-05
Iter: 368 loss: 1.33578424e-05
Iter: 369 loss: 1.33312878e-05
Iter: 370 loss: 1.33820868e-05
Iter: 371 loss: 1.33200319e-05
Iter: 372 loss: 1.32975656e-05
Iter: 373 loss: 1.32974756e-05
Iter: 374 loss: 1.32736195e-05
Iter: 375 loss: 1.34479233e-05
Iter: 376 loss: 1.32714322e-05
Iter: 377 loss: 1.32639525e-05
Iter: 378 loss: 1.32455325e-05
Iter: 379 loss: 1.33991107e-05
Iter: 380 loss: 1.32422238e-05
Iter: 381 loss: 1.32124032e-05
Iter: 382 loss: 1.32084442e-05
Iter: 383 loss: 1.31866609e-05
Iter: 384 loss: 1.31506022e-05
Iter: 385 loss: 1.34251841e-05
Iter: 386 loss: 1.31477354e-05
Iter: 387 loss: 1.3115271e-05
Iter: 388 loss: 1.30887083e-05
Iter: 389 loss: 1.30794106e-05
Iter: 390 loss: 1.30532653e-05
Iter: 391 loss: 1.30488734e-05
Iter: 392 loss: 1.30308381e-05
Iter: 393 loss: 1.32273162e-05
Iter: 394 loss: 1.30305589e-05
Iter: 395 loss: 1.30175422e-05
Iter: 396 loss: 1.29978298e-05
Iter: 397 loss: 1.2998189e-05
Iter: 398 loss: 1.29771915e-05
Iter: 399 loss: 1.30320968e-05
Iter: 400 loss: 1.29700429e-05
Iter: 401 loss: 1.29493692e-05
Iter: 402 loss: 1.29656873e-05
Iter: 403 loss: 1.29371929e-05
Iter: 404 loss: 1.29123819e-05
Iter: 405 loss: 1.31454844e-05
Iter: 406 loss: 1.29109594e-05
Iter: 407 loss: 1.28977008e-05
Iter: 408 loss: 1.2895699e-05
Iter: 409 loss: 1.2890815e-05
Iter: 410 loss: 1.2877359e-05
Iter: 411 loss: 1.30201497e-05
Iter: 412 loss: 1.28760657e-05
Iter: 413 loss: 1.28611864e-05
Iter: 414 loss: 1.28433276e-05
Iter: 415 loss: 1.28418578e-05
Iter: 416 loss: 1.28170805e-05
Iter: 417 loss: 1.30780463e-05
Iter: 418 loss: 1.28175361e-05
Iter: 419 loss: 1.27984704e-05
Iter: 420 loss: 1.27862722e-05
Iter: 421 loss: 1.27780568e-05
Iter: 422 loss: 1.27385065e-05
Iter: 423 loss: 1.29192267e-05
Iter: 424 loss: 1.27304447e-05
Iter: 425 loss: 1.271972e-05
Iter: 426 loss: 1.27142157e-05
Iter: 427 loss: 1.27029371e-05
Iter: 428 loss: 1.26995046e-05
Iter: 429 loss: 1.2693039e-05
Iter: 430 loss: 1.26759587e-05
Iter: 431 loss: 1.26516543e-05
Iter: 432 loss: 1.26510513e-05
Iter: 433 loss: 1.26236291e-05
Iter: 434 loss: 1.28336742e-05
Iter: 435 loss: 1.26206342e-05
Iter: 436 loss: 1.26021896e-05
Iter: 437 loss: 1.26379819e-05
Iter: 438 loss: 1.25942679e-05
Iter: 439 loss: 1.26021641e-05
Iter: 440 loss: 1.25862425e-05
Iter: 441 loss: 1.25825991e-05
Iter: 442 loss: 1.25706811e-05
Iter: 443 loss: 1.25762108e-05
Iter: 444 loss: 1.25600982e-05
Iter: 445 loss: 1.25390288e-05
Iter: 446 loss: 1.26686027e-05
Iter: 447 loss: 1.25368715e-05
Iter: 448 loss: 1.25202805e-05
Iter: 449 loss: 1.25387933e-05
Iter: 450 loss: 1.25111656e-05
Iter: 451 loss: 1.24928365e-05
Iter: 452 loss: 1.25358347e-05
Iter: 453 loss: 1.2486862e-05
Iter: 454 loss: 1.24678472e-05
Iter: 455 loss: 1.25152e-05
Iter: 456 loss: 1.24611706e-05
Iter: 457 loss: 1.24403614e-05
Iter: 458 loss: 1.25099486e-05
Iter: 459 loss: 1.24346188e-05
Iter: 460 loss: 1.24137205e-05
Iter: 461 loss: 1.2629971e-05
Iter: 462 loss: 1.24127373e-05
Iter: 463 loss: 1.23979462e-05
Iter: 464 loss: 1.23875288e-05
Iter: 465 loss: 1.23822974e-05
Iter: 466 loss: 1.23678783e-05
Iter: 467 loss: 1.24069802e-05
Iter: 468 loss: 1.23637437e-05
Iter: 469 loss: 1.23471727e-05
Iter: 470 loss: 1.23489153e-05
Iter: 471 loss: 1.23344662e-05
Iter: 472 loss: 1.2313053e-05
Iter: 473 loss: 1.2475416e-05
Iter: 474 loss: 1.23110785e-05
Iter: 475 loss: 1.23312102e-05
Iter: 476 loss: 1.23063783e-05
Iter: 477 loss: 1.23039435e-05
Iter: 478 loss: 1.22968977e-05
Iter: 479 loss: 1.22737156e-05
Iter: 480 loss: 1.25045644e-05
Iter: 481 loss: 1.2270576e-05
Iter: 482 loss: 1.22474858e-05
Iter: 483 loss: 1.25844963e-05
Iter: 484 loss: 1.22473248e-05
Iter: 485 loss: 1.22320835e-05
Iter: 486 loss: 1.22774218e-05
Iter: 487 loss: 1.22264264e-05
Iter: 488 loss: 1.22117708e-05
Iter: 489 loss: 1.2246981e-05
Iter: 490 loss: 1.22067868e-05
Iter: 491 loss: 1.2194947e-05
Iter: 492 loss: 1.22320725e-05
Iter: 493 loss: 1.21911526e-05
Iter: 494 loss: 1.21822541e-05
Iter: 495 loss: 1.22945639e-05
Iter: 496 loss: 1.21820949e-05
Iter: 497 loss: 1.21722169e-05
Iter: 498 loss: 1.2157836e-05
Iter: 499 loss: 1.21575194e-05
Iter: 500 loss: 1.21422627e-05
Iter: 501 loss: 1.22025904e-05
Iter: 502 loss: 1.21386956e-05
Iter: 503 loss: 1.21257181e-05
Iter: 504 loss: 1.21241392e-05
Iter: 505 loss: 1.21149751e-05
Iter: 506 loss: 1.20972218e-05
Iter: 507 loss: 1.21860485e-05
Iter: 508 loss: 1.2094647e-05
Iter: 509 loss: 1.20876684e-05
Iter: 510 loss: 1.20868499e-05
Iter: 511 loss: 1.2075292e-05
Iter: 512 loss: 1.20558771e-05
Iter: 513 loss: 1.20555796e-05
Iter: 514 loss: 1.20461063e-05
Iter: 515 loss: 1.20453351e-05
Iter: 516 loss: 1.20384975e-05
Iter: 517 loss: 1.20294935e-05
Iter: 518 loss: 1.20227669e-05
Iter: 519 loss: 1.20197856e-05
Iter: 520 loss: 1.19997121e-05
Iter: 521 loss: 1.20639879e-05
Iter: 522 loss: 1.19938268e-05
Iter: 523 loss: 1.19797805e-05
Iter: 524 loss: 1.21093035e-05
Iter: 525 loss: 1.19790229e-05
Iter: 526 loss: 1.19674914e-05
Iter: 527 loss: 1.19544175e-05
Iter: 528 loss: 1.19526585e-05
Iter: 529 loss: 1.193766e-05
Iter: 530 loss: 1.19376318e-05
Iter: 531 loss: 1.19239594e-05
Iter: 532 loss: 1.19466968e-05
Iter: 533 loss: 1.19172146e-05
Iter: 534 loss: 1.1908498e-05
Iter: 535 loss: 1.18989519e-05
Iter: 536 loss: 1.18969019e-05
Iter: 537 loss: 1.18800772e-05
Iter: 538 loss: 1.19574888e-05
Iter: 539 loss: 1.18763601e-05
Iter: 540 loss: 1.18699409e-05
Iter: 541 loss: 1.1869437e-05
Iter: 542 loss: 1.1862885e-05
Iter: 543 loss: 1.1919291e-05
Iter: 544 loss: 1.18626758e-05
Iter: 545 loss: 1.18599182e-05
Iter: 546 loss: 1.18493144e-05
Iter: 547 loss: 1.18593407e-05
Iter: 548 loss: 1.18411454e-05
Iter: 549 loss: 1.18285934e-05
Iter: 550 loss: 1.18948265e-05
Iter: 551 loss: 1.18262378e-05
Iter: 552 loss: 1.18126854e-05
Iter: 553 loss: 1.18349235e-05
Iter: 554 loss: 1.18068892e-05
Iter: 555 loss: 1.17957152e-05
Iter: 556 loss: 1.18692733e-05
Iter: 557 loss: 1.17950658e-05
Iter: 558 loss: 1.17841555e-05
Iter: 559 loss: 1.17867912e-05
Iter: 560 loss: 1.17765831e-05
Iter: 561 loss: 1.17619247e-05
Iter: 562 loss: 1.18378284e-05
Iter: 563 loss: 1.17601949e-05
Iter: 564 loss: 1.17538275e-05
Iter: 565 loss: 1.17532672e-05
Iter: 566 loss: 1.17473783e-05
Iter: 567 loss: 1.17327909e-05
Iter: 568 loss: 1.18758599e-05
Iter: 569 loss: 1.1730951e-05
Iter: 570 loss: 1.17160171e-05
Iter: 571 loss: 1.17668069e-05
Iter: 572 loss: 1.17117725e-05
Iter: 573 loss: 1.17009349e-05
Iter: 574 loss: 1.17435229e-05
Iter: 575 loss: 1.16984329e-05
Iter: 576 loss: 1.17006111e-05
Iter: 577 loss: 1.16934252e-05
Iter: 578 loss: 1.16891788e-05
Iter: 579 loss: 1.16765641e-05
Iter: 580 loss: 1.16922429e-05
Iter: 581 loss: 1.16664924e-05
Iter: 582 loss: 1.16555839e-05
Iter: 583 loss: 1.17184372e-05
Iter: 584 loss: 1.16546607e-05
Iter: 585 loss: 1.16440369e-05
Iter: 586 loss: 1.16278079e-05
Iter: 587 loss: 1.16277497e-05
Iter: 588 loss: 1.16045767e-05
Iter: 589 loss: 1.17684485e-05
Iter: 590 loss: 1.16021483e-05
Iter: 591 loss: 1.15881849e-05
Iter: 592 loss: 1.1643895e-05
Iter: 593 loss: 1.15852663e-05
Iter: 594 loss: 1.15714847e-05
Iter: 595 loss: 1.16030687e-05
Iter: 596 loss: 1.15662442e-05
Iter: 597 loss: 1.15555886e-05
Iter: 598 loss: 1.16003939e-05
Iter: 599 loss: 1.1553373e-05
Iter: 600 loss: 1.15416442e-05
Iter: 601 loss: 1.16131696e-05
Iter: 602 loss: 1.15403936e-05
Iter: 603 loss: 1.15295134e-05
Iter: 604 loss: 1.15136972e-05
Iter: 605 loss: 1.15134881e-05
Iter: 606 loss: 1.14985687e-05
Iter: 607 loss: 1.15685098e-05
Iter: 608 loss: 1.14949889e-05
Iter: 609 loss: 1.14837949e-05
Iter: 610 loss: 1.14883196e-05
Iter: 611 loss: 1.14761515e-05
Iter: 612 loss: 1.14755785e-05
Iter: 613 loss: 1.14669856e-05
Iter: 614 loss: 1.14647773e-05
Iter: 615 loss: 1.14580507e-05
Iter: 616 loss: 1.14647046e-05
Iter: 617 loss: 1.14519826e-05
Iter: 618 loss: 1.14392769e-05
Iter: 619 loss: 1.14271761e-05
Iter: 620 loss: 1.14243048e-05
Iter: 621 loss: 1.14105469e-05
Iter: 622 loss: 1.1409893e-05
Iter: 623 loss: 1.14004361e-05
Iter: 624 loss: 1.13876104e-05
Iter: 625 loss: 1.13875294e-05
Iter: 626 loss: 1.13682072e-05
Iter: 627 loss: 1.15391695e-05
Iter: 628 loss: 1.13678989e-05
Iter: 629 loss: 1.13593333e-05
Iter: 630 loss: 1.14518589e-05
Iter: 631 loss: 1.1359466e-05
Iter: 632 loss: 1.13538435e-05
Iter: 633 loss: 1.13949282e-05
Iter: 634 loss: 1.1352804e-05
Iter: 635 loss: 1.13459782e-05
Iter: 636 loss: 1.13435526e-05
Iter: 637 loss: 1.13399028e-05
Iter: 638 loss: 1.13318329e-05
Iter: 639 loss: 1.13452552e-05
Iter: 640 loss: 1.13283277e-05
Iter: 641 loss: 1.13204096e-05
Iter: 642 loss: 1.13102469e-05
Iter: 643 loss: 1.13095484e-05
Iter: 644 loss: 1.13052201e-05
Iter: 645 loss: 1.13021861e-05
Iter: 646 loss: 1.1291796e-05
Iter: 647 loss: 1.12869438e-05
Iter: 648 loss: 1.12818652e-05
Iter: 649 loss: 1.12776916e-05
Iter: 650 loss: 1.12692796e-05
Iter: 651 loss: 1.13986334e-05
Iter: 652 loss: 1.12683847e-05
Iter: 653 loss: 1.12513135e-05
Iter: 654 loss: 1.12560283e-05
Iter: 655 loss: 1.12397274e-05
Iter: 656 loss: 1.12269554e-05
Iter: 657 loss: 1.13674396e-05
Iter: 658 loss: 1.1226537e-05
Iter: 659 loss: 1.12121334e-05
Iter: 660 loss: 1.12073358e-05
Iter: 661 loss: 1.11987638e-05
Iter: 662 loss: 1.11868503e-05
Iter: 663 loss: 1.11868922e-05
Iter: 664 loss: 1.11783083e-05
Iter: 665 loss: 1.11671079e-05
Iter: 666 loss: 1.11663485e-05
Iter: 667 loss: 1.11547306e-05
Iter: 668 loss: 1.11550871e-05
Iter: 669 loss: 1.11457994e-05
Iter: 670 loss: 1.11523295e-05
Iter: 671 loss: 1.11400132e-05
Iter: 672 loss: 1.11298341e-05
Iter: 673 loss: 1.1118078e-05
Iter: 674 loss: 1.11172576e-05
Iter: 675 loss: 1.11094669e-05
Iter: 676 loss: 1.11091613e-05
Iter: 677 loss: 1.11010286e-05
Iter: 678 loss: 1.11516547e-05
Iter: 679 loss: 1.11003483e-05
Iter: 680 loss: 1.10976525e-05
Iter: 681 loss: 1.10897581e-05
Iter: 682 loss: 1.11348272e-05
Iter: 683 loss: 1.10872461e-05
Iter: 684 loss: 1.10751207e-05
Iter: 685 loss: 1.10761302e-05
Iter: 686 loss: 1.10662486e-05
Iter: 687 loss: 1.10512392e-05
Iter: 688 loss: 1.11254285e-05
Iter: 689 loss: 1.10492238e-05
Iter: 690 loss: 1.10366946e-05
Iter: 691 loss: 1.10367928e-05
Iter: 692 loss: 1.10258788e-05
Iter: 693 loss: 1.10080809e-05
Iter: 694 loss: 1.11713289e-05
Iter: 695 loss: 1.10070005e-05
Iter: 696 loss: 1.09987768e-05
Iter: 697 loss: 1.10450319e-05
Iter: 698 loss: 1.09976572e-05
Iter: 699 loss: 1.09895282e-05
Iter: 700 loss: 1.09848179e-05
Iter: 701 loss: 1.09811381e-05
Iter: 702 loss: 1.09748726e-05
Iter: 703 loss: 1.09729626e-05
Iter: 704 loss: 1.09694647e-05
Iter: 705 loss: 1.09756347e-05
Iter: 706 loss: 1.09673101e-05
Iter: 707 loss: 1.0963151e-05
Iter: 708 loss: 1.09885714e-05
Iter: 709 loss: 1.09625e-05
Iter: 710 loss: 1.09582788e-05
Iter: 711 loss: 1.09805806e-05
Iter: 712 loss: 1.09574166e-05
Iter: 713 loss: 1.09540797e-05
Iter: 714 loss: 1.09435259e-05
Iter: 715 loss: 1.09781504e-05
Iter: 716 loss: 1.09383927e-05
Iter: 717 loss: 1.09259818e-05
Iter: 718 loss: 1.09679077e-05
Iter: 719 loss: 1.09223074e-05
Iter: 720 loss: 1.09095145e-05
Iter: 721 loss: 1.09109233e-05
Iter: 722 loss: 1.08992226e-05
Iter: 723 loss: 1.08860859e-05
Iter: 724 loss: 1.10306901e-05
Iter: 725 loss: 1.08859913e-05
Iter: 726 loss: 1.08771846e-05
Iter: 727 loss: 1.08606973e-05
Iter: 728 loss: 1.08610602e-05
Iter: 729 loss: 1.0842803e-05
Iter: 730 loss: 1.11256595e-05
Iter: 731 loss: 1.08426084e-05
Iter: 732 loss: 1.08325257e-05
Iter: 733 loss: 1.08398581e-05
Iter: 734 loss: 1.08270269e-05
Iter: 735 loss: 1.08123322e-05
Iter: 736 loss: 1.08467484e-05
Iter: 737 loss: 1.08071e-05
Iter: 738 loss: 1.07962824e-05
Iter: 739 loss: 1.07952983e-05
Iter: 740 loss: 1.0792508e-05
Iter: 741 loss: 1.0803863e-05
Iter: 742 loss: 1.07911565e-05
Iter: 743 loss: 1.07871092e-05
Iter: 744 loss: 1.08098857e-05
Iter: 745 loss: 1.0786056e-05
Iter: 746 loss: 1.07828528e-05
Iter: 747 loss: 1.07734413e-05
Iter: 748 loss: 1.08270197e-05
Iter: 749 loss: 1.07711849e-05
Iter: 750 loss: 1.07624755e-05
Iter: 751 loss: 1.07899014e-05
Iter: 752 loss: 1.07595051e-05
Iter: 753 loss: 1.07510496e-05
Iter: 754 loss: 1.07496417e-05
Iter: 755 loss: 1.07438618e-05
Iter: 756 loss: 1.07295418e-05
Iter: 757 loss: 1.0760642e-05
Iter: 758 loss: 1.07236028e-05
Iter: 759 loss: 1.07116284e-05
Iter: 760 loss: 1.07570913e-05
Iter: 761 loss: 1.07084015e-05
Iter: 762 loss: 1.06940024e-05
Iter: 763 loss: 1.06971183e-05
Iter: 764 loss: 1.06831321e-05
Iter: 765 loss: 1.06702264e-05
Iter: 766 loss: 1.07388714e-05
Iter: 767 loss: 1.06676143e-05
Iter: 768 loss: 1.06495163e-05
Iter: 769 loss: 1.0672491e-05
Iter: 770 loss: 1.06400366e-05
Iter: 771 loss: 1.06340703e-05
Iter: 772 loss: 1.06317702e-05
Iter: 773 loss: 1.06251555e-05
Iter: 774 loss: 1.06242333e-05
Iter: 775 loss: 1.06195439e-05
Iter: 776 loss: 1.06155403e-05
Iter: 777 loss: 1.06145562e-05
Iter: 778 loss: 1.06085372e-05
Iter: 779 loss: 1.05966064e-05
Iter: 780 loss: 1.08284694e-05
Iter: 781 loss: 1.05962099e-05
Iter: 782 loss: 1.05889476e-05
Iter: 783 loss: 1.06004072e-05
Iter: 784 loss: 1.0584874e-05
Iter: 785 loss: 1.05767422e-05
Iter: 786 loss: 1.05647296e-05
Iter: 787 loss: 1.05648442e-05
Iter: 788 loss: 1.05482186e-05
Iter: 789 loss: 1.06984935e-05
Iter: 790 loss: 1.05475128e-05
Iter: 791 loss: 1.05397103e-05
Iter: 792 loss: 1.05787258e-05
Iter: 793 loss: 1.05382478e-05
Iter: 794 loss: 1.05301069e-05
Iter: 795 loss: 1.0514148e-05
Iter: 796 loss: 1.08429458e-05
Iter: 797 loss: 1.05141953e-05
Iter: 798 loss: 1.0499386e-05
Iter: 799 loss: 1.04994551e-05
Iter: 800 loss: 1.04904702e-05
Iter: 801 loss: 1.04843148e-05
Iter: 802 loss: 1.0481237e-05
Iter: 803 loss: 1.04677247e-05
Iter: 804 loss: 1.06009302e-05
Iter: 805 loss: 1.04675528e-05
Iter: 806 loss: 1.04643e-05
Iter: 807 loss: 1.04641849e-05
Iter: 808 loss: 1.04601722e-05
Iter: 809 loss: 1.04591563e-05
Iter: 810 loss: 1.04568608e-05
Iter: 811 loss: 1.04514475e-05
Iter: 812 loss: 1.04515802e-05
Iter: 813 loss: 1.04480296e-05
Iter: 814 loss: 1.04390747e-05
Iter: 815 loss: 1.0517093e-05
Iter: 816 loss: 1.0437564e-05
Iter: 817 loss: 1.04321352e-05
Iter: 818 loss: 1.04386436e-05
Iter: 819 loss: 1.04284436e-05
Iter: 820 loss: 1.04204737e-05
Iter: 821 loss: 1.0423134e-05
Iter: 822 loss: 1.04150276e-05
Iter: 823 loss: 1.04052669e-05
Iter: 824 loss: 1.04819974e-05
Iter: 825 loss: 1.04046303e-05
Iter: 826 loss: 1.03974489e-05
Iter: 827 loss: 1.03919792e-05
Iter: 828 loss: 1.03895381e-05
Iter: 829 loss: 1.03741304e-05
Iter: 830 loss: 1.04090204e-05
Iter: 831 loss: 1.03684779e-05
Iter: 832 loss: 1.03590774e-05
Iter: 833 loss: 1.04243863e-05
Iter: 834 loss: 1.03582588e-05
Iter: 835 loss: 1.03462207e-05
Iter: 836 loss: 1.03316379e-05
Iter: 837 loss: 1.03307293e-05
Iter: 838 loss: 1.03191524e-05
Iter: 839 loss: 1.03191433e-05
Iter: 840 loss: 1.03102029e-05
Iter: 841 loss: 1.03497905e-05
Iter: 842 loss: 1.03085622e-05
Iter: 843 loss: 1.02997737e-05
Iter: 844 loss: 1.04037535e-05
Iter: 845 loss: 1.02990925e-05
Iter: 846 loss: 1.02932217e-05
Iter: 847 loss: 1.03571274e-05
Iter: 848 loss: 1.02932154e-05
Iter: 849 loss: 1.02908907e-05
Iter: 850 loss: 1.02844497e-05
Iter: 851 loss: 1.03166567e-05
Iter: 852 loss: 1.02822787e-05
Iter: 853 loss: 1.02734721e-05
Iter: 854 loss: 1.0291531e-05
Iter: 855 loss: 1.02697904e-05
Iter: 856 loss: 1.02591421e-05
Iter: 857 loss: 1.02589911e-05
Iter: 858 loss: 1.02497906e-05
Iter: 859 loss: 1.02399354e-05
Iter: 860 loss: 1.02970171e-05
Iter: 861 loss: 1.02380836e-05
Iter: 862 loss: 1.02270951e-05
Iter: 863 loss: 1.02370932e-05
Iter: 864 loss: 1.02202303e-05
Iter: 865 loss: 1.0210746e-05
Iter: 866 loss: 1.02945605e-05
Iter: 867 loss: 1.02105614e-05
Iter: 868 loss: 1.02027425e-05
Iter: 869 loss: 1.01937876e-05
Iter: 870 loss: 1.01929909e-05
Iter: 871 loss: 1.01848109e-05
Iter: 872 loss: 1.01846981e-05
Iter: 873 loss: 1.017876e-05
Iter: 874 loss: 1.01721107e-05
Iter: 875 loss: 1.01713795e-05
Iter: 876 loss: 1.01690675e-05
Iter: 877 loss: 1.01658425e-05
Iter: 878 loss: 1.01619789e-05
Iter: 879 loss: 1.02160966e-05
Iter: 880 loss: 1.01619971e-05
Iter: 881 loss: 1.01593614e-05
Iter: 882 loss: 1.01571977e-05
Iter: 883 loss: 1.01562819e-05
Iter: 884 loss: 1.01536416e-05
Iter: 885 loss: 1.01461164e-05
Iter: 886 loss: 1.01858668e-05
Iter: 887 loss: 1.01438864e-05
Iter: 888 loss: 1.01332162e-05
Iter: 889 loss: 1.02039612e-05
Iter: 890 loss: 1.01321475e-05
Iter: 891 loss: 1.01265232e-05
Iter: 892 loss: 1.01870837e-05
Iter: 893 loss: 1.01267415e-05
Iter: 894 loss: 1.01219157e-05
Iter: 895 loss: 1.0109814e-05
Iter: 896 loss: 1.01784062e-05
Iter: 897 loss: 1.01062742e-05
Iter: 898 loss: 1.01040023e-05
Iter: 899 loss: 1.01003434e-05
Iter: 900 loss: 1.00949073e-05
Iter: 901 loss: 1.00851476e-05
Iter: 902 loss: 1.00850339e-05
Iter: 903 loss: 1.00757616e-05
Iter: 904 loss: 1.01793448e-05
Iter: 905 loss: 1.00759316e-05
Iter: 906 loss: 1.00701218e-05
Iter: 907 loss: 1.00721309e-05
Iter: 908 loss: 1.00669622e-05
Iter: 909 loss: 1.00604175e-05
Iter: 910 loss: 1.00884963e-05
Iter: 911 loss: 1.00590405e-05
Iter: 912 loss: 1.00556736e-05
Iter: 913 loss: 1.0055157e-05
Iter: 914 loss: 1.0052725e-05
Iter: 915 loss: 1.00561401e-05
Iter: 916 loss: 1.00516891e-05
Iter: 917 loss: 1.00497509e-05
Iter: 918 loss: 1.00456109e-05
Iter: 919 loss: 1.01083951e-05
Iter: 920 loss: 1.00454308e-05
Iter: 921 loss: 1.00401476e-05
Iter: 922 loss: 1.00375482e-05
Iter: 923 loss: 1.00344887e-05
Iter: 924 loss: 1.00279603e-05
Iter: 925 loss: 1.00804627e-05
Iter: 926 loss: 1.00275984e-05
Iter: 927 loss: 1.00228299e-05
Iter: 928 loss: 1.00222733e-05
Iter: 929 loss: 1.00187717e-05
Iter: 930 loss: 1.00102407e-05
Iter: 931 loss: 1.00474263e-05
Iter: 932 loss: 1.0008398e-05
Iter: 933 loss: 1.00040088e-05
Iter: 934 loss: 1.0009755e-05
Iter: 935 loss: 1.0001495e-05
Iter: 936 loss: 9.99514668e-06
Iter: 937 loss: 9.98984797e-06
Iter: 938 loss: 9.98756332e-06
Iter: 939 loss: 9.97913e-06
Iter: 940 loss: 1.01063606e-05
Iter: 941 loss: 9.97920688e-06
Iter: 942 loss: 9.97373081e-06
Iter: 943 loss: 9.97525694e-06
Iter: 944 loss: 9.96966082e-06
Iter: 945 loss: 9.96406652e-06
Iter: 946 loss: 1.00419238e-05
Iter: 947 loss: 9.96375729e-06
Iter: 948 loss: 9.960233e-06
Iter: 949 loss: 9.95948085e-06
Iter: 950 loss: 9.95884147e-06
Iter: 951 loss: 9.95831942e-06
Iter: 952 loss: 9.95773553e-06
Iter: 953 loss: 9.95594564e-06
Iter: 954 loss: 9.95079336e-06
Iter: 955 loss: 9.9882318e-06
Iter: 956 loss: 9.94985476e-06
Iter: 957 loss: 9.94344555e-06
Iter: 958 loss: 9.96370909e-06
Iter: 959 loss: 9.94179936e-06
Iter: 960 loss: 9.93687627e-06
Iter: 961 loss: 9.96298149e-06
Iter: 962 loss: 9.9364579e-06
Iter: 963 loss: 9.9314675e-06
Iter: 964 loss: 9.92920104e-06
Iter: 965 loss: 9.92663445e-06
Iter: 966 loss: 9.921745e-06
Iter: 967 loss: 9.92169e-06
Iter: 968 loss: 9.91888646e-06
Iter: 969 loss: 9.91485103e-06
Iter: 970 loss: 9.91450179e-06
Iter: 971 loss: 9.90705485e-06
Iter: 972 loss: 9.92052446e-06
Iter: 973 loss: 9.90403805e-06
Iter: 974 loss: 9.89925684e-06
Iter: 975 loss: 9.96274139e-06
Iter: 976 loss: 9.89933324e-06
Iter: 977 loss: 9.89492946e-06
Iter: 978 loss: 9.88896591e-06
Iter: 979 loss: 9.88889678e-06
Iter: 980 loss: 9.90890203e-06
Iter: 981 loss: 9.8866567e-06
Iter: 982 loss: 9.88536249e-06
Iter: 983 loss: 9.88351894e-06
Iter: 984 loss: 9.88347892e-06
Iter: 985 loss: 9.88235843e-06
Iter: 986 loss: 9.88778e-06
Iter: 987 loss: 9.88143802e-06
Iter: 988 loss: 9.87974363e-06
Iter: 989 loss: 9.87442581e-06
Iter: 990 loss: 9.90430362e-06
Iter: 991 loss: 9.87281055e-06
Iter: 992 loss: 9.86785744e-06
Iter: 993 loss: 9.9130757e-06
Iter: 994 loss: 9.86757732e-06
Iter: 995 loss: 9.86308e-06
Iter: 996 loss: 9.8620194e-06
Iter: 997 loss: 9.85933912e-06
Iter: 998 loss: 9.85370207e-06
Iter: 999 loss: 9.93527101e-06
Iter: 1000 loss: 9.85364386e-06
Iter: 1001 loss: 9.84949e-06
Iter: 1002 loss: 9.85323823e-06
Iter: 1003 loss: 9.84759208e-06
Iter: 1004 loss: 9.84314283e-06
Iter: 1005 loss: 9.85055885e-06
Iter: 1006 loss: 9.84062717e-06
Iter: 1007 loss: 9.83502741e-06
Iter: 1008 loss: 9.85408224e-06
Iter: 1009 loss: 9.83325663e-06
Iter: 1010 loss: 9.82728852e-06
Iter: 1011 loss: 9.83749487e-06
Iter: 1012 loss: 9.82478105e-06
Iter: 1013 loss: 9.82639722e-06
Iter: 1014 loss: 9.82277379e-06
Iter: 1015 loss: 9.82046276e-06
Iter: 1016 loss: 9.81926769e-06
Iter: 1017 loss: 9.8178607e-06
Iter: 1018 loss: 9.81625089e-06
Iter: 1019 loss: 9.8163473e-06
Iter: 1020 loss: 9.81458288e-06
Iter: 1021 loss: 9.81159246e-06
Iter: 1022 loss: 9.80805726e-06
Iter: 1023 loss: 9.80751611e-06
Iter: 1024 loss: 9.80209279e-06
Iter: 1025 loss: 9.81794801e-06
Iter: 1026 loss: 9.80019e-06
Iter: 1027 loss: 9.79633114e-06
Iter: 1028 loss: 9.78820935e-06
Iter: 1029 loss: 9.93329832e-06
Iter: 1030 loss: 9.78815297e-06
Iter: 1031 loss: 9.78031312e-06
Iter: 1032 loss: 9.78051139e-06
Iter: 1033 loss: 9.77606851e-06
Iter: 1034 loss: 9.77008676e-06
Iter: 1035 loss: 9.76988849e-06
Iter: 1036 loss: 9.76234605e-06
Iter: 1037 loss: 9.86317173e-06
Iter: 1038 loss: 9.7625707e-06
Iter: 1039 loss: 9.75743478e-06
Iter: 1040 loss: 9.76829506e-06
Iter: 1041 loss: 9.75500552e-06
Iter: 1042 loss: 9.74781688e-06
Iter: 1043 loss: 9.73633269e-06
Iter: 1044 loss: 9.73659371e-06
Iter: 1045 loss: 9.72843e-06
Iter: 1046 loss: 9.72773887e-06
Iter: 1047 loss: 9.7356733e-06
Iter: 1048 loss: 9.72673297e-06
Iter: 1049 loss: 9.72604e-06
Iter: 1050 loss: 9.72192174e-06
Iter: 1051 loss: 9.72226644e-06
Iter: 1052 loss: 9.71822556e-06
Iter: 1053 loss: 9.71306872e-06
Iter: 1054 loss: 9.76999945e-06
Iter: 1055 loss: 9.71309055e-06
Iter: 1056 loss: 9.70842e-06
Iter: 1057 loss: 9.71799273e-06
Iter: 1058 loss: 9.70587644e-06
Iter: 1059 loss: 9.70219935e-06
Iter: 1060 loss: 9.69491066e-06
Iter: 1061 loss: 9.86066607e-06
Iter: 1062 loss: 9.69442954e-06
Iter: 1063 loss: 9.68626591e-06
Iter: 1064 loss: 9.764889e-06
Iter: 1065 loss: 9.68629047e-06
Iter: 1066 loss: 9.68093173e-06
Iter: 1067 loss: 9.68465e-06
Iter: 1068 loss: 9.6778931e-06
Iter: 1069 loss: 9.67019878e-06
Iter: 1070 loss: 9.67828419e-06
Iter: 1071 loss: 9.66571679e-06
Iter: 1072 loss: 9.65833715e-06
Iter: 1073 loss: 9.75063176e-06
Iter: 1074 loss: 9.65847357e-06
Iter: 1075 loss: 9.65335676e-06
Iter: 1076 loss: 9.65458821e-06
Iter: 1077 loss: 9.64991887e-06
Iter: 1078 loss: 9.64001811e-06
Iter: 1079 loss: 9.65168329e-06
Iter: 1080 loss: 9.63518505e-06
Iter: 1081 loss: 9.63044749e-06
Iter: 1082 loss: 9.63006096e-06
Iter: 1083 loss: 9.62683771e-06
Iter: 1084 loss: 9.62664944e-06
Iter: 1085 loss: 9.62542617e-06
Iter: 1086 loss: 9.62145e-06
Iter: 1087 loss: 9.612143e-06
Iter: 1088 loss: 9.61196201e-06
Iter: 1089 loss: 9.60746547e-06
Iter: 1090 loss: 9.60453053e-06
Iter: 1091 loss: 9.59901263e-06
Iter: 1092 loss: 9.60546731e-06
Iter: 1093 loss: 9.59663521e-06
Iter: 1094 loss: 9.5914238e-06
Iter: 1095 loss: 9.5841242e-06
Iter: 1096 loss: 9.58393139e-06
Iter: 1097 loss: 9.57467091e-06
Iter: 1098 loss: 9.70927704e-06
Iter: 1099 loss: 9.57489738e-06
Iter: 1100 loss: 9.56841359e-06
Iter: 1101 loss: 9.60256148e-06
Iter: 1102 loss: 9.56696749e-06
Iter: 1103 loss: 9.56025815e-06
Iter: 1104 loss: 9.56208351e-06
Iter: 1105 loss: 9.55511314e-06
Iter: 1106 loss: 9.54736424e-06
Iter: 1107 loss: 9.59182216e-06
Iter: 1108 loss: 9.54624e-06
Iter: 1109 loss: 9.53783274e-06
Iter: 1110 loss: 9.54132247e-06
Iter: 1111 loss: 9.53213203e-06
Iter: 1112 loss: 9.52109258e-06
Iter: 1113 loss: 9.54708048e-06
Iter: 1114 loss: 9.51670154e-06
Iter: 1115 loss: 9.52847586e-06
Iter: 1116 loss: 9.51400943e-06
Iter: 1117 loss: 9.51131187e-06
Iter: 1118 loss: 9.50577942e-06
Iter: 1119 loss: 9.5980995e-06
Iter: 1120 loss: 9.50529102e-06
Iter: 1121 loss: 9.5006817e-06
Iter: 1122 loss: 9.50494905e-06
Iter: 1123 loss: 9.49757123e-06
Iter: 1124 loss: 9.48917295e-06
Iter: 1125 loss: 9.56605345e-06
Iter: 1126 loss: 9.48889e-06
Iter: 1127 loss: 9.48115667e-06
Iter: 1128 loss: 9.48618799e-06
Iter: 1129 loss: 9.47659464e-06
Iter: 1130 loss: 9.46961518e-06
Iter: 1131 loss: 9.46061118e-06
Iter: 1132 loss: 9.45983174e-06
Iter: 1133 loss: 9.44920157e-06
Iter: 1134 loss: 9.48997422e-06
Iter: 1135 loss: 9.44655585e-06
Iter: 1136 loss: 9.43754276e-06
Iter: 1137 loss: 9.44415478e-06
Iter: 1138 loss: 9.43185296e-06
Iter: 1139 loss: 9.42319093e-06
Iter: 1140 loss: 9.44183557e-06
Iter: 1141 loss: 9.41998223e-06
Iter: 1142 loss: 9.41459075e-06
Iter: 1143 loss: 9.40983409e-06
Iter: 1144 loss: 9.40898371e-06
Iter: 1145 loss: 9.40082828e-06
Iter: 1146 loss: 9.41472263e-06
Iter: 1147 loss: 9.39724123e-06
Iter: 1148 loss: 9.40146128e-06
Iter: 1149 loss: 9.39417259e-06
Iter: 1150 loss: 9.39223901e-06
Iter: 1151 loss: 9.38672747e-06
Iter: 1152 loss: 9.44766e-06
Iter: 1153 loss: 9.3862036e-06
Iter: 1154 loss: 9.38035737e-06
Iter: 1155 loss: 9.38319681e-06
Iter: 1156 loss: 9.37626373e-06
Iter: 1157 loss: 9.36874767e-06
Iter: 1158 loss: 9.39214442e-06
Iter: 1159 loss: 9.36677861e-06
Iter: 1160 loss: 9.35911703e-06
Iter: 1161 loss: 9.4365314e-06
Iter: 1162 loss: 9.35883145e-06
Iter: 1163 loss: 9.35388471e-06
Iter: 1164 loss: 9.35783464e-06
Iter: 1165 loss: 9.35118806e-06
Iter: 1166 loss: 9.3459148e-06
Iter: 1167 loss: 9.34720538e-06
Iter: 1168 loss: 9.34210129e-06
Iter: 1169 loss: 9.33436422e-06
Iter: 1170 loss: 9.33621413e-06
Iter: 1171 loss: 9.32821e-06
Iter: 1172 loss: 9.32032617e-06
Iter: 1173 loss: 9.41823146e-06
Iter: 1174 loss: 9.32030343e-06
Iter: 1175 loss: 9.31387e-06
Iter: 1176 loss: 9.30342685e-06
Iter: 1177 loss: 9.30365422e-06
Iter: 1178 loss: 9.2921473e-06
Iter: 1179 loss: 9.44729527e-06
Iter: 1180 loss: 9.29182534e-06
Iter: 1181 loss: 9.28788631e-06
Iter: 1182 loss: 9.34253e-06
Iter: 1183 loss: 9.28791906e-06
Iter: 1184 loss: 9.28371628e-06
Iter: 1185 loss: 9.29704674e-06
Iter: 1186 loss: 9.28260761e-06
Iter: 1187 loss: 9.28024383e-06
Iter: 1188 loss: 9.27555448e-06
Iter: 1189 loss: 9.31258819e-06
Iter: 1190 loss: 9.27456495e-06
Iter: 1191 loss: 9.2660739e-06
Iter: 1192 loss: 9.2934788e-06
Iter: 1193 loss: 9.26385292e-06
Iter: 1194 loss: 9.26019129e-06
Iter: 1195 loss: 9.25981112e-06
Iter: 1196 loss: 9.25683e-06
Iter: 1197 loss: 9.25156928e-06
Iter: 1198 loss: 9.25155473e-06
Iter: 1199 loss: 9.24391952e-06
Iter: 1200 loss: 9.24744e-06
Iter: 1201 loss: 9.23876e-06
Iter: 1202 loss: 9.23185e-06
Iter: 1203 loss: 9.26242683e-06
Iter: 1204 loss: 9.2308328e-06
Iter: 1205 loss: 9.225545e-06
Iter: 1206 loss: 9.22202889e-06
Iter: 1207 loss: 9.22003619e-06
Iter: 1208 loss: 9.21083665e-06
Iter: 1209 loss: 9.27505062e-06
Iter: 1210 loss: 9.20996e-06
Iter: 1211 loss: 9.2051e-06
Iter: 1212 loss: 9.27036854e-06
Iter: 1213 loss: 9.20499e-06
Iter: 1214 loss: 9.2030532e-06
Iter: 1215 loss: 9.23716652e-06
Iter: 1216 loss: 9.20278944e-06
Iter: 1217 loss: 9.2003138e-06
Iter: 1218 loss: 9.20211642e-06
Iter: 1219 loss: 9.1993461e-06
Iter: 1220 loss: 9.19613e-06
Iter: 1221 loss: 9.19049762e-06
Iter: 1222 loss: 9.30016176e-06
Iter: 1223 loss: 9.19015292e-06
Iter: 1224 loss: 9.18494334e-06
Iter: 1225 loss: 9.20754792e-06
Iter: 1226 loss: 9.18339811e-06
Iter: 1227 loss: 9.17889611e-06
Iter: 1228 loss: 9.20893763e-06
Iter: 1229 loss: 9.17801481e-06
Iter: 1230 loss: 9.17250782e-06
Iter: 1231 loss: 9.17118e-06
Iter: 1232 loss: 9.16762656e-06
Iter: 1233 loss: 9.15982582e-06
Iter: 1234 loss: 9.19163631e-06
Iter: 1235 loss: 9.15839064e-06
Iter: 1236 loss: 9.15367855e-06
Iter: 1237 loss: 9.16385943e-06
Iter: 1238 loss: 9.15211058e-06
Iter: 1239 loss: 9.14744305e-06
Iter: 1240 loss: 9.1399088e-06
Iter: 1241 loss: 9.1397842e-06
Iter: 1242 loss: 9.12986434e-06
Iter: 1243 loss: 9.20084676e-06
Iter: 1244 loss: 9.1287e-06
Iter: 1245 loss: 9.1217953e-06
Iter: 1246 loss: 9.158789e-06
Iter: 1247 loss: 9.12082e-06
Iter: 1248 loss: 9.11513234e-06
Iter: 1249 loss: 9.11546e-06
Iter: 1250 loss: 9.11064217e-06
Iter: 1251 loss: 9.11545794e-06
Iter: 1252 loss: 9.10808831e-06
Iter: 1253 loss: 9.10292e-06
Iter: 1254 loss: 9.10078415e-06
Iter: 1255 loss: 9.09805567e-06
Iter: 1256 loss: 9.09266782e-06
Iter: 1257 loss: 9.0956546e-06
Iter: 1258 loss: 9.08943184e-06
Iter: 1259 loss: 9.08311631e-06
Iter: 1260 loss: 9.10512517e-06
Iter: 1261 loss: 9.08126822e-06
Iter: 1262 loss: 9.07406593e-06
Iter: 1263 loss: 9.12075484e-06
Iter: 1264 loss: 9.07288631e-06
Iter: 1265 loss: 9.06858259e-06
Iter: 1266 loss: 9.08173479e-06
Iter: 1267 loss: 9.06720561e-06
Iter: 1268 loss: 9.06336845e-06
Iter: 1269 loss: 9.06072637e-06
Iter: 1270 loss: 9.05937668e-06
Iter: 1271 loss: 9.05191791e-06
Iter: 1272 loss: 9.06201149e-06
Iter: 1273 loss: 9.04874287e-06
Iter: 1274 loss: 9.04338049e-06
Iter: 1275 loss: 9.06406603e-06
Iter: 1276 loss: 9.04233639e-06
Iter: 1277 loss: 9.03620094e-06
Iter: 1278 loss: 9.03167438e-06
Iter: 1279 loss: 9.02942702e-06
Iter: 1280 loss: 9.03611908e-06
Iter: 1281 loss: 9.02684587e-06
Iter: 1282 loss: 9.02433749e-06
Iter: 1283 loss: 9.02948068e-06
Iter: 1284 loss: 9.02369538e-06
Iter: 1285 loss: 9.02076954e-06
Iter: 1286 loss: 9.01620297e-06
Iter: 1287 loss: 9.01606654e-06
Iter: 1288 loss: 9.01032399e-06
Iter: 1289 loss: 9.00362647e-06
Iter: 1290 loss: 9.00292525e-06
Iter: 1291 loss: 8.9916939e-06
Iter: 1292 loss: 9.03258206e-06
Iter: 1293 loss: 8.98857888e-06
Iter: 1294 loss: 8.98676353e-06
Iter: 1295 loss: 8.98474536e-06
Iter: 1296 loss: 8.98182407e-06
Iter: 1297 loss: 8.97790324e-06
Iter: 1298 loss: 8.97776317e-06
Iter: 1299 loss: 8.97038808e-06
Iter: 1300 loss: 8.98333201e-06
Iter: 1301 loss: 8.96779238e-06
Iter: 1302 loss: 8.96250913e-06
Iter: 1303 loss: 8.98180951e-06
Iter: 1304 loss: 8.96096935e-06
Iter: 1305 loss: 8.95459289e-06
Iter: 1306 loss: 8.94755794e-06
Iter: 1307 loss: 8.94681216e-06
Iter: 1308 loss: 8.93623474e-06
Iter: 1309 loss: 9.00527539e-06
Iter: 1310 loss: 8.93504512e-06
Iter: 1311 loss: 8.9294208e-06
Iter: 1312 loss: 8.96491656e-06
Iter: 1313 loss: 8.92890603e-06
Iter: 1314 loss: 8.9232235e-06
Iter: 1315 loss: 8.92313437e-06
Iter: 1316 loss: 8.92138814e-06
Iter: 1317 loss: 8.9142377e-06
Iter: 1318 loss: 8.95861194e-06
Iter: 1319 loss: 8.91263335e-06
Iter: 1320 loss: 8.90381125e-06
Iter: 1321 loss: 8.94962068e-06
Iter: 1322 loss: 8.90275896e-06
Iter: 1323 loss: 8.89762123e-06
Iter: 1324 loss: 8.89644161e-06
Iter: 1325 loss: 8.89281273e-06
Iter: 1326 loss: 8.88404884e-06
Iter: 1327 loss: 8.91411673e-06
Iter: 1328 loss: 8.88131581e-06
Iter: 1329 loss: 8.8782e-06
Iter: 1330 loss: 8.87716669e-06
Iter: 1331 loss: 8.87475107e-06
Iter: 1332 loss: 8.86944508e-06
Iter: 1333 loss: 8.94721234e-06
Iter: 1334 loss: 8.86926227e-06
Iter: 1335 loss: 8.8610177e-06
Iter: 1336 loss: 8.87551323e-06
Iter: 1337 loss: 8.85723603e-06
Iter: 1338 loss: 8.84932069e-06
Iter: 1339 loss: 8.89299827e-06
Iter: 1340 loss: 8.84801739e-06
Iter: 1341 loss: 8.84252131e-06
Iter: 1342 loss: 8.84007e-06
Iter: 1343 loss: 8.83746725e-06
Iter: 1344 loss: 8.82660515e-06
Iter: 1345 loss: 8.85703412e-06
Iter: 1346 loss: 8.8233046e-06
Iter: 1347 loss: 8.8373381e-06
Iter: 1348 loss: 8.82037421e-06
Iter: 1349 loss: 8.81896e-06
Iter: 1350 loss: 8.81486085e-06
Iter: 1351 loss: 8.8524439e-06
Iter: 1352 loss: 8.81407595e-06
Iter: 1353 loss: 8.8088e-06
Iter: 1354 loss: 8.81807318e-06
Iter: 1355 loss: 8.80622429e-06
Iter: 1356 loss: 8.79859908e-06
Iter: 1357 loss: 8.80965399e-06
Iter: 1358 loss: 8.79492927e-06
Iter: 1359 loss: 8.78808169e-06
Iter: 1360 loss: 8.81636e-06
Iter: 1361 loss: 8.78658284e-06
Iter: 1362 loss: 8.78288392e-06
Iter: 1363 loss: 8.78295214e-06
Iter: 1364 loss: 8.7792414e-06
Iter: 1365 loss: 8.77894672e-06
Iter: 1366 loss: 8.77633e-06
Iter: 1367 loss: 8.77201819e-06
Iter: 1368 loss: 8.76667582e-06
Iter: 1369 loss: 8.76612103e-06
Iter: 1370 loss: 8.76016747e-06
Iter: 1371 loss: 8.76037302e-06
Iter: 1372 loss: 8.75421301e-06
Iter: 1373 loss: 8.75500518e-06
Iter: 1374 loss: 8.74945545e-06
Iter: 1375 loss: 8.74421858e-06
Iter: 1376 loss: 8.73910358e-06
Iter: 1377 loss: 8.7378794e-06
Iter: 1378 loss: 8.73603312e-06
Iter: 1379 loss: 8.73446e-06
Iter: 1380 loss: 8.73310546e-06
Iter: 1381 loss: 8.7291337e-06
Iter: 1382 loss: 8.74885245e-06
Iter: 1383 loss: 8.72820419e-06
Iter: 1384 loss: 8.72209785e-06
Iter: 1385 loss: 8.73077806e-06
Iter: 1386 loss: 8.71954e-06
Iter: 1387 loss: 8.71208613e-06
Iter: 1388 loss: 8.7470562e-06
Iter: 1389 loss: 8.71086741e-06
Iter: 1390 loss: 8.70562144e-06
Iter: 1391 loss: 8.73276804e-06
Iter: 1392 loss: 8.70422627e-06
Iter: 1393 loss: 8.69850919e-06
Iter: 1394 loss: 8.70580516e-06
Iter: 1395 loss: 8.69553423e-06
Iter: 1396 loss: 8.68889765e-06
Iter: 1397 loss: 8.72729379e-06
Iter: 1398 loss: 8.68767893e-06
Iter: 1399 loss: 8.67956078e-06
Iter: 1400 loss: 8.69548239e-06
Iter: 1401 loss: 8.67617564e-06
Iter: 1402 loss: 8.67285689e-06
Iter: 1403 loss: 8.66656046e-06
Iter: 1404 loss: 8.81459437e-06
Iter: 1405 loss: 8.66645496e-06
Iter: 1406 loss: 8.65828952e-06
Iter: 1407 loss: 8.72315832e-06
Iter: 1408 loss: 8.65767925e-06
Iter: 1409 loss: 8.65245784e-06
Iter: 1410 loss: 8.6542932e-06
Iter: 1411 loss: 8.64880712e-06
Iter: 1412 loss: 8.65397487e-06
Iter: 1413 loss: 8.64676076e-06
Iter: 1414 loss: 8.64480353e-06
Iter: 1415 loss: 8.64179492e-06
Iter: 1416 loss: 8.70950589e-06
Iter: 1417 loss: 8.64172e-06
Iter: 1418 loss: 8.63666173e-06
Iter: 1419 loss: 8.65430047e-06
Iter: 1420 loss: 8.6353e-06
Iter: 1421 loss: 8.6289474e-06
Iter: 1422 loss: 8.63468085e-06
Iter: 1423 loss: 8.62514389e-06
Iter: 1424 loss: 8.62078105e-06
Iter: 1425 loss: 8.62045e-06
Iter: 1426 loss: 8.6178552e-06
Iter: 1427 loss: 8.616581e-06
Iter: 1428 loss: 8.6155e-06
Iter: 1429 loss: 8.60978616e-06
Iter: 1430 loss: 8.60896216e-06
Iter: 1431 loss: 8.60460204e-06
Iter: 1432 loss: 8.60302953e-06
Iter: 1433 loss: 8.60116688e-06
Iter: 1434 loss: 8.59874672e-06
Iter: 1435 loss: 8.59753163e-06
Iter: 1436 loss: 8.5969732e-06
Iter: 1437 loss: 8.59299871e-06
Iter: 1438 loss: 8.58565181e-06
Iter: 1439 loss: 8.58594285e-06
Iter: 1440 loss: 8.57733357e-06
Iter: 1441 loss: 8.63426885e-06
Iter: 1442 loss: 8.57615851e-06
Iter: 1443 loss: 8.58148e-06
Iter: 1444 loss: 8.57470604e-06
Iter: 1445 loss: 8.5732263e-06
Iter: 1446 loss: 8.57092891e-06
Iter: 1447 loss: 8.57081523e-06
Iter: 1448 loss: 8.56876795e-06
Iter: 1449 loss: 8.56262068e-06
Iter: 1450 loss: 8.59175361e-06
Iter: 1451 loss: 8.56060433e-06
Iter: 1452 loss: 8.55318649e-06
Iter: 1453 loss: 8.61614535e-06
Iter: 1454 loss: 8.552699e-06
Iter: 1455 loss: 8.54658174e-06
Iter: 1456 loss: 8.55200233e-06
Iter: 1457 loss: 8.54302652e-06
Iter: 1458 loss: 8.53513302e-06
Iter: 1459 loss: 8.57045325e-06
Iter: 1460 loss: 8.53374513e-06
Iter: 1461 loss: 8.52876656e-06
Iter: 1462 loss: 8.53995152e-06
Iter: 1463 loss: 8.52702215e-06
Iter: 1464 loss: 8.5215961e-06
Iter: 1465 loss: 8.55164762e-06
Iter: 1466 loss: 8.52078847e-06
Iter: 1467 loss: 8.51606273e-06
Iter: 1468 loss: 8.55122744e-06
Iter: 1469 loss: 8.5154561e-06
Iter: 1470 loss: 8.51271943e-06
Iter: 1471 loss: 8.50969536e-06
Iter: 1472 loss: 8.50943616e-06
Iter: 1473 loss: 8.5030606e-06
Iter: 1474 loss: 8.5176116e-06
Iter: 1475 loss: 8.50108518e-06
Iter: 1476 loss: 8.50654487e-06
Iter: 1477 loss: 8.49918e-06
Iter: 1478 loss: 8.4972944e-06
Iter: 1479 loss: 8.49540447e-06
Iter: 1480 loss: 8.49487697e-06
Iter: 1481 loss: 8.49162279e-06
Iter: 1482 loss: 8.48405853e-06
Iter: 1483 loss: 8.58140538e-06
Iter: 1484 loss: 8.48378841e-06
Iter: 1485 loss: 8.47627325e-06
Iter: 1486 loss: 8.50965534e-06
Iter: 1487 loss: 8.47474439e-06
Iter: 1488 loss: 8.46978583e-06
Iter: 1489 loss: 8.48555646e-06
Iter: 1490 loss: 8.46763214e-06
Iter: 1491 loss: 8.46101648e-06
Iter: 1492 loss: 8.46202238e-06
Iter: 1493 loss: 8.45571867e-06
Iter: 1494 loss: 8.44935676e-06
Iter: 1495 loss: 8.54577047e-06
Iter: 1496 loss: 8.44900114e-06
Iter: 1497 loss: 8.4444473e-06
Iter: 1498 loss: 8.44266e-06
Iter: 1499 loss: 8.44016176e-06
Iter: 1500 loss: 8.43134512e-06
Iter: 1501 loss: 8.45529576e-06
Iter: 1502 loss: 8.42839563e-06
Iter: 1503 loss: 8.42333066e-06
Iter: 1504 loss: 8.42322152e-06
Iter: 1505 loss: 8.41919245e-06
Iter: 1506 loss: 8.41910696e-06
Iter: 1507 loss: 8.41559358e-06
Iter: 1508 loss: 8.40991652e-06
Iter: 1509 loss: 8.40255416e-06
Iter: 1510 loss: 8.40201938e-06
Iter: 1511 loss: 8.41066139e-06
Iter: 1512 loss: 8.39784479e-06
Iter: 1513 loss: 8.39708264e-06
Iter: 1514 loss: 8.39294444e-06
Iter: 1515 loss: 8.396e-06
Iter: 1516 loss: 8.39008317e-06
Iter: 1517 loss: 8.38403867e-06
Iter: 1518 loss: 8.37940934e-06
Iter: 1519 loss: 8.37757852e-06
Iter: 1520 loss: 8.36866e-06
Iter: 1521 loss: 8.43197e-06
Iter: 1522 loss: 8.36847175e-06
Iter: 1523 loss: 8.36251274e-06
Iter: 1524 loss: 8.37111111e-06
Iter: 1525 loss: 8.35946139e-06
Iter: 1526 loss: 8.352441e-06
Iter: 1527 loss: 8.36205709e-06
Iter: 1528 loss: 8.34935327e-06
Iter: 1529 loss: 8.34284219e-06
Iter: 1530 loss: 8.38449705e-06
Iter: 1531 loss: 8.34223465e-06
Iter: 1532 loss: 8.33568083e-06
Iter: 1533 loss: 8.33503236e-06
Iter: 1534 loss: 8.33079866e-06
Iter: 1535 loss: 8.32412297e-06
Iter: 1536 loss: 8.38361302e-06
Iter: 1537 loss: 8.32370733e-06
Iter: 1538 loss: 8.31990837e-06
Iter: 1539 loss: 8.37065454e-06
Iter: 1540 loss: 8.31952184e-06
Iter: 1541 loss: 8.31563375e-06
Iter: 1542 loss: 8.31955185e-06
Iter: 1543 loss: 8.31322905e-06
Iter: 1544 loss: 8.31072066e-06
Iter: 1545 loss: 8.31094076e-06
Iter: 1546 loss: 8.30816316e-06
Iter: 1547 loss: 8.307401e-06
Iter: 1548 loss: 8.3053128e-06
Iter: 1549 loss: 8.30313547e-06
Iter: 1550 loss: 8.2980805e-06
Iter: 1551 loss: 8.34122875e-06
Iter: 1552 loss: 8.29741839e-06
Iter: 1553 loss: 8.28946759e-06
Iter: 1554 loss: 8.2971128e-06
Iter: 1555 loss: 8.28446446e-06
Iter: 1556 loss: 8.27792337e-06
Iter: 1557 loss: 8.33185e-06
Iter: 1558 loss: 8.27762506e-06
Iter: 1559 loss: 8.27206713e-06
Iter: 1560 loss: 8.27770691e-06
Iter: 1561 loss: 8.26910218e-06
Iter: 1562 loss: 8.26215728e-06
Iter: 1563 loss: 8.26647556e-06
Iter: 1564 loss: 8.25797088e-06
Iter: 1565 loss: 8.24912604e-06
Iter: 1566 loss: 8.29881446e-06
Iter: 1567 loss: 8.24767176e-06
Iter: 1568 loss: 8.23994196e-06
Iter: 1569 loss: 8.26843279e-06
Iter: 1570 loss: 8.23832306e-06
Iter: 1571 loss: 8.23186929e-06
Iter: 1572 loss: 8.25185816e-06
Iter: 1573 loss: 8.22983839e-06
Iter: 1574 loss: 8.22389302e-06
Iter: 1575 loss: 8.30075351e-06
Iter: 1576 loss: 8.22393122e-06
Iter: 1577 loss: 8.2190727e-06
Iter: 1578 loss: 8.21820686e-06
Iter: 1579 loss: 8.21495087e-06
Iter: 1580 loss: 8.21042067e-06
Iter: 1581 loss: 8.20956757e-06
Iter: 1582 loss: 8.20844161e-06
Iter: 1583 loss: 8.20438254e-06
Iter: 1584 loss: 8.22020229e-06
Iter: 1585 loss: 8.20287823e-06
Iter: 1586 loss: 8.19742581e-06
Iter: 1587 loss: 8.20403875e-06
Iter: 1588 loss: 8.19454181e-06
Iter: 1589 loss: 8.18913941e-06
Iter: 1590 loss: 8.21186131e-06
Iter: 1591 loss: 8.18814169e-06
Iter: 1592 loss: 8.18282206e-06
Iter: 1593 loss: 8.18173066e-06
Iter: 1594 loss: 8.17785167e-06
Iter: 1595 loss: 8.17115415e-06
Iter: 1596 loss: 8.23121263e-06
Iter: 1597 loss: 8.17093223e-06
Iter: 1598 loss: 8.16583724e-06
Iter: 1599 loss: 8.16335341e-06
Iter: 1600 loss: 8.16097599e-06
Iter: 1601 loss: 8.15243584e-06
Iter: 1602 loss: 8.18455374e-06
Iter: 1603 loss: 8.15061139e-06
Iter: 1604 loss: 8.14337909e-06
Iter: 1605 loss: 8.14354826e-06
Iter: 1606 loss: 8.13778e-06
Iter: 1607 loss: 8.13097722e-06
Iter: 1608 loss: 8.13089173e-06
Iter: 1609 loss: 8.12451435e-06
Iter: 1610 loss: 8.17686e-06
Iter: 1611 loss: 8.12399048e-06
Iter: 1612 loss: 8.12268536e-06
Iter: 1613 loss: 8.12169128e-06
Iter: 1614 loss: 8.12074359e-06
Iter: 1615 loss: 8.11720201e-06
Iter: 1616 loss: 8.13407678e-06
Iter: 1617 loss: 8.11586142e-06
Iter: 1618 loss: 8.11169e-06
Iter: 1619 loss: 8.1082726e-06
Iter: 1620 loss: 8.10694837e-06
Iter: 1621 loss: 8.10102392e-06
Iter: 1622 loss: 8.15965e-06
Iter: 1623 loss: 8.100782e-06
Iter: 1624 loss: 8.09513222e-06
Iter: 1625 loss: 8.09876656e-06
Iter: 1626 loss: 8.09183439e-06
Iter: 1627 loss: 8.08492223e-06
Iter: 1628 loss: 8.12157668e-06
Iter: 1629 loss: 8.08406367e-06
Iter: 1630 loss: 8.07981814e-06
Iter: 1631 loss: 8.09030644e-06
Iter: 1632 loss: 8.07805645e-06
Iter: 1633 loss: 8.07321067e-06
Iter: 1634 loss: 8.08060486e-06
Iter: 1635 loss: 8.07070683e-06
Iter: 1636 loss: 8.06499e-06
Iter: 1637 loss: 8.06728167e-06
Iter: 1638 loss: 8.06081789e-06
Iter: 1639 loss: 8.05389573e-06
Iter: 1640 loss: 8.11924656e-06
Iter: 1641 loss: 8.05389755e-06
Iter: 1642 loss: 8.0524369e-06
Iter: 1643 loss: 8.0512682e-06
Iter: 1644 loss: 8.05007858e-06
Iter: 1645 loss: 8.05204e-06
Iter: 1646 loss: 8.04959382e-06
Iter: 1647 loss: 8.04704e-06
Iter: 1648 loss: 8.04017054e-06
Iter: 1649 loss: 8.08693767e-06
Iter: 1650 loss: 8.03898183e-06
Iter: 1651 loss: 8.03362127e-06
Iter: 1652 loss: 8.04263163e-06
Iter: 1653 loss: 8.03122e-06
Iter: 1654 loss: 8.02563773e-06
Iter: 1655 loss: 8.03508738e-06
Iter: 1656 loss: 8.02379145e-06
Iter: 1657 loss: 8.01718306e-06
Iter: 1658 loss: 8.04447e-06
Iter: 1659 loss: 8.01552233e-06
Iter: 1660 loss: 8.00990165e-06
Iter: 1661 loss: 8.02093746e-06
Iter: 1662 loss: 8.00714679e-06
Iter: 1663 loss: 8.00067755e-06
Iter: 1664 loss: 8.01512124e-06
Iter: 1665 loss: 7.99802e-06
Iter: 1666 loss: 7.99209647e-06
Iter: 1667 loss: 8.01387068e-06
Iter: 1668 loss: 7.9904712e-06
Iter: 1669 loss: 7.98534e-06
Iter: 1670 loss: 8.00107227e-06
Iter: 1671 loss: 7.98418114e-06
Iter: 1672 loss: 7.97897246e-06
Iter: 1673 loss: 7.99069767e-06
Iter: 1674 loss: 7.97705434e-06
Iter: 1675 loss: 7.97372195e-06
Iter: 1676 loss: 7.97363191e-06
Iter: 1677 loss: 7.97182111e-06
Iter: 1678 loss: 7.97183202e-06
Iter: 1679 loss: 7.97024222e-06
Iter: 1680 loss: 7.96681616e-06
Iter: 1681 loss: 8.01766873e-06
Iter: 1682 loss: 7.96695258e-06
Iter: 1683 loss: 7.96328641e-06
Iter: 1684 loss: 7.95954656e-06
Iter: 1685 loss: 7.95903816e-06
Iter: 1686 loss: 7.95464894e-06
Iter: 1687 loss: 7.95654887e-06
Iter: 1688 loss: 7.95185952e-06
Iter: 1689 loss: 7.9457368e-06
Iter: 1690 loss: 7.97363464e-06
Iter: 1691 loss: 7.94424886e-06
Iter: 1692 loss: 7.94035168e-06
Iter: 1693 loss: 7.96202221e-06
Iter: 1694 loss: 7.93991603e-06
Iter: 1695 loss: 7.93588879e-06
Iter: 1696 loss: 7.93739673e-06
Iter: 1697 loss: 7.93281288e-06
Iter: 1698 loss: 7.92782339e-06
Iter: 1699 loss: 7.95145e-06
Iter: 1700 loss: 7.92703941e-06
Iter: 1701 loss: 7.923054e-06
Iter: 1702 loss: 7.92990249e-06
Iter: 1703 loss: 7.92150422e-06
Iter: 1704 loss: 7.91662205e-06
Iter: 1705 loss: 7.92389437e-06
Iter: 1706 loss: 7.91441562e-06
Iter: 1707 loss: 7.91107e-06
Iter: 1708 loss: 7.91107414e-06
Iter: 1709 loss: 7.90922786e-06
Iter: 1710 loss: 7.90928789e-06
Iter: 1711 loss: 7.90778176e-06
Iter: 1712 loss: 7.90946615e-06
Iter: 1713 loss: 7.90704325e-06
Iter: 1714 loss: 7.90514059e-06
Iter: 1715 loss: 7.90140075e-06
Iter: 1716 loss: 7.93876e-06
Iter: 1717 loss: 7.90082777e-06
Iter: 1718 loss: 7.8977082e-06
Iter: 1719 loss: 7.91384628e-06
Iter: 1720 loss: 7.89706428e-06
Iter: 1721 loss: 7.89462138e-06
Iter: 1722 loss: 7.89223668e-06
Iter: 1723 loss: 7.89174555e-06
Iter: 1724 loss: 7.88756824e-06
Iter: 1725 loss: 7.91479124e-06
Iter: 1726 loss: 7.88726e-06
Iter: 1727 loss: 7.88287525e-06
Iter: 1728 loss: 7.88262e-06
Iter: 1729 loss: 7.87962745e-06
Iter: 1730 loss: 7.87316094e-06
Iter: 1731 loss: 7.90723425e-06
Iter: 1732 loss: 7.87168e-06
Iter: 1733 loss: 7.86701912e-06
Iter: 1734 loss: 7.86822784e-06
Iter: 1735 loss: 7.86339206e-06
Iter: 1736 loss: 7.85661905e-06
Iter: 1737 loss: 7.89284786e-06
Iter: 1738 loss: 7.85524e-06
Iter: 1739 loss: 7.85041084e-06
Iter: 1740 loss: 7.87003137e-06
Iter: 1741 loss: 7.84957774e-06
Iter: 1742 loss: 7.84884287e-06
Iter: 1743 loss: 7.84796521e-06
Iter: 1744 loss: 7.8464e-06
Iter: 1745 loss: 7.84697932e-06
Iter: 1746 loss: 7.84560143e-06
Iter: 1747 loss: 7.84270924e-06
Iter: 1748 loss: 7.83735322e-06
Iter: 1749 loss: 7.93284198e-06
Iter: 1750 loss: 7.83715768e-06
Iter: 1751 loss: 7.83330688e-06
Iter: 1752 loss: 7.8455987e-06
Iter: 1753 loss: 7.83233736e-06
Iter: 1754 loss: 7.82889e-06
Iter: 1755 loss: 7.82394272e-06
Iter: 1756 loss: 7.82391817e-06
Iter: 1757 loss: 7.81814379e-06
Iter: 1758 loss: 7.87721547e-06
Iter: 1759 loss: 7.81814106e-06
Iter: 1760 loss: 7.81351628e-06
Iter: 1761 loss: 7.81451672e-06
Iter: 1762 loss: 7.81044309e-06
Iter: 1763 loss: 7.80376286e-06
Iter: 1764 loss: 7.83573432e-06
Iter: 1765 loss: 7.80244409e-06
Iter: 1766 loss: 7.79779e-06
Iter: 1767 loss: 7.80955452e-06
Iter: 1768 loss: 7.79612492e-06
Iter: 1769 loss: 7.79020775e-06
Iter: 1770 loss: 7.7917839e-06
Iter: 1771 loss: 7.78554477e-06
Iter: 1772 loss: 7.78080721e-06
Iter: 1773 loss: 7.84881559e-06
Iter: 1774 loss: 7.7808736e-06
Iter: 1775 loss: 7.77802234e-06
Iter: 1776 loss: 7.77806e-06
Iter: 1777 loss: 7.77446439e-06
Iter: 1778 loss: 7.78025424e-06
Iter: 1779 loss: 7.77344485e-06
Iter: 1780 loss: 7.76977413e-06
Iter: 1781 loss: 7.76922207e-06
Iter: 1782 loss: 7.76653906e-06
Iter: 1783 loss: 7.76353863e-06
Iter: 1784 loss: 7.75948047e-06
Iter: 1785 loss: 7.75938679e-06
Iter: 1786 loss: 7.75461558e-06
Iter: 1787 loss: 7.76624893e-06
Iter: 1788 loss: 7.75306671e-06
Iter: 1789 loss: 7.74768e-06
Iter: 1790 loss: 7.753717e-06
Iter: 1791 loss: 7.74500404e-06
Iter: 1792 loss: 7.73990723e-06
Iter: 1793 loss: 7.75855096e-06
Iter: 1794 loss: 7.73908e-06
Iter: 1795 loss: 7.73288593e-06
Iter: 1796 loss: 7.74341e-06
Iter: 1797 loss: 7.73082138e-06
Iter: 1798 loss: 7.72652493e-06
Iter: 1799 loss: 7.7738714e-06
Iter: 1800 loss: 7.72654403e-06
Iter: 1801 loss: 7.72338353e-06
Iter: 1802 loss: 7.72007297e-06
Iter: 1803 loss: 7.71972827e-06
Iter: 1804 loss: 7.71438772e-06
Iter: 1805 loss: 7.75188892e-06
Iter: 1806 loss: 7.71409395e-06
Iter: 1807 loss: 7.71281248e-06
Iter: 1808 loss: 7.71223677e-06
Iter: 1809 loss: 7.71089e-06
Iter: 1810 loss: 7.71628947e-06
Iter: 1811 loss: 7.71014766e-06
Iter: 1812 loss: 7.70841234e-06
Iter: 1813 loss: 7.708064e-06
Iter: 1814 loss: 7.70693441e-06
Iter: 1815 loss: 7.70484439e-06
Iter: 1816 loss: 7.70164843e-06
Iter: 1817 loss: 7.70158476e-06
Iter: 1818 loss: 7.69845155e-06
Iter: 1819 loss: 7.70620863e-06
Iter: 1820 loss: 7.69736562e-06
Iter: 1821 loss: 7.69347571e-06
Iter: 1822 loss: 7.69597773e-06
Iter: 1823 loss: 7.69092e-06
Iter: 1824 loss: 7.68718564e-06
Iter: 1825 loss: 7.71153282e-06
Iter: 1826 loss: 7.6867791e-06
Iter: 1827 loss: 7.68329664e-06
Iter: 1828 loss: 7.68455175e-06
Iter: 1829 loss: 7.68109203e-06
Iter: 1830 loss: 7.67671736e-06
Iter: 1831 loss: 7.70911e-06
Iter: 1832 loss: 7.67629172e-06
Iter: 1833 loss: 7.67378788e-06
Iter: 1834 loss: 7.67919391e-06
Iter: 1835 loss: 7.67265647e-06
Iter: 1836 loss: 7.6684737e-06
Iter: 1837 loss: 7.66839548e-06
Iter: 1838 loss: 7.6653123e-06
Iter: 1839 loss: 7.66553512e-06
Iter: 1840 loss: 7.6633587e-06
Iter: 1841 loss: 7.66204e-06
Iter: 1842 loss: 7.67538586e-06
Iter: 1843 loss: 7.66180437e-06
Iter: 1844 loss: 7.66062294e-06
Iter: 1845 loss: 7.65916138e-06
Iter: 1846 loss: 7.65916e-06
Iter: 1847 loss: 7.65620371e-06
Iter: 1848 loss: 7.65258119e-06
Iter: 1849 loss: 7.65235e-06
Iter: 1850 loss: 7.64899778e-06
Iter: 1851 loss: 7.6628894e-06
Iter: 1852 loss: 7.64848573e-06
Iter: 1853 loss: 7.64600645e-06
Iter: 1854 loss: 7.64670585e-06
Iter: 1855 loss: 7.64410652e-06
Iter: 1856 loss: 7.64005472e-06
Iter: 1857 loss: 7.65891764e-06
Iter: 1858 loss: 7.63966455e-06
Iter: 1859 loss: 7.63681419e-06
Iter: 1860 loss: 7.64100241e-06
Iter: 1861 loss: 7.63539447e-06
Iter: 1862 loss: 7.63227763e-06
Iter: 1863 loss: 7.646222e-06
Iter: 1864 loss: 7.63131811e-06
Iter: 1865 loss: 7.62907439e-06
Iter: 1866 loss: 7.64313882e-06
Iter: 1867 loss: 7.62884702e-06
Iter: 1868 loss: 7.62695799e-06
Iter: 1869 loss: 7.62607488e-06
Iter: 1870 loss: 7.62534182e-06
Iter: 1871 loss: 7.62346917e-06
Iter: 1872 loss: 7.6234619e-06
Iter: 1873 loss: 7.62233458e-06
Iter: 1874 loss: 7.62230911e-06
Iter: 1875 loss: 7.62176796e-06
Iter: 1876 loss: 7.62150466e-06
Iter: 1877 loss: 7.62103718e-06
Iter: 1878 loss: 7.62022773e-06
Iter: 1879 loss: 7.61840238e-06
Iter: 1880 loss: 7.6566339e-06
Iter: 1881 loss: 7.61865795e-06
Iter: 1882 loss: 7.61588353e-06
Iter: 1883 loss: 7.61842421e-06
Iter: 1884 loss: 7.61430874e-06
Iter: 1885 loss: 7.61213369e-06
Iter: 1886 loss: 7.6185197e-06
Iter: 1887 loss: 7.61123692e-06
Iter: 1888 loss: 7.60861349e-06
Iter: 1889 loss: 7.60952571e-06
Iter: 1890 loss: 7.60677722e-06
Iter: 1891 loss: 7.60386592e-06
Iter: 1892 loss: 7.63244861e-06
Iter: 1893 loss: 7.60385046e-06
Iter: 1894 loss: 7.6014594e-06
Iter: 1895 loss: 7.5995e-06
Iter: 1896 loss: 7.5988637e-06
Iter: 1897 loss: 7.59505656e-06
Iter: 1898 loss: 7.62958643e-06
Iter: 1899 loss: 7.59490285e-06
Iter: 1900 loss: 7.59300747e-06
Iter: 1901 loss: 7.59380328e-06
Iter: 1902 loss: 7.59089335e-06
Iter: 1903 loss: 7.58800888e-06
Iter: 1904 loss: 7.59708837e-06
Iter: 1905 loss: 7.58690112e-06
Iter: 1906 loss: 7.58845908e-06
Iter: 1907 loss: 7.58551278e-06
Iter: 1908 loss: 7.58486203e-06
Iter: 1909 loss: 7.58403121e-06
Iter: 1910 loss: 7.58363876e-06
Iter: 1911 loss: 7.58265196e-06
Iter: 1912 loss: 7.5801272e-06
Iter: 1913 loss: 7.63522439e-06
Iter: 1914 loss: 7.58023361e-06
Iter: 1915 loss: 7.57673297e-06
Iter: 1916 loss: 7.57793578e-06
Iter: 1917 loss: 7.57472935e-06
Iter: 1918 loss: 7.57134512e-06
Iter: 1919 loss: 7.58565056e-06
Iter: 1920 loss: 7.57073121e-06
Iter: 1921 loss: 7.56736972e-06
Iter: 1922 loss: 7.56730424e-06
Iter: 1923 loss: 7.56487316e-06
Iter: 1924 loss: 7.5612038e-06
Iter: 1925 loss: 7.59833802e-06
Iter: 1926 loss: 7.56084046e-06
Iter: 1927 loss: 7.55813153e-06
Iter: 1928 loss: 7.5597377e-06
Iter: 1929 loss: 7.55658539e-06
Iter: 1930 loss: 7.55298242e-06
Iter: 1931 loss: 7.56893041e-06
Iter: 1932 loss: 7.55234623e-06
Iter: 1933 loss: 7.54968733e-06
Iter: 1934 loss: 7.56073769e-06
Iter: 1935 loss: 7.54902794e-06
Iter: 1936 loss: 7.54638586e-06
Iter: 1937 loss: 7.54637631e-06
Iter: 1938 loss: 7.54374287e-06
Iter: 1939 loss: 7.54938901e-06
Iter: 1940 loss: 7.54280381e-06
Iter: 1941 loss: 7.54220946e-06
Iter: 1942 loss: 7.54122721e-06
Iter: 1943 loss: 7.5415619e-06
Iter: 1944 loss: 7.53965742e-06
Iter: 1945 loss: 7.53867062e-06
Iter: 1946 loss: 7.5384819e-06
Iter: 1947 loss: 7.53563836e-06
Iter: 1948 loss: 7.53342647e-06
Iter: 1949 loss: 7.53253289e-06
Iter: 1950 loss: 7.52868618e-06
Iter: 1951 loss: 7.54480379e-06
Iter: 1952 loss: 7.52776305e-06
Iter: 1953 loss: 7.52453889e-06
Iter: 1954 loss: 7.52564e-06
Iter: 1955 loss: 7.52190181e-06
Iter: 1956 loss: 7.51711332e-06
Iter: 1957 loss: 7.53957738e-06
Iter: 1958 loss: 7.5165276e-06
Iter: 1959 loss: 7.51276502e-06
Iter: 1960 loss: 7.5207472e-06
Iter: 1961 loss: 7.51218386e-06
Iter: 1962 loss: 7.50744357e-06
Iter: 1963 loss: 7.51349307e-06
Iter: 1964 loss: 7.50518848e-06
Iter: 1965 loss: 7.50112576e-06
Iter: 1966 loss: 7.52569713e-06
Iter: 1967 loss: 7.50069557e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.8
+ date
Mon Oct 26 19:10:03 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.8/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.8_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -1 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi2.8_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905f0a5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905f0a5ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905f148048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905f09d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905f09d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905f09d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905efcb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ef49bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ef49b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ef78620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ef05400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ef0d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ef0db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905eec9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ee70b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ee40400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ee48268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ede4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905edec9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905edb3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905edb6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ed84bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ed82510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ed51840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905eca9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ecdcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ecf5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ec36950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ec362f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ec7d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ec74268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ebe2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ebe20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ec1d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905ebdc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f905aa13488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.92793149e-05
Iter: 2 loss: 6.1960367e-05
Iter: 3 loss: 5.33994389e-05
Iter: 4 loss: 4.9345821e-05
Iter: 5 loss: 4.38268435e-05
Iter: 6 loss: 4.35527036e-05
Iter: 7 loss: 3.80589408e-05
Iter: 8 loss: 3.2737451e-05
Iter: 9 loss: 3.15450598e-05
Iter: 10 loss: 2.81363737e-05
Iter: 11 loss: 2.80706408e-05
Iter: 12 loss: 2.59358567e-05
Iter: 13 loss: 2.38855009e-05
Iter: 14 loss: 2.34088166e-05
Iter: 15 loss: 2.00650829e-05
Iter: 16 loss: 5.21474285e-05
Iter: 17 loss: 1.99385468e-05
Iter: 18 loss: 1.88160175e-05
Iter: 19 loss: 2.22828912e-05
Iter: 20 loss: 1.84876681e-05
Iter: 21 loss: 1.71452775e-05
Iter: 22 loss: 1.931519e-05
Iter: 23 loss: 1.65260208e-05
Iter: 24 loss: 1.58183502e-05
Iter: 25 loss: 2.06701661e-05
Iter: 26 loss: 1.5750491e-05
Iter: 27 loss: 1.50578007e-05
Iter: 28 loss: 1.50939577e-05
Iter: 29 loss: 1.45145e-05
Iter: 30 loss: 1.38405358e-05
Iter: 31 loss: 1.9897132e-05
Iter: 32 loss: 1.38076421e-05
Iter: 33 loss: 1.33223939e-05
Iter: 34 loss: 1.29194632e-05
Iter: 35 loss: 1.27800668e-05
Iter: 36 loss: 1.33378371e-05
Iter: 37 loss: 1.24728967e-05
Iter: 38 loss: 1.22979418e-05
Iter: 39 loss: 1.19945398e-05
Iter: 40 loss: 1.19941024e-05
Iter: 41 loss: 1.16779256e-05
Iter: 42 loss: 1.14900349e-05
Iter: 43 loss: 1.13583455e-05
Iter: 44 loss: 1.08259974e-05
Iter: 45 loss: 1.32868117e-05
Iter: 46 loss: 1.07265005e-05
Iter: 47 loss: 1.04103547e-05
Iter: 48 loss: 1.13701099e-05
Iter: 49 loss: 1.03159673e-05
Iter: 50 loss: 9.92746391e-06
Iter: 51 loss: 1.03360144e-05
Iter: 52 loss: 9.71305e-06
Iter: 53 loss: 9.45653301e-06
Iter: 54 loss: 9.45609554e-06
Iter: 55 loss: 9.27528163e-06
Iter: 56 loss: 9.04806e-06
Iter: 57 loss: 9.02996544e-06
Iter: 58 loss: 8.78321e-06
Iter: 59 loss: 1.2687342e-05
Iter: 60 loss: 8.78314313e-06
Iter: 61 loss: 8.65084439e-06
Iter: 62 loss: 8.51067125e-06
Iter: 63 loss: 8.486968e-06
Iter: 64 loss: 8.2671977e-06
Iter: 65 loss: 1.0224916e-05
Iter: 66 loss: 8.25665484e-06
Iter: 67 loss: 8.11888276e-06
Iter: 68 loss: 8.15406e-06
Iter: 69 loss: 8.01840906e-06
Iter: 70 loss: 8.11355767e-06
Iter: 71 loss: 7.95087726e-06
Iter: 72 loss: 7.89454862e-06
Iter: 73 loss: 7.78552567e-06
Iter: 74 loss: 1.00527195e-05
Iter: 75 loss: 7.78485446e-06
Iter: 76 loss: 7.70429324e-06
Iter: 77 loss: 7.56850795e-06
Iter: 78 loss: 7.5682683e-06
Iter: 79 loss: 7.47573768e-06
Iter: 80 loss: 7.46817113e-06
Iter: 81 loss: 7.39892403e-06
Iter: 82 loss: 7.28385e-06
Iter: 83 loss: 7.28334e-06
Iter: 84 loss: 7.11106213e-06
Iter: 85 loss: 8.29807595e-06
Iter: 86 loss: 7.0946453e-06
Iter: 87 loss: 7.01079534e-06
Iter: 88 loss: 7.07207e-06
Iter: 89 loss: 6.95939707e-06
Iter: 90 loss: 6.83413964e-06
Iter: 91 loss: 7.49672517e-06
Iter: 92 loss: 6.81499114e-06
Iter: 93 loss: 6.75667707e-06
Iter: 94 loss: 7.11588291e-06
Iter: 95 loss: 6.74986313e-06
Iter: 96 loss: 6.68985922e-06
Iter: 97 loss: 6.59211219e-06
Iter: 98 loss: 6.5915192e-06
Iter: 99 loss: 6.51172923e-06
Iter: 100 loss: 6.51067694e-06
Iter: 101 loss: 6.46108219e-06
Iter: 102 loss: 6.43064186e-06
Iter: 103 loss: 6.41038423e-06
Iter: 104 loss: 6.46157241e-06
Iter: 105 loss: 6.3824923e-06
Iter: 106 loss: 6.36463574e-06
Iter: 107 loss: 6.33707714e-06
Iter: 108 loss: 6.33662148e-06
Iter: 109 loss: 6.30857494e-06
Iter: 110 loss: 6.23468486e-06
Iter: 111 loss: 6.79195955e-06
Iter: 112 loss: 6.21988966e-06
Iter: 113 loss: 6.14212058e-06
Iter: 114 loss: 6.14208875e-06
Iter: 115 loss: 6.09505651e-06
Iter: 116 loss: 6.16590205e-06
Iter: 117 loss: 6.07255743e-06
Iter: 118 loss: 6.01464853e-06
Iter: 119 loss: 6.33851323e-06
Iter: 120 loss: 6.00686235e-06
Iter: 121 loss: 5.97516782e-06
Iter: 122 loss: 5.97492271e-06
Iter: 123 loss: 5.94968878e-06
Iter: 124 loss: 5.88993953e-06
Iter: 125 loss: 6.0160778e-06
Iter: 126 loss: 5.86621e-06
Iter: 127 loss: 5.82868688e-06
Iter: 128 loss: 5.97718144e-06
Iter: 129 loss: 5.82010625e-06
Iter: 130 loss: 5.77079027e-06
Iter: 131 loss: 5.7975185e-06
Iter: 132 loss: 5.73825537e-06
Iter: 133 loss: 5.69386839e-06
Iter: 134 loss: 6.02385262e-06
Iter: 135 loss: 5.69047461e-06
Iter: 136 loss: 5.65343635e-06
Iter: 137 loss: 5.61242086e-06
Iter: 138 loss: 5.6065428e-06
Iter: 139 loss: 5.56920486e-06
Iter: 140 loss: 5.56587838e-06
Iter: 141 loss: 5.56385157e-06
Iter: 142 loss: 5.55418092e-06
Iter: 143 loss: 5.54758117e-06
Iter: 144 loss: 5.52545043e-06
Iter: 145 loss: 5.48230037e-06
Iter: 146 loss: 5.48144681e-06
Iter: 147 loss: 5.44556588e-06
Iter: 148 loss: 5.44424802e-06
Iter: 149 loss: 5.41795589e-06
Iter: 150 loss: 5.40229394e-06
Iter: 151 loss: 5.39134362e-06
Iter: 152 loss: 5.35277923e-06
Iter: 153 loss: 5.67537427e-06
Iter: 154 loss: 5.35042545e-06
Iter: 155 loss: 5.32719059e-06
Iter: 156 loss: 5.32039576e-06
Iter: 157 loss: 5.30668922e-06
Iter: 158 loss: 5.26750227e-06
Iter: 159 loss: 5.60364742e-06
Iter: 160 loss: 5.26552867e-06
Iter: 161 loss: 5.24695315e-06
Iter: 162 loss: 5.28337569e-06
Iter: 163 loss: 5.239026e-06
Iter: 164 loss: 5.21019319e-06
Iter: 165 loss: 5.17508761e-06
Iter: 166 loss: 5.17181616e-06
Iter: 167 loss: 5.15222473e-06
Iter: 168 loss: 5.14969906e-06
Iter: 169 loss: 5.13155e-06
Iter: 170 loss: 5.09815254e-06
Iter: 171 loss: 5.89967431e-06
Iter: 172 loss: 5.09797064e-06
Iter: 173 loss: 5.0729609e-06
Iter: 174 loss: 5.07136338e-06
Iter: 175 loss: 5.06662263e-06
Iter: 176 loss: 5.06267816e-06
Iter: 177 loss: 5.05805929e-06
Iter: 178 loss: 5.04178934e-06
Iter: 179 loss: 5.02002149e-06
Iter: 180 loss: 5.01631621e-06
Iter: 181 loss: 4.99494536e-06
Iter: 182 loss: 4.99411544e-06
Iter: 183 loss: 4.97959809e-06
Iter: 184 loss: 4.95124e-06
Iter: 185 loss: 5.50221648e-06
Iter: 186 loss: 4.95102768e-06
Iter: 187 loss: 4.92947947e-06
Iter: 188 loss: 4.92708205e-06
Iter: 189 loss: 4.90858156e-06
Iter: 190 loss: 4.92686e-06
Iter: 191 loss: 4.89826971e-06
Iter: 192 loss: 4.87126363e-06
Iter: 193 loss: 5.0617582e-06
Iter: 194 loss: 4.86896124e-06
Iter: 195 loss: 4.84868269e-06
Iter: 196 loss: 4.9149503e-06
Iter: 197 loss: 4.84304428e-06
Iter: 198 loss: 4.8204015e-06
Iter: 199 loss: 4.87419311e-06
Iter: 200 loss: 4.8123311e-06
Iter: 201 loss: 4.79631353e-06
Iter: 202 loss: 4.89403192e-06
Iter: 203 loss: 4.79435676e-06
Iter: 204 loss: 4.779341e-06
Iter: 205 loss: 4.75649085e-06
Iter: 206 loss: 4.75619754e-06
Iter: 207 loss: 4.74467106e-06
Iter: 208 loss: 4.74185163e-06
Iter: 209 loss: 4.73543105e-06
Iter: 210 loss: 4.73501859e-06
Iter: 211 loss: 4.72753209e-06
Iter: 212 loss: 4.70603936e-06
Iter: 213 loss: 4.81801771e-06
Iter: 214 loss: 4.69926817e-06
Iter: 215 loss: 4.68307371e-06
Iter: 216 loss: 4.82450332e-06
Iter: 217 loss: 4.68227518e-06
Iter: 218 loss: 4.67339578e-06
Iter: 219 loss: 4.66327583e-06
Iter: 220 loss: 4.66197525e-06
Iter: 221 loss: 4.64397453e-06
Iter: 222 loss: 4.76418882e-06
Iter: 223 loss: 4.64235109e-06
Iter: 224 loss: 4.63107062e-06
Iter: 225 loss: 4.62345724e-06
Iter: 226 loss: 4.61906166e-06
Iter: 227 loss: 4.6013156e-06
Iter: 228 loss: 4.82550422e-06
Iter: 229 loss: 4.60126e-06
Iter: 230 loss: 4.59206512e-06
Iter: 231 loss: 4.57263377e-06
Iter: 232 loss: 4.87919533e-06
Iter: 233 loss: 4.57210581e-06
Iter: 234 loss: 4.5560696e-06
Iter: 235 loss: 4.55519967e-06
Iter: 236 loss: 4.54638e-06
Iter: 237 loss: 4.56436e-06
Iter: 238 loss: 4.54265091e-06
Iter: 239 loss: 4.53038865e-06
Iter: 240 loss: 4.52995118e-06
Iter: 241 loss: 4.52028689e-06
Iter: 242 loss: 4.50904554e-06
Iter: 243 loss: 4.63515744e-06
Iter: 244 loss: 4.50854168e-06
Iter: 245 loss: 4.50283733e-06
Iter: 246 loss: 4.50237894e-06
Iter: 247 loss: 4.49734671e-06
Iter: 248 loss: 4.48182254e-06
Iter: 249 loss: 4.52159748e-06
Iter: 250 loss: 4.47358e-06
Iter: 251 loss: 4.46194281e-06
Iter: 252 loss: 4.61742184e-06
Iter: 253 loss: 4.46202648e-06
Iter: 254 loss: 4.45351088e-06
Iter: 255 loss: 4.44642637e-06
Iter: 256 loss: 4.44386114e-06
Iter: 257 loss: 4.43067074e-06
Iter: 258 loss: 4.56554835e-06
Iter: 259 loss: 4.43016597e-06
Iter: 260 loss: 4.42293231e-06
Iter: 261 loss: 4.42131204e-06
Iter: 262 loss: 4.41677776e-06
Iter: 263 loss: 4.40274425e-06
Iter: 264 loss: 4.43109457e-06
Iter: 265 loss: 4.39727592e-06
Iter: 266 loss: 4.38657389e-06
Iter: 267 loss: 4.41759448e-06
Iter: 268 loss: 4.38341931e-06
Iter: 269 loss: 4.36952905e-06
Iter: 270 loss: 4.37522431e-06
Iter: 271 loss: 4.359591e-06
Iter: 272 loss: 4.35222682e-06
Iter: 273 loss: 4.35156835e-06
Iter: 274 loss: 4.34508911e-06
Iter: 275 loss: 4.33434298e-06
Iter: 276 loss: 4.33445211e-06
Iter: 277 loss: 4.32893967e-06
Iter: 278 loss: 4.32709658e-06
Iter: 279 loss: 4.32148136e-06
Iter: 280 loss: 4.37100653e-06
Iter: 281 loss: 4.32086063e-06
Iter: 282 loss: 4.3177015e-06
Iter: 283 loss: 4.30761065e-06
Iter: 284 loss: 4.3241771e-06
Iter: 285 loss: 4.30055843e-06
Iter: 286 loss: 4.29103693e-06
Iter: 287 loss: 4.29088459e-06
Iter: 288 loss: 4.28550538e-06
Iter: 289 loss: 4.27409395e-06
Iter: 290 loss: 4.45912065e-06
Iter: 291 loss: 4.27363648e-06
Iter: 292 loss: 4.26160113e-06
Iter: 293 loss: 4.26182805e-06
Iter: 294 loss: 4.25517737e-06
Iter: 295 loss: 4.25797953e-06
Iter: 296 loss: 4.25095959e-06
Iter: 297 loss: 4.2400211e-06
Iter: 298 loss: 4.26727956e-06
Iter: 299 loss: 4.23622168e-06
Iter: 300 loss: 4.22778794e-06
Iter: 301 loss: 4.25205553e-06
Iter: 302 loss: 4.2250831e-06
Iter: 303 loss: 4.21430286e-06
Iter: 304 loss: 4.24493828e-06
Iter: 305 loss: 4.21098321e-06
Iter: 306 loss: 4.20339074e-06
Iter: 307 loss: 4.23858455e-06
Iter: 308 loss: 4.20177275e-06
Iter: 309 loss: 4.19485787e-06
Iter: 310 loss: 4.19258e-06
Iter: 311 loss: 4.1883286e-06
Iter: 312 loss: 4.19269327e-06
Iter: 313 loss: 4.18402578e-06
Iter: 314 loss: 4.18131367e-06
Iter: 315 loss: 4.17853153e-06
Iter: 316 loss: 4.17820502e-06
Iter: 317 loss: 4.17527e-06
Iter: 318 loss: 4.16891089e-06
Iter: 319 loss: 4.28313069e-06
Iter: 320 loss: 4.16879e-06
Iter: 321 loss: 4.15894283e-06
Iter: 322 loss: 4.18190075e-06
Iter: 323 loss: 4.15521936e-06
Iter: 324 loss: 4.14892156e-06
Iter: 325 loss: 4.17646879e-06
Iter: 326 loss: 4.14755414e-06
Iter: 327 loss: 4.14022179e-06
Iter: 328 loss: 4.1419753e-06
Iter: 329 loss: 4.1347862e-06
Iter: 330 loss: 4.12880399e-06
Iter: 331 loss: 4.18703348e-06
Iter: 332 loss: 4.12856934e-06
Iter: 333 loss: 4.12266718e-06
Iter: 334 loss: 4.11738938e-06
Iter: 335 loss: 4.1158e-06
Iter: 336 loss: 4.10875327e-06
Iter: 337 loss: 4.18279615e-06
Iter: 338 loss: 4.10849316e-06
Iter: 339 loss: 4.10125813e-06
Iter: 340 loss: 4.10137091e-06
Iter: 341 loss: 4.0952832e-06
Iter: 342 loss: 4.08588585e-06
Iter: 343 loss: 4.15180421e-06
Iter: 344 loss: 4.08502274e-06
Iter: 345 loss: 4.0817331e-06
Iter: 346 loss: 4.08173764e-06
Iter: 347 loss: 4.07775815e-06
Iter: 348 loss: 4.0759187e-06
Iter: 349 loss: 4.07363405e-06
Iter: 350 loss: 4.06914933e-06
Iter: 351 loss: 4.06771051e-06
Iter: 352 loss: 4.06511617e-06
Iter: 353 loss: 4.06055096e-06
Iter: 354 loss: 4.07464267e-06
Iter: 355 loss: 4.05941591e-06
Iter: 356 loss: 4.05391665e-06
Iter: 357 loss: 4.046703e-06
Iter: 358 loss: 4.04621687e-06
Iter: 359 loss: 4.03892091e-06
Iter: 360 loss: 4.1322628e-06
Iter: 361 loss: 4.0388818e-06
Iter: 362 loss: 4.03259583e-06
Iter: 363 loss: 4.02338492e-06
Iter: 364 loss: 4.02326896e-06
Iter: 365 loss: 4.01529678e-06
Iter: 366 loss: 4.01533407e-06
Iter: 367 loss: 4.00960835e-06
Iter: 368 loss: 4.00073895e-06
Iter: 369 loss: 4.00064528e-06
Iter: 370 loss: 3.99391774e-06
Iter: 371 loss: 3.99331111e-06
Iter: 372 loss: 3.98898464e-06
Iter: 373 loss: 3.99272e-06
Iter: 374 loss: 3.98621432e-06
Iter: 375 loss: 3.98094289e-06
Iter: 376 loss: 4.01598663e-06
Iter: 377 loss: 3.98013481e-06
Iter: 378 loss: 3.97577742e-06
Iter: 379 loss: 4.02904107e-06
Iter: 380 loss: 3.97567919e-06
Iter: 381 loss: 3.97366375e-06
Iter: 382 loss: 3.9700335e-06
Iter: 383 loss: 4.05626679e-06
Iter: 384 loss: 3.97005442e-06
Iter: 385 loss: 3.96524e-06
Iter: 386 loss: 3.96440373e-06
Iter: 387 loss: 3.96139649e-06
Iter: 388 loss: 3.95536426e-06
Iter: 389 loss: 3.99494638e-06
Iter: 390 loss: 3.95464576e-06
Iter: 391 loss: 3.95017287e-06
Iter: 392 loss: 3.94630661e-06
Iter: 393 loss: 3.94506651e-06
Iter: 394 loss: 3.93697155e-06
Iter: 395 loss: 3.99100736e-06
Iter: 396 loss: 3.93619757e-06
Iter: 397 loss: 3.93203527e-06
Iter: 398 loss: 3.93867549e-06
Iter: 399 loss: 3.93035316e-06
Iter: 400 loss: 3.92471247e-06
Iter: 401 loss: 3.93422124e-06
Iter: 402 loss: 3.92194579e-06
Iter: 403 loss: 3.91700632e-06
Iter: 404 loss: 3.93503888e-06
Iter: 405 loss: 3.91557433e-06
Iter: 406 loss: 3.9092547e-06
Iter: 407 loss: 3.91408912e-06
Iter: 408 loss: 3.90542482e-06
Iter: 409 loss: 3.90268042e-06
Iter: 410 loss: 3.9023048e-06
Iter: 411 loss: 3.90001787e-06
Iter: 412 loss: 3.92771472e-06
Iter: 413 loss: 3.89983325e-06
Iter: 414 loss: 3.89801971e-06
Iter: 415 loss: 3.89304842e-06
Iter: 416 loss: 3.93194568e-06
Iter: 417 loss: 3.89189108e-06
Iter: 418 loss: 3.88537455e-06
Iter: 419 loss: 3.90410969e-06
Iter: 420 loss: 3.88304579e-06
Iter: 421 loss: 3.8787839e-06
Iter: 422 loss: 3.88915487e-06
Iter: 423 loss: 3.87719956e-06
Iter: 424 loss: 3.8711396e-06
Iter: 425 loss: 3.87725959e-06
Iter: 426 loss: 3.86774673e-06
Iter: 427 loss: 3.86365218e-06
Iter: 428 loss: 3.90542436e-06
Iter: 429 loss: 3.86329657e-06
Iter: 430 loss: 3.8598896e-06
Iter: 431 loss: 3.85538715e-06
Iter: 432 loss: 3.85526437e-06
Iter: 433 loss: 3.84934219e-06
Iter: 434 loss: 3.9045608e-06
Iter: 435 loss: 3.84929353e-06
Iter: 436 loss: 3.8450471e-06
Iter: 437 loss: 3.84452142e-06
Iter: 438 loss: 3.84147552e-06
Iter: 439 loss: 3.83659972e-06
Iter: 440 loss: 3.89565912e-06
Iter: 441 loss: 3.8362582e-06
Iter: 442 loss: 3.83337647e-06
Iter: 443 loss: 3.83230645e-06
Iter: 444 loss: 3.83043789e-06
Iter: 445 loss: 3.82738381e-06
Iter: 446 loss: 3.82684311e-06
Iter: 447 loss: 3.8243511e-06
Iter: 448 loss: 3.82170492e-06
Iter: 449 loss: 3.82121834e-06
Iter: 450 loss: 3.81863674e-06
Iter: 451 loss: 3.82130793e-06
Iter: 452 loss: 3.81713517e-06
Iter: 453 loss: 3.81366817e-06
Iter: 454 loss: 3.80983238e-06
Iter: 455 loss: 3.80913571e-06
Iter: 456 loss: 3.80523898e-06
Iter: 457 loss: 3.86820921e-06
Iter: 458 loss: 3.80522602e-06
Iter: 459 loss: 3.80219308e-06
Iter: 460 loss: 3.79983521e-06
Iter: 461 loss: 3.79860876e-06
Iter: 462 loss: 3.7930597e-06
Iter: 463 loss: 3.81869813e-06
Iter: 464 loss: 3.79200424e-06
Iter: 465 loss: 3.78864797e-06
Iter: 466 loss: 3.79624043e-06
Iter: 467 loss: 3.7871946e-06
Iter: 468 loss: 3.78286404e-06
Iter: 469 loss: 3.79057269e-06
Iter: 470 loss: 3.7810089e-06
Iter: 471 loss: 3.77757328e-06
Iter: 472 loss: 3.79278617e-06
Iter: 473 loss: 3.77699234e-06
Iter: 474 loss: 3.77254742e-06
Iter: 475 loss: 3.77362539e-06
Iter: 476 loss: 3.76954404e-06
Iter: 477 loss: 3.77429774e-06
Iter: 478 loss: 3.76828075e-06
Iter: 479 loss: 3.76725666e-06
Iter: 480 loss: 3.76561911e-06
Iter: 481 loss: 3.76560297e-06
Iter: 482 loss: 3.76325352e-06
Iter: 483 loss: 3.75976811e-06
Iter: 484 loss: 3.75967329e-06
Iter: 485 loss: 3.75564468e-06
Iter: 486 loss: 3.78434652e-06
Iter: 487 loss: 3.75515083e-06
Iter: 488 loss: 3.75227569e-06
Iter: 489 loss: 3.75056061e-06
Iter: 490 loss: 3.74947604e-06
Iter: 491 loss: 3.74384899e-06
Iter: 492 loss: 3.76530647e-06
Iter: 493 loss: 3.74264778e-06
Iter: 494 loss: 3.73965759e-06
Iter: 495 loss: 3.75544846e-06
Iter: 496 loss: 3.7391751e-06
Iter: 497 loss: 3.73558237e-06
Iter: 498 loss: 3.73366652e-06
Iter: 499 loss: 3.73231705e-06
Iter: 500 loss: 3.72823456e-06
Iter: 501 loss: 3.7773591e-06
Iter: 502 loss: 3.728416e-06
Iter: 503 loss: 3.72555655e-06
Iter: 504 loss: 3.72306408e-06
Iter: 505 loss: 3.72244722e-06
Iter: 506 loss: 3.71800729e-06
Iter: 507 loss: 3.7544587e-06
Iter: 508 loss: 3.7176535e-06
Iter: 509 loss: 3.71532633e-06
Iter: 510 loss: 3.72580439e-06
Iter: 511 loss: 3.7148609e-06
Iter: 512 loss: 3.71129909e-06
Iter: 513 loss: 3.72835871e-06
Iter: 514 loss: 3.71062606e-06
Iter: 515 loss: 3.70866405e-06
Iter: 516 loss: 3.7064824e-06
Iter: 517 loss: 3.70631324e-06
Iter: 518 loss: 3.70363523e-06
Iter: 519 loss: 3.70471435e-06
Iter: 520 loss: 3.7017162e-06
Iter: 521 loss: 3.69726035e-06
Iter: 522 loss: 3.70839371e-06
Iter: 523 loss: 3.69554391e-06
Iter: 524 loss: 3.69285976e-06
Iter: 525 loss: 3.70541511e-06
Iter: 526 loss: 3.69223562e-06
Iter: 527 loss: 3.68907308e-06
Iter: 528 loss: 3.68568271e-06
Iter: 529 loss: 3.6851261e-06
Iter: 530 loss: 3.68113388e-06
Iter: 531 loss: 3.68118117e-06
Iter: 532 loss: 3.67823668e-06
Iter: 533 loss: 3.67710254e-06
Iter: 534 loss: 3.67553503e-06
Iter: 535 loss: 3.67133202e-06
Iter: 536 loss: 3.69492363e-06
Iter: 537 loss: 3.67080565e-06
Iter: 538 loss: 3.66811764e-06
Iter: 539 loss: 3.66720951e-06
Iter: 540 loss: 3.66583799e-06
Iter: 541 loss: 3.66148561e-06
Iter: 542 loss: 3.68324845e-06
Iter: 543 loss: 3.66059976e-06
Iter: 544 loss: 3.66021686e-06
Iter: 545 loss: 3.65927394e-06
Iter: 546 loss: 3.65839901e-06
Iter: 547 loss: 3.65624919e-06
Iter: 548 loss: 3.67805501e-06
Iter: 549 loss: 3.6560009e-06
Iter: 550 loss: 3.65329788e-06
Iter: 551 loss: 3.652337e-06
Iter: 552 loss: 3.65052392e-06
Iter: 553 loss: 3.64806056e-06
Iter: 554 loss: 3.68913311e-06
Iter: 555 loss: 3.64819721e-06
Iter: 556 loss: 3.64635275e-06
Iter: 557 loss: 3.64404013e-06
Iter: 558 loss: 3.64390871e-06
Iter: 559 loss: 3.63998788e-06
Iter: 560 loss: 3.65110418e-06
Iter: 561 loss: 3.63872891e-06
Iter: 562 loss: 3.63571053e-06
Iter: 563 loss: 3.6468964e-06
Iter: 564 loss: 3.63478239e-06
Iter: 565 loss: 3.63092158e-06
Iter: 566 loss: 3.63549e-06
Iter: 567 loss: 3.62892911e-06
Iter: 568 loss: 3.62615901e-06
Iter: 569 loss: 3.64647167e-06
Iter: 570 loss: 3.62577816e-06
Iter: 571 loss: 3.62308083e-06
Iter: 572 loss: 3.61951788e-06
Iter: 573 loss: 3.61920047e-06
Iter: 574 loss: 3.61737125e-06
Iter: 575 loss: 3.6170793e-06
Iter: 576 loss: 3.61614593e-06
Iter: 577 loss: 3.61600041e-06
Iter: 578 loss: 3.61485763e-06
Iter: 579 loss: 3.61228763e-06
Iter: 580 loss: 3.63133449e-06
Iter: 581 loss: 3.61157868e-06
Iter: 582 loss: 3.60899207e-06
Iter: 583 loss: 3.62590936e-06
Iter: 584 loss: 3.60852232e-06
Iter: 585 loss: 3.60673448e-06
Iter: 586 loss: 3.60851459e-06
Iter: 587 loss: 3.60574791e-06
Iter: 588 loss: 3.6029835e-06
Iter: 589 loss: 3.60548438e-06
Iter: 590 loss: 3.60140939e-06
Iter: 591 loss: 3.59902106e-06
Iter: 592 loss: 3.61259072e-06
Iter: 593 loss: 3.59869227e-06
Iter: 594 loss: 3.59614523e-06
Iter: 595 loss: 3.59373962e-06
Iter: 596 loss: 3.59323985e-06
Iter: 597 loss: 3.5898961e-06
Iter: 598 loss: 3.64158018e-06
Iter: 599 loss: 3.58980787e-06
Iter: 600 loss: 3.58788839e-06
Iter: 601 loss: 3.58627244e-06
Iter: 602 loss: 3.58565671e-06
Iter: 603 loss: 3.58199782e-06
Iter: 604 loss: 3.60207332e-06
Iter: 605 loss: 3.58149e-06
Iter: 606 loss: 3.57929048e-06
Iter: 607 loss: 3.58255147e-06
Iter: 608 loss: 3.57836097e-06
Iter: 609 loss: 3.57678391e-06
Iter: 610 loss: 3.57665203e-06
Iter: 611 loss: 3.57468e-06
Iter: 612 loss: 3.5742587e-06
Iter: 613 loss: 3.57318413e-06
Iter: 614 loss: 3.5718549e-06
Iter: 615 loss: 3.57023691e-06
Iter: 616 loss: 3.57016643e-06
Iter: 617 loss: 3.56741612e-06
Iter: 618 loss: 3.57227736e-06
Iter: 619 loss: 3.56597593e-06
Iter: 620 loss: 3.56324449e-06
Iter: 621 loss: 3.59329192e-06
Iter: 622 loss: 3.5632595e-06
Iter: 623 loss: 3.56169312e-06
Iter: 624 loss: 3.55983e-06
Iter: 625 loss: 3.55972e-06
Iter: 626 loss: 3.5566477e-06
Iter: 627 loss: 3.57538556e-06
Iter: 628 loss: 3.55621751e-06
Iter: 629 loss: 3.55434804e-06
Iter: 630 loss: 3.55938619e-06
Iter: 631 loss: 3.55381599e-06
Iter: 632 loss: 3.55105226e-06
Iter: 633 loss: 3.54921303e-06
Iter: 634 loss: 3.54840586e-06
Iter: 635 loss: 3.54603571e-06
Iter: 636 loss: 3.54588587e-06
Iter: 637 loss: 3.54449048e-06
Iter: 638 loss: 3.54150734e-06
Iter: 639 loss: 3.54154531e-06
Iter: 640 loss: 3.54322219e-06
Iter: 641 loss: 3.54025519e-06
Iter: 642 loss: 3.53951191e-06
Iter: 643 loss: 3.54059603e-06
Iter: 644 loss: 3.53893029e-06
Iter: 645 loss: 3.5380383e-06
Iter: 646 loss: 3.53557903e-06
Iter: 647 loss: 3.54657095e-06
Iter: 648 loss: 3.53471751e-06
Iter: 649 loss: 3.5318858e-06
Iter: 650 loss: 3.56549504e-06
Iter: 651 loss: 3.53187465e-06
Iter: 652 loss: 3.53007636e-06
Iter: 653 loss: 3.54029089e-06
Iter: 654 loss: 3.52963752e-06
Iter: 655 loss: 3.52783513e-06
Iter: 656 loss: 3.52519191e-06
Iter: 657 loss: 3.52497773e-06
Iter: 658 loss: 3.52247457e-06
Iter: 659 loss: 3.55395764e-06
Iter: 660 loss: 3.5223884e-06
Iter: 661 loss: 3.52065217e-06
Iter: 662 loss: 3.52064671e-06
Iter: 663 loss: 3.51907897e-06
Iter: 664 loss: 3.5163132e-06
Iter: 665 loss: 3.53398127e-06
Iter: 666 loss: 3.51598374e-06
Iter: 667 loss: 3.51419249e-06
Iter: 668 loss: 3.51739527e-06
Iter: 669 loss: 3.51361723e-06
Iter: 670 loss: 3.5109997e-06
Iter: 671 loss: 3.51326821e-06
Iter: 672 loss: 3.50959954e-06
Iter: 673 loss: 3.50828941e-06
Iter: 674 loss: 3.50817959e-06
Iter: 675 loss: 3.50683285e-06
Iter: 676 loss: 3.51815538e-06
Iter: 677 loss: 3.50686173e-06
Iter: 678 loss: 3.50572418e-06
Iter: 679 loss: 3.50287519e-06
Iter: 680 loss: 3.51797689e-06
Iter: 681 loss: 3.50217715e-06
Iter: 682 loss: 3.49941092e-06
Iter: 683 loss: 3.50822484e-06
Iter: 684 loss: 3.49879292e-06
Iter: 685 loss: 3.49547281e-06
Iter: 686 loss: 3.50355322e-06
Iter: 687 loss: 3.49446486e-06
Iter: 688 loss: 3.49165657e-06
Iter: 689 loss: 3.51688959e-06
Iter: 690 loss: 3.49151651e-06
Iter: 691 loss: 3.49018524e-06
Iter: 692 loss: 3.48787e-06
Iter: 693 loss: 3.48787603e-06
Iter: 694 loss: 3.48432059e-06
Iter: 695 loss: 3.5038197e-06
Iter: 696 loss: 3.48383696e-06
Iter: 697 loss: 3.48141111e-06
Iter: 698 loss: 3.49166294e-06
Iter: 699 loss: 3.48098456e-06
Iter: 700 loss: 3.47865125e-06
Iter: 701 loss: 3.47900504e-06
Iter: 702 loss: 3.47680634e-06
Iter: 703 loss: 3.47453783e-06
Iter: 704 loss: 3.4993991e-06
Iter: 705 loss: 3.47449986e-06
Iter: 706 loss: 3.47273249e-06
Iter: 707 loss: 3.47215087e-06
Iter: 708 loss: 3.47129935e-06
Iter: 709 loss: 3.47147488e-06
Iter: 710 loss: 3.4699051e-06
Iter: 711 loss: 3.4692489e-06
Iter: 712 loss: 3.46800311e-06
Iter: 713 loss: 3.49983475e-06
Iter: 714 loss: 3.46792285e-06
Iter: 715 loss: 3.46675188e-06
Iter: 716 loss: 3.46416482e-06
Iter: 717 loss: 3.50032064e-06
Iter: 718 loss: 3.46402499e-06
Iter: 719 loss: 3.46104025e-06
Iter: 720 loss: 3.49778156e-06
Iter: 721 loss: 3.46102343e-06
Iter: 722 loss: 3.45900435e-06
Iter: 723 loss: 3.46362549e-06
Iter: 724 loss: 3.45839135e-06
Iter: 725 loss: 3.45580474e-06
Iter: 726 loss: 3.45967806e-06
Iter: 727 loss: 3.45446733e-06
Iter: 728 loss: 3.45254e-06
Iter: 729 loss: 3.46014531e-06
Iter: 730 loss: 3.4521313e-06
Iter: 731 loss: 3.44976547e-06
Iter: 732 loss: 3.44877344e-06
Iter: 733 loss: 3.44768569e-06
Iter: 734 loss: 3.44576347e-06
Iter: 735 loss: 3.44579598e-06
Iter: 736 loss: 3.4442678e-06
Iter: 737 loss: 3.4427062e-06
Iter: 738 loss: 3.44253181e-06
Iter: 739 loss: 3.43945294e-06
Iter: 740 loss: 3.45341368e-06
Iter: 741 loss: 3.43875968e-06
Iter: 742 loss: 3.43875627e-06
Iter: 743 loss: 3.43792726e-06
Iter: 744 loss: 3.43712259e-06
Iter: 745 loss: 3.43626152e-06
Iter: 746 loss: 3.4362331e-06
Iter: 747 loss: 3.43497231e-06
Iter: 748 loss: 3.43316924e-06
Iter: 749 loss: 3.43309648e-06
Iter: 750 loss: 3.43130478e-06
Iter: 751 loss: 3.43558213e-06
Iter: 752 loss: 3.43080842e-06
Iter: 753 loss: 3.42845146e-06
Iter: 754 loss: 3.43449346e-06
Iter: 755 loss: 3.4276818e-06
Iter: 756 loss: 3.42628755e-06
Iter: 757 loss: 3.44111186e-06
Iter: 758 loss: 3.42607495e-06
Iter: 759 loss: 3.42491785e-06
Iter: 760 loss: 3.4239556e-06
Iter: 761 loss: 3.42349e-06
Iter: 762 loss: 3.42122667e-06
Iter: 763 loss: 3.42901922e-06
Iter: 764 loss: 3.42058729e-06
Iter: 765 loss: 3.41877626e-06
Iter: 766 loss: 3.4181362e-06
Iter: 767 loss: 3.41723444e-06
Iter: 768 loss: 3.41474924e-06
Iter: 769 loss: 3.44055093e-06
Iter: 770 loss: 3.41469786e-06
Iter: 771 loss: 3.41348095e-06
Iter: 772 loss: 3.41343593e-06
Iter: 773 loss: 3.41211125e-06
Iter: 774 loss: 3.41062855e-06
Iter: 775 loss: 3.43608281e-06
Iter: 776 loss: 3.41049849e-06
Iter: 777 loss: 3.4092709e-06
Iter: 778 loss: 3.41429904e-06
Iter: 779 loss: 3.40871748e-06
Iter: 780 loss: 3.40773136e-06
Iter: 781 loss: 3.40672409e-06
Iter: 782 loss: 3.40650377e-06
Iter: 783 loss: 3.40498627e-06
Iter: 784 loss: 3.40315501e-06
Iter: 785 loss: 3.40271208e-06
Iter: 786 loss: 3.40013662e-06
Iter: 787 loss: 3.42670319e-06
Iter: 788 loss: 3.40004908e-06
Iter: 789 loss: 3.39875305e-06
Iter: 790 loss: 3.3981355e-06
Iter: 791 loss: 3.39737312e-06
Iter: 792 loss: 3.39482449e-06
Iter: 793 loss: 3.41444138e-06
Iter: 794 loss: 3.39468124e-06
Iter: 795 loss: 3.3931949e-06
Iter: 796 loss: 3.39788153e-06
Iter: 797 loss: 3.39252347e-06
Iter: 798 loss: 3.39100393e-06
Iter: 799 loss: 3.38835889e-06
Iter: 800 loss: 3.38842233e-06
Iter: 801 loss: 3.38640348e-06
Iter: 802 loss: 3.38636437e-06
Iter: 803 loss: 3.38443306e-06
Iter: 804 loss: 3.38285463e-06
Iter: 805 loss: 3.38229461e-06
Iter: 806 loss: 3.38190557e-06
Iter: 807 loss: 3.3811848e-06
Iter: 808 loss: 3.38039013e-06
Iter: 809 loss: 3.38727114e-06
Iter: 810 loss: 3.38027962e-06
Iter: 811 loss: 3.37946949e-06
Iter: 812 loss: 3.37787924e-06
Iter: 813 loss: 3.40157771e-06
Iter: 814 loss: 3.37780648e-06
Iter: 815 loss: 3.37647134e-06
Iter: 816 loss: 3.38432574e-06
Iter: 817 loss: 3.37623305e-06
Iter: 818 loss: 3.37510755e-06
Iter: 819 loss: 3.37357187e-06
Iter: 820 loss: 3.37349934e-06
Iter: 821 loss: 3.37092342e-06
Iter: 822 loss: 3.38744326e-06
Iter: 823 loss: 3.37060646e-06
Iter: 824 loss: 3.36936728e-06
Iter: 825 loss: 3.37363122e-06
Iter: 826 loss: 3.36880294e-06
Iter: 827 loss: 3.36716266e-06
Iter: 828 loss: 3.36953963e-06
Iter: 829 loss: 3.36634844e-06
Iter: 830 loss: 3.36477979e-06
Iter: 831 loss: 3.37636357e-06
Iter: 832 loss: 3.36459152e-06
Iter: 833 loss: 3.36341918e-06
Iter: 834 loss: 3.36133462e-06
Iter: 835 loss: 3.40637189e-06
Iter: 836 loss: 3.3613444e-06
Iter: 837 loss: 3.35853429e-06
Iter: 838 loss: 3.38003065e-06
Iter: 839 loss: 3.35811546e-06
Iter: 840 loss: 3.35662821e-06
Iter: 841 loss: 3.3571464e-06
Iter: 842 loss: 3.35532422e-06
Iter: 843 loss: 3.35443247e-06
Iter: 844 loss: 3.35392269e-06
Iter: 845 loss: 3.35246546e-06
Iter: 846 loss: 3.35506911e-06
Iter: 847 loss: 3.35188861e-06
Iter: 848 loss: 3.35121604e-06
Iter: 849 loss: 3.34962942e-06
Iter: 850 loss: 3.3704714e-06
Iter: 851 loss: 3.34956985e-06
Iter: 852 loss: 3.34741571e-06
Iter: 853 loss: 3.35277673e-06
Iter: 854 loss: 3.34667766e-06
Iter: 855 loss: 3.34508877e-06
Iter: 856 loss: 3.34585275e-06
Iter: 857 loss: 3.34399738e-06
Iter: 858 loss: 3.34141964e-06
Iter: 859 loss: 3.34985771e-06
Iter: 860 loss: 3.34079186e-06
Iter: 861 loss: 3.33931348e-06
Iter: 862 loss: 3.35535628e-06
Iter: 863 loss: 3.33926891e-06
Iter: 864 loss: 3.33792286e-06
Iter: 865 loss: 3.33723528e-06
Iter: 866 loss: 3.33683374e-06
Iter: 867 loss: 3.33496428e-06
Iter: 868 loss: 3.35085224e-06
Iter: 869 loss: 3.33479784e-06
Iter: 870 loss: 3.33351363e-06
Iter: 871 loss: 3.33179287e-06
Iter: 872 loss: 3.33171101e-06
Iter: 873 loss: 3.32887817e-06
Iter: 874 loss: 3.34522815e-06
Iter: 875 loss: 3.32845207e-06
Iter: 876 loss: 3.32692116e-06
Iter: 877 loss: 3.33220351e-06
Iter: 878 loss: 3.32651416e-06
Iter: 879 loss: 3.32609352e-06
Iter: 880 loss: 3.32556942e-06
Iter: 881 loss: 3.3249496e-06
Iter: 882 loss: 3.32296213e-06
Iter: 883 loss: 3.34141828e-06
Iter: 884 loss: 3.32277295e-06
Iter: 885 loss: 3.3216877e-06
Iter: 886 loss: 3.32331342e-06
Iter: 887 loss: 3.32096852e-06
Iter: 888 loss: 3.31910769e-06
Iter: 889 loss: 3.31964179e-06
Iter: 890 loss: 3.31773936e-06
Iter: 891 loss: 3.31619276e-06
Iter: 892 loss: 3.3253832e-06
Iter: 893 loss: 3.31596971e-06
Iter: 894 loss: 3.31433603e-06
Iter: 895 loss: 3.31419278e-06
Iter: 896 loss: 3.31312117e-06
Iter: 897 loss: 3.3122169e-06
Iter: 898 loss: 3.3122e-06
Iter: 899 loss: 3.31127922e-06
Iter: 900 loss: 3.30957482e-06
Iter: 901 loss: 3.30963735e-06
Iter: 902 loss: 3.30768853e-06
Iter: 903 loss: 3.32359673e-06
Iter: 904 loss: 3.30757643e-06
Iter: 905 loss: 3.3062197e-06
Iter: 906 loss: 3.30575449e-06
Iter: 907 loss: 3.30495027e-06
Iter: 908 loss: 3.30240528e-06
Iter: 909 loss: 3.30969624e-06
Iter: 910 loss: 3.30161583e-06
Iter: 911 loss: 3.30030707e-06
Iter: 912 loss: 3.30859029e-06
Iter: 913 loss: 3.30017656e-06
Iter: 914 loss: 3.29906288e-06
Iter: 915 loss: 3.29892464e-06
Iter: 916 loss: 3.2982839e-06
Iter: 917 loss: 3.2968303e-06
Iter: 918 loss: 3.31274327e-06
Iter: 919 loss: 3.29662748e-06
Iter: 920 loss: 3.29537215e-06
Iter: 921 loss: 3.29744944e-06
Iter: 922 loss: 3.29467207e-06
Iter: 923 loss: 3.29282057e-06
Iter: 924 loss: 3.29472618e-06
Iter: 925 loss: 3.29171417e-06
Iter: 926 loss: 3.28993337e-06
Iter: 927 loss: 3.30680132e-06
Iter: 928 loss: 3.28999886e-06
Iter: 929 loss: 3.28880265e-06
Iter: 930 loss: 3.28746523e-06
Iter: 931 loss: 3.28725719e-06
Iter: 932 loss: 3.2856592e-06
Iter: 933 loss: 3.30889907e-06
Iter: 934 loss: 3.28554734e-06
Iter: 935 loss: 3.2844257e-06
Iter: 936 loss: 3.28569308e-06
Iter: 937 loss: 3.28394208e-06
Iter: 938 loss: 3.28208807e-06
Iter: 939 loss: 3.28177953e-06
Iter: 940 loss: 3.28044757e-06
Iter: 941 loss: 3.27884118e-06
Iter: 942 loss: 3.28457895e-06
Iter: 943 loss: 3.27837984e-06
Iter: 944 loss: 3.27648013e-06
Iter: 945 loss: 3.27863518e-06
Iter: 946 loss: 3.27524731e-06
Iter: 947 loss: 3.27423459e-06
Iter: 948 loss: 3.27421685e-06
Iter: 949 loss: 3.27293719e-06
Iter: 950 loss: 3.27645193e-06
Iter: 951 loss: 3.27246062e-06
Iter: 952 loss: 3.27151e-06
Iter: 953 loss: 3.26930467e-06
Iter: 954 loss: 3.30101147e-06
Iter: 955 loss: 3.2691255e-06
Iter: 956 loss: 3.26781469e-06
Iter: 957 loss: 3.2770663e-06
Iter: 958 loss: 3.26752161e-06
Iter: 959 loss: 3.26591839e-06
Iter: 960 loss: 3.26478312e-06
Iter: 961 loss: 3.26438976e-06
Iter: 962 loss: 3.26279e-06
Iter: 963 loss: 3.26265786e-06
Iter: 964 loss: 3.26134978e-06
Iter: 965 loss: 3.25947622e-06
Iter: 966 loss: 3.25958104e-06
Iter: 967 loss: 3.25750352e-06
Iter: 968 loss: 3.28466558e-06
Iter: 969 loss: 3.25755241e-06
Iter: 970 loss: 3.25624069e-06
Iter: 971 loss: 3.2541966e-06
Iter: 972 loss: 3.25408269e-06
Iter: 973 loss: 3.25178303e-06
Iter: 974 loss: 3.25180054e-06
Iter: 975 loss: 3.25076417e-06
Iter: 976 loss: 3.25336214e-06
Iter: 977 loss: 3.25032261e-06
Iter: 978 loss: 3.24889311e-06
Iter: 979 loss: 3.24852317e-06
Iter: 980 loss: 3.24774328e-06
Iter: 981 loss: 3.2483e-06
Iter: 982 loss: 3.24710663e-06
Iter: 983 loss: 3.24658458e-06
Iter: 984 loss: 3.24521875e-06
Iter: 985 loss: 3.25701785e-06
Iter: 986 loss: 3.2450414e-06
Iter: 987 loss: 3.24371649e-06
Iter: 988 loss: 3.24423036e-06
Iter: 989 loss: 3.24273515e-06
Iter: 990 loss: 3.24132952e-06
Iter: 991 loss: 3.25174233e-06
Iter: 992 loss: 3.24110647e-06
Iter: 993 loss: 3.24009602e-06
Iter: 994 loss: 3.23857739e-06
Iter: 995 loss: 3.23843983e-06
Iter: 996 loss: 3.23679205e-06
Iter: 997 loss: 3.25501674e-06
Iter: 998 loss: 3.23692825e-06
Iter: 999 loss: 3.23544396e-06
Iter: 1000 loss: 3.23339191e-06
Iter: 1001 loss: 3.23336326e-06
Iter: 1002 loss: 3.23216955e-06
Iter: 1003 loss: 3.2317862e-06
Iter: 1004 loss: 3.23093946e-06
Iter: 1005 loss: 3.22920505e-06
Iter: 1006 loss: 3.22922665e-06
Iter: 1007 loss: 3.22716778e-06
Iter: 1008 loss: 3.24323446e-06
Iter: 1009 loss: 3.22719166e-06
Iter: 1010 loss: 3.22587039e-06
Iter: 1011 loss: 3.22562187e-06
Iter: 1012 loss: 3.22476e-06
Iter: 1013 loss: 3.22441269e-06
Iter: 1014 loss: 3.22375104e-06
Iter: 1015 loss: 3.22306119e-06
Iter: 1016 loss: 3.22514734e-06
Iter: 1017 loss: 3.22264054e-06
Iter: 1018 loss: 3.22220649e-06
Iter: 1019 loss: 3.22146661e-06
Iter: 1020 loss: 3.23777977e-06
Iter: 1021 loss: 3.22165988e-06
Iter: 1022 loss: 3.22031178e-06
Iter: 1023 loss: 3.22122742e-06
Iter: 1024 loss: 3.21941889e-06
Iter: 1025 loss: 3.21808557e-06
Iter: 1026 loss: 3.22078245e-06
Iter: 1027 loss: 3.21757875e-06
Iter: 1028 loss: 3.21573179e-06
Iter: 1029 loss: 3.21811967e-06
Iter: 1030 loss: 3.2148705e-06
Iter: 1031 loss: 3.21325092e-06
Iter: 1032 loss: 3.22457254e-06
Iter: 1033 loss: 3.21292191e-06
Iter: 1034 loss: 3.21151174e-06
Iter: 1035 loss: 3.21305743e-06
Iter: 1036 loss: 3.21061043e-06
Iter: 1037 loss: 3.2092787e-06
Iter: 1038 loss: 3.22325377e-06
Iter: 1039 loss: 3.20942536e-06
Iter: 1040 loss: 3.20837853e-06
Iter: 1041 loss: 3.2066e-06
Iter: 1042 loss: 3.20656045e-06
Iter: 1043 loss: 3.20516756e-06
Iter: 1044 loss: 3.20520849e-06
Iter: 1045 loss: 3.20422578e-06
Iter: 1046 loss: 3.20916752e-06
Iter: 1047 loss: 3.2040266e-06
Iter: 1048 loss: 3.20317463e-06
Iter: 1049 loss: 3.2112996e-06
Iter: 1050 loss: 3.20294043e-06
Iter: 1051 loss: 3.20229128e-06
Iter: 1052 loss: 3.20136837e-06
Iter: 1053 loss: 3.20133836e-06
Iter: 1054 loss: 3.20046502e-06
Iter: 1055 loss: 3.20081654e-06
Iter: 1056 loss: 3.19968763e-06
Iter: 1057 loss: 3.19813739e-06
Iter: 1058 loss: 3.20080858e-06
Iter: 1059 loss: 3.19754213e-06
Iter: 1060 loss: 3.19606147e-06
Iter: 1061 loss: 3.19837272e-06
Iter: 1062 loss: 3.19555784e-06
Iter: 1063 loss: 3.19357059e-06
Iter: 1064 loss: 3.19687024e-06
Iter: 1065 loss: 3.1928389e-06
Iter: 1066 loss: 3.19158198e-06
Iter: 1067 loss: 3.19431228e-06
Iter: 1068 loss: 3.19107085e-06
Iter: 1069 loss: 3.18954721e-06
Iter: 1070 loss: 3.19400556e-06
Iter: 1071 loss: 3.18889442e-06
Iter: 1072 loss: 3.18747675e-06
Iter: 1073 loss: 3.19041169e-06
Iter: 1074 loss: 3.18710499e-06
Iter: 1075 loss: 3.1852428e-06
Iter: 1076 loss: 3.19094897e-06
Iter: 1077 loss: 3.18478715e-06
Iter: 1078 loss: 3.1835973e-06
Iter: 1079 loss: 3.19090418e-06
Iter: 1080 loss: 3.18365369e-06
Iter: 1081 loss: 3.18271282e-06
Iter: 1082 loss: 3.18265438e-06
Iter: 1083 loss: 3.18194088e-06
Iter: 1084 loss: 3.18095863e-06
Iter: 1085 loss: 3.18094499e-06
Iter: 1086 loss: 3.18007051e-06
Iter: 1087 loss: 3.18057528e-06
Iter: 1088 loss: 3.17932336e-06
Iter: 1089 loss: 3.17793229e-06
Iter: 1090 loss: 3.17798913e-06
Iter: 1091 loss: 3.17692457e-06
Iter: 1092 loss: 3.17554895e-06
Iter: 1093 loss: 3.19317405e-06
Iter: 1094 loss: 3.17542845e-06
Iter: 1095 loss: 3.17428385e-06
Iter: 1096 loss: 3.17353124e-06
Iter: 1097 loss: 3.17337344e-06
Iter: 1098 loss: 3.17198828e-06
Iter: 1099 loss: 3.18493267e-06
Iter: 1100 loss: 3.17199078e-06
Iter: 1101 loss: 3.1709651e-06
Iter: 1102 loss: 3.16971091e-06
Iter: 1103 loss: 3.16959972e-06
Iter: 1104 loss: 3.16798628e-06
Iter: 1105 loss: 3.19306923e-06
Iter: 1106 loss: 3.16799424e-06
Iter: 1107 loss: 3.16691603e-06
Iter: 1108 loss: 3.16616342e-06
Iter: 1109 loss: 3.16580144e-06
Iter: 1110 loss: 3.16404839e-06
Iter: 1111 loss: 3.17245963e-06
Iter: 1112 loss: 3.16379851e-06
Iter: 1113 loss: 3.16319802e-06
Iter: 1114 loss: 3.16317755e-06
Iter: 1115 loss: 3.16228989e-06
Iter: 1116 loss: 3.16107321e-06
Iter: 1117 loss: 3.16105502e-06
Iter: 1118 loss: 3.1601403e-06
Iter: 1119 loss: 3.16056116e-06
Iter: 1120 loss: 3.15929196e-06
Iter: 1121 loss: 3.15834404e-06
Iter: 1122 loss: 3.15847433e-06
Iter: 1123 loss: 3.15747957e-06
Iter: 1124 loss: 3.15582201e-06
Iter: 1125 loss: 3.16483329e-06
Iter: 1126 loss: 3.15554144e-06
Iter: 1127 loss: 3.15442958e-06
Iter: 1128 loss: 3.15809461e-06
Iter: 1129 loss: 3.15412353e-06
Iter: 1130 loss: 3.15296256e-06
Iter: 1131 loss: 3.15190732e-06
Iter: 1132 loss: 3.15147122e-06
Iter: 1133 loss: 3.14969338e-06
Iter: 1134 loss: 3.15580064e-06
Iter: 1135 loss: 3.14906902e-06
Iter: 1136 loss: 3.14706358e-06
Iter: 1137 loss: 3.15060697e-06
Iter: 1138 loss: 3.14586396e-06
Iter: 1139 loss: 3.14447607e-06
Iter: 1140 loss: 3.16713408e-06
Iter: 1141 loss: 3.14457861e-06
Iter: 1142 loss: 3.14330509e-06
Iter: 1143 loss: 3.14195677e-06
Iter: 1144 loss: 3.1416796e-06
Iter: 1145 loss: 3.14118665e-06
Iter: 1146 loss: 3.14082627e-06
Iter: 1147 loss: 3.14050158e-06
Iter: 1148 loss: 3.14039971e-06
Iter: 1149 loss: 3.14015961e-06
Iter: 1150 loss: 3.13908095e-06
Iter: 1151 loss: 3.14273939e-06
Iter: 1152 loss: 3.13857345e-06
Iter: 1153 loss: 3.13753571e-06
Iter: 1154 loss: 3.1432412e-06
Iter: 1155 loss: 3.13734813e-06
Iter: 1156 loss: 3.13619807e-06
Iter: 1157 loss: 3.13539772e-06
Iter: 1158 loss: 3.13507849e-06
Iter: 1159 loss: 3.13365331e-06
Iter: 1160 loss: 3.14579165e-06
Iter: 1161 loss: 3.13368946e-06
Iter: 1162 loss: 3.13260534e-06
Iter: 1163 loss: 3.13219334e-06
Iter: 1164 loss: 3.13155897e-06
Iter: 1165 loss: 3.13001397e-06
Iter: 1166 loss: 3.14579597e-06
Iter: 1167 loss: 3.12986867e-06
Iter: 1168 loss: 3.12889733e-06
Iter: 1169 loss: 3.12822567e-06
Iter: 1170 loss: 3.12783436e-06
Iter: 1171 loss: 3.12636462e-06
Iter: 1172 loss: 3.13142255e-06
Iter: 1173 loss: 3.1259367e-06
Iter: 1174 loss: 3.12477596e-06
Iter: 1175 loss: 3.12616612e-06
Iter: 1176 loss: 3.12405768e-06
Iter: 1177 loss: 3.12250017e-06
Iter: 1178 loss: 3.1232662e-06
Iter: 1179 loss: 3.12134716e-06
Iter: 1180 loss: 3.12169141e-06
Iter: 1181 loss: 3.12077395e-06
Iter: 1182 loss: 3.12007751e-06
Iter: 1183 loss: 3.11994086e-06
Iter: 1184 loss: 3.11955773e-06
Iter: 1185 loss: 3.11888789e-06
Iter: 1186 loss: 3.11699887e-06
Iter: 1187 loss: 3.14103e-06
Iter: 1188 loss: 3.11684971e-06
Iter: 1189 loss: 3.11539202e-06
Iter: 1190 loss: 3.12187558e-06
Iter: 1191 loss: 3.11505528e-06
Iter: 1192 loss: 3.11356825e-06
Iter: 1193 loss: 3.12337352e-06
Iter: 1194 loss: 3.11344093e-06
Iter: 1195 loss: 3.11254712e-06
Iter: 1196 loss: 3.11228723e-06
Iter: 1197 loss: 3.11158328e-06
Iter: 1198 loss: 3.10991936e-06
Iter: 1199 loss: 3.11713256e-06
Iter: 1200 loss: 3.10968767e-06
Iter: 1201 loss: 3.10850692e-06
Iter: 1202 loss: 3.1143652e-06
Iter: 1203 loss: 3.10839778e-06
Iter: 1204 loss: 3.10724909e-06
Iter: 1205 loss: 3.10563428e-06
Iter: 1206 loss: 3.10566861e-06
Iter: 1207 loss: 3.10407199e-06
Iter: 1208 loss: 3.12508428e-06
Iter: 1209 loss: 3.10407677e-06
Iter: 1210 loss: 3.1028485e-06
Iter: 1211 loss: 3.10450855e-06
Iter: 1212 loss: 3.10228279e-06
Iter: 1213 loss: 3.10117e-06
Iter: 1214 loss: 3.11842041e-06
Iter: 1215 loss: 3.1011939e-06
Iter: 1216 loss: 3.1007994e-06
Iter: 1217 loss: 3.10068708e-06
Iter: 1218 loss: 3.10041514e-06
Iter: 1219 loss: 3.09948791e-06
Iter: 1220 loss: 3.09854545e-06
Iter: 1221 loss: 3.0981173e-06
Iter: 1222 loss: 3.09690586e-06
Iter: 1223 loss: 3.09692655e-06
Iter: 1224 loss: 3.09602524e-06
Iter: 1225 loss: 3.09529878e-06
Iter: 1226 loss: 3.09504389e-06
Iter: 1227 loss: 3.09381767e-06
Iter: 1228 loss: 3.10764153e-06
Iter: 1229 loss: 3.09384359e-06
Iter: 1230 loss: 3.09285792e-06
Iter: 1231 loss: 3.09375605e-06
Iter: 1232 loss: 3.09243933e-06
Iter: 1233 loss: 3.09102097e-06
Iter: 1234 loss: 3.09397956e-06
Iter: 1235 loss: 3.09032453e-06
Iter: 1236 loss: 3.08935114e-06
Iter: 1237 loss: 3.0890169e-06
Iter: 1238 loss: 3.08850122e-06
Iter: 1239 loss: 3.08667222e-06
Iter: 1240 loss: 3.09895177e-06
Iter: 1241 loss: 3.08641847e-06
Iter: 1242 loss: 3.08554718e-06
Iter: 1243 loss: 3.0849651e-06
Iter: 1244 loss: 3.08456174e-06
Iter: 1245 loss: 3.08285189e-06
Iter: 1246 loss: 3.09208417e-06
Iter: 1247 loss: 3.08249741e-06
Iter: 1248 loss: 3.08531435e-06
Iter: 1249 loss: 3.08206e-06
Iter: 1250 loss: 3.08179688e-06
Iter: 1251 loss: 3.08103631e-06
Iter: 1252 loss: 3.08186668e-06
Iter: 1253 loss: 3.08047902e-06
Iter: 1254 loss: 3.07885603e-06
Iter: 1255 loss: 3.08322137e-06
Iter: 1256 loss: 3.0782403e-06
Iter: 1257 loss: 3.07734467e-06
Iter: 1258 loss: 3.08455924e-06
Iter: 1259 loss: 3.07735149e-06
Iter: 1260 loss: 3.07654091e-06
Iter: 1261 loss: 3.07553137e-06
Iter: 1262 loss: 3.0755532e-06
Iter: 1263 loss: 3.07456e-06
Iter: 1264 loss: 3.08565768e-06
Iter: 1265 loss: 3.07459823e-06
Iter: 1266 loss: 3.07369055e-06
Iter: 1267 loss: 3.07281493e-06
Iter: 1268 loss: 3.07255459e-06
Iter: 1269 loss: 3.07144592e-06
Iter: 1270 loss: 3.08867266e-06
Iter: 1271 loss: 3.07133769e-06
Iter: 1272 loss: 3.07034838e-06
Iter: 1273 loss: 3.06901165e-06
Iter: 1274 loss: 3.06899892e-06
Iter: 1275 loss: 3.067742e-06
Iter: 1276 loss: 3.06783409e-06
Iter: 1277 loss: 3.06700235e-06
Iter: 1278 loss: 3.06641414e-06
Iter: 1279 loss: 3.06612674e-06
Iter: 1280 loss: 3.06559218e-06
Iter: 1281 loss: 3.06532365e-06
Iter: 1282 loss: 3.06448464e-06
Iter: 1283 loss: 3.06729316e-06
Iter: 1284 loss: 3.06446259e-06
Iter: 1285 loss: 3.06422749e-06
Iter: 1286 loss: 3.06319e-06
Iter: 1287 loss: 3.06805487e-06
Iter: 1288 loss: 3.06289644e-06
Iter: 1289 loss: 3.06185689e-06
Iter: 1290 loss: 3.07531263e-06
Iter: 1291 loss: 3.06187576e-06
Iter: 1292 loss: 3.06105903e-06
Iter: 1293 loss: 3.06124116e-06
Iter: 1294 loss: 3.0604042e-06
Iter: 1295 loss: 3.05880508e-06
Iter: 1296 loss: 3.06067886e-06
Iter: 1297 loss: 3.05802951e-06
Iter: 1298 loss: 3.05696767e-06
Iter: 1299 loss: 3.06249262e-06
Iter: 1300 loss: 3.05670255e-06
Iter: 1301 loss: 3.05551271e-06
Iter: 1302 loss: 3.054226e-06
Iter: 1303 loss: 3.0539511e-06
Iter: 1304 loss: 3.05258163e-06
Iter: 1305 loss: 3.07136111e-06
Iter: 1306 loss: 3.05252274e-06
Iter: 1307 loss: 3.05154208e-06
Iter: 1308 loss: 3.05387061e-06
Iter: 1309 loss: 3.0511485e-06
Iter: 1310 loss: 3.05002027e-06
Iter: 1311 loss: 3.05632648e-06
Iter: 1312 loss: 3.04999185e-06
Iter: 1313 loss: 3.04896889e-06
Iter: 1314 loss: 3.04884634e-06
Iter: 1315 loss: 3.04832e-06
Iter: 1316 loss: 3.04861214e-06
Iter: 1317 loss: 3.04768446e-06
Iter: 1318 loss: 3.0473584e-06
Iter: 1319 loss: 3.04662535e-06
Iter: 1320 loss: 3.05488675e-06
Iter: 1321 loss: 3.04667878e-06
Iter: 1322 loss: 3.04582636e-06
Iter: 1323 loss: 3.0441106e-06
Iter: 1324 loss: 3.04416744e-06
Iter: 1325 loss: 3.04292894e-06
Iter: 1326 loss: 3.04287278e-06
Iter: 1327 loss: 3.04218634e-06
Iter: 1328 loss: 3.04216223e-06
Iter: 1329 loss: 3.04153809e-06
Iter: 1330 loss: 3.04014361e-06
Iter: 1331 loss: 3.04181776e-06
Iter: 1332 loss: 3.03939487e-06
Iter: 1333 loss: 3.03838306e-06
Iter: 1334 loss: 3.04730929e-06
Iter: 1335 loss: 3.03840557e-06
Iter: 1336 loss: 3.03734896e-06
Iter: 1337 loss: 3.03642878e-06
Iter: 1338 loss: 3.03645902e-06
Iter: 1339 loss: 3.03514526e-06
Iter: 1340 loss: 3.0442211e-06
Iter: 1341 loss: 3.03493744e-06
Iter: 1342 loss: 3.03368824e-06
Iter: 1343 loss: 3.03417528e-06
Iter: 1344 loss: 3.0327692e-06
Iter: 1345 loss: 3.03187153e-06
Iter: 1346 loss: 3.04029436e-06
Iter: 1347 loss: 3.03184675e-06
Iter: 1348 loss: 3.03123375e-06
Iter: 1349 loss: 3.04029095e-06
Iter: 1350 loss: 3.03111347e-06
Iter: 1351 loss: 3.03030902e-06
Iter: 1352 loss: 3.03133584e-06
Iter: 1353 loss: 3.03000979e-06
Iter: 1354 loss: 3.0295e-06
Iter: 1355 loss: 3.02883655e-06
Iter: 1356 loss: 3.02881381e-06
Iter: 1357 loss: 3.02770832e-06
Iter: 1358 loss: 3.02704484e-06
Iter: 1359 loss: 3.0267488e-06
Iter: 1360 loss: 3.02573e-06
Iter: 1361 loss: 3.02578928e-06
Iter: 1362 loss: 3.02503054e-06
Iter: 1363 loss: 3.02389822e-06
Iter: 1364 loss: 3.02382682e-06
Iter: 1365 loss: 3.02281978e-06
Iter: 1366 loss: 3.03938873e-06
Iter: 1367 loss: 3.02276021e-06
Iter: 1368 loss: 3.02184094e-06
Iter: 1369 loss: 3.02218587e-06
Iter: 1370 loss: 3.02121407e-06
Iter: 1371 loss: 3.01992395e-06
Iter: 1372 loss: 3.02479043e-06
Iter: 1373 loss: 3.01956266e-06
Iter: 1374 loss: 3.01867476e-06
Iter: 1375 loss: 3.02313083e-06
Iter: 1376 loss: 3.01853561e-06
Iter: 1377 loss: 3.01746445e-06
Iter: 1378 loss: 3.01673072e-06
Iter: 1379 loss: 3.01644673e-06
Iter: 1380 loss: 3.01555292e-06
Iter: 1381 loss: 3.02762373e-06
Iter: 1382 loss: 3.01550131e-06
Iter: 1383 loss: 3.01508499e-06
Iter: 1384 loss: 3.0150054e-06
Iter: 1385 loss: 3.01443697e-06
Iter: 1386 loss: 3.01317732e-06
Iter: 1387 loss: 3.0304011e-06
Iter: 1388 loss: 3.01313412e-06
Iter: 1389 loss: 3.0124329e-06
Iter: 1390 loss: 3.0183121e-06
Iter: 1391 loss: 3.01243267e-06
Iter: 1392 loss: 3.0119902e-06
Iter: 1393 loss: 3.01086243e-06
Iter: 1394 loss: 3.02404487e-06
Iter: 1395 loss: 3.01079717e-06
Iter: 1396 loss: 3.00928923e-06
Iter: 1397 loss: 3.0243632e-06
Iter: 1398 loss: 3.00929901e-06
Iter: 1399 loss: 3.00862325e-06
Iter: 1400 loss: 3.00826014e-06
Iter: 1401 loss: 3.00796864e-06
Iter: 1402 loss: 3.00640727e-06
Iter: 1403 loss: 3.01238788e-06
Iter: 1404 loss: 3.00617216e-06
Iter: 1405 loss: 3.00522788e-06
Iter: 1406 loss: 3.00837928e-06
Iter: 1407 loss: 3.00500915e-06
Iter: 1408 loss: 3.00379634e-06
Iter: 1409 loss: 3.0058477e-06
Iter: 1410 loss: 3.00333022e-06
Iter: 1411 loss: 3.00245074e-06
Iter: 1412 loss: 3.00696684e-06
Iter: 1413 loss: 3.00244301e-06
Iter: 1414 loss: 3.00157535e-06
Iter: 1415 loss: 3.00071952e-06
Iter: 1416 loss: 3.00068768e-06
Iter: 1417 loss: 2.99965745e-06
Iter: 1418 loss: 3.01355658e-06
Iter: 1419 loss: 2.99962721e-06
Iter: 1420 loss: 2.99938893e-06
Iter: 1421 loss: 2.99918224e-06
Iter: 1422 loss: 2.99887461e-06
Iter: 1423 loss: 2.99732142e-06
Iter: 1424 loss: 3.00631132e-06
Iter: 1425 loss: 2.99706426e-06
Iter: 1426 loss: 2.99625663e-06
Iter: 1427 loss: 3.00248621e-06
Iter: 1428 loss: 2.99611338e-06
Iter: 1429 loss: 2.99537942e-06
Iter: 1430 loss: 2.99472367e-06
Iter: 1431 loss: 2.99455178e-06
Iter: 1432 loss: 2.9935145e-06
Iter: 1433 loss: 3.00444299e-06
Iter: 1434 loss: 2.99338512e-06
Iter: 1435 loss: 2.99269664e-06
Iter: 1436 loss: 2.99271642e-06
Iter: 1437 loss: 2.99211342e-06
Iter: 1438 loss: 2.99070234e-06
Iter: 1439 loss: 2.99174962e-06
Iter: 1440 loss: 2.99012504e-06
Iter: 1441 loss: 2.98882424e-06
Iter: 1442 loss: 2.99246904e-06
Iter: 1443 loss: 2.98848499e-06
Iter: 1444 loss: 2.98695272e-06
Iter: 1445 loss: 2.99448e-06
Iter: 1446 loss: 2.98667101e-06
Iter: 1447 loss: 2.98594318e-06
Iter: 1448 loss: 2.9864168e-06
Iter: 1449 loss: 2.98541408e-06
Iter: 1450 loss: 2.9839639e-06
Iter: 1451 loss: 2.98755276e-06
Iter: 1452 loss: 2.98356395e-06
Iter: 1453 loss: 2.98314808e-06
Iter: 1454 loss: 2.98295936e-06
Iter: 1455 loss: 2.98260329e-06
Iter: 1456 loss: 2.98443456e-06
Iter: 1457 loss: 2.98257714e-06
Iter: 1458 loss: 2.98217378e-06
Iter: 1459 loss: 2.98095438e-06
Iter: 1460 loss: 2.98687451e-06
Iter: 1461 loss: 2.98063287e-06
Iter: 1462 loss: 2.97940392e-06
Iter: 1463 loss: 2.98750729e-06
Iter: 1464 loss: 2.97925658e-06
Iter: 1465 loss: 2.97805036e-06
Iter: 1466 loss: 2.97782208e-06
Iter: 1467 loss: 2.97705878e-06
Iter: 1468 loss: 2.97590941e-06
Iter: 1469 loss: 2.97592533e-06
Iter: 1470 loss: 2.97524366e-06
Iter: 1471 loss: 2.97387805e-06
Iter: 1472 loss: 2.97382348e-06
Iter: 1473 loss: 2.9726516e-06
Iter: 1474 loss: 2.99002727e-06
Iter: 1475 loss: 2.97273095e-06
Iter: 1476 loss: 2.97186057e-06
Iter: 1477 loss: 2.97159545e-06
Iter: 1478 loss: 2.97128054e-06
Iter: 1479 loss: 2.97014822e-06
Iter: 1480 loss: 2.97140923e-06
Iter: 1481 loss: 2.96943608e-06
Iter: 1482 loss: 2.9684802e-06
Iter: 1483 loss: 2.96886424e-06
Iter: 1484 loss: 2.96775033e-06
Iter: 1485 loss: 2.96645044e-06
Iter: 1486 loss: 2.98020359e-06
Iter: 1487 loss: 2.96629969e-06
Iter: 1488 loss: 2.96549615e-06
Iter: 1489 loss: 2.96573603e-06
Iter: 1490 loss: 2.96514031e-06
Iter: 1491 loss: 2.96455482e-06
Iter: 1492 loss: 2.96457347e-06
Iter: 1493 loss: 2.96371e-06
Iter: 1494 loss: 2.96245889e-06
Iter: 1495 loss: 2.96241706e-06
Iter: 1496 loss: 2.96145527e-06
Iter: 1497 loss: 2.97669885e-06
Iter: 1498 loss: 2.96141388e-06
Iter: 1499 loss: 2.96052849e-06
Iter: 1500 loss: 2.9597777e-06
Iter: 1501 loss: 2.9595833e-06
Iter: 1502 loss: 2.95800783e-06
Iter: 1503 loss: 2.97061024e-06
Iter: 1504 loss: 2.95783184e-06
Iter: 1505 loss: 2.95696236e-06
Iter: 1506 loss: 2.95724499e-06
Iter: 1507 loss: 2.95618065e-06
Iter: 1508 loss: 2.95461678e-06
Iter: 1509 loss: 2.95636391e-06
Iter: 1510 loss: 2.95344603e-06
Iter: 1511 loss: 2.95274685e-06
Iter: 1512 loss: 2.96328676e-06
Iter: 1513 loss: 2.95269729e-06
Iter: 1514 loss: 2.95173072e-06
Iter: 1515 loss: 2.95019436e-06
Iter: 1516 loss: 2.95020823e-06
Iter: 1517 loss: 2.9494563e-06
Iter: 1518 loss: 2.94939355e-06
Iter: 1519 loss: 2.94895472e-06
Iter: 1520 loss: 2.9489604e-06
Iter: 1521 loss: 2.94845609e-06
Iter: 1522 loss: 2.94755046e-06
Iter: 1523 loss: 2.9660705e-06
Iter: 1524 loss: 2.94749339e-06
Iter: 1525 loss: 2.94665597e-06
Iter: 1526 loss: 2.95000768e-06
Iter: 1527 loss: 2.94646293e-06
Iter: 1528 loss: 2.94580423e-06
Iter: 1529 loss: 2.94476285e-06
Iter: 1530 loss: 2.97322958e-06
Iter: 1531 loss: 2.9446951e-06
Iter: 1532 loss: 2.94297661e-06
Iter: 1533 loss: 2.95529912e-06
Iter: 1534 loss: 2.94276197e-06
Iter: 1535 loss: 2.94201391e-06
Iter: 1536 loss: 2.94317397e-06
Iter: 1537 loss: 2.9415105e-06
Iter: 1538 loss: 2.94033225e-06
Iter: 1539 loss: 2.9451337e-06
Iter: 1540 loss: 2.94001893e-06
Iter: 1541 loss: 2.93928088e-06
Iter: 1542 loss: 2.94333131e-06
Iter: 1543 loss: 2.93906805e-06
Iter: 1544 loss: 2.93823746e-06
Iter: 1545 loss: 2.93689618e-06
Iter: 1546 loss: 2.93687413e-06
Iter: 1547 loss: 2.93606126e-06
Iter: 1548 loss: 2.93602443e-06
Iter: 1549 loss: 2.93532412e-06
Iter: 1550 loss: 2.93670519e-06
Iter: 1551 loss: 2.93515632e-06
Iter: 1552 loss: 2.93447988e-06
Iter: 1553 loss: 2.93451194e-06
Iter: 1554 loss: 2.93410267e-06
Iter: 1555 loss: 2.93502626e-06
Iter: 1556 loss: 2.93386574e-06
Iter: 1557 loss: 2.93357789e-06
Iter: 1558 loss: 2.93304788e-06
Iter: 1559 loss: 2.94557117e-06
Iter: 1560 loss: 2.93302037e-06
Iter: 1561 loss: 2.93200833e-06
Iter: 1562 loss: 2.93127732e-06
Iter: 1563 loss: 2.93094013e-06
Iter: 1564 loss: 2.92998902e-06
Iter: 1565 loss: 2.93916037e-06
Iter: 1566 loss: 2.92998925e-06
Iter: 1567 loss: 2.92908931e-06
Iter: 1568 loss: 2.92842265e-06
Iter: 1569 loss: 2.92812865e-06
Iter: 1570 loss: 2.92715049e-06
Iter: 1571 loss: 2.92713594e-06
Iter: 1572 loss: 2.92638333e-06
Iter: 1573 loss: 2.92531081e-06
Iter: 1574 loss: 2.92526397e-06
Iter: 1575 loss: 2.92418736e-06
Iter: 1576 loss: 2.92412869e-06
Iter: 1577 loss: 2.92347113e-06
Iter: 1578 loss: 2.92304958e-06
Iter: 1579 loss: 2.92289224e-06
Iter: 1580 loss: 2.92160757e-06
Iter: 1581 loss: 2.92764616e-06
Iter: 1582 loss: 2.92144477e-06
Iter: 1583 loss: 2.92149139e-06
Iter: 1584 loss: 2.92090954e-06
Iter: 1585 loss: 2.92072536e-06
Iter: 1586 loss: 2.92056075e-06
Iter: 1587 loss: 2.9204416e-06
Iter: 1588 loss: 2.92000868e-06
Iter: 1589 loss: 2.91901961e-06
Iter: 1590 loss: 2.9333064e-06
Iter: 1591 loss: 2.91879064e-06
Iter: 1592 loss: 2.91767765e-06
Iter: 1593 loss: 2.92661707e-06
Iter: 1594 loss: 2.91766082e-06
Iter: 1595 loss: 2.91696097e-06
Iter: 1596 loss: 2.91651895e-06
Iter: 1597 loss: 2.91635456e-06
Iter: 1598 loss: 2.91496508e-06
Iter: 1599 loss: 2.92067625e-06
Iter: 1600 loss: 2.91450669e-06
Iter: 1601 loss: 2.91381821e-06
Iter: 1602 loss: 2.91470042e-06
Iter: 1603 loss: 2.9134012e-06
Iter: 1604 loss: 2.91217225e-06
Iter: 1605 loss: 2.91298193e-06
Iter: 1606 loss: 2.91128e-06
Iter: 1607 loss: 2.91029119e-06
Iter: 1608 loss: 2.92200912e-06
Iter: 1609 loss: 2.9102614e-06
Iter: 1610 loss: 2.90934508e-06
Iter: 1611 loss: 2.90798971e-06
Iter: 1612 loss: 2.90798516e-06
Iter: 1613 loss: 2.90666708e-06
Iter: 1614 loss: 2.90665048e-06
Iter: 1615 loss: 2.90598405e-06
Iter: 1616 loss: 2.90970411e-06
Iter: 1617 loss: 2.90605021e-06
Iter: 1618 loss: 2.90503476e-06
Iter: 1619 loss: 2.90756429e-06
Iter: 1620 loss: 2.90477124e-06
Iter: 1621 loss: 2.90428034e-06
Iter: 1622 loss: 2.90405865e-06
Iter: 1623 loss: 2.90361322e-06
Iter: 1624 loss: 2.90287539e-06
Iter: 1625 loss: 2.90300227e-06
Iter: 1626 loss: 2.90250364e-06
Iter: 1627 loss: 2.90134858e-06
Iter: 1628 loss: 2.90103731e-06
Iter: 1629 loss: 2.9003e-06
Iter: 1630 loss: 2.89935451e-06
Iter: 1631 loss: 2.90920525e-06
Iter: 1632 loss: 2.89945547e-06
Iter: 1633 loss: 2.89850664e-06
Iter: 1634 loss: 2.8976242e-06
Iter: 1635 loss: 2.89746117e-06
Iter: 1636 loss: 2.89647551e-06
Iter: 1637 loss: 2.90931212e-06
Iter: 1638 loss: 2.89651052e-06
Iter: 1639 loss: 2.89565151e-06
Iter: 1640 loss: 2.89463128e-06
Iter: 1641 loss: 2.89446689e-06
Iter: 1642 loss: 2.89326454e-06
Iter: 1643 loss: 2.89337368e-06
Iter: 1644 loss: 2.89247146e-06
Iter: 1645 loss: 2.89313311e-06
Iter: 1646 loss: 2.89184072e-06
Iter: 1647 loss: 2.89096829e-06
Iter: 1648 loss: 2.89640866e-06
Iter: 1649 loss: 2.89075524e-06
Iter: 1650 loss: 2.89078685e-06
Iter: 1651 loss: 2.89047125e-06
Iter: 1652 loss: 2.89028549e-06
Iter: 1653 loss: 2.88969886e-06
Iter: 1654 loss: 2.89349236e-06
Iter: 1655 loss: 2.88973069e-06
Iter: 1656 loss: 2.88908973e-06
Iter: 1657 loss: 2.88767774e-06
Iter: 1658 loss: 2.91630249e-06
Iter: 1659 loss: 2.88772526e-06
Iter: 1660 loss: 2.88647061e-06
Iter: 1661 loss: 2.89838613e-06
Iter: 1662 loss: 2.88648289e-06
Iter: 1663 loss: 2.88541742e-06
Iter: 1664 loss: 2.88471847e-06
Iter: 1665 loss: 2.88439242e-06
Iter: 1666 loss: 2.883251e-06
Iter: 1667 loss: 2.88311094e-06
Iter: 1668 loss: 2.88240676e-06
Iter: 1669 loss: 2.88109936e-06
Iter: 1670 loss: 2.91449578e-06
Iter: 1671 loss: 2.88111801e-06
Iter: 1672 loss: 2.87992884e-06
Iter: 1673 loss: 2.8798604e-06
Iter: 1674 loss: 2.87914759e-06
Iter: 1675 loss: 2.88144497e-06
Iter: 1676 loss: 2.87891044e-06
Iter: 1677 loss: 2.87792773e-06
Iter: 1678 loss: 2.87653552e-06
Iter: 1679 loss: 2.87645253e-06
Iter: 1680 loss: 2.875684e-06
Iter: 1681 loss: 2.87572539e-06
Iter: 1682 loss: 2.87531384e-06
Iter: 1683 loss: 2.87525017e-06
Iter: 1684 loss: 2.87487183e-06
Iter: 1685 loss: 2.8740244e-06
Iter: 1686 loss: 2.89034983e-06
Iter: 1687 loss: 2.87402645e-06
Iter: 1688 loss: 2.8733466e-06
Iter: 1689 loss: 2.87563944e-06
Iter: 1690 loss: 2.87325247e-06
Iter: 1691 loss: 2.87243756e-06
Iter: 1692 loss: 2.87167313e-06
Iter: 1693 loss: 2.87159651e-06
Iter: 1694 loss: 2.87023659e-06
Iter: 1695 loss: 2.87901912e-06
Iter: 1696 loss: 2.86990507e-06
Iter: 1697 loss: 2.86927479e-06
Iter: 1698 loss: 2.86909381e-06
Iter: 1699 loss: 2.86861541e-06
Iter: 1700 loss: 2.86738077e-06
Iter: 1701 loss: 2.8748459e-06
Iter: 1702 loss: 2.86718387e-06
Iter: 1703 loss: 2.86662362e-06
Iter: 1704 loss: 2.86813975e-06
Iter: 1705 loss: 2.86626118e-06
Iter: 1706 loss: 2.86512568e-06
Iter: 1707 loss: 2.86465229e-06
Iter: 1708 loss: 2.86416571e-06
Iter: 1709 loss: 2.86320801e-06
Iter: 1710 loss: 2.8747927e-06
Iter: 1711 loss: 2.86315799e-06
Iter: 1712 loss: 2.8622394e-06
Iter: 1713 loss: 2.8627519e-06
Iter: 1714 loss: 2.8615716e-06
Iter: 1715 loss: 2.86155864e-06
Iter: 1716 loss: 2.86124737e-06
Iter: 1717 loss: 2.86076238e-06
Iter: 1718 loss: 2.86160025e-06
Iter: 1719 loss: 2.86074146e-06
Iter: 1720 loss: 2.86026125e-06
Iter: 1721 loss: 2.85961164e-06
Iter: 1722 loss: 2.86577961e-06
Iter: 1723 loss: 2.85955321e-06
Iter: 1724 loss: 2.85837632e-06
Iter: 1725 loss: 2.86509726e-06
Iter: 1726 loss: 2.85830447e-06
Iter: 1727 loss: 2.85758961e-06
Iter: 1728 loss: 2.85893179e-06
Iter: 1729 loss: 2.85743863e-06
Iter: 1730 loss: 2.85617216e-06
Iter: 1731 loss: 2.85592705e-06
Iter: 1732 loss: 2.85512715e-06
Iter: 1733 loss: 2.85425449e-06
Iter: 1734 loss: 2.86387694e-06
Iter: 1735 loss: 2.85414649e-06
Iter: 1736 loss: 2.85325814e-06
Iter: 1737 loss: 2.8524878e-06
Iter: 1738 loss: 2.85238411e-06
Iter: 1739 loss: 2.85154692e-06
Iter: 1740 loss: 2.85150486e-06
Iter: 1741 loss: 2.8509146e-06
Iter: 1742 loss: 2.84987186e-06
Iter: 1743 loss: 2.84981957e-06
Iter: 1744 loss: 2.84900852e-06
Iter: 1745 loss: 2.84885618e-06
Iter: 1746 loss: 2.84865405e-06
Iter: 1747 loss: 2.84851535e-06
Iter: 1748 loss: 2.84832799e-06
Iter: 1749 loss: 2.84832367e-06
Iter: 1750 loss: 2.84795919e-06
Iter: 1751 loss: 2.84749012e-06
Iter: 1752 loss: 2.84646376e-06
Iter: 1753 loss: 2.86564409e-06
Iter: 1754 loss: 2.84645648e-06
Iter: 1755 loss: 2.84564476e-06
Iter: 1756 loss: 2.84936095e-06
Iter: 1757 loss: 2.84557336e-06
Iter: 1758 loss: 2.84451335e-06
Iter: 1759 loss: 2.84344196e-06
Iter: 1760 loss: 2.84329599e-06
Iter: 1761 loss: 2.84229168e-06
Iter: 1762 loss: 2.84228713e-06
Iter: 1763 loss: 2.84142698e-06
Iter: 1764 loss: 2.84045109e-06
Iter: 1765 loss: 2.84036446e-06
Iter: 1766 loss: 2.83910958e-06
Iter: 1767 loss: 2.85110264e-06
Iter: 1768 loss: 2.83897634e-06
Iter: 1769 loss: 2.83826967e-06
Iter: 1770 loss: 2.83741701e-06
Iter: 1771 loss: 2.83719874e-06
Iter: 1772 loss: 2.83566146e-06
Iter: 1773 loss: 2.84791918e-06
Iter: 1774 loss: 2.83571762e-06
Iter: 1775 loss: 2.83473514e-06
Iter: 1776 loss: 2.83469581e-06
Iter: 1777 loss: 2.83417126e-06
Iter: 1778 loss: 2.83274971e-06
Iter: 1779 loss: 2.84196699e-06
Iter: 1780 loss: 2.83252666e-06
Iter: 1781 loss: 2.83299823e-06
Iter: 1782 loss: 2.83206555e-06
Iter: 1783 loss: 2.83196687e-06
Iter: 1784 loss: 2.83147847e-06
Iter: 1785 loss: 2.83609938e-06
Iter: 1786 loss: 2.83137729e-06
Iter: 1787 loss: 2.83067516e-06
Iter: 1788 loss: 2.82986798e-06
Iter: 1789 loss: 2.82974884e-06
Iter: 1790 loss: 2.82908513e-06
Iter: 1791 loss: 2.82919723e-06
Iter: 1792 loss: 2.82849464e-06
Iter: 1793 loss: 2.82770179e-06
Iter: 1794 loss: 2.82767132e-06
Iter: 1795 loss: 2.82695237e-06
Iter: 1796 loss: 2.83911368e-06
Iter: 1797 loss: 2.82686256e-06
Iter: 1798 loss: 2.8262557e-06
Iter: 1799 loss: 2.82542919e-06
Iter: 1800 loss: 2.82549331e-06
Iter: 1801 loss: 2.8245156e-06
Iter: 1802 loss: 2.83354325e-06
Iter: 1803 loss: 2.82447172e-06
Iter: 1804 loss: 2.82381711e-06
Iter: 1805 loss: 2.8234449e-06
Iter: 1806 loss: 2.8230661e-06
Iter: 1807 loss: 2.82217843e-06
Iter: 1808 loss: 2.82541669e-06
Iter: 1809 loss: 2.82189694e-06
Iter: 1810 loss: 2.82101337e-06
Iter: 1811 loss: 2.81969687e-06
Iter: 1812 loss: 2.81969778e-06
Iter: 1813 loss: 2.81963503e-06
Iter: 1814 loss: 2.81920529e-06
Iter: 1815 loss: 2.81870143e-06
Iter: 1816 loss: 2.81788925e-06
Iter: 1817 loss: 2.81781e-06
Iter: 1818 loss: 2.81725784e-06
Iter: 1819 loss: 2.81646135e-06
Iter: 1820 loss: 2.81641269e-06
Iter: 1821 loss: 2.81516895e-06
Iter: 1822 loss: 2.82362407e-06
Iter: 1823 loss: 2.81508846e-06
Iter: 1824 loss: 2.81436223e-06
Iter: 1825 loss: 2.81537e-06
Iter: 1826 loss: 2.81391021e-06
Iter: 1827 loss: 2.81297139e-06
Iter: 1828 loss: 2.81550297e-06
Iter: 1829 loss: 2.81260873e-06
Iter: 1830 loss: 2.81183975e-06
Iter: 1831 loss: 2.81854409e-06
Iter: 1832 loss: 2.81180746e-06
Iter: 1833 loss: 2.81124221e-06
Iter: 1834 loss: 2.81012e-06
Iter: 1835 loss: 2.81018401e-06
Iter: 1836 loss: 2.80934978e-06
Iter: 1837 loss: 2.82261703e-06
Iter: 1838 loss: 2.80925269e-06
Iter: 1839 loss: 2.80862423e-06
Iter: 1840 loss: 2.80769882e-06
Iter: 1841 loss: 2.8074387e-06
Iter: 1842 loss: 2.80649738e-06
Iter: 1843 loss: 2.82074893e-06
Iter: 1844 loss: 2.80656081e-06
Iter: 1845 loss: 2.80614131e-06
Iter: 1846 loss: 2.80603763e-06
Iter: 1847 loss: 2.80580616e-06
Iter: 1848 loss: 2.80540576e-06
Iter: 1849 loss: 2.80532208e-06
Iter: 1850 loss: 2.80490622e-06
Iter: 1851 loss: 2.80448694e-06
Iter: 1852 loss: 2.80439508e-06
Iter: 1853 loss: 2.80389213e-06
Iter: 1854 loss: 2.80376958e-06
Iter: 1855 loss: 2.8035065e-06
Iter: 1856 loss: 2.8022655e-06
Iter: 1857 loss: 2.80328231e-06
Iter: 1858 loss: 2.80177665e-06
Iter: 1859 loss: 2.80093263e-06
Iter: 1860 loss: 2.81111238e-06
Iter: 1861 loss: 2.80089534e-06
Iter: 1862 loss: 2.80007407e-06
Iter: 1863 loss: 2.79927463e-06
Iter: 1864 loss: 2.79908681e-06
Iter: 1865 loss: 2.79789947e-06
Iter: 1866 loss: 2.80826453e-06
Iter: 1867 loss: 2.7978e-06
Iter: 1868 loss: 2.79690539e-06
Iter: 1869 loss: 2.79674396e-06
Iter: 1870 loss: 2.79621258e-06
Iter: 1871 loss: 2.79483811e-06
Iter: 1872 loss: 2.8031759e-06
Iter: 1873 loss: 2.79461665e-06
Iter: 1874 loss: 2.79396136e-06
Iter: 1875 loss: 2.79559572e-06
Iter: 1876 loss: 2.79366895e-06
Iter: 1877 loss: 2.79274172e-06
Iter: 1878 loss: 2.79260667e-06
Iter: 1879 loss: 2.7919732e-06
Iter: 1880 loss: 2.79154597e-06
Iter: 1881 loss: 2.79150481e-06
Iter: 1882 loss: 2.7908377e-06
Iter: 1883 loss: 2.79206961e-06
Iter: 1884 loss: 2.7906417e-06
Iter: 1885 loss: 2.79006554e-06
Iter: 1886 loss: 2.78876291e-06
Iter: 1887 loss: 2.7973656e-06
Iter: 1888 loss: 2.78848279e-06
Iter: 1889 loss: 2.78778543e-06
Iter: 1890 loss: 2.78785456e-06
Iter: 1891 loss: 2.78695188e-06
Iter: 1892 loss: 2.7862925e-06
Iter: 1893 loss: 2.785991e-06
Iter: 1894 loss: 2.78515836e-06
Iter: 1895 loss: 2.785265e-06
Iter: 1896 loss: 2.78448488e-06
Iter: 1897 loss: 2.78516859e-06
Iter: 1898 loss: 2.78422749e-06
Iter: 1899 loss: 2.78321431e-06
Iter: 1900 loss: 2.78637799e-06
Iter: 1901 loss: 2.78286825e-06
Iter: 1902 loss: 2.7821925e-06
Iter: 1903 loss: 2.78356038e-06
Iter: 1904 loss: 2.7817523e-06
Iter: 1905 loss: 2.7807198e-06
Iter: 1906 loss: 2.78056677e-06
Iter: 1907 loss: 2.77975369e-06
Iter: 1908 loss: 2.77878803e-06
Iter: 1909 loss: 2.77883464e-06
Iter: 1910 loss: 2.77831214e-06
Iter: 1911 loss: 2.77808363e-06
Iter: 1912 loss: 2.77778645e-06
Iter: 1913 loss: 2.77771278e-06
Iter: 1914 loss: 2.77739809e-06
Iter: 1915 loss: 2.77715435e-06
Iter: 1916 loss: 2.77645017e-06
Iter: 1917 loss: 2.78701418e-06
Iter: 1918 loss: 2.77641425e-06
Iter: 1919 loss: 2.77599338e-06
Iter: 1920 loss: 2.77474282e-06
Iter: 1921 loss: 2.79260075e-06
Iter: 1922 loss: 2.77473259e-06
Iter: 1923 loss: 2.77324375e-06
Iter: 1924 loss: 2.7825397e-06
Iter: 1925 loss: 2.77291701e-06
Iter: 1926 loss: 2.77184336e-06
Iter: 1927 loss: 2.77215486e-06
Iter: 1928 loss: 2.77121308e-06
Iter: 1929 loss: 2.76976743e-06
Iter: 1930 loss: 2.77729373e-06
Iter: 1931 loss: 2.76954643e-06
Iter: 1932 loss: 2.76861556e-06
Iter: 1933 loss: 2.76912806e-06
Iter: 1934 loss: 2.7679032e-06
Iter: 1935 loss: 2.76675701e-06
Iter: 1936 loss: 2.78008474e-06
Iter: 1937 loss: 2.76676701e-06
Iter: 1938 loss: 2.76595938e-06
Iter: 1939 loss: 2.76839819e-06
Iter: 1940 loss: 2.76577202e-06
Iter: 1941 loss: 2.7650965e-06
Iter: 1942 loss: 2.76402534e-06
Iter: 1943 loss: 2.76384753e-06
Iter: 1944 loss: 2.76288165e-06
Iter: 1945 loss: 2.77400522e-06
Iter: 1946 loss: 2.76311448e-06
Iter: 1947 loss: 2.76239325e-06
Iter: 1948 loss: 2.76741e-06
Iter: 1949 loss: 2.76233641e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi2.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi3
+ date
Mon Oct 26 19:37:01 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi3/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi3_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi3_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi3_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi3/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -1 --phi 3 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi-1_phi3_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928e3bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928e3bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928f4cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928f4c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928e99488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928db58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928dfe950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928d6b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928d96048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928d961e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928d2a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928d2a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928d21620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928caa8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928c816a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928c41730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928c4a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928c4aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928c107b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928ba5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928bd4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928b6ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb928bdd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb9026ab730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb90269f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb90266fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb9026349d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb90259e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb90269fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb9025c3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb9025e07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8dc66d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8dc66db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8dc640510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8dc646730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8dc5d46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.4444654e-05
Iter: 2 loss: 1.17314839e-05
Iter: 3 loss: 4.00825083e-05
Iter: 4 loss: 1.16614692e-05
Iter: 5 loss: 1.02515787e-05
Iter: 6 loss: 2.48662291e-05
Iter: 7 loss: 1.02119193e-05
Iter: 8 loss: 9.69262055e-06
Iter: 9 loss: 8.75146179e-06
Iter: 10 loss: 3.12800112e-05
Iter: 11 loss: 8.75119076e-06
Iter: 12 loss: 7.90943886e-06
Iter: 13 loss: 1.50227925e-05
Iter: 14 loss: 7.86093551e-06
Iter: 15 loss: 7.50824438e-06
Iter: 16 loss: 7.0468077e-06
Iter: 17 loss: 7.01743738e-06
Iter: 18 loss: 6.48888044e-06
Iter: 19 loss: 1.41201699e-05
Iter: 20 loss: 6.48814603e-06
Iter: 21 loss: 6.19341927e-06
Iter: 22 loss: 5.97369035e-06
Iter: 23 loss: 5.87648174e-06
Iter: 24 loss: 5.37996948e-06
Iter: 25 loss: 9.87375824e-06
Iter: 26 loss: 5.35674826e-06
Iter: 27 loss: 5.18042816e-06
Iter: 28 loss: 6.53927555e-06
Iter: 29 loss: 5.16745649e-06
Iter: 30 loss: 4.99816633e-06
Iter: 31 loss: 4.78304901e-06
Iter: 32 loss: 4.76701e-06
Iter: 33 loss: 4.53928851e-06
Iter: 34 loss: 5.29292629e-06
Iter: 35 loss: 4.47650837e-06
Iter: 36 loss: 4.22116227e-06
Iter: 37 loss: 5.30645912e-06
Iter: 38 loss: 4.16788225e-06
Iter: 39 loss: 4.05691571e-06
Iter: 40 loss: 4.81787129e-06
Iter: 41 loss: 4.04606408e-06
Iter: 42 loss: 3.88207354e-06
Iter: 43 loss: 4.21276218e-06
Iter: 44 loss: 3.81541577e-06
Iter: 45 loss: 3.75620766e-06
Iter: 46 loss: 3.74809e-06
Iter: 47 loss: 3.70647467e-06
Iter: 48 loss: 3.62128276e-06
Iter: 49 loss: 3.54674739e-06
Iter: 50 loss: 3.52438792e-06
Iter: 51 loss: 3.41956297e-06
Iter: 52 loss: 3.41803798e-06
Iter: 53 loss: 3.3678632e-06
Iter: 54 loss: 3.33900016e-06
Iter: 55 loss: 3.31754927e-06
Iter: 56 loss: 3.22421647e-06
Iter: 57 loss: 3.38198174e-06
Iter: 58 loss: 3.18201046e-06
Iter: 59 loss: 3.10817791e-06
Iter: 60 loss: 3.20326808e-06
Iter: 61 loss: 3.07039795e-06
Iter: 62 loss: 2.96999792e-06
Iter: 63 loss: 3.51014023e-06
Iter: 64 loss: 2.95495647e-06
Iter: 65 loss: 2.90687831e-06
Iter: 66 loss: 3.29134764e-06
Iter: 67 loss: 2.90359822e-06
Iter: 68 loss: 2.8500483e-06
Iter: 69 loss: 2.78457787e-06
Iter: 70 loss: 2.77852314e-06
Iter: 71 loss: 2.71757767e-06
Iter: 72 loss: 3.06308198e-06
Iter: 73 loss: 2.70913506e-06
Iter: 74 loss: 2.65919698e-06
Iter: 75 loss: 3.23510176e-06
Iter: 76 loss: 2.65829203e-06
Iter: 77 loss: 2.61501418e-06
Iter: 78 loss: 3.00849229e-06
Iter: 79 loss: 2.61314153e-06
Iter: 80 loss: 2.60101615e-06
Iter: 81 loss: 2.5693962e-06
Iter: 82 loss: 2.82419182e-06
Iter: 83 loss: 2.56386306e-06
Iter: 84 loss: 2.51480378e-06
Iter: 85 loss: 2.60567231e-06
Iter: 86 loss: 2.49378763e-06
Iter: 87 loss: 2.46272066e-06
Iter: 88 loss: 2.57873e-06
Iter: 89 loss: 2.45523506e-06
Iter: 90 loss: 2.41698126e-06
Iter: 91 loss: 2.57374768e-06
Iter: 92 loss: 2.40869099e-06
Iter: 93 loss: 2.38657e-06
Iter: 94 loss: 2.50762355e-06
Iter: 95 loss: 2.38336725e-06
Iter: 96 loss: 2.36365986e-06
Iter: 97 loss: 2.318214e-06
Iter: 98 loss: 2.90564685e-06
Iter: 99 loss: 2.31516015e-06
Iter: 100 loss: 2.27477176e-06
Iter: 101 loss: 2.64107393e-06
Iter: 102 loss: 2.27295868e-06
Iter: 103 loss: 2.23531333e-06
Iter: 104 loss: 2.45512229e-06
Iter: 105 loss: 2.23042889e-06
Iter: 106 loss: 2.21030677e-06
Iter: 107 loss: 2.39041083e-06
Iter: 108 loss: 2.2093061e-06
Iter: 109 loss: 2.19131653e-06
Iter: 110 loss: 2.15500768e-06
Iter: 111 loss: 2.8327961e-06
Iter: 112 loss: 2.15459067e-06
Iter: 113 loss: 2.23587222e-06
Iter: 114 loss: 2.14817783e-06
Iter: 115 loss: 2.14297643e-06
Iter: 116 loss: 2.12998975e-06
Iter: 117 loss: 2.26016596e-06
Iter: 118 loss: 2.12844452e-06
Iter: 119 loss: 2.11059955e-06
Iter: 120 loss: 2.08560778e-06
Iter: 121 loss: 2.08452161e-06
Iter: 122 loss: 2.06186587e-06
Iter: 123 loss: 2.31254444e-06
Iter: 124 loss: 2.06135246e-06
Iter: 125 loss: 2.04148773e-06
Iter: 126 loss: 2.06530694e-06
Iter: 127 loss: 2.03111426e-06
Iter: 128 loss: 2.01711509e-06
Iter: 129 loss: 2.01716875e-06
Iter: 130 loss: 2.00526529e-06
Iter: 131 loss: 1.98674661e-06
Iter: 132 loss: 1.98649059e-06
Iter: 133 loss: 1.96800329e-06
Iter: 134 loss: 2.2142674e-06
Iter: 135 loss: 1.96788824e-06
Iter: 136 loss: 1.95813118e-06
Iter: 137 loss: 1.94015752e-06
Iter: 138 loss: 2.36097958e-06
Iter: 139 loss: 1.94017457e-06
Iter: 140 loss: 1.92019957e-06
Iter: 141 loss: 2.21504501e-06
Iter: 142 loss: 1.92011339e-06
Iter: 143 loss: 1.91017489e-06
Iter: 144 loss: 1.90821811e-06
Iter: 145 loss: 1.90159813e-06
Iter: 146 loss: 1.88652075e-06
Iter: 147 loss: 2.04149e-06
Iter: 148 loss: 1.88613421e-06
Iter: 149 loss: 1.87497687e-06
Iter: 150 loss: 1.98865473e-06
Iter: 151 loss: 1.87471971e-06
Iter: 152 loss: 1.86915372e-06
Iter: 153 loss: 1.85618296e-06
Iter: 154 loss: 2.01449848e-06
Iter: 155 loss: 1.85520014e-06
Iter: 156 loss: 1.83752161e-06
Iter: 157 loss: 1.84647229e-06
Iter: 158 loss: 1.82588e-06
Iter: 159 loss: 1.81280654e-06
Iter: 160 loss: 1.85515762e-06
Iter: 161 loss: 1.80910888e-06
Iter: 162 loss: 1.79392032e-06
Iter: 163 loss: 1.88320359e-06
Iter: 164 loss: 1.79190806e-06
Iter: 165 loss: 1.78427956e-06
Iter: 166 loss: 1.86256625e-06
Iter: 167 loss: 1.78399227e-06
Iter: 168 loss: 1.77756601e-06
Iter: 169 loss: 1.76526578e-06
Iter: 170 loss: 2.02936053e-06
Iter: 171 loss: 1.7652817e-06
Iter: 172 loss: 1.75012042e-06
Iter: 173 loss: 1.88090235e-06
Iter: 174 loss: 1.7492498e-06
Iter: 175 loss: 1.74144839e-06
Iter: 176 loss: 1.73352055e-06
Iter: 177 loss: 1.73199044e-06
Iter: 178 loss: 1.72071054e-06
Iter: 179 loss: 1.89058585e-06
Iter: 180 loss: 1.72069133e-06
Iter: 181 loss: 1.71333738e-06
Iter: 182 loss: 1.71808051e-06
Iter: 183 loss: 1.70862666e-06
Iter: 184 loss: 1.69895202e-06
Iter: 185 loss: 1.69889449e-06
Iter: 186 loss: 1.69253281e-06
Iter: 187 loss: 1.69232533e-06
Iter: 188 loss: 1.68733561e-06
Iter: 189 loss: 1.68291751e-06
Iter: 190 loss: 1.67644805e-06
Iter: 191 loss: 1.67624228e-06
Iter: 192 loss: 1.66674124e-06
Iter: 193 loss: 1.7163436e-06
Iter: 194 loss: 1.66518862e-06
Iter: 195 loss: 1.65969595e-06
Iter: 196 loss: 1.6579055e-06
Iter: 197 loss: 1.65470601e-06
Iter: 198 loss: 1.64541302e-06
Iter: 199 loss: 1.71567149e-06
Iter: 200 loss: 1.64477206e-06
Iter: 201 loss: 1.63932157e-06
Iter: 202 loss: 1.66780296e-06
Iter: 203 loss: 1.63850439e-06
Iter: 204 loss: 1.6327125e-06
Iter: 205 loss: 1.62482581e-06
Iter: 206 loss: 1.62445212e-06
Iter: 207 loss: 1.6186525e-06
Iter: 208 loss: 1.61860794e-06
Iter: 209 loss: 1.61398441e-06
Iter: 210 loss: 1.60746595e-06
Iter: 211 loss: 1.60733327e-06
Iter: 212 loss: 1.60114701e-06
Iter: 213 loss: 1.60096397e-06
Iter: 214 loss: 1.59824742e-06
Iter: 215 loss: 1.62140645e-06
Iter: 216 loss: 1.59811111e-06
Iter: 217 loss: 1.59390743e-06
Iter: 218 loss: 1.58947728e-06
Iter: 219 loss: 1.58878186e-06
Iter: 220 loss: 1.58279681e-06
Iter: 221 loss: 1.59904744e-06
Iter: 222 loss: 1.5807766e-06
Iter: 223 loss: 1.5771227e-06
Iter: 224 loss: 1.57235945e-06
Iter: 225 loss: 1.57202578e-06
Iter: 226 loss: 1.56379247e-06
Iter: 227 loss: 1.60042327e-06
Iter: 228 loss: 1.56232647e-06
Iter: 229 loss: 1.55763519e-06
Iter: 230 loss: 1.56527108e-06
Iter: 231 loss: 1.55544853e-06
Iter: 232 loss: 1.54930649e-06
Iter: 233 loss: 1.58270541e-06
Iter: 234 loss: 1.54844975e-06
Iter: 235 loss: 1.54443126e-06
Iter: 236 loss: 1.56550914e-06
Iter: 237 loss: 1.54382974e-06
Iter: 238 loss: 1.53943688e-06
Iter: 239 loss: 1.53274607e-06
Iter: 240 loss: 1.53258031e-06
Iter: 241 loss: 1.52755842e-06
Iter: 242 loss: 1.58988723e-06
Iter: 243 loss: 1.52752148e-06
Iter: 244 loss: 1.52236294e-06
Iter: 245 loss: 1.51478571e-06
Iter: 246 loss: 1.51452912e-06
Iter: 247 loss: 1.50945459e-06
Iter: 248 loss: 1.50905191e-06
Iter: 249 loss: 1.50697315e-06
Iter: 250 loss: 1.5068922e-06
Iter: 251 loss: 1.50422693e-06
Iter: 252 loss: 1.49756283e-06
Iter: 253 loss: 1.55796329e-06
Iter: 254 loss: 1.49650816e-06
Iter: 255 loss: 1.49211837e-06
Iter: 256 loss: 1.54265024e-06
Iter: 257 loss: 1.49201321e-06
Iter: 258 loss: 1.48911772e-06
Iter: 259 loss: 1.48667323e-06
Iter: 260 loss: 1.48582842e-06
Iter: 261 loss: 1.48136269e-06
Iter: 262 loss: 1.50812639e-06
Iter: 263 loss: 1.48087702e-06
Iter: 264 loss: 1.47731589e-06
Iter: 265 loss: 1.47084415e-06
Iter: 266 loss: 1.61700336e-06
Iter: 267 loss: 1.47097057e-06
Iter: 268 loss: 1.46735442e-06
Iter: 269 loss: 1.4665319e-06
Iter: 270 loss: 1.46356092e-06
Iter: 271 loss: 1.4656091e-06
Iter: 272 loss: 1.46162006e-06
Iter: 273 loss: 1.45676097e-06
Iter: 274 loss: 1.45867671e-06
Iter: 275 loss: 1.45335116e-06
Iter: 276 loss: 1.45007766e-06
Iter: 277 loss: 1.47122796e-06
Iter: 278 loss: 1.44970511e-06
Iter: 279 loss: 1.4456989e-06
Iter: 280 loss: 1.44096202e-06
Iter: 281 loss: 1.44050784e-06
Iter: 282 loss: 1.43777447e-06
Iter: 283 loss: 1.43745433e-06
Iter: 284 loss: 1.43489535e-06
Iter: 285 loss: 1.46221578e-06
Iter: 286 loss: 1.43488762e-06
Iter: 287 loss: 1.4329554e-06
Iter: 288 loss: 1.42908857e-06
Iter: 289 loss: 1.49325615e-06
Iter: 290 loss: 1.42896693e-06
Iter: 291 loss: 1.42634826e-06
Iter: 292 loss: 1.43568866e-06
Iter: 293 loss: 1.42570741e-06
Iter: 294 loss: 1.42202362e-06
Iter: 295 loss: 1.4206114e-06
Iter: 296 loss: 1.41855412e-06
Iter: 297 loss: 1.41534224e-06
Iter: 298 loss: 1.45189836e-06
Iter: 299 loss: 1.41529267e-06
Iter: 300 loss: 1.41234818e-06
Iter: 301 loss: 1.40674729e-06
Iter: 302 loss: 1.51691279e-06
Iter: 303 loss: 1.40665452e-06
Iter: 304 loss: 1.40349402e-06
Iter: 305 loss: 1.40312102e-06
Iter: 306 loss: 1.40055704e-06
Iter: 307 loss: 1.40686097e-06
Iter: 308 loss: 1.39956569e-06
Iter: 309 loss: 1.39668145e-06
Iter: 310 loss: 1.40171e-06
Iter: 311 loss: 1.39520694e-06
Iter: 312 loss: 1.39272879e-06
Iter: 313 loss: 1.3881114e-06
Iter: 314 loss: 1.49399023e-06
Iter: 315 loss: 1.3880699e-06
Iter: 316 loss: 1.38418818e-06
Iter: 317 loss: 1.38397706e-06
Iter: 318 loss: 1.38144082e-06
Iter: 319 loss: 1.37888424e-06
Iter: 320 loss: 1.3783706e-06
Iter: 321 loss: 1.37897473e-06
Iter: 322 loss: 1.37623942e-06
Iter: 323 loss: 1.37541565e-06
Iter: 324 loss: 1.37369557e-06
Iter: 325 loss: 1.40189331e-06
Iter: 326 loss: 1.37355346e-06
Iter: 327 loss: 1.37158031e-06
Iter: 328 loss: 1.36815731e-06
Iter: 329 loss: 1.36813037e-06
Iter: 330 loss: 1.36641165e-06
Iter: 331 loss: 1.36594497e-06
Iter: 332 loss: 1.36427911e-06
Iter: 333 loss: 1.36076551e-06
Iter: 334 loss: 1.41813337e-06
Iter: 335 loss: 1.36062818e-06
Iter: 336 loss: 1.35717823e-06
Iter: 337 loss: 1.39862766e-06
Iter: 338 loss: 1.35716891e-06
Iter: 339 loss: 1.35504729e-06
Iter: 340 loss: 1.35308505e-06
Iter: 341 loss: 1.35259074e-06
Iter: 342 loss: 1.34875768e-06
Iter: 343 loss: 1.3697836e-06
Iter: 344 loss: 1.34823085e-06
Iter: 345 loss: 1.34565073e-06
Iter: 346 loss: 1.35580945e-06
Iter: 347 loss: 1.34503625e-06
Iter: 348 loss: 1.34209677e-06
Iter: 349 loss: 1.34361017e-06
Iter: 350 loss: 1.3402198e-06
Iter: 351 loss: 1.33752906e-06
Iter: 352 loss: 1.36183007e-06
Iter: 353 loss: 1.33746357e-06
Iter: 354 loss: 1.33545223e-06
Iter: 355 loss: 1.33411345e-06
Iter: 356 loss: 1.33336687e-06
Iter: 357 loss: 1.33438414e-06
Iter: 358 loss: 1.33211722e-06
Iter: 359 loss: 1.33126741e-06
Iter: 360 loss: 1.32982473e-06
Iter: 361 loss: 1.35932703e-06
Iter: 362 loss: 1.3297954e-06
Iter: 363 loss: 1.32819707e-06
Iter: 364 loss: 1.32479545e-06
Iter: 365 loss: 1.38868506e-06
Iter: 366 loss: 1.32475429e-06
Iter: 367 loss: 1.3225781e-06
Iter: 368 loss: 1.32256787e-06
Iter: 369 loss: 1.32019568e-06
Iter: 370 loss: 1.32070318e-06
Iter: 371 loss: 1.31847719e-06
Iter: 372 loss: 1.31637739e-06
Iter: 373 loss: 1.33487936e-06
Iter: 374 loss: 1.31636239e-06
Iter: 375 loss: 1.31458751e-06
Iter: 376 loss: 1.31204115e-06
Iter: 377 loss: 1.31198567e-06
Iter: 378 loss: 1.30966555e-06
Iter: 379 loss: 1.34436232e-06
Iter: 380 loss: 1.30961769e-06
Iter: 381 loss: 1.3073568e-06
Iter: 382 loss: 1.30630588e-06
Iter: 383 loss: 1.30522881e-06
Iter: 384 loss: 1.30257786e-06
Iter: 385 loss: 1.30255762e-06
Iter: 386 loss: 1.30104627e-06
Iter: 387 loss: 1.29809973e-06
Iter: 388 loss: 1.35262781e-06
Iter: 389 loss: 1.29811383e-06
Iter: 390 loss: 1.29430555e-06
Iter: 391 loss: 1.33121671e-06
Iter: 392 loss: 1.29415957e-06
Iter: 393 loss: 1.29208047e-06
Iter: 394 loss: 1.30314834e-06
Iter: 395 loss: 1.29180012e-06
Iter: 396 loss: 1.28917338e-06
Iter: 397 loss: 1.30262401e-06
Iter: 398 loss: 1.28871852e-06
Iter: 399 loss: 1.28762667e-06
Iter: 400 loss: 1.28664146e-06
Iter: 401 loss: 1.2862788e-06
Iter: 402 loss: 1.28456975e-06
Iter: 403 loss: 1.28236456e-06
Iter: 404 loss: 1.28221257e-06
Iter: 405 loss: 1.27977216e-06
Iter: 406 loss: 1.31423201e-06
Iter: 407 loss: 1.27969975e-06
Iter: 408 loss: 1.27830822e-06
Iter: 409 loss: 1.2793239e-06
Iter: 410 loss: 1.27741089e-06
Iter: 411 loss: 1.27514249e-06
Iter: 412 loss: 1.28408647e-06
Iter: 413 loss: 1.27462192e-06
Iter: 414 loss: 1.27317435e-06
Iter: 415 loss: 1.27205908e-06
Iter: 416 loss: 1.27158614e-06
Iter: 417 loss: 1.26851319e-06
Iter: 418 loss: 1.27875296e-06
Iter: 419 loss: 1.26762416e-06
Iter: 420 loss: 1.26576867e-06
Iter: 421 loss: 1.27860687e-06
Iter: 422 loss: 1.26563566e-06
Iter: 423 loss: 1.26343207e-06
Iter: 424 loss: 1.26504062e-06
Iter: 425 loss: 1.2621507e-06
Iter: 426 loss: 1.26077396e-06
Iter: 427 loss: 1.26715531e-06
Iter: 428 loss: 1.26050577e-06
Iter: 429 loss: 1.25943075e-06
Iter: 430 loss: 1.27672433e-06
Iter: 431 loss: 1.25945189e-06
Iter: 432 loss: 1.2581736e-06
Iter: 433 loss: 1.256524e-06
Iter: 434 loss: 1.25632107e-06
Iter: 435 loss: 1.2553844e-06
Iter: 436 loss: 1.25439726e-06
Iter: 437 loss: 1.25416705e-06
Iter: 438 loss: 1.25175904e-06
Iter: 439 loss: 1.254376e-06
Iter: 440 loss: 1.25045767e-06
Iter: 441 loss: 1.24916869e-06
Iter: 442 loss: 1.26779753e-06
Iter: 443 loss: 1.24921905e-06
Iter: 444 loss: 1.24799635e-06
Iter: 445 loss: 1.24694657e-06
Iter: 446 loss: 1.24664018e-06
Iter: 447 loss: 1.24511064e-06
Iter: 448 loss: 1.26642067e-06
Iter: 449 loss: 1.24514304e-06
Iter: 450 loss: 1.24407518e-06
Iter: 451 loss: 1.24282633e-06
Iter: 452 loss: 1.24266421e-06
Iter: 453 loss: 1.24088911e-06
Iter: 454 loss: 1.25605834e-06
Iter: 455 loss: 1.24080918e-06
Iter: 456 loss: 1.23962741e-06
Iter: 457 loss: 1.23985956e-06
Iter: 458 loss: 1.23862333e-06
Iter: 459 loss: 1.23679558e-06
Iter: 460 loss: 1.2507985e-06
Iter: 461 loss: 1.23668133e-06
Iter: 462 loss: 1.23568043e-06
Iter: 463 loss: 1.2357641e-06
Iter: 464 loss: 1.23497443e-06
Iter: 465 loss: 1.23378527e-06
Iter: 466 loss: 1.23377481e-06
Iter: 467 loss: 1.23276993e-06
Iter: 468 loss: 1.23067878e-06
Iter: 469 loss: 1.26000759e-06
Iter: 470 loss: 1.23056748e-06
Iter: 471 loss: 1.22924416e-06
Iter: 472 loss: 1.22996869e-06
Iter: 473 loss: 1.22844972e-06
Iter: 474 loss: 1.22649419e-06
Iter: 475 loss: 1.23481959e-06
Iter: 476 loss: 1.22616132e-06
Iter: 477 loss: 1.22494316e-06
Iter: 478 loss: 1.22475308e-06
Iter: 479 loss: 1.2239318e-06
Iter: 480 loss: 1.22228425e-06
Iter: 481 loss: 1.23951315e-06
Iter: 482 loss: 1.22219933e-06
Iter: 483 loss: 1.22118377e-06
Iter: 484 loss: 1.22354675e-06
Iter: 485 loss: 1.22075869e-06
Iter: 486 loss: 1.21928156e-06
Iter: 487 loss: 1.21945914e-06
Iter: 488 loss: 1.21810911e-06
Iter: 489 loss: 1.21708308e-06
Iter: 490 loss: 1.22896881e-06
Iter: 491 loss: 1.2170749e-06
Iter: 492 loss: 1.21629341e-06
Iter: 493 loss: 1.21653829e-06
Iter: 494 loss: 1.21568814e-06
Iter: 495 loss: 1.21477524e-06
Iter: 496 loss: 1.22231609e-06
Iter: 497 loss: 1.21471385e-06
Iter: 498 loss: 1.21406151e-06
Iter: 499 loss: 1.21491507e-06
Iter: 500 loss: 1.21365338e-06
Iter: 501 loss: 1.21249968e-06
Iter: 502 loss: 1.21784615e-06
Iter: 503 loss: 1.21230198e-06
Iter: 504 loss: 1.21186304e-06
Iter: 505 loss: 1.21117262e-06
Iter: 506 loss: 1.21112771e-06
Iter: 507 loss: 1.21011192e-06
Iter: 508 loss: 1.20896948e-06
Iter: 509 loss: 1.20880486e-06
Iter: 510 loss: 1.20767277e-06
Iter: 511 loss: 1.21687299e-06
Iter: 512 loss: 1.20758205e-06
Iter: 513 loss: 1.20630307e-06
Iter: 514 loss: 1.20855964e-06
Iter: 515 loss: 1.20576226e-06
Iter: 516 loss: 1.20494e-06
Iter: 517 loss: 1.21301912e-06
Iter: 518 loss: 1.20484992e-06
Iter: 519 loss: 1.20406571e-06
Iter: 520 loss: 1.20472532e-06
Iter: 521 loss: 1.20368384e-06
Iter: 522 loss: 1.20297966e-06
Iter: 523 loss: 1.20931395e-06
Iter: 524 loss: 1.20298193e-06
Iter: 525 loss: 1.20248478e-06
Iter: 526 loss: 1.20168954e-06
Iter: 527 loss: 1.20170375e-06
Iter: 528 loss: 1.20080199e-06
Iter: 529 loss: 1.20087736e-06
Iter: 530 loss: 1.20033019e-06
Iter: 531 loss: 1.20099594e-06
Iter: 532 loss: 1.2000794e-06
Iter: 533 loss: 1.19948811e-06
Iter: 534 loss: 1.20733489e-06
Iter: 535 loss: 1.199503e-06
Iter: 536 loss: 1.19901119e-06
Iter: 537 loss: 1.19792367e-06
Iter: 538 loss: 1.2165965e-06
Iter: 539 loss: 1.19799347e-06
Iter: 540 loss: 1.19730271e-06
Iter: 541 loss: 1.19759693e-06
Iter: 542 loss: 1.19681863e-06
Iter: 543 loss: 1.19588481e-06
Iter: 544 loss: 1.20010623e-06
Iter: 545 loss: 1.19583069e-06
Iter: 546 loss: 1.19506331e-06
Iter: 547 loss: 1.19415756e-06
Iter: 548 loss: 1.19407628e-06
Iter: 549 loss: 1.19271795e-06
Iter: 550 loss: 1.20736286e-06
Iter: 551 loss: 1.19269384e-06
Iter: 552 loss: 1.19208062e-06
Iter: 553 loss: 1.19156766e-06
Iter: 554 loss: 1.19133188e-06
Iter: 555 loss: 1.19028573e-06
Iter: 556 loss: 1.20450761e-06
Iter: 557 loss: 1.19027516e-06
Iter: 558 loss: 1.18969763e-06
Iter: 559 loss: 1.19015272e-06
Iter: 560 loss: 1.18941659e-06
Iter: 561 loss: 1.18859259e-06
Iter: 562 loss: 1.18862386e-06
Iter: 563 loss: 1.187875e-06
Iter: 564 loss: 1.18720959e-06
Iter: 565 loss: 1.18714934e-06
Iter: 566 loss: 1.18654157e-06
Iter: 567 loss: 1.18612093e-06
Iter: 568 loss: 1.18595949e-06
Iter: 569 loss: 1.18575463e-06
Iter: 570 loss: 1.18550429e-06
Iter: 571 loss: 1.18522939e-06
Iter: 572 loss: 1.18599348e-06
Iter: 573 loss: 1.18507978e-06
Iter: 574 loss: 1.18480079e-06
Iter: 575 loss: 1.18402204e-06
Iter: 576 loss: 1.18584126e-06
Iter: 577 loss: 1.18359173e-06
Iter: 578 loss: 1.18261823e-06
Iter: 579 loss: 1.1972578e-06
Iter: 580 loss: 1.18261642e-06
Iter: 581 loss: 1.18207231e-06
Iter: 582 loss: 1.18116839e-06
Iter: 583 loss: 1.18109392e-06
Iter: 584 loss: 1.18036303e-06
Iter: 585 loss: 1.19259323e-06
Iter: 586 loss: 1.18032744e-06
Iter: 587 loss: 1.17964248e-06
Iter: 588 loss: 1.17936042e-06
Iter: 589 loss: 1.17894729e-06
Iter: 590 loss: 1.17837465e-06
Iter: 591 loss: 1.18314802e-06
Iter: 592 loss: 1.17824572e-06
Iter: 593 loss: 1.1776358e-06
Iter: 594 loss: 1.17769468e-06
Iter: 595 loss: 1.1771258e-06
Iter: 596 loss: 1.17642685e-06
Iter: 597 loss: 1.18274647e-06
Iter: 598 loss: 1.1764472e-06
Iter: 599 loss: 1.17603872e-06
Iter: 600 loss: 1.17576508e-06
Iter: 601 loss: 1.17564491e-06
Iter: 602 loss: 1.17507568e-06
Iter: 603 loss: 1.18034893e-06
Iter: 604 loss: 1.17505499e-06
Iter: 605 loss: 1.17468358e-06
Iter: 606 loss: 1.17561285e-06
Iter: 607 loss: 1.174521e-06
Iter: 608 loss: 1.17412367e-06
Iter: 609 loss: 1.17437082e-06
Iter: 610 loss: 1.17391153e-06
Iter: 611 loss: 1.17338027e-06
Iter: 612 loss: 1.17280672e-06
Iter: 613 loss: 1.17272657e-06
Iter: 614 loss: 1.17214097e-06
Iter: 615 loss: 1.17213506e-06
Iter: 616 loss: 1.17168611e-06
Iter: 617 loss: 1.17090917e-06
Iter: 618 loss: 1.18619573e-06
Iter: 619 loss: 1.1708662e-06
Iter: 620 loss: 1.17020818e-06
Iter: 621 loss: 1.17019249e-06
Iter: 622 loss: 1.16974149e-06
Iter: 623 loss: 1.16875117e-06
Iter: 624 loss: 1.18326489e-06
Iter: 625 loss: 1.16882404e-06
Iter: 626 loss: 1.16803949e-06
Iter: 627 loss: 1.16806041e-06
Iter: 628 loss: 1.16725062e-06
Iter: 629 loss: 1.16697481e-06
Iter: 630 loss: 1.16663296e-06
Iter: 631 loss: 1.16576075e-06
Iter: 632 loss: 1.16905039e-06
Iter: 633 loss: 1.16550416e-06
Iter: 634 loss: 1.16462297e-06
Iter: 635 loss: 1.17554043e-06
Iter: 636 loss: 1.1646282e-06
Iter: 637 loss: 1.16391629e-06
Iter: 638 loss: 1.17080435e-06
Iter: 639 loss: 1.16383035e-06
Iter: 640 loss: 1.1635525e-06
Iter: 641 loss: 1.1632153e-06
Iter: 642 loss: 1.16317619e-06
Iter: 643 loss: 1.16254034e-06
Iter: 644 loss: 1.16232195e-06
Iter: 645 loss: 1.16190336e-06
Iter: 646 loss: 1.16123624e-06
Iter: 647 loss: 1.1712134e-06
Iter: 648 loss: 1.16125216e-06
Iter: 649 loss: 1.1608206e-06
Iter: 650 loss: 1.1602699e-06
Iter: 651 loss: 1.16022898e-06
Iter: 652 loss: 1.15920102e-06
Iter: 653 loss: 1.16235924e-06
Iter: 654 loss: 1.15891976e-06
Iter: 655 loss: 1.15818079e-06
Iter: 656 loss: 1.15918931e-06
Iter: 657 loss: 1.15776504e-06
Iter: 658 loss: 1.15672071e-06
Iter: 659 loss: 1.1587083e-06
Iter: 660 loss: 1.15620094e-06
Iter: 661 loss: 1.15557714e-06
Iter: 662 loss: 1.15809223e-06
Iter: 663 loss: 1.15538364e-06
Iter: 664 loss: 1.15435159e-06
Iter: 665 loss: 1.15437388e-06
Iter: 666 loss: 1.15368971e-06
Iter: 667 loss: 1.15301611e-06
Iter: 668 loss: 1.15298599e-06
Iter: 669 loss: 1.15257376e-06
Iter: 670 loss: 1.15920136e-06
Iter: 671 loss: 1.15255625e-06
Iter: 672 loss: 1.15214516e-06
Iter: 673 loss: 1.1513448e-06
Iter: 674 loss: 1.15137357e-06
Iter: 675 loss: 1.15075852e-06
Iter: 676 loss: 1.15400826e-06
Iter: 677 loss: 1.15067246e-06
Iter: 678 loss: 1.15019429e-06
Iter: 679 loss: 1.14948057e-06
Iter: 680 loss: 1.14949717e-06
Iter: 681 loss: 1.14864679e-06
Iter: 682 loss: 1.15972023e-06
Iter: 683 loss: 1.14857812e-06
Iter: 684 loss: 1.14807813e-06
Iter: 685 loss: 1.14784768e-06
Iter: 686 loss: 1.14753675e-06
Iter: 687 loss: 1.14654006e-06
Iter: 688 loss: 1.14942804e-06
Iter: 689 loss: 1.14615307e-06
Iter: 690 loss: 1.14551517e-06
Iter: 691 loss: 1.14784837e-06
Iter: 692 loss: 1.14527927e-06
Iter: 693 loss: 1.1444198e-06
Iter: 694 loss: 1.1439713e-06
Iter: 695 loss: 1.14359386e-06
Iter: 696 loss: 1.14277964e-06
Iter: 697 loss: 1.14442776e-06
Iter: 698 loss: 1.14237116e-06
Iter: 699 loss: 1.14118325e-06
Iter: 700 loss: 1.14739669e-06
Iter: 701 loss: 1.14097816e-06
Iter: 702 loss: 1.14054751e-06
Iter: 703 loss: 1.1460794e-06
Iter: 704 loss: 1.1405341e-06
Iter: 705 loss: 1.13988415e-06
Iter: 706 loss: 1.14123554e-06
Iter: 707 loss: 1.13968053e-06
Iter: 708 loss: 1.13911142e-06
Iter: 709 loss: 1.13889905e-06
Iter: 710 loss: 1.13857811e-06
Iter: 711 loss: 1.13805152e-06
Iter: 712 loss: 1.138725e-06
Iter: 713 loss: 1.13779163e-06
Iter: 714 loss: 1.13684587e-06
Iter: 715 loss: 1.13744829e-06
Iter: 716 loss: 1.13625515e-06
Iter: 717 loss: 1.13573412e-06
Iter: 718 loss: 1.13574151e-06
Iter: 719 loss: 1.13530382e-06
Iter: 720 loss: 1.13445469e-06
Iter: 721 loss: 1.15368312e-06
Iter: 722 loss: 1.13449585e-06
Iter: 723 loss: 1.13321391e-06
Iter: 724 loss: 1.14077659e-06
Iter: 725 loss: 1.13306817e-06
Iter: 726 loss: 1.132394e-06
Iter: 727 loss: 1.13200281e-06
Iter: 728 loss: 1.13170495e-06
Iter: 729 loss: 1.1305857e-06
Iter: 730 loss: 1.13996634e-06
Iter: 731 loss: 1.13046815e-06
Iter: 732 loss: 1.12974601e-06
Iter: 733 loss: 1.12892201e-06
Iter: 734 loss: 1.12882685e-06
Iter: 735 loss: 1.12776e-06
Iter: 736 loss: 1.14290401e-06
Iter: 737 loss: 1.12766622e-06
Iter: 738 loss: 1.12718772e-06
Iter: 739 loss: 1.13308715e-06
Iter: 740 loss: 1.12724899e-06
Iter: 741 loss: 1.12656994e-06
Iter: 742 loss: 1.12603266e-06
Iter: 743 loss: 1.12585644e-06
Iter: 744 loss: 1.1251534e-06
Iter: 745 loss: 1.12712132e-06
Iter: 746 loss: 1.12490136e-06
Iter: 747 loss: 1.12439125e-06
Iter: 748 loss: 1.12439852e-06
Iter: 749 loss: 1.12396788e-06
Iter: 750 loss: 1.12307e-06
Iter: 751 loss: 1.12553812e-06
Iter: 752 loss: 1.1227155e-06
Iter: 753 loss: 1.12209023e-06
Iter: 754 loss: 1.12588839e-06
Iter: 755 loss: 1.12197699e-06
Iter: 756 loss: 1.12127145e-06
Iter: 757 loss: 1.12046837e-06
Iter: 758 loss: 1.12034638e-06
Iter: 759 loss: 1.11949839e-06
Iter: 760 loss: 1.13246881e-06
Iter: 761 loss: 1.11948282e-06
Iter: 762 loss: 1.1189743e-06
Iter: 763 loss: 1.11786824e-06
Iter: 764 loss: 1.13464318e-06
Iter: 765 loss: 1.11793065e-06
Iter: 766 loss: 1.11703855e-06
Iter: 767 loss: 1.1169077e-06
Iter: 768 loss: 1.11632744e-06
Iter: 769 loss: 1.11526288e-06
Iter: 770 loss: 1.13942065e-06
Iter: 771 loss: 1.11520194e-06
Iter: 772 loss: 1.11386805e-06
Iter: 773 loss: 1.12730595e-06
Iter: 774 loss: 1.11381212e-06
Iter: 775 loss: 1.11317297e-06
Iter: 776 loss: 1.11811346e-06
Iter: 777 loss: 1.11312215e-06
Iter: 778 loss: 1.11221175e-06
Iter: 779 loss: 1.11387772e-06
Iter: 780 loss: 1.11181271e-06
Iter: 781 loss: 1.11156442e-06
Iter: 782 loss: 1.11102668e-06
Iter: 783 loss: 1.11107806e-06
Iter: 784 loss: 1.11028419e-06
Iter: 785 loss: 1.11047177e-06
Iter: 786 loss: 1.10977408e-06
Iter: 787 loss: 1.10930705e-06
Iter: 788 loss: 1.10920973e-06
Iter: 789 loss: 1.10885867e-06
Iter: 790 loss: 1.10799078e-06
Iter: 791 loss: 1.1171619e-06
Iter: 792 loss: 1.10788e-06
Iter: 793 loss: 1.1073e-06
Iter: 794 loss: 1.10721135e-06
Iter: 795 loss: 1.10669362e-06
Iter: 796 loss: 1.10619067e-06
Iter: 797 loss: 1.10614246e-06
Iter: 798 loss: 1.10516066e-06
Iter: 799 loss: 1.10927908e-06
Iter: 800 loss: 1.10497911e-06
Iter: 801 loss: 1.10436724e-06
Iter: 802 loss: 1.10461224e-06
Iter: 803 loss: 1.10392978e-06
Iter: 804 loss: 1.10315477e-06
Iter: 805 loss: 1.10847782e-06
Iter: 806 loss: 1.1030354e-06
Iter: 807 loss: 1.10249061e-06
Iter: 808 loss: 1.10222538e-06
Iter: 809 loss: 1.10202586e-06
Iter: 810 loss: 1.10172778e-06
Iter: 811 loss: 1.10151677e-06
Iter: 812 loss: 1.10115593e-06
Iter: 813 loss: 1.10137012e-06
Iter: 814 loss: 1.10094436e-06
Iter: 815 loss: 1.10064468e-06
Iter: 816 loss: 1.10020039e-06
Iter: 817 loss: 1.11006557e-06
Iter: 818 loss: 1.10016526e-06
Iter: 819 loss: 1.09942755e-06
Iter: 820 loss: 1.10210249e-06
Iter: 821 loss: 1.09923462e-06
Iter: 822 loss: 1.09873781e-06
Iter: 823 loss: 1.10018721e-06
Iter: 824 loss: 1.09856569e-06
Iter: 825 loss: 1.09781331e-06
Iter: 826 loss: 1.09797941e-06
Iter: 827 loss: 1.09732878e-06
Iter: 828 loss: 1.0966977e-06
Iter: 829 loss: 1.10217877e-06
Iter: 830 loss: 1.09671873e-06
Iter: 831 loss: 1.09611324e-06
Iter: 832 loss: 1.09693042e-06
Iter: 833 loss: 1.09586585e-06
Iter: 834 loss: 1.09534335e-06
Iter: 835 loss: 1.09850771e-06
Iter: 836 loss: 1.09524808e-06
Iter: 837 loss: 1.09496659e-06
Iter: 838 loss: 1.09409541e-06
Iter: 839 loss: 1.10409292e-06
Iter: 840 loss: 1.09400571e-06
Iter: 841 loss: 1.09338293e-06
Iter: 842 loss: 1.09329221e-06
Iter: 843 loss: 1.09292068e-06
Iter: 844 loss: 1.09222856e-06
Iter: 845 loss: 1.09216717e-06
Iter: 846 loss: 1.09206155e-06
Iter: 847 loss: 1.09181178e-06
Iter: 848 loss: 1.09138443e-06
Iter: 849 loss: 1.09251368e-06
Iter: 850 loss: 1.09124437e-06
Iter: 851 loss: 1.09096607e-06
Iter: 852 loss: 1.09017424e-06
Iter: 853 loss: 1.09210407e-06
Iter: 854 loss: 1.08970107e-06
Iter: 855 loss: 1.08871427e-06
Iter: 856 loss: 1.08865606e-06
Iter: 857 loss: 1.08807274e-06
Iter: 858 loss: 1.08901338e-06
Iter: 859 loss: 1.08774384e-06
Iter: 860 loss: 1.08709389e-06
Iter: 861 loss: 1.09076382e-06
Iter: 862 loss: 1.08696599e-06
Iter: 863 loss: 1.08644076e-06
Iter: 864 loss: 1.08625875e-06
Iter: 865 loss: 1.08591712e-06
Iter: 866 loss: 1.08482118e-06
Iter: 867 loss: 1.08740846e-06
Iter: 868 loss: 1.08446307e-06
Iter: 869 loss: 1.08371319e-06
Iter: 870 loss: 1.08631093e-06
Iter: 871 loss: 1.08359518e-06
Iter: 872 loss: 1.08280392e-06
Iter: 873 loss: 1.08245979e-06
Iter: 874 loss: 1.08202221e-06
Iter: 875 loss: 1.08116228e-06
Iter: 876 loss: 1.08396182e-06
Iter: 877 loss: 1.08096128e-06
Iter: 878 loss: 1.08020936e-06
Iter: 879 loss: 1.08464917e-06
Iter: 880 loss: 1.08001746e-06
Iter: 881 loss: 1.07995811e-06
Iter: 882 loss: 1.0798185e-06
Iter: 883 loss: 1.07957771e-06
Iter: 884 loss: 1.07886729e-06
Iter: 885 loss: 1.08007771e-06
Iter: 886 loss: 1.07839799e-06
Iter: 887 loss: 1.0775301e-06
Iter: 888 loss: 1.0857882e-06
Iter: 889 loss: 1.07748565e-06
Iter: 890 loss: 1.07689584e-06
Iter: 891 loss: 1.07621577e-06
Iter: 892 loss: 1.07615108e-06
Iter: 893 loss: 1.07546498e-06
Iter: 894 loss: 1.07537733e-06
Iter: 895 loss: 1.07485755e-06
Iter: 896 loss: 1.07502694e-06
Iter: 897 loss: 1.07454764e-06
Iter: 898 loss: 1.07362177e-06
Iter: 899 loss: 1.07445908e-06
Iter: 900 loss: 1.0731161e-06
Iter: 901 loss: 1.0725081e-06
Iter: 902 loss: 1.07476421e-06
Iter: 903 loss: 1.07232154e-06
Iter: 904 loss: 1.07150163e-06
Iter: 905 loss: 1.07372648e-06
Iter: 906 loss: 1.07132973e-06
Iter: 907 loss: 1.07068718e-06
Iter: 908 loss: 1.07311894e-06
Iter: 909 loss: 1.07052097e-06
Iter: 910 loss: 1.06997311e-06
Iter: 911 loss: 1.06921448e-06
Iter: 912 loss: 1.06923085e-06
Iter: 913 loss: 1.06961113e-06
Iter: 914 loss: 1.06896937e-06
Iter: 915 loss: 1.06862637e-06
Iter: 916 loss: 1.06841162e-06
Iter: 917 loss: 1.06831362e-06
Iter: 918 loss: 1.06786706e-06
Iter: 919 loss: 1.06683183e-06
Iter: 920 loss: 1.08407107e-06
Iter: 921 loss: 1.06681443e-06
Iter: 922 loss: 1.06600464e-06
Iter: 923 loss: 1.07165965e-06
Iter: 924 loss: 1.06590119e-06
Iter: 925 loss: 1.06506423e-06
Iter: 926 loss: 1.06709399e-06
Iter: 927 loss: 1.06483117e-06
Iter: 928 loss: 1.06433686e-06
Iter: 929 loss: 1.06947073e-06
Iter: 930 loss: 1.06430707e-06
Iter: 931 loss: 1.06371431e-06
Iter: 932 loss: 1.06295192e-06
Iter: 933 loss: 1.06290122e-06
Iter: 934 loss: 1.06231607e-06
Iter: 935 loss: 1.07017013e-06
Iter: 936 loss: 1.06229891e-06
Iter: 937 loss: 1.06168841e-06
Iter: 938 loss: 1.06138668e-06
Iter: 939 loss: 1.06114669e-06
Iter: 940 loss: 1.0603153e-06
Iter: 941 loss: 1.06915388e-06
Iter: 942 loss: 1.06024356e-06
Iter: 943 loss: 1.05993774e-06
Iter: 944 loss: 1.05936101e-06
Iter: 945 loss: 1.06945913e-06
Iter: 946 loss: 1.05938113e-06
Iter: 947 loss: 1.05886284e-06
Iter: 948 loss: 1.05880918e-06
Iter: 949 loss: 1.0584331e-06
Iter: 950 loss: 1.06451967e-06
Iter: 951 loss: 1.0584306e-06
Iter: 952 loss: 1.05827769e-06
Iter: 953 loss: 1.05785955e-06
Iter: 954 loss: 1.05905144e-06
Iter: 955 loss: 1.05765105e-06
Iter: 956 loss: 1.05693141e-06
Iter: 957 loss: 1.05914194e-06
Iter: 958 loss: 1.05677282e-06
Iter: 959 loss: 1.05619938e-06
Iter: 960 loss: 1.05619483e-06
Iter: 961 loss: 1.05575896e-06
Iter: 962 loss: 1.0551978e-06
Iter: 963 loss: 1.05524293e-06
Iter: 964 loss: 1.05493916e-06
Iter: 965 loss: 1.05540016e-06
Iter: 966 loss: 1.05481911e-06
Iter: 967 loss: 1.05431673e-06
Iter: 968 loss: 1.05402739e-06
Iter: 969 loss: 1.05383322e-06
Iter: 970 loss: 1.05336107e-06
Iter: 971 loss: 1.05655772e-06
Iter: 972 loss: 1.05331424e-06
Iter: 973 loss: 1.05275217e-06
Iter: 974 loss: 1.05323579e-06
Iter: 975 loss: 1.05242884e-06
Iter: 976 loss: 1.052073e-06
Iter: 977 loss: 1.05820754e-06
Iter: 978 loss: 1.05202776e-06
Iter: 979 loss: 1.05179333e-06
Iter: 980 loss: 1.05139907e-06
Iter: 981 loss: 1.05134e-06
Iter: 982 loss: 1.05123354e-06
Iter: 983 loss: 1.05111258e-06
Iter: 984 loss: 1.05086542e-06
Iter: 985 loss: 1.05105755e-06
Iter: 986 loss: 1.05075912e-06
Iter: 987 loss: 1.05060883e-06
Iter: 988 loss: 1.05018751e-06
Iter: 989 loss: 1.0509782e-06
Iter: 990 loss: 1.04991614e-06
Iter: 991 loss: 1.0496077e-06
Iter: 992 loss: 1.049503e-06
Iter: 993 loss: 1.04919593e-06
Iter: 994 loss: 1.04893309e-06
Iter: 995 loss: 1.04890603e-06
Iter: 996 loss: 1.04842343e-06
Iter: 997 loss: 1.05058621e-06
Iter: 998 loss: 1.04834271e-06
Iter: 999 loss: 1.04789513e-06
Iter: 1000 loss: 1.04858077e-06
Iter: 1001 loss: 1.04773926e-06
Iter: 1002 loss: 1.04735182e-06
Iter: 1003 loss: 1.04824232e-06
Iter: 1004 loss: 1.04719527e-06
Iter: 1005 loss: 1.0468807e-06
Iter: 1006 loss: 1.04794469e-06
Iter: 1007 loss: 1.04678918e-06
Iter: 1008 loss: 1.04642197e-06
Iter: 1009 loss: 1.04667254e-06
Iter: 1010 loss: 1.0462345e-06
Iter: 1011 loss: 1.04590481e-06
Iter: 1012 loss: 1.04676474e-06
Iter: 1013 loss: 1.04578271e-06
Iter: 1014 loss: 1.04551737e-06
Iter: 1015 loss: 1.04643073e-06
Iter: 1016 loss: 1.04536809e-06
Iter: 1017 loss: 1.045094e-06
Iter: 1018 loss: 1.04511992e-06
Iter: 1019 loss: 1.045006e-06
Iter: 1020 loss: 1.04466881e-06
Iter: 1021 loss: 1.04695e-06
Iter: 1022 loss: 1.04452965e-06
Iter: 1023 loss: 1.04401249e-06
Iter: 1024 loss: 1.04514447e-06
Iter: 1025 loss: 1.04379865e-06
Iter: 1026 loss: 1.04349135e-06
Iter: 1027 loss: 1.04658329e-06
Iter: 1028 loss: 1.04351079e-06
Iter: 1029 loss: 1.04319088e-06
Iter: 1030 loss: 1.0427085e-06
Iter: 1031 loss: 1.04268088e-06
Iter: 1032 loss: 1.04240166e-06
Iter: 1033 loss: 1.04235323e-06
Iter: 1034 loss: 1.04213609e-06
Iter: 1035 loss: 1.04163541e-06
Iter: 1036 loss: 1.04908599e-06
Iter: 1037 loss: 1.04165701e-06
Iter: 1038 loss: 1.04113906e-06
Iter: 1039 loss: 1.04113951e-06
Iter: 1040 loss: 1.04083654e-06
Iter: 1041 loss: 1.04088292e-06
Iter: 1042 loss: 1.04058972e-06
Iter: 1043 loss: 1.04010542e-06
Iter: 1044 loss: 1.04204219e-06
Iter: 1045 loss: 1.03996433e-06
Iter: 1046 loss: 1.03966829e-06
Iter: 1047 loss: 1.04059211e-06
Iter: 1048 loss: 1.03959701e-06
Iter: 1049 loss: 1.03925152e-06
Iter: 1050 loss: 1.04037656e-06
Iter: 1051 loss: 1.03911657e-06
Iter: 1052 loss: 1.03870491e-06
Iter: 1053 loss: 1.04276035e-06
Iter: 1054 loss: 1.0386924e-06
Iter: 1055 loss: 1.03864284e-06
Iter: 1056 loss: 1.03831712e-06
Iter: 1057 loss: 1.03907041e-06
Iter: 1058 loss: 1.03817376e-06
Iter: 1059 loss: 1.03772948e-06
Iter: 1060 loss: 1.04128537e-06
Iter: 1061 loss: 1.03764773e-06
Iter: 1062 loss: 1.03738194e-06
Iter: 1063 loss: 1.03882462e-06
Iter: 1064 loss: 1.03734192e-06
Iter: 1065 loss: 1.03701791e-06
Iter: 1066 loss: 1.03658726e-06
Iter: 1067 loss: 1.03661091e-06
Iter: 1068 loss: 1.03610614e-06
Iter: 1069 loss: 1.03617572e-06
Iter: 1070 loss: 1.03590014e-06
Iter: 1071 loss: 1.0354504e-06
Iter: 1072 loss: 1.04579715e-06
Iter: 1073 loss: 1.03544198e-06
Iter: 1074 loss: 1.03505715e-06
Iter: 1075 loss: 1.03499008e-06
Iter: 1076 loss: 1.03477566e-06
Iter: 1077 loss: 1.03444586e-06
Iter: 1078 loss: 1.03444177e-06
Iter: 1079 loss: 1.03395587e-06
Iter: 1080 loss: 1.03709476e-06
Iter: 1081 loss: 1.03384264e-06
Iter: 1082 loss: 1.03351454e-06
Iter: 1083 loss: 1.03799562e-06
Iter: 1084 loss: 1.0335059e-06
Iter: 1085 loss: 1.03328625e-06
Iter: 1086 loss: 1.03493062e-06
Iter: 1087 loss: 1.03332673e-06
Iter: 1088 loss: 1.03303955e-06
Iter: 1089 loss: 1.03274681e-06
Iter: 1090 loss: 1.03276932e-06
Iter: 1091 loss: 1.03244793e-06
Iter: 1092 loss: 1.03203752e-06
Iter: 1093 loss: 1.03198e-06
Iter: 1094 loss: 1.03127707e-06
Iter: 1095 loss: 1.0353649e-06
Iter: 1096 loss: 1.03117645e-06
Iter: 1097 loss: 1.03087973e-06
Iter: 1098 loss: 1.03434797e-06
Iter: 1099 loss: 1.03084244e-06
Iter: 1100 loss: 1.03061484e-06
Iter: 1101 loss: 1.03002503e-06
Iter: 1102 loss: 1.04185665e-06
Iter: 1103 loss: 1.0300754e-06
Iter: 1104 loss: 1.02959041e-06
Iter: 1105 loss: 1.03485922e-06
Iter: 1106 loss: 1.02960041e-06
Iter: 1107 loss: 1.02916169e-06
Iter: 1108 loss: 1.02867421e-06
Iter: 1109 loss: 1.02859963e-06
Iter: 1110 loss: 1.02810463e-06
Iter: 1111 loss: 1.03151319e-06
Iter: 1112 loss: 1.02808735e-06
Iter: 1113 loss: 1.02750971e-06
Iter: 1114 loss: 1.02784873e-06
Iter: 1115 loss: 1.02715524e-06
Iter: 1116 loss: 1.0266956e-06
Iter: 1117 loss: 1.02986519e-06
Iter: 1118 loss: 1.0267056e-06
Iter: 1119 loss: 1.02645436e-06
Iter: 1120 loss: 1.02872696e-06
Iter: 1121 loss: 1.02634317e-06
Iter: 1122 loss: 1.02593663e-06
Iter: 1123 loss: 1.02625529e-06
Iter: 1124 loss: 1.02563399e-06
Iter: 1125 loss: 1.02535523e-06
Iter: 1126 loss: 1.02569686e-06
Iter: 1127 loss: 1.02518743e-06
Iter: 1128 loss: 1.02488286e-06
Iter: 1129 loss: 1.02451736e-06
Iter: 1130 loss: 1.02452941e-06
Iter: 1131 loss: 1.02410309e-06
Iter: 1132 loss: 1.02964782e-06
Iter: 1133 loss: 1.02407694e-06
Iter: 1134 loss: 1.02381159e-06
Iter: 1135 loss: 1.023295e-06
Iter: 1136 loss: 1.0359704e-06
Iter: 1137 loss: 1.02331205e-06
Iter: 1138 loss: 1.0230292e-06
Iter: 1139 loss: 1.0229719e-06
Iter: 1140 loss: 1.02266915e-06
Iter: 1141 loss: 1.02213994e-06
Iter: 1142 loss: 1.03376897e-06
Iter: 1143 loss: 1.02208992e-06
Iter: 1144 loss: 1.02136244e-06
Iter: 1145 loss: 1.02628655e-06
Iter: 1146 loss: 1.02123772e-06
Iter: 1147 loss: 1.02094486e-06
Iter: 1148 loss: 1.02106196e-06
Iter: 1149 loss: 1.02067315e-06
Iter: 1150 loss: 1.02020715e-06
Iter: 1151 loss: 1.02249157e-06
Iter: 1152 loss: 1.02008482e-06
Iter: 1153 loss: 1.01979833e-06
Iter: 1154 loss: 1.02004867e-06
Iter: 1155 loss: 1.01957312e-06
Iter: 1156 loss: 1.01909302e-06
Iter: 1157 loss: 1.0207425e-06
Iter: 1158 loss: 1.0189226e-06
Iter: 1159 loss: 1.01907517e-06
Iter: 1160 loss: 1.01882233e-06
Iter: 1161 loss: 1.01866124e-06
Iter: 1162 loss: 1.01838759e-06
Iter: 1163 loss: 1.02050763e-06
Iter: 1164 loss: 1.01829687e-06
Iter: 1165 loss: 1.01789476e-06
Iter: 1166 loss: 1.01831267e-06
Iter: 1167 loss: 1.01769137e-06
Iter: 1168 loss: 1.01732303e-06
Iter: 1169 loss: 1.01989303e-06
Iter: 1170 loss: 1.0173344e-06
Iter: 1171 loss: 1.01692513e-06
Iter: 1172 loss: 1.01638341e-06
Iter: 1173 loss: 1.01639091e-06
Iter: 1174 loss: 1.01597868e-06
Iter: 1175 loss: 1.01595811e-06
Iter: 1176 loss: 1.0156657e-06
Iter: 1177 loss: 1.01567207e-06
Iter: 1178 loss: 1.01550586e-06
Iter: 1179 loss: 1.01508022e-06
Iter: 1180 loss: 1.0173535e-06
Iter: 1181 loss: 1.01503656e-06
Iter: 1182 loss: 1.01484147e-06
Iter: 1183 loss: 1.01462524e-06
Iter: 1184 loss: 1.01465059e-06
Iter: 1185 loss: 1.01414889e-06
Iter: 1186 loss: 1.01558e-06
Iter: 1187 loss: 1.01399723e-06
Iter: 1188 loss: 1.01367311e-06
Iter: 1189 loss: 1.01481135e-06
Iter: 1190 loss: 1.01358455e-06
Iter: 1191 loss: 1.01316687e-06
Iter: 1192 loss: 1.01447e-06
Iter: 1193 loss: 1.01305022e-06
Iter: 1194 loss: 1.01259877e-06
Iter: 1195 loss: 1.01797082e-06
Iter: 1196 loss: 1.01259036e-06
Iter: 1197 loss: 1.01252226e-06
Iter: 1198 loss: 1.01221872e-06
Iter: 1199 loss: 1.01622868e-06
Iter: 1200 loss: 1.01222679e-06
Iter: 1201 loss: 1.01182627e-06
Iter: 1202 loss: 1.01212106e-06
Iter: 1203 loss: 1.0115773e-06
Iter: 1204 loss: 1.01128262e-06
Iter: 1205 loss: 1.0149439e-06
Iter: 1206 loss: 1.01125636e-06
Iter: 1207 loss: 1.0109693e-06
Iter: 1208 loss: 1.01064393e-06
Iter: 1209 loss: 1.01061721e-06
Iter: 1210 loss: 1.01021828e-06
Iter: 1211 loss: 1.01022601e-06
Iter: 1212 loss: 1.01002115e-06
Iter: 1213 loss: 1.00976035e-06
Iter: 1214 loss: 1.0096694e-06
Iter: 1215 loss: 1.00923558e-06
Iter: 1216 loss: 1.01172634e-06
Iter: 1217 loss: 1.0091303e-06
Iter: 1218 loss: 1.00890691e-06
Iter: 1219 loss: 1.00860029e-06
Iter: 1220 loss: 1.00859222e-06
Iter: 1221 loss: 1.00812622e-06
Iter: 1222 loss: 1.0120616e-06
Iter: 1223 loss: 1.0080912e-06
Iter: 1224 loss: 1.00782029e-06
Iter: 1225 loss: 1.00789634e-06
Iter: 1226 loss: 1.00762372e-06
Iter: 1227 loss: 1.00724492e-06
Iter: 1228 loss: 1.00810882e-06
Iter: 1229 loss: 1.00708303e-06
Iter: 1230 loss: 1.00712396e-06
Iter: 1231 loss: 1.0069557e-06
Iter: 1232 loss: 1.00685043e-06
Iter: 1233 loss: 1.00657951e-06
Iter: 1234 loss: 1.00863167e-06
Iter: 1235 loss: 1.00646935e-06
Iter: 1236 loss: 1.00614125e-06
Iter: 1237 loss: 1.00783427e-06
Iter: 1238 loss: 1.00609759e-06
Iter: 1239 loss: 1.00576324e-06
Iter: 1240 loss: 1.00616944e-06
Iter: 1241 loss: 1.00570446e-06
Iter: 1242 loss: 1.00529201e-06
Iter: 1243 loss: 1.00592138e-06
Iter: 1244 loss: 1.00518469e-06
Iter: 1245 loss: 1.00495799e-06
Iter: 1246 loss: 1.00531179e-06
Iter: 1247 loss: 1.00481316e-06
Iter: 1248 loss: 1.00454076e-06
Iter: 1249 loss: 1.00609077e-06
Iter: 1250 loss: 1.00442549e-06
Iter: 1251 loss: 1.00425223e-06
Iter: 1252 loss: 1.00508305e-06
Iter: 1253 loss: 1.00421471e-06
Iter: 1254 loss: 1.00399529e-06
Iter: 1255 loss: 1.00359932e-06
Iter: 1256 loss: 1.00361922e-06
Iter: 1257 loss: 1.00327952e-06
Iter: 1258 loss: 1.00832085e-06
Iter: 1259 loss: 1.00328748e-06
Iter: 1260 loss: 1.00301213e-06
Iter: 1261 loss: 1.00282944e-06
Iter: 1262 loss: 1.00272166e-06
Iter: 1263 loss: 1.00262048e-06
Iter: 1264 loss: 1.00252475e-06
Iter: 1265 loss: 1.00243767e-06
Iter: 1266 loss: 1.00396755e-06
Iter: 1267 loss: 1.00240277e-06
Iter: 1268 loss: 1.00232546e-06
Iter: 1269 loss: 1.00204818e-06
Iter: 1270 loss: 1.0025675e-06
Iter: 1271 loss: 1.00195734e-06
Iter: 1272 loss: 1.00170143e-06
Iter: 1273 loss: 1.00170769e-06
Iter: 1274 loss: 1.00153557e-06
Iter: 1275 loss: 1.00136413e-06
Iter: 1276 loss: 1.0013465e-06
Iter: 1277 loss: 1.00107013e-06
Iter: 1278 loss: 1.00440127e-06
Iter: 1279 loss: 1.00107604e-06
Iter: 1280 loss: 1.00097691e-06
Iter: 1281 loss: 1.00082354e-06
Iter: 1282 loss: 1.00076545e-06
Iter: 1283 loss: 1.00051818e-06
Iter: 1284 loss: 1.00289617e-06
Iter: 1285 loss: 1.00056855e-06
Iter: 1286 loss: 1.00042394e-06
Iter: 1287 loss: 1.00113152e-06
Iter: 1288 loss: 1.0003572e-06
Iter: 1289 loss: 1.00022942e-06
Iter: 1290 loss: 9.99922918e-07
Iter: 1291 loss: 9.99931e-07
Iter: 1292 loss: 9.99744e-07
Iter: 1293 loss: 1.00068723e-06
Iter: 1294 loss: 9.99704412e-07
Iter: 1295 loss: 9.9944566e-07
Iter: 1296 loss: 1.00039642e-06
Iter: 1297 loss: 9.9941883e-07
Iter: 1298 loss: 9.99518761e-07
Iter: 1299 loss: 9.99341751e-07
Iter: 1300 loss: 9.99284225e-07
Iter: 1301 loss: 9.99098233e-07
Iter: 1302 loss: 9.99522626e-07
Iter: 1303 loss: 9.98939186e-07
Iter: 1304 loss: 9.98711812e-07
Iter: 1305 loss: 1.0012709e-06
Iter: 1306 loss: 9.98727501e-07
Iter: 1307 loss: 9.98591076e-07
Iter: 1308 loss: 9.98391215e-07
Iter: 1309 loss: 9.98394285e-07
Iter: 1310 loss: 9.98117343e-07
Iter: 1311 loss: 9.99267741e-07
Iter: 1312 loss: 9.98079e-07
Iter: 1313 loss: 9.97882353e-07
Iter: 1314 loss: 9.98664177e-07
Iter: 1315 loss: 9.97794587e-07
Iter: 1316 loss: 9.97682719e-07
Iter: 1317 loss: 9.97813459e-07
Iter: 1318 loss: 9.97604e-07
Iter: 1319 loss: 9.97365e-07
Iter: 1320 loss: 9.9799513e-07
Iter: 1321 loss: 9.97327334e-07
Iter: 1322 loss: 9.97101552e-07
Iter: 1323 loss: 9.97278e-07
Iter: 1324 loss: 9.96961376e-07
Iter: 1325 loss: 9.96792778e-07
Iter: 1326 loss: 9.96908511e-07
Iter: 1327 loss: 9.9666056e-07
Iter: 1328 loss: 9.96331664e-07
Iter: 1329 loss: 9.96577228e-07
Iter: 1330 loss: 9.9612771e-07
Iter: 1331 loss: 9.96199105e-07
Iter: 1332 loss: 9.96060635e-07
Iter: 1333 loss: 9.95892378e-07
Iter: 1334 loss: 9.95788355e-07
Iter: 1335 loss: 9.95720484e-07
Iter: 1336 loss: 9.95549613e-07
Iter: 1337 loss: 9.95457071e-07
Iter: 1338 loss: 9.95336e-07
Iter: 1339 loss: 9.95122e-07
Iter: 1340 loss: 9.95199798e-07
Iter: 1341 loss: 9.94992e-07
Iter: 1342 loss: 9.94617267e-07
Iter: 1343 loss: 9.97040161e-07
Iter: 1344 loss: 9.94638071e-07
Iter: 1345 loss: 9.94463335e-07
Iter: 1346 loss: 9.95164783e-07
Iter: 1347 loss: 9.94496304e-07
Iter: 1348 loss: 9.94279617e-07
Iter: 1349 loss: 9.9413387e-07
Iter: 1350 loss: 9.94013703e-07
Iter: 1351 loss: 9.93773483e-07
Iter: 1352 loss: 9.95836e-07
Iter: 1353 loss: 9.93760295e-07
Iter: 1354 loss: 9.93502e-07
Iter: 1355 loss: 9.93962885e-07
Iter: 1356 loss: 9.93389e-07
Iter: 1357 loss: 9.93147182e-07
Iter: 1358 loss: 9.93472554e-07
Iter: 1359 loss: 9.93066124e-07
Iter: 1360 loss: 9.92654918e-07
Iter: 1361 loss: 9.92490072e-07
Iter: 1362 loss: 9.92233254e-07
Iter: 1363 loss: 9.92053856e-07
Iter: 1364 loss: 9.92007472e-07
Iter: 1365 loss: 9.91822503e-07
Iter: 1366 loss: 9.93600111e-07
Iter: 1367 loss: 9.9181193e-07
Iter: 1368 loss: 9.91683e-07
Iter: 1369 loss: 9.91376e-07
Iter: 1370 loss: 9.94609081e-07
Iter: 1371 loss: 9.91402317e-07
Iter: 1372 loss: 9.91163688e-07
Iter: 1373 loss: 9.91766228e-07
Iter: 1374 loss: 9.91075126e-07
Iter: 1375 loss: 9.90767603e-07
Iter: 1376 loss: 9.91792376e-07
Iter: 1377 loss: 9.90701892e-07
Iter: 1378 loss: 9.90443255e-07
Iter: 1379 loss: 9.91249067e-07
Iter: 1380 loss: 9.90354e-07
Iter: 1381 loss: 9.9005365e-07
Iter: 1382 loss: 9.89892328e-07
Iter: 1383 loss: 9.89722366e-07
Iter: 1384 loss: 9.89493856e-07
Iter: 1385 loss: 9.93835783e-07
Iter: 1386 loss: 9.89477257e-07
Iter: 1387 loss: 9.89276714e-07
Iter: 1388 loss: 9.89226237e-07
Iter: 1389 loss: 9.89059913e-07
Iter: 1390 loss: 9.88801389e-07
Iter: 1391 loss: 9.91218712e-07
Iter: 1392 loss: 9.88737838e-07
Iter: 1393 loss: 9.8847363e-07
Iter: 1394 loss: 9.88271267e-07
Iter: 1395 loss: 9.88192141e-07
Iter: 1396 loss: 9.88049806e-07
Iter: 1397 loss: 9.87985459e-07
Iter: 1398 loss: 9.87915087e-07
Iter: 1399 loss: 9.88687361e-07
Iter: 1400 loss: 9.8780572e-07
Iter: 1401 loss: 9.87759904e-07
Iter: 1402 loss: 9.87490694e-07
Iter: 1403 loss: 9.90168e-07
Iter: 1404 loss: 9.87463e-07
Iter: 1405 loss: 9.87166459e-07
Iter: 1406 loss: 9.87866088e-07
Iter: 1407 loss: 9.87053113e-07
Iter: 1408 loss: 9.86718533e-07
Iter: 1409 loss: 9.86566306e-07
Iter: 1410 loss: 9.86405212e-07
Iter: 1411 loss: 9.85947736e-07
Iter: 1412 loss: 9.91110937e-07
Iter: 1413 loss: 9.85896463e-07
Iter: 1414 loss: 9.85658403e-07
Iter: 1415 loss: 9.86000714e-07
Iter: 1416 loss: 9.85481847e-07
Iter: 1417 loss: 9.85074e-07
Iter: 1418 loss: 9.86803343e-07
Iter: 1419 loss: 9.85033921e-07
Iter: 1420 loss: 9.84758458e-07
Iter: 1421 loss: 9.8458338e-07
Iter: 1422 loss: 9.84479129e-07
Iter: 1423 loss: 9.84174e-07
Iter: 1424 loss: 9.84164672e-07
Iter: 1425 loss: 9.84038707e-07
Iter: 1426 loss: 9.84038252e-07
Iter: 1427 loss: 9.83891e-07
Iter: 1428 loss: 9.83639e-07
Iter: 1429 loss: 9.84784492e-07
Iter: 1430 loss: 9.83570317e-07
Iter: 1431 loss: 9.83539735e-07
Iter: 1432 loss: 9.83520522e-07
Iter: 1433 loss: 9.83482e-07
Iter: 1434 loss: 9.83286327e-07
Iter: 1435 loss: 9.84366238e-07
Iter: 1436 loss: 9.83166274e-07
Iter: 1437 loss: 9.82850338e-07
Iter: 1438 loss: 9.83407062e-07
Iter: 1439 loss: 9.82757911e-07
Iter: 1440 loss: 9.82429356e-07
Iter: 1441 loss: 9.82044185e-07
Iter: 1442 loss: 9.82091706e-07
Iter: 1443 loss: 9.81479616e-07
Iter: 1444 loss: 9.88042416e-07
Iter: 1445 loss: 9.81522817e-07
Iter: 1446 loss: 9.81218e-07
Iter: 1447 loss: 9.82306801e-07
Iter: 1448 loss: 9.81142193e-07
Iter: 1449 loss: 9.80783398e-07
Iter: 1450 loss: 9.81475296e-07
Iter: 1451 loss: 9.80666e-07
Iter: 1452 loss: 9.80420623e-07
Iter: 1453 loss: 9.80923915e-07
Iter: 1454 loss: 9.80228151e-07
Iter: 1455 loss: 9.79963488e-07
Iter: 1456 loss: 9.81661515e-07
Iter: 1457 loss: 9.79951892e-07
Iter: 1458 loss: 9.79860715e-07
Iter: 1459 loss: 9.80288178e-07
Iter: 1460 loss: 9.79681317e-07
Iter: 1461 loss: 9.79480092e-07
Iter: 1462 loss: 9.79033302e-07
Iter: 1463 loss: 9.79011702e-07
Iter: 1464 loss: 9.79713604e-07
Iter: 1465 loss: 9.78903699e-07
Iter: 1466 loss: 9.78828439e-07
Iter: 1467 loss: 9.78588e-07
Iter: 1468 loss: 9.8186274e-07
Iter: 1469 loss: 9.78563094e-07
Iter: 1470 loss: 9.78232265e-07
Iter: 1471 loss: 9.78285357e-07
Iter: 1472 loss: 9.78053777e-07
Iter: 1473 loss: 9.77710215e-07
Iter: 1474 loss: 9.77974082e-07
Iter: 1475 loss: 9.77496143e-07
Iter: 1476 loss: 9.77060495e-07
Iter: 1477 loss: 9.80455752e-07
Iter: 1478 loss: 9.77093919e-07
Iter: 1479 loss: 9.76873e-07
Iter: 1480 loss: 9.76875072e-07
Iter: 1481 loss: 9.76629849e-07
Iter: 1482 loss: 9.76243086e-07
Iter: 1483 loss: 9.77634e-07
Iter: 1484 loss: 9.76141564e-07
Iter: 1485 loss: 9.75901685e-07
Iter: 1486 loss: 9.77478521e-07
Iter: 1487 loss: 9.7581119e-07
Iter: 1488 loss: 9.75539479e-07
Iter: 1489 loss: 9.75649868e-07
Iter: 1490 loss: 9.75379862e-07
Iter: 1491 loss: 9.75099283e-07
Iter: 1492 loss: 9.76940782e-07
Iter: 1493 loss: 9.75114631e-07
Iter: 1494 loss: 9.7479176e-07
Iter: 1495 loss: 9.74708655e-07
Iter: 1496 loss: 9.74541649e-07
Iter: 1497 loss: 9.74360091e-07
Iter: 1498 loss: 9.74325076e-07
Iter: 1499 loss: 9.74152726e-07
Iter: 1500 loss: 9.74490376e-07
Iter: 1501 loss: 9.74122258e-07
Iter: 1502 loss: 9.74e-07
Iter: 1503 loss: 9.73598617e-07
Iter: 1504 loss: 9.75295734e-07
Iter: 1505 loss: 9.73485385e-07
Iter: 1506 loss: 9.73127271e-07
Iter: 1507 loss: 9.7641032e-07
Iter: 1508 loss: 9.73116812e-07
Iter: 1509 loss: 9.72741873e-07
Iter: 1510 loss: 9.73236183e-07
Iter: 1511 loss: 9.72551334e-07
Iter: 1512 loss: 9.72219595e-07
Iter: 1513 loss: 9.73175474e-07
Iter: 1514 loss: 9.72091243e-07
Iter: 1515 loss: 9.71723694e-07
Iter: 1516 loss: 9.7183954e-07
Iter: 1517 loss: 9.71443114e-07
Iter: 1518 loss: 9.71078748e-07
Iter: 1519 loss: 9.74241061e-07
Iter: 1520 loss: 9.71117515e-07
Iter: 1521 loss: 9.70745759e-07
Iter: 1522 loss: 9.70591827e-07
Iter: 1523 loss: 9.70439942e-07
Iter: 1524 loss: 9.70044539e-07
Iter: 1525 loss: 9.73000851e-07
Iter: 1526 loss: 9.69974167e-07
Iter: 1527 loss: 9.6970814e-07
Iter: 1528 loss: 9.70968472e-07
Iter: 1529 loss: 9.69602524e-07
Iter: 1530 loss: 9.69383109e-07
Iter: 1531 loss: 9.69714165e-07
Iter: 1532 loss: 9.69267603e-07
Iter: 1533 loss: 9.69062171e-07
Iter: 1534 loss: 9.68972699e-07
Iter: 1535 loss: 9.68913128e-07
Iter: 1536 loss: 9.68584573e-07
Iter: 1537 loss: 9.69237817e-07
Iter: 1538 loss: 9.68375844e-07
Iter: 1539 loss: 9.67863571e-07
Iter: 1540 loss: 9.71160375e-07
Iter: 1541 loss: 9.67855271e-07
Iter: 1542 loss: 9.67541723e-07
Iter: 1543 loss: 9.67674623e-07
Iter: 1544 loss: 9.67376536e-07
Iter: 1545 loss: 9.67018536e-07
Iter: 1546 loss: 9.69442453e-07
Iter: 1547 loss: 9.66960897e-07
Iter: 1548 loss: 9.66676112e-07
Iter: 1549 loss: 9.67251822e-07
Iter: 1550 loss: 9.66583343e-07
Iter: 1551 loss: 9.6629924e-07
Iter: 1552 loss: 9.67299911e-07
Iter: 1553 loss: 9.66208177e-07
Iter: 1554 loss: 9.65978643e-07
Iter: 1555 loss: 9.6606027e-07
Iter: 1556 loss: 9.65843128e-07
Iter: 1557 loss: 9.65500135e-07
Iter: 1558 loss: 9.68348104e-07
Iter: 1559 loss: 9.65468303e-07
Iter: 1560 loss: 9.65167146e-07
Iter: 1561 loss: 9.65733079e-07
Iter: 1562 loss: 9.65075401e-07
Iter: 1563 loss: 9.64783567e-07
Iter: 1564 loss: 9.66101879e-07
Iter: 1565 loss: 9.64713877e-07
Iter: 1566 loss: 9.64582682e-07
Iter: 1567 loss: 9.64550281e-07
Iter: 1568 loss: 9.64420906e-07
Iter: 1569 loss: 9.64216838e-07
Iter: 1570 loss: 9.66860171e-07
Iter: 1571 loss: 9.64175e-07
Iter: 1572 loss: 9.63804e-07
Iter: 1573 loss: 9.64158176e-07
Iter: 1574 loss: 9.63578714e-07
Iter: 1575 loss: 9.63272782e-07
Iter: 1576 loss: 9.63827915e-07
Iter: 1577 loss: 9.63121238e-07
Iter: 1578 loss: 9.62662398e-07
Iter: 1579 loss: 9.63826892e-07
Iter: 1580 loss: 9.6256224e-07
Iter: 1581 loss: 9.62212653e-07
Iter: 1582 loss: 9.63273692e-07
Iter: 1583 loss: 9.62092258e-07
Iter: 1584 loss: 9.6174324e-07
Iter: 1585 loss: 9.63387492e-07
Iter: 1586 loss: 9.6168e-07
Iter: 1587 loss: 9.61500291e-07
Iter: 1588 loss: 9.6227e-07
Iter: 1589 loss: 9.61385695e-07
Iter: 1590 loss: 9.61105343e-07
Iter: 1591 loss: 9.61161277e-07
Iter: 1592 loss: 9.60924126e-07
Iter: 1593 loss: 9.60620469e-07
Iter: 1594 loss: 9.62668537e-07
Iter: 1595 loss: 9.60598072e-07
Iter: 1596 loss: 9.60325451e-07
Iter: 1597 loss: 9.60566922e-07
Iter: 1598 loss: 9.60095804e-07
Iter: 1599 loss: 9.60048283e-07
Iter: 1600 loss: 9.5995847e-07
Iter: 1601 loss: 9.59873205e-07
Iter: 1602 loss: 9.59813519e-07
Iter: 1603 loss: 9.59723366e-07
Iter: 1604 loss: 9.59588419e-07
Iter: 1605 loss: 9.59285444e-07
Iter: 1606 loss: 9.63753791e-07
Iter: 1607 loss: 9.59270665e-07
Iter: 1608 loss: 9.58910277e-07
Iter: 1609 loss: 9.60995067e-07
Iter: 1610 loss: 9.58914484e-07
Iter: 1611 loss: 9.5851e-07
Iter: 1612 loss: 9.58164605e-07
Iter: 1613 loss: 9.58063652e-07
Iter: 1614 loss: 9.57749307e-07
Iter: 1615 loss: 9.57752718e-07
Iter: 1616 loss: 9.57382667e-07
Iter: 1617 loss: 9.57265229e-07
Iter: 1618 loss: 9.57042e-07
Iter: 1619 loss: 9.56778194e-07
Iter: 1620 loss: 9.60740522e-07
Iter: 1621 loss: 9.56805252e-07
Iter: 1622 loss: 9.5656435e-07
Iter: 1623 loss: 9.5631e-07
Iter: 1624 loss: 9.56238864e-07
Iter: 1625 loss: 9.55963742e-07
Iter: 1626 loss: 9.59420163e-07
Iter: 1627 loss: 9.5593407e-07
Iter: 1628 loss: 9.55788664e-07
Iter: 1629 loss: 9.55534688e-07
Iter: 1630 loss: 9.61226306e-07
Iter: 1631 loss: 9.5551934e-07
Iter: 1632 loss: 9.55489668e-07
Iter: 1633 loss: 9.55457608e-07
Iter: 1634 loss: 9.55265477e-07
Iter: 1635 loss: 9.55542e-07
Iter: 1636 loss: 9.55220685e-07
Iter: 1637 loss: 9.55161e-07
Iter: 1638 loss: 9.55082e-07
Iter: 1639 loss: 9.55025826e-07
Iter: 1640 loss: 9.54907136e-07
Iter: 1641 loss: 9.55161113e-07
Iter: 1642 loss: 9.54794587e-07
Iter: 1643 loss: 9.54595862e-07
Iter: 1644 loss: 9.54318921e-07
Iter: 1645 loss: 9.5428e-07
Iter: 1646 loss: 9.53777203e-07
Iter: 1647 loss: 9.55439646e-07
Iter: 1648 loss: 9.53696372e-07
Iter: 1649 loss: 9.53265271e-07
Iter: 1650 loss: 9.55916221e-07
Iter: 1651 loss: 9.53227527e-07
Iter: 1652 loss: 9.52959226e-07
Iter: 1653 loss: 9.53181086e-07
Iter: 1654 loss: 9.52763e-07
Iter: 1655 loss: 9.52407504e-07
Iter: 1656 loss: 9.53168069e-07
Iter: 1657 loss: 9.52275059e-07
Iter: 1658 loss: 9.52045468e-07
Iter: 1659 loss: 9.53207632e-07
Iter: 1660 loss: 9.5195378e-07
Iter: 1661 loss: 9.51594188e-07
Iter: 1662 loss: 9.51796608e-07
Iter: 1663 loss: 9.51326228e-07
Iter: 1664 loss: 9.51104653e-07
Iter: 1665 loss: 9.54217853e-07
Iter: 1666 loss: 9.51065545e-07
Iter: 1667 loss: 9.5095254e-07
Iter: 1668 loss: 9.50996e-07
Iter: 1669 loss: 9.50810659e-07
Iter: 1670 loss: 9.50485969e-07
Iter: 1671 loss: 9.5351669e-07
Iter: 1672 loss: 9.50474032e-07
Iter: 1673 loss: 9.50255867e-07
Iter: 1674 loss: 9.52782557e-07
Iter: 1675 loss: 9.50285596e-07
Iter: 1676 loss: 9.50167305e-07
Iter: 1677 loss: 9.49949538e-07
Iter: 1678 loss: 9.49955108e-07
Iter: 1679 loss: 9.49672142e-07
Iter: 1680 loss: 9.50721756e-07
Iter: 1681 loss: 9.49601372e-07
Iter: 1682 loss: 9.49361606e-07
Iter: 1683 loss: 9.49462e-07
Iter: 1684 loss: 9.49157311e-07
Iter: 1685 loss: 9.4894034e-07
Iter: 1686 loss: 9.51218169e-07
Iter: 1687 loss: 9.48863033e-07
Iter: 1688 loss: 9.4864032e-07
Iter: 1689 loss: 9.48498837e-07
Iter: 1690 loss: 9.4842386e-07
Iter: 1691 loss: 9.48167e-07
Iter: 1692 loss: 9.51191794e-07
Iter: 1693 loss: 9.48122931e-07
Iter: 1694 loss: 9.47931596e-07
Iter: 1695 loss: 9.4801743e-07
Iter: 1696 loss: 9.47771582e-07
Iter: 1697 loss: 9.4747935e-07
Iter: 1698 loss: 9.49733703e-07
Iter: 1699 loss: 9.47456897e-07
Iter: 1700 loss: 9.47336e-07
Iter: 1701 loss: 9.47367482e-07
Iter: 1702 loss: 9.4726056e-07
Iter: 1703 loss: 9.47153922e-07
Iter: 1704 loss: 9.47135618e-07
Iter: 1705 loss: 9.46910347e-07
Iter: 1706 loss: 9.46753289e-07
Iter: 1707 loss: 9.46728392e-07
Iter: 1708 loss: 9.4650369e-07
Iter: 1709 loss: 9.48490481e-07
Iter: 1710 loss: 9.46478906e-07
Iter: 1711 loss: 9.46313207e-07
Iter: 1712 loss: 9.46234536e-07
Iter: 1713 loss: 9.46106752e-07
Iter: 1714 loss: 9.45874e-07
Iter: 1715 loss: 9.48229399e-07
Iter: 1716 loss: 9.45873921e-07
Iter: 1717 loss: 9.4565155e-07
Iter: 1718 loss: 9.45231932e-07
Iter: 1719 loss: 9.53624067e-07
Iter: 1720 loss: 9.45218972e-07
Iter: 1721 loss: 9.44969e-07
Iter: 1722 loss: 9.49769401e-07
Iter: 1723 loss: 9.44940609e-07
Iter: 1724 loss: 9.44661224e-07
Iter: 1725 loss: 9.44527699e-07
Iter: 1726 loss: 9.44384738e-07
Iter: 1727 loss: 9.44081307e-07
Iter: 1728 loss: 9.45954866e-07
Iter: 1729 loss: 9.44050328e-07
Iter: 1730 loss: 9.43737348e-07
Iter: 1731 loss: 9.4455e-07
Iter: 1732 loss: 9.43657824e-07
Iter: 1733 loss: 9.43485134e-07
Iter: 1734 loss: 9.45915417e-07
Iter: 1735 loss: 9.43520547e-07
Iter: 1736 loss: 9.43337909e-07
Iter: 1737 loss: 9.43328e-07
Iter: 1738 loss: 9.43224165e-07
Iter: 1739 loss: 9.4304977e-07
Iter: 1740 loss: 9.43101895e-07
Iter: 1741 loss: 9.42908e-07
Iter: 1742 loss: 9.43114514e-07
Iter: 1743 loss: 9.42861789e-07
Iter: 1744 loss: 9.42745373e-07
Iter: 1745 loss: 9.42798579e-07
Iter: 1746 loss: 9.42614463e-07
Iter: 1747 loss: 9.42391182e-07
Iter: 1748 loss: 9.43988312e-07
Iter: 1749 loss: 9.4237646e-07
Iter: 1750 loss: 9.42274596e-07
Iter: 1751 loss: 9.42103213e-07
Iter: 1752 loss: 9.4205609e-07
Iter: 1753 loss: 9.41788585e-07
Iter: 1754 loss: 9.44069768e-07
Iter: 1755 loss: 9.41803194e-07
Iter: 1756 loss: 9.41614815e-07
Iter: 1757 loss: 9.41538929e-07
Iter: 1758 loss: 9.41452129e-07
Iter: 1759 loss: 9.41261874e-07
Iter: 1760 loss: 9.43353314e-07
Iter: 1761 loss: 9.41286316e-07
Iter: 1762 loss: 9.4108276e-07
Iter: 1763 loss: 9.40985046e-07
Iter: 1764 loss: 9.40955772e-07
Iter: 1765 loss: 9.40704751e-07
Iter: 1766 loss: 9.43207397e-07
Iter: 1767 loss: 9.40676557e-07
Iter: 1768 loss: 9.40480049e-07
Iter: 1769 loss: 9.40640405e-07
Iter: 1770 loss: 9.40387622e-07
Iter: 1771 loss: 9.40191057e-07
Iter: 1772 loss: 9.40222378e-07
Iter: 1773 loss: 9.40063728e-07
Iter: 1774 loss: 9.39976815e-07
Iter: 1775 loss: 9.39981589e-07
Iter: 1776 loss: 9.39810775e-07
Iter: 1777 loss: 9.39721758e-07
Iter: 1778 loss: 9.39618872e-07
Iter: 1779 loss: 9.39380641e-07
Iter: 1780 loss: 9.41005624e-07
Iter: 1781 loss: 9.39275424e-07
Iter: 1782 loss: 9.39157474e-07
Iter: 1783 loss: 9.38941355e-07
Iter: 1784 loss: 9.3891714e-07
Iter: 1785 loss: 9.385883e-07
Iter: 1786 loss: 9.41042231e-07
Iter: 1787 loss: 9.38520316e-07
Iter: 1788 loss: 9.38398671e-07
Iter: 1789 loss: 9.38675612e-07
Iter: 1790 loss: 9.38293283e-07
Iter: 1791 loss: 9.38089443e-07
Iter: 1792 loss: 9.40441453e-07
Iter: 1793 loss: 9.3807239e-07
Iter: 1794 loss: 9.37973766e-07
Iter: 1795 loss: 9.37810114e-07
Iter: 1796 loss: 9.37809375e-07
Iter: 1797 loss: 9.37588595e-07
Iter: 1798 loss: 9.38389064e-07
Iter: 1799 loss: 9.37504694e-07
Iter: 1800 loss: 9.37325581e-07
Iter: 1801 loss: 9.37717459e-07
Iter: 1802 loss: 9.37232073e-07
Iter: 1803 loss: 9.37045115e-07
Iter: 1804 loss: 9.37539085e-07
Iter: 1805 loss: 9.36949618e-07
Iter: 1806 loss: 9.37002142e-07
Iter: 1807 loss: 9.36894594e-07
Iter: 1808 loss: 9.36833203e-07
Iter: 1809 loss: 9.36699792e-07
Iter: 1810 loss: 9.38187895e-07
Iter: 1811 loss: 9.36644255e-07
Iter: 1812 loss: 9.36454455e-07
Iter: 1813 loss: 9.36615265e-07
Iter: 1814 loss: 9.36411e-07
Iter: 1815 loss: 9.36207e-07
Iter: 1816 loss: 9.36078436e-07
Iter: 1817 loss: 9.35977255e-07
Iter: 1818 loss: 9.35754315e-07
Iter: 1819 loss: 9.39208917e-07
Iter: 1820 loss: 9.35745675e-07
Iter: 1821 loss: 9.35651087e-07
Iter: 1822 loss: 9.36084689e-07
Iter: 1823 loss: 9.35590833e-07
Iter: 1824 loss: 9.35446451e-07
Iter: 1825 loss: 9.3524352e-07
Iter: 1826 loss: 9.35224705e-07
Iter: 1827 loss: 9.34981358e-07
Iter: 1828 loss: 9.37636969e-07
Iter: 1829 loss: 9.34997615e-07
Iter: 1830 loss: 9.34812078e-07
Iter: 1831 loss: 9.34821742e-07
Iter: 1832 loss: 9.34668208e-07
Iter: 1833 loss: 9.34364039e-07
Iter: 1834 loss: 9.35494768e-07
Iter: 1835 loss: 9.34269451e-07
Iter: 1836 loss: 9.34052309e-07
Iter: 1837 loss: 9.33911679e-07
Iter: 1838 loss: 9.33810611e-07
Iter: 1839 loss: 9.33635761e-07
Iter: 1840 loss: 9.33629622e-07
Iter: 1841 loss: 9.33525143e-07
Iter: 1842 loss: 9.33469664e-07
Iter: 1843 loss: 9.33405e-07
Iter: 1844 loss: 9.33288447e-07
Iter: 1845 loss: 9.33698914e-07
Iter: 1846 loss: 9.33099159e-07
Iter: 1847 loss: 9.32792204e-07
Iter: 1848 loss: 9.34474201e-07
Iter: 1849 loss: 9.32681701e-07
Iter: 1850 loss: 9.32435569e-07
Iter: 1851 loss: 9.32462285e-07
Iter: 1852 loss: 9.32168518e-07
Iter: 1853 loss: 9.31965076e-07
Iter: 1854 loss: 9.3201038e-07
Iter: 1855 loss: 9.31851105e-07
Iter: 1856 loss: 9.31721331e-07
Iter: 1857 loss: 9.31659201e-07
Iter: 1858 loss: 9.31349405e-07
Iter: 1859 loss: 9.316052e-07
Iter: 1860 loss: 9.31179329e-07
Iter: 1861 loss: 9.30945703e-07
Iter: 1862 loss: 9.32086e-07
Iter: 1863 loss: 9.30934789e-07
Iter: 1864 loss: 9.3065853e-07
Iter: 1865 loss: 9.30635338e-07
Iter: 1866 loss: 9.30454235e-07
Iter: 1867 loss: 9.30263809e-07
Iter: 1868 loss: 9.32620765e-07
Iter: 1869 loss: 9.30239821e-07
Iter: 1870 loss: 9.30024612e-07
Iter: 1871 loss: 9.2972931e-07
Iter: 1872 loss: 9.29660359e-07
Iter: 1873 loss: 9.29668261e-07
Iter: 1874 loss: 9.29559178e-07
Iter: 1875 loss: 9.29376085e-07
Iter: 1876 loss: 9.2981503e-07
Iter: 1877 loss: 9.29379553e-07
Iter: 1878 loss: 9.29266321e-07
Iter: 1879 loss: 9.28852501e-07
Iter: 1880 loss: 9.31664829e-07
Iter: 1881 loss: 9.28810039e-07
Iter: 1882 loss: 9.28436066e-07
Iter: 1883 loss: 9.30141255e-07
Iter: 1884 loss: 9.28409463e-07
Iter: 1885 loss: 9.28059876e-07
Iter: 1886 loss: 9.29076e-07
Iter: 1887 loss: 9.27949145e-07
Iter: 1888 loss: 9.27736153e-07
Iter: 1889 loss: 9.28491e-07
Iter: 1890 loss: 9.27674478e-07
Iter: 1891 loss: 9.27395718e-07
Iter: 1892 loss: 9.28024861e-07
Iter: 1893 loss: 9.2729681e-07
Iter: 1894 loss: 9.27032659e-07
Iter: 1895 loss: 9.29292582e-07
Iter: 1896 loss: 9.27032431e-07
Iter: 1897 loss: 9.26939151e-07
Iter: 1898 loss: 9.26610255e-07
Iter: 1899 loss: 9.33315334e-07
Iter: 1900 loss: 9.2655165e-07
Iter: 1901 loss: 9.26326834e-07
Iter: 1902 loss: 9.30481406e-07
Iter: 1903 loss: 9.26324788e-07
Iter: 1904 loss: 9.2612413e-07
Iter: 1905 loss: 9.25992651e-07
Iter: 1906 loss: 9.25927e-07
Iter: 1907 loss: 9.25621066e-07
Iter: 1908 loss: 9.27797146e-07
Iter: 1909 loss: 9.25613449e-07
Iter: 1910 loss: 9.25509312e-07
Iter: 1911 loss: 9.25538416e-07
Iter: 1912 loss: 9.25438599e-07
Iter: 1913 loss: 9.25229642e-07
Iter: 1914 loss: 9.27020551e-07
Iter: 1915 loss: 9.25206223e-07
Iter: 1916 loss: 9.24991809e-07
Iter: 1917 loss: 9.25009317e-07
Iter: 1918 loss: 9.24697417e-07
Iter: 1919 loss: 9.24476467e-07
Iter: 1920 loss: 9.24850951e-07
Iter: 1921 loss: 9.24293886e-07
Iter: 1922 loss: 9.23867447e-07
Iter: 1923 loss: 9.25799327e-07
Iter: 1924 loss: 9.23759671e-07
Iter: 1925 loss: 9.2353406e-07
Iter: 1926 loss: 9.24403821e-07
Iter: 1927 loss: 9.23478751e-07
Iter: 1928 loss: 9.23190953e-07
Iter: 1929 loss: 9.23740345e-07
Iter: 1930 loss: 9.22995127e-07
Iter: 1931 loss: 9.22702498e-07
Iter: 1932 loss: 9.24654898e-07
Iter: 1933 loss: 9.22646e-07
Iter: 1934 loss: 9.22429081e-07
Iter: 1935 loss: 9.2209649e-07
Iter: 1936 loss: 9.2210945e-07
Iter: 1937 loss: 9.21800734e-07
Iter: 1938 loss: 9.21818241e-07
Iter: 1939 loss: 9.21561195e-07
Iter: 1940 loss: 9.21459446e-07
Iter: 1941 loss: 9.21380433e-07
Iter: 1942 loss: 9.212701e-07
Iter: 1943 loss: 9.21192395e-07
Iter: 1944 loss: 9.21052447e-07
Iter: 1945 loss: 9.2096235e-07
Iter: 1946 loss: 9.2089374e-07
Iter: 1947 loss: 9.20698653e-07
Iter: 1948 loss: 9.20448e-07
Iter: 1949 loss: 9.26016469e-07
Iter: 1950 loss: 9.20358445e-07
Iter: 1951 loss: 9.20003e-07
Iter: 1952 loss: 9.21519245e-07
Iter: 1953 loss: 9.19840545e-07
Iter: 1954 loss: 9.19468334e-07
Iter: 1955 loss: 9.19418255e-07
Iter: 1956 loss: 9.19178945e-07
Iter: 1957 loss: 9.18687306e-07
Iter: 1958 loss: 9.23820721e-07
Iter: 1959 loss: 9.18705382e-07
Iter: 1960 loss: 9.18487842e-07
Iter: 1961 loss: 9.18259786e-07
Iter: 1962 loss: 9.18142291e-07
Iter: 1963 loss: 9.17801572e-07
Iter: 1964 loss: 9.20451669e-07
Iter: 1965 loss: 9.17783041e-07
Iter: 1966 loss: 9.17539921e-07
Iter: 1967 loss: 9.1889433e-07
Iter: 1968 loss: 9.17558168e-07
Iter: 1969 loss: 9.17362854e-07
Iter: 1970 loss: 9.16844e-07
Iter: 1971 loss: 9.20772379e-07
Iter: 1972 loss: 9.16803515e-07
Iter: 1973 loss: 9.16534191e-07
Iter: 1974 loss: 9.16474562e-07
Iter: 1975 loss: 9.16268448e-07
Iter: 1976 loss: 9.16205579e-07
Iter: 1977 loss: 9.16039198e-07
Iter: 1978 loss: 9.16214958e-07
Iter: 1979 loss: 9.15995827e-07
Iter: 1980 loss: 9.15898113e-07
Iter: 1981 loss: 9.15695637e-07
Iter: 1982 loss: 9.15719966e-07
Iter: 1983 loss: 9.15524197e-07
Iter: 1984 loss: 9.15342639e-07
Iter: 1985 loss: 9.15256e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi3/500_500_500_500_1
