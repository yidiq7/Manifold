+ RUN=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='2 3'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 4000 				 --batch_size 5000 				 --max_epochs 1000 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8faa07ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8faa2c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8faa07730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa91aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa8fc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa8fc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa87cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa87c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa8fc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa907c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa8fcae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa7b7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9060a5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa7a3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa7527b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa7526a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa7a32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa8fc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa708620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa6d9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8fa6d9ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3d086a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3d08f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3cd99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3cd9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3c7f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3cd9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3c53a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3c07598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3bafd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3baf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3b679d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3b67950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3b31730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3b96400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc8f3b316a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.023281602
test_loss: 0.023674391
train_loss: 0.017600222
test_loss: 0.01820292
train_loss: 0.013131002
test_loss: 0.013187241
train_loss: 0.010435581
test_loss: 0.010573539
train_loss: 0.008716203
test_loss: 0.00874117
train_loss: 0.007329205
test_loss: 0.0076652365
train_loss: 0.006389765
test_loss: 0.0069586453
train_loss: 0.006020185
test_loss: 0.006160149
train_loss: 0.0051232358
test_loss: 0.0055851624
train_loss: 0.0047415113
test_loss: 0.0052555604
train_loss: 0.0041434886
test_loss: 0.005129301
train_loss: 0.0042567328
test_loss: 0.0046922066
train_loss: 0.0038135336
test_loss: 0.0045108637
train_loss: 0.0035657773
test_loss: 0.0042443094
train_loss: 0.0033598663
test_loss: 0.004122568
train_loss: 0.0035283712
test_loss: 0.003923204
train_loss: 0.0033556665
test_loss: 0.003719439
train_loss: 0.0030884794
test_loss: 0.0037192754
train_loss: 0.003189836
test_loss: 0.0037244111
train_loss: 0.0030286827
test_loss: 0.0034588436
train_loss: 0.0030411435
test_loss: 0.0035489611
train_loss: 0.0030983062
test_loss: 0.003632875
train_loss: 0.0029148264
test_loss: 0.0037459326
train_loss: 0.0029928428
test_loss: 0.0034856754
train_loss: 0.0031258396
test_loss: 0.0035388314
train_loss: 0.0029965676
test_loss: 0.0035494065
train_loss: 0.0028571442
test_loss: 0.003423041
train_loss: 0.0027411494
test_loss: 0.0033508798
train_loss: 0.002685335
test_loss: 0.0035167148
train_loss: 0.0029400662
test_loss: 0.0033021672
train_loss: 0.0029932866
test_loss: 0.003623214
train_loss: 0.0032343408
test_loss: 0.00335235
train_loss: 0.0031588133
test_loss: 0.003479702
train_loss: 0.0028501926
test_loss: 0.003453581
train_loss: 0.0029970002
test_loss: 0.003470646
train_loss: 0.00278009
test_loss: 0.0032888278
train_loss: 0.0027223367
test_loss: 0.0032387753
train_loss: 0.0026229941
test_loss: 0.0032088417
train_loss: 0.0027068078
test_loss: 0.0031546303
train_loss: 0.0026597762
test_loss: 0.0031478393
train_loss: 0.0026008836
test_loss: 0.0032849982
train_loss: 0.0026496171
test_loss: 0.0031974043
train_loss: 0.0026762453
test_loss: 0.0031141816
train_loss: 0.0028034265
test_loss: 0.0031318462
train_loss: 0.002665431
test_loss: 0.0031262708
train_loss: 0.002518273
test_loss: 0.0031223306
train_loss: 0.0027856894
test_loss: 0.003148775
train_loss: 0.0024950276
test_loss: 0.0030476206
train_loss: 0.0024547256
test_loss: 0.0029508206
train_loss: 0.0024821046
test_loss: 0.0030232626
train_loss: 0.0024282672
test_loss: 0.0029368326
train_loss: 0.0023290417
test_loss: 0.0029729977
train_loss: 0.0023427012
test_loss: 0.0029771796
train_loss: 0.0025644954
test_loss: 0.00307295
train_loss: 0.0024149925
test_loss: 0.0030869734
train_loss: 0.0024231093
test_loss: 0.0031388153
train_loss: 0.0025046647
test_loss: 0.0029725553
train_loss: 0.0023298748
test_loss: 0.0029906197
train_loss: 0.0024861859
test_loss: 0.0030821518
train_loss: 0.0026200775
test_loss: 0.0029561264
train_loss: 0.0027256696
test_loss: 0.003167997
train_loss: 0.0030256142
test_loss: 0.0030337924
train_loss: 0.002440175
test_loss: 0.0033073176
train_loss: 0.002542649
test_loss: 0.0031453425
train_loss: 0.0023985133
test_loss: 0.0030078057
train_loss: 0.002356911
test_loss: 0.002895087
train_loss: 0.0024619256
test_loss: 0.0030283164
train_loss: 0.002468153
test_loss: 0.0029248847
train_loss: 0.0024240608
test_loss: 0.0031541411
train_loss: 0.0024629394
test_loss: 0.0029664787
train_loss: 0.0024302695
test_loss: 0.0029245848
train_loss: 0.0025776199
test_loss: 0.0029781705
train_loss: 0.0026844207
test_loss: 0.0031881726
train_loss: 0.0026769242
test_loss: 0.003194492
train_loss: 0.0025674123
test_loss: 0.0030957996
train_loss: 0.0024994311
test_loss: 0.0030556773
train_loss: 0.0024682921
test_loss: 0.002910117
train_loss: 0.0025009816
test_loss: 0.0030611583
train_loss: 0.0024534294
test_loss: 0.003008069
train_loss: 0.002254292
test_loss: 0.0029876805
train_loss: 0.0023235534
test_loss: 0.0029823089
train_loss: 0.0022711311
test_loss: 0.0032239656
train_loss: 0.0025488534
test_loss: 0.0031509197
train_loss: 0.00253385
test_loss: 0.003152228
train_loss: 0.0026647984
test_loss: 0.002924788
train_loss: 0.002488608
test_loss: 0.0032556604
train_loss: 0.002527342
test_loss: 0.0030278922
train_loss: 0.0025367965
test_loss: 0.0030398914
train_loss: 0.002233264
test_loss: 0.0030404807
train_loss: 0.002267229
test_loss: 0.0029572072
train_loss: 0.0023403885
test_loss: 0.0028917594
train_loss: 0.002301976
test_loss: 0.002997967
train_loss: 0.0023188053
test_loss: 0.0030353237
train_loss: 0.002305457
test_loss: 0.0030824041
train_loss: 0.0023985563
test_loss: 0.0031060306
train_loss: 0.0021849566
test_loss: 0.0028038537
train_loss: 0.002165359
test_loss: 0.0028008686
train_loss: 0.0021682351
test_loss: 0.0028654425
train_loss: 0.0020925251
test_loss: 0.0028506806
train_loss: 0.0022861476
test_loss: 0.002899016
train_loss: 0.0021401804
test_loss: 0.0028439912
train_loss: 0.0021858693
test_loss: 0.002930707
train_loss: 0.0022797205
test_loss: 0.003003991
train_loss: 0.002274658
test_loss: 0.0028940141
train_loss: 0.0022413011
test_loss: 0.0030645248
train_loss: 0.002458662
test_loss: 0.0029815582
train_loss: 0.0023505208
test_loss: 0.0028948986
train_loss: 0.0022010384
test_loss: 0.0029395714
train_loss: 0.0023623342
test_loss: 0.0031274902
train_loss: 0.0023071794
test_loss: 0.0030034452
train_loss: 0.0022569178
test_loss: 0.0029734676
train_loss: 0.0021766098
test_loss: 0.0029097365
train_loss: 0.0021876362
test_loss: 0.0031753043
train_loss: 0.002614322
test_loss: 0.00334722
train_loss: 0.0026371756
test_loss: 0.0030197988
train_loss: 0.002457975
test_loss: 0.0028034765
train_loss: 0.002391187
test_loss: 0.002944909
train_loss: 0.0023831297
test_loss: 0.0030024857
train_loss: 0.0024631412
test_loss: 0.002908242
train_loss: 0.002435823
test_loss: 0.002939647
train_loss: 0.0022527245
test_loss: 0.002929493
train_loss: 0.0022752648
test_loss: 0.0031119492
train_loss: 0.0023713359
test_loss: 0.0029575394
train_loss: 0.0023122265
test_loss: 0.002938892
train_loss: 0.002267067
test_loss: 0.002822773
train_loss: 0.0020350644
test_loss: 0.0029512835
train_loss: 0.0023785392
test_loss: 0.002878081
train_loss: 0.0020943028
test_loss: 0.002915708
train_loss: 0.0023346776
test_loss: 0.0030584068
train_loss: 0.002449566
test_loss: 0.0031214836
train_loss: 0.00255901
test_loss: 0.0029532544
train_loss: 0.0022104783
test_loss: 0.0029041804
train_loss: 0.0021070826
test_loss: 0.0028439309
train_loss: 0.0021348442
test_loss: 0.0028659871
train_loss: 0.0019888652
test_loss: 0.0029063262
train_loss: 0.0021355776
test_loss: 0.0028355613
train_loss: 0.0020610294
test_loss: 0.0028201502
train_loss: 0.0020552473
test_loss: 0.0028307242
train_loss: 0.0021340451
test_loss: 0.0028487754
train_loss: 0.0021286085
test_loss: 0.0028847556
train_loss: 0.0020713112
test_loss: 0.0029040317
train_loss: 0.0022417884
test_loss: 0.0029048733
train_loss: 0.0023599844
test_loss: 0.0032485449
train_loss: 0.0025932328
test_loss: 0.003071059
train_loss: 0.0022346387
test_loss: 0.0029410864
train_loss: 0.0022268738
test_loss: 0.0029903667
train_loss: 0.0023078397
test_loss: 0.0031716642
train_loss: 0.0024178652
test_loss: 0.0031321757
train_loss: 0.00226436
test_loss: 0.0028917075
train_loss: 0.0022039388
test_loss: 0.0027848189
train_loss: 0.0021938072
test_loss: 0.0028415225
train_loss: 0.0021971748
test_loss: 0.0028752333
train_loss: 0.0022054545
test_loss: 0.002822012
train_loss: 0.002010301
test_loss: 0.0027651081
train_loss: 0.0020138172
test_loss: 0.0029393504
train_loss: 0.0021014845
test_loss: 0.0029935904
train_loss: 0.0025175016
test_loss: 0.002931903
train_loss: 0.0023186
test_loss: 0.002980459
train_loss: 0.0022557122
test_loss: 0.0029869627
train_loss: 0.0022644834
test_loss: 0.003003378
train_loss: 0.0024678067
test_loss: 0.0031118873
train_loss: 0.0024041578
test_loss: 0.0029771954
train_loss: 0.002257825
test_loss: 0.0031203565
train_loss: 0.0022066466
test_loss: 0.002883648
train_loss: 0.0021703285
test_loss: 0.0027992309
train_loss: 0.0020283726
test_loss: 0.0028520282
train_loss: 0.0019619088
test_loss: 0.0027908958
train_loss: 0.0020041103
test_loss: 0.0027883835
train_loss: 0.0019255109
test_loss: 0.0027474347
train_loss: 0.001966459
test_loss: 0.0028080638
train_loss: 0.0020047901
test_loss: 0.0027981838
train_loss: 0.00209501
test_loss: 0.0030167245
train_loss: 0.002278577
test_loss: 0.002909366
train_loss: 0.002131422
test_loss: 0.0028828934
train_loss: 0.0021447253
test_loss: 0.0028709674
train_loss: 0.0022511445
test_loss: 0.002928179
train_loss: 0.0021872772
test_loss: 0.0029139707
train_loss: 0.0021162496
test_loss: 0.0028439001
train_loss: 0.002084996
test_loss: 0.00298301
train_loss: 0.0021336647
test_loss: 0.0029559846
train_loss: 0.0023657428
test_loss: 0.0029309478
train_loss: 0.0020874795
test_loss: 0.0028795337
train_loss: 0.0021470906
test_loss: 0.0027989803
train_loss: 0.0020544892
test_loss: 0.0028889978
train_loss: 0.0022693488
test_loss: 0.0028513123
train_loss: 0.0021486548
test_loss: 0.0032805775
train_loss: 0.0025998985
test_loss: 0.0028324556
train_loss: 0.002131052
test_loss: 0.0033461605
train_loss: 0.0027958485
test_loss: 0.002907238
train_loss: 0.0021720286
test_loss: 0.0034058364
train_loss: 0.0025782003
test_loss: 0.0029577669
train_loss: 0.0022533871
test_loss: 0.0030875576
train_loss: 0.002458551
test_loss: 0.0030532214
train_loss: 0.002263182
test_loss: 0.002997885
train_loss: 0.0021589908
test_loss: 0.0029270963
train_loss: 0.0022158406
test_loss: 0.0029125046
train_loss: 0.002223909
test_loss: 0.0028089925
train_loss: 0.0021834753
test_loss: 0.0030044434
train_loss: 0.0021688803
test_loss: 0.0029148436
train_loss: 0.0021798124
test_loss: 0.0028883368
train_loss: 0.0019397662
test_loss: 0.0027338637
train_loss: 0.0020859102
test_loss: 0.0026843573
train_loss: 0.0018556587
test_loss: 0.0028037676
train_loss: 0.0019946145
test_loss: 0.0027718812
train_loss: 0.0019722353
test_loss: 0.0027914592
train_loss: 0.0019695647
test_loss: 0.0028120114
train_loss: 0.0019378955
test_loss: 0.002865575
train_loss: 0.0020852927
test_loss: 0.0027823965
train_loss: 0.0019294157
test_loss: 0.0028607845
train_loss: 0.0020543013
test_loss: 0.0027879644
train_loss: 0.0020944078
test_loss: 0.0028917845
train_loss: 0.0021797796
test_loss: 0.0029270644
train_loss: 0.0021036011
test_loss: 0.0028737783
train_loss: 0.0022860907
test_loss: 0.0029352594
train_loss: 0.0020315812
test_loss: 0.0028724836
train_loss: 0.002018941
test_loss: 0.0027488736
train_loss: 0.0020797823
test_loss: 0.0028690456
train_loss: 0.0020746193
test_loss: 0.0027686965
train_loss: 0.0019779748
test_loss: 0.002820025
train_loss: 0.0020724006
test_loss: 0.0027822554
train_loss: 0.002215461
test_loss: 0.0028695792
train_loss: 0.0021039534
test_loss: 0.0028309617
train_loss: 0.002206022
test_loss: 0.0028898164
train_loss: 0.0020041969
test_loss: 0.002912061
train_loss: 0.002224048
test_loss: 0.0029521792
train_loss: 0.0020434312
test_loss: 0.0030215823
train_loss: 0.0021994435
test_loss: 0.0029676727
train_loss: 0.0021618032
test_loss: 0.0030848226
train_loss: 0.0020514522
test_loss: 0.0028626446
train_loss: 0.0020879887
test_loss: 0.0028731287
train_loss: 0.0020605547
test_loss: 0.002939709
train_loss: 0.002072949
test_loss: 0.0028773095
train_loss: 0.0020135802
test_loss: 0.002890857
train_loss: 0.002367842
test_loss: 0.0029698273
train_loss: 0.0020415124
test_loss: 0.002833685
train_loss: 0.0021118624
test_loss: 0.0029358112
train_loss: 0.002015056
test_loss: 0.0028085622
train_loss: 0.0018665236
test_loss: 0.0026479703
train_loss: 0.0018624507
test_loss: 0.0028745208
train_loss: 0.002000044
test_loss: 0.0030381964
train_loss: 0.0019788763
test_loss: 0.0030166649
train_loss: 0.0021465896
test_loss: 0.0029274407
train_loss: 0.0018558605
test_loss: 0.0027406276
train_loss: 0.0018610695
test_loss: 0.0027518182
train_loss: 0.0018156505
test_loss: 0.0027376583
train_loss: 0.0020896634
test_loss: 0.0028406645
train_loss: 0.0018469164
test_loss: 0.0027914327
train_loss: 0.0018770205
test_loss: 0.002870522
train_loss: 0.0021295967
test_loss: 0.0027191907
train_loss: 0.0017967776
test_loss: 0.0027805462
train_loss: 0.002105359
test_loss: 0.0027736377
train_loss: 0.002073007
test_loss: 0.002861744
train_loss: 0.002112769
test_loss: 0.0028799144
train_loss: 0.0021677136
test_loss: 0.0028414151
train_loss: 0.001991025
test_loss: 0.0028780887
train_loss: 0.0021706317
test_loss: 0.002887097
train_loss: 0.0019822856
test_loss: 0.0028010616
train_loss: 0.002136155
test_loss: 0.0028870963
train_loss: 0.0018834295
test_loss: 0.0028767863
train_loss: 0.002104784
test_loss: 0.0028426363
train_loss: 0.0019025418
test_loss: 0.0028120785
train_loss: 0.0017895005
test_loss: 0.0027596029
train_loss: 0.0019699268
test_loss: 0.002710361
train_loss: 0.0020474582
test_loss: 0.0029402645
train_loss: 0.0019806307
test_loss: 0.0029801992
train_loss: 0.002021287
test_loss: 0.0027919076
train_loss: 0.0018136553
test_loss: 0.0028443614
train_loss: 0.0017820353
test_loss: 0.0028429513
train_loss: 0.0017541753
test_loss: 0.0027772824
train_loss: 0.0019037955
test_loss: 0.002804745
train_loss: 0.0021305617
test_loss: 0.0029749933
train_loss: 0.0021931098
test_loss: 0.0027816757
train_loss: 0.0020907656
test_loss: 0.0027298837
train_loss: 0.0019223567
test_loss: 0.0027662814
train_loss: 0.0021829214
test_loss: 0.0028321485
train_loss: 0.0022296482
test_loss: 0.0028263517
train_loss: 0.0020310348
test_loss: 0.0029916072
train_loss: 0.0021379804
test_loss: 0.0029859024
train_loss: 0.0018968171
test_loss: 0.0029365676
train_loss: 0.0019982355
test_loss: 0.002875163
train_loss: 0.0018945319
test_loss: 0.0028283554
train_loss: 0.0017557561
test_loss: 0.0028861037
train_loss: 0.0026050955
test_loss: 0.0028260539
train_loss: 0.0019611246
test_loss: 0.003184606
train_loss: 0.002576334
test_loss: 0.0029765056
train_loss: 0.0021312956
test_loss: 0.0030302054
train_loss: 0.0023303535
test_loss: 0.0029454476
train_loss: 0.0022274852
test_loss: 0.0031843083
train_loss: 0.0022827054
test_loss: 0.0028685203
train_loss: 0.002336772
test_loss: 0.003082258
train_loss: 0.0021482778
test_loss: 0.0029880062
train_loss: 0.0023152735
test_loss: 0.0030589362
train_loss: 0.0019690963
test_loss: 0.0030254433
train_loss: 0.0022487906
test_loss: 0.002863858
train_loss: 0.001958665
test_loss: 0.002954774
train_loss: 0.0020459683
test_loss: 0.002893997
train_loss: 0.0021263943
test_loss: 0.002984992
train_loss: 0.0020428144
test_loss: 0.0028311275
train_loss: 0.0021536853
test_loss: 0.0028319561
train_loss: 0.002043908
test_loss: 0.0029743607
train_loss: 0.0017875606
test_loss: 0.0028512345
train_loss: 0.0019439872
test_loss: 0.0028531204
train_loss: 0.0019802046
test_loss: 0.0029836528
train_loss: 0.002178125
test_loss: 0.0029107884
train_loss: 0.0020869453
test_loss: 0.0028135888
train_loss: 0.0017952193
test_loss: 0.0027188237
train_loss: 0.0018503522
test_loss: 0.0029181542
train_loss: 0.0020155446
test_loss: 0.0029266996
train_loss: 0.0020147203
test_loss: 0.002958273
train_loss: 0.002118285
test_loss: 0.003037399
train_loss: 0.0019766805
test_loss: 0.0029254796
train_loss: 0.0020081308
test_loss: 0.0028916122
train_loss: 0.0020294911
test_loss: 0.0029096392
train_loss: 0.0019901125
test_loss: 0.0029943106
train_loss: 0.0021193882
test_loss: 0.0029722874
train_loss: 0.0019331328
test_loss: 0.0030737529
train_loss: 0.0019530506
test_loss: 0.0028637534
train_loss: 0.0020548946
test_loss: 0.0028624537
train_loss: 0.0020226052
test_loss: 0.002887549
train_loss: 0.0019782402
test_loss: 0.0028567712
train_loss: 0.0020049075
test_loss: 0.002782125
train_loss: 0.0018260416
test_loss: 0.0028939822
train_loss: 0.0017745635
test_loss: 0.0029590023
train_loss: 0.0019411619
test_loss: 0.0028716011
train_loss: 0.0018922316
test_loss: 0.0028600104
train_loss: 0.0017273441
test_loss: 0.0027919
train_loss: 0.001770274
test_loss: 0.002894107
train_loss: 0.0018070742
test_loss: 0.002769669
train_loss: 0.0018076603
test_loss: 0.0027676392
train_loss: 0.0019727037
test_loss: 0.002792579
train_loss: 0.0020000313
test_loss: 0.002813882
train_loss: 0.0019514298
test_loss: 0.0029801943
train_loss: 0.0018422488
test_loss: 0.0028835463
train_loss: 0.002015198
test_loss: 0.002898483
train_loss: 0.0020173353
test_loss: 0.0028997173
train_loss: 0.0019934159
test_loss: 0.0028891042
train_loss: 0.0018621036
test_loss: 0.0029204187
train_loss: 0.00206572
test_loss: 0.0029280935
train_loss: 0.0020038614
test_loss: 0.00297276
train_loss: 0.002024162
test_loss: 0.002961533
train_loss: 0.0020740214
test_loss: 0.0029934514
train_loss: 0.002129856
test_loss: 0.0029791058
train_loss: 0.0020053857
test_loss: 0.00299707
train_loss: 0.002232222
test_loss: 0.0030545392
train_loss: 0.0021238483
test_loss: 0.0029752997
train_loss: 0.0020075664
test_loss: 0.003004103
train_loss: 0.0019893032
test_loss: 0.0029710326
train_loss: 0.00215704
test_loss: 0.0028636162
train_loss: 0.0018818758
test_loss: 0.002870636
train_loss: 0.0019190251
test_loss: 0.0028194268
train_loss: 0.0017951173
test_loss: 0.0027919032
train_loss: 0.0017495233
test_loss: 0.002796555
train_loss: 0.0018209814
test_loss: 0.0027764095
train_loss: 0.0018602856
test_loss: 0.0028050977
train_loss: 0.0017890406
test_loss: 0.002851442
train_loss: 0.0018704883
test_loss: 0.0028444077
train_loss: 0.0018556935
test_loss: 0.0027673796
train_loss: 0.0017704224
test_loss: 0.0028117185
train_loss: 0.0016997124
test_loss: 0.002791752
train_loss: 0.0018220918
test_loss: 0.002798324
train_loss: 0.0017667447
test_loss: 0.0026775687
train_loss: 0.001724128
test_loss: 0.002734745
train_loss: 0.0017992994
test_loss: 0.002939019
train_loss: 0.0019118695
test_loss: 0.0028755974
train_loss: 0.0017987371
test_loss: 0.002801773
train_loss: 0.0016923434
test_loss: 0.0028137513
train_loss: 0.0018648608
test_loss: 0.0028958982
train_loss: 0.0017734333
test_loss: 0.0027955768
train_loss: 0.0017322835
test_loss: 0.0027054227
train_loss: 0.0018928059
test_loss: 0.0028878432
train_loss: 0.0021012942
test_loss: 0.003014049
train_loss: 0.0022160322
test_loss: 0.0027978586
train_loss: 0.0019329477
test_loss: 0.0028005233
train_loss: 0.001959998
test_loss: 0.0030500216
train_loss: 0.0021461232
test_loss: 0.0029694205
train_loss: 0.0020596846
test_loss: 0.0029079139
train_loss: 0.0018648478
test_loss: 0.0028379231
train_loss: 0.0018728403
test_loss: 0.0027819085
train_loss: 0.0016189762
test_loss: 0.0027813578
train_loss: 0.0018079863
test_loss: 0.0028233216
train_loss: 0.0018513049
test_loss: 0.0028296714
train_loss: 0.0018131055
test_loss: 0.0028647082
train_loss: 0.0017848039
test_loss: 0.0027954562
train_loss: 0.001702187
test_loss: 0.002906432
train_loss: 0.0018800359
test_loss: 0.0029744308
train_loss: 0.0019040196
test_loss: 0.0027797413
train_loss: 0.001803566
test_loss: 0.0028218518
train_loss: 0.0018808302
test_loss: 0.0028448892
train_loss: 0.0018942766
test_loss: 0.002758393
train_loss: 0.0017866988
test_loss: 0.0027731054
train_loss: 0.0016714138
test_loss: 0.0027258876
train_loss: 0.0018135323
test_loss: 0.0027105447
train_loss: 0.001931595
test_loss: 0.002768179
train_loss: 0.0018717631
test_loss: 0.0028348602
train_loss: 0.0019443445
test_loss: 0.0028839712
train_loss: 0.0019094823
test_loss: 0.00289251
train_loss: 0.0018579831
test_loss: 0.002828074
train_loss: 0.0019408438
test_loss: 0.002877741
train_loss: 0.0018090697
test_loss: 0.002867936
train_loss: 0.0018906351
test_loss: 0.002876664
train_loss: 0.0018055441
test_loss: 0.0028178655
train_loss: 0.0018077563
test_loss: 0.0029392475
train_loss: 0.0017930026
test_loss: 0.0029162164
train_loss: 0.0018930666
test_loss: 0.0029285091
train_loss: 0.0020005263
test_loss: 0.0028318288
train_loss: 0.0017946847
test_loss: 0.0027483045
train_loss: 0.0018878523
test_loss: 0.0028269426
train_loss: 0.0017887384
test_loss: 0.0030131664
train_loss: 0.0018609682
test_loss: 0.002926576
train_loss: 0.001873211
test_loss: 0.002903935
train_loss: 0.0018158193
test_loss: 0.002836894
train_loss: 0.0018352142
test_loss: 0.0028443155
train_loss: 0.0017388514
test_loss: 0.0028583237
train_loss: 0.0017488296
test_loss: 0.0028689934
train_loss: 0.0017113144
test_loss: 0.002867186
train_loss: 0.0017891278
test_loss: 0.0029130362
train_loss: 0.0018615385
test_loss: 0.0028325692
train_loss: 0.001810729
test_loss: 0.0027440153
train_loss: 0.0019263523
test_loss: 0.0028424773
train_loss: 0.0020313677
test_loss: 0.002988198
train_loss: 0.0020913258
test_loss: 0.0028975853
train_loss: 0.0018638126
test_loss: 0.0028096735
train_loss: 0.0019818475
test_loss: 0.002996788
train_loss: 0.0018859308
test_loss: 0.0029397104
train_loss: 0.0019557695
test_loss: 0.0028425963
train_loss: 0.0020271013
test_loss: 0.00283702
train_loss: 0.0018461423
test_loss: 0.0028684284
train_loss: 0.0018314752
test_loss: 0.0029201615
train_loss: 0.0018422534
test_loss: 0.0027621563
train_loss: 0.0017535293
test_loss: 0.0027773378
train_loss: 0.0019104294
test_loss: 0.0028925077
train_loss: 0.0018801771
test_loss: 0.0029091618
train_loss: 0.001941961
test_loss: 0.0029398007
train_loss: 0.002403248
test_loss: 0.002967617
train_loss: 0.002012498
test_loss: 0.002965562
train_loss: 0.0019994015
test_loss: 0.0028768072
train_loss: 0.0018041675
test_loss: 0.0028769898
train_loss: 0.0017915949
test_loss: 0.0028931212
train_loss: 0.0018911432
test_loss: 0.0029291238
train_loss: 0.0019063187
test_loss: 0.0028583095
train_loss: 0.0017909553
test_loss: 0.0028691876
train_loss: 0.0018249687
test_loss: 0.002821082
train_loss: 0.0018371532
test_loss: 0.0028431949
train_loss: 0.0019178409
test_loss: 0.0029064887
train_loss: 0.0020235453
test_loss: 0.0029548835
train_loss: 0.0019632839
test_loss: 0.0029248188
train_loss: 0.0021507293
test_loss: 0.0029323944
train_loss: 0.001888548
test_loss: 0.0028621214
train_loss: 0.0019058039
test_loss: 0.003068451
train_loss: 0.0021056402
test_loss: 0.0028459853
train_loss: 0.00180661
test_loss: 0.0030384182
train_loss: 0.0021049194
test_loss: 0.0029160006
train_loss: 0.0018917527
test_loss: 0.002813842
train_loss: 0.0018067393
test_loss: 0.0028847167
train_loss: 0.0018233976
test_loss: 0.0028243244
train_loss: 0.001675385
test_loss: 0.0027984218
train_loss: 0.0018292912
test_loss: 0.0027585838
train_loss: 0.0017715467
test_loss: 0.002865464
train_loss: 0.0016647719
test_loss: 0.0028690877
train_loss: 0.0017272278
test_loss: 0.0027342797
train_loss: 0.0017281394
test_loss: 0.0028165404
train_loss: 0.0016876
test_loss: 0.0027666353
train_loss: 0.001808735
test_loss: 0.0029360978
train_loss: 0.0017535731
test_loss: 0.0028491006
train_loss: 0.0017629979
test_loss: 0.0029763964
train_loss: 0.001791596
test_loss: 0.0028887459
train_loss: 0.0019337144
test_loss: 0.0029553003
train_loss: 0.0016426
test_loss: 0.0028799626
train_loss: 0.0017066109
test_loss: 0.0028936064
train_loss: 0.002060813
test_loss: 0.0029261077
train_loss: 0.0019785906
test_loss: 0.0029607876
train_loss: 0.0019116127
test_loss: 0.002904731
train_loss: 0.002017361
test_loss: 0.0028631962
train_loss: 0.0018449978
test_loss: 0.0027762782
train_loss: 0.0018879438
test_loss: 0.0029153528
train_loss: 0.0020703468
test_loss: 0.0029278172
train_loss: 0.0018538854
test_loss: 0.0028654614
train_loss: 0.0017142463
test_loss: 0.0028391855
train_loss: 0.001690737
test_loss: 0.0028003312
train_loss: 0.0017924875
test_loss: 0.0027988253
train_loss: 0.0020127874
test_loss: 0.0028596164
train_loss: 0.0017090803
test_loss: 0.0029428932
train_loss: 0.0018922971
test_loss: 0.002880172
train_loss: 0.0017533188
test_loss: 0.0028243717
train_loss: 0.001867961
test_loss: 0.0029188932
train_loss: 0.0021436254
test_loss: 0.0028765276
train_loss: 0.0019800025
test_loss: 0.002881895
train_loss: 0.0018139855
test_loss: 0.0029864283
train_loss: 0.0017567702
test_loss: 0.0028730594
train_loss: 0.0017600945
test_loss: 0.0028251174
train_loss: 0.0018815841
test_loss: 0.0029930028
train_loss: 0.0018993751
test_loss: 0.002977816
train_loss: 0.0018477035
test_loss: 0.0028838532
train_loss: 0.001971493
test_loss: 0.0029200658
train_loss: 0.001914847
test_loss: 0.0029632438
train_loss: 0.001978904
test_loss: 0.0029546844
train_loss: 0.0018436209
test_loss: 0.0028961145
train_loss: 0.001873311
test_loss: 0.002839446
train_loss: 0.001658453
test_loss: 0.00292365
train_loss: 0.001756202
test_loss: 0.002814605
train_loss: 0.001864952
test_loss: 0.0029270134
train_loss: 0.0018257584
test_loss: 0.0029008011
train_loss: 0.0019372553
test_loss: 0.0029336493
train_loss: 0.0019325812
test_loss: 0.0028515803
train_loss: 0.0018677525
test_loss: 0.002884763
train_loss: 0.0020021575
test_loss: 0.0029210665
train_loss: 0.0019508414
test_loss: 0.0029399318
train_loss: 0.0020702207
test_loss: 0.0030289113
train_loss: 0.002133663
test_loss: 0.0030150781
train_loss: 0.0020769392
test_loss: 0.003121337
train_loss: 0.0021005485
test_loss: 0.0029948973
train_loss: 0.0019216338
test_loss: 0.002941878
train_loss: 0.0018120476
test_loss: 0.0028221095
train_loss: 0.0019136341
test_loss: 0.002856146
train_loss: 0.001739786
test_loss: 0.0028671287
train_loss: 0.0017884095
test_loss: 0.0029001818
train_loss: 0.0018388235
test_loss: 0.0028241202
train_loss: 0.0016932179
test_loss: 0.0029015706
train_loss: 0.0018019557
test_loss: 0.0028211544
train_loss: 0.0016810689
test_loss: 0.0028731776
train_loss: 0.0017316868
test_loss: 0.0030180889
train_loss: 0.0016998035
test_loss: 0.0029771787
train_loss: 0.0019896752
test_loss: 0.0029962137
train_loss: 0.0019382301
test_loss: 0.0029529952
train_loss: 0.0020101687
test_loss: 0.002879528
train_loss: 0.001989623
test_loss: 0.0028884427
train_loss: 0.0017966287
test_loss: 0.0027905304
train_loss: 0.0016946561
test_loss: 0.0027739157
train_loss: 0.0018291149
test_loss: 0.002790798
train_loss: 0.00167595
test_loss: 0.0028356174
train_loss: 0.0017461975
test_loss: 0.0028934488
train_loss: 0.0016163306
test_loss: 0.002777815
train_loss: 0.0017446151
test_loss: 0.0028897652
train_loss: 0.0018407686
test_loss: 0.0029333902
train_loss: 0.001896671
test_loss: 0.0028444754
train_loss: 0.0016935895
test_loss: 0.002947377
train_loss: 0.0017604039
test_loss: 0.0028652346
train_loss: 0.0019066703
test_loss: 0.0030686518
train_loss: 0.0020610054
test_loss: 0.0029795838
train_loss: 0.0018650984
test_loss: 0.0029456106
train_loss: 0.0018422588
test_loss: 0.002872675
train_loss: 0.001958137
test_loss: 0.0029630878
train_loss: 0.001803505
test_loss: 0.0028343906
train_loss: 0.0017166748
test_loss: 0.00283481
train_loss: 0.0017473651
test_loss: 0.0028857444
train_loss: 0.001882832
test_loss: 0.0030201322
train_loss: 0.0019954154
test_loss: 0.0029534681
train_loss: 0.0018634583
test_loss: 0.0028870308
train_loss: 0.0016496258
test_loss: 0.0028638025
train_loss: 0.0017044658
test_loss: 0.002804853
train_loss: 0.0017808911
test_loss: 0.0029591213
train_loss: 0.0018890377
test_loss: 0.002888392
train_loss: 0.0017784215
test_loss: 0.002866262
train_loss: 0.0016652757
test_loss: 0.002820074
train_loss: 0.001890197
test_loss: 0.0029379188
train_loss: 0.0018745433
test_loss: 0.0029344077
train_loss: 0.0019788465
test_loss: 0.0028106403
train_loss: 0.0016996815
test_loss: 0.0028763697
train_loss: 0.0016382721
test_loss: 0.0028648977
train_loss: 0.0017730551
test_loss: 0.0028993262
train_loss: 0.0017445632
test_loss: 0.0028260937
train_loss: 0.001727201
test_loss: 0.0028968235
train_loss: 0.0017225966
test_loss: 0.002871865
train_loss: 0.0017038977
test_loss: 0.0029211915
train_loss: 0.0017045776
test_loss: 0.002853531
train_loss: 0.0017412072
test_loss: 0.0028363327
train_loss: 0.0017737808
test_loss: 0.0028916579
train_loss: 0.0017025452
test_loss: 0.0028540527
train_loss: 0.0017520385
test_loss: 0.002870126
train_loss: 0.0019406747
test_loss: 0.0028784242
train_loss: 0.0019452735
test_loss: 0.0028833218
train_loss: 0.0018825481
test_loss: 0.002922001
train_loss: 0.0017167032
test_loss: 0.0029684252
train_loss: 0.0017949798
test_loss: 0.0029195298
train_loss: 0.001980884
test_loss: 0.0028895254
train_loss: 0.0018197116
test_loss: 0.0028806434
train_loss: 0.0018342447
test_loss: 0.0028550543
train_loss: 0.0018737434
test_loss: 0.0029610915
train_loss: 0.001766248
test_loss: 0.002924271
train_loss: 0.0018649291
test_loss: 0.0028994165
train_loss: 0.0018465291
test_loss: 0.0029470904
train_loss: 0.0018231978
test_loss: 0.002911379
train_loss: 0.0016931748
test_loss: 0.0029374268
train_loss: 0.0019149126
test_loss: 0.0029998517
train_loss: 0.0018048555
test_loss: 0.0029928805
train_loss: 0.0018905547
test_loss: 0.002953079
train_loss: 0.0017992319
test_loss: 0.0028557954
train_loss: 0.0016618789
test_loss: 0.0028082198
train_loss: 0.0018227561
test_loss: 0.0027709932
train_loss: 0.0017025501
test_loss: 0.0028439513
train_loss: 0.0017812748
test_loss: 0.0028907806
train_loss: 0.0019169219
test_loss: 0.0029884954
train_loss: 0.0017261005
test_loss: 0.0029834185
train_loss: 0.0018487445
test_loss: 0.003023783
train_loss: 0.0018105544
test_loss: 0.0028008074
train_loss: 0.0017176887
test_loss: 0.0028796317
train_loss: 0.0018052885
test_loss: 0.0030536605
train_loss: 0.0018519282
test_loss: 0.0029430706
train_loss: 0.0017252829
test_loss: 0.0028792457
train_loss: 0.0017030961
test_loss: 0.0028767241
train_loss: 0.0017042523
test_loss: 0.0027769192
train_loss: 0.0017221784
test_loss: 0.002878577
train_loss: 0.0017223614
test_loss: 0.0028026416
train_loss: 0.0016454379
test_loss: 0.0028512054
train_loss: 0.0016078055
test_loss: 0.0028303436
train_loss: 0.0016803783
test_loss: 0.0028456051
train_loss: 0.0016255223
test_loss: 0.0028234394
train_loss: 0.0019085323
test_loss: 0.0030289525
train_loss: 0.0017484465
test_loss: 0.002921813
train_loss: 0.0018447831
test_loss: 0.0029308514
train_loss: 0.001686665
test_loss: 0.0028002353
train_loss: 0.001648647
test_loss: 0.0028264245
train_loss: 0.0018548465
test_loss: 0.0029786027
train_loss: 0.0018999045
test_loss: 0.0028957
train_loss: 0.001869398
test_loss: 0.0029007597
train_loss: 0.0017257794
test_loss: 0.0029794974
train_loss: 0.0017641969
test_loss: 0.0030362222
train_loss: 0.0019505564
test_loss: 0.002936239
train_loss: 0.0016835651
test_loss: 0.0028927238
train_loss: 0.0016973733
test_loss: 0.0029734569
train_loss: 0.001663647
test_loss: 0.002861376
train_loss: 0.0016424884
test_loss: 0.002911869
train_loss: 0.0016807725
test_loss: 0.0028285806
train_loss: 0.0016897536
test_loss: 0.0027917929
train_loss: 0.0016340311
test_loss: 0.002945059
train_loss: 0.0017008653
test_loss: 0.00301375
train_loss: 0.0019658126
test_loss: 0.0029943932
train_loss: 0.0018059066
test_loss: 0.0029546511
train_loss: 0.0018257682
test_loss: 0.0029310784
train_loss: 0.0016973709
test_loss: 0.002929667
train_loss: 0.001733006
test_loss: 0.0028398829
train_loss: 0.0015857865
test_loss: 0.0028623191
train_loss: 0.0017303852
test_loss: 0.002955624
train_loss: 0.001710171
test_loss: 0.0029451349
train_loss: 0.0018442331
test_loss: 0.0030454334
train_loss: 0.0020977592
test_loss: 0.0031283537
train_loss: 0.002001473
test_loss: 0.0030713803
train_loss: 0.0018405032
test_loss: 0.0030967917
train_loss: 0.0018091684
test_loss: 0.0030331386
train_loss: 0.0019643842
test_loss: 0.0029456841
train_loss: 0.0018894607
test_loss: 0.0028730242
train_loss: 0.0017273205
test_loss: 0.0028160089
train_loss: 0.0016586522
test_loss: 0.003014973
train_loss: 0.0016696212
test_loss: 0.0029224
train_loss: 0.0018063775
test_loss: 0.0028923214
train_loss: 0.001675939
test_loss: 0.0028738182
train_loss: 0.0016855032
test_loss: 0.0029687558
train_loss: 0.0015677764
test_loss: 0.002854902
train_loss: 0.0016613126
test_loss: 0.0028287007
train_loss: 0.0015790563
test_loss: 0.00292536
train_loss: 0.001651623
test_loss: 0.002864248
train_loss: 0.0018280905
test_loss: 0.0029352012
train_loss: 0.0017988663
test_loss: 0.0030331882
train_loss: 0.0017625384
test_loss: 0.0029003546
train_loss: 0.0017527264
test_loss: 0.0029535596
train_loss: 0.0017771563
test_loss: 0.0029941506
train_loss: 0.0016845437
test_loss: 0.002955532
train_loss: 0.001849541
test_loss: 0.0030069265
train_loss: 0.0020160466
test_loss: 0.0030720283
train_loss: 0.0018720493
test_loss: 0.002975887
train_loss: 0.0020166482
test_loss: 0.0030672434
train_loss: 0.0018927112
test_loss: 0.0029559338
train_loss: 0.0018272381
test_loss: 0.002886224
train_loss: 0.0018360799
test_loss: 0.002947502
train_loss: 0.0019541911
test_loss: 0.0028968132
train_loss: 0.0017023616
test_loss: 0.0028859694
train_loss: 0.0018050554
test_loss: 0.0030239616
train_loss: 0.0021849452
test_loss: 0.0030837653
train_loss: 0.0019416935
test_loss: 0.003158663
train_loss: 0.0019903635
test_loss: 0.0030842752
train_loss: 0.0019617495
test_loss: 0.0030961556
train_loss: 0.0020574154
test_loss: 0.0030126309
train_loss: 0.0019320801
test_loss: 0.0029851173
train_loss: 0.0016561709
test_loss: 0.0029411302
train_loss: 0.0017045027
test_loss: 0.0028989336
train_loss: 0.0016194495
test_loss: 0.0028797535
train_loss: 0.0017811409
test_loss: 0.002941119
train_loss: 0.0017045562
test_loss: 0.0029328293
train_loss: 0.0016851611
test_loss: 0.0028672926
train_loss: 0.0017493463
test_loss: 0.0028779511
train_loss: 0.0016178519
test_loss: 0.0029206863
train_loss: 0.0017521593
test_loss: 0.0029129817
train_loss: 0.001859392
test_loss: 0.0029697905
train_loss: 0.0019447304
test_loss: 0.003056571
train_loss: 0.0018714032
test_loss: 0.002943118
train_loss: 0.0017775949
test_loss: 0.003021125
train_loss: 0.0017255048
test_loss: 0.0029356792
train_loss: 0.0016774544
test_loss: 0.002922067
train_loss: 0.0016470901
test_loss: 0.0030862603
train_loss: 0.0017401509
test_loss: 0.002966953
train_loss: 0.0017923221
test_loss: 0.0029442445
train_loss: 0.0018983549
test_loss: 0.0030248933
train_loss: 0.0017810754
test_loss: 0.0032187165
train_loss: 0.002123327
test_loss: 0.0028934102
train_loss: 0.0018878276
test_loss: 0.0029661804
train_loss: 0.0018513598
test_loss: 0.0029257045
train_loss: 0.0017202414
test_loss: 0.0028825146
train_loss: 0.0016330865
test_loss: 0.0029429703
train_loss: 0.001660075
test_loss: 0.0029128871
train_loss: 0.0016031897
test_loss: 0.0028561149
train_loss: 0.0017027722
test_loss: 0.002881612
train_loss: 0.0017388857
test_loss: 0.0028670505
train_loss: 0.0017355374
test_loss: 0.0028876122
train_loss: 0.0016448284
test_loss: 0.0029378238
train_loss: 0.0017978535
test_loss: 0.002918413
train_loss: 0.0017030189
test_loss: 0.0030971034
train_loss: 0.0018836553
test_loss: 0.0030896165
train_loss: 0.0018849787
test_loss: 0.0029758322
train_loss: 0.0017830323
test_loss: 0.0030107582
train_loss: 0.0016810296
test_loss: 0.0028732885
train_loss: 0.0016876067
test_loss: 0.0028667666
train_loss: 0.001763307
test_loss: 0.002840398
train_loss: 0.0020139255
test_loss: 0.0029062028
train_loss: 0.0018515433
test_loss: 0.0029720748
train_loss: 0.001768325
test_loss: 0.002849366
train_loss: 0.0017394938
test_loss: 0.0030071763
train_loss: 0.0017261016
test_loss: 0.0029123935
train_loss: 0.0017986036
test_loss: 0.002949646
train_loss: 0.0017868488
test_loss: 0.0029330114
train_loss: 0.0018402868
test_loss: 0.0029047362
train_loss: 0.0017896676
test_loss: 0.0028854744
train_loss: 0.0017412292
test_loss: 0.002982765
train_loss: 0.0015163617
test_loss: 0.0029541287
train_loss: 0.0015935994
test_loss: 0.0029870104
train_loss: 0.0017735342
test_loss: 0.0029606265
train_loss: 0.0017359478
test_loss: 0.0028924586
train_loss: 0.0018791894
test_loss: 0.0030260289
train_loss: 0.0017556208
test_loss: 0.0029864197
train_loss: 0.00188309
test_loss: 0.0029183463
train_loss: 0.0017730333
test_loss: 0.0029284644
train_loss: 0.0017825142
test_loss: 0.002985007
train_loss: 0.0018086127
test_loss: 0.0033128178
train_loss: 0.0022523808
test_loss: 0.002965251
train_loss: 0.0017064258
test_loss: 0.0030814935
train_loss: 0.0017361846
test_loss: 0.0029833878
train_loss: 0.0016871815
test_loss: 0.002864736
train_loss: 0.0018149598
test_loss: 0.0028988784
train_loss: 0.0017115492
test_loss: 0.002895982
train_loss: 0.0016992693
test_loss: 0.0030167229
train_loss: 0.0018471556
test_loss: 0.0030985812
train_loss: 0.0019262831
test_loss: 0.0030412714
train_loss: 0.0018252839
test_loss: 0.0028909962
train_loss: 0.0018216646
test_loss: 0.003082113
train_loss: 0.0018378636
test_loss: 0.003049116
train_loss: 0.0019106572
test_loss: 0.0030188714
train_loss: 0.0018440443
test_loss: 0.002908267
train_loss: 0.0018652069
test_loss: 0.0029125705
train_loss: 0.00178878
test_loss: 0.0029926456
train_loss: 0.0017021669
test_loss: 0.0030000096
train_loss: 0.0018584155
test_loss: 0.0029529042
train_loss: 0.0017476747
test_loss: 0.0029908146
train_loss: 0.0016719305
test_loss: 0.0029621145
train_loss: 0.0016875758
test_loss: 0.0029226637
train_loss: 0.0017773489
test_loss: 0.0029706475
train_loss: 0.0017156675
test_loss: 0.0029408084
train_loss: 0.0017613091
test_loss: 0.00310088
train_loss: 0.0019862002
test_loss: 0.0030287371
train_loss: 0.002077093
test_loss: 0.0029654244
train_loss: 0.0017115532
test_loss: 0.002937436
train_loss: 0.001672111
test_loss: 0.0029456
train_loss: 0.0016060808
test_loss: 0.002980625
train_loss: 0.0016616716
test_loss: 0.0030793913
train_loss: 0.0018300391
test_loss: 0.003088067
train_loss: 0.0018284002
test_loss: 0.0030226344
train_loss: 0.0018694676
test_loss: 0.0029772343
train_loss: 0.0018104373
test_loss: 0.0031058262
train_loss: 0.0018493321
test_loss: 0.0029296211
train_loss: 0.0017956792
test_loss: 0.0029586968
train_loss: 0.0018309922
test_loss: 0.0030001758
train_loss: 0.0018842873
test_loss: 0.0030565653
train_loss: 0.0019983412
test_loss: 0.0030247539
train_loss: 0.0018085912
test_loss: 0.003188499
train_loss: 0.0020299605
test_loss: 0.0031093934
train_loss: 0.0018325993
test_loss: 0.003052344
train_loss: 0.0016980956
test_loss: 0.0029557042
train_loss: 0.0017668033
test_loss: 0.0030192663
train_loss: 0.0018291767
test_loss: 0.0030710844
train_loss: 0.0016536724
test_loss: 0.0030715703
train_loss: 0.0017172581
test_loss: 0.0030586468
train_loss: 0.0018728344
test_loss: 0.0030206572
train_loss: 0.0017812669
test_loss: 0.0030409638
train_loss: 0.0017146937
test_loss: 0.002923058
train_loss: 0.0017166587
test_loss: 0.0029253305
train_loss: 0.0017114095
test_loss: 0.0030734632
train_loss: 0.0019232496
test_loss: 0.0030047519
train_loss: 0.0019043904
test_loss: 0.0029986755
train_loss: 0.0017875365
test_loss: 0.0030510155
train_loss: 0.0017465404
test_loss: 0.0031606646
train_loss: 0.0018800289
test_loss: 0.0029946386
train_loss: 0.0018396131
test_loss: 0.0030073267
train_loss: 0.001762454
test_loss: 0.0030549997
train_loss: 0.0018809144
test_loss: 0.0032553433
train_loss: 0.0018720462
test_loss: 0.0030178416
train_loss: 0.0018129095
test_loss: 0.002951526
train_loss: 0.0016702709
test_loss: 0.0030915346
train_loss: 0.001812924
test_loss: 0.002985417
train_loss: 0.0016814888
test_loss: 0.0030078448
train_loss: 0.0017103935
test_loss: 0.003011879
train_loss: 0.001922372
test_loss: 0.0029809023
train_loss: 0.0019146531
test_loss: 0.0029937285
train_loss: 0.0016568195
test_loss: 0.0031052867
train_loss: 0.0019807864
test_loss: 0.0031508354
train_loss: 0.0019112179
test_loss: 0.0030741151
train_loss: 0.0019316371
test_loss: 0.0031096512
train_loss: 0.0018662754
test_loss: 0.0031409282
train_loss: 0.0018071812
test_loss: 0.0030206305
train_loss: 0.0018303613
test_loss: 0.0029779822
train_loss: 0.0016205801
test_loss: 0.0029950715
train_loss: 0.0018675029
test_loss: 0.0030200768
train_loss: 0.0020505583
test_loss: 0.003089064
train_loss: 0.001814845
test_loss: 0.003150495
train_loss: 0.001927632
test_loss: 0.0030127252
train_loss: 0.0017613432
test_loss: 0.0030668816
train_loss: 0.0018889023
test_loss: 0.0030177596
train_loss: 0.0017072811
test_loss: 0.003052998
train_loss: 0.0017983956
test_loss: 0.0029733991
train_loss: 0.0019426993
test_loss: 0.0031159173
train_loss: 0.0019570908
test_loss: 0.0031820463
train_loss: 0.0019229302
test_loss: 0.0030471603
train_loss: 0.0018497265
test_loss: 0.0031529563
train_loss: 0.0017118491
test_loss: 0.0030737193
train_loss: 0.0017476704
test_loss: 0.0031016238
train_loss: 0.0017691609
test_loss: 0.003041784
train_loss: 0.0019339687
test_loss: 0.003150306
train_loss: 0.001689313
test_loss: 0.0029791624
train_loss: 0.0016277309
test_loss: 0.0029275105
train_loss: 0.0017698166
test_loss: 0.0030822612
train_loss: 0.0017371223
test_loss: 0.00289095
train_loss: 0.0016132846
test_loss: 0.0028847118
train_loss: 0.001814126
test_loss: 0.0029069493
train_loss: 0.0016607863
test_loss: 0.0029107705
train_loss: 0.0017405581
test_loss: 0.0030089326
train_loss: 0.0017602015
test_loss: 0.0029706133
train_loss: 0.0016354363
test_loss: 0.002988549
train_loss: 0.0016834537
test_loss: 0.0030636468
train_loss: 0.0018530132
test_loss: 0.0030353635
train_loss: 0.0017852833
test_loss: 0.002956594
train_loss: 0.001745014
test_loss: 0.00298695
train_loss: 0.001682364
test_loss: 0.0030154444
train_loss: 0.0017136617
test_loss: 0.0031602432
train_loss: 0.0017493062
test_loss: 0.0030511492
train_loss: 0.0017997838
test_loss: 0.0029881909
train_loss: 0.0018917965
test_loss: 0.0029466138
train_loss: 0.0017931911
test_loss: 0.0029850462
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0016608541
test_loss: 0.0030343826
train_loss: 0.0016863733
test_loss: 0.0030629945
train_loss: 0.0017877519
test_loss: 0.0032232609
train_loss: 0.0023466488
test_loss: 0.0031197523
train_loss: 0.0018263254
test_loss: 0.0033275571
train_loss: 0.001950665
test_loss: 0.0030591395
train_loss: 0.0017323856
test_loss: 0.0030493336
train_loss: 0.0018428119
test_loss: 0.0031845362
train_loss: 0.001961458
test_loss: 0.0031215567
train_loss: 0.00206666
test_loss: 0.003110273
train_loss: 0.0016657659
test_loss: 0.0032816043
train_loss: 0.0020802503
test_loss: 0.003022101
train_loss: 0.0017634092
test_loss: 0.003060592
train_loss: 0.0018971998
test_loss: 0.0029602896
train_loss: 0.0016052157
test_loss: 0.0030584608
train_loss: 0.001748187
test_loss: 0.0029903068
train_loss: 0.0017345761
test_loss: 0.003104298
train_loss: 0.0016762156
test_loss: 0.0030238666
train_loss: 0.001769471
test_loss: 0.0029687516
train_loss: 0.0017431946
test_loss: 0.0029736394
train_loss: 0.0016158613
test_loss: 0.002931078
train_loss: 0.0017219471
test_loss: 0.0029008205
train_loss: 0.0017002672
test_loss: 0.0029822693
train_loss: 0.0015664282
test_loss: 0.0029221668
train_loss: 0.0015242508
test_loss: 0.0029222372
train_loss: 0.0015751899
test_loss: 0.0030193068
train_loss: 0.0017410491
test_loss: 0.0031075794
train_loss: 0.0016720971
test_loss: 0.0030544656
train_loss: 0.0017741097
test_loss: 0.0029409116
train_loss: 0.0017483498
test_loss: 0.0029599124
train_loss: 0.0015694499
test_loss: 0.002949906
train_loss: 0.0016768291
test_loss: 0.0029977884
train_loss: 0.0017307664
test_loss: 0.0030071505
train_loss: 0.0016808084
test_loss: 0.0029509673
train_loss: 0.0016897509
test_loss: 0.00297774
train_loss: 0.001610594
test_loss: 0.00300102
train_loss: 0.0018074709
test_loss: 0.003094547
train_loss: 0.0018352105
test_loss: 0.0030050084
train_loss: 0.0016263436
test_loss: 0.0031194105
train_loss: 0.0016832448
test_loss: 0.003033105
train_loss: 0.0016933911
test_loss: 0.0030332718
train_loss: 0.0017700136
test_loss: 0.0031589305
train_loss: 0.0019102201
test_loss: 0.0031065627
train_loss: 0.0019400949
test_loss: 0.0030503953
train_loss: 0.001809052
test_loss: 0.0029795922
train_loss: 0.0017225668
test_loss: 0.0031709524
train_loss: 0.0019860598
test_loss: 0.0029777654
train_loss: 0.0016476107
test_loss: 0.0029697348
train_loss: 0.0015783636
test_loss: 0.003121667
train_loss: 0.0018266053
test_loss: 0.0030589744
train_loss: 0.0017748594
test_loss: 0.003013788
train_loss: 0.0017779578
test_loss: 0.003087866
train_loss: 0.0018451023
test_loss: 0.0031176978
train_loss: 0.0016206083
test_loss: 0.0030766104
train_loss: 0.0017054259
test_loss: 0.0029191503
train_loss: 0.001774718
test_loss: 0.003021141
train_loss: 0.0017121639
test_loss: 0.00305643
train_loss: 0.0016439658
test_loss: 0.0030107044
train_loss: 0.0015987868
test_loss: 0.0030091736
train_loss: 0.0017436636
test_loss: 0.003057927
train_loss: 0.0018121658
test_loss: 0.003023017
train_loss: 0.0016944582
test_loss: 0.0030110532
train_loss: 0.0016889828
test_loss: 0.0029518
train_loss: 0.0016243822
test_loss: 0.0029876493
train_loss: 0.0016647653
test_loss: 0.002967007
train_loss: 0.0016173325
test_loss: 0.0030457566
train_loss: 0.0017086959
test_loss: 0.0029959143
train_loss: 0.0016351466
test_loss: 0.0030165776
train_loss: 0.0016763929
test_loss: 0.0030785524
train_loss: 0.0017017261
test_loss: 0.0030997382
train_loss: 0.0016995767
test_loss: 0.0029672163
train_loss: 0.001734379
test_loss: 0.0030377428
train_loss: 0.0017602191
test_loss: 0.003127289
train_loss: 0.0016685349
test_loss: 0.0030024077
train_loss: 0.001616579
test_loss: 0.0029764478
train_loss: 0.0017849228
test_loss: 0.0031049908
train_loss: 0.0018233868
test_loss: 0.0032154785
train_loss: 0.0018152304
test_loss: 0.0031121112
train_loss: 0.0017755548
test_loss: 0.0030789995
train_loss: 0.0017727409
test_loss: 0.003013732
train_loss: 0.0018223153
test_loss: 0.0030572552
train_loss: 0.0018571926
test_loss: 0.0030972857
train_loss: 0.0018403609
test_loss: 0.0031100642
train_loss: 0.0020019568
test_loss: 0.0031123508
train_loss: 0.001816398
test_loss: 0.0030896105
train_loss: 0.0018433268
test_loss: 0.0030097705
train_loss: 0.0016768184
test_loss: 0.0031571644
train_loss: 0.0017252439
test_loss: 0.003115237
train_loss: 0.0018398567
test_loss: 0.0030965167
train_loss: 0.0018994446
test_loss: 0.0030712287
train_loss: 0.0016010429
test_loss: 0.0029950116
train_loss: 0.0015987917
test_loss: 0.0029479752
train_loss: 0.0016769678
test_loss: 0.003229288
train_loss: 0.0019219117
test_loss: 0.0030872873
train_loss: 0.0017566499
test_loss: 0.003017077
train_loss: 0.0016918508
test_loss: 0.0031606664
train_loss: 0.0017856683
test_loss: 0.003144501
train_loss: 0.0017947017
test_loss: 0.0031170393
train_loss: 0.0017668199
test_loss: 0.0031234853
train_loss: 0.0017434332
test_loss: 0.0031965883
train_loss: 0.0018766277
test_loss: 0.003077184
train_loss: 0.0016749504
test_loss: 0.0030078657
train_loss: 0.0016131806
test_loss: 0.003024031
train_loss: 0.001743923
test_loss: 0.0031289677
train_loss: 0.0018244672
test_loss: 0.0031494598
train_loss: 0.0018682592
test_loss: 0.003061956
train_loss: 0.001680473
test_loss: 0.0030357186
train_loss: 0.001708481
test_loss: 0.0030962557
train_loss: 0.001721905
test_loss: 0.0031852012
train_loss: 0.001815533
test_loss: 0.0030973041
train_loss: 0.0018709078
test_loss: 0.0032437057
train_loss: 0.0018476605
test_loss: 0.0030864922
train_loss: 0.0016411274
test_loss: 0.0030384932
train_loss: 0.0016837763
test_loss: 0.0030728336
train_loss: 0.001761703
test_loss: 0.0031328101
train_loss: 0.0018512998
test_loss: 0.0030431454
train_loss: 0.0018451705
test_loss: 0.0030938077
train_loss: 0.001776828
test_loss: 0.0030928266
train_loss: 0.0017963217
test_loss: 0.0031029133
train_loss: 0.0017934211
test_loss: 0.0031113424
train_loss: 0.001841855
test_loss: 0.0031006983
train_loss: 0.0016601756
test_loss: 0.0030570962
train_loss: 0.0017103493
test_loss: 0.003033467
train_loss: 0.0017031161
test_loss: 0.0031445522
train_loss: 0.0017616294
test_loss: 0.0031064444
train_loss: 0.0017817754
test_loss: 0.0031153222
train_loss: 0.0019060401
test_loss: 0.003101697
train_loss: 0.0016205413
test_loss: 0.0031511073
train_loss: 0.0017760813
test_loss: 0.0031544713
train_loss: 0.0018131731
test_loss: 0.0031482503
train_loss: 0.0017734264
test_loss: 0.003094603
train_loss: 0.0016145888
test_loss: 0.0030944603
train_loss: 0.0016317258
test_loss: 0.0030642494
train_loss: 0.001714383
test_loss: 0.0030395845
train_loss: 0.0016373624
test_loss: 0.0030524773
train_loss: 0.0016680774
test_loss: 0.0029650598
train_loss: 0.0016780585
test_loss: 0.0031204147
train_loss: 0.0017753972
test_loss: 0.003255329
train_loss: 0.0017937734
test_loss: 0.0032505523
train_loss: 0.0018785216
test_loss: 0.0030515583
train_loss: 0.0016253022
test_loss: 0.0030831448
train_loss: 0.0016995644
test_loss: 0.0031702928
train_loss: 0.0018229471
test_loss: 0.0030659907
train_loss: 0.0017622864
test_loss: 0.0032204038
train_loss: 0.0016389609
test_loss: 0.0031230997
train_loss: 0.0016876776
test_loss: 0.0032343748
train_loss: 0.001821087
test_loss: 0.0031009077
train_loss: 0.0016720574
test_loss: 0.0031582485
train_loss: 0.0016729167
test_loss: 0.0030823185
train_loss: 0.0016310487
test_loss: 0.0030833397
train_loss: 0.0017953169
test_loss: 0.0031313093
train_loss: 0.0016418959
test_loss: 0.003198758
train_loss: 0.0018343814
test_loss: 0.0032223868
train_loss: 0.002026883
test_loss: 0.0030972608
train_loss: 0.0018053893
test_loss: 0.0035031002
train_loss: 0.0021623387
test_loss: 0.0032882856
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1 --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b62d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b67d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b698bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b67d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b5f5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b543bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b5f5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b4ca6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b4de378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b4a4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b4a49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b475048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b44d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b475598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe14b475ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0e6f54510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0e6f546a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0e6f21378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0e6ed52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0e6ef8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0e6ef8d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0e6eb5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c0743a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c0727730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c0727b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c06c62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c0695620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c0695c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c06472f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c066cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c060e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c0636598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c0636268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c05f0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c05a28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe0c05b1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.09567506
test_loss: 0.090925
train_loss: 0.07460588
test_loss: 0.07446611
train_loss: 0.06323123
test_loss: 0.061169825
train_loss: 0.052458197
test_loss: 0.051383533
train_loss: 0.04263374
test_loss: 0.043127116
train_loss: 0.036212515
test_loss: 0.036373407
train_loss: 0.029187024
test_loss: 0.030258793
train_loss: 0.02372916
test_loss: 0.025415596
train_loss: 0.020254696
test_loss: 0.021964673
train_loss: 0.017947907
test_loss: 0.01933607
train_loss: 0.015485976
test_loss: 0.017263938
train_loss: 0.013584308
test_loss: 0.015867071
train_loss: 0.012017283
test_loss: 0.014430903
train_loss: 0.010822188
test_loss: 0.013456371
train_loss: 0.009757446
test_loss: 0.012504749
train_loss: 0.009351442
test_loss: 0.011576639
train_loss: 0.008614998
test_loss: 0.010867899
train_loss: 0.007888542
test_loss: 0.010257673
train_loss: 0.007307744
test_loss: 0.00980502
train_loss: 0.0070114657
test_loss: 0.009470851
train_loss: 0.006790871
test_loss: 0.009052403
train_loss: 0.0063217245
test_loss: 0.008691167
train_loss: 0.006025835
test_loss: 0.008257767
train_loss: 0.005872496
test_loss: 0.007999303
train_loss: 0.0056629265
test_loss: 0.007879
train_loss: 0.005425075
test_loss: 0.0076297116
train_loss: 0.005183745
test_loss: 0.0073467107
train_loss: 0.005342831
test_loss: 0.007258551
train_loss: 0.0052970843
test_loss: 0.0070981556
train_loss: 0.0048107887
test_loss: 0.0070185373
train_loss: 0.004760617
test_loss: 0.0068141376
train_loss: 0.0047307685
test_loss: 0.006709361
train_loss: 0.0045962576
test_loss: 0.0066056335
train_loss: 0.004379462
test_loss: 0.0064150947
train_loss: 0.004514652
test_loss: 0.006439635
train_loss: 0.0046577165
test_loss: 0.006410593
train_loss: 0.0042884266
test_loss: 0.006302127
train_loss: 0.004296844
test_loss: 0.0060303933
train_loss: 0.004169849
test_loss: 0.0060390756
train_loss: 0.0042424565
test_loss: 0.006155159
train_loss: 0.004116858
test_loss: 0.006043218
train_loss: 0.0038083026
test_loss: 0.0058774203
train_loss: 0.0038690632
test_loss: 0.005858934
train_loss: 0.0038703016
test_loss: 0.0057307812
train_loss: 0.0036046128
test_loss: 0.005756228
train_loss: 0.0038433778
test_loss: 0.005625284
train_loss: 0.0035010949
test_loss: 0.0055940063
train_loss: 0.003713755
test_loss: 0.005446797
train_loss: 0.0034227616
test_loss: 0.005639414
train_loss: 0.0035545137
test_loss: 0.0053537358
train_loss: 0.0034570724
test_loss: 0.0053543965
train_loss: 0.003355899
test_loss: 0.0053315
train_loss: 0.0033523007
test_loss: 0.0051761125
train_loss: 0.0031782119
test_loss: 0.0051828446
train_loss: 0.0033208241
test_loss: 0.0052318242
train_loss: 0.00326656
test_loss: 0.005249424
train_loss: 0.0034579893
test_loss: 0.005165919
train_loss: 0.0033115936
test_loss: 0.00531042
train_loss: 0.0033696564
test_loss: 0.0051695523
train_loss: 0.0033501138
test_loss: 0.005285528
train_loss: 0.0033666012
test_loss: 0.0051648216
train_loss: 0.0033972955
test_loss: 0.0051392843
train_loss: 0.0031233528
test_loss: 0.005037652
train_loss: 0.0032534366
test_loss: 0.005105374
train_loss: 0.0032495
test_loss: 0.0049847635
train_loss: 0.0029402152
test_loss: 0.005008167
train_loss: 0.0031369114
test_loss: 0.0048560607
train_loss: 0.002890951
test_loss: 0.004988314
train_loss: 0.0029368307
test_loss: 0.004875232
train_loss: 0.0029412515
test_loss: 0.005006785
train_loss: 0.003008253
test_loss: 0.0048101107
train_loss: 0.002900446
test_loss: 0.004897486
train_loss: 0.0029461435
test_loss: 0.004856767
train_loss: 0.0029770797
test_loss: 0.004840625
train_loss: 0.0028426086
test_loss: 0.0047995606
train_loss: 0.0027451175
test_loss: 0.004629543
train_loss: 0.0027779487
test_loss: 0.0046779877
train_loss: 0.0029357988
test_loss: 0.0047159246
train_loss: 0.002805198
test_loss: 0.0049391002
train_loss: 0.002861952
test_loss: 0.0046394025
train_loss: 0.0027409792
test_loss: 0.0048306393
train_loss: 0.00294104
test_loss: 0.004809482
train_loss: 0.002864667
test_loss: 0.004665834
train_loss: 0.002880359
test_loss: 0.004601693
train_loss: 0.0028189858
test_loss: 0.004575881
train_loss: 0.0028511814
test_loss: 0.0047235214
train_loss: 0.002648828
test_loss: 0.0046654986
train_loss: 0.0027027167
test_loss: 0.0045027295
train_loss: 0.0026901108
test_loss: 0.004499785
train_loss: 0.0026616296
test_loss: 0.0046617878
train_loss: 0.0027684588
test_loss: 0.0046621882
train_loss: 0.002776289
test_loss: 0.00478442
train_loss: 0.0027622613
test_loss: 0.0045261295
train_loss: 0.0028202543
test_loss: 0.004753146
train_loss: 0.002890043
test_loss: 0.0046868376
train_loss: 0.0028766743
test_loss: 0.004529388
train_loss: 0.0027342688
test_loss: 0.004490557
train_loss: 0.0026218947
test_loss: 0.004425074
train_loss: 0.0025473188
test_loss: 0.0045403466
train_loss: 0.0025555976
test_loss: 0.004518773
train_loss: 0.0026244991
test_loss: 0.00452965
train_loss: 0.0024913964
test_loss: 0.004568471
train_loss: 0.0026819813
test_loss: 0.0044040014
train_loss: 0.002523143
test_loss: 0.004404994
train_loss: 0.002551481
test_loss: 0.004491044
train_loss: 0.0026135112
test_loss: 0.004437889
train_loss: 0.0026527685
test_loss: 0.0043354025
train_loss: 0.0024450761
test_loss: 0.004420927
train_loss: 0.00245067
test_loss: 0.0042933957
train_loss: 0.0023055659
test_loss: 0.0044935783
train_loss: 0.0025853256
test_loss: 0.0045351777
train_loss: 0.0025010486
test_loss: 0.004385168
train_loss: 0.0025509307
test_loss: 0.0046184077
train_loss: 0.0027281563
test_loss: 0.0044571185
train_loss: 0.00254794
test_loss: 0.004415714
train_loss: 0.0025228406
test_loss: 0.004426165
train_loss: 0.002569232
test_loss: 0.004374219
train_loss: 0.0024548464
test_loss: 0.004444303
train_loss: 0.0023731035
test_loss: 0.004262099
train_loss: 0.002324999
test_loss: 0.004265507
train_loss: 0.0023809122
test_loss: 0.004407916
train_loss: 0.0022262656
test_loss: 0.0044325013
train_loss: 0.0024492028
test_loss: 0.0043279403
train_loss: 0.002343852
test_loss: 0.0043826764
train_loss: 0.002490586
test_loss: 0.004298294
train_loss: 0.0024759746
test_loss: 0.004268642
train_loss: 0.0024047056
test_loss: 0.0041916654
train_loss: 0.0024247845
test_loss: 0.00444154
train_loss: 0.0025463616
test_loss: 0.0042957813
train_loss: 0.0022776858
test_loss: 0.0041936403
train_loss: 0.0023225984
test_loss: 0.004278613
train_loss: 0.0021757642
test_loss: 0.004212119
train_loss: 0.002140835
test_loss: 0.004274845
train_loss: 0.0022870786
test_loss: 0.0042533693
train_loss: 0.0024144435
test_loss: 0.0043281806
train_loss: 0.002612106
test_loss: 0.0043671383
train_loss: 0.0022576025
test_loss: 0.0042260108
train_loss: 0.0022817291
test_loss: 0.0044099484
train_loss: 0.002365034
test_loss: 0.004360931
train_loss: 0.002522554
test_loss: 0.004235485
train_loss: 0.002187251
test_loss: 0.004233552
train_loss: 0.0022132876
test_loss: 0.0042378167
train_loss: 0.0022481342
test_loss: 0.0043098005
train_loss: 0.0023366408
test_loss: 0.004205571
train_loss: 0.0022577292
test_loss: 0.0042480757
train_loss: 0.0022245767
test_loss: 0.004266479
train_loss: 0.002437101
test_loss: 0.004295973
train_loss: 0.0023377659
test_loss: 0.004454711
train_loss: 0.0026110716
test_loss: 0.004304203
train_loss: 0.0023109317
test_loss: 0.00426836
train_loss: 0.0023096623
test_loss: 0.0043190755
train_loss: 0.0024089613
test_loss: 0.0043801204
train_loss: 0.0024149106
test_loss: 0.004263807
train_loss: 0.002329832
test_loss: 0.0041279024
train_loss: 0.0021346107
test_loss: 0.0042018425
train_loss: 0.0021198366
test_loss: 0.0041862368
train_loss: 0.0022261962
test_loss: 0.0040625427
train_loss: 0.0021175859
test_loss: 0.0041073454
train_loss: 0.0021283382
test_loss: 0.004254404
train_loss: 0.0022507529
test_loss: 0.0041726865
train_loss: 0.0022666126
test_loss: 0.00420941
train_loss: 0.0022515173
test_loss: 0.004355622
train_loss: 0.002324743
test_loss: 0.004088047
train_loss: 0.0022520954
test_loss: 0.004235862
train_loss: 0.002309234
test_loss: 0.0042435243
train_loss: 0.0022862735
test_loss: 0.004509465
train_loss: 0.0024632043
test_loss: 0.004292712
train_loss: 0.0023324895
test_loss: 0.004388492
train_loss: 0.0025167952
test_loss: 0.004174083
train_loss: 0.0021724326
test_loss: 0.004351927
train_loss: 0.002448052
test_loss: 0.00421875
train_loss: 0.0025947797
test_loss: 0.004260832
train_loss: 0.0023937759
test_loss: 0.0041956976
train_loss: 0.002449991
test_loss: 0.0042580613
train_loss: 0.0023549485
test_loss: 0.004301444
train_loss: 0.0024233577
test_loss: 0.0042326245
train_loss: 0.0022508712
test_loss: 0.0044144653
train_loss: 0.00239984
test_loss: 0.0042722183
train_loss: 0.0023142928
test_loss: 0.0042029303
train_loss: 0.002230308
test_loss: 0.0041726395
train_loss: 0.002195644
test_loss: 0.0041348687
train_loss: 0.0022193906
test_loss: 0.0042587514
train_loss: 0.0022759135
test_loss: 0.004128667
train_loss: 0.0021863899
test_loss: 0.0042024055
train_loss: 0.0021568995
test_loss: 0.0041245893
train_loss: 0.0021836394
test_loss: 0.004168975
train_loss: 0.0020654113
test_loss: 0.004138371
train_loss: 0.0021411157
test_loss: 0.004129476
train_loss: 0.002147456
test_loss: 0.0041870764
train_loss: 0.0021126003
test_loss: 0.0042025377
train_loss: 0.0020810138
test_loss: 0.004180982
train_loss: 0.0023903563
test_loss: 0.0041778916
train_loss: 0.002410145
test_loss: 0.0042146053
train_loss: 0.0021928058
test_loss: 0.0041973176
train_loss: 0.0023145091
test_loss: 0.004245766
train_loss: 0.0025407795
test_loss: 0.0042963987
train_loss: 0.0022794285
test_loss: 0.0043278025
train_loss: 0.0023346827
test_loss: 0.0042262655
train_loss: 0.002111304
test_loss: 0.004116097
train_loss: 0.0021167737
test_loss: 0.004196304
train_loss: 0.0020498883
test_loss: 0.004098486
train_loss: 0.0020022234
test_loss: 0.004221125
train_loss: 0.0021168657
test_loss: 0.0042165807
train_loss: 0.0022713072
test_loss: 0.0041176165
train_loss: 0.0023362215
test_loss: 0.004153922
train_loss: 0.0021855175
test_loss: 0.004348939
train_loss: 0.0023096746
test_loss: 0.0041669873
train_loss: 0.0022884416
test_loss: 0.0042014085
train_loss: 0.0020994097
test_loss: 0.0041484404
train_loss: 0.0023733536
test_loss: 0.004141118
train_loss: 0.0020911915
test_loss: 0.0042586895
train_loss: 0.0023527008
test_loss: 0.004234453
train_loss: 0.002110995
test_loss: 0.0042449576
train_loss: 0.0021532984
test_loss: 0.0041892086
train_loss: 0.001995688
test_loss: 0.0041023022
train_loss: 0.0020014793
test_loss: 0.004074343
train_loss: 0.0019807967
test_loss: 0.004127887
train_loss: 0.002099557
test_loss: 0.004119316
train_loss: 0.0020709748
test_loss: 0.0042741266
train_loss: 0.0021057213
test_loss: 0.004152344
train_loss: 0.002075159
test_loss: 0.0041686636
train_loss: 0.0021545838
test_loss: 0.00404521
train_loss: 0.0019761543
test_loss: 0.0040766923
train_loss: 0.0020744402
test_loss: 0.0040624077
train_loss: 0.0019993293
test_loss: 0.004008862
train_loss: 0.0018691027
test_loss: 0.004003553
train_loss: 0.0019771117
test_loss: 0.0040369276
train_loss: 0.0019058201
test_loss: 0.0040104804
train_loss: 0.0020666537
test_loss: 0.004132837
train_loss: 0.0021332696
test_loss: 0.0040424676
train_loss: 0.0019371731
test_loss: 0.00414187
train_loss: 0.0020056786
test_loss: 0.0041635744
train_loss: 0.002192103
test_loss: 0.0041563837
train_loss: 0.0022198865
test_loss: 0.0040255506
train_loss: 0.0020130838
test_loss: 0.0041175364
train_loss: 0.0021089292
test_loss: 0.0042265584
train_loss: 0.0021062414
test_loss: 0.0040794327
train_loss: 0.0019101566
test_loss: 0.004102629
train_loss: 0.001960726
test_loss: 0.0040964354
train_loss: 0.0021818113
test_loss: 0.004069137
train_loss: 0.0020674982
test_loss: 0.0040098354
train_loss: 0.001987212
test_loss: 0.0041642278
train_loss: 0.0020109126
test_loss: 0.003978894
train_loss: 0.0020720544
test_loss: 0.0040639807
train_loss: 0.0020467541
test_loss: 0.0041458122
train_loss: 0.0021999427
test_loss: 0.0042388453
train_loss: 0.0023860836
test_loss: 0.0041476693
train_loss: 0.0021192017
test_loss: 0.004068088
train_loss: 0.0021294877
test_loss: 0.004250505
train_loss: 0.0020696467
test_loss: 0.0042577544
train_loss: 0.0021170217
test_loss: 0.0041724346
train_loss: 0.0021779046
test_loss: 0.0041758954
train_loss: 0.0020599533
test_loss: 0.0041644885
train_loss: 0.0023014862
test_loss: 0.0042388816
train_loss: 0.002259385
test_loss: 0.0042545972
train_loss: 0.002068329
test_loss: 0.004100917
train_loss: 0.0020981147
test_loss: 0.004078616
train_loss: 0.002082947
test_loss: 0.00415579
train_loss: 0.0019607903
test_loss: 0.0040377215
train_loss: 0.0019778654
test_loss: 0.004235444
train_loss: 0.0020056635
test_loss: 0.00412618
train_loss: 0.002017478
test_loss: 0.0041122986
train_loss: 0.0021224557
test_loss: 0.0040865634
train_loss: 0.0020106214
test_loss: 0.004113689
train_loss: 0.0021160147
test_loss: 0.0041968767
train_loss: 0.002116926
test_loss: 0.004121274
train_loss: 0.0021077143
test_loss: 0.0040934333
train_loss: 0.0019973533
test_loss: 0.004046379
train_loss: 0.0019820102
test_loss: 0.0040004454
train_loss: 0.0019480067
test_loss: 0.004025768
train_loss: 0.0020634837
test_loss: 0.004116422
train_loss: 0.0019941279
test_loss: 0.004160194
train_loss: 0.0019750344
test_loss: 0.004043455
train_loss: 0.0019888321
test_loss: 0.0041143345
train_loss: 0.0021742238
test_loss: 0.0041224933
train_loss: 0.0021539545
test_loss: 0.0041126804
train_loss: 0.0021341196
test_loss: 0.0040830737
train_loss: 0.0020682192
test_loss: 0.0040539275
train_loss: 0.002059236
test_loss: 0.004099973
train_loss: 0.0018908819
test_loss: 0.004163683
train_loss: 0.0020329151
test_loss: 0.004207034
train_loss: 0.0020939726
test_loss: 0.0041171107
train_loss: 0.0021303692
test_loss: 0.0041743857
train_loss: 0.0021446734
test_loss: 0.0040761884
train_loss: 0.0019523676
test_loss: 0.0041227182
train_loss: 0.0022372673
test_loss: 0.0040885652
train_loss: 0.002102661
test_loss: 0.004032387
train_loss: 0.0019227782
test_loss: 0.004099014
train_loss: 0.002113031
test_loss: 0.0040691835
train_loss: 0.002111058
test_loss: 0.004056725
train_loss: 0.0020755087
test_loss: 0.004151518
train_loss: 0.0022368927
test_loss: 0.0041611977
train_loss: 0.002119056
test_loss: 0.004050406
train_loss: 0.0019256216
test_loss: 0.004063344
train_loss: 0.0019626115
test_loss: 0.004136613
train_loss: 0.0020719015
test_loss: 0.0040747514
train_loss: 0.0019886084
test_loss: 0.0040438604
train_loss: 0.0018881285
test_loss: 0.0040120934
train_loss: 0.0019060014
test_loss: 0.004056641
train_loss: 0.0018690459
test_loss: 0.004182948
train_loss: 0.0018378231
test_loss: 0.0041016256
train_loss: 0.0019510756
test_loss: 0.0040637446
train_loss: 0.0019059548
test_loss: 0.004159878
train_loss: 0.002080685
test_loss: 0.0040949075
train_loss: 0.0020000592
test_loss: 0.004128177
train_loss: 0.0019627926
test_loss: 0.004046539
train_loss: 0.0019696264
test_loss: 0.0040307133
train_loss: 0.0019911486
test_loss: 0.00419102
train_loss: 0.0021161376
test_loss: 0.0041634184
train_loss: 0.0018143009
test_loss: 0.004051201
train_loss: 0.0017681578
test_loss: 0.0041926564
train_loss: 0.0019136684
test_loss: 0.0040814676
train_loss: 0.002175115
test_loss: 0.0041662725
train_loss: 0.0021698924
test_loss: 0.004140234
train_loss: 0.0019981407
test_loss: 0.004141698
train_loss: 0.0018856507
test_loss: 0.0041408855
train_loss: 0.0019199718
test_loss: 0.0039472925
train_loss: 0.00179351
test_loss: 0.004047497
train_loss: 0.0018299051
test_loss: 0.004007382
train_loss: 0.0019385659
test_loss: 0.004076763
train_loss: 0.0021211633
test_loss: 0.004120871
train_loss: 0.0019543367
test_loss: 0.004088576
train_loss: 0.0018722917
test_loss: 0.0041024475
train_loss: 0.0018695962
test_loss: 0.0039947196
train_loss: 0.0021396421
test_loss: 0.004062203
train_loss: 0.0019439877
test_loss: 0.0040548895
train_loss: 0.0019927921
test_loss: 0.0040576817
train_loss: 0.0018993156
test_loss: 0.0040011713
train_loss: 0.0019327182
test_loss: 0.004036713
train_loss: 0.0019544433
test_loss: 0.0040590563
train_loss: 0.0019685575
test_loss: 0.004017512
train_loss: 0.0018838192
test_loss: 0.004044413
train_loss: 0.0020089827
test_loss: 0.004188037
train_loss: 0.0019657016
test_loss: 0.0039798543
train_loss: 0.0019508944
test_loss: 0.00408574
train_loss: 0.0019574054
test_loss: 0.0041570435
train_loss: 0.0018964865
test_loss: 0.004117718
train_loss: 0.0019387209
test_loss: 0.00408225
train_loss: 0.0019145817
test_loss: 0.0041455156
train_loss: 0.0019993521
test_loss: 0.004154317
train_loss: 0.001992121
test_loss: 0.0041271155
train_loss: 0.0019886317
test_loss: 0.0040948563
train_loss: 0.001965095
test_loss: 0.0041570948
train_loss: 0.0019042534
test_loss: 0.0041926345
train_loss: 0.0020105746
test_loss: 0.004100154
train_loss: 0.0020451415
test_loss: 0.0040122867
train_loss: 0.0020802366
test_loss: 0.0041813715
train_loss: 0.001968515
test_loss: 0.004035816
train_loss: 0.0018906423
test_loss: 0.00401784
train_loss: 0.0018543454
test_loss: 0.004048128
train_loss: 0.0021045287
test_loss: 0.0040396173
train_loss: 0.002062466
test_loss: 0.0040327124
train_loss: 0.0019827527
test_loss: 0.004015056
train_loss: 0.0019525682
test_loss: 0.004052425
train_loss: 0.0021115355
test_loss: 0.004077756
train_loss: 0.002185101
test_loss: 0.004103533
train_loss: 0.0019019544
test_loss: 0.004125184
train_loss: 0.001813961
test_loss: 0.004062137
train_loss: 0.0017846639
test_loss: 0.004025613
train_loss: 0.0020248923
test_loss: 0.004018859
train_loss: 0.0020356807
test_loss: 0.00417418
train_loss: 0.0020057415
test_loss: 0.0040521934
train_loss: 0.0019828142
test_loss: 0.0040950617
train_loss: 0.0019968157
test_loss: 0.0040465095
train_loss: 0.0019880342
test_loss: 0.0041969246
train_loss: 0.0019357826
test_loss: 0.0040103183
train_loss: 0.0019845262
test_loss: 0.0041086692
train_loss: 0.0018778521
test_loss: 0.004090711
train_loss: 0.0019192655
test_loss: 0.00405754
train_loss: 0.0020920532
test_loss: 0.0040886146
train_loss: 0.001975234
test_loss: 0.004124711
train_loss: 0.0019443217
test_loss: 0.0040761684
train_loss: 0.002038032
test_loss: 0.004035352
train_loss: 0.0018906225
test_loss: 0.004153504
train_loss: 0.0019550328
test_loss: 0.004054748
train_loss: 0.001962596
test_loss: 0.00403547
train_loss: 0.0017918189
test_loss: 0.0040698457
train_loss: 0.0018046609
test_loss: 0.00402264
train_loss: 0.0019039274
test_loss: 0.0040061176
train_loss: 0.0018911061
test_loss: 0.003996896
train_loss: 0.0019344615
test_loss: 0.0040773507
train_loss: 0.0018430989
test_loss: 0.004071851
train_loss: 0.001847163
test_loss: 0.004048796
train_loss: 0.001875328
test_loss: 0.003979791
train_loss: 0.0017867463
test_loss: 0.004043139
train_loss: 0.0019238107
test_loss: 0.004059542
train_loss: 0.0018873606
test_loss: 0.004138744
train_loss: 0.0018974519
test_loss: 0.0041154786
train_loss: 0.0018026726
test_loss: 0.0040878663
train_loss: 0.001909792
test_loss: 0.004119614
train_loss: 0.0018863489
test_loss: 0.0041210833
train_loss: 0.0018054794
test_loss: 0.0040772036
train_loss: 0.0017884049
test_loss: 0.004169981
train_loss: 0.0018741048
test_loss: 0.0041181636
train_loss: 0.0017279477
test_loss: 0.0042021913
train_loss: 0.0018969954
test_loss: 0.0041369507
train_loss: 0.0021906935
test_loss: 0.0042301184
train_loss: 0.0020234876
test_loss: 0.004066681
train_loss: 0.0019607276
test_loss: 0.0040739058
train_loss: 0.0021049315
test_loss: 0.0041634464
train_loss: 0.002111016
test_loss: 0.0041816887
train_loss: 0.0020215146
test_loss: 0.0041328096
train_loss: 0.0019405191
test_loss: 0.0042046853
train_loss: 0.0026471042
test_loss: 0.004164313
train_loss: 0.0020865842
test_loss: 0.0044171717
train_loss: 0.0024065976
test_loss: 0.004091636
train_loss: 0.0019862037
test_loss: 0.004198672
train_loss: 0.0020147332
test_loss: 0.0040481165
train_loss: 0.0019701913
test_loss: 0.004089642
train_loss: 0.0019361275
test_loss: 0.0041959346
train_loss: 0.0020820384
test_loss: 0.0041253273
train_loss: 0.0021073685
test_loss: 0.004198616
train_loss: 0.0021622633
test_loss: 0.004136082
train_loss: 0.0022160993
test_loss: 0.0042301514
train_loss: 0.0021082559
test_loss: 0.004206848
train_loss: 0.002232579
test_loss: 0.004253664
train_loss: 0.0021490678
test_loss: 0.0042905393
train_loss: 0.002105128
test_loss: 0.004114312
train_loss: 0.0019577988
test_loss: 0.0042068055
train_loss: 0.0020821402
test_loss: 0.0040810155
train_loss: 0.0020861614
test_loss: 0.0042066206
train_loss: 0.001956274
test_loss: 0.0041821543
train_loss: 0.0019169648
test_loss: 0.004106657
train_loss: 0.001889359
test_loss: 0.0041501396
train_loss: 0.0018569523
test_loss: 0.0040586693
train_loss: 0.0018793242
test_loss: 0.004099571
train_loss: 0.0019002091
test_loss: 0.004021676
train_loss: 0.001963077
test_loss: 0.004062138
train_loss: 0.0018112253
test_loss: 0.0040174047
train_loss: 0.0018408109
test_loss: 0.0040733027
train_loss: 0.001937073
test_loss: 0.004186241
train_loss: 0.0017764885
test_loss: 0.004225759
train_loss: 0.0018832544
test_loss: 0.0040872833
train_loss: 0.0018849166
test_loss: 0.004391528
train_loss: 0.0019759706
test_loss: 0.0041456483
train_loss: 0.001933647
test_loss: 0.004213888
train_loss: 0.0018718409
test_loss: 0.0041062697
train_loss: 0.001914159
test_loss: 0.0041982643
train_loss: 0.0020543851
test_loss: 0.0041479883
train_loss: 0.0018939446
test_loss: 0.0041378574
train_loss: 0.0020717552
test_loss: 0.0041560703
train_loss: 0.0018858956
test_loss: 0.004146917
train_loss: 0.0019781683
test_loss: 0.0040153945
train_loss: 0.0017231494
test_loss: 0.004101653
train_loss: 0.0017145775
test_loss: 0.004083174
train_loss: 0.0018777137
test_loss: 0.004212535
train_loss: 0.001932855
test_loss: 0.0040831375
train_loss: 0.0019519432
test_loss: 0.0040865005
train_loss: 0.0018144425
test_loss: 0.004125343
train_loss: 0.0017804058
test_loss: 0.0041618706
train_loss: 0.0018793193
test_loss: 0.0042619603
train_loss: 0.0021059457
test_loss: 0.004183091
train_loss: 0.0020193267
test_loss: 0.0042135916
train_loss: 0.0019595304
test_loss: 0.004110855
train_loss: 0.002076639
test_loss: 0.004093584
train_loss: 0.0019567423
test_loss: 0.0042999275
train_loss: 0.002148668
test_loss: 0.004044588
train_loss: 0.0020392984
test_loss: 0.004117642
train_loss: 0.0018999326
test_loss: 0.0041573327
train_loss: 0.0018745249
test_loss: 0.0041487487
train_loss: 0.0019517334
test_loss: 0.0041937497
train_loss: 0.0020352146
test_loss: 0.004331249
train_loss: 0.0020596085
test_loss: 0.0040899636
train_loss: 0.0018256315
test_loss: 0.004084334
train_loss: 0.0018797908
test_loss: 0.0040408364
train_loss: 0.0019198495
test_loss: 0.004112289
train_loss: 0.002054302
test_loss: 0.004099121
train_loss: 0.0020528429
test_loss: 0.0042424453
train_loss: 0.0020578813
test_loss: 0.00404565
train_loss: 0.0018616179
test_loss: 0.004111465
train_loss: 0.0020220764
test_loss: 0.0041752085
train_loss: 0.0020262334
test_loss: 0.004112958
train_loss: 0.0018506715
test_loss: 0.0041784085
train_loss: 0.0018285657
test_loss: 0.0040912344
train_loss: 0.0016969956
test_loss: 0.0040586544
train_loss: 0.0016832304
test_loss: 0.0040546856
train_loss: 0.0017273119
test_loss: 0.004036539
train_loss: 0.001774515
test_loss: 0.0042219386
train_loss: 0.0017984683
test_loss: 0.004119777
train_loss: 0.0017942028
test_loss: 0.004002906
train_loss: 0.0018063921
test_loss: 0.0040785978
train_loss: 0.0018367702
test_loss: 0.0041881315
train_loss: 0.0019675242
test_loss: 0.004145649
train_loss: 0.001795408
test_loss: 0.00407327
train_loss: 0.0019064425
test_loss: 0.004272381
train_loss: 0.0019584105
test_loss: 0.0041729673
train_loss: 0.0018308673
test_loss: 0.004220083
train_loss: 0.0019089844
test_loss: 0.0041042417
train_loss: 0.0017882833
test_loss: 0.004055553
train_loss: 0.0018799019
test_loss: 0.0040918044
train_loss: 0.0018034698
test_loss: 0.0040693777
train_loss: 0.0019289647
test_loss: 0.004090904
train_loss: 0.001923046
test_loss: 0.004098389
train_loss: 0.0017785626
test_loss: 0.0040592444
train_loss: 0.0018540907
test_loss: 0.0040716147
train_loss: 0.0018932368
test_loss: 0.004186835
train_loss: 0.0021269403
test_loss: 0.004175195
train_loss: 0.0023335032
test_loss: 0.0041779983
train_loss: 0.0019366473
test_loss: 0.004245068
train_loss: 0.0018592922
test_loss: 0.0040307078
train_loss: 0.001851362
test_loss: 0.0041106
train_loss: 0.0019322527
test_loss: 0.004117237
train_loss: 0.00197927
test_loss: 0.004177504
train_loss: 0.0017352761
test_loss: 0.004041013
train_loss: 0.0017809032
test_loss: 0.0040601
train_loss: 0.001897708
test_loss: 0.004077656
train_loss: 0.0020280788
test_loss: 0.004105914
train_loss: 0.001883259
test_loss: 0.0041835895
train_loss: 0.0017529745
test_loss: 0.0041367593
train_loss: 0.0018396669
test_loss: 0.0041438667
train_loss: 0.0018838535
test_loss: 0.0041646287
train_loss: 0.0020258555
test_loss: 0.0041834903
train_loss: 0.0019027159
test_loss: 0.004147694
train_loss: 0.0017223876
test_loss: 0.0041795797
train_loss: 0.00182095
test_loss: 0.004234545
train_loss: 0.0018903567
test_loss: 0.004162653
train_loss: 0.0017948834
test_loss: 0.0041292757
train_loss: 0.0018424318
test_loss: 0.004237098
train_loss: 0.0019311027
test_loss: 0.004215587
train_loss: 0.0018691379
test_loss: 0.0042690462
train_loss: 0.0019672916
test_loss: 0.0042009507
train_loss: 0.0020215416
test_loss: 0.0041468665
train_loss: 0.0019663067
test_loss: 0.0041414322
train_loss: 0.0017580229
test_loss: 0.0043378463
train_loss: 0.0019954797
test_loss: 0.004176721
train_loss: 0.001982055
test_loss: 0.00428902
train_loss: 0.0019831648
test_loss: 0.00410441
train_loss: 0.0017696787
test_loss: 0.004145092
train_loss: 0.001875303
test_loss: 0.004130128
train_loss: 0.0017914472
test_loss: 0.0041287313
train_loss: 0.0017651855
test_loss: 0.00409039
train_loss: 0.0017561319
test_loss: 0.0040924586
train_loss: 0.0018515103
test_loss: 0.0041263853
train_loss: 0.0018377875
test_loss: 0.004088696
train_loss: 0.001737841
test_loss: 0.0041616387
train_loss: 0.0018406264
test_loss: 0.0041464907
train_loss: 0.0019525653
test_loss: 0.004132424
train_loss: 0.0018962345
test_loss: 0.004197641
train_loss: 0.0019282582
test_loss: 0.004154948
train_loss: 0.0019897548
test_loss: 0.0042390972
train_loss: 0.0017891083
test_loss: 0.0041510356
train_loss: 0.0018597592
test_loss: 0.004095013
train_loss: 0.00184576
test_loss: 0.004294925
train_loss: 0.001972827
test_loss: 0.004196327
train_loss: 0.0020800566
test_loss: 0.004135143
train_loss: 0.001870817
test_loss: 0.00416927
train_loss: 0.0017883519
test_loss: 0.0041130935
train_loss: 0.00189342
test_loss: 0.004093804
train_loss: 0.0018663475
test_loss: 0.004165396
train_loss: 0.001820836
test_loss: 0.0041121845
train_loss: 0.0018364787
test_loss: 0.004319734
train_loss: 0.0018217298
test_loss: 0.0041839723
train_loss: 0.001959576
test_loss: 0.0041921283
train_loss: 0.0020189174
test_loss: 0.0042296997
train_loss: 0.0019953055
test_loss: 0.004209875
train_loss: 0.0017898716
test_loss: 0.0041617164
train_loss: 0.0017741468
test_loss: 0.0041145934
train_loss: 0.0017495436
test_loss: 0.0041245585
train_loss: 0.0017843095
test_loss: 0.0040497244
train_loss: 0.0017676556
test_loss: 0.0041276733
train_loss: 0.0017690401
test_loss: 0.0041356343
train_loss: 0.001766529
test_loss: 0.0041896333
train_loss: 0.0018493268
test_loss: 0.004138141
train_loss: 0.0020018264
test_loss: 0.0041913977
train_loss: 0.0019129765
test_loss: 0.0041703773
train_loss: 0.0019966206
test_loss: 0.0044080117
train_loss: 0.0021898497
test_loss: 0.004189043
train_loss: 0.0021046733
test_loss: 0.0042574448
train_loss: 0.0018443592
test_loss: 0.0042131618
train_loss: 0.0018876257
test_loss: 0.004103582
train_loss: 0.0020173271
test_loss: 0.0043368675
train_loss: 0.0020150102
test_loss: 0.0041894973
train_loss: 0.0019742076
test_loss: 0.004136642
train_loss: 0.0017842143
test_loss: 0.0041408064
train_loss: 0.0018698436
test_loss: 0.0041758954
train_loss: 0.00179121
test_loss: 0.0041473852
train_loss: 0.0017907755
test_loss: 0.0041776276
train_loss: 0.001889332
test_loss: 0.004114667
train_loss: 0.0020885838
test_loss: 0.0042775758
train_loss: 0.002154254
test_loss: 0.0042084334
train_loss: 0.0021654456
test_loss: 0.0041840705
train_loss: 0.0018288064
test_loss: 0.0042345985
train_loss: 0.0019120597
test_loss: 0.004087232
train_loss: 0.0018487726
test_loss: 0.0041117235
train_loss: 0.0018153237
test_loss: 0.0041317195
train_loss: 0.0016971091
test_loss: 0.0041249655
train_loss: 0.0018889378
test_loss: 0.004198874
train_loss: 0.0017045767
test_loss: 0.004133268
train_loss: 0.0019829846
test_loss: 0.004140392
train_loss: 0.0019082625
test_loss: 0.0041344073
train_loss: 0.001946048
test_loss: 0.0040975297
train_loss: 0.0017362221
test_loss: 0.0042127525
train_loss: 0.0018693297
test_loss: 0.0040729125
train_loss: 0.0018959934
test_loss: 0.0041271774
train_loss: 0.0017271609
test_loss: 0.004209766
train_loss: 0.001827951
test_loss: 0.004239164
train_loss: 0.0019947775
test_loss: 0.004373852
train_loss: 0.0018973392
test_loss: 0.004250526
train_loss: 0.0020923284
test_loss: 0.0043152454
train_loss: 0.0022748588
test_loss: 0.0042885956
train_loss: 0.0019066442
test_loss: 0.0043134848
train_loss: 0.0019858368
test_loss: 0.004192251
train_loss: 0.001884216
test_loss: 0.004170061
train_loss: 0.0019004119
test_loss: 0.00418741
train_loss: 0.00178406
test_loss: 0.004187489
train_loss: 0.0019884629
test_loss: 0.00423121
train_loss: 0.0018448293
test_loss: 0.004116637
train_loss: 0.0018506199
test_loss: 0.0041390616
train_loss: 0.0018370315
test_loss: 0.0041809217
train_loss: 0.0016859459
test_loss: 0.004205133
train_loss: 0.0019859278
test_loss: 0.004206579
train_loss: 0.0019053017
test_loss: 0.0041352017
train_loss: 0.0016929152
test_loss: 0.0042350246
train_loss: 0.0016424933
test_loss: 0.0041905143
train_loss: 0.0019661833
test_loss: 0.0042165127
train_loss: 0.0018356842
test_loss: 0.004211616
train_loss: 0.0018836941
test_loss: 0.004258148
train_loss: 0.0018934951
test_loss: 0.004096144
train_loss: 0.0018056427
test_loss: 0.004117341
train_loss: 0.0019682501
test_loss: 0.0040956703
train_loss: 0.0017659264
test_loss: 0.004130167
train_loss: 0.001668604
test_loss: 0.0040950337
train_loss: 0.00171279
test_loss: 0.0041060387
train_loss: 0.0016574212
test_loss: 0.0041070515
train_loss: 0.0016570934
test_loss: 0.0041002636
train_loss: 0.001690575
test_loss: 0.004164112
train_loss: 0.0018419693
test_loss: 0.004338027
train_loss: 0.0018765702
test_loss: 0.0041156686
train_loss: 0.0018244875
test_loss: 0.0040896144
train_loss: 0.0017773183
test_loss: 0.0040977052
train_loss: 0.001961891
test_loss: 0.004207892
train_loss: 0.0017288435
test_loss: 0.0040933145
train_loss: 0.0017208972
test_loss: 0.004137803
train_loss: 0.0017542503
test_loss: 0.004125524
train_loss: 0.0015514028
test_loss: 0.0041002976
train_loss: 0.0017364523
test_loss: 0.004197958
train_loss: 0.0016916327
test_loss: 0.0041290275
train_loss: 0.00178242
test_loss: 0.004168732
train_loss: 0.0019006504
test_loss: 0.004133028
train_loss: 0.0018594749
test_loss: 0.0040981933
train_loss: 0.0018469081
test_loss: 0.0041155196
train_loss: 0.0017862716
test_loss: 0.0040768078
train_loss: 0.0017850944
test_loss: 0.004043984
train_loss: 0.0017415355
test_loss: 0.004185681
train_loss: 0.0017111772
test_loss: 0.0041742465
train_loss: 0.0017596803
test_loss: 0.004139745
train_loss: 0.0017288069
test_loss: 0.0041059977
train_loss: 0.0018179596
test_loss: 0.0041387933
train_loss: 0.0017602128
test_loss: 0.0040985257
train_loss: 0.0018061923
test_loss: 0.004199978
train_loss: 0.0017985101
test_loss: 0.0041741156
train_loss: 0.0017930067
test_loss: 0.0042365547
train_loss: 0.0019216478
test_loss: 0.0042361342
train_loss: 0.0020419217
test_loss: 0.0042837663
train_loss: 0.0019505343
test_loss: 0.0042537684
train_loss: 0.0019129892
test_loss: 0.0042331656
train_loss: 0.001858312
test_loss: 0.0042104907
train_loss: 0.0017827565
test_loss: 0.004171024
train_loss: 0.0017796089
test_loss: 0.004213359
train_loss: 0.0018659432
test_loss: 0.0042654085
train_loss: 0.0022844935
test_loss: 0.004189983
train_loss: 0.0018294653
test_loss: 0.0042538666
train_loss: 0.0018681682
test_loss: 0.0042840834
train_loss: 0.0018004348
test_loss: 0.0043839794
train_loss: 0.0022707938
test_loss: 0.004180423
train_loss: 0.002033451
test_loss: 0.0041435882
train_loss: 0.0018197487
test_loss: 0.0043723388
train_loss: 0.0020411806
test_loss: 0.0041974713
train_loss: 0.001834203
test_loss: 0.0042202948
train_loss: 0.0017528211
test_loss: 0.0042340523
train_loss: 0.0018379112
test_loss: 0.004214344
train_loss: 0.0017219434
test_loss: 0.004166797
train_loss: 0.0017988975
test_loss: 0.004231126
train_loss: 0.0017628042
test_loss: 0.0041599427
train_loss: 0.0019910606
test_loss: 0.004198862
train_loss: 0.001955099
test_loss: 0.0042049503
train_loss: 0.0018503072
test_loss: 0.0042095706
train_loss: 0.0018935793
test_loss: 0.0043294095
train_loss: 0.0021389192
test_loss: 0.004265723
train_loss: 0.001899604
test_loss: 0.0043520676
train_loss: 0.0019233949
test_loss: 0.0043990007
train_loss: 0.0019285036
test_loss: 0.004155493
train_loss: 0.0019069263
test_loss: 0.0041776206
train_loss: 0.0018172567
test_loss: 0.0041273744
train_loss: 0.0017658989
test_loss: 0.004260716
train_loss: 0.0019460549
test_loss: 0.004167002
train_loss: 0.0017820934
test_loss: 0.0042143674
train_loss: 0.002020147
test_loss: 0.0042304476
train_loss: 0.0017534739
test_loss: 0.0042081554
train_loss: 0.0018936817
test_loss: 0.0041707326
train_loss: 0.0019097021
test_loss: 0.0041388157
train_loss: 0.0019579683
test_loss: 0.0041684676
train_loss: 0.0018185701
test_loss: 0.0042863707
train_loss: 0.0018039143
test_loss: 0.0042046956
train_loss: 0.0017609443
test_loss: 0.004294008
train_loss: 0.0017326194
test_loss: 0.004191395
train_loss: 0.0019710672
test_loss: 0.0042103915
train_loss: 0.0019397822
test_loss: 0.004229453
train_loss: 0.0017221507
test_loss: 0.0042056534
train_loss: 0.0018430755
test_loss: 0.004252916
train_loss: 0.0018061355
test_loss: 0.0042207176
train_loss: 0.0017719582
test_loss: 0.0041662776
train_loss: 0.0016802689
test_loss: 0.004244518
train_loss: 0.0017310546
test_loss: 0.0041193236
train_loss: 0.0018078982
test_loss: 0.004392016
train_loss: 0.001956362
test_loss: 0.0042270184
train_loss: 0.001996004
test_loss: 0.00428059
train_loss: 0.0018668628
test_loss: 0.0042480654
train_loss: 0.0018455117
test_loss: 0.0042203143
train_loss: 0.0017613329
test_loss: 0.0042573777
train_loss: 0.0019329425
test_loss: 0.0042806817
train_loss: 0.0018988706
test_loss: 0.004196639
train_loss: 0.001855233
test_loss: 0.004313322
train_loss: 0.0019271185
test_loss: 0.004359014
train_loss: 0.0019124659
test_loss: 0.00428276
train_loss: 0.0019326032
test_loss: 0.0042242156
train_loss: 0.0019748053
test_loss: 0.004264215
train_loss: 0.001743452
test_loss: 0.004247598
train_loss: 0.001763693
test_loss: 0.004222172
train_loss: 0.0019912368
test_loss: 0.0043261615
train_loss: 0.0017466315
test_loss: 0.0041111712
train_loss: 0.0019298923
test_loss: 0.0042814696
train_loss: 0.0017595254
test_loss: 0.004173567
train_loss: 0.0019166758
test_loss: 0.004419027
train_loss: 0.0020048975
test_loss: 0.0042573083
train_loss: 0.0018808264
test_loss: 0.004244276
train_loss: 0.0019225257
test_loss: 0.004165228
train_loss: 0.0018704447
test_loss: 0.004360476
train_loss: 0.0018847634
test_loss: 0.0041876016
train_loss: 0.0016692011
test_loss: 0.004249411
train_loss: 0.0017543357
test_loss: 0.004242872
train_loss: 0.0018168397
test_loss: 0.0042993827
train_loss: 0.0018930656
test_loss: 0.0042776004
train_loss: 0.0020210997
test_loss: 0.0041902442
train_loss: 0.001838816
test_loss: 0.0042575337
train_loss: 0.0018841855
test_loss: 0.004419522
train_loss: 0.0020767457
test_loss: 0.0042940094
train_loss: 0.002443473
test_loss: 0.004391547
train_loss: 0.0019875183
test_loss: 0.004240363
train_loss: 0.0019609397
test_loss: 0.0043274323
train_loss: 0.0018539873
test_loss: 0.0042912094
train_loss: 0.0020012984
test_loss: 0.004235874
train_loss: 0.0018986817
test_loss: 0.0044276617
train_loss: 0.0019448269
test_loss: 0.0041836146
train_loss: 0.0018811114
test_loss: 0.00421992
train_loss: 0.0018462698
test_loss: 0.004277344
train_loss: 0.0018923266
test_loss: 0.0042357007
train_loss: 0.001843791
test_loss: 0.00425476
train_loss: 0.0019652955
test_loss: 0.00417413
train_loss: 0.0017001976
test_loss: 0.0041901297
train_loss: 0.0018361423
test_loss: 0.0042640436
train_loss: 0.0017640942
test_loss: 0.0041547725
train_loss: 0.0016494443
test_loss: 0.00416128
train_loss: 0.0016433295
test_loss: 0.004279771
train_loss: 0.001756029
test_loss: 0.0042102826
train_loss: 0.0018741232
test_loss: 0.0041726595
train_loss: 0.0016024814
test_loss: 0.0042234664
train_loss: 0.0016580939
test_loss: 0.004184768
train_loss: 0.0017560055
test_loss: 0.004159493
train_loss: 0.0017562793
test_loss: 0.004236713
train_loss: 0.0017397252
test_loss: 0.004199693
train_loss: 0.0017180322
test_loss: 0.0041954783
train_loss: 0.0018533954
test_loss: 0.004306097
train_loss: 0.0018163609
test_loss: 0.0042621563
train_loss: 0.0019850868
test_loss: 0.004339635
train_loss: 0.0020157956
test_loss: 0.004226796
train_loss: 0.0018179694
test_loss: 0.00431221
train_loss: 0.0017174559
test_loss: 0.004200215
train_loss: 0.0017795315
test_loss: 0.0041978722
train_loss: 0.0018582842
test_loss: 0.004251032
train_loss: 0.0018836926
test_loss: 0.0042937193
train_loss: 0.0018318733
test_loss: 0.0042243325
train_loss: 0.002143216
test_loss: 0.004308787
train_loss: 0.002092622
test_loss: 0.004288504
train_loss: 0.0018531259
test_loss: 0.004216438
train_loss: 0.0017653876
test_loss: 0.004253079
train_loss: 0.001868292
test_loss: 0.0042065377
train_loss: 0.0016881975
test_loss: 0.0041931053
train_loss: 0.0017281342
test_loss: 0.004325344
train_loss: 0.0018566584
test_loss: 0.004161865
train_loss: 0.0017059641
test_loss: 0.004246519
train_loss: 0.0017947047
test_loss: 0.0041902955
train_loss: 0.0018254052
test_loss: 0.0042465385
train_loss: 0.001796635
test_loss: 0.004292245
train_loss: 0.0017360728
test_loss: 0.0042176605
train_loss: 0.0017827689
test_loss: 0.0042542447
train_loss: 0.0018277005
test_loss: 0.0042052083
train_loss: 0.0019451374
test_loss: 0.004298314
train_loss: 0.0017765462
test_loss: 0.004322554
train_loss: 0.0018655013
test_loss: 0.004196644
train_loss: 0.0017890822
test_loss: 0.0041886615
train_loss: 0.0016142208
test_loss: 0.0041471384
train_loss: 0.0016730357
test_loss: 0.0042379713
train_loss: 0.0019661984
test_loss: 0.0042763804
train_loss: 0.0019550873
test_loss: 0.004313249
train_loss: 0.0017658697
test_loss: 0.0043197502
train_loss: 0.0018236584
test_loss: 0.004410712
train_loss: 0.0017803276
test_loss: 0.004202551
train_loss: 0.0019065791
test_loss: 0.004244278
train_loss: 0.001943567
test_loss: 0.004255956
train_loss: 0.0018396035
test_loss: 0.004280198
train_loss: 0.0017402356
test_loss: 0.004243837
train_loss: 0.001832292
test_loss: 0.004287135
train_loss: 0.0017129619
test_loss: 0.0042834138
train_loss: 0.0016988554
test_loss: 0.0041564815
train_loss: 0.0016319618
test_loss: 0.0042669573
train_loss: 0.0017224636
test_loss: 0.0042450386
train_loss: 0.001832134
test_loss: 0.004188459
train_loss: 0.0018913181
test_loss: 0.004311529
train_loss: 0.0017799909
test_loss: 0.004177724
train_loss: 0.001782348
test_loss: 0.0042999904
train_loss: 0.0019373517
test_loss: 0.0041957297
train_loss: 0.0017746666
test_loss: 0.0042805756
train_loss: 0.0017624256
test_loss: 0.004244976
train_loss: 0.0018695279
test_loss: 0.0042892196
train_loss: 0.0017669701
test_loss: 0.0041986834
train_loss: 0.0018914876
test_loss: 0.0043048933
train_loss: 0.0018623751
test_loss: 0.004269482
train_loss: 0.0018145734
test_loss: 0.0043114536
train_loss: 0.0018346409
test_loss: 0.0042357277
train_loss: 0.00172556
test_loss: 0.004177079
train_loss: 0.0017007664
test_loss: 0.0042549465
train_loss: 0.0016700792
test_loss: 0.004237212
train_loss: 0.0016398296
test_loss: 0.004335142
train_loss: 0.0017465318
test_loss: 0.004256048
train_loss: 0.0018102562
test_loss: 0.004268554
train_loss: 0.0017588492
test_loss: 0.0042233937
train_loss: 0.0017589403
test_loss: 0.00418494
train_loss: 0.0018767581
test_loss: 0.004305685
train_loss: 0.001967338
test_loss: 0.0042110304
train_loss: 0.0016275041
test_loss: 0.0041844537
train_loss: 0.0018036049
test_loss: 0.0041906065
train_loss: 0.0018483374
test_loss: 0.0043397686
train_loss: 0.0018165091
test_loss: 0.0042589502
train_loss: 0.0017651515
test_loss: 0.004291578
train_loss: 0.001782496
test_loss: 0.004346092
train_loss: 0.0017234739
test_loss: 0.004243383
train_loss: 0.0018443177
test_loss: 0.0043560336
train_loss: 0.0017430431
test_loss: 0.0042777807
train_loss: 0.001774909
test_loss: 0.004233844
train_loss: 0.0017509034
test_loss: 0.0041951733
train_loss: 0.0017796054
test_loss: 0.004273225
train_loss: 0.0016658194
test_loss: 0.004274186
train_loss: 0.0016317093
test_loss: 0.0042342693
train_loss: 0.0017217686
test_loss: 0.0041619325
train_loss: 0.0018113451
test_loss: 0.0042612525
train_loss: 0.001817869
test_loss: 0.0042744963
train_loss: 0.0018032299
test_loss: 0.004467222
train_loss: 0.0019738094
test_loss: 0.0042143934
train_loss: 0.0019484665
test_loss: 0.004308509/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.0017334239
test_loss: 0.0043052323
train_loss: 0.001978967
test_loss: 0.0042337743
train_loss: 0.0017797982
test_loss: 0.004257268
train_loss: 0.001759586
test_loss: 0.0042815707
train_loss: 0.0017847933
test_loss: 0.004357888
train_loss: 0.0020333007
test_loss: 0.004377366
train_loss: 0.0019241817
test_loss: 0.0042803865
train_loss: 0.0018175226
test_loss: 0.004376843
train_loss: 0.0018670071
test_loss: 0.0042985408
train_loss: 0.0019129983
test_loss: 0.0042845285
train_loss: 0.001888986
test_loss: 0.0042835413
train_loss: 0.0018847727
test_loss: 0.004449677
train_loss: 0.0020566285
test_loss: 0.0042887353
train_loss: 0.0021200413
test_loss: 0.0042632953
train_loss: 0.0018504153
test_loss: 0.0043141935
train_loss: 0.0019097333
test_loss: 0.004282419
train_loss: 0.0017427576
test_loss: 0.0045294818
train_loss: 0.002213831
test_loss: 0.0042109094
train_loss: 0.0018792426
test_loss: 0.0043936237
train_loss: 0.0019778812
test_loss: 0.004569169
train_loss: 0.0021363108
test_loss: 0.004355662
train_loss: 0.0019770833
test_loss: 0.004336361
train_loss: 0.0019073433
test_loss: 0.004340288
train_loss: 0.0017916759
test_loss: 0.004240765
train_loss: 0.0017488088
test_loss: 0.004304499
train_loss: 0.0019068434
test_loss: 0.004241538
train_loss: 0.0017133744
test_loss: 0.0044226423
train_loss: 0.002125883
test_loss: 0.004340625
train_loss: 0.0019643675
test_loss: 0.004346388
train_loss: 0.0018448323
test_loss: 0.004437507
train_loss: 0.0020507625
test_loss: 0.004314664
train_loss: 0.0019037849
test_loss: 0.004378046
train_loss: 0.0019995729
test_loss: 0.00427791
train_loss: 0.002040315
test_loss: 0.0042817034
train_loss: 0.0017886229
test_loss: 0.004282915
train_loss: 0.0017485389
test_loss: 0.004221307
train_loss: 0.0018510243
test_loss: 0.004460719
train_loss: 0.0020619421
test_loss: 0.0043035164
train_loss: 0.0018289041
test_loss: 0.004237599
train_loss: 0.001737787
test_loss: 0.004303533
train_loss: 0.0018084219
test_loss: 0.004343513
train_loss: 0.0020354083
test_loss: 0.004374527
train_loss: 0.0019467393
test_loss: 0.0042848634
train_loss: 0.001955839
test_loss: 0.004531136
train_loss: 0.0019215994
test_loss: 0.0042919535
train_loss: 0.0018313858
test_loss: 0.0042773294
train_loss: 0.0017238092
test_loss: 0.0043472284
train_loss: 0.0016420824
test_loss: 0.0043083453
train_loss: 0.0017933012
test_loss: 0.0042426386
train_loss: 0.0017112565
test_loss: 0.0042801835
train_loss: 0.0016716642
test_loss: 0.0042804563
train_loss: 0.0016288857
test_loss: 0.004363109
train_loss: 0.0019500045
test_loss: 0.0043140827
train_loss: 0.0018191066
test_loss: 0.0042526606
train_loss: 0.0018514604
test_loss: 0.0043671653
train_loss: 0.0018907663
test_loss: 0.0042294627
train_loss: 0.0018529269
test_loss: 0.0043902677
train_loss: 0.0018238083
test_loss: 0.0042923633
train_loss: 0.0019439448
test_loss: 0.004324558
train_loss: 0.0017851943
test_loss: 0.0043725744
train_loss: 0.0019470846
test_loss: 0.004334887
train_loss: 0.0018219115
test_loss: 0.0043460834
train_loss: 0.001785262
test_loss: 0.00429036
train_loss: 0.0017837004
test_loss: 0.0042297663
train_loss: 0.0017861545
test_loss: 0.004280049
train_loss: 0.0017467506
test_loss: 0.004237287
train_loss: 0.001633959
test_loss: 0.0042589083
train_loss: 0.0017584213
test_loss: 0.0042117448
train_loss: 0.0016760142
test_loss: 0.0041993964
train_loss: 0.0016904698
test_loss: 0.0042301514
train_loss: 0.001962801
test_loss: 0.0042761485
train_loss: 0.0016376614
test_loss: 0.004337957
train_loss: 0.0017107925
test_loss: 0.004211162
train_loss: 0.0017076448
test_loss: 0.0042165634
train_loss: 0.0016542181
test_loss: 0.0044363844
train_loss: 0.001744221
test_loss: 0.004369295
train_loss: 0.0018562577
test_loss: 0.00424269
train_loss: 0.00175932
test_loss: 0.004268419
train_loss: 0.0017059136
test_loss: 0.004263173
train_loss: 0.0018003734
test_loss: 0.0042738914
train_loss: 0.001807066
test_loss: 0.0042183143
train_loss: 0.0017225889
test_loss: 0.004150947
train_loss: 0.0017836231
test_loss: 0.0042603216
train_loss: 0.001707323
test_loss: 0.0043595782
train_loss: 0.0018009273
test_loss: 0.004312323
train_loss: 0.0017943801
test_loss: 0.004625802
train_loss: 0.0020231802
test_loss: 0.0043753497
train_loss: 0.002060274
test_loss: 0.004483406
train_loss: 0.0019640047
test_loss: 0.004426531
train_loss: 0.0020607093
test_loss: 0.004328817
train_loss: 0.0017508585
test_loss: 0.0043891254
train_loss: 0.001976329
test_loss: 0.0042871884
train_loss: 0.0017752866
test_loss: 0.0043428075
train_loss: 0.0018400996
test_loss: 0.0043022283
train_loss: 0.0017656151
test_loss: 0.0043162336
train_loss: 0.0018745277
test_loss: 0.004283625
train_loss: 0.001759304
test_loss: 0.0045171874
train_loss: 0.001861641
test_loss: 0.0043660672
train_loss: 0.0018314787
test_loss: 0.0042252694
train_loss: 0.001877771
test_loss: 0.004280141
train_loss: 0.0017877872
test_loss: 0.0043629953
train_loss: 0.0017938213
test_loss: 0.0042298427
train_loss: 0.001814344
test_loss: 0.0043174685
train_loss: 0.0017029672
test_loss: 0.004313086
train_loss: 0.0017197315
test_loss: 0.0042895
train_loss: 0.0017870174
test_loss: 0.0042971675
train_loss: 0.0017666185
test_loss: 0.004324017
train_loss: 0.001659607
test_loss: 0.0042505856
train_loss: 0.001644491
test_loss: 0.004237101
train_loss: 0.0016374113
test_loss: 0.0042387303
train_loss: 0.0015900073
test_loss: 0.0043010134
train_loss: 0.0016538951
test_loss: 0.0042893346
train_loss: 0.0016803879
test_loss: 0.0042705243
train_loss: 0.0019684103
test_loss: 0.0042840303
train_loss: 0.0018172616
test_loss: 0.0042818813
train_loss: 0.0016690185
test_loss: 0.0042378227
train_loss: 0.0016479334
test_loss: 0.004285416
train_loss: 0.0017573622
test_loss: 0.004297641
train_loss: 0.0018082367
test_loss: 0.004281143
train_loss: 0.0018352128
test_loss: 0.004271108
train_loss: 0.0018836414
test_loss: 0.004344681
train_loss: 0.0018216306
test_loss: 0.0043881824
train_loss: 0.0018107144
test_loss: 0.0042914934
train_loss: 0.0016601485
test_loss: 0.0043225395
train_loss: 0.001734792
test_loss: 0.0043280656
train_loss: 0.0017667675
test_loss: 0.004357278
train_loss: 0.0017668693
test_loss: 0.004334786
train_loss: 0.0016876804
test_loss: 0.004347701
train_loss: 0.0016245643
test_loss: 0.004372002
train_loss: 0.0017897717
test_loss: 0.004258929
train_loss: 0.0017322579
test_loss: 0.004305582
train_loss: 0.0017409464
test_loss: 0.0042843036
train_loss: 0.001735786
test_loss: 0.0043097013
train_loss: 0.0017732135
test_loss: 0.004300647
train_loss: 0.0017614044
test_loss: 0.004280196
train_loss: 0.0017955943
test_loss: 0.0043662707
train_loss: 0.0017393222
test_loss: 0.004349568
train_loss: 0.0017302339
test_loss: 0.0043003843
train_loss: 0.0017334109
test_loss: 0.0042444505
train_loss: 0.0017139338
test_loss: 0.0042571076
train_loss: 0.0017270149
test_loss: 0.0043914197
train_loss: 0.0017137924
test_loss: 0.0042552184
train_loss: 0.0017861946
test_loss: 0.0043746387
train_loss: 0.001798857
test_loss: 0.0042862524
train_loss: 0.0017478511
test_loss: 0.004339061
train_loss: 0.0016983261
test_loss: 0.0042835805
train_loss: 0.0017076277
test_loss: 0.004260021
train_loss: 0.0017739884
test_loss: 0.004410671
train_loss: 0.0018965695
test_loss: 0.004255609
train_loss: 0.0017588724
test_loss: 0.004308243
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1 --function f1 --psi 2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b5293d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b528f0048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b528f0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b528050d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b52845268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b527a58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b52788620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b52788b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b52740f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b52731950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b52731d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b526c91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b526b88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b52662ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b52662950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b52625bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b525d4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b525e6bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b525be8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b525d47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b525659d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b525be0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b060426a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b525be9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b52565730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b0601a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b05fcb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b05ffa400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b05f80620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b05f80ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ae0053730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ae006ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ae006ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac0447730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac03f5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac03f7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.07261026
test_loss: 0.064928085
train_loss: 0.040760882
test_loss: 0.035996694
train_loss: 0.029529078
test_loss: 0.02761384
train_loss: 0.023620468
test_loss: 0.021421194
train_loss: 0.018383514
test_loss: 0.018765885
train_loss: 0.015051393
test_loss: 0.015529715
train_loss: 0.011659867
test_loss: 0.012708376
train_loss: 0.009641228
test_loss: 0.010989297
train_loss: 0.00802073
test_loss: 0.009376159
train_loss: 0.006758619
test_loss: 0.008565935
train_loss: 0.006007865
test_loss: 0.0077358442
train_loss: 0.0051455228
test_loss: 0.0072225314
train_loss: 0.0047467635
test_loss: 0.006693286
train_loss: 0.004411715
test_loss: 0.0067509343
train_loss: 0.0041686725
test_loss: 0.006348367
train_loss: 0.003707695
test_loss: 0.0059990166
train_loss: 0.0035860874
test_loss: 0.005831568
train_loss: 0.0035590807
test_loss: 0.005733802
train_loss: 0.003273484
test_loss: 0.0057064258
train_loss: 0.003197576
test_loss: 0.0056513464
train_loss: 0.0033189682
test_loss: 0.005533546
train_loss: 0.0030736895
test_loss: 0.0055571324
train_loss: 0.0029287513
test_loss: 0.005476653
train_loss: 0.0029985146
test_loss: 0.0055364924
train_loss: 0.0030978413
test_loss: 0.0053618094
train_loss: 0.002802047
test_loss: 0.005360361
train_loss: 0.0027873544
test_loss: 0.005289989
train_loss: 0.00285262
test_loss: 0.0054842625
train_loss: 0.0027947128
test_loss: 0.0052730055
train_loss: 0.0027815704
test_loss: 0.0052848286
train_loss: 0.0028592346
test_loss: 0.0052897637
train_loss: 0.0029574968
test_loss: 0.0052500837
train_loss: 0.0026032203
test_loss: 0.005364165
train_loss: 0.0026172847
test_loss: 0.005175066
train_loss: 0.0025299964
test_loss: 0.005274445
train_loss: 0.0026047914
test_loss: 0.0051984484
train_loss: 0.0030191941
test_loss: 0.0052342135
train_loss: 0.0024194564
test_loss: 0.005276824
train_loss: 0.0026561278
test_loss: 0.005172954
train_loss: 0.002516734
test_loss: 0.0051833726
train_loss: 0.002578565
test_loss: 0.005162652
train_loss: 0.0024519486
test_loss: 0.0051265103
train_loss: 0.0025677697
test_loss: 0.005127286
train_loss: 0.0024155872
test_loss: 0.005124663
train_loss: 0.0025483994
test_loss: 0.0051435833
train_loss: 0.0024108838
test_loss: 0.005168884
train_loss: 0.0025746254
test_loss: 0.005159204
train_loss: 0.0024894602
test_loss: 0.0052550845
train_loss: 0.0026718536
test_loss: 0.0052442914
train_loss: 0.0025588367
test_loss: 0.005023872
train_loss: 0.002448091
test_loss: 0.0051066363
train_loss: 0.002321529
test_loss: 0.005215828
train_loss: 0.002514594
test_loss: 0.005142114
train_loss: 0.002506588
test_loss: 0.0050744023
train_loss: 0.002177135
test_loss: 0.0050779264
train_loss: 0.002240495
test_loss: 0.005004578
train_loss: 0.0021965462
test_loss: 0.0051003303
train_loss: 0.0021852204
test_loss: 0.005030152
train_loss: 0.002204836
test_loss: 0.0050097285
train_loss: 0.0022498548
test_loss: 0.0052150325
train_loss: 0.0024522045
test_loss: 0.005100081
train_loss: 0.002386324
test_loss: 0.005089535
train_loss: 0.0024415022
test_loss: 0.005066181
train_loss: 0.0021938507
test_loss: 0.005036648
train_loss: 0.0021631983
test_loss: 0.0050177877
train_loss: 0.0021230523
test_loss: 0.0049771685
train_loss: 0.002329934
test_loss: 0.005120699
train_loss: 0.0021536895
test_loss: 0.005200682
train_loss: 0.0022041975
test_loss: 0.0051131696
train_loss: 0.0022990804
test_loss: 0.0050756484
train_loss: 0.0021762038
test_loss: 0.0050939214
train_loss: 0.0021949564
test_loss: 0.0050975075
train_loss: 0.0024080656
test_loss: 0.005051243
train_loss: 0.0020976407
test_loss: 0.004993736
train_loss: 0.0020737213
test_loss: 0.0050836196
train_loss: 0.0021473162
test_loss: 0.005074638
train_loss: 0.0022371293
test_loss: 0.004981664
train_loss: 0.0021635718
test_loss: 0.005112119
train_loss: 0.0022070496
test_loss: 0.0050098463
train_loss: 0.0021251352
test_loss: 0.005055739
train_loss: 0.0022288086
test_loss: 0.0050774715
train_loss: 0.002233417
test_loss: 0.0050537363
train_loss: 0.0022744876
test_loss: 0.0050627785
train_loss: 0.0020770268
test_loss: 0.0050032637
train_loss: 0.0021778997
test_loss: 0.005051713
train_loss: 0.0020755029
test_loss: 0.0049888366
train_loss: 0.0021084282
test_loss: 0.0050151763
train_loss: 0.0021018374
test_loss: 0.0050149392
train_loss: 0.0020530755
test_loss: 0.005133494
train_loss: 0.0022525475
test_loss: 0.0050106454
train_loss: 0.0021968733
test_loss: 0.005240602
train_loss: 0.0025256872
test_loss: 0.0052092685
train_loss: 0.0024223803
test_loss: 0.005109896
train_loss: 0.0022466304
test_loss: 0.005119839
train_loss: 0.0022871653
test_loss: 0.005141986
train_loss: 0.002039667
test_loss: 0.0049675326
train_loss: 0.0020406742
test_loss: 0.0052128336
train_loss: 0.0023844237
test_loss: 0.005084622
train_loss: 0.0022531566
test_loss: 0.005209842
train_loss: 0.0021277592
test_loss: 0.005027063
train_loss: 0.0022042594
test_loss: 0.005077171
train_loss: 0.0021519116
test_loss: 0.0049647014
train_loss: 0.0020807995
test_loss: 0.005120809
train_loss: 0.0021467423
test_loss: 0.005005001
train_loss: 0.0020672379
test_loss: 0.005269146
train_loss: 0.0024141704
test_loss: 0.0051080086
train_loss: 0.002203377
test_loss: 0.0052368282
train_loss: 0.0020836329
test_loss: 0.004967982
train_loss: 0.0020748482
test_loss: 0.005047279
train_loss: 0.002152287
test_loss: 0.0050663105
train_loss: 0.0021502655
test_loss: 0.005036083
train_loss: 0.0019437841
test_loss: 0.0050325985
train_loss: 0.002088246
test_loss: 0.0049439394
train_loss: 0.0023463499
test_loss: 0.0051836446
train_loss: 0.0024037785
test_loss: 0.005061139
train_loss: 0.002058909
test_loss: 0.005008761
train_loss: 0.0022063158
test_loss: 0.0050345217
train_loss: 0.0020264103
test_loss: 0.0051071183
train_loss: 0.002091145
test_loss: 0.005062708
train_loss: 0.0020832599
test_loss: 0.0050037135
train_loss: 0.0020419324
test_loss: 0.0050970754
train_loss: 0.002120553
test_loss: 0.0051276484
train_loss: 0.0021624933
test_loss: 0.00515554
train_loss: 0.0023936424
test_loss: 0.005014999
train_loss: 0.0021079043
test_loss: 0.005153971
train_loss: 0.0021624842
test_loss: 0.0051232865
train_loss: 0.0020836825
test_loss: 0.005081532
train_loss: 0.0020966588
test_loss: 0.0050986
train_loss: 0.0022874884
test_loss: 0.005069311
train_loss: 0.0021472569
test_loss: 0.0050954935
train_loss: 0.0020699934
test_loss: 0.005123871
train_loss: 0.0022688871
test_loss: 0.0051279115
train_loss: 0.0021524117
test_loss: 0.005077409
train_loss: 0.0021166694
test_loss: 0.00509224
train_loss: 0.002383858
test_loss: 0.005092973
train_loss: 0.0024113196
test_loss: 0.0051884917
train_loss: 0.002201742
test_loss: 0.005049531
train_loss: 0.002250176
test_loss: 0.005029604
train_loss: 0.002137748
test_loss: 0.0050584534
train_loss: 0.0021846266
test_loss: 0.0051114066
train_loss: 0.002193578
test_loss: 0.004972755
train_loss: 0.0020570222
test_loss: 0.005120121
train_loss: 0.0019327174
test_loss: 0.004990439
train_loss: 0.001984922
test_loss: 0.005038709
train_loss: 0.0020650832
test_loss: 0.0051012584
train_loss: 0.002118551
test_loss: 0.0050876085
train_loss: 0.002019403
test_loss: 0.0050872485
train_loss: 0.0020880576
test_loss: 0.0051863864
train_loss: 0.002031864
test_loss: 0.0051079653
train_loss: 0.0020758696
test_loss: 0.0050435015
train_loss: 0.0020909882
test_loss: 0.005063724
train_loss: 0.0021871496
test_loss: 0.005216999
train_loss: 0.0023475178
test_loss: 0.0051315865
train_loss: 0.0021920835
test_loss: 0.005166369
train_loss: 0.0021744643
test_loss: 0.005093343
train_loss: 0.0020475844
test_loss: 0.005076449
train_loss: 0.0021519477
test_loss: 0.005075868
train_loss: 0.0021054386
test_loss: 0.005046947
train_loss: 0.0020430824
test_loss: 0.0051029455
train_loss: 0.0021745083
test_loss: 0.0051469156
train_loss: 0.00206308
test_loss: 0.005146052
train_loss: 0.0019745089
test_loss: 0.0050716163
train_loss: 0.0020258059
test_loss: 0.005041586
train_loss: 0.0020136451
test_loss: 0.0050785816
train_loss: 0.0022576023
test_loss: 0.0051413
train_loss: 0.002274427
test_loss: 0.0050305156
train_loss: 0.0021855768
test_loss: 0.005110242
train_loss: 0.0021000446
test_loss: 0.0051864036
train_loss: 0.0022324736
test_loss: 0.005048617
train_loss: 0.0022296058
test_loss: 0.005020956
train_loss: 0.0020904925
test_loss: 0.0054017776
train_loss: 0.0023203955
test_loss: 0.0050997725
train_loss: 0.0021815456
test_loss: 0.005208531
train_loss: 0.0022409384
test_loss: 0.0050523677
train_loss: 0.0022658664
test_loss: 0.0051474413
train_loss: 0.0021233251
test_loss: 0.005089914
train_loss: 0.0021503328
test_loss: 0.0050998186
train_loss: 0.0023061356
test_loss: 0.0051155807
train_loss: 0.0024377685
test_loss: 0.0052160914
train_loss: 0.0021858267
test_loss: 0.0051388647
train_loss: 0.0022985665
test_loss: 0.005044556
train_loss: 0.001983704
test_loss: 0.0051598013
train_loss: 0.0019908138
test_loss: 0.0050998423
train_loss: 0.0019240829
test_loss: 0.0052735177
train_loss: 0.002295003
test_loss: 0.0051187426
train_loss: 0.0022700946
test_loss: 0.005167427
train_loss: 0.0020731527
test_loss: 0.0051194616
train_loss: 0.0020801434
test_loss: 0.0050750226
train_loss: 0.0019761059
test_loss: 0.005071881
train_loss: 0.0020422996
test_loss: 0.005076024
train_loss: 0.0020494794
test_loss: 0.0050617736
train_loss: 0.0020209674
test_loss: 0.0050915144
train_loss: 0.001914266
test_loss: 0.005118085
train_loss: 0.0019697081
test_loss: 0.0050812606
train_loss: 0.0020262406
test_loss: 0.0050709243
train_loss: 0.0019178123
test_loss: 0.0050556255
train_loss: 0.0018293153
test_loss: 0.0051352237
train_loss: 0.0020275842
test_loss: 0.0051224874
train_loss: 0.0019141933
test_loss: 0.005033168
train_loss: 0.0018383521
test_loss: 0.0051073437
train_loss: 0.0019949765
test_loss: 0.0050571063
train_loss: 0.001970853
test_loss: 0.005082966
train_loss: 0.0020657831
test_loss: 0.005204585
train_loss: 0.0020345293
test_loss: 0.005201215
train_loss: 0.0020405988
test_loss: 0.0052355207
train_loss: 0.0021199067
test_loss: 0.005202915
train_loss: 0.0021636307
test_loss: 0.0051197214
train_loss: 0.0021534509
test_loss: 0.005088258
train_loss: 0.0020073913
test_loss: 0.005223062
train_loss: 0.002071016
test_loss: 0.00508459
train_loss: 0.0022035968
test_loss: 0.0050649783
train_loss: 0.0018729542
test_loss: 0.0050265417
train_loss: 0.0020108079
test_loss: 0.0050847335
train_loss: 0.0019360569
test_loss: 0.00507271
train_loss: 0.0019624003
test_loss: 0.005052082
train_loss: 0.0021144666
test_loss: 0.0050916374
train_loss: 0.0021199407
test_loss: 0.0051877485
train_loss: 0.002179715
test_loss: 0.0052535012
train_loss: 0.0021813423
test_loss: 0.0051510115
train_loss: 0.0022449116
test_loss: 0.0051930957
train_loss: 0.0021381448
test_loss: 0.005205601
train_loss: 0.002059637
test_loss: 0.0051148683
train_loss: 0.0022331765
test_loss: 0.0053031603
train_loss: 0.0023019658
test_loss: 0.005154733
train_loss: 0.00212428
test_loss: 0.00519321
train_loss: 0.0021030908
test_loss: 0.0052388916
train_loss: 0.0023194537
test_loss: 0.0052410024
train_loss: 0.0022619832
test_loss: 0.005150915
train_loss: 0.0021005063
test_loss: 0.0051491405
train_loss: 0.002071465
test_loss: 0.0052271965
train_loss: 0.002055124
test_loss: 0.005070443
train_loss: 0.002075707
test_loss: 0.0052115032
train_loss: 0.0021479544
test_loss: 0.005163854
train_loss: 0.0022582537
test_loss: 0.0051698308
train_loss: 0.0020067128
test_loss: 0.0051697455
train_loss: 0.0020815146
test_loss: 0.0052766195
train_loss: 0.0020893414
test_loss: 0.005098818
train_loss: 0.0019995528
test_loss: 0.0050788685
train_loss: 0.002079979
test_loss: 0.0050923945
train_loss: 0.0018649183
test_loss: 0.0051627834
train_loss: 0.0020627794
test_loss: 0.005066997
train_loss: 0.0020067487
test_loss: 0.0052714436
train_loss: 0.0019155487
test_loss: 0.0051714857
train_loss: 0.0020069308
test_loss: 0.0051367893
train_loss: 0.0020424472
test_loss: 0.0051179156
train_loss: 0.002053073
test_loss: 0.005117045
train_loss: 0.001973155
test_loss: 0.0051503116
train_loss: 0.0019383627
test_loss: 0.005181974
train_loss: 0.0020670495
test_loss: 0.005204532
train_loss: 0.00209221
test_loss: 0.005102294
train_loss: 0.0020550792
test_loss: 0.005174833
train_loss: 0.001984552
test_loss: 0.0050693625
train_loss: 0.0019950077
test_loss: 0.0051230597
train_loss: 0.0019694627
test_loss: 0.0051414864
train_loss: 0.0019989978
test_loss: 0.0052945195
train_loss: 0.0020449678
test_loss: 0.0050628223
train_loss: 0.0021959862
test_loss: 0.0054235966
train_loss: 0.0022087921
test_loss: 0.0051802387
train_loss: 0.002124869
test_loss: 0.0053367596
train_loss: 0.0023302347
test_loss: 0.005290574
train_loss: 0.0023539986
test_loss: 0.005190455
train_loss: 0.002063755
test_loss: 0.005152913
train_loss: 0.0021583226
test_loss: 0.0052206987
train_loss: 0.0020455846
test_loss: 0.005126797
train_loss: 0.0022315616
test_loss: 0.0052006473
train_loss: 0.0019941605
test_loss: 0.005206834
train_loss: 0.0021471314
test_loss: 0.0051692096
train_loss: 0.002102053
test_loss: 0.005101368
train_loss: 0.0020913412
test_loss: 0.0051885173
train_loss: 0.0020146207
test_loss: 0.0052259956
train_loss: 0.0024160533
test_loss: 0.0051725614
train_loss: 0.002049678
test_loss: 0.0051796776
train_loss: 0.0020627268
test_loss: 0.0052364473
train_loss: 0.0019701861
test_loss: 0.0050825565
train_loss: 0.002205418
test_loss: 0.0051587946
train_loss: 0.0021256367
test_loss: 0.005134751
train_loss: 0.0018929242
test_loss: 0.005195512
train_loss: 0.0020227993
test_loss: 0.0051650456
train_loss: 0.0022225915
test_loss: 0.005165249
train_loss: 0.002089437
test_loss: 0.005111089
train_loss: 0.002113953
test_loss: 0.0052894936
train_loss: 0.0023095908
test_loss: 0.005154352
train_loss: 0.0019614208
test_loss: 0.005242027
train_loss: 0.0021125765
test_loss: 0.005204182
train_loss: 0.002097633
test_loss: 0.0050714244
train_loss: 0.0020077531
test_loss: 0.005076015
train_loss: 0.0018644414
test_loss: 0.0052613826
train_loss: 0.001921718
test_loss: 0.005075948
train_loss: 0.0019452708
test_loss: 0.0051050736
train_loss: 0.00181033
test_loss: 0.005097361
train_loss: 0.0019602422
test_loss: 0.0052210875
train_loss: 0.0023434886
test_loss: 0.005191952
train_loss: 0.0023034066
test_loss: 0.005241395
train_loss: 0.0021674498
test_loss: 0.00509559
train_loss: 0.0020310425
test_loss: 0.005316547
train_loss: 0.0020614727
test_loss: 0.0052110464
train_loss: 0.0019409847
test_loss: 0.0051226933
train_loss: 0.002142582
test_loss: 0.005146322
train_loss: 0.0020001489
test_loss: 0.0051670186
train_loss: 0.0021483942
test_loss: 0.005097353
train_loss: 0.0019860752
test_loss: 0.005198175
train_loss: 0.0020559086
test_loss: 0.0051916186
train_loss: 0.0020393634
test_loss: 0.00513429
train_loss: 0.002024761
test_loss: 0.0052226274
train_loss: 0.0021115243
test_loss: 0.005130691
train_loss: 0.0020513379
test_loss: 0.005171638
train_loss: 0.0019780518
test_loss: 0.005199934
train_loss: 0.0020929938
test_loss: 0.0051102163
train_loss: 0.0019410247
test_loss: 0.005161693
train_loss: 0.0018526464
test_loss: 0.005115238
train_loss: 0.0019443653
test_loss: 0.00512384
train_loss: 0.0018278966
test_loss: 0.005201882
train_loss: 0.0023090683
test_loss: 0.005111591
train_loss: 0.001992476
test_loss: 0.0051947404
train_loss: 0.0022231757
test_loss: 0.005097133
train_loss: 0.0020358665
test_loss: 0.0051756613
train_loss: 0.0018575822
test_loss: 0.0051115504
train_loss: 0.0019981763
test_loss: 0.005205534
train_loss: 0.0022014084
test_loss: 0.0051972843
train_loss: 0.0020508831
test_loss: 0.0050731506
train_loss: 0.0019803247
test_loss: 0.005134912
train_loss: 0.0018828332
test_loss: 0.005142481
train_loss: 0.0019319758
test_loss: 0.0051588695
train_loss: 0.0018318524
test_loss: 0.0050943745
train_loss: 0.0019412459
test_loss: 0.005136829
train_loss: 0.0020037792
test_loss: 0.0052581825
train_loss: 0.0019058916
test_loss: 0.005052738
train_loss: 0.0019923768
test_loss: 0.005292636
train_loss: 0.00184192
test_loss: 0.005168639
train_loss: 0.0019576058
test_loss: 0.0051325546
train_loss: 0.0022126066
test_loss: 0.0052185925
train_loss: 0.002058613
test_loss: 0.0052040657
train_loss: 0.0019069767
test_loss: 0.0052112187
train_loss: 0.0019733266
test_loss: 0.0050869463
train_loss: 0.0021682896
test_loss: 0.005340893
train_loss: 0.0021268425
test_loss: 0.0051993835
train_loss: 0.0021282292
test_loss: 0.0052061924
train_loss: 0.002130081
test_loss: 0.005259396
train_loss: 0.0020888315
test_loss: 0.0052832123
train_loss: 0.0022372282
test_loss: 0.0054004653
train_loss: 0.002130673
test_loss: 0.0052460837
train_loss: 0.0021984184
test_loss: 0.0052841674
train_loss: 0.0023400125
test_loss: 0.0051373686
train_loss: 0.0019678117
test_loss: 0.005319408
train_loss: 0.0023487937
test_loss: 0.0051353015
train_loss: 0.0020644106
test_loss: 0.0051594265
train_loss: 0.0018943935
test_loss: 0.0053148638
train_loss: 0.0020927226
test_loss: 0.0051218383
train_loss: 0.0021352933
test_loss: 0.0051723067
train_loss: 0.0022633458
test_loss: 0.005132916
train_loss: 0.002096154
test_loss: 0.005135104
train_loss: 0.0019293504
test_loss: 0.005392609
train_loss: 0.0021603387
test_loss: 0.005162065
train_loss: 0.0021536255
test_loss: 0.0051516844
train_loss: 0.0019745945
test_loss: 0.0051817037
train_loss: 0.0018319865
test_loss: 0.005122994
train_loss: 0.0018362916
test_loss: 0.005194941
train_loss: 0.001864193
test_loss: 0.0050877742
train_loss: 0.002118381
test_loss: 0.005236349
train_loss: 0.0021190776
test_loss: 0.005128959
train_loss: 0.001862884
test_loss: 0.0052178027
train_loss: 0.0019519902
test_loss: 0.005107292
train_loss: 0.0020828485
test_loss: 0.0051562
train_loss: 0.0019364592
test_loss: 0.0052279592
train_loss: 0.002261538
test_loss: 0.0051735784
train_loss: 0.001963939
test_loss: 0.0051959786
train_loss: 0.002066622
test_loss: 0.005109964
train_loss: 0.0021722007
test_loss: 0.0053194766
train_loss: 0.0019113773
test_loss: 0.005335407
train_loss: 0.0022039115
test_loss: 0.0051582614
train_loss: 0.0022188174
test_loss: 0.0052141557
train_loss: 0.0021176438
test_loss: 0.0051130974
train_loss: 0.0018649392
test_loss: 0.0051875743
train_loss: 0.0019971398
test_loss: 0.005183687
train_loss: 0.0020534708
test_loss: 0.005132105
train_loss: 0.0021438794
test_loss: 0.0052632936
train_loss: 0.0019599074
test_loss: 0.005278544
train_loss: 0.0019707289
test_loss: 0.0052406904
train_loss: 0.0019829082
test_loss: 0.005166842
train_loss: 0.0021474548
test_loss: 0.005253148
train_loss: 0.0020883402
test_loss: 0.005217166
train_loss: 0.001984201
test_loss: 0.005310894
train_loss: 0.0020637028
test_loss: 0.0051951446
train_loss: 0.001893278
test_loss: 0.0051746564
train_loss: 0.001992629
test_loss: 0.005108645
train_loss: 0.0020383943
test_loss: 0.0051528523
train_loss: 0.0018452071
test_loss: 0.005175297
train_loss: 0.0018771313
test_loss: 0.005159203
train_loss: 0.0016586235
test_loss: 0.005084436
train_loss: 0.0019724637
test_loss: 0.0053440114
train_loss: 0.0020289393
test_loss: 0.005157096
train_loss: 0.002135246
test_loss: 0.00512827
train_loss: 0.002071538
test_loss: 0.005180956
train_loss: 0.0021581638
test_loss: 0.0052259746
train_loss: 0.0021558383
test_loss: 0.0052873916
train_loss: 0.002071351
test_loss: 0.005170681
train_loss: 0.0020250285
test_loss: 0.005200941
train_loss: 0.0019473084
test_loss: 0.005206178
train_loss: 0.0018999965
test_loss: 0.0052101007
train_loss: 0.0018851878
test_loss: 0.0051371963
train_loss: 0.0018953728
test_loss: 0.005137232
train_loss: 0.0017957982
test_loss: 0.005143282
train_loss: 0.001911954
test_loss: 0.0051545184
train_loss: 0.001998796
test_loss: 0.005250809
train_loss: 0.0020121906
test_loss: 0.0051864237
train_loss: 0.0020179553
test_loss: 0.005201901
train_loss: 0.0020153674
test_loss: 0.0052403663
train_loss: 0.0022136862
test_loss: 0.0053013745
train_loss: 0.0020446503
test_loss: 0.005208465
train_loss: 0.0021343015
test_loss: 0.0052161496
train_loss: 0.0020631975
test_loss: 0.0052148052
train_loss: 0.0020052963
test_loss: 0.005324572
train_loss: 0.0021781006
test_loss: 0.0053079026
train_loss: 0.0020026749
test_loss: 0.0051913573
train_loss: 0.0020255167
test_loss: 0.0053048925
train_loss: 0.0020717296
test_loss: 0.005200075
train_loss: 0.002041012
test_loss: 0.005282679
train_loss: 0.0020707303
test_loss: 0.0051852185
train_loss: 0.0018964414
test_loss: 0.0052403817
train_loss: 0.0019992075
test_loss: 0.0051714866
train_loss: 0.0019365654
test_loss: 0.005292494
train_loss: 0.0019649663
test_loss: 0.005174858
train_loss: 0.0020324588
test_loss: 0.0052561485
train_loss: 0.0019888668
test_loss: 0.005298666
train_loss: 0.002032191
test_loss: 0.0052058995
train_loss: 0.002019208
test_loss: 0.0052782884
train_loss: 0.0019580568
test_loss: 0.005252649
train_loss: 0.0020319498
test_loss: 0.0051837834
train_loss: 0.0021053478
test_loss: 0.005332551
train_loss: 0.0020439709
test_loss: 0.0051767146
train_loss: 0.0019887842
test_loss: 0.00523096
train_loss: 0.0019252698
test_loss: 0.0052243304
train_loss: 0.0019266534
test_loss: 0.0052742795
train_loss: 0.0019150486
test_loss: 0.005272183
train_loss: 0.0019091971
test_loss: 0.005179123
train_loss: 0.0018382554
test_loss: 0.0052441624
train_loss: 0.0019569362
test_loss: 0.0052612503
train_loss: 0.0019074406
test_loss: 0.0052324664
train_loss: 0.0019079791
test_loss: 0.005157929
train_loss: 0.0018848781
test_loss: 0.005275598
train_loss: 0.0018889568
test_loss: 0.005213886
train_loss: 0.0019172064
test_loss: 0.005182156
train_loss: 0.0019164969
test_loss: 0.0051906775
train_loss: 0.0019160438
test_loss: 0.0052173794
train_loss: 0.0020218163
test_loss: 0.00517371
train_loss: 0.0019576096
test_loss: 0.005329127
train_loss: 0.001989793
test_loss: 0.0052054897
train_loss: 0.0019961353
test_loss: 0.005226039
train_loss: 0.0019319212
test_loss: 0.0051349094
train_loss: 0.001999423
test_loss: 0.0051811915
train_loss: 0.0019600317
test_loss: 0.0052145394
train_loss: 0.0018494066
test_loss: 0.0053118556
train_loss: 0.0019905735
test_loss: 0.0053434647
train_loss: 0.0023407564
test_loss: 0.0052052615
train_loss: 0.002130915
test_loss: 0.005546032
train_loss: 0.002329503
test_loss: 0.0052382573
train_loss: 0.0023687365
test_loss: 0.0052669146
train_loss: 0.0022142115
test_loss: 0.00526731
train_loss: 0.0020349037
test_loss: 0.0051662996
train_loss: 0.0019426323
test_loss: 0.0052560666
train_loss: 0.0022744765
test_loss: 0.0054291226
train_loss: 0.002051123
test_loss: 0.005353213
train_loss: 0.002063853
test_loss: 0.005365094
train_loss: 0.0020299538
test_loss: 0.0053411433
train_loss: 0.0020812452
test_loss: 0.0052012256
train_loss: 0.0021176215
test_loss: 0.005148984
train_loss: 0.001886565
test_loss: 0.0051887403
train_loss: 0.0019580354
test_loss: 0.0051682275
train_loss: 0.0019826884
test_loss: 0.0051890565
train_loss: 0.0019831695
test_loss: 0.005173216
train_loss: 0.0019166137
test_loss: 0.0052665668
train_loss: 0.0019425151
test_loss: 0.005210746
train_loss: 0.0018983423
test_loss: 0.005310931
train_loss: 0.0017282376
test_loss: 0.0051531014
train_loss: 0.0019578906
test_loss: 0.0053782114
train_loss: 0.0019848887
test_loss: 0.0052700317
train_loss: 0.0020216438
test_loss: 0.0052658576
train_loss: 0.001790848
test_loss: 0.0052193482
train_loss: 0.001814912
test_loss: 0.0052951924
train_loss: 0.0019444212
test_loss: 0.0052458174
train_loss: 0.0018993862
test_loss: 0.00521838
train_loss: 0.0018179154
test_loss: 0.005234297
train_loss: 0.0020368202
test_loss: 0.005278119
train_loss: 0.0018434061
test_loss: 0.0052036517
train_loss: 0.001845848
test_loss: 0.00521276
train_loss: 0.0018451558
test_loss: 0.005211204
train_loss: 0.0018921298
test_loss: 0.005204679
train_loss: 0.0019192507
test_loss: 0.0052049593
train_loss: 0.0018231723
test_loss: 0.0052440315
train_loss: 0.001825393
test_loss: 0.0052603795
train_loss: 0.0017515775
test_loss: 0.005245971
train_loss: 0.0017567286
test_loss: 0.005200016
train_loss: 0.0017580058
test_loss: 0.005214561
train_loss: 0.0018659304
test_loss: 0.005244992
train_loss: 0.0019225697
test_loss: 0.005152893
train_loss: 0.0020915896
test_loss: 0.0052162535
train_loss: 0.0020244203
test_loss: 0.0053041563
train_loss: 0.0020124656
test_loss: 0.005246331
train_loss: 0.002089263
test_loss: 0.005206473
train_loss: 0.0018956168
test_loss: 0.005213428
train_loss: 0.0017885948
test_loss: 0.0051661255
train_loss: 0.0018366477
test_loss: 0.0051928493
train_loss: 0.0017905995
test_loss: 0.005155375
train_loss: 0.0019726963
test_loss: 0.0053141075
train_loss: 0.00181436
test_loss: 0.0051719258
train_loss: 0.0019979698
test_loss: 0.005258209
train_loss: 0.0019400509
test_loss: 0.005287578
train_loss: 0.0022714238
test_loss: 0.0053804773
train_loss: 0.0020538806
test_loss: 0.0053548105
train_loss: 0.0019381973
test_loss: 0.0052587013
train_loss: 0.0021658575
test_loss: 0.005295549
train_loss: 0.0020225337
test_loss: 0.005329468
train_loss: 0.0019514055
test_loss: 0.005398456
train_loss: 0.0020742635
test_loss: 0.005251538
train_loss: 0.0021825708
test_loss: 0.0052772434
train_loss: 0.0019808307
test_loss: 0.0052885306
train_loss: 0.0020701156
test_loss: 0.0054266443
train_loss: 0.0021133404
test_loss: 0.0052399323
train_loss: 0.002025207
test_loss: 0.0052947863
train_loss: 0.0018800613
test_loss: 0.005218334
train_loss: 0.0018966073
test_loss: 0.005171162
train_loss: 0.0020300753
test_loss: 0.0052206535
train_loss: 0.0018972484
test_loss: 0.0052587157
train_loss: 0.0019054486
test_loss: 0.005243058
train_loss: 0.0018528578
test_loss: 0.0052409135
train_loss: 0.0018322617
test_loss: 0.0052465266
train_loss: 0.0018805728
test_loss: 0.0052430984
train_loss: 0.0018337758
test_loss: 0.005325202
train_loss: 0.0019353281
test_loss: 0.005200822
train_loss: 0.002005965
test_loss: 0.0052153906
train_loss: 0.001993239
test_loss: 0.00537277
train_loss: 0.0019876608
test_loss: 0.0052074124
train_loss: 0.0020066188
test_loss: 0.0053536524
train_loss: 0.0019102579
test_loss: 0.0052979044
train_loss: 0.0019909001
test_loss: 0.005264174
train_loss: 0.00200464
test_loss: 0.00530177
train_loss: 0.0018643073
test_loss: 0.005246505
train_loss: 0.0018977274
test_loss: 0.0053120144
train_loss: 0.0020046427
test_loss: 0.0052645165
train_loss: 0.0019488839
test_loss: 0.0053385603
train_loss: 0.0019666553
test_loss: 0.005346169
train_loss: 0.002132468
test_loss: 0.0053307833
train_loss: 0.0019977014
test_loss: 0.005343795
train_loss: 0.0018624776
test_loss: 0.005342903
train_loss: 0.0020680518
test_loss: 0.0053129094
train_loss: 0.0020851183
test_loss: 0.005329357
train_loss: 0.002036232
test_loss: 0.005246285
train_loss: 0.0021873
test_loss: 0.0052251252
train_loss: 0.0020056157
test_loss: 0.005387632
train_loss: 0.002045939
test_loss: 0.0053319056
train_loss: 0.0019656266
test_loss: 0.005226853
train_loss: 0.0020324045
test_loss: 0.005360018
train_loss: 0.0021759598
test_loss: 0.0053244485
train_loss: 0.0020451844
test_loss: 0.0053179944
train_loss: 0.0021362312
test_loss: 0.005264027
train_loss: 0.0019543355
test_loss: 0.0052904366
train_loss: 0.0018635395
test_loss: 0.005367769
train_loss: 0.0020786854
test_loss: 0.005219083
train_loss: 0.0019606545
test_loss: 0.005342716
train_loss: 0.0018299869
test_loss: 0.0052759773
train_loss: 0.0019684832
test_loss: 0.0052968054
train_loss: 0.0020096125
test_loss: 0.0052606924
train_loss: 0.0018862723
test_loss: 0.0053143846
train_loss: 0.001988084
test_loss: 0.0052511087
train_loss: 0.0020123683
test_loss: 0.0053747045
train_loss: 0.001992506
test_loss: 0.005379322
train_loss: 0.0022132823
test_loss: 0.0053617875
train_loss: 0.0019228836
test_loss: 0.0053067487
train_loss: 0.0019187987
test_loss: 0.0052620126
train_loss: 0.001969401
test_loss: 0.0054141264
train_loss: 0.0020514962
test_loss: 0.005232865
train_loss: 0.0021084133
test_loss: 0.0053224904
train_loss: 0.002101323
test_loss: 0.0052989204
train_loss: 0.0019665274
test_loss: 0.0052955328
train_loss: 0.0018519503
test_loss: 0.005379247
train_loss: 0.0019889094
test_loss: 0.005321699
train_loss: 0.0020506014
test_loss: 0.0052851145
train_loss: 0.0020528568
test_loss: 0.005297043
train_loss: 0.002020736
test_loss: 0.0053706905
train_loss: 0.0019139415
test_loss: 0.0053419764
train_loss: 0.0019559788
test_loss: 0.005332464
train_loss: 0.0019305124
test_loss: 0.0052612005
train_loss: 0.0019962105
test_loss: 0.005356025
train_loss: 0.0020236978
test_loss: 0.0054828986
train_loss: 0.002047055
test_loss: 0.0053182365
train_loss: 0.0021919946
test_loss: 0.005388239
train_loss: 0.0020560252
test_loss: 0.0053891516
train_loss: 0.00197345
test_loss: 0.005332682
train_loss: 0.0020158805
test_loss: 0.0054972973
train_loss: 0.0020948714
test_loss: 0.005327436
train_loss: 0.0019560948
test_loss: 0.005294399
train_loss: 0.0019438217
test_loss: 0.0052536447
train_loss: 0.0020436193
test_loss: 0.005305123
train_loss: 0.0018680462
test_loss: 0.0052574957
train_loss: 0.0019796859
test_loss: 0.0053106267
train_loss: 0.0021394934
test_loss: 0.0053148405
train_loss: 0.0019815157
test_loss: 0.005407458
train_loss: 0.00196049
test_loss: 0.005384073
train_loss: 0.0022338745
test_loss: 0.005431042
train_loss: 0.0020097506
test_loss: 0.005260645
train_loss: 0.0020779257
test_loss: 0.0052727894
train_loss: 0.0019911556
test_loss: 0.0056234417
train_loss: 0.0023195473
test_loss: 0.0053371536
train_loss: 0.0020646364
test_loss: 0.0052867993
train_loss: 0.0023663105
test_loss: 0.005514742
train_loss: 0.002269047
test_loss: 0.005501814
train_loss: 0.0019424221
test_loss: 0.005379829
train_loss: 0.0022433086
test_loss: 0.005374348
train_loss: 0.0017545309
test_loss: 0.005275807
train_loss: 0.001882581
test_loss: 0.0052852686
train_loss: 0.0019284289
test_loss: 0.00528948
train_loss: 0.0017634827
test_loss: 0.0052643283
train_loss: 0.0019444751
test_loss: 0.0052742604
train_loss: 0.0019047761
test_loss: 0.005354943
train_loss: 0.0018399868
test_loss: 0.0052524763
train_loss: 0.0021125567
test_loss: 0.005388032
train_loss: 0.0021011848
test_loss: 0.005385406
train_loss: 0.001988833
test_loss: 0.0054491316
train_loss: 0.0019878284
test_loss: 0.0053070765
train_loss: 0.0021849961
test_loss: 0.0053053843
train_loss: 0.0018062785
test_loss: 0.0053655524
train_loss: 0.0020342083
test_loss: 0.005380416
train_loss: 0.002094681
test_loss: 0.005495343
train_loss: 0.0019749994
test_loss: 0.0052269306
train_loss: 0.0020132372
test_loss: 0.0053473953
train_loss: 0.001777077
test_loss: 0.005333146
train_loss: 0.0019629695
test_loss: 0.005397164
train_loss: 0.0019114581
test_loss: 0.0053309975
train_loss: 0.0019137572
test_loss: 0.0055188164
train_loss: 0.0022591362
test_loss: 0.005296938
train_loss: 0.0019360685
test_loss: 0.00535699
train_loss: 0.0019623335
test_loss: 0.0052769287
train_loss: 0.0018571338
test_loss: 0.0054404084
train_loss: 0.0019622482
test_loss: 0.00530826
train_loss: 0.001860103
test_loss: 0.0053728786
train_loss: 0.0019406549
test_loss: 0.005339975
train_loss: 0.0018746732
test_loss: 0.005461955
train_loss: 0.00198315
test_loss: 0.0053551435
train_loss: 0.0020877228
test_loss: 0.00540575
train_loss: 0.0019121885
test_loss: 0.0054385373
train_loss: 0.0022339644
test_loss: 0.005310866
train_loss: 0.0019803976
test_loss: 0.0054509332
train_loss: 0.002077578
test_loss: 0.00536546
train_loss: 0.0019209567
test_loss: 0.0052818744
train_loss: 0.001870288
test_loss: 0.00528177
train_loss: 0.0017449551
test_loss: 0.005376654
train_loss: 0.001762499
test_loss: 0.0052903313
train_loss: 0.001800351
test_loss: 0.005410311
train_loss: 0.0019710728
test_loss: 0.005416448
train_loss: 0.0023080898
test_loss: 0.0053545483
train_loss: 0.00214967
test_loss: 0.005610286
train_loss: 0.0022035765
test_loss: 0.005414937
train_loss: 0.002167372
test_loss: 0.0053008157
train_loss: 0.002100409
test_loss: 0.005395169
train_loss: 0.0021340386
test_loss: 0.0053965743
train_loss: 0.0019187997
test_loss: 0.0054245396
train_loss: 0.0022055365
test_loss: 0.0053668274
train_loss: 0.0019634874
test_loss: 0.005421204
train_loss: 0.0019775087
test_loss: 0.0053662877
train_loss: 0.0022095032
test_loss: 0.005436545
train_loss: 0.0023227201
test_loss: 0.005419506
train_loss: 0.0021654859
test_loss: 0.005476619
train_loss: 0.0022609364
test_loss: 0.0054627713
train_loss: 0.0020770386
test_loss: 0.005487245
train_loss: 0.002041154
test_loss: 0.0053414484
train_loss: 0.0024547358
test_loss: 0.005459688
train_loss: 0.0022365595
test_loss: 0.0055379625
train_loss: 0.0021974933
test_loss: 0.0053803017
train_loss: 0.002205314
test_loss: 0.005411572
train_loss: 0.002063936
test_loss: 0.0053987564
train_loss: 0.0019958913
test_loss: 0.0052941046
train_loss: 0.0020226908
test_loss: 0.005557703
train_loss: 0.0019691077
test_loss: 0.005285082
train_loss: 0.0019335871
test_loss: 0.00548144
train_loss: 0.002115698
test_loss: 0.0053543043
train_loss: 0.0021567447
test_loss: 0.0054341424
train_loss: 0.0018807973
test_loss: 0.0054150554
train_loss: 0.0018020793
test_loss: 0.005292742
train_loss: 0.0018711693
test_loss: 0.005367139
train_loss: 0.001878364
test_loss: 0.0052560912
train_loss: 0.001799984
test_loss: 0.005334164
train_loss: 0.0017407693
test_loss: 0.005352138
train_loss: 0.0018626327
test_loss: 0.005342837
train_loss: 0.001960748
test_loss: 0.0054019326
train_loss: 0.0018231623
test_loss: 0.005388681
train_loss: 0.0019340029
test_loss: 0.0053395163
train_loss: 0.0018464145
test_loss: 0.0053190943
train_loss: 0.0019306778
test_loss: 0.0054563656
train_loss: 0.0020180428
test_loss: 0.005355346
train_loss: 0.0018850933
test_loss: 0.0054535693
train_loss: 0.0018892693
test_loss: 0.0053610997
train_loss: 0.002047445
test_loss: 0.0053668143
train_loss: 0.0020531157
test_loss: 0.0053999717
train_loss: 0.002048652
test_loss: 0.0053125797
train_loss: 0.0018853497
test_loss: 0.0053750635
train_loss: 0.0018764331
test_loss: 0.005379489
train_loss: 0.002010644
test_loss: 0.0053965133
train_loss: 0.0017793006
test_loss: 0.0053010024
train_loss: 0.0018668404
test_loss: 0.005378587
train_loss: 0.0019095489
test_loss: 0.0053004036
train_loss: 0.0018819902
test_loss: 0.005427611
train_loss: 0.0018702942
test_loss: 0.0053157383
train_loss: 0.0019275516
test_loss: 0.0053268424
train_loss: 0.0018460416
test_loss: 0.0053484957
train_loss: 0.0018129774
test_loss: 0.0052804314
train_loss: 0.0018278633
test_loss: 0.005403685
train_loss: 0.0018083677
test_loss: 0.0053334455
train_loss: 0.0017344493
test_loss: 0.0054106098
train_loss: 0.0018996579
test_loss: 0.0053838952
train_loss: 0.0017693918
test_loss: 0.005464064
train_loss: 0.0018378568
test_loss: 0.005308091
train_loss: 0.0017895489
test_loss: 0.005482715
train_loss: 0.0018386017
test_loss: 0.005318445
train_loss: 0.0019052335
test_loss: 0.0056016836
train_loss: 0.002186223
test_loss: 0.005509291
train_loss: 0.002325429
test_loss: 0.0054514585
train_loss: 0.0019142309
test_loss: 0.0054277754
train_loss: 0.0020443918
test_loss: 0.005410312
train_loss: 0.0018262325
test_loss: 0.0054232473
train_loss: 0.0018911092
test_loss: 0.0053985096
train_loss: 0.0019374924
test_loss: 0.0053522307
train_loss: 0.0018399924
test_loss: 0.0054480806
train_loss: 0.0019075768
test_loss: 0.0053319875
train_loss: 0.0016076248
test_loss: 0.005401214
train_loss: 0.0019002695
test_loss: 0.005294022
train_loss: 0.002017612
test_loss: 0.0055086454
train_loss: 0.0019081722
test_loss: 0.00539296
train_loss: 0.002216816
test_loss: 0.0054693017
train_loss: 0.0020590662
test_loss: 0.005411101
train_loss: 0.0020091354
test_loss: 0.0055413595
train_loss: 0.0018980508
test_loss: 0.0054199635
train_loss: 0.0019137356
test_loss: 0.0053915353
train_loss: 0.001791506
test_loss: 0.0054294653
train_loss: 0.0018612652
test_loss: 0.005380139
train_loss: 0.0019514195
test_loss: 0.005313268
train_loss: 0.0020769075
test_loss: 0.005458938
train_loss: 0.0018105381
test_loss: 0.0054459837
train_loss: 0.0019466814
test_loss: 0.0053788717
train_loss: 0.001844711
test_loss: 0.005339432
train_loss: 0.0016609718
test_loss: 0.0053714053
train_loss: 0.0018430045
test_loss: 0.0053091766
train_loss: 0.0018138716
test_loss: 0.0054300106
train_loss: 0.0017407663
test_loss: 0.0053458908
train_loss: 0.0017966903
test_loss: 0.005386733
train_loss: 0.0017975941
test_loss: 0.0053905216
train_loss: 0.0018596249
test_loss: 0.0053383945
train_loss: 0.0018738918
test_loss: 0.005381728
train_loss: 0.0018219373
test_loss: 0.0053496202
train_loss: 0.0019535385
test_loss: 0.00539782
train_loss: 0.0019395394
test_loss: 0.00543002
train_loss: 0.0020043482
test_loss: 0.005389697
train_loss: 0.0018367674
test_loss: 0.0053977286
train_loss: 0.0019014219
test_loss: 0.00545854
train_loss: 0.001832116
test_loss: 0.0054015387
train_loss: 0.0019930506
test_loss: 0.0053604394
train_loss: 0.0018878425
test_loss: 0.0053556985
train_loss: 0.00188102
test_loss: 0.005409275
train_loss: 0.002007984
test_loss: 0.0055425884
train_loss: 0.0019551646
test_loss: 0.0052918475
train_loss: 0.0018790534
test_loss: 0.0054706866
train_loss: 0.0019747037
test_loss: 0.005460743
train_loss: 0.0021360517
test_loss: 0.005437782
train_loss: 0.001813253
test_loss: 0.0053270496
train_loss: 0.0019159788
test_loss: 0.0054830182
train_loss: 0.0018844493
test_loss: 0.0055267555
train_loss: 0.0020072416
test_loss: 0.005333069
train_loss: 0.0020068558
test_loss: 0.0054239854
train_loss: 0.0019374223
test_loss: 0.0053962613
train_loss: 0.0018755696
test_loss: 0.005378499
train_loss: 0.0019969444
test_loss: 0.00538572
train_loss: 0.0019131058
test_loss: 0.0054182266
train_loss: 0.0018590286
test_loss: 0.005331118
train_loss: 0.001864708
test_loss: 0.005362609
train_loss: 0.0018283657
test_loss: 0.005315379
train_loss: 0.0017710825
test_loss: 0.005369492
train_loss: 0.0020496044
test_loss: 0.0053373133
train_loss: 0.0020340714
test_loss: 0.005419414
train_loss: 0.0018690826
test_loss: 0.0054136124
train_loss: 0.0017954679
test_loss: 0.0053375033
train_loss: 0.0018219806
test_loss: 0.005593006
train_loss: 0.0019711
test_loss: 0.0053001917
train_loss: 0.0018895988
test_loss: 0.005351671
train_loss: 0.0018398333
test_loss: 0.005440933
train_loss: 0.0019136367
test_loss: 0.005358366
train_loss: 0.001845334
test_loss: 0.0054929564
train_loss: 0.0017833547
test_loss: 0.005313778
train_loss: 0.0018398992
test_loss: 0.0053427527
train_loss: 0.0018847375
test_loss: 0.005618096
train_loss: 0.0020679268
test_loss: 0.00541192
train_loss: 0.0019089387
test_loss: 0.0054125767
train_loss: 0.0019898096
test_loss: 0.0054702363
train_loss: 0.0020355936
test_loss: 0.0055425917
train_loss: 0.002196627
test_loss: 0.005396679
train_loss: 0.0018867853
test_loss: 0.005462449
train_loss: 0.0019691228
test_loss: 0.0055444003
train_loss: 0.0018913336
test_loss: 0.0054558082
train_loss: 0.00191394
test_loss: 0.0054021114
train_loss: 0.0017331387
test_loss: 0.005457239
train_loss: 0.0019712446
test_loss: 0.005427511
train_loss: 0.0019528503
test_loss: 0.005473003
train_loss: 0.001916626
test_loss: 0.005423414
train_loss: 0.0018350072
test_loss: 0.0054706186
train_loss: 0.0018955398
test_loss: 0.005375291
train_loss: 0.0018674333
test_loss: 0.0057528373
train_loss: 0.0022252619
test_loss: 0.00539415
train_loss: 0.0021860707
test_loss: 0.00559631
train_loss: 0.0022824137
test_loss: 0.005511315
train_loss: 0.0021222408
test_loss: 0.0055114757
train_loss: 0.001964129
test_loss: 0.005513306
train_loss: 0.0020534387
test_loss: 0.005502336
train_loss: 0.001866555
test_loss: 0.0054972256
train_loss: 0.0020018758
test_loss: 0.0055366512
train_loss: 0.0021678857
test_loss: 0.0054414673
train_loss: 0.0018389429
test_loss: 0.0054520303
train_loss: 0.0020363955
test_loss: 0.0055140243
train_loss: 0.001976572
test_loss: 0.005405546
train_loss: 0.0020238569
test_loss: 0.0054119984
train_loss: 0.0016885244
test_loss: 0.00547456
train_loss: 0.0017541184
test_loss: 0.0054815887
train_loss: 0.0017652677
test_loss: 0.0053712
train_loss: 0.0017457096
test_loss: 0.0054522557
train_loss: 0.0017604087
test_loss: 0.005384432
train_loss: 0.0019351123
test_loss: 0.00562974
train_loss: 0.0018867629
test_loss: 0.005456831
train_loss: 0.0019419246
test_loss: 0.0053617023
train_loss: 0.0017639729
test_loss: 0.0053770663
train_loss: 0.001958974
test_loss: 0.0056833485
train_loss: 0.0020263994
test_loss: 0.0054652784
train_loss: 0.0019496183
test_loss: 0.0054353364
train_loss: 0.0019515856
test_loss: 0.005418992
train_loss: 0.0019334783
test_loss: 0.0054728026
train_loss: 0.0020901526
test_loss: 0.0054637515
train_loss: 0.0018758836
test_loss: 0.005500452
train_loss: 0.0018826395
test_loss: 0.005371478
train_loss: 0.0017751731
test_loss: 0.005425814
train_loss: 0.0017376245
test_loss: 0.005365768
train_loss: 0.0018270301
test_loss: 0.005497823
train_loss: 0.0018161648
test_loss: 0.0055266707
train_loss: 0.0016313213
test_loss: 0.0053883544
train_loss: 0.0019124242
test_loss: 0.0055167386
train_loss: 0.0019167736
test_loss: 0.005495118
train_loss: 0.0018889534
test_loss: 0.005505713
train_loss: 0.0019313429
test_loss: 0.005471691
train_loss: 0.0016805562
test_loss: 0.0054274676
train_loss: 0.0018204505
test_loss: 0.0055890675
train_loss: 0.0017222606
test_loss: 0.005388499
train_loss: 0.0017553349
test_loss: 0.0054955627
train_loss: 0.0019429696
test_loss: 0.005373586
train_loss: 0.0018958995
test_loss: 0.0054649357
train_loss: 0.0019113314
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0054314686
train_loss: 0.0019381458
test_loss: 0.005440016
train_loss: 0.0021016342
test_loss: 0.0055601057
train_loss: 0.0020006571
test_loss: 0.0056238286
train_loss: 0.0021911785
test_loss: 0.0055919737
train_loss: 0.002246064
test_loss: 0.005571635
train_loss: 0.0019637467
test_loss: 0.0055645755
train_loss: 0.0020401273
test_loss: 0.005518416
train_loss: 0.0020717848
test_loss: 0.0055622645
train_loss: 0.0019529415
test_loss: 0.005482655
train_loss: 0.0020033447
test_loss: 0.0054190094
train_loss: 0.0019650431
test_loss: 0.005502275
train_loss: 0.002032173
test_loss: 0.0054382896
train_loss: 0.0018073465
test_loss: 0.0054660165
train_loss: 0.0017816701
test_loss: 0.0055079404
train_loss: 0.002122093
test_loss: 0.005481543
train_loss: 0.0019349656
test_loss: 0.005586114
train_loss: 0.0021848772
test_loss: 0.005612993
train_loss: 0.0021528888
test_loss: 0.0055852705
train_loss: 0.0021022134
test_loss: 0.0054797297
train_loss: 0.0021326563
test_loss: 0.0055736527
train_loss: 0.0020146836
test_loss: 0.005569063
train_loss: 0.0019126849
test_loss: 0.0054316144
train_loss: 0.0018540216
test_loss: 0.005509392
train_loss: 0.0019296868
test_loss: 0.0054468927
train_loss: 0.0025464355
test_loss: 0.005514507
train_loss: 0.0018833299
test_loss: 0.005660627
train_loss: 0.0019214154
test_loss: 0.005538612
train_loss: 0.0019208703
test_loss: 0.0055384072
train_loss: 0.002002326
test_loss: 0.005489515
train_loss: 0.0020224745
test_loss: 0.005521056
train_loss: 0.002238891
test_loss: 0.0055126757
train_loss: 0.0019321622
test_loss: 0.005610291
train_loss: 0.0019980774
test_loss: 0.0055319485
train_loss: 0.0020288404
test_loss: 0.0055259126
train_loss: 0.0020439983
test_loss: 0.0054903706
train_loss: 0.0018494987
test_loss: 0.0054959254
train_loss: 0.0019252998
test_loss: 0.0055190423
train_loss: 0.0019989775
test_loss: 0.005451286
train_loss: 0.0018921166
test_loss: 0.005501658
train_loss: 0.0018559671
test_loss: 0.005520448
train_loss: 0.001929308
test_loss: 0.0054159313
train_loss: 0.0017628114
test_loss: 0.005436992
train_loss: 0.001817584
test_loss: 0.005460832
train_loss: 0.0018684455
test_loss: 0.0054324004
train_loss: 0.0018529998
test_loss: 0.005478633
train_loss: 0.0020521316
test_loss: 0.0055598533
train_loss: 0.0019424888
test_loss: 0.0054934714
train_loss: 0.0018882428
test_loss: 0.005378661
train_loss: 0.0019118431
test_loss: 0.0055698953
train_loss: 0.0018644932
test_loss: 0.005422068
train_loss: 0.0019103743
test_loss: 0.0055781705
train_loss: 0.0018512541
test_loss: 0.0054086554
train_loss: 0.0016966187
test_loss: 0.0056068483
train_loss: 0.0018481775
test_loss: 0.005475984
train_loss: 0.00177284
test_loss: 0.0056026634
train_loss: 0.0018678318
test_loss: 0.0054835705
train_loss: 0.0020467686
test_loss: 0.005457289
train_loss: 0.001883406
test_loss: 0.005414794
train_loss: 0.0018550876
test_loss: 0.005509271
train_loss: 0.0019341097
test_loss: 0.005629291
train_loss: 0.0022570193
test_loss: 0.0055054044
train_loss: 0.0019699896
test_loss: 0.0055019953
train_loss: 0.0021035576
test_loss: 0.005612088
train_loss: 0.0020506275
test_loss: 0.005461594
train_loss: 0.0021487
test_loss: 0.0055830623
train_loss: 0.0018750742
test_loss: 0.005431899
train_loss: 0.0018829689
test_loss: 0.005634909
train_loss: 0.0020027002
test_loss: 0.0055176667
train_loss: 0.0018368681
test_loss: 0.0054643033
train_loss: 0.001839726
test_loss: 0.005466334
train_loss: 0.0017743856
test_loss: 0.005445084
train_loss: 0.0018883559
test_loss: 0.0054776026
train_loss: 0.0018122534
test_loss: 0.0055299443
train_loss: 0.001883148
test_loss: 0.0055230167
train_loss: 0.0018969635
test_loss: 0.0054600826
train_loss: 0.0017967105
test_loss: 0.005541594
train_loss: 0.001837189
test_loss: 0.0054713762
train_loss: 0.0018442585
test_loss: 0.005536
train_loss: 0.0018843518
test_loss: 0.005499158
train_loss: 0.0018990188
test_loss: 0.005441778
train_loss: 0.002148518
test_loss: 0.005559303
train_loss: 0.0017409953
test_loss: 0.005462572
train_loss: 0.001966181
test_loss: 0.005439641
train_loss: 0.0017044083
test_loss: 0.005526162
train_loss: 0.0019761864
test_loss: 0.0055442997
train_loss: 0.0018920045
test_loss: 0.0055385553
train_loss: 0.0018662461
test_loss: 0.0054847007
train_loss: 0.001873492
test_loss: 0.00551377
train_loss: 0.0019408179
test_loss: 0.005574066
train_loss: 0.0019912543
test_loss: 0.005465336
train_loss: 0.0018993749
test_loss: 0.0054736407
train_loss: 0.0019661598
test_loss: 0.0055193272
train_loss: 0.0017635103
test_loss: 0.0054836115
train_loss: 0.0017262636
test_loss: 0.005466918
train_loss: 0.0017932958
test_loss: 0.0055793687
train_loss: 0.0018585756
test_loss: 0.005453749
train_loss: 0.0017430957
test_loss: 0.005473707
train_loss: 0.0018473065
test_loss: 0.0054524927
train_loss: 0.0019261318
test_loss: 0.0055047693
train_loss: 0.0018303458
test_loss: 0.005455621
train_loss: 0.001939258
test_loss: 0.005540797
train_loss: 0.001801816
test_loss: 0.00553846
train_loss: 0.001705063
test_loss: 0.0054484624
train_loss: 0.0018205787
test_loss: 0.00548024
train_loss: 0.0018277362
test_loss: 0.005430825
train_loss: 0.0018515507
test_loss: 0.005498269
train_loss: 0.0017946477
test_loss: 0.005537103
train_loss: 0.0018077674
test_loss: 0.0054888837
train_loss: 0.0018081074
test_loss: 0.0054675755
train_loss: 0.0019165636
test_loss: 0.005504164
train_loss: 0.0016741158
test_loss: 0.0054907613
train_loss: 0.0017650614
test_loss: 0.005419742
train_loss: 0.0017191083
test_loss: 0.0055653006
train_loss: 0.0016568622
test_loss: 0.0054643177
train_loss: 0.001760935
test_loss: 0.005659833
train_loss: 0.00196769
test_loss: 0.005546511
train_loss: 0.0019051977
test_loss: 0.005533317
train_loss: 0.0019636939
test_loss: 0.0056249523
train_loss: 0.0019494456
test_loss: 0.0055620777
train_loss: 0.001866921
test_loss: 0.005515473
train_loss: 0.0019148255
test_loss: 0.005555687
train_loss: 0.0017867144
test_loss: 0.005478313
train_loss: 0.0016758927
test_loss: 0.0055415537
train_loss: 0.0018676771
test_loss: 0.005501911
train_loss: 0.0020096926
test_loss: 0.0056215855
train_loss: 0.0019443238
test_loss: 0.005544555
train_loss: 0.0018672696
test_loss: 0.0055657383
train_loss: 0.0018825225
test_loss: 0.0055273743
train_loss: 0.0019483309
test_loss: 0.005503215
train_loss: 0.0018221191
test_loss: 0.0055737775
train_loss: 0.0018142466
test_loss: 0.0055059874
train_loss: 0.0019670147
test_loss: 0.0056925006
train_loss: 0.0022719665
test_loss: 0.005574134
train_loss: 0.0018519713
test_loss: 0.0055556237
train_loss: 0.001853592
test_loss: 0.0057993145
train_loss: 0.0022906123
test_loss: 0.005600954
train_loss: 0.0023296021
test_loss: 0.0055364133
train_loss: 0.0020924849
test_loss: 0.005747975
train_loss: 0.0021534294
test_loss: 0.0056385775
train_loss: 0.0022426501
test_loss: 0.0056680054
train_loss: 0.0020232045
test_loss: 0.0056774523
train_loss: 0.0020518121
test_loss: 0.005464192
train_loss: 0.0020244946
test_loss: 0.005557532
train_loss: 0.0019260746
test_loss: 0.0055356533
train_loss: 0.0017085919
test_loss: 0.005577504
train_loss: 0.0021081548
test_loss: 0.0055704745
train_loss: 0.0019945237
test_loss: 0.0056102923
train_loss: 0.0018097636
test_loss: 0.0054273903
train_loss: 0.0016882858
test_loss: 0.005647085
train_loss: 0.0019158449
test_loss: 0.005544672
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad665158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad6d0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad75be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad67bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad6a3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad601bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad6a37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad596840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad5967b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad551c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad4f5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad50df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad4f5510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad4c2378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad491400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad4a77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad4a7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad4596a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad425048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad425d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad425c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad373d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad344840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad34a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad34a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99ad30b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99496508c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f994967a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9949601620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9949616950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99495cf7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99495cfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99495cfe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99495f0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f99495678c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9924601620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
