+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS=300_300_300_1
+ case $RUN in
+ PSI=2
+ OPTIONS='			 --optimizer adam 				 --n_pairs 50000 				 --batch_size 5000 				 --max_epochs 200 				 --learning_rate 0.001 				 --decay_rate 0.98 				 --loss_func weighted_MSE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output80
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output81
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL='--load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0
+ date
Mon Nov  2 08:23:29 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f402661e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f40266378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f401e4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f4022ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f4022f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f40146400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f4022f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef46e8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f40167620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f4022f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f400a4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f40067f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f40067b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f400c1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef4751d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef4751bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef4641598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef46419d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef44a9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef44a9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef44e2158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef458de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef45cd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef45d0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef46116a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef4611bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef46197b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef46198c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef433a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef433aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef4509048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef4467ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef4486730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef4559598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef43f7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ef440ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 1.1762902e-05
test_loss: 1.0715558e-05
train_loss: 8.725289e-06
test_loss: 8.972613e-06
train_loss: 8.21555e-06
test_loss: 8.521943e-06
train_loss: 7.258821e-06
test_loss: 7.3365595e-06
train_loss: 6.5497934e-06
test_loss: 6.8738377e-06
train_loss: 6.0138896e-06
test_loss: 6.4284395e-06
train_loss: 6.183449e-06
test_loss: 6.130449e-06
train_loss: 5.5171413e-06
test_loss: 5.854846e-06
train_loss: 5.3453646e-06
test_loss: 5.464832e-06
train_loss: 5.033834e-06
test_loss: 5.259502e-06
train_loss: 4.8060174e-06
test_loss: 4.9420023e-06
train_loss: 4.472705e-06
test_loss: 4.7872218e-06
train_loss: 4.3582877e-06
test_loss: 4.702769e-06
train_loss: 4.065923e-06
test_loss: 4.5327624e-06
train_loss: 4.176958e-06
test_loss: 4.3365344e-06
train_loss: 3.6996732e-06
test_loss: 4.248774e-06
train_loss: 3.710264e-06
test_loss: 4.111176e-06
train_loss: 3.821283e-06
test_loss: 3.982779e-06
train_loss: 3.4439668e-06
test_loss: 3.921809e-06
train_loss: 3.4222069e-06
test_loss: 3.817846e-06
train_loss: 3.2881085e-06
test_loss: 3.864025e-06
train_loss: 3.3388887e-06
test_loss: 3.8264266e-06
train_loss: 2.9712628e-06
test_loss: 3.690266e-06
train_loss: 3.2567893e-06
test_loss: 3.6599556e-06
train_loss: 3.271646e-06
test_loss: 3.617764e-06
train_loss: 3.0405304e-06
test_loss: 3.6820609e-06
train_loss: 2.920092e-06
test_loss: 3.5291898e-06
train_loss: 3.146189e-06
test_loss: 3.5252917e-06
train_loss: 2.769925e-06
test_loss: 3.4743111e-06
train_loss: 2.811086e-06
test_loss: 3.4784757e-06
train_loss: 3.0950046e-06
test_loss: 3.433531e-06
train_loss: 2.8296004e-06
test_loss: 3.406043e-06
train_loss: 2.8135723e-06
test_loss: 3.412566e-06
train_loss: 2.818058e-06
test_loss: 3.3783638e-06
train_loss: 2.9947e-06
test_loss: 3.3701945e-06
train_loss: 2.7232013e-06
test_loss: 3.3312108e-06
train_loss: 2.7572485e-06
test_loss: 3.3240676e-06
train_loss: 2.6793714e-06
test_loss: 3.3075078e-06
train_loss: 2.826842e-06
test_loss: 3.3200638e-06
train_loss: 2.6374926e-06
test_loss: 3.3051422e-06
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32c46f0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32c46fa620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32c46f0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32c4603f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32c45fd268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32c45fdd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32c460cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32c460c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32c45b1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32c452f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32c45b1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f329b86be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f329b86b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f329b86ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f329b7e8e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f329b78ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f329b80d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f329b7e88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f329b80d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32746b3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32746b3b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f329b75dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f329b75d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32745f21e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32745e1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32745ac840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32745ac8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32745ac400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32745ac158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32745139d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3274553ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f327460e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f327460e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f327460e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f327446f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32743b8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.68967074e-06
Iter: 2 loss: 2.66432335e-06
Iter: 3 loss: 2.91031029e-06
Iter: 4 loss: 2.66342545e-06
Iter: 5 loss: 2.65535459e-06
Iter: 6 loss: 2.65537074e-06
Iter: 7 loss: 2.65075846e-06
Iter: 8 loss: 2.65459425e-06
Iter: 9 loss: 2.64799519e-06
Iter: 10 loss: 2.64382106e-06
Iter: 11 loss: 2.65268181e-06
Iter: 12 loss: 2.64220444e-06
Iter: 13 loss: 2.63616175e-06
Iter: 14 loss: 2.64476921e-06
Iter: 15 loss: 2.63316224e-06
Iter: 16 loss: 2.62993922e-06
Iter: 17 loss: 2.62229401e-06
Iter: 18 loss: 2.71106501e-06
Iter: 19 loss: 2.62161393e-06
Iter: 20 loss: 2.616031e-06
Iter: 21 loss: 2.61479499e-06
Iter: 22 loss: 2.61125024e-06
Iter: 23 loss: 2.60341358e-06
Iter: 24 loss: 2.71852355e-06
Iter: 25 loss: 2.60308207e-06
Iter: 26 loss: 2.59929607e-06
Iter: 27 loss: 2.6493517e-06
Iter: 28 loss: 2.5992897e-06
Iter: 29 loss: 2.59605486e-06
Iter: 30 loss: 2.62684262e-06
Iter: 31 loss: 2.59595436e-06
Iter: 32 loss: 2.59431567e-06
Iter: 33 loss: 2.58950104e-06
Iter: 34 loss: 2.6110356e-06
Iter: 35 loss: 2.58771956e-06
Iter: 36 loss: 2.58060618e-06
Iter: 37 loss: 2.58941964e-06
Iter: 38 loss: 2.57690181e-06
Iter: 39 loss: 2.56791645e-06
Iter: 40 loss: 2.59456169e-06
Iter: 41 loss: 2.56516523e-06
Iter: 42 loss: 2.56489466e-06
Iter: 43 loss: 2.56257e-06
Iter: 44 loss: 2.56049043e-06
Iter: 45 loss: 2.5777897e-06
Iter: 46 loss: 2.56037265e-06
Iter: 47 loss: 2.55939358e-06
Iter: 48 loss: 2.55889449e-06
Iter: 49 loss: 2.55843224e-06
Iter: 50 loss: 2.55622172e-06
Iter: 51 loss: 2.5566269e-06
Iter: 52 loss: 2.55456143e-06
Iter: 53 loss: 2.55135251e-06
Iter: 54 loss: 2.56027397e-06
Iter: 55 loss: 2.55030182e-06
Iter: 56 loss: 2.54816541e-06
Iter: 57 loss: 2.54614883e-06
Iter: 58 loss: 2.54561792e-06
Iter: 59 loss: 2.54318729e-06
Iter: 60 loss: 2.54292877e-06
Iter: 61 loss: 2.54173347e-06
Iter: 62 loss: 2.53895905e-06
Iter: 63 loss: 2.57120882e-06
Iter: 64 loss: 2.53870621e-06
Iter: 65 loss: 2.53747066e-06
Iter: 66 loss: 2.53738085e-06
Iter: 67 loss: 2.53597455e-06
Iter: 68 loss: 2.53906273e-06
Iter: 69 loss: 2.5354293e-06
Iter: 70 loss: 2.53431608e-06
Iter: 71 loss: 2.53102462e-06
Iter: 72 loss: 2.54322913e-06
Iter: 73 loss: 2.52962604e-06
Iter: 74 loss: 2.52671225e-06
Iter: 75 loss: 2.56139788e-06
Iter: 76 loss: 2.52668883e-06
Iter: 77 loss: 2.52440327e-06
Iter: 78 loss: 2.53302142e-06
Iter: 79 loss: 2.52385553e-06
Iter: 80 loss: 2.52235395e-06
Iter: 81 loss: 2.52273298e-06
Iter: 82 loss: 2.52127506e-06
Iter: 83 loss: 2.51988104e-06
Iter: 84 loss: 2.51967322e-06
Iter: 85 loss: 2.5184811e-06
Iter: 86 loss: 2.5177028e-06
Iter: 87 loss: 2.51727215e-06
Iter: 88 loss: 2.51548022e-06
Iter: 89 loss: 2.52246423e-06
Iter: 90 loss: 2.515083e-06
Iter: 91 loss: 2.51433721e-06
Iter: 92 loss: 2.51426718e-06
Iter: 93 loss: 2.51371853e-06
Iter: 94 loss: 2.51262213e-06
Iter: 95 loss: 2.53422468e-06
Iter: 96 loss: 2.5126069e-06
Iter: 97 loss: 2.51186134e-06
Iter: 98 loss: 2.51187703e-06
Iter: 99 loss: 2.51106303e-06
Iter: 100 loss: 2.50920357e-06
Iter: 101 loss: 2.53492067e-06
Iter: 102 loss: 2.50910125e-06
Iter: 103 loss: 2.50777157e-06
Iter: 104 loss: 2.50911194e-06
Iter: 105 loss: 2.50703715e-06
Iter: 106 loss: 2.50547419e-06
Iter: 107 loss: 2.50547964e-06
Iter: 108 loss: 2.50470725e-06
Iter: 109 loss: 2.50357311e-06
Iter: 110 loss: 2.50352878e-06
Iter: 111 loss: 2.50260473e-06
Iter: 112 loss: 2.50161247e-06
Iter: 113 loss: 2.50142e-06
Iter: 114 loss: 2.4996059e-06
Iter: 115 loss: 2.5029708e-06
Iter: 116 loss: 2.49879622e-06
Iter: 117 loss: 2.49693267e-06
Iter: 118 loss: 2.51198685e-06
Iter: 119 loss: 2.49682671e-06
Iter: 120 loss: 2.49587038e-06
Iter: 121 loss: 2.4996848e-06
Iter: 122 loss: 2.49563163e-06
Iter: 123 loss: 2.49467303e-06
Iter: 124 loss: 2.51001347e-06
Iter: 125 loss: 2.4946637e-06
Iter: 126 loss: 2.49427103e-06
Iter: 127 loss: 2.49464188e-06
Iter: 128 loss: 2.49402e-06
Iter: 129 loss: 2.49346749e-06
Iter: 130 loss: 2.49337381e-06
Iter: 131 loss: 2.4929941e-06
Iter: 132 loss: 2.49183222e-06
Iter: 133 loss: 2.49599771e-06
Iter: 134 loss: 2.49150889e-06
Iter: 135 loss: 2.49076879e-06
Iter: 136 loss: 2.48938318e-06
Iter: 137 loss: 2.52057407e-06
Iter: 138 loss: 2.48940364e-06
Iter: 139 loss: 2.48911965e-06
Iter: 140 loss: 2.48859806e-06
Iter: 141 loss: 2.48827018e-06
Iter: 142 loss: 2.48756623e-06
Iter: 143 loss: 2.4991698e-06
Iter: 144 loss: 2.48754895e-06
Iter: 145 loss: 2.48693232e-06
Iter: 146 loss: 2.48726928e-06
Iter: 147 loss: 2.48650576e-06
Iter: 148 loss: 2.48564925e-06
Iter: 149 loss: 2.49905e-06
Iter: 150 loss: 2.48564947e-06
Iter: 151 loss: 2.48531114e-06
Iter: 152 loss: 2.48427705e-06
Iter: 153 loss: 2.48850392e-06
Iter: 154 loss: 2.48390052e-06
Iter: 155 loss: 2.4823662e-06
Iter: 156 loss: 2.48761717e-06
Iter: 157 loss: 2.48195056e-06
Iter: 158 loss: 2.4823994e-06
Iter: 159 loss: 2.48160563e-06
Iter: 160 loss: 2.48125116e-06
Iter: 161 loss: 2.48107017e-06
Iter: 162 loss: 2.48093602e-06
Iter: 163 loss: 2.48057404e-06
Iter: 164 loss: 2.48252036e-06
Iter: 165 loss: 2.48049764e-06
Iter: 166 loss: 2.48005949e-06
Iter: 167 loss: 2.48049059e-06
Iter: 168 loss: 2.47982734e-06
Iter: 169 loss: 2.47922617e-06
Iter: 170 loss: 2.48009314e-06
Iter: 171 loss: 2.47892467e-06
Iter: 172 loss: 2.4785038e-06
Iter: 173 loss: 2.47885873e-06
Iter: 174 loss: 2.47821254e-06
Iter: 175 loss: 2.47747926e-06
Iter: 176 loss: 2.48094e-06
Iter: 177 loss: 2.47730463e-06
Iter: 178 loss: 2.47702337e-06
Iter: 179 loss: 2.47628873e-06
Iter: 180 loss: 2.48766537e-06
Iter: 181 loss: 2.4762785e-06
Iter: 182 loss: 2.47619596e-06
Iter: 183 loss: 2.47603384e-06
Iter: 184 loss: 2.47572825e-06
Iter: 185 loss: 2.47543903e-06
Iter: 186 loss: 2.47537105e-06
Iter: 187 loss: 2.4748656e-06
Iter: 188 loss: 2.47359e-06
Iter: 189 loss: 2.48374408e-06
Iter: 190 loss: 2.4733647e-06
Iter: 191 loss: 2.47237358e-06
Iter: 192 loss: 2.47236744e-06
Iter: 193 loss: 2.47219077e-06
Iter: 194 loss: 2.47201115e-06
Iter: 195 loss: 2.47177513e-06
Iter: 196 loss: 2.47132766e-06
Iter: 197 loss: 2.48105562e-06
Iter: 198 loss: 2.47132721e-06
Iter: 199 loss: 2.4710489e-06
Iter: 200 loss: 2.4710057e-06
Iter: 201 loss: 2.4708138e-06
Iter: 202 loss: 2.47052049e-06
Iter: 203 loss: 2.47050548e-06
Iter: 204 loss: 2.47006028e-06
Iter: 205 loss: 2.47048183e-06
Iter: 206 loss: 2.46975605e-06
Iter: 207 loss: 2.46934678e-06
Iter: 208 loss: 2.47501839e-06
Iter: 209 loss: 2.46935269e-06
Iter: 210 loss: 2.46892932e-06
Iter: 211 loss: 2.46810987e-06
Iter: 212 loss: 2.48335414e-06
Iter: 213 loss: 2.46805644e-06
Iter: 214 loss: 2.46753598e-06
Iter: 215 loss: 2.46955233e-06
Iter: 216 loss: 2.46743639e-06
Iter: 217 loss: 2.46724403e-06
Iter: 218 loss: 2.46717e-06
Iter: 219 loss: 2.46693253e-06
Iter: 220 loss: 2.46624904e-06
Iter: 221 loss: 2.46987565e-06
Iter: 222 loss: 2.46609807e-06
Iter: 223 loss: 2.4653923e-06
Iter: 224 loss: 2.46616037e-06
Iter: 225 loss: 2.46502918e-06
Iter: 226 loss: 2.46434229e-06
Iter: 227 loss: 2.47207709e-06
Iter: 228 loss: 2.46431591e-06
Iter: 229 loss: 2.4637045e-06
Iter: 230 loss: 2.4695737e-06
Iter: 231 loss: 2.46366835e-06
Iter: 232 loss: 2.46351601e-06
Iter: 233 loss: 2.46385707e-06
Iter: 234 loss: 2.46345462e-06
Iter: 235 loss: 2.46316813e-06
Iter: 236 loss: 2.46325021e-06
Iter: 237 loss: 2.46294962e-06
Iter: 238 loss: 2.46268769e-06
Iter: 239 loss: 2.46293484e-06
Iter: 240 loss: 2.46253171e-06
Iter: 241 loss: 2.46213e-06
Iter: 242 loss: 2.46261175e-06
Iter: 243 loss: 2.46187301e-06
Iter: 244 loss: 2.46135733e-06
Iter: 245 loss: 2.46492209e-06
Iter: 246 loss: 2.46130503e-06
Iter: 247 loss: 2.46104219e-06
Iter: 248 loss: 2.46056061e-06
Iter: 249 loss: 2.47166395e-06
Iter: 250 loss: 2.46055606e-06
Iter: 251 loss: 2.46013815e-06
Iter: 252 loss: 2.46377795e-06
Iter: 253 loss: 2.4601386e-06
Iter: 254 loss: 2.45972387e-06
Iter: 255 loss: 2.4624278e-06
Iter: 256 loss: 2.45968795e-06
Iter: 257 loss: 2.45953879e-06
Iter: 258 loss: 2.45910178e-06
Iter: 259 loss: 2.46035074e-06
Iter: 260 loss: 2.4588785e-06
Iter: 261 loss: 2.45812043e-06
Iter: 262 loss: 2.46067839e-06
Iter: 263 loss: 2.45787851e-06
Iter: 264 loss: 2.45796582e-06
Iter: 265 loss: 2.45759475e-06
Iter: 266 loss: 2.45737147e-06
Iter: 267 loss: 2.45699653e-06
Iter: 268 loss: 2.46563627e-06
Iter: 269 loss: 2.45697811e-06
Iter: 270 loss: 2.4568767e-06
Iter: 271 loss: 2.45676847e-06
Iter: 272 loss: 2.45666934e-06
Iter: 273 loss: 2.45635692e-06
Iter: 274 loss: 2.45800084e-06
Iter: 275 loss: 2.45622709e-06
Iter: 276 loss: 2.45587603e-06
Iter: 277 loss: 2.45588217e-06
Iter: 278 loss: 2.45569663e-06
Iter: 279 loss: 2.45546039e-06
Iter: 280 loss: 2.45542606e-06
Iter: 281 loss: 2.45496039e-06
Iter: 282 loss: 2.45645379e-06
Iter: 283 loss: 2.4548342e-06
Iter: 284 loss: 2.45444289e-06
Iter: 285 loss: 2.45486763e-06
Iter: 286 loss: 2.45422052e-06
Iter: 287 loss: 2.45428578e-06
Iter: 288 loss: 2.45408273e-06
Iter: 289 loss: 2.45394858e-06
Iter: 290 loss: 2.45367914e-06
Iter: 291 loss: 2.45735373e-06
Iter: 292 loss: 2.45364117e-06
Iter: 293 loss: 2.45340107e-06
Iter: 294 loss: 2.45329966e-06
Iter: 295 loss: 2.45316824e-06
Iter: 296 loss: 2.45298475e-06
Iter: 297 loss: 2.45267256e-06
Iter: 298 loss: 2.45265801e-06
Iter: 299 loss: 2.45220326e-06
Iter: 300 loss: 2.4518049e-06
Iter: 301 loss: 2.45168758e-06
Iter: 302 loss: 2.45178262e-06
Iter: 303 loss: 2.45135834e-06
Iter: 304 loss: 2.45123397e-06
Iter: 305 loss: 2.45083356e-06
Iter: 306 loss: 2.45343676e-06
Iter: 307 loss: 2.45070714e-06
Iter: 308 loss: 2.45059755e-06
Iter: 309 loss: 2.45052843e-06
Iter: 310 loss: 2.45036131e-06
Iter: 311 loss: 2.45023057e-06
Iter: 312 loss: 2.45019419e-06
Iter: 313 loss: 2.44994089e-06
Iter: 314 loss: 2.45076149e-06
Iter: 315 loss: 2.44986904e-06
Iter: 316 loss: 2.44954458e-06
Iter: 317 loss: 2.45001502e-06
Iter: 318 loss: 2.44935836e-06
Iter: 319 loss: 2.44909597e-06
Iter: 320 loss: 2.44947796e-06
Iter: 321 loss: 2.44898456e-06
Iter: 322 loss: 2.44853163e-06
Iter: 323 loss: 2.45038359e-06
Iter: 324 loss: 2.44842113e-06
Iter: 325 loss: 2.44824423e-06
Iter: 326 loss: 2.44986813e-06
Iter: 327 loss: 2.44822309e-06
Iter: 328 loss: 2.44805688e-06
Iter: 329 loss: 2.44817e-06
Iter: 330 loss: 2.44794978e-06
Iter: 331 loss: 2.44777266e-06
Iter: 332 loss: 2.44745343e-06
Iter: 333 loss: 2.45526962e-06
Iter: 334 loss: 2.44743978e-06
Iter: 335 loss: 2.44712373e-06
Iter: 336 loss: 2.45132833e-06
Iter: 337 loss: 2.44712419e-06
Iter: 338 loss: 2.44679745e-06
Iter: 339 loss: 2.44783519e-06
Iter: 340 loss: 2.44669309e-06
Iter: 341 loss: 2.44649164e-06
Iter: 342 loss: 2.4463202e-06
Iter: 343 loss: 2.44629086e-06
Iter: 344 loss: 2.44610897e-06
Iter: 345 loss: 2.44609646e-06
Iter: 346 loss: 2.44597913e-06
Iter: 347 loss: 2.4457504e-06
Iter: 348 loss: 2.44871626e-06
Iter: 349 loss: 2.44572152e-06
Iter: 350 loss: 2.44551893e-06
Iter: 351 loss: 2.44551165e-06
Iter: 352 loss: 2.44534385e-06
Iter: 353 loss: 2.44493572e-06
Iter: 354 loss: 2.44994226e-06
Iter: 355 loss: 2.44492367e-06
Iter: 356 loss: 2.44451348e-06
Iter: 357 loss: 2.44451985e-06
Iter: 358 loss: 2.44429293e-06
Iter: 359 loss: 2.44422608e-06
Iter: 360 loss: 2.44405533e-06
Iter: 361 loss: 2.4438948e-06
Iter: 362 loss: 2.44386865e-06
Iter: 363 loss: 2.44379225e-06
Iter: 364 loss: 2.44375224e-06
Iter: 365 loss: 2.44369153e-06
Iter: 366 loss: 2.44354601e-06
Iter: 367 loss: 2.44347439e-06
Iter: 368 loss: 2.44342618e-06
Iter: 369 loss: 2.44324519e-06
Iter: 370 loss: 2.4429969e-06
Iter: 371 loss: 2.44298622e-06
Iter: 372 loss: 2.44266903e-06
Iter: 373 loss: 2.44488228e-06
Iter: 374 loss: 2.44262083e-06
Iter: 375 loss: 2.44237481e-06
Iter: 376 loss: 2.44356488e-06
Iter: 377 loss: 2.44233752e-06
Iter: 378 loss: 2.44203261e-06
Iter: 379 loss: 2.44192415e-06
Iter: 380 loss: 2.44177954e-06
Iter: 381 loss: 2.44154626e-06
Iter: 382 loss: 2.44442526e-06
Iter: 383 loss: 2.44156467e-06
Iter: 384 loss: 2.44137141e-06
Iter: 385 loss: 2.44355738e-06
Iter: 386 loss: 2.44134526e-06
Iter: 387 loss: 2.44124385e-06
Iter: 388 loss: 2.4411047e-06
Iter: 389 loss: 2.44110061e-06
Iter: 390 loss: 2.44085e-06
Iter: 391 loss: 2.44199327e-06
Iter: 392 loss: 2.44081e-06
Iter: 393 loss: 2.440657e-06
Iter: 394 loss: 2.44241642e-06
Iter: 395 loss: 2.44064677e-06
Iter: 396 loss: 2.44052e-06
Iter: 397 loss: 2.4402143e-06
Iter: 398 loss: 2.44364787e-06
Iter: 399 loss: 2.44017428e-06
Iter: 400 loss: 2.44007038e-06
Iter: 401 loss: 2.44003195e-06
Iter: 402 loss: 2.43987415e-06
Iter: 403 loss: 2.43996192e-06
Iter: 404 loss: 2.43978866e-06
Iter: 405 loss: 2.43971908e-06
Iter: 406 loss: 2.440049e-06
Iter: 407 loss: 2.43968611e-06
Iter: 408 loss: 2.43957015e-06
Iter: 409 loss: 2.4396254e-06
Iter: 410 loss: 2.4395008e-06
Iter: 411 loss: 2.43933277e-06
Iter: 412 loss: 2.4390697e-06
Iter: 413 loss: 2.43907516e-06
Iter: 414 loss: 2.43877184e-06
Iter: 415 loss: 2.43891e-06
Iter: 416 loss: 2.43857949e-06
Iter: 417 loss: 2.43868703e-06
Iter: 418 loss: 2.43840168e-06
Iter: 419 loss: 2.43825662e-06
Iter: 420 loss: 2.43815475e-06
Iter: 421 loss: 2.43812292e-06
Iter: 422 loss: 2.43798831e-06
Iter: 423 loss: 2.4390506e-06
Iter: 424 loss: 2.43796353e-06
Iter: 425 loss: 2.43780642e-06
Iter: 426 loss: 2.43795353e-06
Iter: 427 loss: 2.43771774e-06
Iter: 428 loss: 2.43753198e-06
Iter: 429 loss: 2.43787463e-06
Iter: 430 loss: 2.43743443e-06
Iter: 431 loss: 2.43728664e-06
Iter: 432 loss: 2.43724412e-06
Iter: 433 loss: 2.43713293e-06
Iter: 434 loss: 2.4369424e-06
Iter: 435 loss: 2.43696e-06
Iter: 436 loss: 2.43685167e-06
Iter: 437 loss: 2.43662907e-06
Iter: 438 loss: 2.44003263e-06
Iter: 439 loss: 2.43663112e-06
Iter: 440 loss: 2.43659133e-06
Iter: 441 loss: 2.43651948e-06
Iter: 442 loss: 2.43645309e-06
Iter: 443 loss: 2.43631757e-06
Iter: 444 loss: 2.43810018e-06
Iter: 445 loss: 2.43630871e-06
Iter: 446 loss: 2.43609315e-06
Iter: 447 loss: 2.43599516e-06
Iter: 448 loss: 2.43592604e-06
Iter: 449 loss: 2.43570048e-06
Iter: 450 loss: 2.43568775e-06
Iter: 451 loss: 2.43540944e-06
Iter: 452 loss: 2.43562408e-06
Iter: 453 loss: 2.4352471e-06
Iter: 454 loss: 2.43508589e-06
Iter: 455 loss: 2.43541149e-06
Iter: 456 loss: 2.43498562e-06
Iter: 457 loss: 2.43474619e-06
Iter: 458 loss: 2.43609475e-06
Iter: 459 loss: 2.43471641e-06
Iter: 460 loss: 2.43455611e-06
Iter: 461 loss: 2.43531258e-06
Iter: 462 loss: 2.43452541e-06
Iter: 463 loss: 2.43444151e-06
Iter: 464 loss: 2.43430418e-06
Iter: 465 loss: 2.43429986e-06
Iter: 466 loss: 2.43414411e-06
Iter: 467 loss: 2.43415457e-06
Iter: 468 loss: 2.43398972e-06
Iter: 469 loss: 2.43359091e-06
Iter: 470 loss: 2.43717614e-06
Iter: 471 loss: 2.43355407e-06
Iter: 472 loss: 2.43353838e-06
Iter: 473 loss: 2.43340651e-06
Iter: 474 loss: 2.43328122e-06
Iter: 475 loss: 2.43300406e-06
Iter: 476 loss: 2.43747809e-06
Iter: 477 loss: 2.43297472e-06
Iter: 478 loss: 2.43270961e-06
Iter: 479 loss: 2.43322211e-06
Iter: 480 loss: 2.43259137e-06
Iter: 481 loss: 2.43242607e-06
Iter: 482 loss: 2.43289378e-06
Iter: 483 loss: 2.43237082e-06
Iter: 484 loss: 2.43224076e-06
Iter: 485 loss: 2.43225395e-06
Iter: 486 loss: 2.43213981e-06
Iter: 487 loss: 2.4318997e-06
Iter: 488 loss: 2.43448858e-06
Iter: 489 loss: 2.43188651e-06
Iter: 490 loss: 2.4316721e-06
Iter: 491 loss: 2.43167096e-06
Iter: 492 loss: 2.43149179e-06
Iter: 493 loss: 2.4315043e-06
Iter: 494 loss: 2.43138174e-06
Iter: 495 loss: 2.43113482e-06
Iter: 496 loss: 2.43209161e-06
Iter: 497 loss: 2.43108752e-06
Iter: 498 loss: 2.43094428e-06
Iter: 499 loss: 2.4311405e-06
Iter: 500 loss: 2.43088334e-06
Iter: 501 loss: 2.43069121e-06
Iter: 502 loss: 2.43180739e-06
Iter: 503 loss: 2.43067711e-06
Iter: 504 loss: 2.430623e-06
Iter: 505 loss: 2.4305441e-06
Iter: 506 loss: 2.43052909e-06
Iter: 507 loss: 2.43035e-06
Iter: 508 loss: 2.43097702e-06
Iter: 509 loss: 2.43029967e-06
Iter: 510 loss: 2.43010822e-06
Iter: 511 loss: 2.42967326e-06
Iter: 512 loss: 2.43291788e-06
Iter: 513 loss: 2.42956503e-06
Iter: 514 loss: 2.42915894e-06
Iter: 515 loss: 2.43263594e-06
Iter: 516 loss: 2.42914666e-06
Iter: 517 loss: 2.42937313e-06
Iter: 518 loss: 2.42900956e-06
Iter: 519 loss: 2.42893611e-06
Iter: 520 loss: 2.42869737e-06
Iter: 521 loss: 2.43129671e-06
Iter: 522 loss: 2.42868509e-06
Iter: 523 loss: 2.42851365e-06
Iter: 524 loss: 2.42962756e-06
Iter: 525 loss: 2.42848569e-06
Iter: 526 loss: 2.42828037e-06
Iter: 527 loss: 2.42907618e-06
Iter: 528 loss: 2.42824149e-06
Iter: 529 loss: 2.42808596e-06
Iter: 530 loss: 2.42828719e-06
Iter: 531 loss: 2.42801912e-06
Iter: 532 loss: 2.42777378e-06
Iter: 533 loss: 2.42747888e-06
Iter: 534 loss: 2.42747547e-06
Iter: 535 loss: 2.42733358e-06
Iter: 536 loss: 2.42726355e-06
Iter: 537 loss: 2.42709984e-06
Iter: 538 loss: 2.42690885e-06
Iter: 539 loss: 2.42689566e-06
Iter: 540 loss: 2.42678925e-06
Iter: 541 loss: 2.4267913e-06
Iter: 542 loss: 2.42671558e-06
Iter: 543 loss: 2.42655619e-06
Iter: 544 loss: 2.42824035e-06
Iter: 545 loss: 2.42655096e-06
Iter: 546 loss: 2.42635269e-06
Iter: 547 loss: 2.42624355e-06
Iter: 548 loss: 2.42619944e-06
Iter: 549 loss: 2.42599708e-06
Iter: 550 loss: 2.4275414e-06
Iter: 551 loss: 2.42599504e-06
Iter: 552 loss: 2.42586657e-06
Iter: 553 loss: 2.42552187e-06
Iter: 554 loss: 2.43023169e-06
Iter: 555 loss: 2.42549982e-06
Iter: 556 loss: 2.42508099e-06
Iter: 557 loss: 2.42622764e-06
Iter: 558 loss: 2.42494616e-06
Iter: 559 loss: 2.4249407e-06
Iter: 560 loss: 2.42484293e-06
Iter: 561 loss: 2.42474562e-06
Iter: 562 loss: 2.42469696e-06
Iter: 563 loss: 2.4246724e-06
Iter: 564 loss: 2.42453825e-06
Iter: 565 loss: 2.42524038e-06
Iter: 566 loss: 2.42453189e-06
Iter: 567 loss: 2.42440683e-06
Iter: 568 loss: 2.4242795e-06
Iter: 569 loss: 2.42425313e-06
Iter: 570 loss: 2.4240353e-06
Iter: 571 loss: 2.42578722e-06
Iter: 572 loss: 2.42400347e-06
Iter: 573 loss: 2.4238484e-06
Iter: 574 loss: 2.42402439e-06
Iter: 575 loss: 2.42374813e-06
Iter: 576 loss: 2.42352598e-06
Iter: 577 loss: 2.42357805e-06
Iter: 578 loss: 2.42336182e-06
Iter: 579 loss: 2.42309829e-06
Iter: 580 loss: 2.42304304e-06
Iter: 581 loss: 2.42288274e-06
Iter: 582 loss: 2.42284295e-06
Iter: 583 loss: 2.42275109e-06
Iter: 584 loss: 2.42264787e-06
Iter: 585 loss: 2.4224878e-06
Iter: 586 loss: 2.42536316e-06
Iter: 587 loss: 2.42251417e-06
Iter: 588 loss: 2.42223905e-06
Iter: 589 loss: 2.42212172e-06
Iter: 590 loss: 2.42201759e-06
Iter: 591 loss: 2.42185843e-06
Iter: 592 loss: 2.42184592e-06
Iter: 593 loss: 2.42165902e-06
Iter: 594 loss: 2.42195574e-06
Iter: 595 loss: 2.42156602e-06
Iter: 596 loss: 2.42137048e-06
Iter: 597 loss: 2.4218898e-06
Iter: 598 loss: 2.42134433e-06
Iter: 599 loss: 2.4211613e-06
Iter: 600 loss: 2.42171132e-06
Iter: 601 loss: 2.42112606e-06
Iter: 602 loss: 2.42104124e-06
Iter: 603 loss: 2.42236774e-06
Iter: 604 loss: 2.42102851e-06
Iter: 605 loss: 2.42096667e-06
Iter: 606 loss: 2.42081069e-06
Iter: 607 loss: 2.42079932e-06
Iter: 608 loss: 2.42059332e-06
Iter: 609 loss: 2.42190572e-06
Iter: 610 loss: 2.4206056e-06
Iter: 611 loss: 2.4204619e-06
Iter: 612 loss: 2.42013857e-06
Iter: 613 loss: 2.42325086e-06
Iter: 614 loss: 2.42011174e-06
Iter: 615 loss: 2.42002329e-06
Iter: 616 loss: 2.41991756e-06
Iter: 617 loss: 2.41978319e-06
Iter: 618 loss: 2.41959242e-06
Iter: 619 loss: 2.41957378e-06
Iter: 620 loss: 2.41935095e-06
Iter: 621 loss: 2.41936914e-06
Iter: 622 loss: 2.4192077e-06
Iter: 623 loss: 2.41897806e-06
Iter: 624 loss: 2.41938233e-06
Iter: 625 loss: 2.41887687e-06
Iter: 626 loss: 2.41873772e-06
Iter: 627 loss: 2.41872044e-06
Iter: 628 loss: 2.41857606e-06
Iter: 629 loss: 2.41830139e-06
Iter: 630 loss: 2.42256624e-06
Iter: 631 loss: 2.4182882e-06
Iter: 632 loss: 2.41796829e-06
Iter: 633 loss: 2.42193391e-06
Iter: 634 loss: 2.41795533e-06
Iter: 635 loss: 2.41773023e-06
Iter: 636 loss: 2.41808402e-06
Iter: 637 loss: 2.41762109e-06
Iter: 638 loss: 2.41740281e-06
Iter: 639 loss: 2.41901716e-06
Iter: 640 loss: 2.41738371e-06
Iter: 641 loss: 2.41728912e-06
Iter: 642 loss: 2.41773159e-06
Iter: 643 loss: 2.41725047e-06
Iter: 644 loss: 2.41713497e-06
Iter: 645 loss: 2.41689622e-06
Iter: 646 loss: 2.4214487e-06
Iter: 647 loss: 2.41684461e-06
Iter: 648 loss: 2.4166593e-06
Iter: 649 loss: 2.41705766e-06
Iter: 650 loss: 2.41656471e-06
Iter: 651 loss: 2.41635234e-06
Iter: 652 loss: 2.41973157e-06
Iter: 653 loss: 2.41634461e-06
Iter: 654 loss: 2.41621501e-06
Iter: 655 loss: 2.41581483e-06
Iter: 656 loss: 2.41951466e-06
Iter: 657 loss: 2.41577595e-06
Iter: 658 loss: 2.41542193e-06
Iter: 659 loss: 2.41656448e-06
Iter: 660 loss: 2.41534417e-06
Iter: 661 loss: 2.41513294e-06
Iter: 662 loss: 2.41703947e-06
Iter: 663 loss: 2.41511839e-06
Iter: 664 loss: 2.41493944e-06
Iter: 665 loss: 2.41493535e-06
Iter: 666 loss: 2.41480984e-06
Iter: 667 loss: 2.41458702e-06
Iter: 668 loss: 2.41822272e-06
Iter: 669 loss: 2.41458702e-06
Iter: 670 loss: 2.41427642e-06
Iter: 671 loss: 2.41718089e-06
Iter: 672 loss: 2.41424846e-06
Iter: 673 loss: 2.41406133e-06
Iter: 674 loss: 2.41465432e-06
Iter: 675 loss: 2.41400153e-06
Iter: 676 loss: 2.41376074e-06
Iter: 677 loss: 2.41400926e-06
Iter: 678 loss: 2.4136325e-06
Iter: 679 loss: 2.41338012e-06
Iter: 680 loss: 2.41610405e-06
Iter: 681 loss: 2.41338057e-06
Iter: 682 loss: 2.41329326e-06
Iter: 683 loss: 2.41309681e-06
Iter: 684 loss: 2.41529278e-06
Iter: 685 loss: 2.4130693e-06
Iter: 686 loss: 2.41285738e-06
Iter: 687 loss: 2.41425687e-06
Iter: 688 loss: 2.41286807e-06
Iter: 689 loss: 2.41264797e-06
Iter: 690 loss: 2.41411635e-06
Iter: 691 loss: 2.41261296e-06
Iter: 692 loss: 2.41252474e-06
Iter: 693 loss: 2.41216958e-06
Iter: 694 loss: 2.41293401e-06
Iter: 695 loss: 2.41201633e-06
Iter: 696 loss: 2.41160819e-06
Iter: 697 loss: 2.41570024e-06
Iter: 698 loss: 2.41158659e-06
Iter: 699 loss: 2.41136422e-06
Iter: 700 loss: 2.41216389e-06
Iter: 701 loss: 2.41131056e-06
Iter: 702 loss: 2.41121415e-06
Iter: 703 loss: 2.41117368e-06
Iter: 704 loss: 2.41109888e-06
Iter: 705 loss: 2.4108806e-06
Iter: 706 loss: 2.41260204e-06
Iter: 707 loss: 2.41085263e-06
Iter: 708 loss: 2.4105575e-06
Iter: 709 loss: 2.41237194e-06
Iter: 710 loss: 2.41049838e-06
Iter: 711 loss: 2.41025737e-06
Iter: 712 loss: 2.41091493e-06
Iter: 713 loss: 2.41016255e-06
Iter: 714 loss: 2.41004727e-06
Iter: 715 loss: 2.41003636e-06
Iter: 716 loss: 2.4099304e-06
Iter: 717 loss: 2.40976601e-06
Iter: 718 loss: 2.41354564e-06
Iter: 719 loss: 2.40980921e-06
Iter: 720 loss: 2.409638e-06
Iter: 721 loss: 2.41126486e-06
Iter: 722 loss: 2.40964391e-06
Iter: 723 loss: 2.40949385e-06
Iter: 724 loss: 2.4095043e-06
Iter: 725 loss: 2.40935015e-06
Iter: 726 loss: 2.40924828e-06
Iter: 727 loss: 2.40905956e-06
Iter: 728 loss: 2.40904956e-06
Iter: 729 loss: 2.4089818e-06
Iter: 730 loss: 2.40890722e-06
Iter: 731 loss: 2.40886811e-06
Iter: 732 loss: 2.40873896e-06
Iter: 733 loss: 2.41025919e-06
Iter: 734 loss: 2.40869622e-06
Iter: 735 loss: 2.40851864e-06
Iter: 736 loss: 2.40860618e-06
Iter: 737 loss: 2.40842655e-06
Iter: 738 loss: 2.40815802e-06
Iter: 739 loss: 2.40783856e-06
Iter: 740 loss: 2.4078372e-06
Iter: 741 loss: 2.40739791e-06
Iter: 742 loss: 2.41146017e-06
Iter: 743 loss: 2.40738655e-06
Iter: 744 loss: 2.40723693e-06
Iter: 745 loss: 2.40722102e-06
Iter: 746 loss: 2.40711961e-06
Iter: 747 loss: 2.40787313e-06
Iter: 748 loss: 2.40711779e-06
Iter: 749 loss: 2.40702047e-06
Iter: 750 loss: 2.40710938e-06
Iter: 751 loss: 2.40698637e-06
Iter: 752 loss: 2.4069177e-06
Iter: 753 loss: 2.40683949e-06
Iter: 754 loss: 2.40681948e-06
Iter: 755 loss: 2.40666691e-06
Iter: 756 loss: 2.4083181e-06
Iter: 757 loss: 2.40663735e-06
Iter: 758 loss: 2.4065971e-06
Iter: 759 loss: 2.40646978e-06
Iter: 760 loss: 2.40936197e-06
Iter: 761 loss: 2.40646159e-06
Iter: 762 loss: 2.4062083e-06
Iter: 763 loss: 2.40743111e-06
Iter: 764 loss: 2.40614145e-06
Iter: 765 loss: 2.40603686e-06
Iter: 766 loss: 2.4059209e-06
Iter: 767 loss: 2.40588565e-06
Iter: 768 loss: 2.40584541e-06
Iter: 769 loss: 2.40582722e-06
Iter: 770 loss: 2.40574877e-06
Iter: 771 loss: 2.40558711e-06
Iter: 772 loss: 2.40619124e-06
Iter: 773 loss: 2.40553732e-06
Iter: 774 loss: 2.40534564e-06
Iter: 775 loss: 2.40522422e-06
Iter: 776 loss: 2.40512918e-06
Iter: 777 loss: 2.40485724e-06
Iter: 778 loss: 2.40496638e-06
Iter: 779 loss: 2.40468216e-06
Iter: 780 loss: 2.40435384e-06
Iter: 781 loss: 2.4054134e-06
Iter: 782 loss: 2.4042165e-06
Iter: 783 loss: 2.40410736e-06
Iter: 784 loss: 2.40407e-06
Iter: 785 loss: 2.4039241e-06
Iter: 786 loss: 2.40429631e-06
Iter: 787 loss: 2.40385134e-06
Iter: 788 loss: 2.40369218e-06
Iter: 789 loss: 2.40383406e-06
Iter: 790 loss: 2.40359577e-06
Iter: 791 loss: 2.40338022e-06
Iter: 792 loss: 2.4038186e-06
Iter: 793 loss: 2.40332611e-06
Iter: 794 loss: 2.40304098e-06
Iter: 795 loss: 2.40425356e-06
Iter: 796 loss: 2.40297754e-06
Iter: 797 loss: 2.40288136e-06
Iter: 798 loss: 2.40326517e-06
Iter: 799 loss: 2.40281565e-06
Iter: 800 loss: 2.40268832e-06
Iter: 801 loss: 2.40254963e-06
Iter: 802 loss: 2.402478e-06
Iter: 803 loss: 2.40231498e-06
Iter: 804 loss: 2.40268355e-06
Iter: 805 loss: 2.40226723e-06
Iter: 806 loss: 2.40203258e-06
Iter: 807 loss: 2.40296026e-06
Iter: 808 loss: 2.40196141e-06
Iter: 809 loss: 2.40184181e-06
Iter: 810 loss: 2.4015028e-06
Iter: 811 loss: 2.40438794e-06
Iter: 812 loss: 2.40144163e-06
Iter: 813 loss: 2.40101144e-06
Iter: 814 loss: 2.40282202e-06
Iter: 815 loss: 2.40094414e-06
Iter: 816 loss: 2.40067652e-06
Iter: 817 loss: 2.40342706e-06
Iter: 818 loss: 2.40068e-06
Iter: 819 loss: 2.40056147e-06
Iter: 820 loss: 2.40054487e-06
Iter: 821 loss: 2.40042982e-06
Iter: 822 loss: 2.40037161e-06
Iter: 823 loss: 2.40030568e-06
Iter: 824 loss: 2.40016607e-06
Iter: 825 loss: 2.40094346e-06
Iter: 826 loss: 2.4001406e-06
Iter: 827 loss: 2.40001373e-06
Iter: 828 loss: 2.40012128e-06
Iter: 829 loss: 2.39989231e-06
Iter: 830 loss: 2.39970518e-06
Iter: 831 loss: 2.40023383e-06
Iter: 832 loss: 2.39962242e-06
Iter: 833 loss: 2.39948918e-06
Iter: 834 loss: 2.40046461e-06
Iter: 835 loss: 2.39944939e-06
Iter: 836 loss: 2.39929091e-06
Iter: 837 loss: 2.39895962e-06
Iter: 838 loss: 2.40647228e-06
Iter: 839 loss: 2.39896e-06
Iter: 840 loss: 2.39892847e-06
Iter: 841 loss: 2.39885185e-06
Iter: 842 loss: 2.39878136e-06
Iter: 843 loss: 2.39859628e-06
Iter: 844 loss: 2.40177906e-06
Iter: 845 loss: 2.39862766e-06
Iter: 846 loss: 2.39843735e-06
Iter: 847 loss: 2.39817132e-06
Iter: 848 loss: 2.39817928e-06
Iter: 849 loss: 2.39778024e-06
Iter: 850 loss: 2.39867836e-06
Iter: 851 loss: 2.39763949e-06
Iter: 852 loss: 2.39748806e-06
Iter: 853 loss: 2.39744872e-06
Iter: 854 loss: 2.39723022e-06
Iter: 855 loss: 2.39788051e-06
Iter: 856 loss: 2.39718929e-06
Iter: 857 loss: 2.39704787e-06
Iter: 858 loss: 2.39763858e-06
Iter: 859 loss: 2.39703263e-06
Iter: 860 loss: 2.39686779e-06
Iter: 861 loss: 2.39720066e-06
Iter: 862 loss: 2.39681617e-06
Iter: 863 loss: 2.39669635e-06
Iter: 864 loss: 2.39715632e-06
Iter: 865 loss: 2.3966611e-06
Iter: 866 loss: 2.39651558e-06
Iter: 867 loss: 2.39629071e-06
Iter: 868 loss: 2.39628389e-06
Iter: 869 loss: 2.39593714e-06
Iter: 870 loss: 2.39814062e-06
Iter: 871 loss: 2.39588599e-06
Iter: 872 loss: 2.39578594e-06
Iter: 873 loss: 2.39576866e-06
Iter: 874 loss: 2.39566407e-06
Iter: 875 loss: 2.39542669e-06
Iter: 876 loss: 2.39708902e-06
Iter: 877 loss: 2.39540941e-06
Iter: 878 loss: 2.39530686e-06
Iter: 879 loss: 2.39505289e-06
Iter: 880 loss: 2.39787732e-06
Iter: 881 loss: 2.39501333e-06
Iter: 882 loss: 2.39469728e-06
Iter: 883 loss: 2.3949076e-06
Iter: 884 loss: 2.39453107e-06
Iter: 885 loss: 2.3942107e-06
Iter: 886 loss: 2.39689393e-06
Iter: 887 loss: 2.39424e-06
Iter: 888 loss: 2.39403357e-06
Iter: 889 loss: 2.39401629e-06
Iter: 890 loss: 2.39392102e-06
Iter: 891 loss: 2.39387919e-06
Iter: 892 loss: 2.39379938e-06
Iter: 893 loss: 2.39364294e-06
Iter: 894 loss: 2.39535893e-06
Iter: 895 loss: 2.3936409e-06
Iter: 896 loss: 2.3935404e-06
Iter: 897 loss: 2.39363726e-06
Iter: 898 loss: 2.39347173e-06
Iter: 899 loss: 2.39331371e-06
Iter: 900 loss: 2.39330416e-06
Iter: 901 loss: 2.39319093e-06
Iter: 902 loss: 2.39301153e-06
Iter: 903 loss: 2.39496831e-06
Iter: 904 loss: 2.39300562e-06
Iter: 905 loss: 2.39286055e-06
Iter: 906 loss: 2.392635e-06
Iter: 907 loss: 2.39267047e-06
Iter: 908 loss: 2.39260839e-06
Iter: 909 loss: 2.39253927e-06
Iter: 910 loss: 2.39247402e-06
Iter: 911 loss: 2.39229553e-06
Iter: 912 loss: 2.39418227e-06
Iter: 913 loss: 2.39227393e-06
Iter: 914 loss: 2.39205701e-06
Iter: 915 loss: 2.39199335e-06
Iter: 916 loss: 2.39186693e-06
Iter: 917 loss: 2.39155293e-06
Iter: 918 loss: 2.39243809e-06
Iter: 919 loss: 2.39143969e-06
Iter: 920 loss: 2.39130623e-06
Iter: 921 loss: 2.39125666e-06
Iter: 922 loss: 2.39107317e-06
Iter: 923 loss: 2.39089422e-06
Iter: 924 loss: 2.39085148e-06
Iter: 925 loss: 2.39066412e-06
Iter: 926 loss: 2.39388191e-06
Iter: 927 loss: 2.39066139e-06
Iter: 928 loss: 2.39051633e-06
Iter: 929 loss: 2.39050678e-06
Iter: 930 loss: 2.39041742e-06
Iter: 931 loss: 2.39021756e-06
Iter: 932 loss: 2.39125848e-06
Iter: 933 loss: 2.39021665e-06
Iter: 934 loss: 2.39005476e-06
Iter: 935 loss: 2.39097949e-06
Iter: 936 loss: 2.39003521e-06
Iter: 937 loss: 2.3898765e-06
Iter: 938 loss: 2.38962707e-06
Iter: 939 loss: 2.38963207e-06
Iter: 940 loss: 2.38948337e-06
Iter: 941 loss: 2.3894886e-06
Iter: 942 loss: 2.38931466e-06
Iter: 943 loss: 2.38901202e-06
Iter: 944 loss: 2.39546148e-06
Iter: 945 loss: 2.38898861e-06
Iter: 946 loss: 2.38870734e-06
Iter: 947 loss: 2.38874281e-06
Iter: 948 loss: 2.38844177e-06
Iter: 949 loss: 2.38810526e-06
Iter: 950 loss: 2.38915209e-06
Iter: 951 loss: 2.38799885e-06
Iter: 952 loss: 2.38784446e-06
Iter: 953 loss: 2.38784878e-06
Iter: 954 loss: 2.38764164e-06
Iter: 955 loss: 2.38754751e-06
Iter: 956 loss: 2.38747407e-06
Iter: 957 loss: 2.38725306e-06
Iter: 958 loss: 2.38929306e-06
Iter: 959 loss: 2.38726034e-06
Iter: 960 loss: 2.38705752e-06
Iter: 961 loss: 2.38705e-06
Iter: 962 loss: 2.38692473e-06
Iter: 963 loss: 2.38668576e-06
Iter: 964 loss: 2.38758685e-06
Iter: 965 loss: 2.38665848e-06
Iter: 966 loss: 2.38648022e-06
Iter: 967 loss: 2.38711732e-06
Iter: 968 loss: 2.38640951e-06
Iter: 969 loss: 2.38621215e-06
Iter: 970 loss: 2.38636721e-06
Iter: 971 loss: 2.38606208e-06
Iter: 972 loss: 2.38585062e-06
Iter: 973 loss: 2.38662642e-06
Iter: 974 loss: 2.38581333e-06
Iter: 975 loss: 2.38557232e-06
Iter: 976 loss: 2.38571511e-06
Iter: 977 loss: 2.38537177e-06
Iter: 978 loss: 2.38517259e-06
Iter: 979 loss: 2.38496568e-06
Iter: 980 loss: 2.38493703e-06
Iter: 981 loss: 2.38461053e-06
Iter: 982 loss: 2.38491293e-06
Iter: 983 loss: 2.38441498e-06
Iter: 984 loss: 2.38415805e-06
Iter: 985 loss: 2.38415168e-06
Iter: 986 loss: 2.38391635e-06
Iter: 987 loss: 2.38588859e-06
Iter: 988 loss: 2.38388793e-06
Iter: 989 loss: 2.38372422e-06
Iter: 990 loss: 2.38371831e-06
Iter: 991 loss: 2.38364e-06
Iter: 992 loss: 2.38332836e-06
Iter: 993 loss: 2.3839284e-06
Iter: 994 loss: 2.38322809e-06
Iter: 995 loss: 2.38302664e-06
Iter: 996 loss: 2.3839782e-06
Iter: 997 loss: 2.38301891e-06
Iter: 998 loss: 2.38284224e-06
Iter: 999 loss: 2.38323537e-06
Iter: 1000 loss: 2.38279927e-06
Iter: 1001 loss: 2.38259145e-06
Iter: 1002 loss: 2.38320376e-06
Iter: 1003 loss: 2.38256689e-06
Iter: 1004 loss: 2.38239636e-06
Iter: 1005 loss: 2.3823427e-06
Iter: 1006 loss: 2.38223083e-06
Iter: 1007 loss: 2.38199755e-06
Iter: 1008 loss: 2.38375469e-06
Iter: 1009 loss: 2.3820005e-06
Iter: 1010 loss: 2.38185476e-06
Iter: 1011 loss: 2.38149528e-06
Iter: 1012 loss: 2.38474877e-06
Iter: 1013 loss: 2.38146731e-06
Iter: 1014 loss: 2.38101893e-06
Iter: 1015 loss: 2.38183429e-06
Iter: 1016 loss: 2.38087318e-06
Iter: 1017 loss: 2.38046e-06
Iter: 1018 loss: 2.38366852e-06
Iter: 1019 loss: 2.38043549e-06
Iter: 1020 loss: 2.38035727e-06
Iter: 1021 loss: 2.38029429e-06
Iter: 1022 loss: 2.38017287e-06
Iter: 1023 loss: 2.38002121e-06
Iter: 1024 loss: 2.38363282e-06
Iter: 1025 loss: 2.38002144e-06
Iter: 1026 loss: 2.37975883e-06
Iter: 1027 loss: 2.38166103e-06
Iter: 1028 loss: 2.3797229e-06
Iter: 1029 loss: 2.37956715e-06
Iter: 1030 loss: 2.37974928e-06
Iter: 1031 loss: 2.37945392e-06
Iter: 1032 loss: 2.37925315e-06
Iter: 1033 loss: 2.37973654e-06
Iter: 1034 loss: 2.37918584e-06
Iter: 1035 loss: 2.37900508e-06
Iter: 1036 loss: 2.38032976e-06
Iter: 1037 loss: 2.37897257e-06
Iter: 1038 loss: 2.37880772e-06
Iter: 1039 loss: 2.37860604e-06
Iter: 1040 loss: 2.37860377e-06
Iter: 1041 loss: 2.37830386e-06
Iter: 1042 loss: 2.38195207e-06
Iter: 1043 loss: 2.37833e-06
Iter: 1044 loss: 2.37812219e-06
Iter: 1045 loss: 2.37772042e-06
Iter: 1046 loss: 2.38269877e-06
Iter: 1047 loss: 2.37765539e-06
Iter: 1048 loss: 2.37726363e-06
Iter: 1049 loss: 2.37777522e-06
Iter: 1050 loss: 2.37704535e-06
Iter: 1051 loss: 2.37656332e-06
Iter: 1052 loss: 2.37849895e-06
Iter: 1053 loss: 2.37645054e-06
Iter: 1054 loss: 2.37656604e-06
Iter: 1055 loss: 2.37632776e-06
Iter: 1056 loss: 2.37621452e-06
Iter: 1057 loss: 2.37593713e-06
Iter: 1058 loss: 2.37905851e-06
Iter: 1059 loss: 2.37592394e-06
Iter: 1060 loss: 2.37570271e-06
Iter: 1061 loss: 2.37569293e-06
Iter: 1062 loss: 2.37549557e-06
Iter: 1063 loss: 2.37529684e-06
Iter: 1064 loss: 2.37525956e-06
Iter: 1065 loss: 2.37495829e-06
Iter: 1066 loss: 2.37683685e-06
Iter: 1067 loss: 2.37492554e-06
Iter: 1068 loss: 2.37474796e-06
Iter: 1069 loss: 2.37597806e-06
Iter: 1070 loss: 2.37472568e-06
Iter: 1071 loss: 2.37458471e-06
Iter: 1072 loss: 2.37440645e-06
Iter: 1073 loss: 2.37438962e-06
Iter: 1074 loss: 2.37418521e-06
Iter: 1075 loss: 2.3768871e-06
Iter: 1076 loss: 2.37419385e-06
Iter: 1077 loss: 2.3739849e-06
Iter: 1078 loss: 2.37361064e-06
Iter: 1079 loss: 2.38016401e-06
Iter: 1080 loss: 2.37356744e-06
Iter: 1081 loss: 2.37318227e-06
Iter: 1082 loss: 2.37335144e-06
Iter: 1083 loss: 2.37290942e-06
Iter: 1084 loss: 2.37247309e-06
Iter: 1085 loss: 2.37355812e-06
Iter: 1086 loss: 2.37230279e-06
Iter: 1087 loss: 2.37222253e-06
Iter: 1088 loss: 2.37211043e-06
Iter: 1089 loss: 2.37195172e-06
Iter: 1090 loss: 2.37216091e-06
Iter: 1091 loss: 2.3718494e-06
Iter: 1092 loss: 2.37174118e-06
Iter: 1093 loss: 2.37245649e-06
Iter: 1094 loss: 2.37173367e-06
Iter: 1095 loss: 2.37159247e-06
Iter: 1096 loss: 2.37134e-06
Iter: 1097 loss: 2.37133781e-06
Iter: 1098 loss: 2.37104e-06
Iter: 1099 loss: 2.3730986e-06
Iter: 1100 loss: 2.3710229e-06
Iter: 1101 loss: 2.37081304e-06
Iter: 1102 loss: 2.3712837e-06
Iter: 1103 loss: 2.37076324e-06
Iter: 1104 loss: 2.37051e-06
Iter: 1105 loss: 2.37096356e-06
Iter: 1106 loss: 2.37040922e-06
Iter: 1107 loss: 2.37024847e-06
Iter: 1108 loss: 2.37122413e-06
Iter: 1109 loss: 2.37023232e-06
Iter: 1110 loss: 2.37000495e-06
Iter: 1111 loss: 2.36983851e-06
Iter: 1112 loss: 2.36977257e-06
Iter: 1113 loss: 2.36950609e-06
Iter: 1114 loss: 2.36929304e-06
Iter: 1115 loss: 2.36920096e-06
Iter: 1116 loss: 2.36880169e-06
Iter: 1117 loss: 2.36924507e-06
Iter: 1118 loss: 2.36859069e-06
Iter: 1119 loss: 2.36825917e-06
Iter: 1120 loss: 2.36823394e-06
Iter: 1121 loss: 2.36794676e-06
Iter: 1122 loss: 2.37082554e-06
Iter: 1123 loss: 2.36794858e-06
Iter: 1124 loss: 2.36781261e-06
Iter: 1125 loss: 2.36779351e-06
Iter: 1126 loss: 2.36772485e-06
Iter: 1127 loss: 2.36745927e-06
Iter: 1128 loss: 2.36792266e-06
Iter: 1129 loss: 2.36735673e-06
Iter: 1130 loss: 2.36715573e-06
Iter: 1131 loss: 2.36809615e-06
Iter: 1132 loss: 2.36708274e-06
Iter: 1133 loss: 2.36689857e-06
Iter: 1134 loss: 2.36700453e-06
Iter: 1135 loss: 2.36676124e-06
Iter: 1136 loss: 2.36649589e-06
Iter: 1137 loss: 2.36827668e-06
Iter: 1138 loss: 2.36649066e-06
Iter: 1139 loss: 2.36631695e-06
Iter: 1140 loss: 2.36659253e-06
Iter: 1141 loss: 2.36626556e-06
Iter: 1142 loss: 2.36605e-06
Iter: 1143 loss: 2.36641699e-06
Iter: 1144 loss: 2.36595838e-06
Iter: 1145 loss: 2.36574579e-06
Iter: 1146 loss: 2.36541609e-06
Iter: 1147 loss: 2.37170843e-06
Iter: 1148 loss: 2.36544042e-06
Iter: 1149 loss: 2.36495907e-06
Iter: 1150 loss: 2.36584947e-06
Iter: 1151 loss: 2.36481083e-06
Iter: 1152 loss: 2.36436154e-06
Iter: 1153 loss: 2.36610276e-06
Iter: 1154 loss: 2.36424194e-06
Iter: 1155 loss: 2.3639368e-06
Iter: 1156 loss: 2.36393885e-06
Iter: 1157 loss: 2.36376718e-06
Iter: 1158 loss: 2.36359028e-06
Iter: 1159 loss: 2.36359642e-06
Iter: 1160 loss: 2.36334949e-06
Iter: 1161 loss: 2.36335427e-06
Iter: 1162 loss: 2.36325081e-06
Iter: 1163 loss: 2.3630555e-06
Iter: 1164 loss: 2.36303799e-06
Iter: 1165 loss: 2.36271717e-06
Iter: 1166 loss: 2.36337678e-06
Iter: 1167 loss: 2.36258734e-06
Iter: 1168 loss: 2.36230085e-06
Iter: 1169 loss: 2.36476717e-06
Iter: 1170 loss: 2.36227197e-06
Iter: 1171 loss: 2.36209507e-06
Iter: 1172 loss: 2.36208234e-06
Iter: 1173 loss: 2.36195888e-06
Iter: 1174 loss: 2.36168898e-06
Iter: 1175 loss: 2.36452775e-06
Iter: 1176 loss: 2.36168898e-06
Iter: 1177 loss: 2.36154756e-06
Iter: 1178 loss: 2.36119718e-06
Iter: 1179 loss: 2.36387541e-06
Iter: 1180 loss: 2.36113328e-06
Iter: 1181 loss: 2.36075039e-06
Iter: 1182 loss: 2.36226083e-06
Iter: 1183 loss: 2.36066398e-06
Iter: 1184 loss: 2.36033748e-06
Iter: 1185 loss: 2.36013807e-06
Iter: 1186 loss: 2.35997413e-06
Iter: 1187 loss: 2.35997891e-06
Iter: 1188 loss: 2.35968878e-06
Iter: 1189 loss: 2.35949915e-06
Iter: 1190 loss: 2.35927428e-06
Iter: 1191 loss: 2.35925881e-06
Iter: 1192 loss: 2.35911193e-06
Iter: 1193 loss: 2.35908738e-06
Iter: 1194 loss: 2.35892094e-06
Iter: 1195 loss: 2.35859488e-06
Iter: 1196 loss: 2.36332698e-06
Iter: 1197 loss: 2.3585585e-06
Iter: 1198 loss: 2.3582129e-06
Iter: 1199 loss: 2.36342521e-06
Iter: 1200 loss: 2.35818675e-06
Iter: 1201 loss: 2.35801599e-06
Iter: 1202 loss: 2.35811285e-06
Iter: 1203 loss: 2.35785092e-06
Iter: 1204 loss: 2.3575758e-06
Iter: 1205 loss: 2.35781795e-06
Iter: 1206 loss: 2.35736979e-06
Iter: 1207 loss: 2.35718812e-06
Iter: 1208 loss: 2.3571572e-06
Iter: 1209 loss: 2.3570019e-06
Iter: 1210 loss: 2.35662537e-06
Iter: 1211 loss: 2.36017672e-06
Iter: 1212 loss: 2.35657626e-06
Iter: 1213 loss: 2.35616244e-06
Iter: 1214 loss: 2.35674861e-06
Iter: 1215 loss: 2.35596508e-06
Iter: 1216 loss: 2.35558e-06
Iter: 1217 loss: 2.35682228e-06
Iter: 1218 loss: 2.35548805e-06
Iter: 1219 loss: 2.35514653e-06
Iter: 1220 loss: 2.35927655e-06
Iter: 1221 loss: 2.355172e-06
Iter: 1222 loss: 2.35481457e-06
Iter: 1223 loss: 2.35584184e-06
Iter: 1224 loss: 2.35472953e-06
Iter: 1225 loss: 2.35461061e-06
Iter: 1226 loss: 2.35527273e-06
Iter: 1227 loss: 2.35461266e-06
Iter: 1228 loss: 2.35444486e-06
Iter: 1229 loss: 2.35458674e-06
Iter: 1230 loss: 2.35438438e-06
Iter: 1231 loss: 2.35423659e-06
Iter: 1232 loss: 2.35483617e-06
Iter: 1233 loss: 2.3542143e-06
Iter: 1234 loss: 2.35407151e-06
Iter: 1235 loss: 2.3539385e-06
Iter: 1236 loss: 2.35391144e-06
Iter: 1237 loss: 2.35364769e-06
Iter: 1238 loss: 2.35505331e-06
Iter: 1239 loss: 2.3535938e-06
Iter: 1240 loss: 2.3534235e-06
Iter: 1241 loss: 2.35433413e-06
Iter: 1242 loss: 2.35339053e-06
Iter: 1243 loss: 2.35319226e-06
Iter: 1244 loss: 2.3529135e-06
Iter: 1245 loss: 2.35288348e-06
Iter: 1246 loss: 2.35258176e-06
Iter: 1247 loss: 2.35257676e-06
Iter: 1248 loss: 2.35234666e-06
Iter: 1249 loss: 2.35208358e-06
Iter: 1250 loss: 2.3534194e-06
Iter: 1251 loss: 2.35204675e-06
Iter: 1252 loss: 2.35183779e-06
Iter: 1253 loss: 2.35198786e-06
Iter: 1254 loss: 2.35167386e-06
Iter: 1255 loss: 2.35132688e-06
Iter: 1256 loss: 2.35134075e-06
Iter: 1257 loss: 2.35115203e-06
Iter: 1258 loss: 2.35079892e-06
Iter: 1259 loss: 2.35080779e-06
Iter: 1260 loss: 2.35041807e-06
Iter: 1261 loss: 2.35041512e-06
Iter: 1262 loss: 2.35028187e-06
Iter: 1263 loss: 2.35009657e-06
Iter: 1264 loss: 2.35009566e-06
Iter: 1265 loss: 2.34981599e-06
Iter: 1266 loss: 2.35214588e-06
Iter: 1267 loss: 2.3497696e-06
Iter: 1268 loss: 2.34967365e-06
Iter: 1269 loss: 2.34978916e-06
Iter: 1270 loss: 2.3495918e-06
Iter: 1271 loss: 2.34938625e-06
Iter: 1272 loss: 2.34957474e-06
Iter: 1273 loss: 2.34928e-06
Iter: 1274 loss: 2.34914182e-06
Iter: 1275 loss: 2.34913159e-06
Iter: 1276 loss: 2.34898926e-06
Iter: 1277 loss: 2.34861318e-06
Iter: 1278 loss: 2.34906815e-06
Iter: 1279 loss: 2.34833988e-06
Iter: 1280 loss: 2.3478151e-06
Iter: 1281 loss: 2.35347989e-06
Iter: 1282 loss: 2.34780327e-06
Iter: 1283 loss: 2.34753475e-06
Iter: 1284 loss: 2.3484331e-06
Iter: 1285 loss: 2.34741901e-06
Iter: 1286 loss: 2.34735717e-06
Iter: 1287 loss: 2.34734262e-06
Iter: 1288 loss: 2.34721301e-06
Iter: 1289 loss: 2.34696063e-06
Iter: 1290 loss: 2.35211792e-06
Iter: 1291 loss: 2.34694789e-06
Iter: 1292 loss: 2.34679669e-06
Iter: 1293 loss: 2.34677168e-06
Iter: 1294 loss: 2.34667e-06
Iter: 1295 loss: 2.34645427e-06
Iter: 1296 loss: 2.35171819e-06
Iter: 1297 loss: 2.34643562e-06
Iter: 1298 loss: 2.34617846e-06
Iter: 1299 loss: 2.34888603e-06
Iter: 1300 loss: 2.346168e-06
Iter: 1301 loss: 2.34597519e-06
Iter: 1302 loss: 2.34588015e-06
Iter: 1303 loss: 2.34578238e-06
Iter: 1304 loss: 2.34550907e-06
Iter: 1305 loss: 2.34635036e-06
Iter: 1306 loss: 2.34541267e-06
Iter: 1307 loss: 2.3453108e-06
Iter: 1308 loss: 2.34528579e-06
Iter: 1309 loss: 2.34517938e-06
Iter: 1310 loss: 2.34489039e-06
Iter: 1311 loss: 2.34619165e-06
Iter: 1312 loss: 2.34474101e-06
Iter: 1313 loss: 2.3443813e-06
Iter: 1314 loss: 2.34474032e-06
Iter: 1315 loss: 2.34417439e-06
Iter: 1316 loss: 2.34373988e-06
Iter: 1317 loss: 2.34634535e-06
Iter: 1318 loss: 2.34371646e-06
Iter: 1319 loss: 2.34354889e-06
Iter: 1320 loss: 2.34353229e-06
Iter: 1321 loss: 2.34332083e-06
Iter: 1322 loss: 2.34336858e-06
Iter: 1323 loss: 2.34316917e-06
Iter: 1324 loss: 2.34298068e-06
Iter: 1325 loss: 2.34428899e-06
Iter: 1326 loss: 2.34298614e-06
Iter: 1327 loss: 2.34278059e-06
Iter: 1328 loss: 2.34271511e-06
Iter: 1329 loss: 2.34258277e-06
Iter: 1330 loss: 2.34236268e-06
Iter: 1331 loss: 2.34326944e-06
Iter: 1332 loss: 2.34230356e-06
Iter: 1333 loss: 2.34196318e-06
Iter: 1334 loss: 2.34173876e-06
Iter: 1335 loss: 2.34164099e-06
Iter: 1336 loss: 2.34120807e-06
Iter: 1337 loss: 2.3442135e-06
Iter: 1338 loss: 2.34116601e-06
Iter: 1339 loss: 2.34096842e-06
Iter: 1340 loss: 2.34185177e-06
Iter: 1341 loss: 2.34092477e-06
Iter: 1342 loss: 2.34070467e-06
Iter: 1343 loss: 2.34113668e-06
Iter: 1344 loss: 2.34059439e-06
Iter: 1345 loss: 2.34039567e-06
Iter: 1346 loss: 2.33996047e-06
Iter: 1347 loss: 2.34368736e-06
Iter: 1348 loss: 2.33988749e-06
Iter: 1349 loss: 2.33924789e-06
Iter: 1350 loss: 2.34082017e-06
Iter: 1351 loss: 2.33903802e-06
Iter: 1352 loss: 2.33845321e-06
Iter: 1353 loss: 2.34237496e-06
Iter: 1354 loss: 2.33841274e-06
Iter: 1355 loss: 2.33827586e-06
Iter: 1356 loss: 2.33814444e-06
Iter: 1357 loss: 2.33802166e-06
Iter: 1358 loss: 2.33794708e-06
Iter: 1359 loss: 2.33790161e-06
Iter: 1360 loss: 2.33774836e-06
Iter: 1361 loss: 2.34002982e-06
Iter: 1362 loss: 2.3377404e-06
Iter: 1363 loss: 2.33764831e-06
Iter: 1364 loss: 2.33753e-06
Iter: 1365 loss: 2.3375153e-06
Iter: 1366 loss: 2.33725245e-06
Iter: 1367 loss: 2.33744709e-06
Iter: 1368 loss: 2.33707465e-06
Iter: 1369 loss: 2.33676974e-06
Iter: 1370 loss: 2.33675337e-06
Iter: 1371 loss: 2.33652918e-06
Iter: 1372 loss: 2.33609398e-06
Iter: 1373 loss: 2.33768833e-06
Iter: 1374 loss: 2.33597166e-06
Iter: 1375 loss: 2.33584069e-06
Iter: 1376 loss: 2.33581977e-06
Iter: 1377 loss: 2.33568494e-06
Iter: 1378 loss: 2.33538299e-06
Iter: 1379 loss: 2.34051072e-06
Iter: 1380 loss: 2.33536502e-06
Iter: 1381 loss: 2.33508376e-06
Iter: 1382 loss: 2.33500464e-06
Iter: 1383 loss: 2.33481319e-06
Iter: 1384 loss: 2.33441665e-06
Iter: 1385 loss: 2.33441392e-06
Iter: 1386 loss: 2.33407718e-06
Iter: 1387 loss: 2.33335527e-06
Iter: 1388 loss: 2.3367902e-06
Iter: 1389 loss: 2.33322544e-06
Iter: 1390 loss: 2.33316132e-06
Iter: 1391 loss: 2.33305173e-06
Iter: 1392 loss: 2.33286528e-06
Iter: 1393 loss: 2.3327766e-06
Iter: 1394 loss: 2.33269566e-06
Iter: 1395 loss: 2.33248693e-06
Iter: 1396 loss: 2.33239712e-06
Iter: 1397 loss: 2.33226e-06
Iter: 1398 loss: 2.33188166e-06
Iter: 1399 loss: 2.3355251e-06
Iter: 1400 loss: 2.3318787e-06
Iter: 1401 loss: 2.33162405e-06
Iter: 1402 loss: 2.33129731e-06
Iter: 1403 loss: 2.33128685e-06
Iter: 1404 loss: 2.33089895e-06
Iter: 1405 loss: 2.33605351e-06
Iter: 1406 loss: 2.33090782e-06
Iter: 1407 loss: 2.33065111e-06
Iter: 1408 loss: 2.33115838e-06
Iter: 1409 loss: 2.33053083e-06
Iter: 1410 loss: 2.33020501e-06
Iter: 1411 loss: 2.33022683e-06
Iter: 1412 loss: 2.32992807e-06
Iter: 1413 loss: 2.32967386e-06
Iter: 1414 loss: 2.3296725e-06
Iter: 1415 loss: 2.32952198e-06
Iter: 1416 loss: 2.32913317e-06
Iter: 1417 loss: 2.33296942e-06
Iter: 1418 loss: 2.32909315e-06
Iter: 1419 loss: 2.32859179e-06
Iter: 1420 loss: 2.32863249e-06
Iter: 1421 loss: 2.32819525e-06
Iter: 1422 loss: 2.32760681e-06
Iter: 1423 loss: 2.33040737e-06
Iter: 1424 loss: 2.32745037e-06
Iter: 1425 loss: 2.32737216e-06
Iter: 1426 loss: 2.32720095e-06
Iter: 1427 loss: 2.32695584e-06
Iter: 1428 loss: 2.32656635e-06
Iter: 1429 loss: 2.32657885e-06
Iter: 1430 loss: 2.32625166e-06
Iter: 1431 loss: 2.32724074e-06
Iter: 1432 loss: 2.32616048e-06
Iter: 1433 loss: 2.32576258e-06
Iter: 1434 loss: 2.32820139e-06
Iter: 1435 loss: 2.32569505e-06
Iter: 1436 loss: 2.32550519e-06
Iter: 1437 loss: 2.32540515e-06
Iter: 1438 loss: 2.32531784e-06
Iter: 1439 loss: 2.32496836e-06
Iter: 1440 loss: 2.32686716e-06
Iter: 1441 loss: 2.32492107e-06
Iter: 1442 loss: 2.32464527e-06
Iter: 1443 loss: 2.3251996e-06
Iter: 1444 loss: 2.32454363e-06
Iter: 1445 loss: 2.32422872e-06
Iter: 1446 loss: 2.32493858e-06
Iter: 1447 loss: 2.32408161e-06
Iter: 1448 loss: 2.32380535e-06
Iter: 1449 loss: 2.32618231e-06
Iter: 1450 loss: 2.32379512e-06
Iter: 1451 loss: 2.32362163e-06
Iter: 1452 loss: 2.32318257e-06
Iter: 1453 loss: 2.32535081e-06
Iter: 1454 loss: 2.32299794e-06
Iter: 1455 loss: 2.32242382e-06
Iter: 1456 loss: 2.32592e-06
Iter: 1457 loss: 2.3223397e-06
Iter: 1458 loss: 2.32185539e-06
Iter: 1459 loss: 2.32224897e-06
Iter: 1460 loss: 2.32157981e-06
Iter: 1461 loss: 2.32163416e-06
Iter: 1462 loss: 2.32126376e-06
Iter: 1463 loss: 2.32112734e-06
Iter: 1464 loss: 2.32067487e-06
Iter: 1465 loss: 2.32483762e-06
Iter: 1466 loss: 2.32062712e-06
Iter: 1467 loss: 2.32034563e-06
Iter: 1468 loss: 2.32336515e-06
Iter: 1469 loss: 2.32033813e-06
Iter: 1470 loss: 2.32000639e-06
Iter: 1471 loss: 2.32106231e-06
Iter: 1472 loss: 2.31991157e-06
Iter: 1473 loss: 2.31971194e-06
Iter: 1474 loss: 2.319651e-06
Iter: 1475 loss: 2.31956437e-06
Iter: 1476 loss: 2.31916e-06
Iter: 1477 loss: 2.32069942e-06
Iter: 1478 loss: 2.31906961e-06
Iter: 1479 loss: 2.31882814e-06
Iter: 1480 loss: 2.31988793e-06
Iter: 1481 loss: 2.31877902e-06
Iter: 1482 loss: 2.31854096e-06
Iter: 1483 loss: 2.31895092e-06
Iter: 1484 loss: 2.31841454e-06
Iter: 1485 loss: 2.31808326e-06
Iter: 1486 loss: 2.31859235e-06
Iter: 1487 loss: 2.31792092e-06
Iter: 1488 loss: 2.31762397e-06
Iter: 1489 loss: 2.31746799e-06
Iter: 1490 loss: 2.31733316e-06
Iter: 1491 loss: 2.31702757e-06
Iter: 1492 loss: 2.31691388e-06
Iter: 1493 loss: 2.3167172e-06
Iter: 1494 loss: 2.31617241e-06
Iter: 1495 loss: 2.31911508e-06
Iter: 1496 loss: 2.31607851e-06
Iter: 1497 loss: 2.31597824e-06
Iter: 1498 loss: 2.3158434e-06
Iter: 1499 loss: 2.31571494e-06
Iter: 1500 loss: 2.31532613e-06
Iter: 1501 loss: 2.31672539e-06
Iter: 1502 loss: 2.31519221e-06
Iter: 1503 loss: 2.31494505e-06
Iter: 1504 loss: 2.31493232e-06
Iter: 1505 loss: 2.31462673e-06
Iter: 1506 loss: 2.31491686e-06
Iter: 1507 loss: 2.31447325e-06
Iter: 1508 loss: 2.31425838e-06
Iter: 1509 loss: 2.31431386e-06
Iter: 1510 loss: 2.314109e-06
Iter: 1511 loss: 2.31379977e-06
Iter: 1512 loss: 2.3156374e-06
Iter: 1513 loss: 2.31374588e-06
Iter: 1514 loss: 2.3135226e-06
Iter: 1515 loss: 2.31337572e-06
Iter: 1516 loss: 2.31330796e-06
Iter: 1517 loss: 2.3128996e-06
Iter: 1518 loss: 2.31557169e-06
Iter: 1519 loss: 2.31283548e-06
Iter: 1520 loss: 2.31256945e-06
Iter: 1521 loss: 2.31371268e-06
Iter: 1522 loss: 2.31254262e-06
Iter: 1523 loss: 2.31233957e-06
Iter: 1524 loss: 2.31220884e-06
Iter: 1525 loss: 2.31212948e-06
Iter: 1526 loss: 2.31187187e-06
Iter: 1527 loss: 2.31150261e-06
Iter: 1528 loss: 2.31145327e-06
Iter: 1529 loss: 2.31096806e-06
Iter: 1530 loss: 2.31296099e-06
Iter: 1531 loss: 2.31084891e-06
Iter: 1532 loss: 2.31082231e-06
Iter: 1533 loss: 2.3106245e-06
Iter: 1534 loss: 2.310421e-06
Iter: 1535 loss: 2.30997557e-06
Iter: 1536 loss: 2.31685317e-06
Iter: 1537 loss: 2.30997375e-06
Iter: 1538 loss: 2.30971455e-06
Iter: 1539 loss: 2.31332751e-06
Iter: 1540 loss: 2.30972e-06
Iter: 1541 loss: 2.30942533e-06
Iter: 1542 loss: 2.30958221e-06
Iter: 1543 loss: 2.30921933e-06
Iter: 1544 loss: 2.3090397e-06
Iter: 1545 loss: 2.30896785e-06
Iter: 1546 loss: 2.30885826e-06
Iter: 1547 loss: 2.30852584e-06
Iter: 1548 loss: 2.31109948e-06
Iter: 1549 loss: 2.30849218e-06
Iter: 1550 loss: 2.30827118e-06
Iter: 1551 loss: 2.30777096e-06
Iter: 1552 loss: 2.31615286e-06
Iter: 1553 loss: 2.30776868e-06
Iter: 1554 loss: 2.30765272e-06
Iter: 1555 loss: 2.30750311e-06
Iter: 1556 loss: 2.30731234e-06
Iter: 1557 loss: 2.30702062e-06
Iter: 1558 loss: 2.30699584e-06
Iter: 1559 loss: 2.30651176e-06
Iter: 1560 loss: 2.30740216e-06
Iter: 1561 loss: 2.30630485e-06
Iter: 1562 loss: 2.30600108e-06
Iter: 1563 loss: 2.30642627e-06
Iter: 1564 loss: 2.30586647e-06
Iter: 1565 loss: 2.30564342e-06
Iter: 1566 loss: 2.30866794e-06
Iter: 1567 loss: 2.30564456e-06
Iter: 1568 loss: 2.30539808e-06
Iter: 1569 loss: 2.30519299e-06
Iter: 1570 loss: 2.3051075e-06
Iter: 1571 loss: 2.30474188e-06
Iter: 1572 loss: 2.30449041e-06
Iter: 1573 loss: 2.30437922e-06
Iter: 1574 loss: 2.30443152e-06
Iter: 1575 loss: 2.30415958e-06
Iter: 1576 loss: 2.30393516e-06
Iter: 1577 loss: 2.30339447e-06
Iter: 1578 loss: 2.31018248e-06
Iter: 1579 loss: 2.30333876e-06
Iter: 1580 loss: 2.3032826e-06
Iter: 1581 loss: 2.30317801e-06
Iter: 1582 loss: 2.30303021e-06
Iter: 1583 loss: 2.30269734e-06
Iter: 1584 loss: 2.30669457e-06
Iter: 1585 loss: 2.30266619e-06
Iter: 1586 loss: 2.30251771e-06
Iter: 1587 loss: 2.30249566e-06
Iter: 1588 loss: 2.3023274e-06
Iter: 1589 loss: 2.30213345e-06
Iter: 1590 loss: 2.30209639e-06
Iter: 1591 loss: 2.30176283e-06
Iter: 1592 loss: 2.30321825e-06
Iter: 1593 loss: 2.30172554e-06
Iter: 1594 loss: 2.30143405e-06
Iter: 1595 loss: 2.3011446e-06
Iter: 1596 loss: 2.30110754e-06
Iter: 1597 loss: 2.30088972e-06
Iter: 1598 loss: 2.30091086e-06
Iter: 1599 loss: 2.30069213e-06
Iter: 1600 loss: 2.30098522e-06
Iter: 1601 loss: 2.30064279e-06
Iter: 1602 loss: 2.30047704e-06
Iter: 1603 loss: 2.30029787e-06
Iter: 1604 loss: 2.30024443e-06
Iter: 1605 loss: 2.30008095e-06
Iter: 1606 loss: 2.30113619e-06
Iter: 1607 loss: 2.30006231e-06
Iter: 1608 loss: 2.29983834e-06
Iter: 1609 loss: 2.29990496e-06
Iter: 1610 loss: 2.29971079e-06
Iter: 1611 loss: 2.29944681e-06
Iter: 1612 loss: 2.29904072e-06
Iter: 1613 loss: 2.29902616e-06
Iter: 1614 loss: 2.29936904e-06
Iter: 1615 loss: 2.29884927e-06
Iter: 1616 loss: 2.29877242e-06
Iter: 1617 loss: 2.29852594e-06
Iter: 1618 loss: 2.30072646e-06
Iter: 1619 loss: 2.29849047e-06
Iter: 1620 loss: 2.29817988e-06
Iter: 1621 loss: 2.29805096e-06
Iter: 1622 loss: 2.29787156e-06
Iter: 1623 loss: 2.29759189e-06
Iter: 1624 loss: 2.30207797e-06
Iter: 1625 loss: 2.29757916e-06
Iter: 1626 loss: 2.29721854e-06
Iter: 1627 loss: 2.29810416e-06
Iter: 1628 loss: 2.29710622e-06
Iter: 1629 loss: 2.29685338e-06
Iter: 1630 loss: 2.29782063e-06
Iter: 1631 loss: 2.29678835e-06
Iter: 1632 loss: 2.29656484e-06
Iter: 1633 loss: 2.29635361e-06
Iter: 1634 loss: 2.29631928e-06
Iter: 1635 loss: 2.29627858e-06
Iter: 1636 loss: 2.29617672e-06
Iter: 1637 loss: 2.29609077e-06
Iter: 1638 loss: 2.29580587e-06
Iter: 1639 loss: 2.29919988e-06
Iter: 1640 loss: 2.29578268e-06
Iter: 1641 loss: 2.29542502e-06
Iter: 1642 loss: 2.29663237e-06
Iter: 1643 loss: 2.29535044e-06
Iter: 1644 loss: 2.29499597e-06
Iter: 1645 loss: 2.29779653e-06
Iter: 1646 loss: 2.29497027e-06
Iter: 1647 loss: 2.29474517e-06
Iter: 1648 loss: 2.29438024e-06
Iter: 1649 loss: 2.30263299e-06
Iter: 1650 loss: 2.29435545e-06
Iter: 1651 loss: 2.29405805e-06
Iter: 1652 loss: 2.29456873e-06
Iter: 1653 loss: 2.29388479e-06
Iter: 1654 loss: 2.29399757e-06
Iter: 1655 loss: 2.29379e-06
Iter: 1656 loss: 2.29367038e-06
Iter: 1657 loss: 2.2934255e-06
Iter: 1658 loss: 2.29629745e-06
Iter: 1659 loss: 2.29341958e-06
Iter: 1660 loss: 2.29318721e-06
Iter: 1661 loss: 2.29443776e-06
Iter: 1662 loss: 2.29312332e-06
Iter: 1663 loss: 2.29284933e-06
Iter: 1664 loss: 2.29412763e-06
Iter: 1665 loss: 2.29279112e-06
Iter: 1666 loss: 2.29258603e-06
Iter: 1667 loss: 2.29251123e-06
Iter: 1668 loss: 2.29239595e-06
Iter: 1669 loss: 2.29206876e-06
Iter: 1670 loss: 2.29319835e-06
Iter: 1671 loss: 2.2919553e-06
Iter: 1672 loss: 2.29181524e-06
Iter: 1673 loss: 2.2918307e-06
Iter: 1674 loss: 2.29169609e-06
Iter: 1675 loss: 2.2914794e-06
Iter: 1676 loss: 2.29148282e-06
Iter: 1677 loss: 2.29124271e-06
Iter: 1678 loss: 2.29177886e-06
Iter: 1679 loss: 2.29115858e-06
Iter: 1680 loss: 2.29091415e-06
Iter: 1681 loss: 2.29371653e-06
Iter: 1682 loss: 2.29093212e-06
Iter: 1683 loss: 2.29081979e-06
Iter: 1684 loss: 2.29051943e-06
Iter: 1685 loss: 2.29134503e-06
Iter: 1686 loss: 2.29038187e-06
Iter: 1687 loss: 2.29008447e-06
Iter: 1688 loss: 2.2900665e-06
Iter: 1689 loss: 2.28988529e-06
Iter: 1690 loss: 2.29259331e-06
Iter: 1691 loss: 2.28988483e-06
Iter: 1692 loss: 2.28978865e-06
Iter: 1693 loss: 2.2895797e-06
Iter: 1694 loss: 2.29258399e-06
Iter: 1695 loss: 2.28957742e-06
Iter: 1696 loss: 2.28944532e-06
Iter: 1697 loss: 2.2894219e-06
Iter: 1698 loss: 2.28928866e-06
Iter: 1699 loss: 2.28901104e-06
Iter: 1700 loss: 2.29404395e-06
Iter: 1701 loss: 2.28898875e-06
Iter: 1702 loss: 2.28872182e-06
Iter: 1703 loss: 2.29099078e-06
Iter: 1704 loss: 2.28871659e-06
Iter: 1705 loss: 2.28855106e-06
Iter: 1706 loss: 2.28903946e-06
Iter: 1707 loss: 2.28848717e-06
Iter: 1708 loss: 2.28834097e-06
Iter: 1709 loss: 2.28982094e-06
Iter: 1710 loss: 2.28830845e-06
Iter: 1711 loss: 2.28817635e-06
Iter: 1712 loss: 2.28787803e-06
Iter: 1713 loss: 2.29058401e-06
Iter: 1714 loss: 2.28780436e-06
Iter: 1715 loss: 2.2878985e-06
Iter: 1716 loss: 2.28771091e-06
Iter: 1717 loss: 2.28758449e-06
Iter: 1718 loss: 2.28731187e-06
Iter: 1719 loss: 2.28890531e-06
Iter: 1720 loss: 2.28722047e-06
Iter: 1721 loss: 2.28672343e-06
Iter: 1722 loss: 2.28997897e-06
Iter: 1723 loss: 2.28665635e-06
Iter: 1724 loss: 2.28633371e-06
Iter: 1725 loss: 2.28632098e-06
Iter: 1726 loss: 2.2861168e-06
Iter: 1727 loss: 2.2862464e-06
Iter: 1728 loss: 2.28597128e-06
Iter: 1729 loss: 2.28579233e-06
Iter: 1730 loss: 2.28600379e-06
Iter: 1731 loss: 2.28572162e-06
Iter: 1732 loss: 2.28546105e-06
Iter: 1733 loss: 2.28643148e-06
Iter: 1734 loss: 2.28539147e-06
Iter: 1735 loss: 2.28521549e-06
Iter: 1736 loss: 2.28497947e-06
Iter: 1737 loss: 2.28495423e-06
Iter: 1738 loss: 2.28459635e-06
Iter: 1739 loss: 2.28612248e-06
Iter: 1740 loss: 2.28452e-06
Iter: 1741 loss: 2.28418276e-06
Iter: 1742 loss: 2.28495355e-06
Iter: 1743 loss: 2.28408271e-06
Iter: 1744 loss: 2.28367048e-06
Iter: 1745 loss: 2.284321e-06
Iter: 1746 loss: 2.28347881e-06
Iter: 1747 loss: 2.28324961e-06
Iter: 1748 loss: 2.28429144e-06
Iter: 1749 loss: 2.28322438e-06
Iter: 1750 loss: 2.28300951e-06
Iter: 1751 loss: 2.28405361e-06
Iter: 1752 loss: 2.28295767e-06
Iter: 1753 loss: 2.28283e-06
Iter: 1754 loss: 2.28249746e-06
Iter: 1755 loss: 2.28455519e-06
Iter: 1756 loss: 2.28242084e-06
Iter: 1757 loss: 2.28202089e-06
Iter: 1758 loss: 2.28565523e-06
Iter: 1759 loss: 2.28202953e-06
Iter: 1760 loss: 2.28153203e-06
Iter: 1761 loss: 2.28303793e-06
Iter: 1762 loss: 2.28138151e-06
Iter: 1763 loss: 2.28112026e-06
Iter: 1764 loss: 2.2813947e-06
Iter: 1765 loss: 2.28095359e-06
Iter: 1766 loss: 2.28076829e-06
Iter: 1767 loss: 2.28077124e-06
Iter: 1768 loss: 2.28060935e-06
Iter: 1769 loss: 2.28027079e-06
Iter: 1770 loss: 2.28548788e-06
Iter: 1771 loss: 2.28025397e-06
Iter: 1772 loss: 2.2800491e-06
Iter: 1773 loss: 2.28330896e-06
Iter: 1774 loss: 2.28003341e-06
Iter: 1775 loss: 2.27982719e-06
Iter: 1776 loss: 2.27987653e-06
Iter: 1777 loss: 2.27965802e-06
Iter: 1778 loss: 2.27940836e-06
Iter: 1779 loss: 2.28164572e-06
Iter: 1780 loss: 2.27938745e-06
Iter: 1781 loss: 2.27916144e-06
Iter: 1782 loss: 2.27864484e-06
Iter: 1783 loss: 2.28593262e-06
Iter: 1784 loss: 2.27861233e-06
Iter: 1785 loss: 2.27869532e-06
Iter: 1786 loss: 2.27842474e-06
Iter: 1787 loss: 2.27824967e-06
Iter: 1788 loss: 2.27787859e-06
Iter: 1789 loss: 2.28259182e-06
Iter: 1790 loss: 2.27784017e-06
Iter: 1791 loss: 2.27757982e-06
Iter: 1792 loss: 2.27768896e-06
Iter: 1793 loss: 2.2774002e-06
Iter: 1794 loss: 2.27718783e-06
Iter: 1795 loss: 2.27719102e-06
Iter: 1796 loss: 2.27688133e-06
Iter: 1797 loss: 2.27716e-06
Iter: 1798 loss: 2.27671126e-06
Iter: 1799 loss: 2.27658165e-06
Iter: 1800 loss: 2.27728674e-06
Iter: 1801 loss: 2.27651481e-06
Iter: 1802 loss: 2.27624264e-06
Iter: 1803 loss: 2.27611372e-06
Iter: 1804 loss: 2.27597502e-06
Iter: 1805 loss: 2.27564851e-06
Iter: 1806 loss: 2.27642249e-06
Iter: 1807 loss: 2.27556666e-06
Iter: 1808 loss: 2.27535293e-06
Iter: 1809 loss: 2.27533519e-06
Iter: 1810 loss: 2.27522173e-06
Iter: 1811 loss: 2.27528199e-06
Iter: 1812 loss: 2.27513328e-06
Iter: 1813 loss: 2.27491137e-06
Iter: 1814 loss: 2.27491137e-06
Iter: 1815 loss: 2.27471537e-06
Iter: 1816 loss: 2.27446458e-06
Iter: 1817 loss: 2.27469354e-06
Iter: 1818 loss: 2.27433884e-06
Iter: 1819 loss: 2.27408509e-06
Iter: 1820 loss: 2.27407236e-06
Iter: 1821 loss: 2.27396595e-06
Iter: 1822 loss: 2.27365513e-06
Iter: 1823 loss: 2.2756617e-06
Iter: 1824 loss: 2.27358851e-06
Iter: 1825 loss: 2.27322062e-06
Iter: 1826 loss: 2.27373221e-06
Iter: 1827 loss: 2.27307078e-06
Iter: 1828 loss: 2.27309374e-06
Iter: 1829 loss: 2.27290434e-06
Iter: 1830 loss: 2.27272221e-06
Iter: 1831 loss: 2.27242253e-06
Iter: 1832 loss: 2.27892701e-06
Iter: 1833 loss: 2.27241344e-06
Iter: 1834 loss: 2.27212922e-06
Iter: 1835 loss: 2.27637292e-06
Iter: 1836 loss: 2.27211194e-06
Iter: 1837 loss: 2.27182818e-06
Iter: 1838 loss: 2.27134615e-06
Iter: 1839 loss: 2.27134615e-06
Iter: 1840 loss: 2.27112787e-06
Iter: 1841 loss: 2.27330111e-06
Iter: 1842 loss: 2.2711024e-06
Iter: 1843 loss: 2.27092642e-06
Iter: 1844 loss: 2.27210603e-06
Iter: 1845 loss: 2.27092505e-06
Iter: 1846 loss: 2.27073633e-06
Iter: 1847 loss: 2.27082705e-06
Iter: 1848 loss: 2.27063197e-06
Iter: 1849 loss: 2.27038527e-06
Iter: 1850 loss: 2.2702809e-06
Iter: 1851 loss: 2.2701754e-06
Iter: 1852 loss: 2.26988368e-06
Iter: 1853 loss: 2.27134933e-06
Iter: 1854 loss: 2.26984594e-06
Iter: 1855 loss: 2.2694071e-06
Iter: 1856 loss: 2.26963766e-06
Iter: 1857 loss: 2.26918246e-06
Iter: 1858 loss: 2.26891234e-06
Iter: 1859 loss: 2.26862176e-06
Iter: 1860 loss: 2.26862562e-06
Iter: 1861 loss: 2.26823067e-06
Iter: 1862 loss: 2.26946077e-06
Iter: 1863 loss: 2.26815746e-06
Iter: 1864 loss: 2.26805741e-06
Iter: 1865 loss: 2.26800512e-06
Iter: 1866 loss: 2.26781322e-06
Iter: 1867 loss: 2.26730663e-06
Iter: 1868 loss: 2.26987822e-06
Iter: 1869 loss: 2.2671486e-06
Iter: 1870 loss: 2.26710745e-06
Iter: 1871 loss: 2.26687553e-06
Iter: 1872 loss: 2.26666225e-06
Iter: 1873 loss: 2.2662889e-06
Iter: 1874 loss: 2.27523651e-06
Iter: 1875 loss: 2.2662864e-06
Iter: 1876 loss: 2.26602742e-06
Iter: 1877 loss: 2.26859265e-06
Iter: 1878 loss: 2.2660131e-06
Iter: 1879 loss: 2.26576731e-06
Iter: 1880 loss: 2.2669833e-06
Iter: 1881 loss: 2.26569978e-06
Iter: 1882 loss: 2.26557881e-06
Iter: 1883 loss: 2.26578913e-06
Iter: 1884 loss: 2.26551765e-06
Iter: 1885 loss: 2.2653719e-06
Iter: 1886 loss: 2.26501743e-06
Iter: 1887 loss: 2.27103374e-06
Iter: 1888 loss: 2.26500379e-06
Iter: 1889 loss: 2.26457e-06
Iter: 1890 loss: 2.26624934e-06
Iter: 1891 loss: 2.26447446e-06
Iter: 1892 loss: 2.263939e-06
Iter: 1893 loss: 2.26736302e-06
Iter: 1894 loss: 2.26386919e-06
Iter: 1895 loss: 2.26369525e-06
Iter: 1896 loss: 2.26348561e-06
Iter: 1897 loss: 2.26347106e-06
Iter: 1898 loss: 2.26317229e-06
Iter: 1899 loss: 2.26352017e-06
Iter: 1900 loss: 2.26303359e-06
Iter: 1901 loss: 2.26264638e-06
Iter: 1902 loss: 2.26753946e-06
Iter: 1903 loss: 2.26262796e-06
Iter: 1904 loss: 2.26248676e-06
Iter: 1905 loss: 2.26204907e-06
Iter: 1906 loss: 2.26563202e-06
Iter: 1907 loss: 2.2619663e-06
Iter: 1908 loss: 2.26172892e-06
Iter: 1909 loss: 2.26162183e-06
Iter: 1910 loss: 2.2614613e-06
Iter: 1911 loss: 2.26101747e-06
Iter: 1912 loss: 2.26394923e-06
Iter: 1913 loss: 2.26089833e-06
Iter: 1914 loss: 2.26112047e-06
Iter: 1915 loss: 2.26073553e-06
Iter: 1916 loss: 2.26060979e-06
Iter: 1917 loss: 2.26045768e-06
Iter: 1918 loss: 2.2604695e-06
Iter: 1919 loss: 2.26023462e-06
Iter: 1920 loss: 2.2606323e-06
Iter: 1921 loss: 2.26012594e-06
Iter: 1922 loss: 2.25980193e-06
Iter: 1923 loss: 2.25952726e-06
Iter: 1924 loss: 2.25944e-06
Iter: 1925 loss: 2.25914755e-06
Iter: 1926 loss: 2.25915e-06
Iter: 1927 loss: 2.25883832e-06
Iter: 1928 loss: 2.25958911e-06
Iter: 1929 loss: 2.25869917e-06
Iter: 1930 loss: 2.25851682e-06
Iter: 1931 loss: 2.25825966e-06
Iter: 1932 loss: 2.26502698e-06
Iter: 1933 loss: 2.25826852e-06
Iter: 1934 loss: 2.25828376e-06
Iter: 1935 loss: 2.25812823e-06
Iter: 1936 loss: 2.25804274e-06
Iter: 1937 loss: 2.25775057e-06
Iter: 1938 loss: 2.25962231e-06
Iter: 1939 loss: 2.257706e-06
Iter: 1940 loss: 2.25744043e-06
Iter: 1941 loss: 2.26087695e-06
Iter: 1942 loss: 2.25742815e-06
Iter: 1943 loss: 2.25706049e-06
Iter: 1944 loss: 2.25652093e-06
Iter: 1945 loss: 2.25652389e-06
Iter: 1946 loss: 2.25614986e-06
Iter: 1947 loss: 2.25603367e-06
Iter: 1948 loss: 2.25580334e-06
Iter: 1949 loss: 2.25550457e-06
Iter: 1950 loss: 2.25550866e-06
Iter: 1951 loss: 2.25515737e-06
Iter: 1952 loss: 2.25639769e-06
Iter: 1953 loss: 2.25509302e-06
Iter: 1954 loss: 2.25494523e-06
Iter: 1955 loss: 2.25478561e-06
Iter: 1956 loss: 2.25473377e-06
Iter: 1957 loss: 2.25447639e-06
Iter: 1958 loss: 2.25559143e-06
Iter: 1959 loss: 2.25439817e-06
Iter: 1960 loss: 2.25417e-06
Iter: 1961 loss: 2.25458734e-06
Iter: 1962 loss: 2.2541044e-06
Iter: 1963 loss: 2.25369013e-06
Iter: 1964 loss: 2.25371627e-06
Iter: 1965 loss: 2.25336e-06
Iter: 1966 loss: 2.25315443e-06
Iter: 1967 loss: 2.25471808e-06
Iter: 1968 loss: 2.25312669e-06
Iter: 1969 loss: 2.25289068e-06
Iter: 1970 loss: 2.25371946e-06
Iter: 1971 loss: 2.25285703e-06
Iter: 1972 loss: 2.25271469e-06
Iter: 1973 loss: 2.25237955e-06
Iter: 1974 loss: 2.25704298e-06
Iter: 1975 loss: 2.25236317e-06
Iter: 1976 loss: 2.25237227e-06
Iter: 1977 loss: 2.25222357e-06
Iter: 1978 loss: 2.25210761e-06
Iter: 1979 loss: 2.25174404e-06
Iter: 1980 loss: 2.25472081e-06
Iter: 1981 loss: 2.2517022e-06
Iter: 1982 loss: 2.25129816e-06
Iter: 1983 loss: 2.25108897e-06
Iter: 1984 loss: 2.25088615e-06
Iter: 1985 loss: 2.25136569e-06
Iter: 1986 loss: 2.25069607e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script80
+ '[' -r STOP.script80 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0.4
+ date
Mon Nov  2 09:29:34 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6324491268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6324585ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f632448fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63245a9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6324502d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6324502c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63243778c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f632437f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63242e07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63242e0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63241cb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63241a9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6324433730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63243ecd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6324437ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63240e2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63240e2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63240e2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6324324f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63243d7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63243bd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63243bdbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6324218ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63241690d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6324169c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f632407d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f631c747f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63240d7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63240d7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63240a7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f63240378c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f631c6af158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f631c6af0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6324120598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f632414e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f631c6ffc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 4.2659347e-05
test_loss: 4.424275e-05
train_loss: 2.5049098e-05
test_loss: 2.246561e-05
train_loss: 1.6058693e-05
test_loss: 1.6970647e-05
train_loss: 1.5232225e-05
test_loss: 1.4477239e-05
train_loss: 1.3229892e-05
test_loss: 1.3285728e-05
train_loss: 1.1652672e-05
test_loss: 1.2974888e-05
train_loss: 1.1081109e-05
test_loss: 1.2312743e-05
train_loss: 1.0789639e-05
test_loss: 1.1310331e-05
train_loss: 1.014243e-05
test_loss: 1.0678182e-05
train_loss: 9.443966e-06
test_loss: 1.0372764e-05
train_loss: 9.212807e-06
test_loss: 9.751399e-06
train_loss: 9.185166e-06
test_loss: 9.4199195e-06
train_loss: 8.478795e-06
test_loss: 9.170936e-06
train_loss: 8.398453e-06
test_loss: 8.925523e-06
train_loss: 7.825762e-06
test_loss: 8.600521e-06
train_loss: 7.675277e-06
test_loss: 8.794804e-06
train_loss: 7.4333484e-06
test_loss: 8.254997e-06
train_loss: 7.517065e-06
test_loss: 7.943405e-06
train_loss: 6.9926045e-06
test_loss: 7.807391e-06
train_loss: 7.321303e-06
test_loss: 7.720325e-06
train_loss: 6.9880675e-06
test_loss: 7.681369e-06
train_loss: 6.3854504e-06
test_loss: 7.43674e-06
train_loss: 6.783599e-06
test_loss: 7.418516e-06
train_loss: 6.4026644e-06
test_loss: 7.2545895e-06
train_loss: 6.296009e-06
test_loss: 7.1936333e-06
train_loss: 6.337319e-06
test_loss: 7.0849937e-06
train_loss: 6.7448673e-06
test_loss: 7.053194e-06
train_loss: 6.725708e-06
test_loss: 6.9253288e-06
train_loss: 6.0728566e-06
test_loss: 6.914947e-06
train_loss: 6.1754035e-06
test_loss: 6.821174e-06
train_loss: 5.847923e-06
test_loss: 6.8020086e-06
train_loss: 5.416814e-06
test_loss: 6.7097712e-06
train_loss: 6.101156e-06
test_loss: 6.6753805e-06
train_loss: 5.576188e-06
test_loss: 6.58435e-06
train_loss: 5.6309514e-06
test_loss: 6.5539084e-06
train_loss: 5.809044e-06
test_loss: 6.5182344e-06
train_loss: 5.231953e-06
test_loss: 6.5219742e-06
train_loss: 5.597123e-06
test_loss: 6.5064696e-06
train_loss: 5.546082e-06
test_loss: 6.4692103e-06
train_loss: 5.7996463e-06
test_loss: 6.4619744e-06
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76ec88c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76ec7dfea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76ec7df6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76ec7dfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76ec81c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76ec7ebae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76ec7ebc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c1a12950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c1a127b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c1a2e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c19cf158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c19e7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c19e78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c1962d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c1a64400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c1a69c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c1a6f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c1a6fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f769c0498c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76800a3158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76800a32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76800cd158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f769c097400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76800286a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76800288c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7680028c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f768006f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7680062840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76800621e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f761877d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f761870f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76186be510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76186dcb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76186db6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76186649d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7618759158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.38501808e-06
Iter: 2 loss: 5.36048174e-06
Iter: 3 loss: 5.35505296e-06
Iter: 4 loss: 5.33937282e-06
Iter: 5 loss: 5.33673847e-06
Iter: 6 loss: 5.32597096e-06
Iter: 7 loss: 5.31686055e-06
Iter: 8 loss: 5.45429293e-06
Iter: 9 loss: 5.31685555e-06
Iter: 10 loss: 5.30802663e-06
Iter: 11 loss: 5.28760484e-06
Iter: 12 loss: 5.54180633e-06
Iter: 13 loss: 5.28610917e-06
Iter: 14 loss: 5.2737e-06
Iter: 15 loss: 5.40742803e-06
Iter: 16 loss: 5.27342854e-06
Iter: 17 loss: 5.26226086e-06
Iter: 18 loss: 5.30992293e-06
Iter: 19 loss: 5.2599944e-06
Iter: 20 loss: 5.24846109e-06
Iter: 21 loss: 5.26517124e-06
Iter: 22 loss: 5.24283723e-06
Iter: 23 loss: 5.23279323e-06
Iter: 24 loss: 5.24101097e-06
Iter: 25 loss: 5.22685968e-06
Iter: 26 loss: 5.22568689e-06
Iter: 27 loss: 5.22289929e-06
Iter: 28 loss: 5.22019945e-06
Iter: 29 loss: 5.21226684e-06
Iter: 30 loss: 5.24018378e-06
Iter: 31 loss: 5.20866706e-06
Iter: 32 loss: 5.19708237e-06
Iter: 33 loss: 5.20336062e-06
Iter: 34 loss: 5.18949219e-06
Iter: 35 loss: 5.17158242e-06
Iter: 36 loss: 5.19710511e-06
Iter: 37 loss: 5.16289356e-06
Iter: 38 loss: 5.14566909e-06
Iter: 39 loss: 5.26630083e-06
Iter: 40 loss: 5.14399653e-06
Iter: 41 loss: 5.14582416e-06
Iter: 42 loss: 5.13995201e-06
Iter: 43 loss: 5.13618124e-06
Iter: 44 loss: 5.13482428e-06
Iter: 45 loss: 5.1326806e-06
Iter: 46 loss: 5.12975112e-06
Iter: 47 loss: 5.15204556e-06
Iter: 48 loss: 5.12953466e-06
Iter: 49 loss: 5.12569068e-06
Iter: 50 loss: 5.11682538e-06
Iter: 51 loss: 5.22977371e-06
Iter: 52 loss: 5.11625876e-06
Iter: 53 loss: 5.10975906e-06
Iter: 54 loss: 5.13551458e-06
Iter: 55 loss: 5.10821792e-06
Iter: 56 loss: 5.10397513e-06
Iter: 57 loss: 5.10369773e-06
Iter: 58 loss: 5.10122391e-06
Iter: 59 loss: 5.0960607e-06
Iter: 60 loss: 5.18825527e-06
Iter: 61 loss: 5.09599249e-06
Iter: 62 loss: 5.09290476e-06
Iter: 63 loss: 5.09273923e-06
Iter: 64 loss: 5.0907629e-06
Iter: 65 loss: 5.10111386e-06
Iter: 66 loss: 5.09044776e-06
Iter: 67 loss: 5.08858329e-06
Iter: 68 loss: 5.08499852e-06
Iter: 69 loss: 5.1648085e-06
Iter: 70 loss: 5.08496487e-06
Iter: 71 loss: 5.08049e-06
Iter: 72 loss: 5.08076755e-06
Iter: 73 loss: 5.07698815e-06
Iter: 74 loss: 5.07133973e-06
Iter: 75 loss: 5.07903587e-06
Iter: 76 loss: 5.06855577e-06
Iter: 77 loss: 5.06298238e-06
Iter: 78 loss: 5.10054269e-06
Iter: 79 loss: 5.06238302e-06
Iter: 80 loss: 5.05922e-06
Iter: 81 loss: 5.09282108e-06
Iter: 82 loss: 5.05911657e-06
Iter: 83 loss: 5.05593471e-06
Iter: 84 loss: 5.08016e-06
Iter: 85 loss: 5.05571e-06
Iter: 86 loss: 5.05419939e-06
Iter: 87 loss: 5.05100934e-06
Iter: 88 loss: 5.10244e-06
Iter: 89 loss: 5.05093703e-06
Iter: 90 loss: 5.04590616e-06
Iter: 91 loss: 5.08146286e-06
Iter: 92 loss: 5.04548734e-06
Iter: 93 loss: 5.04354648e-06
Iter: 94 loss: 5.04100535e-06
Iter: 95 loss: 5.04082618e-06
Iter: 96 loss: 5.03916726e-06
Iter: 97 loss: 5.0390845e-06
Iter: 98 loss: 5.03712954e-06
Iter: 99 loss: 5.03547062e-06
Iter: 100 loss: 5.03486353e-06
Iter: 101 loss: 5.0331e-06
Iter: 102 loss: 5.03266e-06
Iter: 103 loss: 5.03159026e-06
Iter: 104 loss: 5.02946205e-06
Iter: 105 loss: 5.02938929e-06
Iter: 106 loss: 5.02786497e-06
Iter: 107 loss: 5.02347757e-06
Iter: 108 loss: 5.05114531e-06
Iter: 109 loss: 5.0223648e-06
Iter: 110 loss: 5.01922204e-06
Iter: 111 loss: 5.01920476e-06
Iter: 112 loss: 5.01693785e-06
Iter: 113 loss: 5.02484454e-06
Iter: 114 loss: 5.01630257e-06
Iter: 115 loss: 5.0148e-06
Iter: 116 loss: 5.01673e-06
Iter: 117 loss: 5.0140161e-06
Iter: 118 loss: 5.0119379e-06
Iter: 119 loss: 5.03606179e-06
Iter: 120 loss: 5.01189697e-06
Iter: 121 loss: 5.01082195e-06
Iter: 122 loss: 5.00882652e-06
Iter: 123 loss: 5.05202661e-06
Iter: 124 loss: 5.00880515e-06
Iter: 125 loss: 5.00739225e-06
Iter: 126 loss: 5.00726128e-06
Iter: 127 loss: 5.00616625e-06
Iter: 128 loss: 5.00289434e-06
Iter: 129 loss: 5.01212435e-06
Iter: 130 loss: 5.00114947e-06
Iter: 131 loss: 5.00088845e-06
Iter: 132 loss: 4.99983344e-06
Iter: 133 loss: 4.99850648e-06
Iter: 134 loss: 5.00194255e-06
Iter: 135 loss: 4.99805719e-06
Iter: 136 loss: 4.99727594e-06
Iter: 137 loss: 4.99552971e-06
Iter: 138 loss: 5.01873501e-06
Iter: 139 loss: 4.99538965e-06
Iter: 140 loss: 4.99284943e-06
Iter: 141 loss: 4.99439557e-06
Iter: 142 loss: 4.99116049e-06
Iter: 143 loss: 4.99283578e-06
Iter: 144 loss: 4.9899686e-06
Iter: 145 loss: 4.98907639e-06
Iter: 146 loss: 4.98673171e-06
Iter: 147 loss: 5.00337455e-06
Iter: 148 loss: 4.9862e-06
Iter: 149 loss: 4.98402551e-06
Iter: 150 loss: 4.99910948e-06
Iter: 151 loss: 4.98384179e-06
Iter: 152 loss: 4.9839141e-06
Iter: 153 loss: 4.98336249e-06
Iter: 154 loss: 4.98286454e-06
Iter: 155 loss: 4.98168265e-06
Iter: 156 loss: 4.99623866e-06
Iter: 157 loss: 4.98160716e-06
Iter: 158 loss: 4.98029112e-06
Iter: 159 loss: 4.98977079e-06
Iter: 160 loss: 4.98018289e-06
Iter: 161 loss: 4.97898918e-06
Iter: 162 loss: 4.98329337e-06
Iter: 163 loss: 4.97876817e-06
Iter: 164 loss: 4.97768542e-06
Iter: 165 loss: 4.977549e-06
Iter: 166 loss: 4.97684687e-06
Iter: 167 loss: 4.97548717e-06
Iter: 168 loss: 4.97549672e-06
Iter: 169 loss: 4.97444853e-06
Iter: 170 loss: 4.9736509e-06
Iter: 171 loss: 4.97332576e-06
Iter: 172 loss: 4.97281872e-06
Iter: 173 loss: 4.97154679e-06
Iter: 174 loss: 4.98263307e-06
Iter: 175 loss: 4.97129577e-06
Iter: 176 loss: 4.96966504e-06
Iter: 177 loss: 4.9705759e-06
Iter: 178 loss: 4.96865414e-06
Iter: 179 loss: 4.96772554e-06
Iter: 180 loss: 4.9674818e-06
Iter: 181 loss: 4.96620942e-06
Iter: 182 loss: 4.96555185e-06
Iter: 183 loss: 4.96496796e-06
Iter: 184 loss: 4.96407756e-06
Iter: 185 loss: 4.96356824e-06
Iter: 186 loss: 4.96319262e-06
Iter: 187 loss: 4.96185203e-06
Iter: 188 loss: 4.96643588e-06
Iter: 189 loss: 4.96145367e-06
Iter: 190 loss: 4.96089478e-06
Iter: 191 loss: 4.96065331e-06
Iter: 192 loss: 4.96019675e-06
Iter: 193 loss: 4.95899076e-06
Iter: 194 loss: 4.96782195e-06
Iter: 195 loss: 4.95874974e-06
Iter: 196 loss: 4.95750373e-06
Iter: 197 loss: 4.95750101e-06
Iter: 198 loss: 4.95655604e-06
Iter: 199 loss: 4.95801669e-06
Iter: 200 loss: 4.95613313e-06
Iter: 201 loss: 4.95523045e-06
Iter: 202 loss: 4.95570976e-06
Iter: 203 loss: 4.95468885e-06
Iter: 204 loss: 4.9539326e-06
Iter: 205 loss: 4.95925906e-06
Iter: 206 loss: 4.95384847e-06
Iter: 207 loss: 4.95296081e-06
Iter: 208 loss: 4.95395716e-06
Iter: 209 loss: 4.95249424e-06
Iter: 210 loss: 4.95173481e-06
Iter: 211 loss: 4.950316e-06
Iter: 212 loss: 4.98352256e-06
Iter: 213 loss: 4.95027962e-06
Iter: 214 loss: 4.94913684e-06
Iter: 215 loss: 4.96380426e-06
Iter: 216 loss: 4.94915093e-06
Iter: 217 loss: 4.94801134e-06
Iter: 218 loss: 4.95534641e-06
Iter: 219 loss: 4.94786855e-06
Iter: 220 loss: 4.94723599e-06
Iter: 221 loss: 4.94581764e-06
Iter: 222 loss: 4.96985285e-06
Iter: 223 loss: 4.94578217e-06
Iter: 224 loss: 4.94471715e-06
Iter: 225 loss: 4.94785399e-06
Iter: 226 loss: 4.94437e-06
Iter: 227 loss: 4.94393498e-06
Iter: 228 loss: 4.94366304e-06
Iter: 229 loss: 4.94323331e-06
Iter: 230 loss: 4.94199185e-06
Iter: 231 loss: 4.95227368e-06
Iter: 232 loss: 4.94177402e-06
Iter: 233 loss: 4.94122514e-06
Iter: 234 loss: 4.94107053e-06
Iter: 235 loss: 4.94047345e-06
Iter: 236 loss: 4.9399423e-06
Iter: 237 loss: 4.93978177e-06
Iter: 238 loss: 4.93887228e-06
Iter: 239 loss: 4.94208916e-06
Iter: 240 loss: 4.93868811e-06
Iter: 241 loss: 4.93800781e-06
Iter: 242 loss: 4.94065443e-06
Iter: 243 loss: 4.93784682e-06
Iter: 244 loss: 4.93696734e-06
Iter: 245 loss: 4.93884454e-06
Iter: 246 loss: 4.93662674e-06
Iter: 247 loss: 4.93603147e-06
Iter: 248 loss: 4.93508787e-06
Iter: 249 loss: 4.93509879e-06
Iter: 250 loss: 4.93385642e-06
Iter: 251 loss: 4.93694733e-06
Iter: 252 loss: 4.9334194e-06
Iter: 253 loss: 4.93241532e-06
Iter: 254 loss: 4.93233756e-06
Iter: 255 loss: 4.9319242e-06
Iter: 256 loss: 4.93098378e-06
Iter: 257 loss: 4.94417054e-06
Iter: 258 loss: 4.93093557e-06
Iter: 259 loss: 4.92987e-06
Iter: 260 loss: 4.93016705e-06
Iter: 261 loss: 4.92911386e-06
Iter: 262 loss: 4.92791241e-06
Iter: 263 loss: 4.93387597e-06
Iter: 264 loss: 4.92771369e-06
Iter: 265 loss: 4.92787376e-06
Iter: 266 loss: 4.92722e-06
Iter: 267 loss: 4.92694653e-06
Iter: 268 loss: 4.92612435e-06
Iter: 269 loss: 4.92849085e-06
Iter: 270 loss: 4.92568233e-06
Iter: 271 loss: 4.92471372e-06
Iter: 272 loss: 4.93349307e-06
Iter: 273 loss: 4.92467e-06
Iter: 274 loss: 4.92394156e-06
Iter: 275 loss: 4.93268226e-06
Iter: 276 loss: 4.92390609e-06
Iter: 277 loss: 4.92338586e-06
Iter: 278 loss: 4.92320896e-06
Iter: 279 loss: 4.92291929e-06
Iter: 280 loss: 4.92201298e-06
Iter: 281 loss: 4.9211385e-06
Iter: 282 loss: 4.92093204e-06
Iter: 283 loss: 4.92026811e-06
Iter: 284 loss: 4.93068637e-06
Iter: 285 loss: 4.92027266e-06
Iter: 286 loss: 4.91974606e-06
Iter: 287 loss: 4.92430354e-06
Iter: 288 loss: 4.91970059e-06
Iter: 289 loss: 4.91938681e-06
Iter: 290 loss: 4.91851e-06
Iter: 291 loss: 4.92492109e-06
Iter: 292 loss: 4.91835272e-06
Iter: 293 loss: 4.91827768e-06
Iter: 294 loss: 4.91791661e-06
Iter: 295 loss: 4.9174464e-06
Iter: 296 loss: 4.9165842e-06
Iter: 297 loss: 4.93066909e-06
Iter: 298 loss: 4.91655373e-06
Iter: 299 loss: 4.91558967e-06
Iter: 300 loss: 4.91548235e-06
Iter: 301 loss: 4.91474839e-06
Iter: 302 loss: 4.91548735e-06
Iter: 303 loss: 4.91428364e-06
Iter: 304 loss: 4.91398441e-06
Iter: 305 loss: 4.91334504e-06
Iter: 306 loss: 4.92161735e-06
Iter: 307 loss: 4.91327137e-06
Iter: 308 loss: 4.91270794e-06
Iter: 309 loss: 4.91333913e-06
Iter: 310 loss: 4.91237e-06
Iter: 311 loss: 4.91181027e-06
Iter: 312 loss: 4.92115623e-06
Iter: 313 loss: 4.91178935e-06
Iter: 314 loss: 4.91135734e-06
Iter: 315 loss: 4.91053652e-06
Iter: 316 loss: 4.92513618e-06
Iter: 317 loss: 4.91052288e-06
Iter: 318 loss: 4.90960383e-06
Iter: 319 loss: 4.91625087e-06
Iter: 320 loss: 4.90956518e-06
Iter: 321 loss: 4.90888215e-06
Iter: 322 loss: 4.90823913e-06
Iter: 323 loss: 4.90812863e-06
Iter: 324 loss: 4.90756702e-06
Iter: 325 loss: 4.9075511e-06
Iter: 326 loss: 4.90703e-06
Iter: 327 loss: 4.90891762e-06
Iter: 328 loss: 4.90685534e-06
Iter: 329 loss: 4.90652747e-06
Iter: 330 loss: 4.90569391e-06
Iter: 331 loss: 4.91548144e-06
Iter: 332 loss: 4.90563116e-06
Iter: 333 loss: 4.90511593e-06
Iter: 334 loss: 4.90501088e-06
Iter: 335 loss: 4.90442062e-06
Iter: 336 loss: 4.90359844e-06
Iter: 337 loss: 4.90356615e-06
Iter: 338 loss: 4.90320053e-06
Iter: 339 loss: 4.90315597e-06
Iter: 340 loss: 4.90265711e-06
Iter: 341 loss: 4.9021528e-06
Iter: 342 loss: 4.9021105e-06
Iter: 343 loss: 4.90164166e-06
Iter: 344 loss: 4.90271213e-06
Iter: 345 loss: 4.9014443e-06
Iter: 346 loss: 4.90087768e-06
Iter: 347 loss: 4.90488128e-06
Iter: 348 loss: 4.90080174e-06
Iter: 349 loss: 4.90046568e-06
Iter: 350 loss: 4.89983358e-06
Iter: 351 loss: 4.91039236e-06
Iter: 352 loss: 4.8998e-06
Iter: 353 loss: 4.898935e-06
Iter: 354 loss: 4.90484763e-06
Iter: 355 loss: 4.89884496e-06
Iter: 356 loss: 4.89808463e-06
Iter: 357 loss: 4.8992647e-06
Iter: 358 loss: 4.89775084e-06
Iter: 359 loss: 4.89723789e-06
Iter: 360 loss: 4.89997865e-06
Iter: 361 loss: 4.89717786e-06
Iter: 362 loss: 4.89664399e-06
Iter: 363 loss: 4.89993681e-06
Iter: 364 loss: 4.89655395e-06
Iter: 365 loss: 4.89623244e-06
Iter: 366 loss: 4.89544618e-06
Iter: 367 loss: 4.90566026e-06
Iter: 368 loss: 4.89541162e-06
Iter: 369 loss: 4.89454305e-06
Iter: 370 loss: 4.89564e-06
Iter: 371 loss: 4.89410968e-06
Iter: 372 loss: 4.89419381e-06
Iter: 373 loss: 4.89365129e-06
Iter: 374 loss: 4.89333706e-06
Iter: 375 loss: 4.8925549e-06
Iter: 376 loss: 4.90201865e-06
Iter: 377 loss: 4.89247759e-06
Iter: 378 loss: 4.89243121e-06
Iter: 379 loss: 4.89215927e-06
Iter: 380 loss: 4.89191098e-06
Iter: 381 loss: 4.89138847e-06
Iter: 382 loss: 4.90068442e-06
Iter: 383 loss: 4.89139529e-06
Iter: 384 loss: 4.89088325e-06
Iter: 385 loss: 4.89007743e-06
Iter: 386 loss: 4.89008107e-06
Iter: 387 loss: 4.88977548e-06
Iter: 388 loss: 4.88943397e-06
Iter: 389 loss: 4.88911064e-06
Iter: 390 loss: 4.88859496e-06
Iter: 391 loss: 4.88862e-06
Iter: 392 loss: 4.88804517e-06
Iter: 393 loss: 4.88990963e-06
Iter: 394 loss: 4.88787146e-06
Iter: 395 loss: 4.88735077e-06
Iter: 396 loss: 4.88869364e-06
Iter: 397 loss: 4.88712158e-06
Iter: 398 loss: 4.88670275e-06
Iter: 399 loss: 4.89297872e-06
Iter: 400 loss: 4.88667683e-06
Iter: 401 loss: 4.88648902e-06
Iter: 402 loss: 4.88597243e-06
Iter: 403 loss: 4.88986e-06
Iter: 404 loss: 4.88587739e-06
Iter: 405 loss: 4.88525711e-06
Iter: 406 loss: 4.88985143e-06
Iter: 407 loss: 4.88522255e-06
Iter: 408 loss: 4.88456044e-06
Iter: 409 loss: 4.88565729e-06
Iter: 410 loss: 4.8842312e-06
Iter: 411 loss: 4.88375372e-06
Iter: 412 loss: 4.88315891e-06
Iter: 413 loss: 4.88312435e-06
Iter: 414 loss: 4.88326486e-06
Iter: 415 loss: 4.88278056e-06
Iter: 416 loss: 4.88256819e-06
Iter: 417 loss: 4.88205706e-06
Iter: 418 loss: 4.88413662e-06
Iter: 419 loss: 4.88183514e-06
Iter: 420 loss: 4.88151818e-06
Iter: 421 loss: 4.88142814e-06
Iter: 422 loss: 4.88114256e-06
Iter: 423 loss: 4.88082787e-06
Iter: 424 loss: 4.88074147e-06
Iter: 425 loss: 4.8803181e-06
Iter: 426 loss: 4.87979378e-06
Iter: 427 loss: 4.87973e-06
Iter: 428 loss: 4.87901525e-06
Iter: 429 loss: 4.88979731e-06
Iter: 430 loss: 4.87900888e-06
Iter: 431 loss: 4.8785223e-06
Iter: 432 loss: 4.87973466e-06
Iter: 433 loss: 4.87834541e-06
Iter: 434 loss: 4.87788384e-06
Iter: 435 loss: 4.88034038e-06
Iter: 436 loss: 4.87779607e-06
Iter: 437 loss: 4.87747775e-06
Iter: 438 loss: 4.87685065e-06
Iter: 439 loss: 4.88734531e-06
Iter: 440 loss: 4.87682655e-06
Iter: 441 loss: 4.87687066e-06
Iter: 442 loss: 4.87648822e-06
Iter: 443 loss: 4.87621855e-06
Iter: 444 loss: 4.87567104e-06
Iter: 445 loss: 4.88797468e-06
Iter: 446 loss: 4.8757e-06
Iter: 447 loss: 4.87518e-06
Iter: 448 loss: 4.87465832e-06
Iter: 449 loss: 4.87462648e-06
Iter: 450 loss: 4.87435318e-06
Iter: 451 loss: 4.87418311e-06
Iter: 452 loss: 4.87371926e-06
Iter: 453 loss: 4.87391662e-06
Iter: 454 loss: 4.87342413e-06
Iter: 455 loss: 4.87306534e-06
Iter: 456 loss: 4.87341322e-06
Iter: 457 loss: 4.87289253e-06
Iter: 458 loss: 4.87229863e-06
Iter: 459 loss: 4.87333955e-06
Iter: 460 loss: 4.87200577e-06
Iter: 461 loss: 4.87162652e-06
Iter: 462 loss: 4.87111447e-06
Iter: 463 loss: 4.87106354e-06
Iter: 464 loss: 4.87078159e-06
Iter: 465 loss: 4.87072293e-06
Iter: 466 loss: 4.87041325e-06
Iter: 467 loss: 4.87036868e-06
Iter: 468 loss: 4.87013131e-06
Iter: 469 loss: 4.86966383e-06
Iter: 470 loss: 4.87051329e-06
Iter: 471 loss: 4.86949102e-06
Iter: 472 loss: 4.86904219e-06
Iter: 473 loss: 4.86887711e-06
Iter: 474 loss: 4.86860608e-06
Iter: 475 loss: 4.86797944e-06
Iter: 476 loss: 4.86800855e-06
Iter: 477 loss: 4.86751469e-06
Iter: 478 loss: 4.86773843e-06
Iter: 479 loss: 4.86714362e-06
Iter: 480 loss: 4.86695353e-06
Iter: 481 loss: 4.86650742e-06
Iter: 482 loss: 4.87047237e-06
Iter: 483 loss: 4.86641193e-06
Iter: 484 loss: 4.86597219e-06
Iter: 485 loss: 4.86812405e-06
Iter: 486 loss: 4.86592126e-06
Iter: 487 loss: 4.8653983e-06
Iter: 488 loss: 4.86973886e-06
Iter: 489 loss: 4.86536737e-06
Iter: 490 loss: 4.86512727e-06
Iter: 491 loss: 4.86454974e-06
Iter: 492 loss: 4.8699585e-06
Iter: 493 loss: 4.86436511e-06
Iter: 494 loss: 4.86423414e-06
Iter: 495 loss: 4.86402041e-06
Iter: 496 loss: 4.86374392e-06
Iter: 497 loss: 4.86315093e-06
Iter: 498 loss: 4.87054604e-06
Iter: 499 loss: 4.8631523e-06
Iter: 500 loss: 4.86265935e-06
Iter: 501 loss: 4.86699e-06
Iter: 502 loss: 4.86264344e-06
Iter: 503 loss: 4.86220597e-06
Iter: 504 loss: 4.86481804e-06
Iter: 505 loss: 4.86214049e-06
Iter: 506 loss: 4.86192221e-06
Iter: 507 loss: 4.8615093e-06
Iter: 508 loss: 4.86792032e-06
Iter: 509 loss: 4.86147474e-06
Iter: 510 loss: 4.860849e-06
Iter: 511 loss: 4.86497e-06
Iter: 512 loss: 4.86079125e-06
Iter: 513 loss: 4.86021099e-06
Iter: 514 loss: 4.86080717e-06
Iter: 515 loss: 4.85989631e-06
Iter: 516 loss: 4.8594693e-06
Iter: 517 loss: 4.86166346e-06
Iter: 518 loss: 4.8594311e-06
Iter: 519 loss: 4.85877626e-06
Iter: 520 loss: 4.86043155e-06
Iter: 521 loss: 4.85856344e-06
Iter: 522 loss: 4.8583e-06
Iter: 523 loss: 4.85957298e-06
Iter: 524 loss: 4.85826786e-06
Iter: 525 loss: 4.85795044e-06
Iter: 526 loss: 4.85846385e-06
Iter: 527 loss: 4.85780402e-06
Iter: 528 loss: 4.85753753e-06
Iter: 529 loss: 4.85699729e-06
Iter: 530 loss: 4.86144472e-06
Iter: 531 loss: 4.85684723e-06
Iter: 532 loss: 4.85636883e-06
Iter: 533 loss: 4.85631335e-06
Iter: 534 loss: 4.85581586e-06
Iter: 535 loss: 4.85890268e-06
Iter: 536 loss: 4.85576402e-06
Iter: 537 loss: 4.85553483e-06
Iter: 538 loss: 4.85492637e-06
Iter: 539 loss: 4.86088e-06
Iter: 540 loss: 4.8548618e-06
Iter: 541 loss: 4.85439887e-06
Iter: 542 loss: 4.85440478e-06
Iter: 543 loss: 4.85390228e-06
Iter: 544 loss: 4.85535475e-06
Iter: 545 loss: 4.85374494e-06
Iter: 546 loss: 4.85345163e-06
Iter: 547 loss: 4.85286637e-06
Iter: 548 loss: 4.86608587e-06
Iter: 549 loss: 4.85287273e-06
Iter: 550 loss: 4.85220653e-06
Iter: 551 loss: 4.85675e-06
Iter: 552 loss: 4.85207647e-06
Iter: 553 loss: 4.85151941e-06
Iter: 554 loss: 4.85359578e-06
Iter: 555 loss: 4.85134751e-06
Iter: 556 loss: 4.85113105e-06
Iter: 557 loss: 4.85108194e-06
Iter: 558 loss: 4.85084865e-06
Iter: 559 loss: 4.85054306e-06
Iter: 560 loss: 4.8505e-06
Iter: 561 loss: 4.85020337e-06
Iter: 562 loss: 4.8518832e-06
Iter: 563 loss: 4.85013788e-06
Iter: 564 loss: 4.84969769e-06
Iter: 565 loss: 4.85021428e-06
Iter: 566 loss: 4.84941575e-06
Iter: 567 loss: 4.84912653e-06
Iter: 568 loss: 4.84865632e-06
Iter: 569 loss: 4.84866814e-06
Iter: 570 loss: 4.84818474e-06
Iter: 571 loss: 4.85189503e-06
Iter: 572 loss: 4.84815473e-06
Iter: 573 loss: 4.84762541e-06
Iter: 574 loss: 4.85095188e-06
Iter: 575 loss: 4.84758948e-06
Iter: 576 loss: 4.8472989e-06
Iter: 577 loss: 4.84680822e-06
Iter: 578 loss: 4.85744249e-06
Iter: 579 loss: 4.8468255e-06
Iter: 580 loss: 4.84632437e-06
Iter: 581 loss: 4.84659131e-06
Iter: 582 loss: 4.84599559e-06
Iter: 583 loss: 4.84566044e-06
Iter: 584 loss: 4.84552038e-06
Iter: 585 loss: 4.84532302e-06
Iter: 586 loss: 4.84487873e-06
Iter: 587 loss: 4.85142482e-06
Iter: 588 loss: 4.84480552e-06
Iter: 589 loss: 4.84439261e-06
Iter: 590 loss: 4.84490283e-06
Iter: 591 loss: 4.84414386e-06
Iter: 592 loss: 4.84387101e-06
Iter: 593 loss: 4.84382736e-06
Iter: 594 loss: 4.84354496e-06
Iter: 595 loss: 4.84320844e-06
Iter: 596 loss: 4.84319708e-06
Iter: 597 loss: 4.84288103e-06
Iter: 598 loss: 4.84809198e-06
Iter: 599 loss: 4.84289376e-06
Iter: 600 loss: 4.84262955e-06
Iter: 601 loss: 4.84249904e-06
Iter: 602 loss: 4.84234579e-06
Iter: 603 loss: 4.84191787e-06
Iter: 604 loss: 4.8418915e-06
Iter: 605 loss: 4.84157681e-06
Iter: 606 loss: 4.84115662e-06
Iter: 607 loss: 4.84342945e-06
Iter: 608 loss: 4.84107295e-06
Iter: 609 loss: 4.84063185e-06
Iter: 610 loss: 4.84343036e-06
Iter: 611 loss: 4.84059728e-06
Iter: 612 loss: 4.84033444e-06
Iter: 613 loss: 4.8396505e-06
Iter: 614 loss: 4.84409156e-06
Iter: 615 loss: 4.83948224e-06
Iter: 616 loss: 4.83858776e-06
Iter: 617 loss: 4.84538896e-06
Iter: 618 loss: 4.83853546e-06
Iter: 619 loss: 4.83824169e-06
Iter: 620 loss: 4.83816348e-06
Iter: 621 loss: 4.83794429e-06
Iter: 622 loss: 4.83732947e-06
Iter: 623 loss: 4.8406223e-06
Iter: 624 loss: 4.83716667e-06
Iter: 625 loss: 4.83656368e-06
Iter: 626 loss: 4.84010616e-06
Iter: 627 loss: 4.8365182e-06
Iter: 628 loss: 4.83636086e-06
Iter: 629 loss: 4.83627218e-06
Iter: 630 loss: 4.8360107e-06
Iter: 631 loss: 4.83530039e-06
Iter: 632 loss: 4.83955091e-06
Iter: 633 loss: 4.83511485e-06
Iter: 634 loss: 4.83489248e-06
Iter: 635 loss: 4.83467375e-06
Iter: 636 loss: 4.83429676e-06
Iter: 637 loss: 4.83369331e-06
Iter: 638 loss: 4.83370513e-06
Iter: 639 loss: 4.83325312e-06
Iter: 640 loss: 4.83369877e-06
Iter: 641 loss: 4.83294298e-06
Iter: 642 loss: 4.83258827e-06
Iter: 643 loss: 4.83256144e-06
Iter: 644 loss: 4.83228769e-06
Iter: 645 loss: 4.83209897e-06
Iter: 646 loss: 4.83199e-06
Iter: 647 loss: 4.8315469e-06
Iter: 648 loss: 4.83208942e-06
Iter: 649 loss: 4.83131407e-06
Iter: 650 loss: 4.83083477e-06
Iter: 651 loss: 4.83015583e-06
Iter: 652 loss: 4.83009899e-06
Iter: 653 loss: 4.82958512e-06
Iter: 654 loss: 4.82957284e-06
Iter: 655 loss: 4.82901851e-06
Iter: 656 loss: 4.83045324e-06
Iter: 657 loss: 4.8288266e-06
Iter: 658 loss: 4.82847327e-06
Iter: 659 loss: 4.82781707e-06
Iter: 660 loss: 4.84131579e-06
Iter: 661 loss: 4.82782934e-06
Iter: 662 loss: 4.82723772e-06
Iter: 663 loss: 4.83358599e-06
Iter: 664 loss: 4.8272268e-06
Iter: 665 loss: 4.82666383e-06
Iter: 666 loss: 4.830531e-06
Iter: 667 loss: 4.82658697e-06
Iter: 668 loss: 4.82635551e-06
Iter: 669 loss: 4.82589076e-06
Iter: 670 loss: 4.83655e-06
Iter: 671 loss: 4.82585438e-06
Iter: 672 loss: 4.82570067e-06
Iter: 673 loss: 4.8255547e-06
Iter: 674 loss: 4.82530868e-06
Iter: 675 loss: 4.82462701e-06
Iter: 676 loss: 4.82800669e-06
Iter: 677 loss: 4.82439918e-06
Iter: 678 loss: 4.82379801e-06
Iter: 679 loss: 4.82587438e-06
Iter: 680 loss: 4.82362475e-06
Iter: 681 loss: 4.82300857e-06
Iter: 682 loss: 4.82644282e-06
Iter: 683 loss: 4.82291352e-06
Iter: 684 loss: 4.82232554e-06
Iter: 685 loss: 4.82636e-06
Iter: 686 loss: 4.82233372e-06
Iter: 687 loss: 4.82178439e-06
Iter: 688 loss: 4.82272117e-06
Iter: 689 loss: 4.82155247e-06
Iter: 690 loss: 4.82111591e-06
Iter: 691 loss: 4.82078576e-06
Iter: 692 loss: 4.82061296e-06
Iter: 693 loss: 4.82018277e-06
Iter: 694 loss: 4.82463656e-06
Iter: 695 loss: 4.8201282e-06
Iter: 696 loss: 4.81961251e-06
Iter: 697 loss: 4.82043379e-06
Iter: 698 loss: 4.8192278e-06
Iter: 699 loss: 4.81892539e-06
Iter: 700 loss: 4.81843199e-06
Iter: 701 loss: 4.81842926e-06
Iter: 702 loss: 4.817648e-06
Iter: 703 loss: 4.81810048e-06
Iter: 704 loss: 4.81718416e-06
Iter: 705 loss: 4.81740153e-06
Iter: 706 loss: 4.81677125e-06
Iter: 707 loss: 4.81654024e-06
Iter: 708 loss: 4.81602228e-06
Iter: 709 loss: 4.82368432e-06
Iter: 710 loss: 4.81599091e-06
Iter: 711 loss: 4.81576535e-06
Iter: 712 loss: 4.81566485e-06
Iter: 713 loss: 4.81543611e-06
Iter: 714 loss: 4.81472216e-06
Iter: 715 loss: 4.81891948e-06
Iter: 716 loss: 4.81457846e-06
Iter: 717 loss: 4.81383358e-06
Iter: 718 loss: 4.81377674e-06
Iter: 719 loss: 4.81333245e-06
Iter: 720 loss: 4.81235566e-06
Iter: 721 loss: 4.82086671e-06
Iter: 722 loss: 4.81227471e-06
Iter: 723 loss: 4.81149755e-06
Iter: 724 loss: 4.81639e-06
Iter: 725 loss: 4.81140523e-06
Iter: 726 loss: 4.81081406e-06
Iter: 727 loss: 4.8111192e-06
Iter: 728 loss: 4.8103966e-06
Iter: 729 loss: 4.81015695e-06
Iter: 730 loss: 4.8099937e-06
Iter: 731 loss: 4.80970903e-06
Iter: 732 loss: 4.80892459e-06
Iter: 733 loss: 4.8165175e-06
Iter: 734 loss: 4.80883864e-06
Iter: 735 loss: 4.80823928e-06
Iter: 736 loss: 4.81576808e-06
Iter: 737 loss: 4.808222e-06
Iter: 738 loss: 4.80766221e-06
Iter: 739 loss: 4.81215693e-06
Iter: 740 loss: 4.80764629e-06
Iter: 741 loss: 4.80730387e-06
Iter: 742 loss: 4.80671224e-06
Iter: 743 loss: 4.80670633e-06
Iter: 744 loss: 4.8061338e-06
Iter: 745 loss: 4.80610379e-06
Iter: 746 loss: 4.80589551e-06
Iter: 747 loss: 4.80552626e-06
Iter: 748 loss: 4.80553763e-06
Iter: 749 loss: 4.80490053e-06
Iter: 750 loss: 4.8085958e-06
Iter: 751 loss: 4.80483141e-06
Iter: 752 loss: 4.80447125e-06
Iter: 753 loss: 4.80348444e-06
Iter: 754 loss: 4.8104489e-06
Iter: 755 loss: 4.80327071e-06
Iter: 756 loss: 4.80226163e-06
Iter: 757 loss: 4.80818e-06
Iter: 758 loss: 4.8021393e-06
Iter: 759 loss: 4.80147855e-06
Iter: 760 loss: 4.80475819e-06
Iter: 761 loss: 4.80141443e-06
Iter: 762 loss: 4.80070048e-06
Iter: 763 loss: 4.80464314e-06
Iter: 764 loss: 4.80064136e-06
Iter: 765 loss: 4.80022481e-06
Iter: 766 loss: 4.80466679e-06
Iter: 767 loss: 4.80018116e-06
Iter: 768 loss: 4.79983555e-06
Iter: 769 loss: 4.79923938e-06
Iter: 770 loss: 4.79924347e-06
Iter: 771 loss: 4.79865685e-06
Iter: 772 loss: 4.80203835e-06
Iter: 773 loss: 4.79858136e-06
Iter: 774 loss: 4.79777646e-06
Iter: 775 loss: 4.79760456e-06
Iter: 776 loss: 4.79707251e-06
Iter: 777 loss: 4.7966796e-06
Iter: 778 loss: 4.79664868e-06
Iter: 779 loss: 4.7962626e-06
Iter: 780 loss: 4.79598566e-06
Iter: 781 loss: 4.79588243e-06
Iter: 782 loss: 4.79556093e-06
Iter: 783 loss: 4.79813116e-06
Iter: 784 loss: 4.79549726e-06
Iter: 785 loss: 4.79502842e-06
Iter: 786 loss: 4.79433675e-06
Iter: 787 loss: 4.79432947e-06
Iter: 788 loss: 4.79376104e-06
Iter: 789 loss: 4.79341543e-06
Iter: 790 loss: 4.79314167e-06
Iter: 791 loss: 4.7921726e-06
Iter: 792 loss: 4.79388e-06
Iter: 793 loss: 4.79176924e-06
Iter: 794 loss: 4.79115897e-06
Iter: 795 loss: 4.79116807e-06
Iter: 796 loss: 4.79067603e-06
Iter: 797 loss: 4.79442224e-06
Iter: 798 loss: 4.7906683e-06
Iter: 799 loss: 4.79026312e-06
Iter: 800 loss: 4.79104938e-06
Iter: 801 loss: 4.79013e-06
Iter: 802 loss: 4.78966467e-06
Iter: 803 loss: 4.78895799e-06
Iter: 804 loss: 4.78897164e-06
Iter: 805 loss: 4.78841548e-06
Iter: 806 loss: 4.78836546e-06
Iter: 807 loss: 4.7879189e-06
Iter: 808 loss: 4.78705897e-06
Iter: 809 loss: 4.80301969e-06
Iter: 810 loss: 4.78699076e-06
Iter: 811 loss: 4.78662605e-06
Iter: 812 loss: 4.78648599e-06
Iter: 813 loss: 4.7861522e-06
Iter: 814 loss: 4.78531365e-06
Iter: 815 loss: 4.79079335e-06
Iter: 816 loss: 4.78510901e-06
Iter: 817 loss: 4.78488619e-06
Iter: 818 loss: 4.78462425e-06
Iter: 819 loss: 4.78426045e-06
Iter: 820 loss: 4.78362517e-06
Iter: 821 loss: 4.79407117e-06
Iter: 822 loss: 4.78357e-06
Iter: 823 loss: 4.78284301e-06
Iter: 824 loss: 4.78211587e-06
Iter: 825 loss: 4.78195579e-06
Iter: 826 loss: 4.78094171e-06
Iter: 827 loss: 4.78270886e-06
Iter: 828 loss: 4.7805e-06
Iter: 829 loss: 4.78016682e-06
Iter: 830 loss: 4.78002539e-06
Iter: 831 loss: 4.77956564e-06
Iter: 832 loss: 4.77944468e-06
Iter: 833 loss: 4.77916183e-06
Iter: 834 loss: 4.7785561e-06
Iter: 835 loss: 4.7802514e-06
Iter: 836 loss: 4.77840513e-06
Iter: 837 loss: 4.77784306e-06
Iter: 838 loss: 4.77895583e-06
Iter: 839 loss: 4.7775543e-06
Iter: 840 loss: 4.77697085e-06
Iter: 841 loss: 4.78198353e-06
Iter: 842 loss: 4.77692811e-06
Iter: 843 loss: 4.77655703e-06
Iter: 844 loss: 4.77622689e-06
Iter: 845 loss: 4.77616823e-06
Iter: 846 loss: 4.77533922e-06
Iter: 847 loss: 4.7796666e-06
Iter: 848 loss: 4.77519052e-06
Iter: 849 loss: 4.77490494e-06
Iter: 850 loss: 4.77488038e-06
Iter: 851 loss: 4.77463027e-06
Iter: 852 loss: 4.77407593e-06
Iter: 853 loss: 4.77744561e-06
Iter: 854 loss: 4.77397e-06
Iter: 855 loss: 4.77361891e-06
Iter: 856 loss: 4.77268713e-06
Iter: 857 loss: 4.7784979e-06
Iter: 858 loss: 4.77240155e-06
Iter: 859 loss: 4.77128469e-06
Iter: 860 loss: 4.77996764e-06
Iter: 861 loss: 4.77118556e-06
Iter: 862 loss: 4.77026424e-06
Iter: 863 loss: 4.77154117e-06
Iter: 864 loss: 4.76979494e-06
Iter: 865 loss: 4.76955029e-06
Iter: 866 loss: 4.76931928e-06
Iter: 867 loss: 4.76888772e-06
Iter: 868 loss: 4.76808145e-06
Iter: 869 loss: 4.78739048e-06
Iter: 870 loss: 4.76806281e-06
Iter: 871 loss: 4.76737705e-06
Iter: 872 loss: 4.76867717e-06
Iter: 873 loss: 4.76705645e-06
Iter: 874 loss: 4.7666249e-06
Iter: 875 loss: 4.76653895e-06
Iter: 876 loss: 4.76610103e-06
Iter: 877 loss: 4.76542118e-06
Iter: 878 loss: 4.76540936e-06
Iter: 879 loss: 4.76471e-06
Iter: 880 loss: 4.77326466e-06
Iter: 881 loss: 4.76472815e-06
Iter: 882 loss: 4.76428932e-06
Iter: 883 loss: 4.76707373e-06
Iter: 884 loss: 4.7642452e-06
Iter: 885 loss: 4.76395326e-06
Iter: 886 loss: 4.76354217e-06
Iter: 887 loss: 4.76355899e-06
Iter: 888 loss: 4.76305831e-06
Iter: 889 loss: 4.76980904e-06
Iter: 890 loss: 4.76307559e-06
Iter: 891 loss: 4.76281639e-06
Iter: 892 loss: 4.76212881e-06
Iter: 893 loss: 4.76751666e-06
Iter: 894 loss: 4.76196283e-06
Iter: 895 loss: 4.7610838e-06
Iter: 896 loss: 4.76112473e-06
Iter: 897 loss: 4.7604135e-06
Iter: 898 loss: 4.75924116e-06
Iter: 899 loss: 4.76853165e-06
Iter: 900 loss: 4.75915e-06
Iter: 901 loss: 4.75917795e-06
Iter: 902 loss: 4.7588519e-06
Iter: 903 loss: 4.75857814e-06
Iter: 904 loss: 4.75795423e-06
Iter: 905 loss: 4.76683408e-06
Iter: 906 loss: 4.75796151e-06
Iter: 907 loss: 4.75737625e-06
Iter: 908 loss: 4.75946e-06
Iter: 909 loss: 4.75719207e-06
Iter: 910 loss: 4.75653087e-06
Iter: 911 loss: 4.76263176e-06
Iter: 912 loss: 4.75649904e-06
Iter: 913 loss: 4.75613479e-06
Iter: 914 loss: 4.75587649e-06
Iter: 915 loss: 4.75568777e-06
Iter: 916 loss: 4.75515117e-06
Iter: 917 loss: 4.76223249e-06
Iter: 918 loss: 4.7551448e-06
Iter: 919 loss: 4.7546564e-06
Iter: 920 loss: 4.75452271e-06
Iter: 921 loss: 4.75424e-06
Iter: 922 loss: 4.75374145e-06
Iter: 923 loss: 4.75622437e-06
Iter: 924 loss: 4.75362413e-06
Iter: 925 loss: 4.75306661e-06
Iter: 926 loss: 4.75482e-06
Iter: 927 loss: 4.75287e-06
Iter: 928 loss: 4.75260777e-06
Iter: 929 loss: 4.75191246e-06
Iter: 930 loss: 4.76114e-06
Iter: 931 loss: 4.75187699e-06
Iter: 932 loss: 4.75082288e-06
Iter: 933 loss: 4.75103843e-06
Iter: 934 loss: 4.75005345e-06
Iter: 935 loss: 4.74909393e-06
Iter: 936 loss: 4.75750494e-06
Iter: 937 loss: 4.74901663e-06
Iter: 938 loss: 4.74877834e-06
Iter: 939 loss: 4.74862236e-06
Iter: 940 loss: 4.74831e-06
Iter: 941 loss: 4.74759554e-06
Iter: 942 loss: 4.75481784e-06
Iter: 943 loss: 4.74751141e-06
Iter: 944 loss: 4.74705621e-06
Iter: 945 loss: 4.74706e-06
Iter: 946 loss: 4.74651961e-06
Iter: 947 loss: 4.74681838e-06
Iter: 948 loss: 4.74621265e-06
Iter: 949 loss: 4.7456806e-06
Iter: 950 loss: 4.7458575e-06
Iter: 951 loss: 4.74529133e-06
Iter: 952 loss: 4.74442459e-06
Iter: 953 loss: 4.75014e-06
Iter: 954 loss: 4.74438866e-06
Iter: 955 loss: 4.74378794e-06
Iter: 956 loss: 4.74448279e-06
Iter: 957 loss: 4.74353601e-06
Iter: 958 loss: 4.74312128e-06
Iter: 959 loss: 4.74709122e-06
Iter: 960 loss: 4.74310309e-06
Iter: 961 loss: 4.74274384e-06
Iter: 962 loss: 4.74249464e-06
Iter: 963 loss: 4.74234457e-06
Iter: 964 loss: 4.74183844e-06
Iter: 965 loss: 4.7412741e-06
Iter: 966 loss: 4.74116177e-06
Iter: 967 loss: 4.74035232e-06
Iter: 968 loss: 4.74118315e-06
Iter: 969 loss: 4.7399335e-06
Iter: 970 loss: 4.73872115e-06
Iter: 971 loss: 4.74061744e-06
Iter: 972 loss: 4.73811269e-06
Iter: 973 loss: 4.73738783e-06
Iter: 974 loss: 4.74897024e-06
Iter: 975 loss: 4.73740329e-06
Iter: 976 loss: 4.73684577e-06
Iter: 977 loss: 4.7368394e-06
Iter: 978 loss: 4.73657747e-06
Iter: 979 loss: 4.73588261e-06
Iter: 980 loss: 4.74111857e-06
Iter: 981 loss: 4.73566433e-06
Iter: 982 loss: 4.73533237e-06
Iter: 983 loss: 4.73521504e-06
Iter: 984 loss: 4.73470845e-06
Iter: 985 loss: 4.7341523e-06
Iter: 986 loss: 4.73405089e-06
Iter: 987 loss: 4.73359796e-06
Iter: 988 loss: 4.73693217e-06
Iter: 989 loss: 4.73353612e-06
Iter: 990 loss: 4.73290129e-06
Iter: 991 loss: 4.73248883e-06
Iter: 992 loss: 4.73220689e-06
Iter: 993 loss: 4.73186947e-06
Iter: 994 loss: 4.73186674e-06
Iter: 995 loss: 4.73153341e-06
Iter: 996 loss: 4.73118962e-06
Iter: 997 loss: 4.73109867e-06
Iter: 998 loss: 4.73059299e-06
Iter: 999 loss: 4.72983629e-06
Iter: 1000 loss: 4.72980309e-06
Iter: 1001 loss: 4.72904503e-06
Iter: 1002 loss: 4.72904367e-06
Iter: 1003 loss: 4.72845932e-06
Iter: 1004 loss: 4.72832835e-06
Iter: 1005 loss: 4.72796364e-06
Iter: 1006 loss: 4.72721376e-06
Iter: 1007 loss: 4.72874854e-06
Iter: 1008 loss: 4.72692818e-06
Iter: 1009 loss: 4.72669944e-06
Iter: 1010 loss: 4.72658894e-06
Iter: 1011 loss: 4.72618785e-06
Iter: 1012 loss: 4.72550937e-06
Iter: 1013 loss: 4.73637556e-06
Iter: 1014 loss: 4.72548709e-06
Iter: 1015 loss: 4.72478769e-06
Iter: 1016 loss: 4.72547254e-06
Iter: 1017 loss: 4.72445936e-06
Iter: 1018 loss: 4.72368129e-06
Iter: 1019 loss: 4.723659e-06
Iter: 1020 loss: 4.72335068e-06
Iter: 1021 loss: 4.72274951e-06
Iter: 1022 loss: 4.72274405e-06
Iter: 1023 loss: 4.72255579e-06
Iter: 1024 loss: 4.72242073e-06
Iter: 1025 loss: 4.72222337e-06
Iter: 1026 loss: 4.7217145e-06
Iter: 1027 loss: 4.72470356e-06
Iter: 1028 loss: 4.72153442e-06
Iter: 1029 loss: 4.7209669e-06
Iter: 1030 loss: 4.72097099e-06
Iter: 1031 loss: 4.72059673e-06
Iter: 1032 loss: 4.72004467e-06
Iter: 1033 loss: 4.72001875e-06
Iter: 1034 loss: 4.71936664e-06
Iter: 1035 loss: 4.72217653e-06
Iter: 1036 loss: 4.71920657e-06
Iter: 1037 loss: 4.71849489e-06
Iter: 1038 loss: 4.71795192e-06
Iter: 1039 loss: 4.71768271e-06
Iter: 1040 loss: 4.71688054e-06
Iter: 1041 loss: 4.72268266e-06
Iter: 1042 loss: 4.71677231e-06
Iter: 1043 loss: 4.71639169e-06
Iter: 1044 loss: 4.71630392e-06
Iter: 1045 loss: 4.71597468e-06
Iter: 1046 loss: 4.71509202e-06
Iter: 1047 loss: 4.72047395e-06
Iter: 1048 loss: 4.71487692e-06
Iter: 1049 loss: 4.71413932e-06
Iter: 1050 loss: 4.72299098e-06
Iter: 1051 loss: 4.71412932e-06
Iter: 1052 loss: 4.713419e-06
Iter: 1053 loss: 4.71818748e-06
Iter: 1054 loss: 4.71334806e-06
Iter: 1055 loss: 4.71296516e-06
Iter: 1056 loss: 4.71253e-06
Iter: 1057 loss: 4.71249405e-06
Iter: 1058 loss: 4.71182284e-06
Iter: 1059 loss: 4.72138208e-06
Iter: 1060 loss: 4.71180647e-06
Iter: 1061 loss: 4.71149815e-06
Iter: 1062 loss: 4.71083149e-06
Iter: 1063 loss: 4.72147e-06
Iter: 1064 loss: 4.71083831e-06
Iter: 1065 loss: 4.71023577e-06
Iter: 1066 loss: 4.71021758e-06
Iter: 1067 loss: 4.70988e-06
Iter: 1068 loss: 4.70905661e-06
Iter: 1069 loss: 4.71706335e-06
Iter: 1070 loss: 4.70894338e-06
Iter: 1071 loss: 4.70799932e-06
Iter: 1072 loss: 4.71392423e-06
Iter: 1073 loss: 4.70783152e-06
Iter: 1074 loss: 4.70706e-06
Iter: 1075 loss: 4.70892519e-06
Iter: 1076 loss: 4.70673149e-06
Iter: 1077 loss: 4.70604664e-06
Iter: 1078 loss: 4.70505165e-06
Iter: 1079 loss: 4.70500345e-06
Iter: 1080 loss: 4.70514806e-06
Iter: 1081 loss: 4.70453142e-06
Iter: 1082 loss: 4.7040221e-06
Iter: 1083 loss: 4.70396662e-06
Iter: 1084 loss: 4.70359601e-06
Iter: 1085 loss: 4.70299074e-06
Iter: 1086 loss: 4.70195619e-06
Iter: 1087 loss: 4.70195846e-06
Iter: 1088 loss: 4.70237637e-06
Iter: 1089 loss: 4.70146506e-06
Iter: 1090 loss: 4.70117402e-06
Iter: 1091 loss: 4.70054829e-06
Iter: 1092 loss: 4.71208705e-06
Iter: 1093 loss: 4.70053874e-06
Iter: 1094 loss: 4.69997576e-06
Iter: 1095 loss: 4.69994029e-06
Iter: 1096 loss: 4.69962288e-06
Iter: 1097 loss: 4.69886027e-06
Iter: 1098 loss: 4.70754276e-06
Iter: 1099 loss: 4.69880479e-06
Iter: 1100 loss: 4.69816632e-06
Iter: 1101 loss: 4.69815313e-06
Iter: 1102 loss: 4.69774159e-06
Iter: 1103 loss: 4.69691349e-06
Iter: 1104 loss: 4.71127714e-06
Iter: 1105 loss: 4.69687893e-06
Iter: 1106 loss: 4.69602037e-06
Iter: 1107 loss: 4.69655515e-06
Iter: 1108 loss: 4.69540919e-06
Iter: 1109 loss: 4.69479664e-06
Iter: 1110 loss: 4.69912129e-06
Iter: 1111 loss: 4.69472116e-06
Iter: 1112 loss: 4.69411225e-06
Iter: 1113 loss: 4.6946011e-06
Iter: 1114 loss: 4.69374709e-06
Iter: 1115 loss: 4.69325187e-06
Iter: 1116 loss: 4.69953648e-06
Iter: 1117 loss: 4.69322413e-06
Iter: 1118 loss: 4.69274528e-06
Iter: 1119 loss: 4.69222141e-06
Iter: 1120 loss: 4.69212819e-06
Iter: 1121 loss: 4.69150473e-06
Iter: 1122 loss: 4.69826728e-06
Iter: 1123 loss: 4.691512e-06
Iter: 1124 loss: 4.69082261e-06
Iter: 1125 loss: 4.6901414e-06
Iter: 1126 loss: 4.68998496e-06
Iter: 1127 loss: 4.68945473e-06
Iter: 1128 loss: 4.69501083e-06
Iter: 1129 loss: 4.68947655e-06
Iter: 1130 loss: 4.68888902e-06
Iter: 1131 loss: 4.68971712e-06
Iter: 1132 loss: 4.68858434e-06
Iter: 1133 loss: 4.68823691e-06
Iter: 1134 loss: 4.68854523e-06
Iter: 1135 loss: 4.68808594e-06
Iter: 1136 loss: 4.68746111e-06
Iter: 1137 loss: 4.68942835e-06
Iter: 1138 loss: 4.68731423e-06
Iter: 1139 loss: 4.68687267e-06
Iter: 1140 loss: 4.68606277e-06
Iter: 1141 loss: 4.70228588e-06
Iter: 1142 loss: 4.6860473e-06
Iter: 1143 loss: 4.68494363e-06
Iter: 1144 loss: 4.69072165e-06
Iter: 1145 loss: 4.68476856e-06
Iter: 1146 loss: 4.68378357e-06
Iter: 1147 loss: 4.68614144e-06
Iter: 1148 loss: 4.68344524e-06
Iter: 1149 loss: 4.6827181e-06
Iter: 1150 loss: 4.68672533e-06
Iter: 1151 loss: 4.68261806e-06
Iter: 1152 loss: 4.68189046e-06
Iter: 1153 loss: 4.68517328e-06
Iter: 1154 loss: 4.68172948e-06
Iter: 1155 loss: 4.68121198e-06
Iter: 1156 loss: 4.68186272e-06
Iter: 1157 loss: 4.68091685e-06
Iter: 1158 loss: 4.68034523e-06
Iter: 1159 loss: 4.68684e-06
Iter: 1160 loss: 4.68033704e-06
Iter: 1161 loss: 4.67990594e-06
Iter: 1162 loss: 4.67893369e-06
Iter: 1163 loss: 4.68658482e-06
Iter: 1164 loss: 4.67872724e-06
Iter: 1165 loss: 4.67882819e-06
Iter: 1166 loss: 4.67822201e-06
Iter: 1167 loss: 4.67781956e-06
Iter: 1168 loss: 4.67686323e-06
Iter: 1169 loss: 4.69007318e-06
Iter: 1170 loss: 4.67690097e-06
Iter: 1171 loss: 4.67626433e-06
Iter: 1172 loss: 4.68153212e-06
Iter: 1173 loss: 4.67617247e-06
Iter: 1174 loss: 4.67541395e-06
Iter: 1175 loss: 4.67772907e-06
Iter: 1176 loss: 4.67522932e-06
Iter: 1177 loss: 4.67480777e-06
Iter: 1178 loss: 4.67419977e-06
Iter: 1179 loss: 4.67415794e-06
Iter: 1180 loss: 4.67328346e-06
Iter: 1181 loss: 4.68031431e-06
Iter: 1182 loss: 4.67322e-06
Iter: 1183 loss: 4.67242899e-06
Iter: 1184 loss: 4.67186419e-06
Iter: 1185 loss: 4.67158316e-06
Iter: 1186 loss: 4.67107293e-06
Iter: 1187 loss: 4.67101427e-06
Iter: 1188 loss: 4.67044629e-06
Iter: 1189 loss: 4.66978872e-06
Iter: 1190 loss: 4.66975e-06
Iter: 1191 loss: 4.66909387e-06
Iter: 1192 loss: 4.66948768e-06
Iter: 1193 loss: 4.66868823e-06
Iter: 1194 loss: 4.66830807e-06
Iter: 1195 loss: 4.66824258e-06
Iter: 1196 loss: 4.6679188e-06
Iter: 1197 loss: 4.66713891e-06
Iter: 1198 loss: 4.67676546e-06
Iter: 1199 loss: 4.66711208e-06
Iter: 1200 loss: 4.66666188e-06
Iter: 1201 loss: 4.66657821e-06
Iter: 1202 loss: 4.66609617e-06
Iter: 1203 loss: 4.66566053e-06
Iter: 1204 loss: 4.66557e-06
Iter: 1205 loss: 4.66493793e-06
Iter: 1206 loss: 4.66576512e-06
Iter: 1207 loss: 4.66462552e-06
Iter: 1208 loss: 4.66370921e-06
Iter: 1209 loss: 4.66991332e-06
Iter: 1210 loss: 4.66364372e-06
Iter: 1211 loss: 4.66327037e-06
Iter: 1212 loss: 4.66258325e-06
Iter: 1213 loss: 4.67838163e-06
Iter: 1214 loss: 4.66257188e-06
Iter: 1215 loss: 4.66176607e-06
Iter: 1216 loss: 4.66771689e-06
Iter: 1217 loss: 4.66173333e-06
Iter: 1218 loss: 4.66099254e-06
Iter: 1219 loss: 4.66337588e-06
Iter: 1220 loss: 4.66077518e-06
Iter: 1221 loss: 4.66003621e-06
Iter: 1222 loss: 4.66357596e-06
Iter: 1223 loss: 4.6598534e-06
Iter: 1224 loss: 4.6592495e-06
Iter: 1225 loss: 4.65815629e-06
Iter: 1226 loss: 4.65815356e-06
Iter: 1227 loss: 4.6576024e-06
Iter: 1228 loss: 4.6575642e-06
Iter: 1229 loss: 4.65689436e-06
Iter: 1230 loss: 4.65730409e-06
Iter: 1231 loss: 4.65645917e-06
Iter: 1232 loss: 4.6559353e-06
Iter: 1233 loss: 4.65572612e-06
Iter: 1234 loss: 4.65546145e-06
Iter: 1235 loss: 4.65487665e-06
Iter: 1236 loss: 4.654853e-06
Iter: 1237 loss: 4.65450921e-06
Iter: 1238 loss: 4.65363109e-06
Iter: 1239 loss: 4.66070651e-06
Iter: 1240 loss: 4.65351059e-06
Iter: 1241 loss: 4.65312e-06
Iter: 1242 loss: 4.65296671e-06
Iter: 1243 loss: 4.65247422e-06
Iter: 1244 loss: 4.65196536e-06
Iter: 1245 loss: 4.65185076e-06
Iter: 1246 loss: 4.65119228e-06
Iter: 1247 loss: 4.65103358e-06
Iter: 1248 loss: 4.65057792e-06
Iter: 1249 loss: 4.65009089e-06
Iter: 1250 loss: 4.6500495e-06
Iter: 1251 loss: 4.64956247e-06
Iter: 1252 loss: 4.64965115e-06
Iter: 1253 loss: 4.64922e-06
Iter: 1254 loss: 4.64847153e-06
Iter: 1255 loss: 4.64876484e-06
Iter: 1256 loss: 4.64789809e-06
Iter: 1257 loss: 4.64716049e-06
Iter: 1258 loss: 4.64709956e-06
Iter: 1259 loss: 4.64659024e-06
Iter: 1260 loss: 4.64640971e-06
Iter: 1261 loss: 4.64614732e-06
Iter: 1262 loss: 4.64573623e-06
Iter: 1263 loss: 4.64481036e-06
Iter: 1264 loss: 4.65477842e-06
Iter: 1265 loss: 4.64470077e-06
Iter: 1266 loss: 4.64377854e-06
Iter: 1267 loss: 4.64809818e-06
Iter: 1268 loss: 4.64366076e-06
Iter: 1269 loss: 4.6424384e-06
Iter: 1270 loss: 4.64891855e-06
Iter: 1271 loss: 4.64230061e-06
Iter: 1272 loss: 4.64178902e-06
Iter: 1273 loss: 4.64069217e-06
Iter: 1274 loss: 4.65670382e-06
Iter: 1275 loss: 4.64063942e-06
Iter: 1276 loss: 4.64030563e-06
Iter: 1277 loss: 4.63992865e-06
Iter: 1278 loss: 4.63944934e-06
Iter: 1279 loss: 4.63850256e-06
Iter: 1280 loss: 4.65629364e-06
Iter: 1281 loss: 4.63850074e-06
Iter: 1282 loss: 4.63773358e-06
Iter: 1283 loss: 4.64273398e-06
Iter: 1284 loss: 4.63769902e-06
Iter: 1285 loss: 4.63695142e-06
Iter: 1286 loss: 4.6400728e-06
Iter: 1287 loss: 4.63674496e-06
Iter: 1288 loss: 4.63601282e-06
Iter: 1289 loss: 4.63691777e-06
Iter: 1290 loss: 4.6356181e-06
Iter: 1291 loss: 4.63484412e-06
Iter: 1292 loss: 4.63425249e-06
Iter: 1293 loss: 4.63403148e-06
Iter: 1294 loss: 4.63348e-06
Iter: 1295 loss: 4.63338e-06
Iter: 1296 loss: 4.63277047e-06
Iter: 1297 loss: 4.63199467e-06
Iter: 1298 loss: 4.63196284e-06
Iter: 1299 loss: 4.6311161e-06
Iter: 1300 loss: 4.63073502e-06
Iter: 1301 loss: 4.63032939e-06
Iter: 1302 loss: 4.63053721e-06
Iter: 1303 loss: 4.6298087e-06
Iter: 1304 loss: 4.62936032e-06
Iter: 1305 loss: 4.62819071e-06
Iter: 1306 loss: 4.63903825e-06
Iter: 1307 loss: 4.62806747e-06
Iter: 1308 loss: 4.6270934e-06
Iter: 1309 loss: 4.63558627e-06
Iter: 1310 loss: 4.62708886e-06
Iter: 1311 loss: 4.62597609e-06
Iter: 1312 loss: 4.62741264e-06
Iter: 1313 loss: 4.62539811e-06
Iter: 1314 loss: 4.62480511e-06
Iter: 1315 loss: 4.62463549e-06
Iter: 1316 loss: 4.6242435e-06
Iter: 1317 loss: 4.62338176e-06
Iter: 1318 loss: 4.63287734e-06
Iter: 1319 loss: 4.62336357e-06
Iter: 1320 loss: 4.62261369e-06
Iter: 1321 loss: 4.62392154e-06
Iter: 1322 loss: 4.62230219e-06
Iter: 1323 loss: 4.62157095e-06
Iter: 1324 loss: 4.62205253e-06
Iter: 1325 loss: 4.6211826e-06
Iter: 1326 loss: 4.62038088e-06
Iter: 1327 loss: 4.62120352e-06
Iter: 1328 loss: 4.61995705e-06
Iter: 1329 loss: 4.61879472e-06
Iter: 1330 loss: 4.62719527e-06
Iter: 1331 loss: 4.61866966e-06
Iter: 1332 loss: 4.61817854e-06
Iter: 1333 loss: 4.6173127e-06
Iter: 1334 loss: 4.61727e-06
Iter: 1335 loss: 4.61642639e-06
Iter: 1336 loss: 4.62167236e-06
Iter: 1337 loss: 4.61636319e-06
Iter: 1338 loss: 4.61543596e-06
Iter: 1339 loss: 4.6221553e-06
Iter: 1340 loss: 4.61534864e-06
Iter: 1341 loss: 4.61484024e-06
Iter: 1342 loss: 4.61351556e-06
Iter: 1343 loss: 4.62559819e-06
Iter: 1344 loss: 4.61331365e-06
Iter: 1345 loss: 4.61344644e-06
Iter: 1346 loss: 4.61269974e-06
Iter: 1347 loss: 4.61229592e-06
Iter: 1348 loss: 4.61119089e-06
Iter: 1349 loss: 4.61838044e-06
Iter: 1350 loss: 4.61094214e-06
Iter: 1351 loss: 4.60999217e-06
Iter: 1352 loss: 4.62514845e-06
Iter: 1353 loss: 4.60999308e-06
Iter: 1354 loss: 4.60916954e-06
Iter: 1355 loss: 4.61246418e-06
Iter: 1356 loss: 4.60900264e-06
Iter: 1357 loss: 4.60848059e-06
Iter: 1358 loss: 4.60973388e-06
Iter: 1359 loss: 4.60831325e-06
Iter: 1360 loss: 4.60775709e-06
Iter: 1361 loss: 4.60671527e-06
Iter: 1362 loss: 4.62951e-06
Iter: 1363 loss: 4.60673073e-06
Iter: 1364 loss: 4.60617684e-06
Iter: 1365 loss: 4.60602405e-06
Iter: 1366 loss: 4.60530373e-06
Iter: 1367 loss: 4.60419642e-06
Iter: 1368 loss: 4.60421188e-06
Iter: 1369 loss: 4.60321553e-06
Iter: 1370 loss: 4.60371893e-06
Iter: 1371 loss: 4.60259616e-06
Iter: 1372 loss: 4.60211595e-06
Iter: 1373 loss: 4.60211641e-06
Iter: 1374 loss: 4.60163938e-06
Iter: 1375 loss: 4.60098272e-06
Iter: 1376 loss: 4.60091587e-06
Iter: 1377 loss: 4.60021784e-06
Iter: 1378 loss: 4.59997636e-06
Iter: 1379 loss: 4.5995107e-06
Iter: 1380 loss: 4.59934472e-06
Iter: 1381 loss: 4.59905914e-06
Iter: 1382 loss: 4.59861531e-06
Iter: 1383 loss: 4.59739204e-06
Iter: 1384 loss: 4.60506499e-06
Iter: 1385 loss: 4.59705643e-06
Iter: 1386 loss: 4.59598778e-06
Iter: 1387 loss: 4.60264027e-06
Iter: 1388 loss: 4.59582634e-06
Iter: 1389 loss: 4.59543662e-06
Iter: 1390 loss: 4.59528974e-06
Iter: 1391 loss: 4.59487092e-06
Iter: 1392 loss: 4.59382136e-06
Iter: 1393 loss: 4.60672391e-06
Iter: 1394 loss: 4.59378e-06
Iter: 1395 loss: 4.59297371e-06
Iter: 1396 loss: 4.60299543e-06
Iter: 1397 loss: 4.59299e-06
Iter: 1398 loss: 4.59231796e-06
Iter: 1399 loss: 4.59389139e-06
Iter: 1400 loss: 4.5920824e-06
Iter: 1401 loss: 4.59117655e-06
Iter: 1402 loss: 4.59363309e-06
Iter: 1403 loss: 4.59091552e-06
Iter: 1404 loss: 4.59031435e-06
Iter: 1405 loss: 4.5891411e-06
Iter: 1406 loss: 4.61225454e-06
Iter: 1407 loss: 4.58913928e-06
Iter: 1408 loss: 4.58888371e-06
Iter: 1409 loss: 4.58855357e-06
Iter: 1410 loss: 4.58794921e-06
Iter: 1411 loss: 4.58739123e-06
Iter: 1412 loss: 4.58722025e-06
Iter: 1413 loss: 4.58650402e-06
Iter: 1414 loss: 4.5861434e-06
Iter: 1415 loss: 4.58582645e-06
Iter: 1416 loss: 4.58524391e-06
Iter: 1417 loss: 4.58512795e-06
Iter: 1418 loss: 4.58478871e-06
Iter: 1419 loss: 4.5839015e-06
Iter: 1420 loss: 4.59096736e-06
Iter: 1421 loss: 4.58373233e-06
Iter: 1422 loss: 4.58278e-06
Iter: 1423 loss: 4.58778459e-06
Iter: 1424 loss: 4.58261457e-06
Iter: 1425 loss: 4.58152726e-06
Iter: 1426 loss: 4.59010653e-06
Iter: 1427 loss: 4.58145678e-06
Iter: 1428 loss: 4.58094519e-06
Iter: 1429 loss: 4.57987153e-06
Iter: 1430 loss: 4.59901185e-06
Iter: 1431 loss: 4.57981923e-06
Iter: 1432 loss: 4.57879105e-06
Iter: 1433 loss: 4.57878195e-06
Iter: 1434 loss: 4.57803753e-06
Iter: 1435 loss: 4.58202885e-06
Iter: 1436 loss: 4.57786837e-06
Iter: 1437 loss: 4.57730039e-06
Iter: 1438 loss: 4.57727629e-06
Iter: 1439 loss: 4.57679198e-06
Iter: 1440 loss: 4.5762572e-06
Iter: 1441 loss: 4.57567421e-06
Iter: 1442 loss: 4.57557189e-06
Iter: 1443 loss: 4.57565056e-06
Iter: 1444 loss: 4.57514125e-06
Iter: 1445 loss: 4.57486203e-06
Iter: 1446 loss: 4.5739539e-06
Iter: 1447 loss: 4.57670467e-06
Iter: 1448 loss: 4.57359329e-06
Iter: 1449 loss: 4.57260103e-06
Iter: 1450 loss: 4.58035538e-06
Iter: 1451 loss: 4.57252099e-06
Iter: 1452 loss: 4.57177612e-06
Iter: 1453 loss: 4.58311843e-06
Iter: 1454 loss: 4.5717461e-06
Iter: 1455 loss: 4.57131227e-06
Iter: 1456 loss: 4.57021542e-06
Iter: 1457 loss: 4.5830061e-06
Iter: 1458 loss: 4.57015e-06
Iter: 1459 loss: 4.56987937e-06
Iter: 1460 loss: 4.56977705e-06
Iter: 1461 loss: 4.56931093e-06
Iter: 1462 loss: 4.56867383e-06
Iter: 1463 loss: 4.5686711e-06
Iter: 1464 loss: 4.56797443e-06
Iter: 1465 loss: 4.56787075e-06
Iter: 1466 loss: 4.56734688e-06
Iter: 1467 loss: 4.56713e-06
Iter: 1468 loss: 4.56693033e-06
Iter: 1469 loss: 4.5665679e-06
Iter: 1470 loss: 4.56604539e-06
Iter: 1471 loss: 4.56605176e-06
Iter: 1472 loss: 4.56529415e-06
Iter: 1473 loss: 4.56749649e-06
Iter: 1474 loss: 4.56512e-06
Iter: 1475 loss: 4.56459e-06
Iter: 1476 loss: 4.56575435e-06
Iter: 1477 loss: 4.56436646e-06
Iter: 1478 loss: 4.56377347e-06
Iter: 1479 loss: 4.5716879e-06
Iter: 1480 loss: 4.56379894e-06
Iter: 1481 loss: 4.56351654e-06
Iter: 1482 loss: 4.56268208e-06
Iter: 1483 loss: 4.56670614e-06
Iter: 1484 loss: 4.56241833e-06
Iter: 1485 loss: 4.56144517e-06
Iter: 1486 loss: 4.5669085e-06
Iter: 1487 loss: 4.56133876e-06
Iter: 1488 loss: 4.56077942e-06
Iter: 1489 loss: 4.56074122e-06
Iter: 1490 loss: 4.56039e-06
Iter: 1491 loss: 4.55965619e-06
Iter: 1492 loss: 4.56722319e-06
Iter: 1493 loss: 4.55951158e-06
Iter: 1494 loss: 4.55898544e-06
Iter: 1495 loss: 4.56333919e-06
Iter: 1496 loss: 4.55892405e-06
Iter: 1497 loss: 4.55820691e-06
Iter: 1498 loss: 4.56211865e-06
Iter: 1499 loss: 4.5580764e-06
Iter: 1500 loss: 4.55779355e-06
Iter: 1501 loss: 4.5571428e-06
Iter: 1502 loss: 4.56523958e-06
Iter: 1503 loss: 4.55712234e-06
Iter: 1504 loss: 4.55687814e-06
Iter: 1505 loss: 4.55670488e-06
Iter: 1506 loss: 4.55632289e-06
Iter: 1507 loss: 4.55632653e-06
Iter: 1508 loss: 4.55593454e-06
Iter: 1509 loss: 4.55563895e-06
Iter: 1510 loss: 4.55552572e-06
Iter: 1511 loss: 4.55529789e-06
Iter: 1512 loss: 4.55480495e-06
Iter: 1513 loss: 4.55922418e-06
Iter: 1514 loss: 4.55472491e-06
Iter: 1515 loss: 4.55432746e-06
Iter: 1516 loss: 4.55692225e-06
Iter: 1517 loss: 4.55428744e-06
Iter: 1518 loss: 4.55406507e-06
Iter: 1519 loss: 4.55353802e-06
Iter: 1520 loss: 4.56131193e-06
Iter: 1521 loss: 4.55354211e-06
Iter: 1522 loss: 4.55292229e-06
Iter: 1523 loss: 4.55369081e-06
Iter: 1524 loss: 4.55261306e-06
Iter: 1525 loss: 4.55189092e-06
Iter: 1526 loss: 4.55186773e-06
Iter: 1527 loss: 4.55157033e-06
Iter: 1528 loss: 4.5508832e-06
Iter: 1529 loss: 4.55633563e-06
Iter: 1530 loss: 4.55074314e-06
Iter: 1531 loss: 4.55012287e-06
Iter: 1532 loss: 4.55935833e-06
Iter: 1533 loss: 4.55013787e-06
Iter: 1534 loss: 4.5494603e-06
Iter: 1535 loss: 4.55151167e-06
Iter: 1536 loss: 4.5492834e-06
Iter: 1537 loss: 4.54893416e-06
Iter: 1538 loss: 4.54831752e-06
Iter: 1539 loss: 4.5483248e-06
Iter: 1540 loss: 4.5477891e-06
Iter: 1541 loss: 4.55275222e-06
Iter: 1542 loss: 4.54772e-06
Iter: 1543 loss: 4.54699693e-06
Iter: 1544 loss: 4.54903966e-06
Iter: 1545 loss: 4.54682731e-06
Iter: 1546 loss: 4.54633391e-06
Iter: 1547 loss: 4.54556402e-06
Iter: 1548 loss: 4.56493399e-06
Iter: 1549 loss: 4.54554265e-06
Iter: 1550 loss: 4.54502424e-06
Iter: 1551 loss: 4.5450297e-06
Iter: 1552 loss: 4.54436577e-06
Iter: 1553 loss: 4.5449915e-06
Iter: 1554 loss: 4.54399833e-06
Iter: 1555 loss: 4.54363317e-06
Iter: 1556 loss: 4.54319343e-06
Iter: 1557 loss: 4.54322071e-06
Iter: 1558 loss: 4.54269775e-06
Iter: 1559 loss: 4.549408e-06
Iter: 1560 loss: 4.5427023e-06
Iter: 1561 loss: 4.54218753e-06
Iter: 1562 loss: 4.54273413e-06
Iter: 1563 loss: 4.54192696e-06
Iter: 1564 loss: 4.54128713e-06
Iter: 1565 loss: 4.54192741e-06
Iter: 1566 loss: 4.54094516e-06
Iter: 1567 loss: 4.54022938e-06
Iter: 1568 loss: 4.54001656e-06
Iter: 1569 loss: 4.53961457e-06
Iter: 1570 loss: 4.53911798e-06
Iter: 1571 loss: 4.53898883e-06
Iter: 1572 loss: 4.53869052e-06
Iter: 1573 loss: 4.53777511e-06
Iter: 1574 loss: 4.54529527e-06
Iter: 1575 loss: 4.53762e-06
Iter: 1576 loss: 4.53678103e-06
Iter: 1577 loss: 4.54166138e-06
Iter: 1578 loss: 4.53667781e-06
Iter: 1579 loss: 4.53603661e-06
Iter: 1580 loss: 4.53599841e-06
Iter: 1581 loss: 4.53567327e-06
Iter: 1582 loss: 4.53475968e-06
Iter: 1583 loss: 4.53905886e-06
Iter: 1584 loss: 4.53440862e-06
Iter: 1585 loss: 4.5338e-06
Iter: 1586 loss: 4.53378198e-06
Iter: 1587 loss: 4.53304619e-06
Iter: 1588 loss: 4.53547136e-06
Iter: 1589 loss: 4.53289067e-06
Iter: 1590 loss: 4.53250686e-06
Iter: 1591 loss: 4.5317579e-06
Iter: 1592 loss: 4.54365181e-06
Iter: 1593 loss: 4.53173561e-06
Iter: 1594 loss: 4.53157827e-06
Iter: 1595 loss: 4.53134817e-06
Iter: 1596 loss: 4.53101939e-06
Iter: 1597 loss: 4.53057964e-06
Iter: 1598 loss: 4.53056e-06
Iter: 1599 loss: 4.52989752e-06
Iter: 1600 loss: 4.53153916e-06
Iter: 1601 loss: 4.52969243e-06
Iter: 1602 loss: 4.52899167e-06
Iter: 1603 loss: 4.53010216e-06
Iter: 1604 loss: 4.52869926e-06
Iter: 1605 loss: 4.52814356e-06
Iter: 1606 loss: 4.52815721e-06
Iter: 1607 loss: 4.52783843e-06
Iter: 1608 loss: 4.52706809e-06
Iter: 1609 loss: 4.53181474e-06
Iter: 1610 loss: 4.52682161e-06
Iter: 1611 loss: 4.52624317e-06
Iter: 1612 loss: 4.53575376e-06
Iter: 1613 loss: 4.52621589e-06
Iter: 1614 loss: 4.52559561e-06
Iter: 1615 loss: 4.53107623e-06
Iter: 1616 loss: 4.5255988e-06
Iter: 1617 loss: 4.52527365e-06
Iter: 1618 loss: 4.5244733e-06
Iter: 1619 loss: 4.528e-06
Iter: 1620 loss: 4.52415634e-06
Iter: 1621 loss: 4.52360473e-06
Iter: 1622 loss: 4.52352833e-06
Iter: 1623 loss: 4.52285622e-06
Iter: 1624 loss: 4.52484164e-06
Iter: 1625 loss: 4.52266158e-06
Iter: 1626 loss: 4.5223378e-06
Iter: 1627 loss: 4.52164113e-06
Iter: 1628 loss: 4.53343864e-06
Iter: 1629 loss: 4.52161521e-06
Iter: 1630 loss: 4.52125641e-06
Iter: 1631 loss: 4.52115546e-06
Iter: 1632 loss: 4.52075e-06
Iter: 1633 loss: 4.52020049e-06
Iter: 1634 loss: 4.52017957e-06
Iter: 1635 loss: 4.51961023e-06
Iter: 1636 loss: 4.52321774e-06
Iter: 1637 loss: 4.51952474e-06
Iter: 1638 loss: 4.51911637e-06
Iter: 1639 loss: 4.52324639e-06
Iter: 1640 loss: 4.519105e-06
Iter: 1641 loss: 4.51867754e-06
Iter: 1642 loss: 4.51788173e-06
Iter: 1643 loss: 4.51789128e-06
Iter: 1644 loss: 4.51718552e-06
Iter: 1645 loss: 4.51877895e-06
Iter: 1646 loss: 4.51695223e-06
Iter: 1647 loss: 4.51676215e-06
Iter: 1648 loss: 4.51660435e-06
Iter: 1649 loss: 4.51632468e-06
Iter: 1650 loss: 4.5154884e-06
Iter: 1651 loss: 4.52350559e-06
Iter: 1652 loss: 4.51544e-06
Iter: 1653 loss: 4.51471669e-06
Iter: 1654 loss: 4.51582582e-06
Iter: 1655 loss: 4.5144061e-06
Iter: 1656 loss: 4.51423693e-06
Iter: 1657 loss: 4.51398137e-06
Iter: 1658 loss: 4.51376854e-06
Iter: 1659 loss: 4.51310098e-06
Iter: 1660 loss: 4.51714e-06
Iter: 1661 loss: 4.51284177e-06
Iter: 1662 loss: 4.51212782e-06
Iter: 1663 loss: 4.51601409e-06
Iter: 1664 loss: 4.51202459e-06
Iter: 1665 loss: 4.51110191e-06
Iter: 1666 loss: 4.51621054e-06
Iter: 1667 loss: 4.51093729e-06
Iter: 1668 loss: 4.51054711e-06
Iter: 1669 loss: 4.51053438e-06
Iter: 1670 loss: 4.51022152e-06
Iter: 1671 loss: 4.50953348e-06
Iter: 1672 loss: 4.51106462e-06
Iter: 1673 loss: 4.50926e-06
Iter: 1674 loss: 4.50873586e-06
Iter: 1675 loss: 4.5171646e-06
Iter: 1676 loss: 4.50870357e-06
Iter: 1677 loss: 4.5083525e-06
Iter: 1678 loss: 4.50771267e-06
Iter: 1679 loss: 4.50770858e-06
Iter: 1680 loss: 4.50704738e-06
Iter: 1681 loss: 4.50744119e-06
Iter: 1682 loss: 4.5066563e-06
Iter: 1683 loss: 4.50571861e-06
Iter: 1684 loss: 4.51904e-06
Iter: 1685 loss: 4.50571042e-06
Iter: 1686 loss: 4.50534571e-06
Iter: 1687 loss: 4.50438711e-06
Iter: 1688 loss: 4.51626511e-06
Iter: 1689 loss: 4.50431889e-06
Iter: 1690 loss: 4.5036727e-06
Iter: 1691 loss: 4.50369134e-06
Iter: 1692 loss: 4.50296193e-06
Iter: 1693 loss: 4.50411881e-06
Iter: 1694 loss: 4.5026195e-06
Iter: 1695 loss: 4.50221114e-06
Iter: 1696 loss: 4.50143762e-06
Iter: 1697 loss: 4.51935284e-06
Iter: 1698 loss: 4.50143807e-06
Iter: 1699 loss: 4.50099333e-06
Iter: 1700 loss: 4.50093694e-06
Iter: 1701 loss: 4.50040397e-06
Iter: 1702 loss: 4.49933486e-06
Iter: 1703 loss: 4.52026052e-06
Iter: 1704 loss: 4.49934259e-06
Iter: 1705 loss: 4.498409e-06
Iter: 1706 loss: 4.499846e-06
Iter: 1707 loss: 4.49796971e-06
Iter: 1708 loss: 4.49749086e-06
Iter: 1709 loss: 4.4974272e-06
Iter: 1710 loss: 4.49704976e-06
Iter: 1711 loss: 4.49765366e-06
Iter: 1712 loss: 4.49683967e-06
Iter: 1713 loss: 4.49631261e-06
Iter: 1714 loss: 4.49575418e-06
Iter: 1715 loss: 4.49566369e-06
Iter: 1716 loss: 4.49493291e-06
Iter: 1717 loss: 4.49606705e-06
Iter: 1718 loss: 4.49458275e-06
Iter: 1719 loss: 4.49426807e-06
Iter: 1720 loss: 4.49411891e-06
Iter: 1721 loss: 4.49378513e-06
Iter: 1722 loss: 4.49272693e-06
Iter: 1723 loss: 4.49632216e-06
Iter: 1724 loss: 4.49231629e-06
Iter: 1725 loss: 4.49279423e-06
Iter: 1726 loss: 4.49180061e-06
Iter: 1727 loss: 4.49147137e-06
Iter: 1728 loss: 4.49065465e-06
Iter: 1729 loss: 4.50092284e-06
Iter: 1730 loss: 4.49058189e-06
Iter: 1731 loss: 4.48987112e-06
Iter: 1732 loss: 4.49263553e-06
Iter: 1733 loss: 4.48970832e-06
Iter: 1734 loss: 4.48906121e-06
Iter: 1735 loss: 4.49495474e-06
Iter: 1736 loss: 4.48899755e-06
Iter: 1737 loss: 4.48856235e-06
Iter: 1738 loss: 4.48766968e-06
Iter: 1739 loss: 4.50381685e-06
Iter: 1740 loss: 4.48766286e-06
Iter: 1741 loss: 4.4868857e-06
Iter: 1742 loss: 4.49574873e-06
Iter: 1743 loss: 4.48687842e-06
Iter: 1744 loss: 4.48599076e-06
Iter: 1745 loss: 4.49074741e-06
Iter: 1746 loss: 4.48588253e-06
Iter: 1747 loss: 4.48530591e-06
Iter: 1748 loss: 4.48649644e-06
Iter: 1749 loss: 4.48505625e-06
Iter: 1750 loss: 4.48441824e-06
Iter: 1751 loss: 4.48373885e-06
Iter: 1752 loss: 4.48360424e-06
Iter: 1753 loss: 4.48326864e-06
Iter: 1754 loss: 4.4831595e-06
Iter: 1755 loss: 4.48264291e-06
Iter: 1756 loss: 4.48177e-06
Iter: 1757 loss: 4.48177252e-06
Iter: 1758 loss: 4.48112405e-06
Iter: 1759 loss: 4.4897979e-06
Iter: 1760 loss: 4.48113133e-06
Iter: 1761 loss: 4.48043e-06
Iter: 1762 loss: 4.48046649e-06
Iter: 1763 loss: 4.47987441e-06
Iter: 1764 loss: 4.47929415e-06
Iter: 1765 loss: 4.47873663e-06
Iter: 1766 loss: 4.47862294e-06
Iter: 1767 loss: 4.4780295e-06
Iter: 1768 loss: 4.477939e-06
Iter: 1769 loss: 4.47751245e-06
Iter: 1770 loss: 4.47657067e-06
Iter: 1771 loss: 4.49125764e-06
Iter: 1772 loss: 4.47655566e-06
Iter: 1773 loss: 4.4756307e-06
Iter: 1774 loss: 4.47861157e-06
Iter: 1775 loss: 4.47541606e-06
Iter: 1776 loss: 4.47484354e-06
Iter: 1777 loss: 4.47482489e-06
Iter: 1778 loss: 4.47439379e-06
Iter: 1779 loss: 4.47413368e-06
Iter: 1780 loss: 4.47393222e-06
Iter: 1781 loss: 4.47319371e-06
Iter: 1782 loss: 4.47363755e-06
Iter: 1783 loss: 4.47261755e-06
Iter: 1784 loss: 4.47188222e-06
Iter: 1785 loss: 4.4737335e-06
Iter: 1786 loss: 4.47155753e-06
Iter: 1787 loss: 4.47071761e-06
Iter: 1788 loss: 4.48225683e-06
Iter: 1789 loss: 4.4706776e-06
Iter: 1790 loss: 4.47029834e-06
Iter: 1791 loss: 4.46985723e-06
Iter: 1792 loss: 4.46982676e-06
Iter: 1793 loss: 4.4691069e-06
Iter: 1794 loss: 4.47730508e-06
Iter: 1795 loss: 4.46904414e-06
Iter: 1796 loss: 4.46867944e-06
Iter: 1797 loss: 4.46786498e-06
Iter: 1798 loss: 4.47874663e-06
Iter: 1799 loss: 4.46774629e-06
Iter: 1800 loss: 4.46751346e-06
Iter: 1801 loss: 4.46736431e-06
Iter: 1802 loss: 4.46694139e-06
Iter: 1803 loss: 4.46639115e-06
Iter: 1804 loss: 4.4663866e-06
Iter: 1805 loss: 4.46568311e-06
Iter: 1806 loss: 4.46501826e-06
Iter: 1807 loss: 4.46486683e-06
Iter: 1808 loss: 4.46428476e-06
Iter: 1809 loss: 4.46419108e-06
Iter: 1810 loss: 4.46371632e-06
Iter: 1811 loss: 4.4636663e-06
Iter: 1812 loss: 4.46327567e-06
Iter: 1813 loss: 4.46268814e-06
Iter: 1814 loss: 4.46521517e-06
Iter: 1815 loss: 4.46257536e-06
Iter: 1816 loss: 4.46196054e-06
Iter: 1817 loss: 4.4616163e-06
Iter: 1818 loss: 4.46139393e-06
Iter: 1819 loss: 4.46122249e-06
Iter: 1820 loss: 4.46098693e-06
Iter: 1821 loss: 4.4606868e-06
Iter: 1822 loss: 4.45994556e-06
Iter: 1823 loss: 4.46586546e-06
Iter: 1824 loss: 4.45982e-06
Iter: 1825 loss: 4.45945898e-06
Iter: 1826 loss: 4.45938076e-06
Iter: 1827 loss: 4.45886872e-06
Iter: 1828 loss: 4.45779142e-06
Iter: 1829 loss: 4.47729917e-06
Iter: 1830 loss: 4.45777232e-06
Iter: 1831 loss: 4.45711885e-06
Iter: 1832 loss: 4.46243303e-06
Iter: 1833 loss: 4.45710521e-06
Iter: 1834 loss: 4.45640671e-06
Iter: 1835 loss: 4.45915066e-06
Iter: 1836 loss: 4.45626029e-06
Iter: 1837 loss: 4.45586329e-06
Iter: 1838 loss: 4.45517162e-06
Iter: 1839 loss: 4.45515934e-06
Iter: 1840 loss: 4.4546814e-06
Iter: 1841 loss: 4.45467504e-06
Iter: 1842 loss: 4.45423575e-06
Iter: 1843 loss: 4.45374235e-06
Iter: 1844 loss: 4.45370279e-06
Iter: 1845 loss: 4.45282285e-06
Iter: 1846 loss: 4.45440219e-06
Iter: 1847 loss: 4.45245314e-06
Iter: 1848 loss: 4.4514145e-06
Iter: 1849 loss: 4.45325441e-06
Iter: 1850 loss: 4.45093474e-06
Iter: 1851 loss: 4.4502176e-06
Iter: 1852 loss: 4.45520345e-06
Iter: 1853 loss: 4.4501021e-06
Iter: 1854 loss: 4.44925945e-06
Iter: 1855 loss: 4.45182559e-06
Iter: 1856 loss: 4.44893976e-06
Iter: 1857 loss: 4.44852731e-06
Iter: 1858 loss: 4.44847183e-06
Iter: 1859 loss: 4.4481676e-06
Iter: 1860 loss: 4.4473345e-06
Iter: 1861 loss: 4.45086516e-06
Iter: 1862 loss: 4.44718216e-06
Iter: 1863 loss: 4.4466542e-06
Iter: 1864 loss: 4.44579746e-06
Iter: 1865 loss: 4.46768081e-06
Iter: 1866 loss: 4.4458252e-06
Iter: 1867 loss: 4.44520037e-06
Iter: 1868 loss: 4.44509942e-06
Iter: 1869 loss: 4.44468196e-06
Iter: 1870 loss: 4.44355874e-06
Iter: 1871 loss: 4.45531714e-06
Iter: 1872 loss: 4.44342641e-06
Iter: 1873 loss: 4.44237548e-06
Iter: 1874 loss: 4.44733359e-06
Iter: 1875 loss: 4.44221814e-06
Iter: 1876 loss: 4.4416588e-06
Iter: 1877 loss: 4.44154466e-06
Iter: 1878 loss: 4.44115267e-06
Iter: 1879 loss: 4.44047919e-06
Iter: 1880 loss: 4.44046236e-06
Iter: 1881 loss: 4.43968838e-06
Iter: 1882 loss: 4.44453235e-06
Iter: 1883 loss: 4.43958925e-06
Iter: 1884 loss: 4.43899535e-06
Iter: 1885 loss: 4.43837234e-06
Iter: 1886 loss: 4.43828549e-06
Iter: 1887 loss: 4.43772751e-06
Iter: 1888 loss: 4.43755471e-06
Iter: 1889 loss: 4.43713179e-06
Iter: 1890 loss: 4.43591853e-06
Iter: 1891 loss: 4.44700163e-06
Iter: 1892 loss: 4.43576846e-06
Iter: 1893 loss: 4.43566796e-06
Iter: 1894 loss: 4.43528143e-06
Iter: 1895 loss: 4.43478348e-06
Iter: 1896 loss: 4.43374392e-06
Iter: 1897 loss: 4.44847547e-06
Iter: 1898 loss: 4.43370709e-06
Iter: 1899 loss: 4.432889e-06
Iter: 1900 loss: 4.43501267e-06
Iter: 1901 loss: 4.43259705e-06
Iter: 1902 loss: 4.43197177e-06
Iter: 1903 loss: 4.43196768e-06
Iter: 1904 loss: 4.43165209e-06
Iter: 1905 loss: 4.4308731e-06
Iter: 1906 loss: 4.43700719e-06
Iter: 1907 loss: 4.43074396e-06
Iter: 1908 loss: 4.4300491e-06
Iter: 1909 loss: 4.43004956e-06
Iter: 1910 loss: 4.42931605e-06
Iter: 1911 loss: 4.42961391e-06
Iter: 1912 loss: 4.42878172e-06
Iter: 1913 loss: 4.4281278e-06
Iter: 1914 loss: 4.43041608e-06
Iter: 1915 loss: 4.42795499e-06
Iter: 1916 loss: 4.42732608e-06
Iter: 1917 loss: 4.42758e-06
Iter: 1918 loss: 4.42689361e-06
Iter: 1919 loss: 4.42641158e-06
Iter: 1920 loss: 4.43250474e-06
Iter: 1921 loss: 4.42638157e-06
Iter: 1922 loss: 4.42581177e-06
Iter: 1923 loss: 4.4263561e-06
Iter: 1924 loss: 4.42547298e-06
Iter: 1925 loss: 4.42509099e-06
Iter: 1926 loss: 4.42443343e-06
Iter: 1927 loss: 4.42440523e-06
Iter: 1928 loss: 4.42413557e-06
Iter: 1929 loss: 4.42390865e-06
Iter: 1930 loss: 4.42353348e-06
Iter: 1931 loss: 4.42263035e-06
Iter: 1932 loss: 4.43085e-06
Iter: 1933 loss: 4.42252531e-06
Iter: 1934 loss: 4.42163e-06
Iter: 1935 loss: 4.42446344e-06
Iter: 1936 loss: 4.42138662e-06
Iter: 1937 loss: 4.42052715e-06
Iter: 1938 loss: 4.42050896e-06
Iter: 1939 loss: 4.42021155e-06
Iter: 1940 loss: 4.41933207e-06
Iter: 1941 loss: 4.43060799e-06
Iter: 1942 loss: 4.41930342e-06
Iter: 1943 loss: 4.4186545e-06
Iter: 1944 loss: 4.42221062e-06
Iter: 1945 loss: 4.418549e-06
Iter: 1946 loss: 4.4180133e-06
Iter: 1947 loss: 4.41802877e-06
Iter: 1948 loss: 4.41770499e-06
Iter: 1949 loss: 4.41694601e-06
Iter: 1950 loss: 4.42581768e-06
Iter: 1951 loss: 4.41686507e-06
Iter: 1952 loss: 4.41632119e-06
Iter: 1953 loss: 4.41634484e-06
Iter: 1954 loss: 4.41588918e-06
Iter: 1955 loss: 4.4174194e-06
Iter: 1956 loss: 4.41578277e-06
Iter: 1957 loss: 4.41532029e-06
Iter: 1958 loss: 4.41585144e-06
Iter: 1959 loss: 4.41509746e-06
Iter: 1960 loss: 4.41466182e-06
Iter: 1961 loss: 4.41423799e-06
Iter: 1962 loss: 4.41418752e-06
Iter: 1963 loss: 4.4142439e-06
Iter: 1964 loss: 4.41388738e-06
Iter: 1965 loss: 4.41371049e-06
Iter: 1966 loss: 4.41315296e-06
Iter: 1967 loss: 4.41416068e-06
Iter: 1968 loss: 4.41280099e-06
Iter: 1969 loss: 4.4118915e-06
Iter: 1970 loss: 4.41304746e-06
Iter: 1971 loss: 4.41142492e-06
Iter: 1972 loss: 4.4107e-06
Iter: 1973 loss: 4.41888142e-06
Iter: 1974 loss: 4.41067095e-06
Iter: 1975 loss: 4.40984877e-06
Iter: 1976 loss: 4.41365546e-06
Iter: 1977 loss: 4.40965232e-06
Iter: 1978 loss: 4.40935855e-06
Iter: 1979 loss: 4.40896611e-06
Iter: 1980 loss: 4.40891154e-06
Iter: 1981 loss: 4.40845497e-06
Iter: 1982 loss: 4.41135035e-06
Iter: 1983 loss: 4.40845088e-06
Iter: 1984 loss: 4.40780377e-06
Iter: 1985 loss: 4.40895656e-06
Iter: 1986 loss: 4.40758367e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script80
+ '[' -r STOP.script80 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0.8
+ date
Mon Nov  2 10:28:55 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff07006b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff070175ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff070175e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0701758c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0700dc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0700dc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0245fb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff07000e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff07000e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0245fb7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff024441158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0245158c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0245156a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0244087b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff024512488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0242e6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff02430b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff024585510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff02449c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff02449cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0244b9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0244b98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff02439b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0243be730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0243be840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff024373378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0243be048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0245a7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0245a7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff02459a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff02423c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0242bd1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0242bd378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0240ab730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff02409aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff024024e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.000110840556
test_loss: 0.00011164363
train_loss: 5.018834e-05
test_loss: 5.1531686e-05
train_loss: 3.763741e-05
test_loss: 3.8829094e-05
train_loss: 3.183205e-05
test_loss: 3.257144e-05
train_loss: 2.6738124e-05
test_loss: 2.7612961e-05
train_loss: 2.6812182e-05
test_loss: 2.6245876e-05
train_loss: 2.2030512e-05
test_loss: 2.322338e-05
train_loss: 2.265193e-05
test_loss: 2.2808847e-05
train_loss: 2.058542e-05
test_loss: 2.0061896e-05
train_loss: 1.9356257e-05
test_loss: 1.9638659e-05
train_loss: 1.7076398e-05
test_loss: 1.8763621e-05
train_loss: 1.8499168e-05
test_loss: 1.7641303e-05
train_loss: 1.7449891e-05
test_loss: 1.7366712e-05
train_loss: 1.5311976e-05
test_loss: 1.6441885e-05
train_loss: 1.5614327e-05
test_loss: 1.6066899e-05
train_loss: 1.3348181e-05
test_loss: 1.5742346e-05
train_loss: 1.3901568e-05
test_loss: 1.5775235e-05
train_loss: 1.3435518e-05
test_loss: 1.4945982e-05
train_loss: 1.31414945e-05
test_loss: 1.4327831e-05
train_loss: 1.3621717e-05
test_loss: 1.4335505e-05
train_loss: 1.3750631e-05
test_loss: 1.3942823e-05
train_loss: 1.299424e-05
test_loss: 1.3984981e-05
train_loss: 1.253644e-05
test_loss: 1.345122e-05
train_loss: 1.2758319e-05
test_loss: 1.3229103e-05
train_loss: 1.1492744e-05
test_loss: 1.314862e-05
train_loss: 1.0886415e-05
test_loss: 1.2872368e-05
train_loss: 1.2192499e-05
test_loss: 1.2601171e-05
train_loss: 1.1250667e-05
test_loss: 1.2446103e-05
train_loss: 1.0766375e-05
test_loss: 1.256062e-05
train_loss: 1.0210021e-05
test_loss: 1.2273548e-05
train_loss: 1.119811e-05
test_loss: 1.2113755e-05
train_loss: 1.1778667e-05
test_loss: 1.2059328e-05
train_loss: 1.06448015e-05
test_loss: 1.2018799e-05
train_loss: 9.51434e-06
test_loss: 1.1994785e-05
train_loss: 1.0597596e-05
test_loss: 1.1876305e-05
train_loss: 1.06939315e-05
test_loss: 1.183786e-05
train_loss: 1.0979727e-05
test_loss: 1.1705721e-05
train_loss: 1.0784923e-05
test_loss: 1.1710317e-05
train_loss: 1.0194955e-05
test_loss: 1.1609615e-05
train_loss: 1.0364964e-05
test_loss: 1.1565663e-05
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb665416268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6654c7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6654208c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6654e5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6654f0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6386cd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb63869dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6386d82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb63865c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb63865ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6653947b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6653948c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6103db8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6385f4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb63860b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb638615d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb63861d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb63861d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6103470d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb610347d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6102d9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6102d9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb610232730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6102d9ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb61023d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6102246a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6101df730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6101df158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6101c2b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb610180a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6101046a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6100b0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6100ae2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6100cfd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb61027ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb610277f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 9.68948734e-06
Iter: 2 loss: 9.63135335e-06
Iter: 3 loss: 1.03833327e-05
Iter: 4 loss: 9.63087223e-06
Iter: 5 loss: 9.59811678e-06
Iter: 6 loss: 9.70137262e-06
Iter: 7 loss: 9.58854616e-06
Iter: 8 loss: 9.56549866e-06
Iter: 9 loss: 9.58274177e-06
Iter: 10 loss: 9.55134601e-06
Iter: 11 loss: 9.52365917e-06
Iter: 12 loss: 9.69372377e-06
Iter: 13 loss: 9.52021583e-06
Iter: 14 loss: 9.49757e-06
Iter: 15 loss: 9.55013638e-06
Iter: 16 loss: 9.48912748e-06
Iter: 17 loss: 9.47132139e-06
Iter: 18 loss: 9.49431524e-06
Iter: 19 loss: 9.46217824e-06
Iter: 20 loss: 9.4357838e-06
Iter: 21 loss: 9.54540337e-06
Iter: 22 loss: 9.43014857e-06
Iter: 23 loss: 9.41574217e-06
Iter: 24 loss: 9.40390237e-06
Iter: 25 loss: 9.39973e-06
Iter: 26 loss: 9.38406083e-06
Iter: 27 loss: 9.38320954e-06
Iter: 28 loss: 9.37496497e-06
Iter: 29 loss: 9.35235403e-06
Iter: 30 loss: 9.48473644e-06
Iter: 31 loss: 9.34598756e-06
Iter: 32 loss: 9.31505656e-06
Iter: 33 loss: 9.37743062e-06
Iter: 34 loss: 9.30249826e-06
Iter: 35 loss: 9.27261317e-06
Iter: 36 loss: 9.40268546e-06
Iter: 37 loss: 9.26652592e-06
Iter: 38 loss: 9.24410506e-06
Iter: 39 loss: 9.3280687e-06
Iter: 40 loss: 9.23863536e-06
Iter: 41 loss: 9.22686e-06
Iter: 42 loss: 9.22670552e-06
Iter: 43 loss: 9.2140308e-06
Iter: 44 loss: 9.24694632e-06
Iter: 45 loss: 9.2097107e-06
Iter: 46 loss: 9.20161619e-06
Iter: 47 loss: 9.20791e-06
Iter: 48 loss: 9.19672402e-06
Iter: 49 loss: 9.18727892e-06
Iter: 50 loss: 9.29707676e-06
Iter: 51 loss: 9.18720798e-06
Iter: 52 loss: 9.18232e-06
Iter: 53 loss: 9.17938087e-06
Iter: 54 loss: 9.17738907e-06
Iter: 55 loss: 9.16902172e-06
Iter: 56 loss: 9.21575611e-06
Iter: 57 loss: 9.16782938e-06
Iter: 58 loss: 9.16116e-06
Iter: 59 loss: 9.18414298e-06
Iter: 60 loss: 9.15941746e-06
Iter: 61 loss: 9.15422061e-06
Iter: 62 loss: 9.14726843e-06
Iter: 63 loss: 9.14690736e-06
Iter: 64 loss: 9.13886288e-06
Iter: 65 loss: 9.13874737e-06
Iter: 66 loss: 9.13480926e-06
Iter: 67 loss: 9.1243237e-06
Iter: 68 loss: 9.19883405e-06
Iter: 69 loss: 9.12193536e-06
Iter: 70 loss: 9.10891e-06
Iter: 71 loss: 9.15955479e-06
Iter: 72 loss: 9.10590916e-06
Iter: 73 loss: 9.0970243e-06
Iter: 74 loss: 9.12870564e-06
Iter: 75 loss: 9.09462869e-06
Iter: 76 loss: 9.08524453e-06
Iter: 77 loss: 9.09368282e-06
Iter: 78 loss: 9.07971116e-06
Iter: 79 loss: 9.0773774e-06
Iter: 80 loss: 9.07531103e-06
Iter: 81 loss: 9.06991772e-06
Iter: 82 loss: 9.06442438e-06
Iter: 83 loss: 9.06340392e-06
Iter: 84 loss: 9.05892375e-06
Iter: 85 loss: 9.10829294e-06
Iter: 86 loss: 9.05884554e-06
Iter: 87 loss: 9.05436718e-06
Iter: 88 loss: 9.05989145e-06
Iter: 89 loss: 9.05204615e-06
Iter: 90 loss: 9.04828448e-06
Iter: 91 loss: 9.0538806e-06
Iter: 92 loss: 9.04641456e-06
Iter: 93 loss: 9.04095123e-06
Iter: 94 loss: 9.06441892e-06
Iter: 95 loss: 9.03983801e-06
Iter: 96 loss: 9.03632554e-06
Iter: 97 loss: 9.03574619e-06
Iter: 98 loss: 9.03327509e-06
Iter: 99 loss: 9.02801e-06
Iter: 100 loss: 9.05978595e-06
Iter: 101 loss: 9.02737156e-06
Iter: 102 loss: 9.02339616e-06
Iter: 103 loss: 9.04234912e-06
Iter: 104 loss: 9.02261672e-06
Iter: 105 loss: 9.01955536e-06
Iter: 106 loss: 9.01354e-06
Iter: 107 loss: 9.1329257e-06
Iter: 108 loss: 9.01345084e-06
Iter: 109 loss: 9.00709256e-06
Iter: 110 loss: 9.01166914e-06
Iter: 111 loss: 9.0031408e-06
Iter: 112 loss: 8.99543193e-06
Iter: 113 loss: 9.02553802e-06
Iter: 114 loss: 8.99367114e-06
Iter: 115 loss: 8.98785947e-06
Iter: 116 loss: 9.03337877e-06
Iter: 117 loss: 8.98739472e-06
Iter: 118 loss: 8.98322469e-06
Iter: 119 loss: 8.99265069e-06
Iter: 120 loss: 8.98165672e-06
Iter: 121 loss: 8.97891823e-06
Iter: 122 loss: 8.97850714e-06
Iter: 123 loss: 8.97664268e-06
Iter: 124 loss: 8.97188875e-06
Iter: 125 loss: 9.01168096e-06
Iter: 126 loss: 8.97106656e-06
Iter: 127 loss: 8.96944e-06
Iter: 128 loss: 8.96804158e-06
Iter: 129 loss: 8.96624e-06
Iter: 130 loss: 8.96229449e-06
Iter: 131 loss: 9.0171643e-06
Iter: 132 loss: 8.9621044e-06
Iter: 133 loss: 8.95921e-06
Iter: 134 loss: 8.95888752e-06
Iter: 135 loss: 8.95674293e-06
Iter: 136 loss: 8.95372796e-06
Iter: 137 loss: 8.95357789e-06
Iter: 138 loss: 8.95112e-06
Iter: 139 loss: 8.97785776e-06
Iter: 140 loss: 8.95109e-06
Iter: 141 loss: 8.94860932e-06
Iter: 142 loss: 8.95024641e-06
Iter: 143 loss: 8.94701589e-06
Iter: 144 loss: 8.94360528e-06
Iter: 145 loss: 8.94954246e-06
Iter: 146 loss: 8.94209097e-06
Iter: 147 loss: 8.93941797e-06
Iter: 148 loss: 8.93661854e-06
Iter: 149 loss: 8.93611468e-06
Iter: 150 loss: 8.93156448e-06
Iter: 151 loss: 8.93976085e-06
Iter: 152 loss: 8.92959815e-06
Iter: 153 loss: 8.92593926e-06
Iter: 154 loss: 8.96144411e-06
Iter: 155 loss: 8.92581829e-06
Iter: 156 loss: 8.92303524e-06
Iter: 157 loss: 8.96623351e-06
Iter: 158 loss: 8.92303251e-06
Iter: 159 loss: 8.92137086e-06
Iter: 160 loss: 8.91827221e-06
Iter: 161 loss: 8.97898e-06
Iter: 162 loss: 8.9182613e-06
Iter: 163 loss: 8.91655691e-06
Iter: 164 loss: 8.9160967e-06
Iter: 165 loss: 8.91475065e-06
Iter: 166 loss: 8.91161562e-06
Iter: 167 loss: 8.95139056e-06
Iter: 168 loss: 8.91144555e-06
Iter: 169 loss: 8.91005129e-06
Iter: 170 loss: 8.90945e-06
Iter: 171 loss: 8.90843694e-06
Iter: 172 loss: 8.90601e-06
Iter: 173 loss: 8.93769447e-06
Iter: 174 loss: 8.90586489e-06
Iter: 175 loss: 8.90362389e-06
Iter: 176 loss: 8.93524e-06
Iter: 177 loss: 8.90363117e-06
Iter: 178 loss: 8.90141291e-06
Iter: 179 loss: 8.90285082e-06
Iter: 180 loss: 8.90004048e-06
Iter: 181 loss: 8.89779221e-06
Iter: 182 loss: 8.90174942e-06
Iter: 183 loss: 8.89690091e-06
Iter: 184 loss: 8.8943807e-06
Iter: 185 loss: 8.89413423e-06
Iter: 186 loss: 8.89229614e-06
Iter: 187 loss: 8.88955219e-06
Iter: 188 loss: 8.89337389e-06
Iter: 189 loss: 8.88825e-06
Iter: 190 loss: 8.88516843e-06
Iter: 191 loss: 8.89192415e-06
Iter: 192 loss: 8.88400427e-06
Iter: 193 loss: 8.88023897e-06
Iter: 194 loss: 8.88845261e-06
Iter: 195 loss: 8.8787674e-06
Iter: 196 loss: 8.87567694e-06
Iter: 197 loss: 8.89129842e-06
Iter: 198 loss: 8.87509668e-06
Iter: 199 loss: 8.87438364e-06
Iter: 200 loss: 8.8738625e-06
Iter: 201 loss: 8.87261922e-06
Iter: 202 loss: 8.86998714e-06
Iter: 203 loss: 8.9144678e-06
Iter: 204 loss: 8.86990165e-06
Iter: 205 loss: 8.8693123e-06
Iter: 206 loss: 8.86877933e-06
Iter: 207 loss: 8.86779253e-06
Iter: 208 loss: 8.86527141e-06
Iter: 209 loss: 8.88071099e-06
Iter: 210 loss: 8.86456382e-06
Iter: 211 loss: 8.8628467e-06
Iter: 212 loss: 8.86267208e-06
Iter: 213 loss: 8.86092312e-06
Iter: 214 loss: 8.86076577e-06
Iter: 215 loss: 8.85949248e-06
Iter: 216 loss: 8.85794907e-06
Iter: 217 loss: 8.86074486e-06
Iter: 218 loss: 8.85735517e-06
Iter: 219 loss: 8.85534701e-06
Iter: 220 loss: 8.86635371e-06
Iter: 221 loss: 8.85505324e-06
Iter: 222 loss: 8.85340796e-06
Iter: 223 loss: 8.85594272e-06
Iter: 224 loss: 8.85258305e-06
Iter: 225 loss: 8.85093e-06
Iter: 226 loss: 8.84898509e-06
Iter: 227 loss: 8.84878136e-06
Iter: 228 loss: 8.84606197e-06
Iter: 229 loss: 8.84762449e-06
Iter: 230 loss: 8.84433666e-06
Iter: 231 loss: 8.84133e-06
Iter: 232 loss: 8.86439e-06
Iter: 233 loss: 8.84110705e-06
Iter: 234 loss: 8.83979101e-06
Iter: 235 loss: 8.83973735e-06
Iter: 236 loss: 8.83813482e-06
Iter: 237 loss: 8.83704524e-06
Iter: 238 loss: 8.83644771e-06
Iter: 239 loss: 8.83477514e-06
Iter: 240 loss: 8.84739165e-06
Iter: 241 loss: 8.83466782e-06
Iter: 242 loss: 8.8327406e-06
Iter: 243 loss: 8.83303164e-06
Iter: 244 loss: 8.83132634e-06
Iter: 245 loss: 8.8298184e-06
Iter: 246 loss: 8.83175926e-06
Iter: 247 loss: 8.8290717e-06
Iter: 248 loss: 8.82677523e-06
Iter: 249 loss: 8.83858684e-06
Iter: 250 loss: 8.82646418e-06
Iter: 251 loss: 8.8256038e-06
Iter: 252 loss: 8.8241959e-06
Iter: 253 loss: 8.8241668e-06
Iter: 254 loss: 8.82268341e-06
Iter: 255 loss: 8.8226534e-06
Iter: 256 loss: 8.82149e-06
Iter: 257 loss: 8.81998312e-06
Iter: 258 loss: 8.81994e-06
Iter: 259 loss: 8.8179313e-06
Iter: 260 loss: 8.82484801e-06
Iter: 261 loss: 8.81748201e-06
Iter: 262 loss: 8.81549931e-06
Iter: 263 loss: 8.8146935e-06
Iter: 264 loss: 8.81371216e-06
Iter: 265 loss: 8.81141932e-06
Iter: 266 loss: 8.81871074e-06
Iter: 267 loss: 8.81073538e-06
Iter: 268 loss: 8.81022243e-06
Iter: 269 loss: 8.80963125e-06
Iter: 270 loss: 8.80871085e-06
Iter: 271 loss: 8.80676e-06
Iter: 272 loss: 8.8353454e-06
Iter: 273 loss: 8.80666266e-06
Iter: 274 loss: 8.80552761e-06
Iter: 275 loss: 8.80538119e-06
Iter: 276 loss: 8.80437074e-06
Iter: 277 loss: 8.80232074e-06
Iter: 278 loss: 8.83757821e-06
Iter: 279 loss: 8.80225707e-06
Iter: 280 loss: 8.80091739e-06
Iter: 281 loss: 8.8008419e-06
Iter: 282 loss: 8.7994631e-06
Iter: 283 loss: 8.79911204e-06
Iter: 284 loss: 8.79819254e-06
Iter: 285 loss: 8.79686195e-06
Iter: 286 loss: 8.79746221e-06
Iter: 287 loss: 8.79589788e-06
Iter: 288 loss: 8.79427535e-06
Iter: 289 loss: 8.79428717e-06
Iter: 290 loss: 8.79341587e-06
Iter: 291 loss: 8.79132767e-06
Iter: 292 loss: 8.8153e-06
Iter: 293 loss: 8.79118306e-06
Iter: 294 loss: 8.78893934e-06
Iter: 295 loss: 8.80987136e-06
Iter: 296 loss: 8.78883657e-06
Iter: 297 loss: 8.78710762e-06
Iter: 298 loss: 8.78678475e-06
Iter: 299 loss: 8.78562696e-06
Iter: 300 loss: 8.78438095e-06
Iter: 301 loss: 8.78428636e-06
Iter: 302 loss: 8.78311948e-06
Iter: 303 loss: 8.7854587e-06
Iter: 304 loss: 8.78262745e-06
Iter: 305 loss: 8.78171886e-06
Iter: 306 loss: 8.781828e-06
Iter: 307 loss: 8.78098763e-06
Iter: 308 loss: 8.77914863e-06
Iter: 309 loss: 8.78394349e-06
Iter: 310 loss: 8.77855109e-06
Iter: 311 loss: 8.77751e-06
Iter: 312 loss: 8.7776807e-06
Iter: 313 loss: 8.77674e-06
Iter: 314 loss: 8.77544335e-06
Iter: 315 loss: 8.79204e-06
Iter: 316 loss: 8.77537059e-06
Iter: 317 loss: 8.77451839e-06
Iter: 318 loss: 8.77263301e-06
Iter: 319 loss: 8.80110747e-06
Iter: 320 loss: 8.77259117e-06
Iter: 321 loss: 8.77184539e-06
Iter: 322 loss: 8.77151433e-06
Iter: 323 loss: 8.77054663e-06
Iter: 324 loss: 8.76913509e-06
Iter: 325 loss: 8.76911417e-06
Iter: 326 loss: 8.7676417e-06
Iter: 327 loss: 8.76738432e-06
Iter: 328 loss: 8.76632112e-06
Iter: 329 loss: 8.76429658e-06
Iter: 330 loss: 8.78440824e-06
Iter: 331 loss: 8.76423564e-06
Iter: 332 loss: 8.7628905e-06
Iter: 333 loss: 8.76332524e-06
Iter: 334 loss: 8.76199192e-06
Iter: 335 loss: 8.76111062e-06
Iter: 336 loss: 8.76090053e-06
Iter: 337 loss: 8.76021477e-06
Iter: 338 loss: 8.75878322e-06
Iter: 339 loss: 8.78236e-06
Iter: 340 loss: 8.75873411e-06
Iter: 341 loss: 8.75754813e-06
Iter: 342 loss: 8.75753449e-06
Iter: 343 loss: 8.75662499e-06
Iter: 344 loss: 8.75478872e-06
Iter: 345 loss: 8.78857827e-06
Iter: 346 loss: 8.75474507e-06
Iter: 347 loss: 8.75386104e-06
Iter: 348 loss: 8.75373644e-06
Iter: 349 loss: 8.75272053e-06
Iter: 350 loss: 8.75100613e-06
Iter: 351 loss: 8.75099067e-06
Iter: 352 loss: 8.74958641e-06
Iter: 353 loss: 8.75190744e-06
Iter: 354 loss: 8.7489334e-06
Iter: 355 loss: 8.747842e-06
Iter: 356 loss: 8.74782836e-06
Iter: 357 loss: 8.7469889e-06
Iter: 358 loss: 8.74516e-06
Iter: 359 loss: 8.77041566e-06
Iter: 360 loss: 8.74504258e-06
Iter: 361 loss: 8.74326906e-06
Iter: 362 loss: 8.74598118e-06
Iter: 363 loss: 8.74245779e-06
Iter: 364 loss: 8.74054786e-06
Iter: 365 loss: 8.75776914e-06
Iter: 366 loss: 8.74049238e-06
Iter: 367 loss: 8.73901809e-06
Iter: 368 loss: 8.74310354e-06
Iter: 369 loss: 8.73851423e-06
Iter: 370 loss: 8.73750105e-06
Iter: 371 loss: 8.73746285e-06
Iter: 372 loss: 8.73681165e-06
Iter: 373 loss: 8.73539193e-06
Iter: 374 loss: 8.75598744e-06
Iter: 375 loss: 8.73530371e-06
Iter: 376 loss: 8.73404497e-06
Iter: 377 loss: 8.73405315e-06
Iter: 378 loss: 8.73312092e-06
Iter: 379 loss: 8.73112185e-06
Iter: 380 loss: 8.75902879e-06
Iter: 381 loss: 8.73098907e-06
Iter: 382 loss: 8.73010777e-06
Iter: 383 loss: 8.72982127e-06
Iter: 384 loss: 8.72887085e-06
Iter: 385 loss: 8.72811142e-06
Iter: 386 loss: 8.72777127e-06
Iter: 387 loss: 8.72657711e-06
Iter: 388 loss: 8.72702549e-06
Iter: 389 loss: 8.72582496e-06
Iter: 390 loss: 8.72454257e-06
Iter: 391 loss: 8.72452893e-06
Iter: 392 loss: 8.72390137e-06
Iter: 393 loss: 8.72212058e-06
Iter: 394 loss: 8.72790315e-06
Iter: 395 loss: 8.72131659e-06
Iter: 396 loss: 8.71868451e-06
Iter: 397 loss: 8.7356093e-06
Iter: 398 loss: 8.7184344e-06
Iter: 399 loss: 8.71648535e-06
Iter: 400 loss: 8.72605051e-06
Iter: 401 loss: 8.7162507e-06
Iter: 402 loss: 8.71478551e-06
Iter: 403 loss: 8.72564669e-06
Iter: 404 loss: 8.71463635e-06
Iter: 405 loss: 8.71366319e-06
Iter: 406 loss: 8.72290184e-06
Iter: 407 loss: 8.71355587e-06
Iter: 408 loss: 8.71268367e-06
Iter: 409 loss: 8.71151497e-06
Iter: 410 loss: 8.71135671e-06
Iter: 411 loss: 8.71037719e-06
Iter: 412 loss: 8.71035354e-06
Iter: 413 loss: 8.70929216e-06
Iter: 414 loss: 8.70762597e-06
Iter: 415 loss: 8.70764779e-06
Iter: 416 loss: 8.70617714e-06
Iter: 417 loss: 8.71501925e-06
Iter: 418 loss: 8.7059525e-06
Iter: 419 loss: 8.70442091e-06
Iter: 420 loss: 8.71165685e-06
Iter: 421 loss: 8.70405347e-06
Iter: 422 loss: 8.70321128e-06
Iter: 423 loss: 8.70217536e-06
Iter: 424 loss: 8.70212716e-06
Iter: 425 loss: 8.70122767e-06
Iter: 426 loss: 8.70113945e-06
Iter: 427 loss: 8.70035728e-06
Iter: 428 loss: 8.69850737e-06
Iter: 429 loss: 8.72054534e-06
Iter: 430 loss: 8.69831456e-06
Iter: 431 loss: 8.69653e-06
Iter: 432 loss: 8.69659743e-06
Iter: 433 loss: 8.69512e-06
Iter: 434 loss: 8.69287e-06
Iter: 435 loss: 8.71011071e-06
Iter: 436 loss: 8.69268297e-06
Iter: 437 loss: 8.69112409e-06
Iter: 438 loss: 8.70032818e-06
Iter: 439 loss: 8.69096129e-06
Iter: 440 loss: 8.6895634e-06
Iter: 441 loss: 8.69367523e-06
Iter: 442 loss: 8.68905772e-06
Iter: 443 loss: 8.68780717e-06
Iter: 444 loss: 8.70499389e-06
Iter: 445 loss: 8.68786174e-06
Iter: 446 loss: 8.68679854e-06
Iter: 447 loss: 8.68551524e-06
Iter: 448 loss: 8.6854343e-06
Iter: 449 loss: 8.68434199e-06
Iter: 450 loss: 8.70024633e-06
Iter: 451 loss: 8.68436473e-06
Iter: 452 loss: 8.68334791e-06
Iter: 453 loss: 8.68256484e-06
Iter: 454 loss: 8.68218831e-06
Iter: 455 loss: 8.68102688e-06
Iter: 456 loss: 8.68462485e-06
Iter: 457 loss: 8.6807031e-06
Iter: 458 loss: 8.67944163e-06
Iter: 459 loss: 8.68825e-06
Iter: 460 loss: 8.67928247e-06
Iter: 461 loss: 8.67849758e-06
Iter: 462 loss: 8.67694689e-06
Iter: 463 loss: 8.70607801e-06
Iter: 464 loss: 8.67687595e-06
Iter: 465 loss: 8.67559447e-06
Iter: 466 loss: 8.67548897e-06
Iter: 467 loss: 8.67454582e-06
Iter: 468 loss: 8.67258859e-06
Iter: 469 loss: 8.71061457e-06
Iter: 470 loss: 8.67253766e-06
Iter: 471 loss: 8.67088329e-06
Iter: 472 loss: 8.67134077e-06
Iter: 473 loss: 8.66961909e-06
Iter: 474 loss: 8.66746814e-06
Iter: 475 loss: 8.67915151e-06
Iter: 476 loss: 8.66715163e-06
Iter: 477 loss: 8.66643677e-06
Iter: 478 loss: 8.66618e-06
Iter: 479 loss: 8.66520531e-06
Iter: 480 loss: 8.664083e-06
Iter: 481 loss: 8.66395112e-06
Iter: 482 loss: 8.66263872e-06
Iter: 483 loss: 8.66887603e-06
Iter: 484 loss: 8.66239498e-06
Iter: 485 loss: 8.66112e-06
Iter: 486 loss: 8.66608116e-06
Iter: 487 loss: 8.66083337e-06
Iter: 488 loss: 8.6596574e-06
Iter: 489 loss: 8.66230766e-06
Iter: 490 loss: 8.65914444e-06
Iter: 491 loss: 8.65832862e-06
Iter: 492 loss: 8.65829952e-06
Iter: 493 loss: 8.65756374e-06
Iter: 494 loss: 8.65607e-06
Iter: 495 loss: 8.66402843e-06
Iter: 496 loss: 8.65579932e-06
Iter: 497 loss: 8.65475158e-06
Iter: 498 loss: 8.65339371e-06
Iter: 499 loss: 8.65327e-06
Iter: 500 loss: 8.65233778e-06
Iter: 501 loss: 8.65211223e-06
Iter: 502 loss: 8.65148195e-06
Iter: 503 loss: 8.64955564e-06
Iter: 504 loss: 8.66292e-06
Iter: 505 loss: 8.64912909e-06
Iter: 506 loss: 8.64690082e-06
Iter: 507 loss: 8.65198854e-06
Iter: 508 loss: 8.6460368e-06
Iter: 509 loss: 8.64384765e-06
Iter: 510 loss: 8.64745562e-06
Iter: 511 loss: 8.64284084e-06
Iter: 512 loss: 8.64283629e-06
Iter: 513 loss: 8.64166e-06
Iter: 514 loss: 8.64077083e-06
Iter: 515 loss: 8.63906735e-06
Iter: 516 loss: 8.67538438e-06
Iter: 517 loss: 8.63906462e-06
Iter: 518 loss: 8.63760397e-06
Iter: 519 loss: 8.65120273e-06
Iter: 520 loss: 8.63756941e-06
Iter: 521 loss: 8.63623e-06
Iter: 522 loss: 8.63999776e-06
Iter: 523 loss: 8.63577861e-06
Iter: 524 loss: 8.6347e-06
Iter: 525 loss: 8.63610512e-06
Iter: 526 loss: 8.63416517e-06
Iter: 527 loss: 8.63285459e-06
Iter: 528 loss: 8.63529e-06
Iter: 529 loss: 8.63234163e-06
Iter: 530 loss: 8.63070818e-06
Iter: 531 loss: 8.63996502e-06
Iter: 532 loss: 8.630509e-06
Iter: 533 loss: 8.62953493e-06
Iter: 534 loss: 8.62791512e-06
Iter: 535 loss: 8.62799243e-06
Iter: 536 loss: 8.62579691e-06
Iter: 537 loss: 8.65466154e-06
Iter: 538 loss: 8.62580691e-06
Iter: 539 loss: 8.6248383e-06
Iter: 540 loss: 8.62325305e-06
Iter: 541 loss: 8.62325214e-06
Iter: 542 loss: 8.62150591e-06
Iter: 543 loss: 8.62235356e-06
Iter: 544 loss: 8.62036177e-06
Iter: 545 loss: 8.61824083e-06
Iter: 546 loss: 8.6293494e-06
Iter: 547 loss: 8.61794e-06
Iter: 548 loss: 8.61736135e-06
Iter: 549 loss: 8.61690751e-06
Iter: 550 loss: 8.61616172e-06
Iter: 551 loss: 8.61387343e-06
Iter: 552 loss: 8.62421803e-06
Iter: 553 loss: 8.61308e-06
Iter: 554 loss: 8.61228273e-06
Iter: 555 loss: 8.61185e-06
Iter: 556 loss: 8.61065473e-06
Iter: 557 loss: 8.60981891e-06
Iter: 558 loss: 8.60936052e-06
Iter: 559 loss: 8.60805631e-06
Iter: 560 loss: 8.61188255e-06
Iter: 561 loss: 8.6076252e-06
Iter: 562 loss: 8.60612636e-06
Iter: 563 loss: 8.61284e-06
Iter: 564 loss: 8.60575256e-06
Iter: 565 loss: 8.60444834e-06
Iter: 566 loss: 8.60573891e-06
Iter: 567 loss: 8.60365162e-06
Iter: 568 loss: 8.60202454e-06
Iter: 569 loss: 8.60679393e-06
Iter: 570 loss: 8.60147429e-06
Iter: 571 loss: 8.60005639e-06
Iter: 572 loss: 8.61171793e-06
Iter: 573 loss: 8.59998909e-06
Iter: 574 loss: 8.59892862e-06
Iter: 575 loss: 8.59659121e-06
Iter: 576 loss: 8.63542118e-06
Iter: 577 loss: 8.59654392e-06
Iter: 578 loss: 8.59442662e-06
Iter: 579 loss: 8.59952161e-06
Iter: 580 loss: 8.59360534e-06
Iter: 581 loss: 8.59144166e-06
Iter: 582 loss: 8.59771e-06
Iter: 583 loss: 8.59072316e-06
Iter: 584 loss: 8.5903639e-06
Iter: 585 loss: 8.58964904e-06
Iter: 586 loss: 8.58889507e-06
Iter: 587 loss: 8.58713793e-06
Iter: 588 loss: 8.60182e-06
Iter: 589 loss: 8.58682142e-06
Iter: 590 loss: 8.5853917e-06
Iter: 591 loss: 8.60689124e-06
Iter: 592 loss: 8.58540443e-06
Iter: 593 loss: 8.5838119e-06
Iter: 594 loss: 8.5852007e-06
Iter: 595 loss: 8.58292151e-06
Iter: 596 loss: 8.58177e-06
Iter: 597 loss: 8.58280509e-06
Iter: 598 loss: 8.58116346e-06
Iter: 599 loss: 8.57944906e-06
Iter: 600 loss: 8.58745443e-06
Iter: 601 loss: 8.57916893e-06
Iter: 602 loss: 8.5780448e-06
Iter: 603 loss: 8.57669e-06
Iter: 604 loss: 8.57656e-06
Iter: 605 loss: 8.57495252e-06
Iter: 606 loss: 8.57494069e-06
Iter: 607 loss: 8.57381383e-06
Iter: 608 loss: 8.57452e-06
Iter: 609 loss: 8.5731408e-06
Iter: 610 loss: 8.57182476e-06
Iter: 611 loss: 8.57340092e-06
Iter: 612 loss: 8.57109e-06
Iter: 613 loss: 8.56964471e-06
Iter: 614 loss: 8.56685074e-06
Iter: 615 loss: 8.62759225e-06
Iter: 616 loss: 8.5668571e-06
Iter: 617 loss: 8.56435327e-06
Iter: 618 loss: 8.59463671e-06
Iter: 619 loss: 8.56433e-06
Iter: 620 loss: 8.56266706e-06
Iter: 621 loss: 8.56265706e-06
Iter: 622 loss: 8.56178667e-06
Iter: 623 loss: 8.55974395e-06
Iter: 624 loss: 8.58215481e-06
Iter: 625 loss: 8.55942744e-06
Iter: 626 loss: 8.55845883e-06
Iter: 627 loss: 8.55829239e-06
Iter: 628 loss: 8.55715462e-06
Iter: 629 loss: 8.55660255e-06
Iter: 630 loss: 8.55603685e-06
Iter: 631 loss: 8.55478e-06
Iter: 632 loss: 8.55677808e-06
Iter: 633 loss: 8.55422604e-06
Iter: 634 loss: 8.55224152e-06
Iter: 635 loss: 8.56090628e-06
Iter: 636 loss: 8.55186772e-06
Iter: 637 loss: 8.55070721e-06
Iter: 638 loss: 8.54923292e-06
Iter: 639 loss: 8.54908285e-06
Iter: 640 loss: 8.5476e-06
Iter: 641 loss: 8.54755308e-06
Iter: 642 loss: 8.5467e-06
Iter: 643 loss: 8.5450738e-06
Iter: 644 loss: 8.58084786e-06
Iter: 645 loss: 8.54508835e-06
Iter: 646 loss: 8.54314203e-06
Iter: 647 loss: 8.55319377e-06
Iter: 648 loss: 8.54288e-06
Iter: 649 loss: 8.54127302e-06
Iter: 650 loss: 8.54034533e-06
Iter: 651 loss: 8.53977e-06
Iter: 652 loss: 8.53714846e-06
Iter: 653 loss: 8.54145947e-06
Iter: 654 loss: 8.53593883e-06
Iter: 655 loss: 8.53566598e-06
Iter: 656 loss: 8.5348729e-06
Iter: 657 loss: 8.53368419e-06
Iter: 658 loss: 8.5313477e-06
Iter: 659 loss: 8.575842e-06
Iter: 660 loss: 8.53139318e-06
Iter: 661 loss: 8.52953781e-06
Iter: 662 loss: 8.53821348e-06
Iter: 663 loss: 8.52920675e-06
Iter: 664 loss: 8.52752237e-06
Iter: 665 loss: 8.54414429e-06
Iter: 666 loss: 8.52746052e-06
Iter: 667 loss: 8.52641824e-06
Iter: 668 loss: 8.52429912e-06
Iter: 669 loss: 8.55701e-06
Iter: 670 loss: 8.52417088e-06
Iter: 671 loss: 8.52262292e-06
Iter: 672 loss: 8.5224774e-06
Iter: 673 loss: 8.52131052e-06
Iter: 674 loss: 8.51917139e-06
Iter: 675 loss: 8.56845e-06
Iter: 676 loss: 8.51913865e-06
Iter: 677 loss: 8.51788718e-06
Iter: 678 loss: 8.51778e-06
Iter: 679 loss: 8.5164811e-06
Iter: 680 loss: 8.51502e-06
Iter: 681 loss: 8.51478399e-06
Iter: 682 loss: 8.51306504e-06
Iter: 683 loss: 8.5148522e-06
Iter: 684 loss: 8.51213736e-06
Iter: 685 loss: 8.50952529e-06
Iter: 686 loss: 8.51718323e-06
Iter: 687 loss: 8.50879678e-06
Iter: 688 loss: 8.50692e-06
Iter: 689 loss: 8.50875222e-06
Iter: 690 loss: 8.50584274e-06
Iter: 691 loss: 8.50496326e-06
Iter: 692 loss: 8.50461e-06
Iter: 693 loss: 8.5036e-06
Iter: 694 loss: 8.5015854e-06
Iter: 695 loss: 8.54119935e-06
Iter: 696 loss: 8.50157357e-06
Iter: 697 loss: 8.49992375e-06
Iter: 698 loss: 8.50902416e-06
Iter: 699 loss: 8.49967e-06
Iter: 700 loss: 8.49756361e-06
Iter: 701 loss: 8.50181823e-06
Iter: 702 loss: 8.49674325e-06
Iter: 703 loss: 8.49533353e-06
Iter: 704 loss: 8.49481148e-06
Iter: 705 loss: 8.4940566e-06
Iter: 706 loss: 8.49137541e-06
Iter: 707 loss: 8.50912875e-06
Iter: 708 loss: 8.49107164e-06
Iter: 709 loss: 8.4898229e-06
Iter: 710 loss: 8.48901618e-06
Iter: 711 loss: 8.48853597e-06
Iter: 712 loss: 8.48669697e-06
Iter: 713 loss: 8.51164259e-06
Iter: 714 loss: 8.48670152e-06
Iter: 715 loss: 8.48564e-06
Iter: 716 loss: 8.48321179e-06
Iter: 717 loss: 8.51579443e-06
Iter: 718 loss: 8.48304626e-06
Iter: 719 loss: 8.48064701e-06
Iter: 720 loss: 8.5016527e-06
Iter: 721 loss: 8.48059517e-06
Iter: 722 loss: 8.47839e-06
Iter: 723 loss: 8.47812225e-06
Iter: 724 loss: 8.47656884e-06
Iter: 725 loss: 8.47509091e-06
Iter: 726 loss: 8.47499e-06
Iter: 727 loss: 8.47348838e-06
Iter: 728 loss: 8.47338197e-06
Iter: 729 loss: 8.47226511e-06
Iter: 730 loss: 8.47086267e-06
Iter: 731 loss: 8.47097908e-06
Iter: 732 loss: 8.46982857e-06
Iter: 733 loss: 8.4681742e-06
Iter: 734 loss: 8.46819e-06
Iter: 735 loss: 8.46722833e-06
Iter: 736 loss: 8.46498369e-06
Iter: 737 loss: 8.4949952e-06
Iter: 738 loss: 8.46481817e-06
Iter: 739 loss: 8.46344e-06
Iter: 740 loss: 8.46328203e-06
Iter: 741 loss: 8.46185594e-06
Iter: 742 loss: 8.46016519e-06
Iter: 743 loss: 8.45990689e-06
Iter: 744 loss: 8.45840168e-06
Iter: 745 loss: 8.4818148e-06
Iter: 746 loss: 8.45837621e-06
Iter: 747 loss: 8.45690192e-06
Iter: 748 loss: 8.45452905e-06
Iter: 749 loss: 8.45462546e-06
Iter: 750 loss: 8.45240902e-06
Iter: 751 loss: 8.45758586e-06
Iter: 752 loss: 8.45166778e-06
Iter: 753 loss: 8.44922852e-06
Iter: 754 loss: 8.45751947e-06
Iter: 755 loss: 8.44858278e-06
Iter: 756 loss: 8.44614624e-06
Iter: 757 loss: 8.44816714e-06
Iter: 758 loss: 8.44464739e-06
Iter: 759 loss: 8.44312e-06
Iter: 760 loss: 8.44285387e-06
Iter: 761 loss: 8.44176429e-06
Iter: 762 loss: 8.43937778e-06
Iter: 763 loss: 8.46507191e-06
Iter: 764 loss: 8.43908492e-06
Iter: 765 loss: 8.43791804e-06
Iter: 766 loss: 8.43760608e-06
Iter: 767 loss: 8.43637e-06
Iter: 768 loss: 8.43439102e-06
Iter: 769 loss: 8.43435919e-06
Iter: 770 loss: 8.43255293e-06
Iter: 771 loss: 8.44075294e-06
Iter: 772 loss: 8.43221733e-06
Iter: 773 loss: 8.43009e-06
Iter: 774 loss: 8.43987436e-06
Iter: 775 loss: 8.4296671e-06
Iter: 776 loss: 8.42833288e-06
Iter: 777 loss: 8.42849477e-06
Iter: 778 loss: 8.42744e-06
Iter: 779 loss: 8.42504051e-06
Iter: 780 loss: 8.43343696e-06
Iter: 781 loss: 8.42446e-06
Iter: 782 loss: 8.42293048e-06
Iter: 783 loss: 8.42128429e-06
Iter: 784 loss: 8.42100781e-06
Iter: 785 loss: 8.41856763e-06
Iter: 786 loss: 8.43014641e-06
Iter: 787 loss: 8.41809378e-06
Iter: 788 loss: 8.4157582e-06
Iter: 789 loss: 8.41861947e-06
Iter: 790 loss: 8.41441215e-06
Iter: 791 loss: 8.41287874e-06
Iter: 792 loss: 8.41288511e-06
Iter: 793 loss: 8.41096789e-06
Iter: 794 loss: 8.40916618e-06
Iter: 795 loss: 8.40872599e-06
Iter: 796 loss: 8.40685425e-06
Iter: 797 loss: 8.41102e-06
Iter: 798 loss: 8.40618941e-06
Iter: 799 loss: 8.40392204e-06
Iter: 800 loss: 8.42463396e-06
Iter: 801 loss: 8.40382108e-06
Iter: 802 loss: 8.40271514e-06
Iter: 803 loss: 8.40010125e-06
Iter: 804 loss: 8.43208363e-06
Iter: 805 loss: 8.39988297e-06
Iter: 806 loss: 8.39841778e-06
Iter: 807 loss: 8.39805398e-06
Iter: 808 loss: 8.3964278e-06
Iter: 809 loss: 8.39440327e-06
Iter: 810 loss: 8.39423319e-06
Iter: 811 loss: 8.39268705e-06
Iter: 812 loss: 8.39265613e-06
Iter: 813 loss: 8.39123186e-06
Iter: 814 loss: 8.38847882e-06
Iter: 815 loss: 8.44714123e-06
Iter: 816 loss: 8.38847336e-06
Iter: 817 loss: 8.38595497e-06
Iter: 818 loss: 8.38742108e-06
Iter: 819 loss: 8.3842624e-06
Iter: 820 loss: 8.38155756e-06
Iter: 821 loss: 8.41682413e-06
Iter: 822 loss: 8.3815e-06
Iter: 823 loss: 8.37959578e-06
Iter: 824 loss: 8.38586493e-06
Iter: 825 loss: 8.37901e-06
Iter: 826 loss: 8.37665903e-06
Iter: 827 loss: 8.38898086e-06
Iter: 828 loss: 8.3763216e-06
Iter: 829 loss: 8.37481184e-06
Iter: 830 loss: 8.37265088e-06
Iter: 831 loss: 8.37260268e-06
Iter: 832 loss: 8.3717614e-06
Iter: 833 loss: 8.3713494e-06
Iter: 834 loss: 8.37026073e-06
Iter: 835 loss: 8.3677387e-06
Iter: 836 loss: 8.40426492e-06
Iter: 837 loss: 8.36757317e-06
Iter: 838 loss: 8.36527943e-06
Iter: 839 loss: 8.37214793e-06
Iter: 840 loss: 8.36464824e-06
Iter: 841 loss: 8.36184881e-06
Iter: 842 loss: 8.38510732e-06
Iter: 843 loss: 8.36165782e-06
Iter: 844 loss: 8.36015352e-06
Iter: 845 loss: 8.3590121e-06
Iter: 846 loss: 8.3585e-06
Iter: 847 loss: 8.35571882e-06
Iter: 848 loss: 8.37879816e-06
Iter: 849 loss: 8.35563515e-06
Iter: 850 loss: 8.35429e-06
Iter: 851 loss: 8.35183e-06
Iter: 852 loss: 8.40339635e-06
Iter: 853 loss: 8.35181e-06
Iter: 854 loss: 8.34892217e-06
Iter: 855 loss: 8.36117306e-06
Iter: 856 loss: 8.34833e-06
Iter: 857 loss: 8.34565617e-06
Iter: 858 loss: 8.36080471e-06
Iter: 859 loss: 8.3452378e-06
Iter: 860 loss: 8.34327784e-06
Iter: 861 loss: 8.36153777e-06
Iter: 862 loss: 8.34318416e-06
Iter: 863 loss: 8.3411e-06
Iter: 864 loss: 8.34040657e-06
Iter: 865 loss: 8.33914e-06
Iter: 866 loss: 8.33719241e-06
Iter: 867 loss: 8.33683589e-06
Iter: 868 loss: 8.33550712e-06
Iter: 869 loss: 8.33292415e-06
Iter: 870 loss: 8.33291415e-06
Iter: 871 loss: 8.33185823e-06
Iter: 872 loss: 8.32935802e-06
Iter: 873 loss: 8.35868832e-06
Iter: 874 loss: 8.32919e-06
Iter: 875 loss: 8.32734258e-06
Iter: 876 loss: 8.32727892e-06
Iter: 877 loss: 8.32524893e-06
Iter: 878 loss: 8.32379192e-06
Iter: 879 loss: 8.32306068e-06
Iter: 880 loss: 8.32120713e-06
Iter: 881 loss: 8.33741797e-06
Iter: 882 loss: 8.32111436e-06
Iter: 883 loss: 8.31910711e-06
Iter: 884 loss: 8.31905709e-06
Iter: 885 loss: 8.31745274e-06
Iter: 886 loss: 8.3156574e-06
Iter: 887 loss: 8.31479883e-06
Iter: 888 loss: 8.31389752e-06
Iter: 889 loss: 8.31144644e-06
Iter: 890 loss: 8.31785928e-06
Iter: 891 loss: 8.31065518e-06
Iter: 892 loss: 8.30830413e-06
Iter: 893 loss: 8.34463208e-06
Iter: 894 loss: 8.30830413e-06
Iter: 895 loss: 8.30686076e-06
Iter: 896 loss: 8.31830403e-06
Iter: 897 loss: 8.30673707e-06
Iter: 898 loss: 8.30544741e-06
Iter: 899 loss: 8.30241243e-06
Iter: 900 loss: 8.33910781e-06
Iter: 901 loss: 8.30209683e-06
Iter: 902 loss: 8.30015142e-06
Iter: 903 loss: 8.30008139e-06
Iter: 904 loss: 8.29806e-06
Iter: 905 loss: 8.3007e-06
Iter: 906 loss: 8.29703e-06
Iter: 907 loss: 8.2954266e-06
Iter: 908 loss: 8.29367127e-06
Iter: 909 loss: 8.29337932e-06
Iter: 910 loss: 8.29279e-06
Iter: 911 loss: 8.29221062e-06
Iter: 912 loss: 8.2911065e-06
Iter: 913 loss: 8.28840712e-06
Iter: 914 loss: 8.3219129e-06
Iter: 915 loss: 8.28823431e-06
Iter: 916 loss: 8.2869883e-06
Iter: 917 loss: 8.28675547e-06
Iter: 918 loss: 8.28540578e-06
Iter: 919 loss: 8.28260545e-06
Iter: 920 loss: 8.33199e-06
Iter: 921 loss: 8.28256634e-06
Iter: 922 loss: 8.28029079e-06
Iter: 923 loss: 8.28321663e-06
Iter: 924 loss: 8.27910117e-06
Iter: 925 loss: 8.27667191e-06
Iter: 926 loss: 8.28451266e-06
Iter: 927 loss: 8.27598797e-06
Iter: 928 loss: 8.27434633e-06
Iter: 929 loss: 8.27425174e-06
Iter: 930 loss: 8.27275653e-06
Iter: 931 loss: 8.2741135e-06
Iter: 932 loss: 8.2718816e-06
Iter: 933 loss: 8.27026815e-06
Iter: 934 loss: 8.27138774e-06
Iter: 935 loss: 8.26934411e-06
Iter: 936 loss: 8.2677825e-06
Iter: 937 loss: 8.27696567e-06
Iter: 938 loss: 8.26754331e-06
Iter: 939 loss: 8.26571886e-06
Iter: 940 loss: 8.267677e-06
Iter: 941 loss: 8.26469477e-06
Iter: 942 loss: 8.26340329e-06
Iter: 943 loss: 8.261517e-06
Iter: 944 loss: 8.26148425e-06
Iter: 945 loss: 8.26047471e-06
Iter: 946 loss: 8.2599945e-06
Iter: 947 loss: 8.25889674e-06
Iter: 948 loss: 8.2567276e-06
Iter: 949 loss: 8.29839337e-06
Iter: 950 loss: 8.25665848e-06
Iter: 951 loss: 8.25533698e-06
Iter: 952 loss: 8.25524876e-06
Iter: 953 loss: 8.2539018e-06
Iter: 954 loss: 8.25163625e-06
Iter: 955 loss: 8.25158531e-06
Iter: 956 loss: 8.24954e-06
Iter: 957 loss: 8.25050665e-06
Iter: 958 loss: 8.24810922e-06
Iter: 959 loss: 8.24545714e-06
Iter: 960 loss: 8.2493043e-06
Iter: 961 loss: 8.24416747e-06
Iter: 962 loss: 8.24442395e-06
Iter: 963 loss: 8.24281233e-06
Iter: 964 loss: 8.2418137e-06
Iter: 965 loss: 8.23997652e-06
Iter: 966 loss: 8.28505472e-06
Iter: 967 loss: 8.23992741e-06
Iter: 968 loss: 8.23788287e-06
Iter: 969 loss: 8.24805738e-06
Iter: 970 loss: 8.2375218e-06
Iter: 971 loss: 8.23606933e-06
Iter: 972 loss: 8.24438484e-06
Iter: 973 loss: 8.23585833e-06
Iter: 974 loss: 8.23438495e-06
Iter: 975 loss: 8.2367751e-06
Iter: 976 loss: 8.23371738e-06
Iter: 977 loss: 8.23248502e-06
Iter: 978 loss: 8.23106893e-06
Iter: 979 loss: 8.23088612e-06
Iter: 980 loss: 8.22994934e-06
Iter: 981 loss: 8.22967741e-06
Iter: 982 loss: 8.2286524e-06
Iter: 983 loss: 8.22608581e-06
Iter: 984 loss: 8.25823099e-06
Iter: 985 loss: 8.22595575e-06
Iter: 986 loss: 8.22482616e-06
Iter: 987 loss: 8.22464244e-06
Iter: 988 loss: 8.22335278e-06
Iter: 989 loss: 8.22089351e-06
Iter: 990 loss: 8.27239091e-06
Iter: 991 loss: 8.22086167e-06
Iter: 992 loss: 8.21876711e-06
Iter: 993 loss: 8.22298443e-06
Iter: 994 loss: 8.21786853e-06
Iter: 995 loss: 8.21559479e-06
Iter: 996 loss: 8.21583853e-06
Iter: 997 loss: 8.21387e-06
Iter: 998 loss: 8.21289e-06
Iter: 999 loss: 8.21223693e-06
Iter: 1000 loss: 8.21046706e-06
Iter: 1001 loss: 8.20977e-06
Iter: 1002 loss: 8.20887726e-06
Iter: 1003 loss: 8.20738114e-06
Iter: 1004 loss: 8.21203e-06
Iter: 1005 loss: 8.20689729e-06
Iter: 1006 loss: 8.20502464e-06
Iter: 1007 loss: 8.21084814e-06
Iter: 1008 loss: 8.20448258e-06
Iter: 1009 loss: 8.20278547e-06
Iter: 1010 loss: 8.20822424e-06
Iter: 1011 loss: 8.20228161e-06
Iter: 1012 loss: 8.20086098e-06
Iter: 1013 loss: 8.19929392e-06
Iter: 1014 loss: 8.19913475e-06
Iter: 1015 loss: 8.19767411e-06
Iter: 1016 loss: 8.19752495e-06
Iter: 1017 loss: 8.19629167e-06
Iter: 1018 loss: 8.1934686e-06
Iter: 1019 loss: 8.23559822e-06
Iter: 1020 loss: 8.1933922e-06
Iter: 1021 loss: 8.19147681e-06
Iter: 1022 loss: 8.19142679e-06
Iter: 1023 loss: 8.18960325e-06
Iter: 1024 loss: 8.18927765e-06
Iter: 1025 loss: 8.18800072e-06
Iter: 1026 loss: 8.18639637e-06
Iter: 1027 loss: 8.18357057e-06
Iter: 1028 loss: 8.18352146e-06
Iter: 1029 loss: 8.18024819e-06
Iter: 1030 loss: 8.20316291e-06
Iter: 1031 loss: 8.17994169e-06
Iter: 1032 loss: 8.1783628e-06
Iter: 1033 loss: 8.17809178e-06
Iter: 1034 loss: 8.17685213e-06
Iter: 1035 loss: 8.17374712e-06
Iter: 1036 loss: 8.20355581e-06
Iter: 1037 loss: 8.17337605e-06
Iter: 1038 loss: 8.17151249e-06
Iter: 1039 loss: 8.1714e-06
Iter: 1040 loss: 8.16955071e-06
Iter: 1041 loss: 8.17153432e-06
Iter: 1042 loss: 8.16857391e-06
Iter: 1043 loss: 8.16682e-06
Iter: 1044 loss: 8.17035652e-06
Iter: 1045 loss: 8.16607917e-06
Iter: 1046 loss: 8.16426837e-06
Iter: 1047 loss: 8.16639476e-06
Iter: 1048 loss: 8.16331703e-06
Iter: 1049 loss: 8.16126794e-06
Iter: 1050 loss: 8.18775243e-06
Iter: 1051 loss: 8.16129068e-06
Iter: 1052 loss: 8.16027386e-06
Iter: 1053 loss: 8.15786825e-06
Iter: 1054 loss: 8.18594708e-06
Iter: 1055 loss: 8.15761177e-06
Iter: 1056 loss: 8.15635576e-06
Iter: 1057 loss: 8.15589556e-06
Iter: 1058 loss: 8.15473322e-06
Iter: 1059 loss: 8.15164276e-06
Iter: 1060 loss: 8.1780363e-06
Iter: 1061 loss: 8.15109161e-06
Iter: 1062 loss: 8.14822124e-06
Iter: 1063 loss: 8.15937e-06
Iter: 1064 loss: 8.1475373e-06
Iter: 1065 loss: 8.14544546e-06
Iter: 1066 loss: 8.17274304e-06
Iter: 1067 loss: 8.14543728e-06
Iter: 1068 loss: 8.14287705e-06
Iter: 1069 loss: 8.14712257e-06
Iter: 1070 loss: 8.14181567e-06
Iter: 1071 loss: 8.14041778e-06
Iter: 1072 loss: 8.13932093e-06
Iter: 1073 loss: 8.13884526e-06
Iter: 1074 loss: 8.1369144e-06
Iter: 1075 loss: 8.13694169e-06
Iter: 1076 loss: 8.13569568e-06
Iter: 1077 loss: 8.13435145e-06
Iter: 1078 loss: 8.13418228e-06
Iter: 1079 loss: 8.13200859e-06
Iter: 1080 loss: 8.13982479e-06
Iter: 1081 loss: 8.1314538e-06
Iter: 1082 loss: 8.12992948e-06
Iter: 1083 loss: 8.14587656e-06
Iter: 1084 loss: 8.12991675e-06
Iter: 1085 loss: 8.12832059e-06
Iter: 1086 loss: 8.126377e-06
Iter: 1087 loss: 8.12618418e-06
Iter: 1088 loss: 8.12414e-06
Iter: 1089 loss: 8.13133283e-06
Iter: 1090 loss: 8.12357666e-06
Iter: 1091 loss: 8.12102e-06
Iter: 1092 loss: 8.13307088e-06
Iter: 1093 loss: 8.12057806e-06
Iter: 1094 loss: 8.11918926e-06
Iter: 1095 loss: 8.11728751e-06
Iter: 1096 loss: 8.11720565e-06
Iter: 1097 loss: 8.11489e-06
Iter: 1098 loss: 8.11905738e-06
Iter: 1099 loss: 8.11387e-06
Iter: 1100 loss: 8.11369e-06
Iter: 1101 loss: 8.11266818e-06
Iter: 1102 loss: 8.11183872e-06
Iter: 1103 loss: 8.10952224e-06
Iter: 1104 loss: 8.12525377e-06
Iter: 1105 loss: 8.10896381e-06
Iter: 1106 loss: 8.10704478e-06
Iter: 1107 loss: 8.13658153e-06
Iter: 1108 loss: 8.10700658e-06
Iter: 1109 loss: 8.10513575e-06
Iter: 1110 loss: 8.11061727e-06
Iter: 1111 loss: 8.10455822e-06
Iter: 1112 loss: 8.10328402e-06
Iter: 1113 loss: 8.10276106e-06
Iter: 1114 loss: 8.10201891e-06
Iter: 1115 loss: 8.10002166e-06
Iter: 1116 loss: 8.1115395e-06
Iter: 1117 loss: 8.09972153e-06
Iter: 1118 loss: 8.09811536e-06
Iter: 1119 loss: 8.10944675e-06
Iter: 1120 loss: 8.09798803e-06
Iter: 1121 loss: 8.09660924e-06
Iter: 1122 loss: 8.09420089e-06
Iter: 1123 loss: 8.09420089e-06
Iter: 1124 loss: 8.09272206e-06
Iter: 1125 loss: 8.09268e-06
Iter: 1126 loss: 8.09120684e-06
Iter: 1127 loss: 8.08892e-06
Iter: 1128 loss: 8.08887671e-06
Iter: 1129 loss: 8.08681e-06
Iter: 1130 loss: 8.0869595e-06
Iter: 1131 loss: 8.08515506e-06
Iter: 1132 loss: 8.08393452e-06
Iter: 1133 loss: 8.08375808e-06
Iter: 1134 loss: 8.08214281e-06
Iter: 1135 loss: 8.08327422e-06
Iter: 1136 loss: 8.08118057e-06
Iter: 1137 loss: 8.07971082e-06
Iter: 1138 loss: 8.07744345e-06
Iter: 1139 loss: 8.07737433e-06
Iter: 1140 loss: 8.07688593e-06
Iter: 1141 loss: 8.07599736e-06
Iter: 1142 loss: 8.07513698e-06
Iter: 1143 loss: 8.07313e-06
Iter: 1144 loss: 8.10050369e-06
Iter: 1145 loss: 8.07299875e-06
Iter: 1146 loss: 8.07120796e-06
Iter: 1147 loss: 8.0907721e-06
Iter: 1148 loss: 8.07122342e-06
Iter: 1149 loss: 8.0697755e-06
Iter: 1150 loss: 8.07614833e-06
Iter: 1151 loss: 8.06943535e-06
Iter: 1152 loss: 8.06788739e-06
Iter: 1153 loss: 8.06825574e-06
Iter: 1154 loss: 8.06671051e-06
Iter: 1155 loss: 8.06483786e-06
Iter: 1156 loss: 8.0654936e-06
Iter: 1157 loss: 8.06356275e-06
Iter: 1158 loss: 8.06141907e-06
Iter: 1159 loss: 8.06137905e-06
Iter: 1160 loss: 8.06038224e-06
Iter: 1161 loss: 8.05823402e-06
Iter: 1162 loss: 8.09924313e-06
Iter: 1163 loss: 8.05823402e-06
Iter: 1164 loss: 8.05615e-06
Iter: 1165 loss: 8.06396929e-06
Iter: 1166 loss: 8.05560194e-06
Iter: 1167 loss: 8.05450873e-06
Iter: 1168 loss: 8.05427771e-06
Iter: 1169 loss: 8.05343916e-06
Iter: 1170 loss: 8.0511727e-06
Iter: 1171 loss: 8.06384378e-06
Iter: 1172 loss: 8.05044056e-06
Iter: 1173 loss: 8.04881347e-06
Iter: 1174 loss: 8.04877527e-06
Iter: 1175 loss: 8.04683714e-06
Iter: 1176 loss: 8.0468717e-06
Iter: 1177 loss: 8.04538377e-06
Iter: 1178 loss: 8.04363481e-06
Iter: 1179 loss: 8.04605406e-06
Iter: 1180 loss: 8.04284173e-06
Iter: 1181 loss: 8.04153206e-06
Iter: 1182 loss: 8.04149386e-06
Iter: 1183 loss: 8.04044794e-06
Iter: 1184 loss: 8.04173214e-06
Iter: 1185 loss: 8.03990224e-06
Iter: 1186 loss: 8.03849616e-06
Iter: 1187 loss: 8.03723e-06
Iter: 1188 loss: 8.03691364e-06
Iter: 1189 loss: 8.03534203e-06
Iter: 1190 loss: 8.05845684e-06
Iter: 1191 loss: 8.03535295e-06
Iter: 1192 loss: 8.03372586e-06
Iter: 1193 loss: 8.03134481e-06
Iter: 1194 loss: 8.03130933e-06
Iter: 1195 loss: 8.02918839e-06
Iter: 1196 loss: 8.03317744e-06
Iter: 1197 loss: 8.02830618e-06
Iter: 1198 loss: 8.02751492e-06
Iter: 1199 loss: 8.02723844e-06
Iter: 1200 loss: 8.02607428e-06
Iter: 1201 loss: 8.02411159e-06
Iter: 1202 loss: 8.02416616e-06
Iter: 1203 loss: 8.0223881e-06
Iter: 1204 loss: 8.02471186e-06
Iter: 1205 loss: 8.02152e-06
Iter: 1206 loss: 8.01972874e-06
Iter: 1207 loss: 8.01971328e-06
Iter: 1208 loss: 8.01863735e-06
Iter: 1209 loss: 8.01600436e-06
Iter: 1210 loss: 8.04346564e-06
Iter: 1211 loss: 8.01567e-06
Iter: 1212 loss: 8.01433544e-06
Iter: 1213 loss: 8.01415445e-06
Iter: 1214 loss: 8.01284659e-06
Iter: 1215 loss: 8.01678834e-06
Iter: 1216 loss: 8.01242641e-06
Iter: 1217 loss: 8.01116221e-06
Iter: 1218 loss: 8.01164424e-06
Iter: 1219 loss: 8.01026727e-06
Iter: 1220 loss: 8.00878388e-06
Iter: 1221 loss: 8.0121481e-06
Iter: 1222 loss: 8.0082491e-06
Iter: 1223 loss: 8.00646922e-06
Iter: 1224 loss: 8.01509668e-06
Iter: 1225 loss: 8.00618272e-06
Iter: 1226 loss: 8.00480848e-06
Iter: 1227 loss: 8.00248199e-06
Iter: 1228 loss: 8.05842e-06
Iter: 1229 loss: 8.00245652e-06
Iter: 1230 loss: 8.00063e-06
Iter: 1231 loss: 8.02792601e-06
Iter: 1232 loss: 8.00063663e-06
Iter: 1233 loss: 7.99883492e-06
Iter: 1234 loss: 8.00803e-06
Iter: 1235 loss: 7.99853e-06
Iter: 1236 loss: 7.99761801e-06
Iter: 1237 loss: 7.99566806e-06
Iter: 1238 loss: 8.02683098e-06
Iter: 1239 loss: 7.99557711e-06
Iter: 1240 loss: 7.99485133e-06
Iter: 1241 loss: 7.99440113e-06
Iter: 1242 loss: 7.99338886e-06
Iter: 1243 loss: 7.99155e-06
Iter: 1244 loss: 8.03454441e-06
Iter: 1245 loss: 7.99155532e-06
Iter: 1246 loss: 7.98966175e-06
Iter: 1247 loss: 7.99213376e-06
Iter: 1248 loss: 7.98864858e-06
Iter: 1249 loss: 7.98686e-06
Iter: 1250 loss: 7.98688052e-06
Iter: 1251 loss: 7.98564906e-06
Iter: 1252 loss: 7.98598376e-06
Iter: 1253 loss: 7.98479869e-06
Iter: 1254 loss: 7.98330711e-06
Iter: 1255 loss: 7.98633391e-06
Iter: 1256 loss: 7.98274414e-06
Iter: 1257 loss: 7.98165456e-06
Iter: 1258 loss: 7.99392546e-06
Iter: 1259 loss: 7.98162455e-06
Iter: 1260 loss: 7.98060137e-06
Iter: 1261 loss: 7.97817847e-06
Iter: 1262 loss: 8.01296119e-06
Iter: 1263 loss: 7.97814209e-06
Iter: 1264 loss: 7.97613757e-06
Iter: 1265 loss: 7.98370093e-06
Iter: 1266 loss: 7.97563371e-06
Iter: 1267 loss: 7.97431585e-06
Iter: 1268 loss: 7.97418215e-06
Iter: 1269 loss: 7.97327e-06
Iter: 1270 loss: 7.9711017e-06
Iter: 1271 loss: 7.99858844e-06
Iter: 1272 loss: 7.97094071e-06
Iter: 1273 loss: 7.96952554e-06
Iter: 1274 loss: 7.99006e-06
Iter: 1275 loss: 7.96949917e-06
Iter: 1276 loss: 7.96797849e-06
Iter: 1277 loss: 7.97083794e-06
Iter: 1278 loss: 7.96735458e-06
Iter: 1279 loss: 7.96617e-06
Iter: 1280 loss: 7.96434e-06
Iter: 1281 loss: 7.96433596e-06
Iter: 1282 loss: 7.96325e-06
Iter: 1283 loss: 7.96311906e-06
Iter: 1284 loss: 7.96182758e-06
Iter: 1285 loss: 7.96129279e-06
Iter: 1286 loss: 7.96072345e-06
Iter: 1287 loss: 7.95891901e-06
Iter: 1288 loss: 7.96398308e-06
Iter: 1289 loss: 7.95840879e-06
Iter: 1290 loss: 7.95701089e-06
Iter: 1291 loss: 7.96424e-06
Iter: 1292 loss: 7.95684e-06
Iter: 1293 loss: 7.95529195e-06
Iter: 1294 loss: 7.95690266e-06
Iter: 1295 loss: 7.95441338e-06
Iter: 1296 loss: 7.95311644e-06
Iter: 1297 loss: 7.95126107e-06
Iter: 1298 loss: 7.95118194e-06
Iter: 1299 loss: 7.95019e-06
Iter: 1300 loss: 7.94984226e-06
Iter: 1301 loss: 7.94846346e-06
Iter: 1302 loss: 7.94689549e-06
Iter: 1303 loss: 7.94664811e-06
Iter: 1304 loss: 7.9452484e-06
Iter: 1305 loss: 7.94666539e-06
Iter: 1306 loss: 7.9445017e-06
Iter: 1307 loss: 7.94301e-06
Iter: 1308 loss: 7.94293555e-06
Iter: 1309 loss: 7.94214429e-06
Iter: 1310 loss: 7.94023072e-06
Iter: 1311 loss: 7.970757e-06
Iter: 1312 loss: 7.94016e-06
Iter: 1313 loss: 7.93855543e-06
Iter: 1314 loss: 7.95137e-06
Iter: 1315 loss: 7.9384854e-06
Iter: 1316 loss: 7.93667459e-06
Iter: 1317 loss: 7.94440712e-06
Iter: 1318 loss: 7.93635627e-06
Iter: 1319 loss: 7.93512663e-06
Iter: 1320 loss: 7.93512572e-06
Iter: 1321 loss: 7.93407071e-06
Iter: 1322 loss: 7.93226809e-06
Iter: 1323 loss: 7.93967411e-06
Iter: 1324 loss: 7.93181243e-06
Iter: 1325 loss: 7.93043637e-06
Iter: 1326 loss: 7.94303742e-06
Iter: 1327 loss: 7.93037907e-06
Iter: 1328 loss: 7.9294332e-06
Iter: 1329 loss: 7.927566e-06
Iter: 1330 loss: 7.96638051e-06
Iter: 1331 loss: 7.92756e-06
Iter: 1332 loss: 7.92581159e-06
Iter: 1333 loss: 7.93637537e-06
Iter: 1334 loss: 7.92557239e-06
Iter: 1335 loss: 7.9235142e-06
Iter: 1336 loss: 7.93606705e-06
Iter: 1337 loss: 7.92326409e-06
Iter: 1338 loss: 7.92234368e-06
Iter: 1339 loss: 7.92047831e-06
Iter: 1340 loss: 7.95449159e-06
Iter: 1341 loss: 7.92048e-06
Iter: 1342 loss: 7.91931325e-06
Iter: 1343 loss: 7.91917046e-06
Iter: 1344 loss: 7.91788261e-06
Iter: 1345 loss: 7.91626644e-06
Iter: 1346 loss: 7.91615548e-06
Iter: 1347 loss: 7.91454295e-06
Iter: 1348 loss: 7.914e-06
Iter: 1349 loss: 7.91305501e-06
Iter: 1350 loss: 7.91204911e-06
Iter: 1351 loss: 7.91159255e-06
Iter: 1352 loss: 7.91046477e-06
Iter: 1353 loss: 7.90862e-06
Iter: 1354 loss: 7.9086e-06
Iter: 1355 loss: 7.90645208e-06
Iter: 1356 loss: 7.9191359e-06
Iter: 1357 loss: 7.90612285e-06
Iter: 1358 loss: 7.90462309e-06
Iter: 1359 loss: 7.91314415e-06
Iter: 1360 loss: 7.90429294e-06
Iter: 1361 loss: 7.90264858e-06
Iter: 1362 loss: 7.90192917e-06
Iter: 1363 loss: 7.90110425e-06
Iter: 1364 loss: 7.89932164e-06
Iter: 1365 loss: 7.89966725e-06
Iter: 1366 loss: 7.89797559e-06
Iter: 1367 loss: 7.89720434e-06
Iter: 1368 loss: 7.89669321e-06
Iter: 1369 loss: 7.89565729e-06
Iter: 1370 loss: 7.8929952e-06
Iter: 1371 loss: 7.91744242e-06
Iter: 1372 loss: 7.89259684e-06
Iter: 1373 loss: 7.89077149e-06
Iter: 1374 loss: 7.91951243e-06
Iter: 1375 loss: 7.89078149e-06
Iter: 1376 loss: 7.8890007e-06
Iter: 1377 loss: 7.8932062e-06
Iter: 1378 loss: 7.88837497e-06
Iter: 1379 loss: 7.88705529e-06
Iter: 1380 loss: 7.88449688e-06
Iter: 1381 loss: 7.93301388e-06
Iter: 1382 loss: 7.88441412e-06
Iter: 1383 loss: 7.88239e-06
Iter: 1384 loss: 7.88235502e-06
Iter: 1385 loss: 7.88036778e-06
Iter: 1386 loss: 7.88868056e-06
Iter: 1387 loss: 7.87995668e-06
Iter: 1388 loss: 7.87871249e-06
Iter: 1389 loss: 7.87774934e-06
Iter: 1390 loss: 7.87724457e-06
Iter: 1391 loss: 7.87503632e-06
Iter: 1392 loss: 7.88830494e-06
Iter: 1393 loss: 7.87471254e-06
Iter: 1394 loss: 7.8730809e-06
Iter: 1395 loss: 7.8826e-06
Iter: 1396 loss: 7.87283352e-06
Iter: 1397 loss: 7.87167301e-06
Iter: 1398 loss: 7.86998135e-06
Iter: 1399 loss: 7.8699195e-06
Iter: 1400 loss: 7.86808e-06
Iter: 1401 loss: 7.8816056e-06
Iter: 1402 loss: 7.8679368e-06
Iter: 1403 loss: 7.86577766e-06
Iter: 1404 loss: 7.87281897e-06
Iter: 1405 loss: 7.86510827e-06
Iter: 1406 loss: 7.86400051e-06
Iter: 1407 loss: 7.86158125e-06
Iter: 1408 loss: 7.89994283e-06
Iter: 1409 loss: 7.86147393e-06
Iter: 1410 loss: 7.86061446e-06
Iter: 1411 loss: 7.85987504e-06
Iter: 1412 loss: 7.85892826e-06
Iter: 1413 loss: 7.85686825e-06
Iter: 1414 loss: 7.88647685e-06
Iter: 1415 loss: 7.85674e-06
Iter: 1416 loss: 7.85449811e-06
Iter: 1417 loss: 7.85657e-06
Iter: 1418 loss: 7.8531948e-06
Iter: 1419 loss: 7.85253451e-06
Iter: 1420 loss: 7.85190787e-06
Iter: 1421 loss: 7.85075827e-06
Iter: 1422 loss: 7.8481271e-06
Iter: 1423 loss: 7.88896796e-06
Iter: 1424 loss: 7.84805889e-06
Iter: 1425 loss: 7.84606164e-06
Iter: 1426 loss: 7.87212048e-06
Iter: 1427 loss: 7.84605163e-06
Iter: 1428 loss: 7.84425629e-06
Iter: 1429 loss: 7.84606709e-06
Iter: 1430 loss: 7.84324311e-06
Iter: 1431 loss: 7.84108488e-06
Iter: 1432 loss: 7.84835447e-06
Iter: 1433 loss: 7.84047825e-06
Iter: 1434 loss: 7.8388839e-06
Iter: 1435 loss: 7.8379544e-06
Iter: 1436 loss: 7.83722135e-06
Iter: 1437 loss: 7.83665655e-06
Iter: 1438 loss: 7.83620817e-06
Iter: 1439 loss: 7.83521318e-06
Iter: 1440 loss: 7.83282121e-06
Iter: 1441 loss: 7.85960401e-06
Iter: 1442 loss: 7.83257838e-06
Iter: 1443 loss: 7.83037649e-06
Iter: 1444 loss: 7.83936321e-06
Iter: 1445 loss: 7.82990901e-06
Iter: 1446 loss: 7.82802636e-06
Iter: 1447 loss: 7.85749216e-06
Iter: 1448 loss: 7.82806273e-06
Iter: 1449 loss: 7.82692405e-06
Iter: 1450 loss: 7.82417555e-06
Iter: 1451 loss: 7.84663825e-06
Iter: 1452 loss: 7.82375082e-06
Iter: 1453 loss: 7.82097231e-06
Iter: 1454 loss: 7.83217729e-06
Iter: 1455 loss: 7.82036295e-06
Iter: 1456 loss: 7.81866947e-06
Iter: 1457 loss: 7.81846575e-06
Iter: 1458 loss: 7.81741073e-06
Iter: 1459 loss: 7.81508425e-06
Iter: 1460 loss: 7.8480316e-06
Iter: 1461 loss: 7.81493509e-06
Iter: 1462 loss: 7.81284871e-06
Iter: 1463 loss: 7.84648e-06
Iter: 1464 loss: 7.81285598e-06
Iter: 1465 loss: 7.8111816e-06
Iter: 1466 loss: 7.81282779e-06
Iter: 1467 loss: 7.81019753e-06
Iter: 1468 loss: 7.80847e-06
Iter: 1469 loss: 7.81358176e-06
Iter: 1470 loss: 7.80788469e-06
Iter: 1471 loss: 7.80635855e-06
Iter: 1472 loss: 7.80613391e-06
Iter: 1473 loss: 7.80507253e-06
Iter: 1474 loss: 7.80410483e-06
Iter: 1475 loss: 7.80379e-06
Iter: 1476 loss: 7.80294067e-06
Iter: 1477 loss: 7.80051778e-06
Iter: 1478 loss: 7.81464405e-06
Iter: 1479 loss: 7.79985203e-06
Iter: 1480 loss: 7.79751463e-06
Iter: 1481 loss: 7.81625567e-06
Iter: 1482 loss: 7.79737547e-06
Iter: 1483 loss: 7.79492711e-06
Iter: 1484 loss: 7.81135259e-06
Iter: 1485 loss: 7.79467246e-06
Iter: 1486 loss: 7.79361835e-06
Iter: 1487 loss: 7.79114634e-06
Iter: 1488 loss: 7.81983181e-06
Iter: 1489 loss: 7.79088623e-06
Iter: 1490 loss: 7.78806771e-06
Iter: 1491 loss: 7.8029916e-06
Iter: 1492 loss: 7.78755748e-06
Iter: 1493 loss: 7.78671892e-06
Iter: 1494 loss: 7.7862569e-06
Iter: 1495 loss: 7.78522735e-06
Iter: 1496 loss: 7.78266076e-06
Iter: 1497 loss: 7.80518167e-06
Iter: 1498 loss: 7.78225512e-06
Iter: 1499 loss: 7.7809209e-06
Iter: 1500 loss: 7.78077356e-06
Iter: 1501 loss: 7.77929745e-06
Iter: 1502 loss: 7.77916921e-06
Iter: 1503 loss: 7.77814421e-06
Iter: 1504 loss: 7.7763907e-06
Iter: 1505 loss: 7.78061076e-06
Iter: 1506 loss: 7.77577407e-06
Iter: 1507 loss: 7.77372315e-06
Iter: 1508 loss: 7.77473e-06
Iter: 1509 loss: 7.7723289e-06
Iter: 1510 loss: 7.77139212e-06
Iter: 1511 loss: 7.77112291e-06
Iter: 1512 loss: 7.77000241e-06
Iter: 1513 loss: 7.76790057e-06
Iter: 1514 loss: 7.81841845e-06
Iter: 1515 loss: 7.76791421e-06
Iter: 1516 loss: 7.76607521e-06
Iter: 1517 loss: 7.76617071e-06
Iter: 1518 loss: 7.7646564e-06
Iter: 1519 loss: 7.76317e-06
Iter: 1520 loss: 7.76309753e-06
Iter: 1521 loss: 7.76143679e-06
Iter: 1522 loss: 7.76356e-06
Iter: 1523 loss: 7.7605664e-06
Iter: 1524 loss: 7.75928675e-06
Iter: 1525 loss: 7.75695571e-06
Iter: 1526 loss: 7.80833e-06
Iter: 1527 loss: 7.75695298e-06
Iter: 1528 loss: 7.75673288e-06
Iter: 1529 loss: 7.75566514e-06
Iter: 1530 loss: 7.75460921e-06
Iter: 1531 loss: 7.75236185e-06
Iter: 1532 loss: 7.7840923e-06
Iter: 1533 loss: 7.75224362e-06
Iter: 1534 loss: 7.75019726e-06
Iter: 1535 loss: 7.76408615e-06
Iter: 1536 loss: 7.75001354e-06
Iter: 1537 loss: 7.74823638e-06
Iter: 1538 loss: 7.76215802e-06
Iter: 1539 loss: 7.74811451e-06
Iter: 1540 loss: 7.74692853e-06
Iter: 1541 loss: 7.74488763e-06
Iter: 1542 loss: 7.79236e-06
Iter: 1543 loss: 7.74496e-06
Iter: 1544 loss: 7.74273394e-06
Iter: 1545 loss: 7.77072637e-06
Iter: 1546 loss: 7.74273485e-06
Iter: 1547 loss: 7.74164891e-06
Iter: 1548 loss: 7.7501445e-06
Iter: 1549 loss: 7.74158889e-06
Iter: 1550 loss: 7.74032378e-06
Iter: 1551 loss: 7.73859756e-06
Iter: 1552 loss: 7.7384575e-06
Iter: 1553 loss: 7.73665488e-06
Iter: 1554 loss: 7.7360628e-06
Iter: 1555 loss: 7.73504689e-06
Iter: 1556 loss: 7.73303691e-06
Iter: 1557 loss: 7.75233912e-06
Iter: 1558 loss: 7.73300053e-06
Iter: 1559 loss: 7.73125794e-06
Iter: 1560 loss: 7.7483e-06
Iter: 1561 loss: 7.73117063e-06
Iter: 1562 loss: 7.73023385e-06
Iter: 1563 loss: 7.72784e-06
Iter: 1564 loss: 7.75275566e-06
Iter: 1565 loss: 7.72761268e-06
Iter: 1566 loss: 7.7268669e-06
Iter: 1567 loss: 7.72640851e-06
Iter: 1568 loss: 7.72510703e-06
Iter: 1569 loss: 7.72306885e-06
Iter: 1570 loss: 7.72302337e-06
Iter: 1571 loss: 7.72146177e-06
Iter: 1572 loss: 7.72393378e-06
Iter: 1573 loss: 7.72073417e-06
Iter: 1574 loss: 7.71859777e-06
Iter: 1575 loss: 7.73934516e-06
Iter: 1576 loss: 7.71852046e-06
Iter: 1577 loss: 7.71747e-06
Iter: 1578 loss: 7.71587111e-06
Iter: 1579 loss: 7.7158611e-06
Iter: 1580 loss: 7.7137e-06
Iter: 1581 loss: 7.72530439e-06
Iter: 1582 loss: 7.7134091e-06
Iter: 1583 loss: 7.71156556e-06
Iter: 1584 loss: 7.72739531e-06
Iter: 1585 loss: 7.71150462e-06
Iter: 1586 loss: 7.70999e-06
Iter: 1587 loss: 7.71239866e-06
Iter: 1588 loss: 7.70924089e-06
Iter: 1589 loss: 7.70813131e-06
Iter: 1590 loss: 7.70595398e-06
Iter: 1591 loss: 7.75739136e-06
Iter: 1592 loss: 7.70596489e-06
Iter: 1593 loss: 7.70385395e-06
Iter: 1594 loss: 7.72035673e-06
Iter: 1595 loss: 7.70369752e-06
Iter: 1596 loss: 7.70168663e-06
Iter: 1597 loss: 7.72003204e-06
Iter: 1598 loss: 7.7015784e-06
Iter: 1599 loss: 7.7005925e-06
Iter: 1600 loss: 7.69816052e-06
Iter: 1601 loss: 7.72515068e-06
Iter: 1602 loss: 7.69796497e-06
Iter: 1603 loss: 7.69762573e-06
Iter: 1604 loss: 7.69678718e-06
Iter: 1605 loss: 7.6956585e-06
Iter: 1606 loss: 7.69357757e-06
Iter: 1607 loss: 7.73904867e-06
Iter: 1608 loss: 7.69356848e-06
Iter: 1609 loss: 7.69165308e-06
Iter: 1610 loss: 7.6953e-06
Iter: 1611 loss: 7.69084e-06
Iter: 1612 loss: 7.68931204e-06
Iter: 1613 loss: 7.68924656e-06
Iter: 1614 loss: 7.68804603e-06
Iter: 1615 loss: 7.68547e-06
Iter: 1616 loss: 7.72306612e-06
Iter: 1617 loss: 7.68532118e-06
Iter: 1618 loss: 7.68371683e-06
Iter: 1619 loss: 7.68374593e-06
Iter: 1620 loss: 7.68214886e-06
Iter: 1621 loss: 7.6873057e-06
Iter: 1622 loss: 7.68174323e-06
Iter: 1623 loss: 7.68016525e-06
Iter: 1624 loss: 7.68285463e-06
Iter: 1625 loss: 7.67944584e-06
Iter: 1626 loss: 7.67817437e-06
Iter: 1627 loss: 7.67643269e-06
Iter: 1628 loss: 7.67631718e-06
Iter: 1629 loss: 7.67421716e-06
Iter: 1630 loss: 7.68866084e-06
Iter: 1631 loss: 7.67406436e-06
Iter: 1632 loss: 7.67200436e-06
Iter: 1633 loss: 7.68890641e-06
Iter: 1634 loss: 7.6719225e-06
Iter: 1635 loss: 7.67085839e-06
Iter: 1636 loss: 7.66860376e-06
Iter: 1637 loss: 7.70553561e-06
Iter: 1638 loss: 7.66854646e-06
Iter: 1639 loss: 7.66748053e-06
Iter: 1640 loss: 7.66721223e-06
Iter: 1641 loss: 7.66583526e-06
Iter: 1642 loss: 7.66400444e-06
Iter: 1643 loss: 7.66386529e-06
Iter: 1644 loss: 7.66218545e-06
Iter: 1645 loss: 7.66285848e-06
Iter: 1646 loss: 7.66104e-06
Iter: 1647 loss: 7.65973255e-06
Iter: 1648 loss: 7.65955792e-06
Iter: 1649 loss: 7.65844561e-06
Iter: 1650 loss: 7.65600089e-06
Iter: 1651 loss: 7.69505186e-06
Iter: 1652 loss: 7.65590266e-06
Iter: 1653 loss: 7.65344794e-06
Iter: 1654 loss: 7.66224275e-06
Iter: 1655 loss: 7.65285131e-06
Iter: 1656 loss: 7.6516526e-06
Iter: 1657 loss: 7.65138793e-06
Iter: 1658 loss: 7.6505e-06
Iter: 1659 loss: 7.64934885e-06
Iter: 1660 loss: 7.64931701e-06
Iter: 1661 loss: 7.64738252e-06
Iter: 1662 loss: 7.64734796e-06
Iter: 1663 loss: 7.64583274e-06
Iter: 1664 loss: 7.64396464e-06
Iter: 1665 loss: 7.65002e-06
Iter: 1666 loss: 7.64348351e-06
Iter: 1667 loss: 7.64130255e-06
Iter: 1668 loss: 7.6617107e-06
Iter: 1669 loss: 7.64120341e-06
Iter: 1670 loss: 7.6400147e-06
Iter: 1671 loss: 7.63736716e-06
Iter: 1672 loss: 7.68121572e-06
Iter: 1673 loss: 7.63733078e-06
Iter: 1674 loss: 7.63652133e-06
Iter: 1675 loss: 7.63607386e-06
Iter: 1676 loss: 7.6348324e-06
Iter: 1677 loss: 7.63331172e-06
Iter: 1678 loss: 7.63316166e-06
Iter: 1679 loss: 7.63144089e-06
Iter: 1680 loss: 7.63144453e-06
Iter: 1681 loss: 7.63010758e-06
Iter: 1682 loss: 7.62885702e-06
Iter: 1683 loss: 7.62865557e-06
Iter: 1684 loss: 7.62746549e-06
Iter: 1685 loss: 7.62468153e-06
Iter: 1686 loss: 7.65718687e-06
Iter: 1687 loss: 7.62439595e-06
Iter: 1688 loss: 7.62236095e-06
Iter: 1689 loss: 7.64409197e-06
Iter: 1690 loss: 7.62233094e-06
Iter: 1691 loss: 7.62017044e-06
Iter: 1692 loss: 7.63052776e-06
Iter: 1693 loss: 7.61977208e-06
Iter: 1694 loss: 7.61848924e-06
Iter: 1695 loss: 7.61892034e-06
Iter: 1696 loss: 7.61749925e-06
Iter: 1697 loss: 7.6156839e-06
Iter: 1698 loss: 7.61696265e-06
Iter: 1699 loss: 7.61460069e-06
Iter: 1700 loss: 7.61260253e-06
Iter: 1701 loss: 7.61692172e-06
Iter: 1702 loss: 7.61183401e-06
Iter: 1703 loss: 7.60975854e-06
Iter: 1704 loss: 7.64166361e-06
Iter: 1705 loss: 7.60975308e-06
Iter: 1706 loss: 7.60862349e-06
Iter: 1707 loss: 7.60595867e-06
Iter: 1708 loss: 7.63883509e-06
Iter: 1709 loss: 7.60578132e-06
Iter: 1710 loss: 7.60482e-06
Iter: 1711 loss: 7.60447801e-06
Iter: 1712 loss: 7.60309649e-06
Iter: 1713 loss: 7.60198509e-06
Iter: 1714 loss: 7.60158127e-06
Iter: 1715 loss: 7.60002149e-06
Iter: 1716 loss: 7.59836894e-06
Iter: 1717 loss: 7.59808154e-06
Iter: 1718 loss: 7.59692421e-06
Iter: 1719 loss: 7.5965163e-06
Iter: 1720 loss: 7.59544446e-06
Iter: 1721 loss: 7.59316299e-06
Iter: 1722 loss: 7.63159187e-06
Iter: 1723 loss: 7.59310296e-06
Iter: 1724 loss: 7.59144496e-06
Iter: 1725 loss: 7.61542515e-06
Iter: 1726 loss: 7.59141221e-06
Iter: 1727 loss: 7.58964e-06
Iter: 1728 loss: 7.59210434e-06
Iter: 1729 loss: 7.58872238e-06
Iter: 1730 loss: 7.58731767e-06
Iter: 1731 loss: 7.58598162e-06
Iter: 1732 loss: 7.58568e-06
Iter: 1733 loss: 7.58320448e-06
Iter: 1734 loss: 7.5981452e-06
Iter: 1735 loss: 7.58286569e-06
Iter: 1736 loss: 7.58123269e-06
Iter: 1737 loss: 7.58103897e-06
Iter: 1738 loss: 7.57978432e-06
Iter: 1739 loss: 7.57844919e-06
Iter: 1740 loss: 7.57826683e-06
Iter: 1741 loss: 7.57731459e-06
Iter: 1742 loss: 7.57500311e-06
Iter: 1743 loss: 7.59621435e-06
Iter: 1744 loss: 7.57463431e-06
Iter: 1745 loss: 7.57373755e-06
Iter: 1746 loss: 7.57339603e-06
Iter: 1747 loss: 7.57209727e-06
Iter: 1748 loss: 7.57058751e-06
Iter: 1749 loss: 7.57043517e-06
Iter: 1750 loss: 7.56878444e-06
Iter: 1751 loss: 7.56765667e-06
Iter: 1752 loss: 7.56707641e-06
Iter: 1753 loss: 7.56505415e-06
Iter: 1754 loss: 7.58425904e-06
Iter: 1755 loss: 7.56496593e-06
Iter: 1756 loss: 7.56292411e-06
Iter: 1757 loss: 7.57625367e-06
Iter: 1758 loss: 7.56271857e-06
Iter: 1759 loss: 7.56168856e-06
Iter: 1760 loss: 7.55935e-06
Iter: 1761 loss: 7.59078966e-06
Iter: 1762 loss: 7.55919791e-06
Iter: 1763 loss: 7.5573e-06
Iter: 1764 loss: 7.58611895e-06
Iter: 1765 loss: 7.55727251e-06
Iter: 1766 loss: 7.55542442e-06
Iter: 1767 loss: 7.56737927e-06
Iter: 1768 loss: 7.55517613e-06
Iter: 1769 loss: 7.55405972e-06
Iter: 1770 loss: 7.55271049e-06
Iter: 1771 loss: 7.55258634e-06
Iter: 1772 loss: 7.55068686e-06
Iter: 1773 loss: 7.56376767e-06
Iter: 1774 loss: 7.55049859e-06
Iter: 1775 loss: 7.54954e-06
Iter: 1776 loss: 7.56030431e-06
Iter: 1777 loss: 7.54952271e-06
Iter: 1778 loss: 7.54855682e-06
Iter: 1779 loss: 7.54683106e-06
Iter: 1780 loss: 7.58646502e-06
Iter: 1781 loss: 7.54680514e-06
Iter: 1782 loss: 7.54517896e-06
Iter: 1783 loss: 7.55272958e-06
Iter: 1784 loss: 7.54485245e-06
Iter: 1785 loss: 7.54336861e-06
Iter: 1786 loss: 7.56040208e-06
Iter: 1787 loss: 7.54332768e-06
Iter: 1788 loss: 7.54247958e-06
Iter: 1789 loss: 7.5402686e-06
Iter: 1790 loss: 7.55108e-06
Iter: 1791 loss: 7.53942322e-06
Iter: 1792 loss: 7.53684617e-06
Iter: 1793 loss: 7.55483552e-06
Iter: 1794 loss: 7.53661425e-06
Iter: 1795 loss: 7.53534e-06
Iter: 1796 loss: 7.53527183e-06
Iter: 1797 loss: 7.53393761e-06
Iter: 1798 loss: 7.53446e-06
Iter: 1799 loss: 7.53297627e-06
Iter: 1800 loss: 7.53163886e-06
Iter: 1801 loss: 7.52914366e-06
Iter: 1802 loss: 7.58667102e-06
Iter: 1803 loss: 7.52914275e-06
Iter: 1804 loss: 7.53078893e-06
Iter: 1805 loss: 7.52823917e-06
Iter: 1806 loss: 7.5275e-06
Iter: 1807 loss: 7.52587221e-06
Iter: 1808 loss: 7.55198698e-06
Iter: 1809 loss: 7.52585856e-06
Iter: 1810 loss: 7.52448477e-06
Iter: 1811 loss: 7.53699851e-06
Iter: 1812 loss: 7.52444885e-06
Iter: 1813 loss: 7.52333563e-06
Iter: 1814 loss: 7.52759661e-06
Iter: 1815 loss: 7.52307551e-06
Iter: 1816 loss: 7.52197047e-06
Iter: 1817 loss: 7.52303094e-06
Iter: 1818 loss: 7.52127198e-06
Iter: 1819 loss: 7.5199996e-06
Iter: 1820 loss: 7.5193384e-06
Iter: 1821 loss: 7.51866583e-06
Iter: 1822 loss: 7.51765901e-06
Iter: 1823 loss: 7.51757398e-06
Iter: 1824 loss: 7.51650532e-06
Iter: 1825 loss: 7.51451125e-06
Iter: 1826 loss: 7.55690553e-06
Iter: 1827 loss: 7.51452126e-06
Iter: 1828 loss: 7.51288735e-06
Iter: 1829 loss: 7.51359e-06
Iter: 1830 loss: 7.51176322e-06
Iter: 1831 loss: 7.51051311e-06
Iter: 1832 loss: 7.51050447e-06
Iter: 1833 loss: 7.50896743e-06
Iter: 1834 loss: 7.50992876e-06
Iter: 1835 loss: 7.50801337e-06
Iter: 1836 loss: 7.50683193e-06
Iter: 1837 loss: 7.50527579e-06
Iter: 1838 loss: 7.50516256e-06
Iter: 1839 loss: 7.50540903e-06
Iter: 1840 loss: 7.50429808e-06
Iter: 1841 loss: 7.50360778e-06
Iter: 1842 loss: 7.50205299e-06
Iter: 1843 loss: 7.52549568e-06
Iter: 1844 loss: 7.50200797e-06
Iter: 1845 loss: 7.50053459e-06
Iter: 1846 loss: 7.50603749e-06
Iter: 1847 loss: 7.50019808e-06
Iter: 1848 loss: 7.49862602e-06
Iter: 1849 loss: 7.51110383e-06
Iter: 1850 loss: 7.49851915e-06
Iter: 1851 loss: 7.49755463e-06
Iter: 1852 loss: 7.49776245e-06
Iter: 1853 loss: 7.49674473e-06
Iter: 1854 loss: 7.49538458e-06
Iter: 1855 loss: 7.49624132e-06
Iter: 1856 loss: 7.49450373e-06
Iter: 1857 loss: 7.49336323e-06
Iter: 1858 loss: 7.49332139e-06
Iter: 1859 loss: 7.49225546e-06
Iter: 1860 loss: 7.49113815e-06
Iter: 1861 loss: 7.49089304e-06
Iter: 1862 loss: 7.48976709e-06
Iter: 1863 loss: 7.48801222e-06
Iter: 1864 loss: 7.48795355e-06
Iter: 1865 loss: 7.48716093e-06
Iter: 1866 loss: 7.48665843e-06
Iter: 1867 loss: 7.48551656e-06
Iter: 1868 loss: 7.48604725e-06
Iter: 1869 loss: 7.48462253e-06
Iter: 1870 loss: 7.48351931e-06
Iter: 1871 loss: 7.48326147e-06
Iter: 1872 loss: 7.48255388e-06
Iter: 1873 loss: 7.48179173e-06
Iter: 1874 loss: 7.48154e-06
Iter: 1875 loss: 7.48096636e-06
Iter: 1876 loss: 7.47940157e-06
Iter: 1877 loss: 7.48949788e-06
Iter: 1878 loss: 7.47905597e-06
Iter: 1879 loss: 7.47780632e-06
Iter: 1880 loss: 7.49179e-06
Iter: 1881 loss: 7.47780359e-06
Iter: 1882 loss: 7.47631839e-06
Iter: 1883 loss: 7.48023695e-06
Iter: 1884 loss: 7.47578133e-06
Iter: 1885 loss: 7.47468948e-06
Iter: 1886 loss: 7.47596e-06
Iter: 1887 loss: 7.47412196e-06
Iter: 1888 loss: 7.47277136e-06
Iter: 1889 loss: 7.47527156e-06
Iter: 1890 loss: 7.47226113e-06
Iter: 1891 loss: 7.47113245e-06
Iter: 1892 loss: 7.48675e-06
Iter: 1893 loss: 7.47107424e-06
Iter: 1894 loss: 7.47029662e-06
Iter: 1895 loss: 7.46849128e-06
Iter: 1896 loss: 7.49931496e-06
Iter: 1897 loss: 7.46847218e-06
Iter: 1898 loss: 7.46671822e-06
Iter: 1899 loss: 7.4700506e-06
Iter: 1900 loss: 7.46602973e-06
Iter: 1901 loss: 7.46481237e-06
Iter: 1902 loss: 7.46483511e-06
Iter: 1903 loss: 7.46341175e-06
Iter: 1904 loss: 7.46288833e-06
Iter: 1905 loss: 7.46217756e-06
Iter: 1906 loss: 7.46115211e-06
Iter: 1907 loss: 7.46340947e-06
Iter: 1908 loss: 7.46080786e-06
Iter: 1909 loss: 7.45932857e-06
Iter: 1910 loss: 7.46869864e-06
Iter: 1911 loss: 7.45915531e-06
Iter: 1912 loss: 7.45848365e-06
Iter: 1913 loss: 7.4569034e-06
Iter: 1914 loss: 7.48007915e-06
Iter: 1915 loss: 7.45689385e-06
Iter: 1916 loss: 7.45546413e-06
Iter: 1917 loss: 7.46749e-06
Iter: 1918 loss: 7.45535726e-06
Iter: 1919 loss: 7.45385751e-06
Iter: 1920 loss: 7.46396563e-06
Iter: 1921 loss: 7.45375155e-06
Iter: 1922 loss: 7.45290799e-06
Iter: 1923 loss: 7.45168654e-06
Iter: 1924 loss: 7.45162e-06
Iter: 1925 loss: 7.45037551e-06
Iter: 1926 loss: 7.46935757e-06
Iter: 1927 loss: 7.45035186e-06
Iter: 1928 loss: 7.44956196e-06
Iter: 1929 loss: 7.45315356e-06
Iter: 1930 loss: 7.44941372e-06
Iter: 1931 loss: 7.44855333e-06
Iter: 1932 loss: 7.44744284e-06
Iter: 1933 loss: 7.44740419e-06
Iter: 1934 loss: 7.44598765e-06
Iter: 1935 loss: 7.44479e-06
Iter: 1936 loss: 7.44441695e-06
Iter: 1937 loss: 7.4439904e-06
Iter: 1938 loss: 7.44344197e-06
Iter: 1939 loss: 7.44245153e-06
Iter: 1940 loss: 7.441261e-06
Iter: 1941 loss: 7.44112822e-06
Iter: 1942 loss: 7.43993178e-06
Iter: 1943 loss: 7.44509089e-06
Iter: 1944 loss: 7.439618e-06
Iter: 1945 loss: 7.43827377e-06
Iter: 1946 loss: 7.44913086e-06
Iter: 1947 loss: 7.43816418e-06
Iter: 1948 loss: 7.43746659e-06
Iter: 1949 loss: 7.43571854e-06
Iter: 1950 loss: 7.45516081e-06
Iter: 1951 loss: 7.43557121e-06
Iter: 1952 loss: 7.43426335e-06
Iter: 1953 loss: 7.44775434e-06
Iter: 1954 loss: 7.4342e-06
Iter: 1955 loss: 7.43256078e-06
Iter: 1956 loss: 7.43723285e-06
Iter: 1957 loss: 7.43208375e-06
Iter: 1958 loss: 7.43110786e-06
Iter: 1959 loss: 7.43015653e-06
Iter: 1960 loss: 7.4299569e-06
Iter: 1961 loss: 7.42879e-06
Iter: 1962 loss: 7.42878137e-06
Iter: 1963 loss: 7.42777956e-06
Iter: 1964 loss: 7.42893e-06
Iter: 1965 loss: 7.427323e-06
Iter: 1966 loss: 7.42621296e-06
Iter: 1967 loss: 7.42748307e-06
Iter: 1968 loss: 7.42557677e-06
Iter: 1969 loss: 7.42438124e-06
Iter: 1970 loss: 7.42335396e-06
Iter: 1971 loss: 7.4230843e-06
Iter: 1972 loss: 7.42168731e-06
Iter: 1973 loss: 7.44094632e-06
Iter: 1974 loss: 7.42167595e-06
Iter: 1975 loss: 7.42017755e-06
Iter: 1976 loss: 7.4233285e-06
Iter: 1977 loss: 7.41961685e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script80
+ '[' -r STOP.script80 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi1.2
+ date
Mon Nov  2 11:30:49 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a0169268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f805479bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a0110d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a011ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a0094378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80a0094a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054739620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054759950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054759598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054739598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054578730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80544d4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80544d4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054457620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054457950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054402d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80543fc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f805440ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80546c5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80546c5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80546b40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80546b4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f805435d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054359730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054359840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80545b6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054290840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054474620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054479620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f805448a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80542f68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80542d6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80542b4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80541869d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f80541a4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8054246d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.00025087054
test_loss: 0.00026597004
train_loss: 0.000100449004
test_loss: 0.000104083156
train_loss: 7.4870215e-05
test_loss: 7.1547605e-05
train_loss: 5.599969e-05
test_loss: 5.797235e-05
train_loss: 4.752586e-05
test_loss: 4.843311e-05
train_loss: 4.0742838e-05
test_loss: 4.445567e-05
train_loss: 4.1080664e-05
test_loss: 4.5239303e-05
train_loss: 3.5079043e-05
test_loss: 3.744623e-05
train_loss: 3.411966e-05
test_loss: 3.542311e-05
train_loss: 3.167053e-05
test_loss: 3.291175e-05
train_loss: 2.8689636e-05
test_loss: 3.1371503e-05
train_loss: 2.8962328e-05
test_loss: 3.203292e-05
train_loss: 2.8750734e-05
test_loss: 2.9068877e-05
train_loss: 2.6294867e-05
test_loss: 2.83582e-05
train_loss: 2.5237692e-05
test_loss: 2.7459004e-05
train_loss: 2.4253102e-05
test_loss: 2.6333932e-05
train_loss: 2.4173954e-05
test_loss: 2.526502e-05
train_loss: 2.3320525e-05
test_loss: 2.4926252e-05
train_loss: 2.2361193e-05
test_loss: 2.4246787e-05
train_loss: 2.3253939e-05
test_loss: 2.382923e-05
train_loss: 1.910225e-05
test_loss: 2.335132e-05
train_loss: 2.1558275e-05
test_loss: 2.2887865e-05
train_loss: 1.8811017e-05
test_loss: 2.2080589e-05
train_loss: 2.0461732e-05
test_loss: 2.1561402e-05
train_loss: 1.8784542e-05
test_loss: 2.1133026e-05
train_loss: 1.8747083e-05
test_loss: 2.0983443e-05
train_loss: 1.952659e-05
test_loss: 2.077218e-05
train_loss: 1.9416319e-05
test_loss: 2.053037e-05
train_loss: 1.8424049e-05
test_loss: 2.0046326e-05
train_loss: 1.6799857e-05
test_loss: 2.008354e-05
train_loss: 1.8186363e-05
test_loss: 1.9651621e-05
train_loss: 1.7149256e-05
test_loss: 1.9656312e-05
train_loss: 1.6421003e-05
test_loss: 1.948766e-05
train_loss: 1.6739095e-05
test_loss: 1.9240179e-05
train_loss: 1.6883954e-05
test_loss: 1.9079393e-05
train_loss: 1.595868e-05
test_loss: 1.9105335e-05
train_loss: 1.7712628e-05
test_loss: 1.8912857e-05
train_loss: 1.7662485e-05
test_loss: 1.8766472e-05
train_loss: 1.8275136e-05
test_loss: 1.8654588e-05
train_loss: 1.5707934e-05
test_loss: 1.8692168e-05
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fef127510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fef082b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fef082ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fef070048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fef0580d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0feefebe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fef01c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0feefac598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0feef97730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0feef87378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fc62c67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fc6211730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fc6211400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fc6277f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fc6254400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fc6242c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fc6246620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fa0104ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fa00eb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fa00ebd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fa007b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fa007b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0fc61ce950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f80097ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f80097400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f800707b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f80030730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f8004d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f80030ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f8004d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f14747950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f14717048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f14706268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f14736c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f146d08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0f146c4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.56770293e-05
Iter: 2 loss: 1.55479884e-05
Iter: 3 loss: 1.55480011e-05
Iter: 4 loss: 1.54746103e-05
Iter: 5 loss: 1.57254e-05
Iter: 6 loss: 1.54550016e-05
Iter: 7 loss: 1.54042518e-05
Iter: 8 loss: 1.55315684e-05
Iter: 9 loss: 1.53865e-05
Iter: 10 loss: 1.53410547e-05
Iter: 11 loss: 1.54936679e-05
Iter: 12 loss: 1.53287619e-05
Iter: 13 loss: 1.52847679e-05
Iter: 14 loss: 1.53999335e-05
Iter: 15 loss: 1.52699286e-05
Iter: 16 loss: 1.52356297e-05
Iter: 17 loss: 1.52243338e-05
Iter: 18 loss: 1.52045304e-05
Iter: 19 loss: 1.51754339e-05
Iter: 20 loss: 1.51740242e-05
Iter: 21 loss: 1.51529839e-05
Iter: 22 loss: 1.51106742e-05
Iter: 23 loss: 1.59014162e-05
Iter: 24 loss: 1.51100849e-05
Iter: 25 loss: 1.50864525e-05
Iter: 26 loss: 1.50838796e-05
Iter: 27 loss: 1.50597243e-05
Iter: 28 loss: 1.502334e-05
Iter: 29 loss: 1.50226324e-05
Iter: 30 loss: 1.49834395e-05
Iter: 31 loss: 1.50399474e-05
Iter: 32 loss: 1.49642856e-05
Iter: 33 loss: 1.49185289e-05
Iter: 34 loss: 1.49609714e-05
Iter: 35 loss: 1.4892159e-05
Iter: 36 loss: 1.48540985e-05
Iter: 37 loss: 1.50329925e-05
Iter: 38 loss: 1.48471536e-05
Iter: 39 loss: 1.48341342e-05
Iter: 40 loss: 1.48308991e-05
Iter: 41 loss: 1.48126855e-05
Iter: 42 loss: 1.48189847e-05
Iter: 43 loss: 1.47997644e-05
Iter: 44 loss: 1.47860901e-05
Iter: 45 loss: 1.48390536e-05
Iter: 46 loss: 1.47828814e-05
Iter: 47 loss: 1.47670507e-05
Iter: 48 loss: 1.48147119e-05
Iter: 49 loss: 1.47622986e-05
Iter: 50 loss: 1.4750276e-05
Iter: 51 loss: 1.47396049e-05
Iter: 52 loss: 1.47364872e-05
Iter: 53 loss: 1.47192477e-05
Iter: 54 loss: 1.49345015e-05
Iter: 55 loss: 1.4719084e-05
Iter: 56 loss: 1.47077062e-05
Iter: 57 loss: 1.47198225e-05
Iter: 58 loss: 1.47015235e-05
Iter: 59 loss: 1.46856619e-05
Iter: 60 loss: 1.47187457e-05
Iter: 61 loss: 1.467937e-05
Iter: 62 loss: 1.46691764e-05
Iter: 63 loss: 1.47126957e-05
Iter: 64 loss: 1.46670391e-05
Iter: 65 loss: 1.46533712e-05
Iter: 66 loss: 1.46545926e-05
Iter: 67 loss: 1.46428392e-05
Iter: 68 loss: 1.46301682e-05
Iter: 69 loss: 1.46285747e-05
Iter: 70 loss: 1.4619508e-05
Iter: 71 loss: 1.46033472e-05
Iter: 72 loss: 1.46475359e-05
Iter: 73 loss: 1.45980757e-05
Iter: 74 loss: 1.45812946e-05
Iter: 75 loss: 1.46294569e-05
Iter: 76 loss: 1.4576086e-05
Iter: 77 loss: 1.45760205e-05
Iter: 78 loss: 1.45701524e-05
Iter: 79 loss: 1.45642771e-05
Iter: 80 loss: 1.45503891e-05
Iter: 81 loss: 1.47144856e-05
Iter: 82 loss: 1.45492286e-05
Iter: 83 loss: 1.4542e-05
Iter: 84 loss: 1.45416907e-05
Iter: 85 loss: 1.45340928e-05
Iter: 86 loss: 1.45269578e-05
Iter: 87 loss: 1.45251415e-05
Iter: 88 loss: 1.45162703e-05
Iter: 89 loss: 1.45335716e-05
Iter: 90 loss: 1.45125687e-05
Iter: 91 loss: 1.45051863e-05
Iter: 92 loss: 1.46141938e-05
Iter: 93 loss: 1.45051708e-05
Iter: 94 loss: 1.4498597e-05
Iter: 95 loss: 1.44898531e-05
Iter: 96 loss: 1.44893229e-05
Iter: 97 loss: 1.44811675e-05
Iter: 98 loss: 1.46020011e-05
Iter: 99 loss: 1.44811984e-05
Iter: 100 loss: 1.44745109e-05
Iter: 101 loss: 1.44711648e-05
Iter: 102 loss: 1.44680389e-05
Iter: 103 loss: 1.44579835e-05
Iter: 104 loss: 1.45352478e-05
Iter: 105 loss: 1.44572605e-05
Iter: 106 loss: 1.44517689e-05
Iter: 107 loss: 1.44413098e-05
Iter: 108 loss: 1.46675802e-05
Iter: 109 loss: 1.44412134e-05
Iter: 110 loss: 1.44305413e-05
Iter: 111 loss: 1.44926726e-05
Iter: 112 loss: 1.44290971e-05
Iter: 113 loss: 1.44207788e-05
Iter: 114 loss: 1.44739715e-05
Iter: 115 loss: 1.44198812e-05
Iter: 116 loss: 1.44099668e-05
Iter: 117 loss: 1.44590877e-05
Iter: 118 loss: 1.44082915e-05
Iter: 119 loss: 1.44043988e-05
Iter: 120 loss: 1.43966508e-05
Iter: 121 loss: 1.45490412e-05
Iter: 122 loss: 1.43965226e-05
Iter: 123 loss: 1.43874677e-05
Iter: 124 loss: 1.43875068e-05
Iter: 125 loss: 1.43841498e-05
Iter: 126 loss: 1.43763191e-05
Iter: 127 loss: 1.44738478e-05
Iter: 128 loss: 1.43757352e-05
Iter: 129 loss: 1.43692023e-05
Iter: 130 loss: 1.43691623e-05
Iter: 131 loss: 1.43634061e-05
Iter: 132 loss: 1.43725565e-05
Iter: 133 loss: 1.43607595e-05
Iter: 134 loss: 1.435586e-05
Iter: 135 loss: 1.4360885e-05
Iter: 136 loss: 1.43531688e-05
Iter: 137 loss: 1.43471498e-05
Iter: 138 loss: 1.43914822e-05
Iter: 139 loss: 1.43466505e-05
Iter: 140 loss: 1.43430225e-05
Iter: 141 loss: 1.43445195e-05
Iter: 142 loss: 1.4340565e-05
Iter: 143 loss: 1.43346961e-05
Iter: 144 loss: 1.43405778e-05
Iter: 145 loss: 1.43314592e-05
Iter: 146 loss: 1.43260058e-05
Iter: 147 loss: 1.43182315e-05
Iter: 148 loss: 1.43179805e-05
Iter: 149 loss: 1.43144443e-05
Iter: 150 loss: 1.431282e-05
Iter: 151 loss: 1.43077104e-05
Iter: 152 loss: 1.4323824e-05
Iter: 153 loss: 1.43062171e-05
Iter: 154 loss: 1.43026482e-05
Iter: 155 loss: 1.42967801e-05
Iter: 156 loss: 1.42967183e-05
Iter: 157 loss: 1.42954013e-05
Iter: 158 loss: 1.42935924e-05
Iter: 159 loss: 1.42913e-05
Iter: 160 loss: 1.42849985e-05
Iter: 161 loss: 1.43211691e-05
Iter: 162 loss: 1.42832232e-05
Iter: 163 loss: 1.4278191e-05
Iter: 164 loss: 1.43458637e-05
Iter: 165 loss: 1.42781091e-05
Iter: 166 loss: 1.42728732e-05
Iter: 167 loss: 1.42914823e-05
Iter: 168 loss: 1.42715344e-05
Iter: 169 loss: 1.42674435e-05
Iter: 170 loss: 1.42642148e-05
Iter: 171 loss: 1.42629524e-05
Iter: 172 loss: 1.42576282e-05
Iter: 173 loss: 1.4257681e-05
Iter: 174 loss: 1.4254324e-05
Iter: 175 loss: 1.42538447e-05
Iter: 176 loss: 1.42515546e-05
Iter: 177 loss: 1.42474e-05
Iter: 178 loss: 1.42588979e-05
Iter: 179 loss: 1.42460112e-05
Iter: 180 loss: 1.42408153e-05
Iter: 181 loss: 1.42432245e-05
Iter: 182 loss: 1.42372628e-05
Iter: 183 loss: 1.42327735e-05
Iter: 184 loss: 1.4239964e-05
Iter: 185 loss: 1.42307526e-05
Iter: 186 loss: 1.42271938e-05
Iter: 187 loss: 1.42268327e-05
Iter: 188 loss: 1.42248919e-05
Iter: 189 loss: 1.42208019e-05
Iter: 190 loss: 1.42842691e-05
Iter: 191 loss: 1.42206572e-05
Iter: 192 loss: 1.42170111e-05
Iter: 193 loss: 1.42580011e-05
Iter: 194 loss: 1.42169811e-05
Iter: 195 loss: 1.4212872e-05
Iter: 196 loss: 1.42141653e-05
Iter: 197 loss: 1.42100434e-05
Iter: 198 loss: 1.42066019e-05
Iter: 199 loss: 1.42014251e-05
Iter: 200 loss: 1.42013823e-05
Iter: 201 loss: 1.42001991e-05
Iter: 202 loss: 1.41984929e-05
Iter: 203 loss: 1.41956043e-05
Iter: 204 loss: 1.4189749e-05
Iter: 205 loss: 1.43003954e-05
Iter: 206 loss: 1.41897099e-05
Iter: 207 loss: 1.41858518e-05
Iter: 208 loss: 1.42410681e-05
Iter: 209 loss: 1.41857727e-05
Iter: 210 loss: 1.41820838e-05
Iter: 211 loss: 1.41873725e-05
Iter: 212 loss: 1.4180132e-05
Iter: 213 loss: 1.41769669e-05
Iter: 214 loss: 1.41799692e-05
Iter: 215 loss: 1.41750907e-05
Iter: 216 loss: 1.41714509e-05
Iter: 217 loss: 1.41867586e-05
Iter: 218 loss: 1.41707515e-05
Iter: 219 loss: 1.41666987e-05
Iter: 220 loss: 1.41674318e-05
Iter: 221 loss: 1.41637702e-05
Iter: 222 loss: 1.41618075e-05
Iter: 223 loss: 1.416134e-05
Iter: 224 loss: 1.41588807e-05
Iter: 225 loss: 1.41544124e-05
Iter: 226 loss: 1.41544569e-05
Iter: 227 loss: 1.41504715e-05
Iter: 228 loss: 1.41546261e-05
Iter: 229 loss: 1.41482969e-05
Iter: 230 loss: 1.41456876e-05
Iter: 231 loss: 1.41452801e-05
Iter: 232 loss: 1.41435648e-05
Iter: 233 loss: 1.41395194e-05
Iter: 234 loss: 1.41866203e-05
Iter: 235 loss: 1.4139152e-05
Iter: 236 loss: 1.4134107e-05
Iter: 237 loss: 1.41472956e-05
Iter: 238 loss: 1.4132349e-05
Iter: 239 loss: 1.41301507e-05
Iter: 240 loss: 1.41295541e-05
Iter: 241 loss: 1.41275505e-05
Iter: 242 loss: 1.41221117e-05
Iter: 243 loss: 1.41592272e-05
Iter: 244 loss: 1.41208438e-05
Iter: 245 loss: 1.41189439e-05
Iter: 246 loss: 1.41178625e-05
Iter: 247 loss: 1.41151122e-05
Iter: 248 loss: 1.41120345e-05
Iter: 249 loss: 1.41116725e-05
Iter: 250 loss: 1.41077035e-05
Iter: 251 loss: 1.411226e-05
Iter: 252 loss: 1.41055216e-05
Iter: 253 loss: 1.4101448e-05
Iter: 254 loss: 1.4131454e-05
Iter: 255 loss: 1.41010687e-05
Iter: 256 loss: 1.40976917e-05
Iter: 257 loss: 1.41249593e-05
Iter: 258 loss: 1.40974489e-05
Iter: 259 loss: 1.40948932e-05
Iter: 260 loss: 1.41068103e-05
Iter: 261 loss: 1.40943821e-05
Iter: 262 loss: 1.40921857e-05
Iter: 263 loss: 1.40867523e-05
Iter: 264 loss: 1.41360124e-05
Iter: 265 loss: 1.40859029e-05
Iter: 266 loss: 1.40860839e-05
Iter: 267 loss: 1.40838729e-05
Iter: 268 loss: 1.40817783e-05
Iter: 269 loss: 1.40774173e-05
Iter: 270 loss: 1.41527071e-05
Iter: 271 loss: 1.40772654e-05
Iter: 272 loss: 1.40732163e-05
Iter: 273 loss: 1.40815146e-05
Iter: 274 loss: 1.40716329e-05
Iter: 275 loss: 1.40673747e-05
Iter: 276 loss: 1.40943985e-05
Iter: 277 loss: 1.40668963e-05
Iter: 278 loss: 1.40623742e-05
Iter: 279 loss: 1.40857728e-05
Iter: 280 loss: 1.40617194e-05
Iter: 281 loss: 1.40596694e-05
Iter: 282 loss: 1.40556131e-05
Iter: 283 loss: 1.41296905e-05
Iter: 284 loss: 1.40555512e-05
Iter: 285 loss: 1.40517095e-05
Iter: 286 loss: 1.40515112e-05
Iter: 287 loss: 1.40493203e-05
Iter: 288 loss: 1.40451648e-05
Iter: 289 loss: 1.41408782e-05
Iter: 290 loss: 1.40451521e-05
Iter: 291 loss: 1.40408902e-05
Iter: 292 loss: 1.40510137e-05
Iter: 293 loss: 1.40394186e-05
Iter: 294 loss: 1.40366665e-05
Iter: 295 loss: 1.40363572e-05
Iter: 296 loss: 1.40337206e-05
Iter: 297 loss: 1.40359152e-05
Iter: 298 loss: 1.40321245e-05
Iter: 299 loss: 1.40297543e-05
Iter: 300 loss: 1.40339816e-05
Iter: 301 loss: 1.4028672e-05
Iter: 302 loss: 1.40258126e-05
Iter: 303 loss: 1.40243465e-05
Iter: 304 loss: 1.40229167e-05
Iter: 305 loss: 1.40207176e-05
Iter: 306 loss: 1.4020352e-05
Iter: 307 loss: 1.40187094e-05
Iter: 308 loss: 1.40141365e-05
Iter: 309 loss: 1.40347202e-05
Iter: 310 loss: 1.40124284e-05
Iter: 311 loss: 1.40075281e-05
Iter: 312 loss: 1.40566808e-05
Iter: 313 loss: 1.40073362e-05
Iter: 314 loss: 1.4004333e-05
Iter: 315 loss: 1.40480897e-05
Iter: 316 loss: 1.40043812e-05
Iter: 317 loss: 1.40014408e-05
Iter: 318 loss: 1.39978374e-05
Iter: 319 loss: 1.39975109e-05
Iter: 320 loss: 1.39938084e-05
Iter: 321 loss: 1.40004e-05
Iter: 322 loss: 1.3992184e-05
Iter: 323 loss: 1.39878866e-05
Iter: 324 loss: 1.40393167e-05
Iter: 325 loss: 1.39878575e-05
Iter: 326 loss: 1.39857593e-05
Iter: 327 loss: 1.39813765e-05
Iter: 328 loss: 1.40519505e-05
Iter: 329 loss: 1.39812355e-05
Iter: 330 loss: 1.3977351e-05
Iter: 331 loss: 1.40245174e-05
Iter: 332 loss: 1.39773156e-05
Iter: 333 loss: 1.39737731e-05
Iter: 334 loss: 1.40038774e-05
Iter: 335 loss: 1.39736185e-05
Iter: 336 loss: 1.39717431e-05
Iter: 337 loss: 1.39685944e-05
Iter: 338 loss: 1.39686072e-05
Iter: 339 loss: 1.39649437e-05
Iter: 340 loss: 1.39892454e-05
Iter: 341 loss: 1.39646099e-05
Iter: 342 loss: 1.39611248e-05
Iter: 343 loss: 1.39691892e-05
Iter: 344 loss: 1.39599188e-05
Iter: 345 loss: 1.39561707e-05
Iter: 346 loss: 1.39735794e-05
Iter: 347 loss: 1.39554541e-05
Iter: 348 loss: 1.3953073e-05
Iter: 349 loss: 1.39483254e-05
Iter: 350 loss: 1.40425582e-05
Iter: 351 loss: 1.39482609e-05
Iter: 352 loss: 1.3943476e-05
Iter: 353 loss: 1.3974437e-05
Iter: 354 loss: 1.39430012e-05
Iter: 355 loss: 1.39399308e-05
Iter: 356 loss: 1.39399372e-05
Iter: 357 loss: 1.39381873e-05
Iter: 358 loss: 1.39347976e-05
Iter: 359 loss: 1.39981184e-05
Iter: 360 loss: 1.39347485e-05
Iter: 361 loss: 1.3931176e-05
Iter: 362 loss: 1.39529493e-05
Iter: 363 loss: 1.39306867e-05
Iter: 364 loss: 1.39272906e-05
Iter: 365 loss: 1.39505273e-05
Iter: 366 loss: 1.39269196e-05
Iter: 367 loss: 1.39248423e-05
Iter: 368 loss: 1.39193016e-05
Iter: 369 loss: 1.39663616e-05
Iter: 370 loss: 1.39183421e-05
Iter: 371 loss: 1.39249714e-05
Iter: 372 loss: 1.39167523e-05
Iter: 373 loss: 1.39153417e-05
Iter: 374 loss: 1.39114036e-05
Iter: 375 loss: 1.39331614e-05
Iter: 376 loss: 1.39102585e-05
Iter: 377 loss: 1.39060467e-05
Iter: 378 loss: 1.39252743e-05
Iter: 379 loss: 1.39052863e-05
Iter: 380 loss: 1.39021686e-05
Iter: 381 loss: 1.39460872e-05
Iter: 382 loss: 1.39021877e-05
Iter: 383 loss: 1.38995692e-05
Iter: 384 loss: 1.39003205e-05
Iter: 385 loss: 1.38976829e-05
Iter: 386 loss: 1.38944715e-05
Iter: 387 loss: 1.39060276e-05
Iter: 388 loss: 1.38937348e-05
Iter: 389 loss: 1.38908517e-05
Iter: 390 loss: 1.38869291e-05
Iter: 391 loss: 1.38867654e-05
Iter: 392 loss: 1.38839114e-05
Iter: 393 loss: 1.38838204e-05
Iter: 394 loss: 1.3880529e-05
Iter: 395 loss: 1.38813211e-05
Iter: 396 loss: 1.38782279e-05
Iter: 397 loss: 1.38750947e-05
Iter: 398 loss: 1.38759297e-05
Iter: 399 loss: 1.38728983e-05
Iter: 400 loss: 1.3869465e-05
Iter: 401 loss: 1.38990972e-05
Iter: 402 loss: 1.38692994e-05
Iter: 403 loss: 1.38656869e-05
Iter: 404 loss: 1.38704945e-05
Iter: 405 loss: 1.38637915e-05
Iter: 406 loss: 1.38612304e-05
Iter: 407 loss: 1.38599517e-05
Iter: 408 loss: 1.38587347e-05
Iter: 409 loss: 1.38548057e-05
Iter: 410 loss: 1.38548348e-05
Iter: 411 loss: 1.38531632e-05
Iter: 412 loss: 1.38488185e-05
Iter: 413 loss: 1.38824862e-05
Iter: 414 loss: 1.38479918e-05
Iter: 415 loss: 1.38430914e-05
Iter: 416 loss: 1.38713676e-05
Iter: 417 loss: 1.38424821e-05
Iter: 418 loss: 1.38386877e-05
Iter: 419 loss: 1.3838704e-05
Iter: 420 loss: 1.38360665e-05
Iter: 421 loss: 1.38326313e-05
Iter: 422 loss: 1.38324267e-05
Iter: 423 loss: 1.38289961e-05
Iter: 424 loss: 1.38529776e-05
Iter: 425 loss: 1.38286887e-05
Iter: 426 loss: 1.38251226e-05
Iter: 427 loss: 1.38239102e-05
Iter: 428 loss: 1.38218693e-05
Iter: 429 loss: 1.38192572e-05
Iter: 430 loss: 1.3819139e-05
Iter: 431 loss: 1.38166542e-05
Iter: 432 loss: 1.38116138e-05
Iter: 433 loss: 1.3900858e-05
Iter: 434 loss: 1.38115774e-05
Iter: 435 loss: 1.38066134e-05
Iter: 436 loss: 1.38261394e-05
Iter: 437 loss: 1.38054638e-05
Iter: 438 loss: 1.38023042e-05
Iter: 439 loss: 1.3802337e-05
Iter: 440 loss: 1.37996449e-05
Iter: 441 loss: 1.37966763e-05
Iter: 442 loss: 1.37962361e-05
Iter: 443 loss: 1.37941715e-05
Iter: 444 loss: 1.37939587e-05
Iter: 445 loss: 1.37916013e-05
Iter: 446 loss: 1.37868883e-05
Iter: 447 loss: 1.38843625e-05
Iter: 448 loss: 1.37868701e-05
Iter: 449 loss: 1.37831757e-05
Iter: 450 loss: 1.37817215e-05
Iter: 451 loss: 1.3779706e-05
Iter: 452 loss: 1.37765928e-05
Iter: 453 loss: 1.37764382e-05
Iter: 454 loss: 1.37732113e-05
Iter: 455 loss: 1.37795669e-05
Iter: 456 loss: 1.3771818e-05
Iter: 457 loss: 1.37690404e-05
Iter: 458 loss: 1.37641418e-05
Iter: 459 loss: 1.37641073e-05
Iter: 460 loss: 1.37594516e-05
Iter: 461 loss: 1.38088162e-05
Iter: 462 loss: 1.37593524e-05
Iter: 463 loss: 1.37551597e-05
Iter: 464 loss: 1.37772295e-05
Iter: 465 loss: 1.37545521e-05
Iter: 466 loss: 1.37515508e-05
Iter: 467 loss: 1.37613588e-05
Iter: 468 loss: 1.37507068e-05
Iter: 469 loss: 1.37473489e-05
Iter: 470 loss: 1.37452544e-05
Iter: 471 loss: 1.37438528e-05
Iter: 472 loss: 1.37402076e-05
Iter: 473 loss: 1.3748564e-05
Iter: 474 loss: 1.3738867e-05
Iter: 475 loss: 1.37346797e-05
Iter: 476 loss: 1.37774769e-05
Iter: 477 loss: 1.37344923e-05
Iter: 478 loss: 1.37318057e-05
Iter: 479 loss: 1.37337811e-05
Iter: 480 loss: 1.37302522e-05
Iter: 481 loss: 1.37267261e-05
Iter: 482 loss: 1.37528805e-05
Iter: 483 loss: 1.37264542e-05
Iter: 484 loss: 1.37239849e-05
Iter: 485 loss: 1.37187253e-05
Iter: 486 loss: 1.3803382e-05
Iter: 487 loss: 1.37185361e-05
Iter: 488 loss: 1.3713925e-05
Iter: 489 loss: 1.37138959e-05
Iter: 490 loss: 1.37102161e-05
Iter: 491 loss: 1.3707483e-05
Iter: 492 loss: 1.37064471e-05
Iter: 493 loss: 1.37028437e-05
Iter: 494 loss: 1.37051466e-05
Iter: 495 loss: 1.37005782e-05
Iter: 496 loss: 1.36968283e-05
Iter: 497 loss: 1.36920771e-05
Iter: 498 loss: 1.36916742e-05
Iter: 499 loss: 1.36868593e-05
Iter: 500 loss: 1.37307525e-05
Iter: 501 loss: 1.36867784e-05
Iter: 502 loss: 1.36815233e-05
Iter: 503 loss: 1.3704881e-05
Iter: 504 loss: 1.36804028e-05
Iter: 505 loss: 1.36774961e-05
Iter: 506 loss: 1.36751351e-05
Iter: 507 loss: 1.3674211e-05
Iter: 508 loss: 1.3669176e-05
Iter: 509 loss: 1.37015668e-05
Iter: 510 loss: 1.36686522e-05
Iter: 511 loss: 1.36651724e-05
Iter: 512 loss: 1.36765575e-05
Iter: 513 loss: 1.36642029e-05
Iter: 514 loss: 1.3660916e-05
Iter: 515 loss: 1.3683838e-05
Iter: 516 loss: 1.36606141e-05
Iter: 517 loss: 1.36577855e-05
Iter: 518 loss: 1.36584176e-05
Iter: 519 loss: 1.36555564e-05
Iter: 520 loss: 1.36511972e-05
Iter: 521 loss: 1.36683684e-05
Iter: 522 loss: 1.36501158e-05
Iter: 523 loss: 1.36472208e-05
Iter: 524 loss: 1.36405342e-05
Iter: 525 loss: 1.37261522e-05
Iter: 526 loss: 1.36400595e-05
Iter: 527 loss: 1.36342787e-05
Iter: 528 loss: 1.36683893e-05
Iter: 529 loss: 1.36335402e-05
Iter: 530 loss: 1.36304297e-05
Iter: 531 loss: 1.36301551e-05
Iter: 532 loss: 1.36270646e-05
Iter: 533 loss: 1.36237513e-05
Iter: 534 loss: 1.3623242e-05
Iter: 535 loss: 1.36189501e-05
Iter: 536 loss: 1.36151339e-05
Iter: 537 loss: 1.36141352e-05
Iter: 538 loss: 1.36116178e-05
Iter: 539 loss: 1.36105764e-05
Iter: 540 loss: 1.36071467e-05
Iter: 541 loss: 1.36054623e-05
Iter: 542 loss: 1.36038179e-05
Iter: 543 loss: 1.36000608e-05
Iter: 544 loss: 1.35980263e-05
Iter: 545 loss: 1.3596301e-05
Iter: 546 loss: 1.35927185e-05
Iter: 547 loss: 1.35925638e-05
Iter: 548 loss: 1.35895789e-05
Iter: 549 loss: 1.35973205e-05
Iter: 550 loss: 1.35884693e-05
Iter: 551 loss: 1.35852679e-05
Iter: 552 loss: 1.35941045e-05
Iter: 553 loss: 1.35841383e-05
Iter: 554 loss: 1.35804257e-05
Iter: 555 loss: 1.3581237e-05
Iter: 556 loss: 1.35777118e-05
Iter: 557 loss: 1.35733826e-05
Iter: 558 loss: 1.35949958e-05
Iter: 559 loss: 1.35726905e-05
Iter: 560 loss: 1.35692553e-05
Iter: 561 loss: 1.35629662e-05
Iter: 562 loss: 1.37136e-05
Iter: 563 loss: 1.35630071e-05
Iter: 564 loss: 1.35570199e-05
Iter: 565 loss: 1.35955961e-05
Iter: 566 loss: 1.35564742e-05
Iter: 567 loss: 1.35526398e-05
Iter: 568 loss: 1.35524779e-05
Iter: 569 loss: 1.35502296e-05
Iter: 570 loss: 1.35448099e-05
Iter: 571 loss: 1.36053186e-05
Iter: 572 loss: 1.35442606e-05
Iter: 573 loss: 1.3538338e-05
Iter: 574 loss: 1.35632836e-05
Iter: 575 loss: 1.35371265e-05
Iter: 576 loss: 1.35344435e-05
Iter: 577 loss: 1.35340088e-05
Iter: 578 loss: 1.35313312e-05
Iter: 579 loss: 1.35249547e-05
Iter: 580 loss: 1.36049903e-05
Iter: 581 loss: 1.35244791e-05
Iter: 582 loss: 1.35187483e-05
Iter: 583 loss: 1.35299269e-05
Iter: 584 loss: 1.35163364e-05
Iter: 585 loss: 1.35135124e-05
Iter: 586 loss: 1.35124437e-05
Iter: 587 loss: 1.35096543e-05
Iter: 588 loss: 1.35076e-05
Iter: 589 loss: 1.35066612e-05
Iter: 590 loss: 1.35024784e-05
Iter: 591 loss: 1.35280206e-05
Iter: 592 loss: 1.35019545e-05
Iter: 593 loss: 1.34983147e-05
Iter: 594 loss: 1.34938955e-05
Iter: 595 loss: 1.34935472e-05
Iter: 596 loss: 1.34882703e-05
Iter: 597 loss: 1.35326718e-05
Iter: 598 loss: 1.34879574e-05
Iter: 599 loss: 1.34835682e-05
Iter: 600 loss: 1.34847496e-05
Iter: 601 loss: 1.34804832e-05
Iter: 602 loss: 1.34764432e-05
Iter: 603 loss: 1.35005284e-05
Iter: 604 loss: 1.34760812e-05
Iter: 605 loss: 1.34705915e-05
Iter: 606 loss: 1.34759466e-05
Iter: 607 loss: 1.34675474e-05
Iter: 608 loss: 1.34641432e-05
Iter: 609 loss: 1.34590518e-05
Iter: 610 loss: 1.34589154e-05
Iter: 611 loss: 1.3452649e-05
Iter: 612 loss: 1.35064438e-05
Iter: 613 loss: 1.34522688e-05
Iter: 614 loss: 1.34482834e-05
Iter: 615 loss: 1.34483689e-05
Iter: 616 loss: 1.34454658e-05
Iter: 617 loss: 1.34387365e-05
Iter: 618 loss: 1.35148894e-05
Iter: 619 loss: 1.34380525e-05
Iter: 620 loss: 1.34318725e-05
Iter: 621 loss: 1.34594675e-05
Iter: 622 loss: 1.34306883e-05
Iter: 623 loss: 1.34267602e-05
Iter: 624 loss: 1.34263828e-05
Iter: 625 loss: 1.34237325e-05
Iter: 626 loss: 1.34189768e-05
Iter: 627 loss: 1.35367864e-05
Iter: 628 loss: 1.34190013e-05
Iter: 629 loss: 1.34149095e-05
Iter: 630 loss: 1.34680886e-05
Iter: 631 loss: 1.34148286e-05
Iter: 632 loss: 1.34110924e-05
Iter: 633 loss: 1.34068396e-05
Iter: 634 loss: 1.34062848e-05
Iter: 635 loss: 1.34009533e-05
Iter: 636 loss: 1.33979629e-05
Iter: 637 loss: 1.33956337e-05
Iter: 638 loss: 1.33912472e-05
Iter: 639 loss: 1.33906342e-05
Iter: 640 loss: 1.33868616e-05
Iter: 641 loss: 1.33811463e-05
Iter: 642 loss: 1.3380999e-05
Iter: 643 loss: 1.33757858e-05
Iter: 644 loss: 1.33757858e-05
Iter: 645 loss: 1.33711364e-05
Iter: 646 loss: 1.33869889e-05
Iter: 647 loss: 1.33698777e-05
Iter: 648 loss: 1.33662943e-05
Iter: 649 loss: 1.33594085e-05
Iter: 650 loss: 1.349784e-05
Iter: 651 loss: 1.33594031e-05
Iter: 652 loss: 1.33543117e-05
Iter: 653 loss: 1.34343572e-05
Iter: 654 loss: 1.33542972e-05
Iter: 655 loss: 1.3348601e-05
Iter: 656 loss: 1.33564281e-05
Iter: 657 loss: 1.33456888e-05
Iter: 658 loss: 1.33411013e-05
Iter: 659 loss: 1.33395424e-05
Iter: 660 loss: 1.33369031e-05
Iter: 661 loss: 1.33319754e-05
Iter: 662 loss: 1.33805679e-05
Iter: 663 loss: 1.33317244e-05
Iter: 664 loss: 1.33264366e-05
Iter: 665 loss: 1.33494832e-05
Iter: 666 loss: 1.33253661e-05
Iter: 667 loss: 1.3322152e-05
Iter: 668 loss: 1.33237572e-05
Iter: 669 loss: 1.33199192e-05
Iter: 670 loss: 1.33146486e-05
Iter: 671 loss: 1.33244139e-05
Iter: 672 loss: 1.33123267e-05
Iter: 673 loss: 1.33084923e-05
Iter: 674 loss: 1.33025151e-05
Iter: 675 loss: 1.33024205e-05
Iter: 676 loss: 1.32963287e-05
Iter: 677 loss: 1.33811945e-05
Iter: 678 loss: 1.32963669e-05
Iter: 679 loss: 1.32914211e-05
Iter: 680 loss: 1.33022404e-05
Iter: 681 loss: 1.32895029e-05
Iter: 682 loss: 1.32844743e-05
Iter: 683 loss: 1.32863233e-05
Iter: 684 loss: 1.328086e-05
Iter: 685 loss: 1.32754931e-05
Iter: 686 loss: 1.3315017e-05
Iter: 687 loss: 1.32751156e-05
Iter: 688 loss: 1.32688319e-05
Iter: 689 loss: 1.32908572e-05
Iter: 690 loss: 1.32673022e-05
Iter: 691 loss: 1.32640525e-05
Iter: 692 loss: 1.3257677e-05
Iter: 693 loss: 1.33945678e-05
Iter: 694 loss: 1.32577597e-05
Iter: 695 loss: 1.32557379e-05
Iter: 696 loss: 1.32542345e-05
Iter: 697 loss: 1.32508248e-05
Iter: 698 loss: 1.32450095e-05
Iter: 699 loss: 1.33890453e-05
Iter: 700 loss: 1.32449368e-05
Iter: 701 loss: 1.32388959e-05
Iter: 702 loss: 1.32524274e-05
Iter: 703 loss: 1.32366604e-05
Iter: 704 loss: 1.32334835e-05
Iter: 705 loss: 1.32330597e-05
Iter: 706 loss: 1.3229821e-05
Iter: 707 loss: 1.3227198e-05
Iter: 708 loss: 1.32262467e-05
Iter: 709 loss: 1.32221849e-05
Iter: 710 loss: 1.32835021e-05
Iter: 711 loss: 1.32222185e-05
Iter: 712 loss: 1.32197429e-05
Iter: 713 loss: 1.32137902e-05
Iter: 714 loss: 1.32802188e-05
Iter: 715 loss: 1.32132263e-05
Iter: 716 loss: 1.32072473e-05
Iter: 717 loss: 1.32336581e-05
Iter: 718 loss: 1.32059995e-05
Iter: 719 loss: 1.31992465e-05
Iter: 720 loss: 1.32276909e-05
Iter: 721 loss: 1.31978986e-05
Iter: 722 loss: 1.31928227e-05
Iter: 723 loss: 1.31959587e-05
Iter: 724 loss: 1.31896313e-05
Iter: 725 loss: 1.31841789e-05
Iter: 726 loss: 1.32201894e-05
Iter: 727 loss: 1.31835841e-05
Iter: 728 loss: 1.31784964e-05
Iter: 729 loss: 1.32190535e-05
Iter: 730 loss: 1.31781726e-05
Iter: 731 loss: 1.31756042e-05
Iter: 732 loss: 1.31683883e-05
Iter: 733 loss: 1.32046043e-05
Iter: 734 loss: 1.31660236e-05
Iter: 735 loss: 1.31662582e-05
Iter: 736 loss: 1.31619345e-05
Iter: 737 loss: 1.31584547e-05
Iter: 738 loss: 1.31535635e-05
Iter: 739 loss: 1.31533807e-05
Iter: 740 loss: 1.31473143e-05
Iter: 741 loss: 1.31524694e-05
Iter: 742 loss: 1.31437837e-05
Iter: 743 loss: 1.31401221e-05
Iter: 744 loss: 1.3139851e-05
Iter: 745 loss: 1.3136e-05
Iter: 746 loss: 1.31339721e-05
Iter: 747 loss: 1.31322413e-05
Iter: 748 loss: 1.31291654e-05
Iter: 749 loss: 1.31289617e-05
Iter: 750 loss: 1.31269144e-05
Iter: 751 loss: 1.3120738e-05
Iter: 752 loss: 1.31439629e-05
Iter: 753 loss: 1.31180823e-05
Iter: 754 loss: 1.3110739e-05
Iter: 755 loss: 1.31785955e-05
Iter: 756 loss: 1.31104771e-05
Iter: 757 loss: 1.31036995e-05
Iter: 758 loss: 1.3125682e-05
Iter: 759 loss: 1.31018587e-05
Iter: 760 loss: 1.309654e-05
Iter: 761 loss: 1.31063725e-05
Iter: 762 loss: 1.30943517e-05
Iter: 763 loss: 1.30902154e-05
Iter: 764 loss: 1.31479464e-05
Iter: 765 loss: 1.30901826e-05
Iter: 766 loss: 1.30857115e-05
Iter: 767 loss: 1.30794888e-05
Iter: 768 loss: 1.30792669e-05
Iter: 769 loss: 1.3073879e-05
Iter: 770 loss: 1.3076412e-05
Iter: 771 loss: 1.30703083e-05
Iter: 772 loss: 1.30638537e-05
Iter: 773 loss: 1.30638609e-05
Iter: 774 loss: 1.30596509e-05
Iter: 775 loss: 1.30528551e-05
Iter: 776 loss: 1.30528042e-05
Iter: 777 loss: 1.30463923e-05
Iter: 778 loss: 1.30723038e-05
Iter: 779 loss: 1.30449798e-05
Iter: 780 loss: 1.30413137e-05
Iter: 781 loss: 1.30410954e-05
Iter: 782 loss: 1.30381377e-05
Iter: 783 loss: 1.3034276e-05
Iter: 784 loss: 1.30339968e-05
Iter: 785 loss: 1.30280005e-05
Iter: 786 loss: 1.30847193e-05
Iter: 787 loss: 1.30277967e-05
Iter: 788 loss: 1.30249191e-05
Iter: 789 loss: 1.30178259e-05
Iter: 790 loss: 1.30891922e-05
Iter: 791 loss: 1.30169701e-05
Iter: 792 loss: 1.3011073e-05
Iter: 793 loss: 1.30110548e-05
Iter: 794 loss: 1.30060953e-05
Iter: 795 loss: 1.30094413e-05
Iter: 796 loss: 1.30029084e-05
Iter: 797 loss: 1.29964765e-05
Iter: 798 loss: 1.30095123e-05
Iter: 799 loss: 1.29939062e-05
Iter: 800 loss: 1.29906493e-05
Iter: 801 loss: 1.29901255e-05
Iter: 802 loss: 1.29875898e-05
Iter: 803 loss: 1.29801256e-05
Iter: 804 loss: 1.30135877e-05
Iter: 805 loss: 1.29774407e-05
Iter: 806 loss: 1.29712162e-05
Iter: 807 loss: 1.29711852e-05
Iter: 808 loss: 1.29647615e-05
Iter: 809 loss: 1.29837699e-05
Iter: 810 loss: 1.29628925e-05
Iter: 811 loss: 1.29585205e-05
Iter: 812 loss: 1.2955662e-05
Iter: 813 loss: 1.29540294e-05
Iter: 814 loss: 1.29486243e-05
Iter: 815 loss: 1.299225e-05
Iter: 816 loss: 1.29482451e-05
Iter: 817 loss: 1.29424216e-05
Iter: 818 loss: 1.29622276e-05
Iter: 819 loss: 1.29408654e-05
Iter: 820 loss: 1.29377386e-05
Iter: 821 loss: 1.29665368e-05
Iter: 822 loss: 1.29375421e-05
Iter: 823 loss: 1.29343443e-05
Iter: 824 loss: 1.2926861e-05
Iter: 825 loss: 1.30164317e-05
Iter: 826 loss: 1.29261916e-05
Iter: 827 loss: 1.29193413e-05
Iter: 828 loss: 1.29372993e-05
Iter: 829 loss: 1.29170103e-05
Iter: 830 loss: 1.29095224e-05
Iter: 831 loss: 1.29584823e-05
Iter: 832 loss: 1.29087184e-05
Iter: 833 loss: 1.29031341e-05
Iter: 834 loss: 1.2921997e-05
Iter: 835 loss: 1.29017062e-05
Iter: 836 loss: 1.28974134e-05
Iter: 837 loss: 1.29067939e-05
Iter: 838 loss: 1.28957226e-05
Iter: 839 loss: 1.28888832e-05
Iter: 840 loss: 1.29064274e-05
Iter: 841 loss: 1.28866386e-05
Iter: 842 loss: 1.28825814e-05
Iter: 843 loss: 1.28761812e-05
Iter: 844 loss: 1.28761367e-05
Iter: 845 loss: 1.28723359e-05
Iter: 846 loss: 1.28714746e-05
Iter: 847 loss: 1.28672928e-05
Iter: 848 loss: 1.28627889e-05
Iter: 849 loss: 1.28621286e-05
Iter: 850 loss: 1.28562851e-05
Iter: 851 loss: 1.28628526e-05
Iter: 852 loss: 1.28531919e-05
Iter: 853 loss: 1.28498868e-05
Iter: 854 loss: 1.2849423e-05
Iter: 855 loss: 1.28460706e-05
Iter: 856 loss: 1.28419088e-05
Iter: 857 loss: 1.2841575e-05
Iter: 858 loss: 1.28365846e-05
Iter: 859 loss: 1.29141699e-05
Iter: 860 loss: 1.28365546e-05
Iter: 861 loss: 1.28338615e-05
Iter: 862 loss: 1.28260453e-05
Iter: 863 loss: 1.28661886e-05
Iter: 864 loss: 1.28235697e-05
Iter: 865 loss: 1.28147331e-05
Iter: 866 loss: 1.28772945e-05
Iter: 867 loss: 1.28140509e-05
Iter: 868 loss: 1.28066204e-05
Iter: 869 loss: 1.28287384e-05
Iter: 870 loss: 1.28044485e-05
Iter: 871 loss: 1.28006177e-05
Iter: 872 loss: 1.28001457e-05
Iter: 873 loss: 1.27969852e-05
Iter: 874 loss: 1.27957674e-05
Iter: 875 loss: 1.27941503e-05
Iter: 876 loss: 1.2788114e-05
Iter: 877 loss: 1.27938965e-05
Iter: 878 loss: 1.27847106e-05
Iter: 879 loss: 1.27797866e-05
Iter: 880 loss: 1.27849707e-05
Iter: 881 loss: 1.27769381e-05
Iter: 882 loss: 1.277105e-05
Iter: 883 loss: 1.28452693e-05
Iter: 884 loss: 1.27709764e-05
Iter: 885 loss: 1.2767222e-05
Iter: 886 loss: 1.27616677e-05
Iter: 887 loss: 1.27615631e-05
Iter: 888 loss: 1.27558887e-05
Iter: 889 loss: 1.28005086e-05
Iter: 890 loss: 1.27555932e-05
Iter: 891 loss: 1.27496423e-05
Iter: 892 loss: 1.27786989e-05
Iter: 893 loss: 1.27486146e-05
Iter: 894 loss: 1.2745194e-05
Iter: 895 loss: 1.27509375e-05
Iter: 896 loss: 1.27436842e-05
Iter: 897 loss: 1.27386384e-05
Iter: 898 loss: 1.27380026e-05
Iter: 899 loss: 1.27344447e-05
Iter: 900 loss: 1.27292951e-05
Iter: 901 loss: 1.27221501e-05
Iter: 902 loss: 1.27219337e-05
Iter: 903 loss: 1.27129788e-05
Iter: 904 loss: 1.27779831e-05
Iter: 905 loss: 1.27122594e-05
Iter: 906 loss: 1.27089797e-05
Iter: 907 loss: 1.27082931e-05
Iter: 908 loss: 1.27048334e-05
Iter: 909 loss: 1.27010317e-05
Iter: 910 loss: 1.27005314e-05
Iter: 911 loss: 1.26951745e-05
Iter: 912 loss: 1.27128351e-05
Iter: 913 loss: 1.26936739e-05
Iter: 914 loss: 1.26877594e-05
Iter: 915 loss: 1.27022e-05
Iter: 916 loss: 1.26855766e-05
Iter: 917 loss: 1.26813775e-05
Iter: 918 loss: 1.27260546e-05
Iter: 919 loss: 1.26812201e-05
Iter: 920 loss: 1.26771338e-05
Iter: 921 loss: 1.26690338e-05
Iter: 922 loss: 1.2824029e-05
Iter: 923 loss: 1.26688919e-05
Iter: 924 loss: 1.26635541e-05
Iter: 925 loss: 1.27413932e-05
Iter: 926 loss: 1.26635514e-05
Iter: 927 loss: 1.26581526e-05
Iter: 928 loss: 1.26796131e-05
Iter: 929 loss: 1.26569612e-05
Iter: 930 loss: 1.26535624e-05
Iter: 931 loss: 1.26536006e-05
Iter: 932 loss: 1.26508e-05
Iter: 933 loss: 1.26460782e-05
Iter: 934 loss: 1.26790064e-05
Iter: 935 loss: 1.2645708e-05
Iter: 936 loss: 1.26420491e-05
Iter: 937 loss: 1.26331634e-05
Iter: 938 loss: 1.27159528e-05
Iter: 939 loss: 1.26318737e-05
Iter: 940 loss: 1.26243194e-05
Iter: 941 loss: 1.26789255e-05
Iter: 942 loss: 1.26236673e-05
Iter: 943 loss: 1.26191835e-05
Iter: 944 loss: 1.26189361e-05
Iter: 945 loss: 1.26143623e-05
Iter: 946 loss: 1.26104442e-05
Iter: 947 loss: 1.26092054e-05
Iter: 948 loss: 1.26036903e-05
Iter: 949 loss: 1.26088107e-05
Iter: 950 loss: 1.26004697e-05
Iter: 951 loss: 1.25960569e-05
Iter: 952 loss: 1.25959741e-05
Iter: 953 loss: 1.2592538e-05
Iter: 954 loss: 1.25900333e-05
Iter: 955 loss: 1.25887873e-05
Iter: 956 loss: 1.25828428e-05
Iter: 957 loss: 1.26089963e-05
Iter: 958 loss: 1.2581615e-05
Iter: 959 loss: 1.25776569e-05
Iter: 960 loss: 1.25843781e-05
Iter: 961 loss: 1.25758561e-05
Iter: 962 loss: 1.25704501e-05
Iter: 963 loss: 1.26060331e-05
Iter: 964 loss: 1.25698989e-05
Iter: 965 loss: 1.25667038e-05
Iter: 966 loss: 1.25622564e-05
Iter: 967 loss: 1.25621045e-05
Iter: 968 loss: 1.25576134e-05
Iter: 969 loss: 1.26158684e-05
Iter: 970 loss: 1.25576335e-05
Iter: 971 loss: 1.25532606e-05
Iter: 972 loss: 1.25478009e-05
Iter: 973 loss: 1.25473907e-05
Iter: 974 loss: 1.25417409e-05
Iter: 975 loss: 1.25431752e-05
Iter: 976 loss: 1.25375936e-05
Iter: 977 loss: 1.25357255e-05
Iter: 978 loss: 1.25335009e-05
Iter: 979 loss: 1.2530787e-05
Iter: 980 loss: 1.25238375e-05
Iter: 981 loss: 1.25837851e-05
Iter: 982 loss: 1.25227361e-05
Iter: 983 loss: 1.25171346e-05
Iter: 984 loss: 1.26010864e-05
Iter: 985 loss: 1.25171255e-05
Iter: 986 loss: 1.2511874e-05
Iter: 987 loss: 1.25365877e-05
Iter: 988 loss: 1.25109927e-05
Iter: 989 loss: 1.25069364e-05
Iter: 990 loss: 1.25076131e-05
Iter: 991 loss: 1.25038359e-05
Iter: 992 loss: 1.24984299e-05
Iter: 993 loss: 1.25232809e-05
Iter: 994 loss: 1.24974e-05
Iter: 995 loss: 1.24938806e-05
Iter: 996 loss: 1.2524848e-05
Iter: 997 loss: 1.2493656e-05
Iter: 998 loss: 1.24898052e-05
Iter: 999 loss: 1.24856215e-05
Iter: 1000 loss: 1.24849557e-05
Iter: 1001 loss: 1.24807739e-05
Iter: 1002 loss: 1.24846283e-05
Iter: 1003 loss: 1.24783683e-05
Iter: 1004 loss: 1.2472392e-05
Iter: 1005 loss: 1.25234856e-05
Iter: 1006 loss: 1.24720409e-05
Iter: 1007 loss: 1.24683047e-05
Iter: 1008 loss: 1.24627632e-05
Iter: 1009 loss: 1.24626586e-05
Iter: 1010 loss: 1.24562739e-05
Iter: 1011 loss: 1.24888265e-05
Iter: 1012 loss: 1.24551934e-05
Iter: 1013 loss: 1.24489534e-05
Iter: 1014 loss: 1.25152501e-05
Iter: 1015 loss: 1.24488006e-05
Iter: 1016 loss: 1.2445571e-05
Iter: 1017 loss: 1.24389771e-05
Iter: 1018 loss: 1.25609458e-05
Iter: 1019 loss: 1.24388343e-05
Iter: 1020 loss: 1.24337157e-05
Iter: 1021 loss: 1.25136248e-05
Iter: 1022 loss: 1.24336984e-05
Iter: 1023 loss: 1.24281441e-05
Iter: 1024 loss: 1.24345206e-05
Iter: 1025 loss: 1.24251574e-05
Iter: 1026 loss: 1.24213238e-05
Iter: 1027 loss: 1.24338912e-05
Iter: 1028 loss: 1.24202979e-05
Iter: 1029 loss: 1.24160015e-05
Iter: 1030 loss: 1.24281669e-05
Iter: 1031 loss: 1.24146645e-05
Iter: 1032 loss: 1.24104454e-05
Iter: 1033 loss: 1.2439019e-05
Iter: 1034 loss: 1.2410037e-05
Iter: 1035 loss: 1.24067265e-05
Iter: 1036 loss: 1.24031285e-05
Iter: 1037 loss: 1.24026337e-05
Iter: 1038 loss: 1.23979007e-05
Iter: 1039 loss: 1.24047492e-05
Iter: 1040 loss: 1.23955288e-05
Iter: 1041 loss: 1.2389226e-05
Iter: 1042 loss: 1.24384596e-05
Iter: 1043 loss: 1.23886712e-05
Iter: 1044 loss: 1.23853333e-05
Iter: 1045 loss: 1.2378001e-05
Iter: 1046 loss: 1.24940707e-05
Iter: 1047 loss: 1.23777136e-05
Iter: 1048 loss: 1.23744503e-05
Iter: 1049 loss: 1.23733662e-05
Iter: 1050 loss: 1.23689952e-05
Iter: 1051 loss: 1.23688187e-05
Iter: 1052 loss: 1.236555e-05
Iter: 1053 loss: 1.23607988e-05
Iter: 1054 loss: 1.23579466e-05
Iter: 1055 loss: 1.23560203e-05
Iter: 1056 loss: 1.23518503e-05
Iter: 1057 loss: 1.23516947e-05
Iter: 1058 loss: 1.23474429e-05
Iter: 1059 loss: 1.23442824e-05
Iter: 1060 loss: 1.23427671e-05
Iter: 1061 loss: 1.23383215e-05
Iter: 1062 loss: 1.23543805e-05
Iter: 1063 loss: 1.2337141e-05
Iter: 1064 loss: 1.23320242e-05
Iter: 1065 loss: 1.2367107e-05
Iter: 1066 loss: 1.23315476e-05
Iter: 1067 loss: 1.23276077e-05
Iter: 1068 loss: 1.23342043e-05
Iter: 1069 loss: 1.23258751e-05
Iter: 1070 loss: 1.23220898e-05
Iter: 1071 loss: 1.23222435e-05
Iter: 1072 loss: 1.23190375e-05
Iter: 1073 loss: 1.23142472e-05
Iter: 1074 loss: 1.23236914e-05
Iter: 1075 loss: 1.23123009e-05
Iter: 1076 loss: 1.23066729e-05
Iter: 1077 loss: 1.23518312e-05
Iter: 1078 loss: 1.23062282e-05
Iter: 1079 loss: 1.23029222e-05
Iter: 1080 loss: 1.22957226e-05
Iter: 1081 loss: 1.24042972e-05
Iter: 1082 loss: 1.22954334e-05
Iter: 1083 loss: 1.22940201e-05
Iter: 1084 loss: 1.22918646e-05
Iter: 1085 loss: 1.22882657e-05
Iter: 1086 loss: 1.22828242e-05
Iter: 1087 loss: 1.22827205e-05
Iter: 1088 loss: 1.22769698e-05
Iter: 1089 loss: 1.22808815e-05
Iter: 1090 loss: 1.22734054e-05
Iter: 1091 loss: 1.22658575e-05
Iter: 1092 loss: 1.22940764e-05
Iter: 1093 loss: 1.22640649e-05
Iter: 1094 loss: 1.22604597e-05
Iter: 1095 loss: 1.22598103e-05
Iter: 1096 loss: 1.22575148e-05
Iter: 1097 loss: 1.22527763e-05
Iter: 1098 loss: 1.23255368e-05
Iter: 1099 loss: 1.22525526e-05
Iter: 1100 loss: 1.22491983e-05
Iter: 1101 loss: 1.22489928e-05
Iter: 1102 loss: 1.22458387e-05
Iter: 1103 loss: 1.22486426e-05
Iter: 1104 loss: 1.2244087e-05
Iter: 1105 loss: 1.22405454e-05
Iter: 1106 loss: 1.22427273e-05
Iter: 1107 loss: 1.22382189e-05
Iter: 1108 loss: 1.2233193e-05
Iter: 1109 loss: 1.22347883e-05
Iter: 1110 loss: 1.22296115e-05
Iter: 1111 loss: 1.22255278e-05
Iter: 1112 loss: 1.22254896e-05
Iter: 1113 loss: 1.22217889e-05
Iter: 1114 loss: 1.22163719e-05
Iter: 1115 loss: 1.221618e-05
Iter: 1116 loss: 1.22116926e-05
Iter: 1117 loss: 1.22484525e-05
Iter: 1118 loss: 1.2211427e-05
Iter: 1119 loss: 1.22068432e-05
Iter: 1120 loss: 1.22270822e-05
Iter: 1121 loss: 1.22059073e-05
Iter: 1122 loss: 1.22026213e-05
Iter: 1123 loss: 1.21963258e-05
Iter: 1124 loss: 1.23226819e-05
Iter: 1125 loss: 1.21962776e-05
Iter: 1126 loss: 1.21896182e-05
Iter: 1127 loss: 1.22204301e-05
Iter: 1128 loss: 1.21884059e-05
Iter: 1129 loss: 1.21846188e-05
Iter: 1130 loss: 1.21843423e-05
Iter: 1131 loss: 1.21806952e-05
Iter: 1132 loss: 1.21751264e-05
Iter: 1133 loss: 1.21749599e-05
Iter: 1134 loss: 1.21718331e-05
Iter: 1135 loss: 1.21718094e-05
Iter: 1136 loss: 1.21682515e-05
Iter: 1137 loss: 1.21677394e-05
Iter: 1138 loss: 1.21653211e-05
Iter: 1139 loss: 1.21611838e-05
Iter: 1140 loss: 1.21716839e-05
Iter: 1141 loss: 1.21597768e-05
Iter: 1142 loss: 1.2155584e-05
Iter: 1143 loss: 1.21610783e-05
Iter: 1144 loss: 1.2153535e-05
Iter: 1145 loss: 1.21491466e-05
Iter: 1146 loss: 1.21634093e-05
Iter: 1147 loss: 1.21478915e-05
Iter: 1148 loss: 1.21427256e-05
Iter: 1149 loss: 1.21628245e-05
Iter: 1150 loss: 1.21415342e-05
Iter: 1151 loss: 1.21379335e-05
Iter: 1152 loss: 1.21331886e-05
Iter: 1153 loss: 1.21328649e-05
Iter: 1154 loss: 1.21315425e-05
Iter: 1155 loss: 1.21298172e-05
Iter: 1156 loss: 1.21275407e-05
Iter: 1157 loss: 1.21219437e-05
Iter: 1158 loss: 1.21802859e-05
Iter: 1159 loss: 1.21213052e-05
Iter: 1160 loss: 1.21147577e-05
Iter: 1161 loss: 1.21384846e-05
Iter: 1162 loss: 1.21130906e-05
Iter: 1163 loss: 1.2109288e-05
Iter: 1164 loss: 1.21091571e-05
Iter: 1165 loss: 1.21050252e-05
Iter: 1166 loss: 1.21031444e-05
Iter: 1167 loss: 1.21011708e-05
Iter: 1168 loss: 1.20976456e-05
Iter: 1169 loss: 1.21259081e-05
Iter: 1170 loss: 1.20973837e-05
Iter: 1171 loss: 1.20928089e-05
Iter: 1172 loss: 1.2090456e-05
Iter: 1173 loss: 1.2088346e-05
Iter: 1174 loss: 1.20842869e-05
Iter: 1175 loss: 1.209324e-05
Iter: 1176 loss: 1.20826771e-05
Iter: 1177 loss: 1.20779951e-05
Iter: 1178 loss: 1.20925333e-05
Iter: 1179 loss: 1.2076589e-05
Iter: 1180 loss: 1.20717441e-05
Iter: 1181 loss: 1.20794703e-05
Iter: 1182 loss: 1.2069484e-05
Iter: 1183 loss: 1.20649929e-05
Iter: 1184 loss: 1.21148041e-05
Iter: 1185 loss: 1.20648656e-05
Iter: 1186 loss: 1.20620552e-05
Iter: 1187 loss: 1.20569193e-05
Iter: 1188 loss: 1.21780922e-05
Iter: 1189 loss: 1.2056892e-05
Iter: 1190 loss: 1.20544209e-05
Iter: 1191 loss: 1.20537679e-05
Iter: 1192 loss: 1.20505902e-05
Iter: 1193 loss: 1.20445275e-05
Iter: 1194 loss: 1.2167463e-05
Iter: 1195 loss: 1.2044392e-05
Iter: 1196 loss: 1.20390669e-05
Iter: 1197 loss: 1.20559962e-05
Iter: 1198 loss: 1.20375989e-05
Iter: 1199 loss: 1.20330878e-05
Iter: 1200 loss: 1.20330715e-05
Iter: 1201 loss: 1.20299228e-05
Iter: 1202 loss: 1.20278764e-05
Iter: 1203 loss: 1.20265704e-05
Iter: 1204 loss: 1.20238847e-05
Iter: 1205 loss: 1.20237974e-05
Iter: 1206 loss: 1.20209952e-05
Iter: 1207 loss: 1.20142795e-05
Iter: 1208 loss: 1.20915438e-05
Iter: 1209 loss: 1.20137656e-05
Iter: 1210 loss: 1.20088707e-05
Iter: 1211 loss: 1.20536042e-05
Iter: 1212 loss: 1.20086734e-05
Iter: 1213 loss: 1.2004014e-05
Iter: 1214 loss: 1.20089944e-05
Iter: 1215 loss: 1.20014911e-05
Iter: 1216 loss: 1.19968699e-05
Iter: 1217 loss: 1.20260956e-05
Iter: 1218 loss: 1.19963606e-05
Iter: 1219 loss: 1.19923234e-05
Iter: 1220 loss: 1.20030527e-05
Iter: 1221 loss: 1.19909837e-05
Iter: 1222 loss: 1.19870383e-05
Iter: 1223 loss: 1.1983655e-05
Iter: 1224 loss: 1.19826127e-05
Iter: 1225 loss: 1.1980208e-05
Iter: 1226 loss: 1.1979273e-05
Iter: 1227 loss: 1.19767628e-05
Iter: 1228 loss: 1.1970481e-05
Iter: 1229 loss: 1.20299901e-05
Iter: 1230 loss: 1.19695778e-05
Iter: 1231 loss: 1.19639253e-05
Iter: 1232 loss: 1.20043369e-05
Iter: 1233 loss: 1.19634078e-05
Iter: 1234 loss: 1.19587166e-05
Iter: 1235 loss: 1.20123932e-05
Iter: 1236 loss: 1.19585848e-05
Iter: 1237 loss: 1.19553642e-05
Iter: 1238 loss: 1.19522438e-05
Iter: 1239 loss: 1.19515025e-05
Iter: 1240 loss: 1.19484957e-05
Iter: 1241 loss: 1.19481911e-05
Iter: 1242 loss: 1.19461483e-05
Iter: 1243 loss: 1.19407268e-05
Iter: 1244 loss: 1.19808328e-05
Iter: 1245 loss: 1.1939559e-05
Iter: 1246 loss: 1.19351025e-05
Iter: 1247 loss: 1.19909455e-05
Iter: 1248 loss: 1.19350843e-05
Iter: 1249 loss: 1.19306924e-05
Iter: 1250 loss: 1.19336837e-05
Iter: 1251 loss: 1.19279639e-05
Iter: 1252 loss: 1.19229508e-05
Iter: 1253 loss: 1.19546203e-05
Iter: 1254 loss: 1.19224724e-05
Iter: 1255 loss: 1.19182578e-05
Iter: 1256 loss: 1.19264641e-05
Iter: 1257 loss: 1.1916527e-05
Iter: 1258 loss: 1.1912487e-05
Iter: 1259 loss: 1.19142587e-05
Iter: 1260 loss: 1.19097404e-05
Iter: 1261 loss: 1.19055912e-05
Iter: 1262 loss: 1.19055503e-05
Iter: 1263 loss: 1.19029673e-05
Iter: 1264 loss: 1.1897383e-05
Iter: 1265 loss: 1.19876731e-05
Iter: 1266 loss: 1.18971921e-05
Iter: 1267 loss: 1.18922217e-05
Iter: 1268 loss: 1.19208235e-05
Iter: 1269 loss: 1.1891625e-05
Iter: 1270 loss: 1.18869812e-05
Iter: 1271 loss: 1.19376655e-05
Iter: 1272 loss: 1.18868511e-05
Iter: 1273 loss: 1.18845091e-05
Iter: 1274 loss: 1.18837252e-05
Iter: 1275 loss: 1.18823282e-05
Iter: 1276 loss: 1.18781199e-05
Iter: 1277 loss: 1.19036113e-05
Iter: 1278 loss: 1.18775579e-05
Iter: 1279 loss: 1.18747612e-05
Iter: 1280 loss: 1.18691005e-05
Iter: 1281 loss: 1.19774486e-05
Iter: 1282 loss: 1.18690659e-05
Iter: 1283 loss: 1.18638127e-05
Iter: 1284 loss: 1.19001052e-05
Iter: 1285 loss: 1.18633743e-05
Iter: 1286 loss: 1.18579392e-05
Iter: 1287 loss: 1.18808248e-05
Iter: 1288 loss: 1.18568623e-05
Iter: 1289 loss: 1.18532644e-05
Iter: 1290 loss: 1.18645539e-05
Iter: 1291 loss: 1.18521912e-05
Iter: 1292 loss: 1.18479948e-05
Iter: 1293 loss: 1.18508488e-05
Iter: 1294 loss: 1.18452044e-05
Iter: 1295 loss: 1.18416592e-05
Iter: 1296 loss: 1.18659664e-05
Iter: 1297 loss: 1.18413063e-05
Iter: 1298 loss: 1.18374865e-05
Iter: 1299 loss: 1.1846545e-05
Iter: 1300 loss: 1.18361022e-05
Iter: 1301 loss: 1.18326843e-05
Iter: 1302 loss: 1.18282478e-05
Iter: 1303 loss: 1.18279586e-05
Iter: 1304 loss: 1.18232556e-05
Iter: 1305 loss: 1.18629287e-05
Iter: 1306 loss: 1.18229746e-05
Iter: 1307 loss: 1.18182179e-05
Iter: 1308 loss: 1.18453445e-05
Iter: 1309 loss: 1.18175703e-05
Iter: 1310 loss: 1.18152029e-05
Iter: 1311 loss: 1.18195057e-05
Iter: 1312 loss: 1.18141252e-05
Iter: 1313 loss: 1.18102944e-05
Iter: 1314 loss: 1.18131366e-05
Iter: 1315 loss: 1.18079706e-05
Iter: 1316 loss: 1.18045373e-05
Iter: 1317 loss: 1.18015778e-05
Iter: 1318 loss: 1.1800631e-05
Iter: 1319 loss: 1.17957043e-05
Iter: 1320 loss: 1.18209409e-05
Iter: 1321 loss: 1.17948503e-05
Iter: 1322 loss: 1.1789818e-05
Iter: 1323 loss: 1.18212083e-05
Iter: 1324 loss: 1.17891868e-05
Iter: 1325 loss: 1.17862555e-05
Iter: 1326 loss: 1.17895561e-05
Iter: 1327 loss: 1.17845721e-05
Iter: 1328 loss: 1.17801665e-05
Iter: 1329 loss: 1.179016e-05
Iter: 1330 loss: 1.17784657e-05
Iter: 1331 loss: 1.17753607e-05
Iter: 1332 loss: 1.17944046e-05
Iter: 1333 loss: 1.17749323e-05
Iter: 1334 loss: 1.17712925e-05
Iter: 1335 loss: 1.17721229e-05
Iter: 1336 loss: 1.17685831e-05
Iter: 1337 loss: 1.17648242e-05
Iter: 1338 loss: 1.17644422e-05
Iter: 1339 loss: 1.17616855e-05
Iter: 1340 loss: 1.17576046e-05
Iter: 1341 loss: 1.179422e-05
Iter: 1342 loss: 1.17574255e-05
Iter: 1343 loss: 1.17527488e-05
Iter: 1344 loss: 1.1764806e-05
Iter: 1345 loss: 1.1751179e-05
Iter: 1346 loss: 1.17486907e-05
Iter: 1347 loss: 1.17636082e-05
Iter: 1348 loss: 1.17484096e-05
Iter: 1349 loss: 1.17451964e-05
Iter: 1350 loss: 1.17412383e-05
Iter: 1351 loss: 1.17409027e-05
Iter: 1352 loss: 1.17369718e-05
Iter: 1353 loss: 1.17454656e-05
Iter: 1354 loss: 1.17353866e-05
Iter: 1355 loss: 1.17312748e-05
Iter: 1356 loss: 1.17369309e-05
Iter: 1357 loss: 1.17292939e-05
Iter: 1358 loss: 1.17240324e-05
Iter: 1359 loss: 1.17751088e-05
Iter: 1360 loss: 1.1723876e-05
Iter: 1361 loss: 1.17212658e-05
Iter: 1362 loss: 1.17203854e-05
Iter: 1363 loss: 1.17189011e-05
Iter: 1364 loss: 1.17138616e-05
Iter: 1365 loss: 1.17313548e-05
Iter: 1366 loss: 1.17125401e-05
Iter: 1367 loss: 1.17096097e-05
Iter: 1368 loss: 1.17333948e-05
Iter: 1369 loss: 1.1709415e-05
Iter: 1370 loss: 1.17062445e-05
Iter: 1371 loss: 1.17025593e-05
Iter: 1372 loss: 1.17020518e-05
Iter: 1373 loss: 1.16971805e-05
Iter: 1374 loss: 1.17026766e-05
Iter: 1375 loss: 1.1694673e-05
Iter: 1376 loss: 1.16906313e-05
Iter: 1377 loss: 1.17437157e-05
Iter: 1378 loss: 1.16906431e-05
Iter: 1379 loss: 1.16863503e-05
Iter: 1380 loss: 1.16938099e-05
Iter: 1381 loss: 1.1684413e-05
Iter: 1382 loss: 1.16820866e-05
Iter: 1383 loss: 1.17014206e-05
Iter: 1384 loss: 1.16819774e-05
Iter: 1385 loss: 1.16792799e-05
Iter: 1386 loss: 1.16747924e-05
Iter: 1387 loss: 1.16747615e-05
Iter: 1388 loss: 1.16706142e-05
Iter: 1389 loss: 1.1668817e-05
Iter: 1390 loss: 1.16666342e-05
Iter: 1391 loss: 1.16612609e-05
Iter: 1392 loss: 1.17345935e-05
Iter: 1393 loss: 1.16612191e-05
Iter: 1394 loss: 1.16567717e-05
Iter: 1395 loss: 1.16781666e-05
Iter: 1396 loss: 1.16560095e-05
Iter: 1397 loss: 1.16528281e-05
Iter: 1398 loss: 1.16526453e-05
Iter: 1399 loss: 1.16502551e-05
Iter: 1400 loss: 1.16457468e-05
Iter: 1401 loss: 1.1681931e-05
Iter: 1402 loss: 1.16454466e-05
Iter: 1403 loss: 1.16422852e-05
Iter: 1404 loss: 1.1648428e-05
Iter: 1405 loss: 1.16409137e-05
Iter: 1406 loss: 1.16368828e-05
Iter: 1407 loss: 1.16445781e-05
Iter: 1408 loss: 1.1635253e-05
Iter: 1409 loss: 1.16317933e-05
Iter: 1410 loss: 1.16279161e-05
Iter: 1411 loss: 1.1627435e-05
Iter: 1412 loss: 1.16237024e-05
Iter: 1413 loss: 1.16236979e-05
Iter: 1414 loss: 1.16200463e-05
Iter: 1415 loss: 1.16329848e-05
Iter: 1416 loss: 1.16191404e-05
Iter: 1417 loss: 1.16167412e-05
Iter: 1418 loss: 1.16185365e-05
Iter: 1419 loss: 1.16152441e-05
Iter: 1420 loss: 1.16108804e-05
Iter: 1421 loss: 1.16151596e-05
Iter: 1422 loss: 1.16084084e-05
Iter: 1423 loss: 1.16052e-05
Iter: 1424 loss: 1.15997445e-05
Iter: 1425 loss: 1.15997445e-05
Iter: 1426 loss: 1.15938074e-05
Iter: 1427 loss: 1.16376896e-05
Iter: 1428 loss: 1.15933e-05
Iter: 1429 loss: 1.15889516e-05
Iter: 1430 loss: 1.16553601e-05
Iter: 1431 loss: 1.15889306e-05
Iter: 1432 loss: 1.15860466e-05
Iter: 1433 loss: 1.15827861e-05
Iter: 1434 loss: 1.15823559e-05
Iter: 1435 loss: 1.1578e-05
Iter: 1436 loss: 1.16250785e-05
Iter: 1437 loss: 1.15779621e-05
Iter: 1438 loss: 1.15740368e-05
Iter: 1439 loss: 1.15782859e-05
Iter: 1440 loss: 1.1571994e-05
Iter: 1441 loss: 1.15686016e-05
Iter: 1442 loss: 1.15885005e-05
Iter: 1443 loss: 1.15681187e-05
Iter: 1444 loss: 1.15650273e-05
Iter: 1445 loss: 1.1559925e-05
Iter: 1446 loss: 1.15599087e-05
Iter: 1447 loss: 1.1555936e-05
Iter: 1448 loss: 1.16023275e-05
Iter: 1449 loss: 1.15559005e-05
Iter: 1450 loss: 1.15514977e-05
Iter: 1451 loss: 1.15676594e-05
Iter: 1452 loss: 1.15503644e-05
Iter: 1453 loss: 1.15473867e-05
Iter: 1454 loss: 1.15503335e-05
Iter: 1455 loss: 1.1545626e-05
Iter: 1456 loss: 1.15419671e-05
Iter: 1457 loss: 1.15667353e-05
Iter: 1458 loss: 1.1541646e-05
Iter: 1459 loss: 1.15390631e-05
Iter: 1460 loss: 1.15328658e-05
Iter: 1461 loss: 1.16038955e-05
Iter: 1462 loss: 1.15323119e-05
Iter: 1463 loss: 1.152609e-05
Iter: 1464 loss: 1.15454313e-05
Iter: 1465 loss: 1.15242929e-05
Iter: 1466 loss: 1.15190433e-05
Iter: 1467 loss: 1.15795738e-05
Iter: 1468 loss: 1.15190178e-05
Iter: 1469 loss: 1.1514906e-05
Iter: 1470 loss: 1.15461571e-05
Iter: 1471 loss: 1.15146149e-05
Iter: 1472 loss: 1.15119492e-05
Iter: 1473 loss: 1.15086714e-05
Iter: 1474 loss: 1.15084422e-05
Iter: 1475 loss: 1.15049679e-05
Iter: 1476 loss: 1.15048688e-05
Iter: 1477 loss: 1.15026251e-05
Iter: 1478 loss: 1.14977638e-05
Iter: 1479 loss: 1.15747725e-05
Iter: 1480 loss: 1.14975664e-05
Iter: 1481 loss: 1.14920349e-05
Iter: 1482 loss: 1.15552884e-05
Iter: 1483 loss: 1.14919676e-05
Iter: 1484 loss: 1.14889481e-05
Iter: 1485 loss: 1.14849363e-05
Iter: 1486 loss: 1.14847426e-05
Iter: 1487 loss: 1.14824234e-05
Iter: 1488 loss: 1.14817158e-05
Iter: 1489 loss: 1.14789964e-05
Iter: 1490 loss: 1.14768272e-05
Iter: 1491 loss: 1.14760196e-05
Iter: 1492 loss: 1.14727691e-05
Iter: 1493 loss: 1.14856739e-05
Iter: 1494 loss: 1.14720115e-05
Iter: 1495 loss: 1.1467896e-05
Iter: 1496 loss: 1.14751101e-05
Iter: 1497 loss: 1.14660588e-05
Iter: 1498 loss: 1.14632603e-05
Iter: 1499 loss: 1.14578706e-05
Iter: 1500 loss: 1.15647254e-05
Iter: 1501 loss: 1.1457887e-05
Iter: 1502 loss: 1.14527857e-05
Iter: 1503 loss: 1.14527684e-05
Iter: 1504 loss: 1.14488666e-05
Iter: 1505 loss: 1.1478176e-05
Iter: 1506 loss: 1.1448592e-05
Iter: 1507 loss: 1.14459335e-05
Iter: 1508 loss: 1.14428549e-05
Iter: 1509 loss: 1.14425329e-05
Iter: 1510 loss: 1.14390896e-05
Iter: 1511 loss: 1.14391132e-05
Iter: 1512 loss: 1.14363684e-05
Iter: 1513 loss: 1.14324503e-05
Iter: 1514 loss: 1.14323957e-05
Iter: 1515 loss: 1.14286231e-05
Iter: 1516 loss: 1.14606391e-05
Iter: 1517 loss: 1.14284685e-05
Iter: 1518 loss: 1.14247978e-05
Iter: 1519 loss: 1.14245413e-05
Iter: 1520 loss: 1.14217992e-05
Iter: 1521 loss: 1.14194672e-05
Iter: 1522 loss: 1.14192235e-05
Iter: 1523 loss: 1.14168197e-05
Iter: 1524 loss: 1.14130926e-05
Iter: 1525 loss: 1.14130371e-05
Iter: 1526 loss: 1.14096829e-05
Iter: 1527 loss: 1.14321274e-05
Iter: 1528 loss: 1.14093755e-05
Iter: 1529 loss: 1.14058112e-05
Iter: 1530 loss: 1.14151153e-05
Iter: 1531 loss: 1.14046343e-05
Iter: 1532 loss: 1.14020086e-05
Iter: 1533 loss: 1.13964215e-05
Iter: 1534 loss: 1.14866452e-05
Iter: 1535 loss: 1.13962506e-05
Iter: 1536 loss: 1.13916813e-05
Iter: 1537 loss: 1.1391694e-05
Iter: 1538 loss: 1.13873793e-05
Iter: 1539 loss: 1.14034938e-05
Iter: 1540 loss: 1.13862907e-05
Iter: 1541 loss: 1.13835576e-05
Iter: 1542 loss: 1.13841234e-05
Iter: 1543 loss: 1.13815604e-05
Iter: 1544 loss: 1.13773858e-05
Iter: 1545 loss: 1.14101895e-05
Iter: 1546 loss: 1.13771675e-05
Iter: 1547 loss: 1.13743108e-05
Iter: 1548 loss: 1.13717142e-05
Iter: 1549 loss: 1.1371043e-05
Iter: 1550 loss: 1.13672613e-05
Iter: 1551 loss: 1.13873602e-05
Iter: 1552 loss: 1.13667365e-05
Iter: 1553 loss: 1.13626047e-05
Iter: 1554 loss: 1.13707092e-05
Iter: 1555 loss: 1.13608457e-05
Iter: 1556 loss: 1.1358301e-05
Iter: 1557 loss: 1.13582064e-05
Iter: 1558 loss: 1.13562583e-05
Iter: 1559 loss: 1.13521664e-05
Iter: 1560 loss: 1.14271097e-05
Iter: 1561 loss: 1.13520991e-05
Iter: 1562 loss: 1.13488777e-05
Iter: 1563 loss: 1.1391202e-05
Iter: 1564 loss: 1.13488377e-05
Iter: 1565 loss: 1.134584e-05
Iter: 1566 loss: 1.13471924e-05
Iter: 1567 loss: 1.13437445e-05
Iter: 1568 loss: 1.13405167e-05
Iter: 1569 loss: 1.13378546e-05
Iter: 1570 loss: 1.1336906e-05
Iter: 1571 loss: 1.1332656e-05
Iter: 1572 loss: 1.13615606e-05
Iter: 1573 loss: 1.13322676e-05
Iter: 1574 loss: 1.13274764e-05
Iter: 1575 loss: 1.13505748e-05
Iter: 1576 loss: 1.13267006e-05
Iter: 1577 loss: 1.13242113e-05
Iter: 1578 loss: 1.13254773e-05
Iter: 1579 loss: 1.13226188e-05
Iter: 1580 loss: 1.1318526e-05
Iter: 1581 loss: 1.13343322e-05
Iter: 1582 loss: 1.13174801e-05
Iter: 1583 loss: 1.13148271e-05
Iter: 1584 loss: 1.13138021e-05
Iter: 1585 loss: 1.13122551e-05
Iter: 1586 loss: 1.1308839e-05
Iter: 1587 loss: 1.13261485e-05
Iter: 1588 loss: 1.13083288e-05
Iter: 1589 loss: 1.13043743e-05
Iter: 1590 loss: 1.13175702e-05
Iter: 1591 loss: 1.1303242e-05
Iter: 1592 loss: 1.13004535e-05
Iter: 1593 loss: 1.13311671e-05
Iter: 1594 loss: 1.13003134e-05
Iter: 1595 loss: 1.12984817e-05
Iter: 1596 loss: 1.12940334e-05
Iter: 1597 loss: 1.13452033e-05
Iter: 1598 loss: 1.12936814e-05
Iter: 1599 loss: 1.12909547e-05
Iter: 1600 loss: 1.12905309e-05
Iter: 1601 loss: 1.12881171e-05
Iter: 1602 loss: 1.12850867e-05
Iter: 1603 loss: 1.12847556e-05
Iter: 1604 loss: 1.1280983e-05
Iter: 1605 loss: 1.1283124e-05
Iter: 1606 loss: 1.12784483e-05
Iter: 1607 loss: 1.12742728e-05
Iter: 1608 loss: 1.12998332e-05
Iter: 1609 loss: 1.12737889e-05
Iter: 1610 loss: 1.12697871e-05
Iter: 1611 loss: 1.1303684e-05
Iter: 1612 loss: 1.12694906e-05
Iter: 1613 loss: 1.12670732e-05
Iter: 1614 loss: 1.12647758e-05
Iter: 1615 loss: 1.1264221e-05
Iter: 1616 loss: 1.12603702e-05
Iter: 1617 loss: 1.13096085e-05
Iter: 1618 loss: 1.1260312e-05
Iter: 1619 loss: 1.12582984e-05
Iter: 1620 loss: 1.12542239e-05
Iter: 1621 loss: 1.13354436e-05
Iter: 1622 loss: 1.12541993e-05
Iter: 1623 loss: 1.12503249e-05
Iter: 1624 loss: 1.1284561e-05
Iter: 1625 loss: 1.12500584e-05
Iter: 1626 loss: 1.12467869e-05
Iter: 1627 loss: 1.12786447e-05
Iter: 1628 loss: 1.12466223e-05
Iter: 1629 loss: 1.12444595e-05
Iter: 1630 loss: 1.12470098e-05
Iter: 1631 loss: 1.1243359e-05
Iter: 1632 loss: 1.12404869e-05
Iter: 1633 loss: 1.12397101e-05
Iter: 1634 loss: 1.12379112e-05
Iter: 1635 loss: 1.12353282e-05
Iter: 1636 loss: 1.12665475e-05
Iter: 1637 loss: 1.12353409e-05
Iter: 1638 loss: 1.12325379e-05
Iter: 1639 loss: 1.12281523e-05
Iter: 1640 loss: 1.12281505e-05
Iter: 1641 loss: 1.12243779e-05
Iter: 1642 loss: 1.12254756e-05
Iter: 1643 loss: 1.12217176e-05
Iter: 1644 loss: 1.12176431e-05
Iter: 1645 loss: 1.12676553e-05
Iter: 1646 loss: 1.1217594e-05
Iter: 1647 loss: 1.12145563e-05
Iter: 1648 loss: 1.12456082e-05
Iter: 1649 loss: 1.12144535e-05
Iter: 1650 loss: 1.12123e-05
Iter: 1651 loss: 1.12090402e-05
Iter: 1652 loss: 1.12090074e-05
Iter: 1653 loss: 1.12062153e-05
Iter: 1654 loss: 1.1206148e-05
Iter: 1655 loss: 1.12045072e-05
Iter: 1656 loss: 1.12003427e-05
Iter: 1657 loss: 1.12328053e-05
Iter: 1658 loss: 1.11995341e-05
Iter: 1659 loss: 1.11956242e-05
Iter: 1660 loss: 1.12566649e-05
Iter: 1661 loss: 1.11956315e-05
Iter: 1662 loss: 1.11924746e-05
Iter: 1663 loss: 1.1216227e-05
Iter: 1664 loss: 1.119218e-05
Iter: 1665 loss: 1.11899708e-05
Iter: 1666 loss: 1.11878344e-05
Iter: 1667 loss: 1.11873032e-05
Iter: 1668 loss: 1.11835179e-05
Iter: 1669 loss: 1.12074158e-05
Iter: 1670 loss: 1.11829677e-05
Iter: 1671 loss: 1.11805066e-05
Iter: 1672 loss: 1.11851386e-05
Iter: 1673 loss: 1.11794925e-05
Iter: 1674 loss: 1.11759491e-05
Iter: 1675 loss: 1.11804911e-05
Iter: 1676 loss: 1.11740956e-05
Iter: 1677 loss: 1.11712407e-05
Iter: 1678 loss: 1.11676018e-05
Iter: 1679 loss: 1.1167358e-05
Iter: 1680 loss: 1.11626796e-05
Iter: 1681 loss: 1.11883455e-05
Iter: 1682 loss: 1.11620811e-05
Iter: 1683 loss: 1.11587733e-05
Iter: 1684 loss: 1.11588433e-05
Iter: 1685 loss: 1.11565078e-05
Iter: 1686 loss: 1.11539366e-05
Iter: 1687 loss: 1.1153611e-05
Iter: 1688 loss: 1.115129e-05
Iter: 1689 loss: 1.11512727e-05
Iter: 1690 loss: 1.11492755e-05
Iter: 1691 loss: 1.11452055e-05
Iter: 1692 loss: 1.12173429e-05
Iter: 1693 loss: 1.11451072e-05
Iter: 1694 loss: 1.11413856e-05
Iter: 1695 loss: 1.11545833e-05
Iter: 1696 loss: 1.11403542e-05
Iter: 1697 loss: 1.11369664e-05
Iter: 1698 loss: 1.11369673e-05
Iter: 1699 loss: 1.11353474e-05
Iter: 1700 loss: 1.11320032e-05
Iter: 1701 loss: 1.1189115e-05
Iter: 1702 loss: 1.11318514e-05
Iter: 1703 loss: 1.11280033e-05
Iter: 1704 loss: 1.11700874e-05
Iter: 1705 loss: 1.11279351e-05
Iter: 1706 loss: 1.11252775e-05
Iter: 1707 loss: 1.1128197e-05
Iter: 1708 loss: 1.11238196e-05
Iter: 1709 loss: 1.11204954e-05
Iter: 1710 loss: 1.11322552e-05
Iter: 1711 loss: 1.1119706e-05
Iter: 1712 loss: 1.11167992e-05
Iter: 1713 loss: 1.11142126e-05
Iter: 1714 loss: 1.11134486e-05
Iter: 1715 loss: 1.11095733e-05
Iter: 1716 loss: 1.11140034e-05
Iter: 1717 loss: 1.11074442e-05
Iter: 1718 loss: 1.11040672e-05
Iter: 1719 loss: 1.11557e-05
Iter: 1720 loss: 1.11040599e-05
Iter: 1721 loss: 1.1101044e-05
Iter: 1722 loss: 1.11174741e-05
Iter: 1723 loss: 1.11005847e-05
Iter: 1724 loss: 1.10983692e-05
Iter: 1725 loss: 1.10963665e-05
Iter: 1726 loss: 1.10959472e-05
Iter: 1727 loss: 1.10928067e-05
Iter: 1728 loss: 1.11326299e-05
Iter: 1729 loss: 1.10927995e-05
Iter: 1730 loss: 1.1090191e-05
Iter: 1731 loss: 1.10858418e-05
Iter: 1732 loss: 1.10857982e-05
Iter: 1733 loss: 1.10827186e-05
Iter: 1734 loss: 1.11185755e-05
Iter: 1735 loss: 1.1082665e-05
Iter: 1736 loss: 1.10788596e-05
Iter: 1737 loss: 1.10834844e-05
Iter: 1738 loss: 1.10768979e-05
Iter: 1739 loss: 1.10743085e-05
Iter: 1740 loss: 1.10726405e-05
Iter: 1741 loss: 1.10717046e-05
Iter: 1742 loss: 1.10683741e-05
Iter: 1743 loss: 1.1117304e-05
Iter: 1744 loss: 1.10683422e-05
Iter: 1745 loss: 1.10655146e-05
Iter: 1746 loss: 1.10630781e-05
Iter: 1747 loss: 1.10621813e-05
Iter: 1748 loss: 1.1059059e-05
Iter: 1749 loss: 1.10840765e-05
Iter: 1750 loss: 1.10588153e-05
Iter: 1751 loss: 1.10554647e-05
Iter: 1752 loss: 1.10509318e-05
Iter: 1753 loss: 1.10507117e-05
Iter: 1754 loss: 1.1046639e-05
Iter: 1755 loss: 1.10613801e-05
Iter: 1756 loss: 1.10455621e-05
Iter: 1757 loss: 1.10422898e-05
Iter: 1758 loss: 1.10422607e-05
Iter: 1759 loss: 1.10399242e-05
Iter: 1760 loss: 1.10363862e-05
Iter: 1761 loss: 1.10363071e-05
Iter: 1762 loss: 1.10333149e-05
Iter: 1763 loss: 1.107352e-05
Iter: 1764 loss: 1.1033253e-05
Iter: 1765 loss: 1.10302699e-05
Iter: 1766 loss: 1.10291494e-05
Iter: 1767 loss: 1.10275505e-05
Iter: 1768 loss: 1.1024159e-05
Iter: 1769 loss: 1.10282817e-05
Iter: 1770 loss: 1.10223373e-05
Iter: 1771 loss: 1.10182846e-05
Iter: 1772 loss: 1.10810561e-05
Iter: 1773 loss: 1.10182837e-05
Iter: 1774 loss: 1.10163892e-05
Iter: 1775 loss: 1.10119199e-05
Iter: 1776 loss: 1.10663277e-05
Iter: 1777 loss: 1.10115989e-05
Iter: 1778 loss: 1.10075807e-05
Iter: 1779 loss: 1.10430537e-05
Iter: 1780 loss: 1.10073534e-05
Iter: 1781 loss: 1.10036981e-05
Iter: 1782 loss: 1.1028229e-05
Iter: 1783 loss: 1.10033325e-05
Iter: 1784 loss: 1.10007431e-05
Iter: 1785 loss: 1.0997248e-05
Iter: 1786 loss: 1.09970988e-05
Iter: 1787 loss: 1.09935609e-05
Iter: 1788 loss: 1.1022963e-05
Iter: 1789 loss: 1.09933217e-05
Iter: 1790 loss: 1.09894745e-05
Iter: 1791 loss: 1.09911925e-05
Iter: 1792 loss: 1.09869179e-05
Iter: 1793 loss: 1.09831199e-05
Iter: 1794 loss: 1.09921511e-05
Iter: 1795 loss: 1.09817947e-05
Iter: 1796 loss: 1.09782932e-05
Iter: 1797 loss: 1.09782795e-05
Iter: 1798 loss: 1.0976195e-05
Iter: 1799 loss: 1.0972637e-05
Iter: 1800 loss: 1.09726861e-05
Iter: 1801 loss: 1.09687562e-05
Iter: 1802 loss: 1.09935281e-05
Iter: 1803 loss: 1.09683187e-05
Iter: 1804 loss: 1.0964055e-05
Iter: 1805 loss: 1.09763823e-05
Iter: 1806 loss: 1.09626853e-05
Iter: 1807 loss: 1.0960558e-05
Iter: 1808 loss: 1.09701459e-05
Iter: 1809 loss: 1.09601606e-05
Iter: 1810 loss: 1.09569046e-05
Iter: 1811 loss: 1.09544417e-05
Iter: 1812 loss: 1.09534185e-05
Iter: 1813 loss: 1.09501198e-05
Iter: 1814 loss: 1.09493694e-05
Iter: 1815 loss: 1.09472094e-05
Iter: 1816 loss: 1.09439643e-05
Iter: 1817 loss: 1.09439843e-05
Iter: 1818 loss: 1.09406919e-05
Iter: 1819 loss: 1.09463581e-05
Iter: 1820 loss: 1.09393768e-05
Iter: 1821 loss: 1.09368902e-05
Iter: 1822 loss: 1.09328621e-05
Iter: 1823 loss: 1.09328366e-05
Iter: 1824 loss: 1.09289758e-05
Iter: 1825 loss: 1.09806033e-05
Iter: 1826 loss: 1.09290186e-05
Iter: 1827 loss: 1.09250595e-05
Iter: 1828 loss: 1.09286757e-05
Iter: 1829 loss: 1.09228e-05
Iter: 1830 loss: 1.0919739e-05
Iter: 1831 loss: 1.09478042e-05
Iter: 1832 loss: 1.09196581e-05
Iter: 1833 loss: 1.09164102e-05
Iter: 1834 loss: 1.09183629e-05
Iter: 1835 loss: 1.09143248e-05
Iter: 1836 loss: 1.09111679e-05
Iter: 1837 loss: 1.09119792e-05
Iter: 1838 loss: 1.09088078e-05
Iter: 1839 loss: 1.09061311e-05
Iter: 1840 loss: 1.09480607e-05
Iter: 1841 loss: 1.09061129e-05
Iter: 1842 loss: 1.09033153e-05
Iter: 1843 loss: 1.09015737e-05
Iter: 1844 loss: 1.09004468e-05
Iter: 1845 loss: 1.08984677e-05
Iter: 1846 loss: 1.0898214e-05
Iter: 1847 loss: 1.08966397e-05
Iter: 1848 loss: 1.08928289e-05
Iter: 1849 loss: 1.09285356e-05
Iter: 1850 loss: 1.08922541e-05
Iter: 1851 loss: 1.08877775e-05
Iter: 1852 loss: 1.08989789e-05
Iter: 1853 loss: 1.08862096e-05
Iter: 1854 loss: 1.08832546e-05
Iter: 1855 loss: 1.08832228e-05
Iter: 1856 loss: 1.0879904e-05
Iter: 1857 loss: 1.08788026e-05
Iter: 1858 loss: 1.08769582e-05
Iter: 1859 loss: 1.08736895e-05
Iter: 1860 loss: 1.0870569e-05
Iter: 1861 loss: 1.0869785e-05
Iter: 1862 loss: 1.08653603e-05
Iter: 1863 loss: 1.09080956e-05
Iter: 1864 loss: 1.08652039e-05
Iter: 1865 loss: 1.08616196e-05
Iter: 1866 loss: 1.0896174e-05
Iter: 1867 loss: 1.08614422e-05
Iter: 1868 loss: 1.08589948e-05
Iter: 1869 loss: 1.086271e-05
Iter: 1870 loss: 1.08578151e-05
Iter: 1871 loss: 1.08546283e-05
Iter: 1872 loss: 1.08608547e-05
Iter: 1873 loss: 1.08533495e-05
Iter: 1874 loss: 1.08506274e-05
Iter: 1875 loss: 1.08484783e-05
Iter: 1876 loss: 1.08476734e-05
Iter: 1877 loss: 1.08458116e-05
Iter: 1878 loss: 1.08453314e-05
Iter: 1879 loss: 1.08432969e-05
Iter: 1880 loss: 1.08411477e-05
Iter: 1881 loss: 1.08407894e-05
Iter: 1882 loss: 1.08378572e-05
Iter: 1883 loss: 1.08725581e-05
Iter: 1884 loss: 1.08378335e-05
Iter: 1885 loss: 1.08358126e-05
Iter: 1886 loss: 1.08312e-05
Iter: 1887 loss: 1.08800732e-05
Iter: 1888 loss: 1.08306567e-05
Iter: 1889 loss: 1.08264721e-05
Iter: 1890 loss: 1.08475888e-05
Iter: 1891 loss: 1.08257391e-05
Iter: 1892 loss: 1.08212334e-05
Iter: 1893 loss: 1.08262366e-05
Iter: 1894 loss: 1.08189461e-05
Iter: 1895 loss: 1.08173526e-05
Iter: 1896 loss: 1.08166232e-05
Iter: 1897 loss: 1.08142358e-05
Iter: 1898 loss: 1.08093391e-05
Iter: 1899 loss: 1.08959239e-05
Iter: 1900 loss: 1.08092854e-05
Iter: 1901 loss: 1.08053646e-05
Iter: 1902 loss: 1.08346339e-05
Iter: 1903 loss: 1.08050308e-05
Iter: 1904 loss: 1.08023487e-05
Iter: 1905 loss: 1.08262957e-05
Iter: 1906 loss: 1.08021468e-05
Iter: 1907 loss: 1.07994019e-05
Iter: 1908 loss: 1.07981541e-05
Iter: 1909 loss: 1.07967262e-05
Iter: 1910 loss: 1.07932583e-05
Iter: 1911 loss: 1.08300146e-05
Iter: 1912 loss: 1.07931774e-05
Iter: 1913 loss: 1.07912774e-05
Iter: 1914 loss: 1.07869955e-05
Iter: 1915 loss: 1.0852943e-05
Iter: 1916 loss: 1.07868491e-05
Iter: 1917 loss: 1.07849028e-05
Iter: 1918 loss: 1.07843834e-05
Iter: 1919 loss: 1.07818851e-05
Iter: 1920 loss: 1.07811738e-05
Iter: 1921 loss: 1.07796914e-05
Iter: 1922 loss: 1.0777032e-05
Iter: 1923 loss: 1.08123186e-05
Iter: 1924 loss: 1.07770111e-05
Iter: 1925 loss: 1.07749984e-05
Iter: 1926 loss: 1.07711221e-05
Iter: 1927 loss: 1.08505701e-05
Iter: 1928 loss: 1.07710603e-05
Iter: 1929 loss: 1.0766933e-05
Iter: 1930 loss: 1.07707483e-05
Iter: 1931 loss: 1.07644737e-05
Iter: 1932 loss: 1.07599153e-05
Iter: 1933 loss: 1.07679462e-05
Iter: 1934 loss: 1.07578699e-05
Iter: 1935 loss: 1.07531359e-05
Iter: 1936 loss: 1.07711694e-05
Iter: 1937 loss: 1.07520191e-05
Iter: 1938 loss: 1.07492697e-05
Iter: 1939 loss: 1.07489213e-05
Iter: 1940 loss: 1.07468313e-05
Iter: 1941 loss: 1.074385e-05
Iter: 1942 loss: 1.07437463e-05
Iter: 1943 loss: 1.07407523e-05
Iter: 1944 loss: 1.07639316e-05
Iter: 1945 loss: 1.0740594e-05
Iter: 1946 loss: 1.07377246e-05
Iter: 1947 loss: 1.07428987e-05
Iter: 1948 loss: 1.07364376e-05
Iter: 1949 loss: 1.07336946e-05
Iter: 1950 loss: 1.07488777e-05
Iter: 1951 loss: 1.07332526e-05
Iter: 1952 loss: 1.0730394e-05
Iter: 1953 loss: 1.07318592e-05
Iter: 1954 loss: 1.07284122e-05
Iter: 1955 loss: 1.0726143e-05
Iter: 1956 loss: 1.07322703e-05
Iter: 1957 loss: 1.07253845e-05
Iter: 1958 loss: 1.07219294e-05
Iter: 1959 loss: 1.07308852e-05
Iter: 1960 loss: 1.07207306e-05
Iter: 1961 loss: 1.07183459e-05
Iter: 1962 loss: 1.07158e-05
Iter: 1963 loss: 1.07154574e-05
Iter: 1964 loss: 1.07125206e-05
Iter: 1965 loss: 1.07490987e-05
Iter: 1966 loss: 1.07124833e-05
Iter: 1967 loss: 1.07095593e-05
Iter: 1968 loss: 1.07100605e-05
Iter: 1969 loss: 1.07073665e-05
Iter: 1970 loss: 1.07041142e-05
Iter: 1971 loss: 1.0699996e-05
Iter: 1972 loss: 1.0699674e-05
Iter: 1973 loss: 1.06949592e-05
Iter: 1974 loss: 1.07286523e-05
Iter: 1975 loss: 1.06945254e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script80
+ '[' -r STOP.script80 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi1.6
+ date
Mon Nov  2 12:30:49 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd02037c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd020262ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0202626a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd020262488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0202e9ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0202e9ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0200c6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0200da6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0200da400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd020147158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd020055158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd020066d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd020066d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0201fe158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd020140ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd45a1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd45c9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd45bc488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd458a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd458ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd454d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd454dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd45e3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd45e4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd45e4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd43fe7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd43ca6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd4467840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd44672f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd446d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd438e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd4515510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd45000d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd02022be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0202299d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcfd42b5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.00065181626
test_loss: 0.00069567346
train_loss: 0.00023683385
test_loss: 0.0002493511
train_loss: 0.0001359825
test_loss: 0.00015311557
train_loss: 0.00011522058
test_loss: 0.000118979624
train_loss: 0.00010556792
test_loss: 0.000100006866
train_loss: 7.7050245e-05
test_loss: 8.385087e-05
train_loss: 7.7533296e-05
test_loss: 7.794478e-05
train_loss: 7.511769e-05
test_loss: 7.063238e-05
train_loss: 7.026529e-05
test_loss: 6.846906e-05
train_loss: 6.1419225e-05
test_loss: 6.1306404e-05
train_loss: 5.3527612e-05
test_loss: 5.7848116e-05
train_loss: 5.43735e-05
test_loss: 5.7138066e-05
train_loss: 5.2080508e-05
test_loss: 5.7092595e-05
train_loss: 4.9942606e-05
test_loss: 5.089472e-05
train_loss: 4.6624584e-05
test_loss: 5.066935e-05
train_loss: 4.7545553e-05
test_loss: 4.7860274e-05
train_loss: 4.2789594e-05
test_loss: 4.68093e-05
train_loss: 4.7856476e-05
test_loss: 4.539834e-05
train_loss: 4.4627177e-05
test_loss: 4.7519607e-05
train_loss: 4.1600197e-05
test_loss: 4.2824257e-05
train_loss: 4.0565727e-05
test_loss: 4.2759606e-05
train_loss: 3.633418e-05
test_loss: 4.1495998e-05
train_loss: 3.991769e-05
test_loss: 4.102625e-05
train_loss: 3.3557324e-05
test_loss: 4.009603e-05
train_loss: 3.4684956e-05
test_loss: 3.9687606e-05
train_loss: 3.338621e-05
test_loss: 3.91572e-05
train_loss: 3.477891e-05
test_loss: 3.8229686e-05
train_loss: 3.2448712e-05
test_loss: 3.8001795e-05
train_loss: 3.370148e-05
test_loss: 3.764946e-05
train_loss: 3.3342363e-05
test_loss: 3.8277514e-05
train_loss: 3.3843615e-05
test_loss: 3.708487e-05
train_loss: 3.2823355e-05
test_loss: 3.664898e-05
train_loss: 3.3108066e-05
test_loss: 3.6427184e-05
train_loss: 3.1074087e-05
test_loss: 3.6209924e-05
train_loss: 2.920326e-05
test_loss: 3.5576435e-05
train_loss: 3.0415187e-05
test_loss: 3.583401e-05
train_loss: 3.3184257e-05
test_loss: 3.5866913e-05
train_loss: 2.8918053e-05
test_loss: 3.5248617e-05
train_loss: 3.161734e-05
test_loss: 3.513492e-05
train_loss: 3.1934844e-05
test_loss: 3.4836346e-05
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4404c24268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4404c24730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4404d39d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4404c24c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4404cf1d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f79f1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4404be9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4404bfa8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f7971598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f7971840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f7971510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f7949d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f7949ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f78f8ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f78f8b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f78f86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f78d7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f7897a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f7844ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f7844f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f77aa598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f77aa950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f77eb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f77e28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f7739840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f77028c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f76be8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f76be7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f76ad510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f777c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f75eda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f75edd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f76181e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f767f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f7618c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f751ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.95220525e-05
Iter: 2 loss: 2.967201e-05
Iter: 3 loss: 2.92283821e-05
Iter: 4 loss: 2.91004981e-05
Iter: 5 loss: 2.91021061e-05
Iter: 6 loss: 2.8998682e-05
Iter: 7 loss: 2.88535266e-05
Iter: 8 loss: 2.92513487e-05
Iter: 9 loss: 2.88061856e-05
Iter: 10 loss: 2.8709921e-05
Iter: 11 loss: 2.8859331e-05
Iter: 12 loss: 2.86647773e-05
Iter: 13 loss: 2.85804981e-05
Iter: 14 loss: 2.93394041e-05
Iter: 15 loss: 2.85764545e-05
Iter: 16 loss: 2.85107053e-05
Iter: 17 loss: 2.84519356e-05
Iter: 18 loss: 2.84352245e-05
Iter: 19 loss: 2.83560075e-05
Iter: 20 loss: 2.89594973e-05
Iter: 21 loss: 2.83500212e-05
Iter: 22 loss: 2.82705878e-05
Iter: 23 loss: 2.8356244e-05
Iter: 24 loss: 2.8227103e-05
Iter: 25 loss: 2.81529e-05
Iter: 26 loss: 2.83118698e-05
Iter: 27 loss: 2.81240791e-05
Iter: 28 loss: 2.80410259e-05
Iter: 29 loss: 2.84788439e-05
Iter: 30 loss: 2.80282402e-05
Iter: 31 loss: 2.79581127e-05
Iter: 32 loss: 2.80907425e-05
Iter: 33 loss: 2.79285468e-05
Iter: 34 loss: 2.78719526e-05
Iter: 35 loss: 2.78357911e-05
Iter: 36 loss: 2.78136049e-05
Iter: 37 loss: 2.77461295e-05
Iter: 38 loss: 2.81846405e-05
Iter: 39 loss: 2.77387207e-05
Iter: 40 loss: 2.77170675e-05
Iter: 41 loss: 2.77060371e-05
Iter: 42 loss: 2.76799092e-05
Iter: 43 loss: 2.76377068e-05
Iter: 44 loss: 2.7637223e-05
Iter: 45 loss: 2.76008304e-05
Iter: 46 loss: 2.77784202e-05
Iter: 47 loss: 2.7594313e-05
Iter: 48 loss: 2.75539569e-05
Iter: 49 loss: 2.76244464e-05
Iter: 50 loss: 2.75360908e-05
Iter: 51 loss: 2.75095827e-05
Iter: 52 loss: 2.76192404e-05
Iter: 53 loss: 2.75039383e-05
Iter: 54 loss: 2.7469423e-05
Iter: 55 loss: 2.74689373e-05
Iter: 56 loss: 2.74416416e-05
Iter: 57 loss: 2.74087852e-05
Iter: 58 loss: 2.74934282e-05
Iter: 59 loss: 2.73975475e-05
Iter: 60 loss: 2.73615278e-05
Iter: 61 loss: 2.75392467e-05
Iter: 62 loss: 2.73554651e-05
Iter: 63 loss: 2.73242367e-05
Iter: 64 loss: 2.74081358e-05
Iter: 65 loss: 2.73138685e-05
Iter: 66 loss: 2.72914549e-05
Iter: 67 loss: 2.73354544e-05
Iter: 68 loss: 2.72820325e-05
Iter: 69 loss: 2.72518027e-05
Iter: 70 loss: 2.73492424e-05
Iter: 71 loss: 2.72433172e-05
Iter: 72 loss: 2.72197376e-05
Iter: 73 loss: 2.72428879e-05
Iter: 74 loss: 2.72064153e-05
Iter: 75 loss: 2.71817225e-05
Iter: 76 loss: 2.72265388e-05
Iter: 77 loss: 2.71709523e-05
Iter: 78 loss: 2.71595763e-05
Iter: 79 loss: 2.71540739e-05
Iter: 80 loss: 2.71456956e-05
Iter: 81 loss: 2.71199478e-05
Iter: 82 loss: 2.71866902e-05
Iter: 83 loss: 2.71059325e-05
Iter: 84 loss: 2.70860855e-05
Iter: 85 loss: 2.70846413e-05
Iter: 86 loss: 2.70674864e-05
Iter: 87 loss: 2.71011086e-05
Iter: 88 loss: 2.7060436e-05
Iter: 89 loss: 2.70423e-05
Iter: 90 loss: 2.70454948e-05
Iter: 91 loss: 2.7028862e-05
Iter: 92 loss: 2.70087148e-05
Iter: 93 loss: 2.72451944e-05
Iter: 94 loss: 2.70083819e-05
Iter: 95 loss: 2.69948305e-05
Iter: 96 loss: 2.69773427e-05
Iter: 97 loss: 2.69762222e-05
Iter: 98 loss: 2.69533557e-05
Iter: 99 loss: 2.70792698e-05
Iter: 100 loss: 2.69500488e-05
Iter: 101 loss: 2.69315442e-05
Iter: 102 loss: 2.70843593e-05
Iter: 103 loss: 2.69304e-05
Iter: 104 loss: 2.6917849e-05
Iter: 105 loss: 2.68971326e-05
Iter: 106 loss: 2.6897038e-05
Iter: 107 loss: 2.68806361e-05
Iter: 108 loss: 2.68797594e-05
Iter: 109 loss: 2.68684344e-05
Iter: 110 loss: 2.68493859e-05
Iter: 111 loss: 2.68493295e-05
Iter: 112 loss: 2.68355761e-05
Iter: 113 loss: 2.68352414e-05
Iter: 114 loss: 2.68211461e-05
Iter: 115 loss: 2.68657495e-05
Iter: 116 loss: 2.68169952e-05
Iter: 117 loss: 2.68075037e-05
Iter: 118 loss: 2.67872347e-05
Iter: 119 loss: 2.71179415e-05
Iter: 120 loss: 2.67864943e-05
Iter: 121 loss: 2.67698488e-05
Iter: 122 loss: 2.69369793e-05
Iter: 123 loss: 2.6769274e-05
Iter: 124 loss: 2.67529431e-05
Iter: 125 loss: 2.68246531e-05
Iter: 126 loss: 2.67496926e-05
Iter: 127 loss: 2.67393134e-05
Iter: 128 loss: 2.67480646e-05
Iter: 129 loss: 2.67331052e-05
Iter: 130 loss: 2.67196247e-05
Iter: 131 loss: 2.6765747e-05
Iter: 132 loss: 2.67160976e-05
Iter: 133 loss: 2.6700669e-05
Iter: 134 loss: 2.67141477e-05
Iter: 135 loss: 2.66915813e-05
Iter: 136 loss: 2.66765346e-05
Iter: 137 loss: 2.66759798e-05
Iter: 138 loss: 2.66641946e-05
Iter: 139 loss: 2.66468596e-05
Iter: 140 loss: 2.6646816e-05
Iter: 141 loss: 2.66365132e-05
Iter: 142 loss: 2.66298666e-05
Iter: 143 loss: 2.66257266e-05
Iter: 144 loss: 2.6611855e-05
Iter: 145 loss: 2.67079977e-05
Iter: 146 loss: 2.6610549e-05
Iter: 147 loss: 2.65983763e-05
Iter: 148 loss: 2.66344159e-05
Iter: 149 loss: 2.65946182e-05
Iter: 150 loss: 2.65860617e-05
Iter: 151 loss: 2.66894185e-05
Iter: 152 loss: 2.65860817e-05
Iter: 153 loss: 2.65769268e-05
Iter: 154 loss: 2.65669441e-05
Iter: 155 loss: 2.65654799e-05
Iter: 156 loss: 2.65555864e-05
Iter: 157 loss: 2.65366325e-05
Iter: 158 loss: 2.69334741e-05
Iter: 159 loss: 2.65365034e-05
Iter: 160 loss: 2.65318922e-05
Iter: 161 loss: 2.65275921e-05
Iter: 162 loss: 2.65176732e-05
Iter: 163 loss: 2.65093531e-05
Iter: 164 loss: 2.65064809e-05
Iter: 165 loss: 2.64962891e-05
Iter: 166 loss: 2.65640174e-05
Iter: 167 loss: 2.64952978e-05
Iter: 168 loss: 2.64842383e-05
Iter: 169 loss: 2.64927912e-05
Iter: 170 loss: 2.64775081e-05
Iter: 171 loss: 2.64658247e-05
Iter: 172 loss: 2.64964474e-05
Iter: 173 loss: 2.64619521e-05
Iter: 174 loss: 2.64509799e-05
Iter: 175 loss: 2.64604059e-05
Iter: 176 loss: 2.64443515e-05
Iter: 177 loss: 2.64288356e-05
Iter: 178 loss: 2.65419621e-05
Iter: 179 loss: 2.6427535e-05
Iter: 180 loss: 2.641844e-05
Iter: 181 loss: 2.64137343e-05
Iter: 182 loss: 2.64095452e-05
Iter: 183 loss: 2.63984039e-05
Iter: 184 loss: 2.65382878e-05
Iter: 185 loss: 2.63983293e-05
Iter: 186 loss: 2.63903348e-05
Iter: 187 loss: 2.64290102e-05
Iter: 188 loss: 2.63890106e-05
Iter: 189 loss: 2.63805141e-05
Iter: 190 loss: 2.64023583e-05
Iter: 191 loss: 2.63775764e-05
Iter: 192 loss: 2.63699767e-05
Iter: 193 loss: 2.63581387e-05
Iter: 194 loss: 2.63580077e-05
Iter: 195 loss: 2.63459442e-05
Iter: 196 loss: 2.63547463e-05
Iter: 197 loss: 2.633845e-05
Iter: 198 loss: 2.63293878e-05
Iter: 199 loss: 2.63287548e-05
Iter: 200 loss: 2.63199927e-05
Iter: 201 loss: 2.63186084e-05
Iter: 202 loss: 2.63123275e-05
Iter: 203 loss: 2.63027432e-05
Iter: 204 loss: 2.63161783e-05
Iter: 205 loss: 2.62980429e-05
Iter: 206 loss: 2.62879112e-05
Iter: 207 loss: 2.6390584e-05
Iter: 208 loss: 2.62875747e-05
Iter: 209 loss: 2.62797621e-05
Iter: 210 loss: 2.62697577e-05
Iter: 211 loss: 2.62690046e-05
Iter: 212 loss: 2.62574595e-05
Iter: 213 loss: 2.63460915e-05
Iter: 214 loss: 2.62564536e-05
Iter: 215 loss: 2.6246742e-05
Iter: 216 loss: 2.62935428e-05
Iter: 217 loss: 2.62449466e-05
Iter: 218 loss: 2.62354588e-05
Iter: 219 loss: 2.62303256e-05
Iter: 220 loss: 2.62260692e-05
Iter: 221 loss: 2.62199374e-05
Iter: 222 loss: 2.62194517e-05
Iter: 223 loss: 2.62126887e-05
Iter: 224 loss: 2.62200738e-05
Iter: 225 loss: 2.6209118e-05
Iter: 226 loss: 2.62009562e-05
Iter: 227 loss: 2.62092726e-05
Iter: 228 loss: 2.61964033e-05
Iter: 229 loss: 2.61884743e-05
Iter: 230 loss: 2.6191432e-05
Iter: 231 loss: 2.61829227e-05
Iter: 232 loss: 2.61731366e-05
Iter: 233 loss: 2.61682253e-05
Iter: 234 loss: 2.6163485e-05
Iter: 235 loss: 2.61577625e-05
Iter: 236 loss: 2.61561181e-05
Iter: 237 loss: 2.61485766e-05
Iter: 238 loss: 2.61396908e-05
Iter: 239 loss: 2.61387286e-05
Iter: 240 loss: 2.61273453e-05
Iter: 241 loss: 2.61697496e-05
Iter: 242 loss: 2.61244786e-05
Iter: 243 loss: 2.61153618e-05
Iter: 244 loss: 2.61645182e-05
Iter: 245 loss: 2.61140103e-05
Iter: 246 loss: 2.61036221e-05
Iter: 247 loss: 2.61052337e-05
Iter: 248 loss: 2.6095644e-05
Iter: 249 loss: 2.60859197e-05
Iter: 250 loss: 2.61005898e-05
Iter: 251 loss: 2.60813722e-05
Iter: 252 loss: 2.60707911e-05
Iter: 253 loss: 2.61846835e-05
Iter: 254 loss: 2.60706729e-05
Iter: 255 loss: 2.60625693e-05
Iter: 256 loss: 2.60642209e-05
Iter: 257 loss: 2.60567576e-05
Iter: 258 loss: 2.60504694e-05
Iter: 259 loss: 2.60503712e-05
Iter: 260 loss: 2.60437191e-05
Iter: 261 loss: 2.60339e-05
Iter: 262 loss: 2.60336747e-05
Iter: 263 loss: 2.602526e-05
Iter: 264 loss: 2.60833913e-05
Iter: 265 loss: 2.60245761e-05
Iter: 266 loss: 2.60169072e-05
Iter: 267 loss: 2.60090892e-05
Iter: 268 loss: 2.60076831e-05
Iter: 269 loss: 2.59968037e-05
Iter: 270 loss: 2.6037218e-05
Iter: 271 loss: 2.59941953e-05
Iter: 272 loss: 2.59850203e-05
Iter: 273 loss: 2.6065698e-05
Iter: 274 loss: 2.59846311e-05
Iter: 275 loss: 2.59755034e-05
Iter: 276 loss: 2.59866611e-05
Iter: 277 loss: 2.59706558e-05
Iter: 278 loss: 2.5962112e-05
Iter: 279 loss: 2.59543085e-05
Iter: 280 loss: 2.59522403e-05
Iter: 281 loss: 2.5944335e-05
Iter: 282 loss: 2.59441149e-05
Iter: 283 loss: 2.59365552e-05
Iter: 284 loss: 2.59337921e-05
Iter: 285 loss: 2.59297703e-05
Iter: 286 loss: 2.59189474e-05
Iter: 287 loss: 2.59419612e-05
Iter: 288 loss: 2.5914811e-05
Iter: 289 loss: 2.59063272e-05
Iter: 290 loss: 2.59724548e-05
Iter: 291 loss: 2.59057451e-05
Iter: 292 loss: 2.58964319e-05
Iter: 293 loss: 2.59017343e-05
Iter: 294 loss: 2.58903237e-05
Iter: 295 loss: 2.58849523e-05
Iter: 296 loss: 2.58839682e-05
Iter: 297 loss: 2.58804794e-05
Iter: 298 loss: 2.58704313e-05
Iter: 299 loss: 2.59072403e-05
Iter: 300 loss: 2.58659875e-05
Iter: 301 loss: 2.58581204e-05
Iter: 302 loss: 2.58577547e-05
Iter: 303 loss: 2.58507316e-05
Iter: 304 loss: 2.58426298e-05
Iter: 305 loss: 2.5841744e-05
Iter: 306 loss: 2.58306318e-05
Iter: 307 loss: 2.58836044e-05
Iter: 308 loss: 2.58285436e-05
Iter: 309 loss: 2.58196524e-05
Iter: 310 loss: 2.59215867e-05
Iter: 311 loss: 2.5819445e-05
Iter: 312 loss: 2.58124855e-05
Iter: 313 loss: 2.58013915e-05
Iter: 314 loss: 2.58013297e-05
Iter: 315 loss: 2.57892607e-05
Iter: 316 loss: 2.58474774e-05
Iter: 317 loss: 2.57871598e-05
Iter: 318 loss: 2.57782685e-05
Iter: 319 loss: 2.58352284e-05
Iter: 320 loss: 2.57771353e-05
Iter: 321 loss: 2.57672036e-05
Iter: 322 loss: 2.57734282e-05
Iter: 323 loss: 2.57607135e-05
Iter: 324 loss: 2.57522424e-05
Iter: 325 loss: 2.57593656e-05
Iter: 326 loss: 2.57471784e-05
Iter: 327 loss: 2.57386273e-05
Iter: 328 loss: 2.57385e-05
Iter: 329 loss: 2.5732732e-05
Iter: 330 loss: 2.57585343e-05
Iter: 331 loss: 2.57316351e-05
Iter: 332 loss: 2.57257816e-05
Iter: 333 loss: 2.57278989e-05
Iter: 334 loss: 2.57217544e-05
Iter: 335 loss: 2.57147403e-05
Iter: 336 loss: 2.57014326e-05
Iter: 337 loss: 2.59846602e-05
Iter: 338 loss: 2.5701338e-05
Iter: 339 loss: 2.569149e-05
Iter: 340 loss: 2.56914427e-05
Iter: 341 loss: 2.56819876e-05
Iter: 342 loss: 2.56825697e-05
Iter: 343 loss: 2.56746807e-05
Iter: 344 loss: 2.56657968e-05
Iter: 345 loss: 2.57242173e-05
Iter: 346 loss: 2.56648091e-05
Iter: 347 loss: 2.56566527e-05
Iter: 348 loss: 2.56951353e-05
Iter: 349 loss: 2.56551066e-05
Iter: 350 loss: 2.56474959e-05
Iter: 351 loss: 2.56414241e-05
Iter: 352 loss: 2.56389685e-05
Iter: 353 loss: 2.5629e-05
Iter: 354 loss: 2.56297681e-05
Iter: 355 loss: 2.56213207e-05
Iter: 356 loss: 2.56146668e-05
Iter: 357 loss: 2.56132571e-05
Iter: 358 loss: 2.56074109e-05
Iter: 359 loss: 2.56014719e-05
Iter: 360 loss: 2.56004332e-05
Iter: 361 loss: 2.55906016e-05
Iter: 362 loss: 2.56262065e-05
Iter: 363 loss: 2.55881241e-05
Iter: 364 loss: 2.55816976e-05
Iter: 365 loss: 2.5581603e-05
Iter: 366 loss: 2.55770537e-05
Iter: 367 loss: 2.55735195e-05
Iter: 368 loss: 2.55721134e-05
Iter: 369 loss: 2.55633095e-05
Iter: 370 loss: 2.5571564e-05
Iter: 371 loss: 2.55583436e-05
Iter: 372 loss: 2.55504892e-05
Iter: 373 loss: 2.55452123e-05
Iter: 374 loss: 2.55423438e-05
Iter: 375 loss: 2.55307114e-05
Iter: 376 loss: 2.56304738e-05
Iter: 377 loss: 2.55301165e-05
Iter: 378 loss: 2.55214654e-05
Iter: 379 loss: 2.55613268e-05
Iter: 380 loss: 2.55198e-05
Iter: 381 loss: 2.55121158e-05
Iter: 382 loss: 2.55156774e-05
Iter: 383 loss: 2.55068844e-05
Iter: 384 loss: 2.54987535e-05
Iter: 385 loss: 2.56073781e-05
Iter: 386 loss: 2.54986353e-05
Iter: 387 loss: 2.54924889e-05
Iter: 388 loss: 2.54803581e-05
Iter: 389 loss: 2.57126139e-05
Iter: 390 loss: 2.54802035e-05
Iter: 391 loss: 2.54662482e-05
Iter: 392 loss: 2.55161e-05
Iter: 393 loss: 2.54626721e-05
Iter: 394 loss: 2.54534516e-05
Iter: 395 loss: 2.55889099e-05
Iter: 396 loss: 2.5453437e-05
Iter: 397 loss: 2.54443275e-05
Iter: 398 loss: 2.54394836e-05
Iter: 399 loss: 2.54353163e-05
Iter: 400 loss: 2.54303e-05
Iter: 401 loss: 2.54299794e-05
Iter: 402 loss: 2.54238639e-05
Iter: 403 loss: 2.54188635e-05
Iter: 404 loss: 2.54170536e-05
Iter: 405 loss: 2.54091774e-05
Iter: 406 loss: 2.54336428e-05
Iter: 407 loss: 2.54068345e-05
Iter: 408 loss: 2.53980234e-05
Iter: 409 loss: 2.54037841e-05
Iter: 410 loss: 2.53926883e-05
Iter: 411 loss: 2.53822473e-05
Iter: 412 loss: 2.53796825e-05
Iter: 413 loss: 2.53733324e-05
Iter: 414 loss: 2.53607195e-05
Iter: 415 loss: 2.54638253e-05
Iter: 416 loss: 2.5359921e-05
Iter: 417 loss: 2.5349962e-05
Iter: 418 loss: 2.54019869e-05
Iter: 419 loss: 2.53483777e-05
Iter: 420 loss: 2.53391463e-05
Iter: 421 loss: 2.53467588e-05
Iter: 422 loss: 2.53335493e-05
Iter: 423 loss: 2.53246599e-05
Iter: 424 loss: 2.54265342e-05
Iter: 425 loss: 2.53245198e-05
Iter: 426 loss: 2.53178405e-05
Iter: 427 loss: 2.5304922e-05
Iter: 428 loss: 2.55707473e-05
Iter: 429 loss: 2.53047765e-05
Iter: 430 loss: 2.52931241e-05
Iter: 431 loss: 2.53503385e-05
Iter: 432 loss: 2.5291054e-05
Iter: 433 loss: 2.52788068e-05
Iter: 434 loss: 2.5360614e-05
Iter: 435 loss: 2.52775426e-05
Iter: 436 loss: 2.52671562e-05
Iter: 437 loss: 2.52929058e-05
Iter: 438 loss: 2.52635764e-05
Iter: 439 loss: 2.52568243e-05
Iter: 440 loss: 2.52567079e-05
Iter: 441 loss: 2.52522041e-05
Iter: 442 loss: 2.52410864e-05
Iter: 443 loss: 2.53510807e-05
Iter: 444 loss: 2.52396894e-05
Iter: 445 loss: 2.52305217e-05
Iter: 446 loss: 2.53614417e-05
Iter: 447 loss: 2.52305654e-05
Iter: 448 loss: 2.52227674e-05
Iter: 449 loss: 2.52192658e-05
Iter: 450 loss: 2.5215295e-05
Iter: 451 loss: 2.52036389e-05
Iter: 452 loss: 2.52137725e-05
Iter: 453 loss: 2.51968031e-05
Iter: 454 loss: 2.51828897e-05
Iter: 455 loss: 2.52110913e-05
Iter: 456 loss: 2.5177309e-05
Iter: 457 loss: 2.51661804e-05
Iter: 458 loss: 2.51661368e-05
Iter: 459 loss: 2.51588099e-05
Iter: 460 loss: 2.5155281e-05
Iter: 461 loss: 2.51517358e-05
Iter: 462 loss: 2.51391211e-05
Iter: 463 loss: 2.52008958e-05
Iter: 464 loss: 2.51369038e-05
Iter: 465 loss: 2.51280708e-05
Iter: 466 loss: 2.51310339e-05
Iter: 467 loss: 2.51217152e-05
Iter: 468 loss: 2.51099773e-05
Iter: 469 loss: 2.51079418e-05
Iter: 470 loss: 2.50999437e-05
Iter: 471 loss: 2.50900212e-05
Iter: 472 loss: 2.50898665e-05
Iter: 473 loss: 2.50797857e-05
Iter: 474 loss: 2.50966496e-05
Iter: 475 loss: 2.50752801e-05
Iter: 476 loss: 2.50665398e-05
Iter: 477 loss: 2.51773017e-05
Iter: 478 loss: 2.50665435e-05
Iter: 479 loss: 2.50611592e-05
Iter: 480 loss: 2.50490248e-05
Iter: 481 loss: 2.52166283e-05
Iter: 482 loss: 2.50483718e-05
Iter: 483 loss: 2.50353278e-05
Iter: 484 loss: 2.50910125e-05
Iter: 485 loss: 2.5032532e-05
Iter: 486 loss: 2.50222565e-05
Iter: 487 loss: 2.51171077e-05
Iter: 488 loss: 2.50217163e-05
Iter: 489 loss: 2.50132507e-05
Iter: 490 loss: 2.49976965e-05
Iter: 491 loss: 2.53695671e-05
Iter: 492 loss: 2.49976674e-05
Iter: 493 loss: 2.49831064e-05
Iter: 494 loss: 2.50369521e-05
Iter: 495 loss: 2.49794575e-05
Iter: 496 loss: 2.49665445e-05
Iter: 497 loss: 2.51085603e-05
Iter: 498 loss: 2.49663281e-05
Iter: 499 loss: 2.49557306e-05
Iter: 500 loss: 2.4998275e-05
Iter: 501 loss: 2.49534478e-05
Iter: 502 loss: 2.49456589e-05
Iter: 503 loss: 2.49587029e-05
Iter: 504 loss: 2.49421173e-05
Iter: 505 loss: 2.49314089e-05
Iter: 506 loss: 2.49409877e-05
Iter: 507 loss: 2.49251389e-05
Iter: 508 loss: 2.49130462e-05
Iter: 509 loss: 2.49289697e-05
Iter: 510 loss: 2.49070727e-05
Iter: 511 loss: 2.48952856e-05
Iter: 512 loss: 2.49352124e-05
Iter: 513 loss: 2.48922406e-05
Iter: 514 loss: 2.48816104e-05
Iter: 515 loss: 2.48815268e-05
Iter: 516 loss: 2.48760225e-05
Iter: 517 loss: 2.48872493e-05
Iter: 518 loss: 2.48738106e-05
Iter: 519 loss: 2.48663255e-05
Iter: 520 loss: 2.48519136e-05
Iter: 521 loss: 2.51403217e-05
Iter: 522 loss: 2.48518045e-05
Iter: 523 loss: 2.48407796e-05
Iter: 524 loss: 2.48746182e-05
Iter: 525 loss: 2.48374672e-05
Iter: 526 loss: 2.482329e-05
Iter: 527 loss: 2.48845936e-05
Iter: 528 loss: 2.48203232e-05
Iter: 529 loss: 2.4808247e-05
Iter: 530 loss: 2.48443375e-05
Iter: 531 loss: 2.48046363e-05
Iter: 532 loss: 2.47956923e-05
Iter: 533 loss: 2.47784828e-05
Iter: 534 loss: 2.5136047e-05
Iter: 535 loss: 2.47784046e-05
Iter: 536 loss: 2.47656135e-05
Iter: 537 loss: 2.4765377e-05
Iter: 538 loss: 2.47550524e-05
Iter: 539 loss: 2.47798835e-05
Iter: 540 loss: 2.4751349e-05
Iter: 541 loss: 2.47367861e-05
Iter: 542 loss: 2.47737407e-05
Iter: 543 loss: 2.47319294e-05
Iter: 544 loss: 2.47231928e-05
Iter: 545 loss: 2.47892749e-05
Iter: 546 loss: 2.47224743e-05
Iter: 547 loss: 2.47137941e-05
Iter: 548 loss: 2.47013086e-05
Iter: 549 loss: 2.47009411e-05
Iter: 550 loss: 2.46894087e-05
Iter: 551 loss: 2.4791163e-05
Iter: 552 loss: 2.46887284e-05
Iter: 553 loss: 2.4682995e-05
Iter: 554 loss: 2.46827658e-05
Iter: 555 loss: 2.46779218e-05
Iter: 556 loss: 2.46649597e-05
Iter: 557 loss: 2.47603639e-05
Iter: 558 loss: 2.46622731e-05
Iter: 559 loss: 2.46512718e-05
Iter: 560 loss: 2.47547687e-05
Iter: 561 loss: 2.46507952e-05
Iter: 562 loss: 2.46398558e-05
Iter: 563 loss: 2.46692107e-05
Iter: 564 loss: 2.4636216e-05
Iter: 565 loss: 2.46274176e-05
Iter: 566 loss: 2.46179625e-05
Iter: 567 loss: 2.4616289e-05
Iter: 568 loss: 2.46012387e-05
Iter: 569 loss: 2.46781601e-05
Iter: 570 loss: 2.45986448e-05
Iter: 571 loss: 2.45884366e-05
Iter: 572 loss: 2.47353109e-05
Iter: 573 loss: 2.45885258e-05
Iter: 574 loss: 2.45806405e-05
Iter: 575 loss: 2.45653955e-05
Iter: 576 loss: 2.48715987e-05
Iter: 577 loss: 2.45653264e-05
Iter: 578 loss: 2.45497449e-05
Iter: 579 loss: 2.46308773e-05
Iter: 580 loss: 2.45473311e-05
Iter: 581 loss: 2.45362062e-05
Iter: 582 loss: 2.45832634e-05
Iter: 583 loss: 2.45339033e-05
Iter: 584 loss: 2.45216e-05
Iter: 585 loss: 2.46181025e-05
Iter: 586 loss: 2.45208048e-05
Iter: 587 loss: 2.45141418e-05
Iter: 588 loss: 2.45153078e-05
Iter: 589 loss: 2.45091469e-05
Iter: 590 loss: 2.44988732e-05
Iter: 591 loss: 2.45710271e-05
Iter: 592 loss: 2.44980092e-05
Iter: 593 loss: 2.4490224e-05
Iter: 594 loss: 2.45250449e-05
Iter: 595 loss: 2.44887815e-05
Iter: 596 loss: 2.4482395e-05
Iter: 597 loss: 2.44702278e-05
Iter: 598 loss: 2.47392236e-05
Iter: 599 loss: 2.4470166e-05
Iter: 600 loss: 2.44553594e-05
Iter: 601 loss: 2.44876264e-05
Iter: 602 loss: 2.44495823e-05
Iter: 603 loss: 2.4437868e-05
Iter: 604 loss: 2.45261253e-05
Iter: 605 loss: 2.44369148e-05
Iter: 606 loss: 2.44242146e-05
Iter: 607 loss: 2.44542334e-05
Iter: 608 loss: 2.44195526e-05
Iter: 609 loss: 2.44107068e-05
Iter: 610 loss: 2.44093753e-05
Iter: 611 loss: 2.44031598e-05
Iter: 612 loss: 2.43894065e-05
Iter: 613 loss: 2.44045332e-05
Iter: 614 loss: 2.4381985e-05
Iter: 615 loss: 2.43712348e-05
Iter: 616 loss: 2.43707582e-05
Iter: 617 loss: 2.43628019e-05
Iter: 618 loss: 2.43447721e-05
Iter: 619 loss: 2.45801239e-05
Iter: 620 loss: 2.43435297e-05
Iter: 621 loss: 2.43268678e-05
Iter: 622 loss: 2.44854054e-05
Iter: 623 loss: 2.43262257e-05
Iter: 624 loss: 2.43145187e-05
Iter: 625 loss: 2.43944232e-05
Iter: 626 loss: 2.43133109e-05
Iter: 627 loss: 2.43011091e-05
Iter: 628 loss: 2.4335066e-05
Iter: 629 loss: 2.42971655e-05
Iter: 630 loss: 2.42899732e-05
Iter: 631 loss: 2.43419199e-05
Iter: 632 loss: 2.42893148e-05
Iter: 633 loss: 2.42805218e-05
Iter: 634 loss: 2.42777369e-05
Iter: 635 loss: 2.4272711e-05
Iter: 636 loss: 2.4263365e-05
Iter: 637 loss: 2.43144132e-05
Iter: 638 loss: 2.42619699e-05
Iter: 639 loss: 2.42540918e-05
Iter: 640 loss: 2.42402803e-05
Iter: 641 loss: 2.4240333e-05
Iter: 642 loss: 2.4225199e-05
Iter: 643 loss: 2.43669783e-05
Iter: 644 loss: 2.42246024e-05
Iter: 645 loss: 2.42147798e-05
Iter: 646 loss: 2.43043396e-05
Iter: 647 loss: 2.42143597e-05
Iter: 648 loss: 2.4207422e-05
Iter: 649 loss: 2.41914167e-05
Iter: 650 loss: 2.44015428e-05
Iter: 651 loss: 2.41904145e-05
Iter: 652 loss: 2.41742746e-05
Iter: 653 loss: 2.4273133e-05
Iter: 654 loss: 2.41724192e-05
Iter: 655 loss: 2.41592115e-05
Iter: 656 loss: 2.4296156e-05
Iter: 657 loss: 2.4158915e-05
Iter: 658 loss: 2.41481e-05
Iter: 659 loss: 2.4147228e-05
Iter: 660 loss: 2.41391208e-05
Iter: 661 loss: 2.41251419e-05
Iter: 662 loss: 2.41399102e-05
Iter: 663 loss: 2.41174912e-05
Iter: 664 loss: 2.41086909e-05
Iter: 665 loss: 2.41082726e-05
Iter: 666 loss: 2.40995178e-05
Iter: 667 loss: 2.41046218e-05
Iter: 668 loss: 2.40938625e-05
Iter: 669 loss: 2.4085115e-05
Iter: 670 loss: 2.41899834e-05
Iter: 671 loss: 2.4084904e-05
Iter: 672 loss: 2.40781064e-05
Iter: 673 loss: 2.40679256e-05
Iter: 674 loss: 2.40677728e-05
Iter: 675 loss: 2.40565059e-05
Iter: 676 loss: 2.4078261e-05
Iter: 677 loss: 2.40518602e-05
Iter: 678 loss: 2.40372574e-05
Iter: 679 loss: 2.40923437e-05
Iter: 680 loss: 2.4033754e-05
Iter: 681 loss: 2.40238e-05
Iter: 682 loss: 2.4051018e-05
Iter: 683 loss: 2.40206136e-05
Iter: 684 loss: 2.40091395e-05
Iter: 685 loss: 2.40563659e-05
Iter: 686 loss: 2.40067966e-05
Iter: 687 loss: 2.3998673e-05
Iter: 688 loss: 2.39908604e-05
Iter: 689 loss: 2.39890578e-05
Iter: 690 loss: 2.39751171e-05
Iter: 691 loss: 2.39916753e-05
Iter: 692 loss: 2.39676301e-05
Iter: 693 loss: 2.395712e-05
Iter: 694 loss: 2.39567198e-05
Iter: 695 loss: 2.39475012e-05
Iter: 696 loss: 2.39368692e-05
Iter: 697 loss: 2.39355577e-05
Iter: 698 loss: 2.39195888e-05
Iter: 699 loss: 2.39797409e-05
Iter: 700 loss: 2.39156525e-05
Iter: 701 loss: 2.39088495e-05
Iter: 702 loss: 2.390808e-05
Iter: 703 loss: 2.39010806e-05
Iter: 704 loss: 2.38950142e-05
Iter: 705 loss: 2.38932062e-05
Iter: 706 loss: 2.38829853e-05
Iter: 707 loss: 2.39811125e-05
Iter: 708 loss: 2.38826469e-05
Iter: 709 loss: 2.38767389e-05
Iter: 710 loss: 2.38608191e-05
Iter: 711 loss: 2.39596393e-05
Iter: 712 loss: 2.38566045e-05
Iter: 713 loss: 2.38472512e-05
Iter: 714 loss: 2.38459907e-05
Iter: 715 loss: 2.38363318e-05
Iter: 716 loss: 2.38389875e-05
Iter: 717 loss: 2.38294597e-05
Iter: 718 loss: 2.38159537e-05
Iter: 719 loss: 2.38449466e-05
Iter: 720 loss: 2.38106604e-05
Iter: 721 loss: 2.37989898e-05
Iter: 722 loss: 2.39054243e-05
Iter: 723 loss: 2.37984859e-05
Iter: 724 loss: 2.37904787e-05
Iter: 725 loss: 2.37742315e-05
Iter: 726 loss: 2.40841255e-05
Iter: 727 loss: 2.37740242e-05
Iter: 728 loss: 2.37586282e-05
Iter: 729 loss: 2.38335342e-05
Iter: 730 loss: 2.37560853e-05
Iter: 731 loss: 2.37436616e-05
Iter: 732 loss: 2.38921621e-05
Iter: 733 loss: 2.37434579e-05
Iter: 734 loss: 2.37330623e-05
Iter: 735 loss: 2.37482564e-05
Iter: 736 loss: 2.37281347e-05
Iter: 737 loss: 2.3717821e-05
Iter: 738 loss: 2.37613931e-05
Iter: 739 loss: 2.37156382e-05
Iter: 740 loss: 2.37024e-05
Iter: 741 loss: 2.37387267e-05
Iter: 742 loss: 2.36980795e-05
Iter: 743 loss: 2.369102e-05
Iter: 744 loss: 2.37147578e-05
Iter: 745 loss: 2.36892338e-05
Iter: 746 loss: 2.3680197e-05
Iter: 747 loss: 2.36702872e-05
Iter: 748 loss: 2.36688611e-05
Iter: 749 loss: 2.36551823e-05
Iter: 750 loss: 2.36786072e-05
Iter: 751 loss: 2.36491214e-05
Iter: 752 loss: 2.36381075e-05
Iter: 753 loss: 2.37199765e-05
Iter: 754 loss: 2.36373562e-05
Iter: 755 loss: 2.36245578e-05
Iter: 756 loss: 2.36396136e-05
Iter: 757 loss: 2.36178257e-05
Iter: 758 loss: 2.36069827e-05
Iter: 759 loss: 2.36372143e-05
Iter: 760 loss: 2.36034502e-05
Iter: 761 loss: 2.35910229e-05
Iter: 762 loss: 2.36197047e-05
Iter: 763 loss: 2.35864391e-05
Iter: 764 loss: 2.35740263e-05
Iter: 765 loss: 2.35864172e-05
Iter: 766 loss: 2.35669704e-05
Iter: 767 loss: 2.35546177e-05
Iter: 768 loss: 2.35649404e-05
Iter: 769 loss: 2.35474436e-05
Iter: 770 loss: 2.35347234e-05
Iter: 771 loss: 2.36722444e-05
Iter: 772 loss: 2.35344123e-05
Iter: 773 loss: 2.35237494e-05
Iter: 774 loss: 2.35891166e-05
Iter: 775 loss: 2.35224488e-05
Iter: 776 loss: 2.35143361e-05
Iter: 777 loss: 2.35597709e-05
Iter: 778 loss: 2.35131974e-05
Iter: 779 loss: 2.35053685e-05
Iter: 780 loss: 2.34937852e-05
Iter: 781 loss: 2.34934778e-05
Iter: 782 loss: 2.34834733e-05
Iter: 783 loss: 2.35985717e-05
Iter: 784 loss: 2.34833096e-05
Iter: 785 loss: 2.34734143e-05
Iter: 786 loss: 2.34650925e-05
Iter: 787 loss: 2.34623676e-05
Iter: 788 loss: 2.34513172e-05
Iter: 789 loss: 2.346572e-05
Iter: 790 loss: 2.34457366e-05
Iter: 791 loss: 2.34327727e-05
Iter: 792 loss: 2.36043888e-05
Iter: 793 loss: 2.34326853e-05
Iter: 794 loss: 2.34243125e-05
Iter: 795 loss: 2.3421122e-05
Iter: 796 loss: 2.34166182e-05
Iter: 797 loss: 2.3406028e-05
Iter: 798 loss: 2.34570944e-05
Iter: 799 loss: 2.34042109e-05
Iter: 800 loss: 2.33920655e-05
Iter: 801 loss: 2.33973424e-05
Iter: 802 loss: 2.33837327e-05
Iter: 803 loss: 2.33711398e-05
Iter: 804 loss: 2.33804894e-05
Iter: 805 loss: 2.33634782e-05
Iter: 806 loss: 2.33473802e-05
Iter: 807 loss: 2.34150139e-05
Iter: 808 loss: 2.33441497e-05
Iter: 809 loss: 2.3335986e-05
Iter: 810 loss: 2.3335846e-05
Iter: 811 loss: 2.33259634e-05
Iter: 812 loss: 2.33218561e-05
Iter: 813 loss: 2.33166356e-05
Iter: 814 loss: 2.33100127e-05
Iter: 815 loss: 2.33685932e-05
Iter: 816 loss: 2.33096398e-05
Iter: 817 loss: 2.33029241e-05
Iter: 818 loss: 2.32897255e-05
Iter: 819 loss: 2.35547122e-05
Iter: 820 loss: 2.32896418e-05
Iter: 821 loss: 2.3278948e-05
Iter: 822 loss: 2.32787916e-05
Iter: 823 loss: 2.32711609e-05
Iter: 824 loss: 2.32640759e-05
Iter: 825 loss: 2.32625134e-05
Iter: 826 loss: 2.32501789e-05
Iter: 827 loss: 2.32572183e-05
Iter: 828 loss: 2.32423281e-05
Iter: 829 loss: 2.32330294e-05
Iter: 830 loss: 2.32324164e-05
Iter: 831 loss: 2.32242983e-05
Iter: 832 loss: 2.32159764e-05
Iter: 833 loss: 2.32144921e-05
Iter: 834 loss: 2.32021612e-05
Iter: 835 loss: 2.32494258e-05
Iter: 836 loss: 2.31993e-05
Iter: 837 loss: 2.3187742e-05
Iter: 838 loss: 2.32362545e-05
Iter: 839 loss: 2.31852646e-05
Iter: 840 loss: 2.31746508e-05
Iter: 841 loss: 2.31695522e-05
Iter: 842 loss: 2.31644262e-05
Iter: 843 loss: 2.31511403e-05
Iter: 844 loss: 2.31964768e-05
Iter: 845 loss: 2.31475515e-05
Iter: 846 loss: 2.31410304e-05
Iter: 847 loss: 2.31399754e-05
Iter: 848 loss: 2.31319318e-05
Iter: 849 loss: 2.31212161e-05
Iter: 850 loss: 2.31205522e-05
Iter: 851 loss: 2.31114718e-05
Iter: 852 loss: 2.3167031e-05
Iter: 853 loss: 2.31105078e-05
Iter: 854 loss: 2.31000122e-05
Iter: 855 loss: 2.30977566e-05
Iter: 856 loss: 2.309099e-05
Iter: 857 loss: 2.30823571e-05
Iter: 858 loss: 2.31863669e-05
Iter: 859 loss: 2.30822334e-05
Iter: 860 loss: 2.30743735e-05
Iter: 861 loss: 2.30634614e-05
Iter: 862 loss: 2.3062923e-05
Iter: 863 loss: 2.30498299e-05
Iter: 864 loss: 2.3108516e-05
Iter: 865 loss: 2.30472906e-05
Iter: 866 loss: 2.30380665e-05
Iter: 867 loss: 2.31446738e-05
Iter: 868 loss: 2.30378955e-05
Iter: 869 loss: 2.30290607e-05
Iter: 870 loss: 2.30167643e-05
Iter: 871 loss: 2.30162987e-05
Iter: 872 loss: 2.30050264e-05
Iter: 873 loss: 2.3074881e-05
Iter: 874 loss: 2.30037331e-05
Iter: 875 loss: 2.29918314e-05
Iter: 876 loss: 2.30137684e-05
Iter: 877 loss: 2.2986691e-05
Iter: 878 loss: 2.29748985e-05
Iter: 879 loss: 2.29850339e-05
Iter: 880 loss: 2.29678699e-05
Iter: 881 loss: 2.29555772e-05
Iter: 882 loss: 2.30349706e-05
Iter: 883 loss: 2.29544021e-05
Iter: 884 loss: 2.29418365e-05
Iter: 885 loss: 2.30460355e-05
Iter: 886 loss: 2.29412581e-05
Iter: 887 loss: 2.29356883e-05
Iter: 888 loss: 2.29252746e-05
Iter: 889 loss: 2.31461308e-05
Iter: 890 loss: 2.2925291e-05
Iter: 891 loss: 2.29139077e-05
Iter: 892 loss: 2.30499063e-05
Iter: 893 loss: 2.29138022e-05
Iter: 894 loss: 2.29050675e-05
Iter: 895 loss: 2.29038069e-05
Iter: 896 loss: 2.28975605e-05
Iter: 897 loss: 2.28883509e-05
Iter: 898 loss: 2.29699617e-05
Iter: 899 loss: 2.28879107e-05
Iter: 900 loss: 2.28797653e-05
Iter: 901 loss: 2.28704666e-05
Iter: 902 loss: 2.2869257e-05
Iter: 903 loss: 2.28579447e-05
Iter: 904 loss: 2.29489451e-05
Iter: 905 loss: 2.28573263e-05
Iter: 906 loss: 2.28467834e-05
Iter: 907 loss: 2.28902627e-05
Iter: 908 loss: 2.28445297e-05
Iter: 909 loss: 2.28355821e-05
Iter: 910 loss: 2.28264817e-05
Iter: 911 loss: 2.28247391e-05
Iter: 912 loss: 2.28134959e-05
Iter: 913 loss: 2.28927311e-05
Iter: 914 loss: 2.2812359e-05
Iter: 915 loss: 2.28000572e-05
Iter: 916 loss: 2.2817796e-05
Iter: 917 loss: 2.27940054e-05
Iter: 918 loss: 2.27824712e-05
Iter: 919 loss: 2.27949604e-05
Iter: 920 loss: 2.27761593e-05
Iter: 921 loss: 2.27691635e-05
Iter: 922 loss: 2.27684432e-05
Iter: 923 loss: 2.27602286e-05
Iter: 924 loss: 2.27557357e-05
Iter: 925 loss: 2.27521487e-05
Iter: 926 loss: 2.27437922e-05
Iter: 927 loss: 2.27383698e-05
Iter: 928 loss: 2.2735112e-05
Iter: 929 loss: 2.2724842e-05
Iter: 930 loss: 2.27248074e-05
Iter: 931 loss: 2.2717968e-05
Iter: 932 loss: 2.27122655e-05
Iter: 933 loss: 2.27105011e-05
Iter: 934 loss: 2.26987249e-05
Iter: 935 loss: 2.27533674e-05
Iter: 936 loss: 2.26965258e-05
Iter: 937 loss: 2.26856901e-05
Iter: 938 loss: 2.2698021e-05
Iter: 939 loss: 2.26799621e-05
Iter: 940 loss: 2.26691827e-05
Iter: 941 loss: 2.2670094e-05
Iter: 942 loss: 2.26608936e-05
Iter: 943 loss: 2.26488974e-05
Iter: 944 loss: 2.28269728e-05
Iter: 945 loss: 2.26488955e-05
Iter: 946 loss: 2.2638098e-05
Iter: 947 loss: 2.26480115e-05
Iter: 948 loss: 2.26319426e-05
Iter: 949 loss: 2.26213979e-05
Iter: 950 loss: 2.26205739e-05
Iter: 951 loss: 2.2612654e-05
Iter: 952 loss: 2.26014563e-05
Iter: 953 loss: 2.27637429e-05
Iter: 954 loss: 2.26014745e-05
Iter: 955 loss: 2.25936546e-05
Iter: 956 loss: 2.2615226e-05
Iter: 957 loss: 2.25911826e-05
Iter: 958 loss: 2.25820113e-05
Iter: 959 loss: 2.26378725e-05
Iter: 960 loss: 2.25809035e-05
Iter: 961 loss: 2.25740987e-05
Iter: 962 loss: 2.25607655e-05
Iter: 963 loss: 2.28317367e-05
Iter: 964 loss: 2.25607073e-05
Iter: 965 loss: 2.25480326e-05
Iter: 966 loss: 2.26377233e-05
Iter: 967 loss: 2.2546963e-05
Iter: 968 loss: 2.25344447e-05
Iter: 969 loss: 2.26104676e-05
Iter: 970 loss: 2.25329059e-05
Iter: 971 loss: 2.25259e-05
Iter: 972 loss: 2.251775e-05
Iter: 973 loss: 2.25168405e-05
Iter: 974 loss: 2.25042095e-05
Iter: 975 loss: 2.25315889e-05
Iter: 976 loss: 2.24992073e-05
Iter: 977 loss: 2.24889645e-05
Iter: 978 loss: 2.24890337e-05
Iter: 979 loss: 2.24811083e-05
Iter: 980 loss: 2.24752257e-05
Iter: 981 loss: 2.24726791e-05
Iter: 982 loss: 2.24614487e-05
Iter: 983 loss: 2.25557051e-05
Iter: 984 loss: 2.24608812e-05
Iter: 985 loss: 2.24515716e-05
Iter: 986 loss: 2.24572468e-05
Iter: 987 loss: 2.24457253e-05
Iter: 988 loss: 2.2435388e-05
Iter: 989 loss: 2.24267787e-05
Iter: 990 loss: 2.24240048e-05
Iter: 991 loss: 2.24272153e-05
Iter: 992 loss: 2.24178184e-05
Iter: 993 loss: 2.24122741e-05
Iter: 994 loss: 2.23997122e-05
Iter: 995 loss: 2.25691947e-05
Iter: 996 loss: 2.239899e-05
Iter: 997 loss: 2.23874486e-05
Iter: 998 loss: 2.24187297e-05
Iter: 999 loss: 2.23837087e-05
Iter: 1000 loss: 2.23738898e-05
Iter: 1001 loss: 2.25128315e-05
Iter: 1002 loss: 2.23737516e-05
Iter: 1003 loss: 2.23660718e-05
Iter: 1004 loss: 2.23670795e-05
Iter: 1005 loss: 2.23603129e-05
Iter: 1006 loss: 2.23502393e-05
Iter: 1007 loss: 2.2393313e-05
Iter: 1008 loss: 2.23481402e-05
Iter: 1009 loss: 2.2339369e-05
Iter: 1010 loss: 2.23340685e-05
Iter: 1011 loss: 2.23304487e-05
Iter: 1012 loss: 2.23197039e-05
Iter: 1013 loss: 2.24084488e-05
Iter: 1014 loss: 2.23191237e-05
Iter: 1015 loss: 2.23092247e-05
Iter: 1016 loss: 2.23570678e-05
Iter: 1017 loss: 2.23075021e-05
Iter: 1018 loss: 2.23000825e-05
Iter: 1019 loss: 2.2295797e-05
Iter: 1020 loss: 2.2292621e-05
Iter: 1021 loss: 2.22809085e-05
Iter: 1022 loss: 2.2375043e-05
Iter: 1023 loss: 2.22802191e-05
Iter: 1024 loss: 2.22718281e-05
Iter: 1025 loss: 2.22698618e-05
Iter: 1026 loss: 2.22646777e-05
Iter: 1027 loss: 2.22589024e-05
Iter: 1028 loss: 2.22578801e-05
Iter: 1029 loss: 2.22517647e-05
Iter: 1030 loss: 2.22409253e-05
Iter: 1031 loss: 2.22408562e-05
Iter: 1032 loss: 2.22311755e-05
Iter: 1033 loss: 2.22300259e-05
Iter: 1034 loss: 2.22229755e-05
Iter: 1035 loss: 2.22160961e-05
Iter: 1036 loss: 2.22146173e-05
Iter: 1037 loss: 2.22083836e-05
Iter: 1038 loss: 2.22013459e-05
Iter: 1039 loss: 2.22004455e-05
Iter: 1040 loss: 2.21902446e-05
Iter: 1041 loss: 2.22385715e-05
Iter: 1042 loss: 2.21883383e-05
Iter: 1043 loss: 2.21787413e-05
Iter: 1044 loss: 2.21928767e-05
Iter: 1045 loss: 2.21740884e-05
Iter: 1046 loss: 2.21645241e-05
Iter: 1047 loss: 2.21751143e-05
Iter: 1048 loss: 2.21593782e-05
Iter: 1049 loss: 2.21460741e-05
Iter: 1050 loss: 2.22445124e-05
Iter: 1051 loss: 2.21449645e-05
Iter: 1052 loss: 2.2137996e-05
Iter: 1053 loss: 2.21368973e-05
Iter: 1054 loss: 2.21319915e-05
Iter: 1055 loss: 2.21223527e-05
Iter: 1056 loss: 2.21824375e-05
Iter: 1057 loss: 2.21212395e-05
Iter: 1058 loss: 2.21112823e-05
Iter: 1059 loss: 2.21150931e-05
Iter: 1060 loss: 2.21043265e-05
Iter: 1061 loss: 2.20981565e-05
Iter: 1062 loss: 2.20971524e-05
Iter: 1063 loss: 2.20924703e-05
Iter: 1064 loss: 2.20803395e-05
Iter: 1065 loss: 2.21753508e-05
Iter: 1066 loss: 2.20780566e-05
Iter: 1067 loss: 2.2066235e-05
Iter: 1068 loss: 2.21243463e-05
Iter: 1069 loss: 2.20643142e-05
Iter: 1070 loss: 2.2057171e-05
Iter: 1071 loss: 2.20570855e-05
Iter: 1072 loss: 2.20512356e-05
Iter: 1073 loss: 2.20390175e-05
Iter: 1074 loss: 2.22520612e-05
Iter: 1075 loss: 2.20387483e-05
Iter: 1076 loss: 2.20285019e-05
Iter: 1077 loss: 2.21652936e-05
Iter: 1078 loss: 2.20283509e-05
Iter: 1079 loss: 2.20194561e-05
Iter: 1080 loss: 2.20257843e-05
Iter: 1081 loss: 2.20138354e-05
Iter: 1082 loss: 2.20041184e-05
Iter: 1083 loss: 2.20382371e-05
Iter: 1084 loss: 2.20015645e-05
Iter: 1085 loss: 2.1991018e-05
Iter: 1086 loss: 2.20523543e-05
Iter: 1087 loss: 2.19896283e-05
Iter: 1088 loss: 2.19808644e-05
Iter: 1089 loss: 2.19739723e-05
Iter: 1090 loss: 2.19712729e-05
Iter: 1091 loss: 2.19629292e-05
Iter: 1092 loss: 2.19629947e-05
Iter: 1093 loss: 2.19555022e-05
Iter: 1094 loss: 2.19657213e-05
Iter: 1095 loss: 2.1951837e-05
Iter: 1096 loss: 2.19429712e-05
Iter: 1097 loss: 2.20085767e-05
Iter: 1098 loss: 2.19422436e-05
Iter: 1099 loss: 2.19368703e-05
Iter: 1100 loss: 2.19256217e-05
Iter: 1101 loss: 2.21083501e-05
Iter: 1102 loss: 2.19252197e-05
Iter: 1103 loss: 2.19139565e-05
Iter: 1104 loss: 2.20101374e-05
Iter: 1105 loss: 2.19132125e-05
Iter: 1106 loss: 2.19022259e-05
Iter: 1107 loss: 2.19689955e-05
Iter: 1108 loss: 2.19008216e-05
Iter: 1109 loss: 2.18938731e-05
Iter: 1110 loss: 2.18809728e-05
Iter: 1111 loss: 2.21846458e-05
Iter: 1112 loss: 2.18809691e-05
Iter: 1113 loss: 2.18700443e-05
Iter: 1114 loss: 2.18699643e-05
Iter: 1115 loss: 2.18616333e-05
Iter: 1116 loss: 2.18603091e-05
Iter: 1117 loss: 2.1854481e-05
Iter: 1118 loss: 2.184408e-05
Iter: 1119 loss: 2.18765035e-05
Iter: 1120 loss: 2.18410332e-05
Iter: 1121 loss: 2.18316036e-05
Iter: 1122 loss: 2.19687918e-05
Iter: 1123 loss: 2.18316509e-05
Iter: 1124 loss: 2.1825701e-05
Iter: 1125 loss: 2.18171062e-05
Iter: 1126 loss: 2.18168061e-05
Iter: 1127 loss: 2.18090881e-05
Iter: 1128 loss: 2.19202593e-05
Iter: 1129 loss: 2.18091045e-05
Iter: 1130 loss: 2.18026835e-05
Iter: 1131 loss: 2.18320529e-05
Iter: 1132 loss: 2.18014065e-05
Iter: 1133 loss: 2.17949055e-05
Iter: 1134 loss: 2.1799442e-05
Iter: 1135 loss: 2.17908382e-05
Iter: 1136 loss: 2.17829638e-05
Iter: 1137 loss: 2.1774651e-05
Iter: 1138 loss: 2.17732741e-05
Iter: 1139 loss: 2.17645775e-05
Iter: 1140 loss: 2.1849206e-05
Iter: 1141 loss: 2.17642882e-05
Iter: 1142 loss: 2.17550114e-05
Iter: 1143 loss: 2.17803426e-05
Iter: 1144 loss: 2.17519955e-05
Iter: 1145 loss: 2.17451543e-05
Iter: 1146 loss: 2.17397337e-05
Iter: 1147 loss: 2.17376146e-05
Iter: 1148 loss: 2.17280522e-05
Iter: 1149 loss: 2.17899615e-05
Iter: 1150 loss: 2.17270735e-05
Iter: 1151 loss: 2.17172965e-05
Iter: 1152 loss: 2.17389825e-05
Iter: 1153 loss: 2.17135457e-05
Iter: 1154 loss: 2.17053221e-05
Iter: 1155 loss: 2.17118195e-05
Iter: 1156 loss: 2.17003835e-05
Iter: 1157 loss: 2.16903136e-05
Iter: 1158 loss: 2.18072528e-05
Iter: 1159 loss: 2.16901826e-05
Iter: 1160 loss: 2.16836233e-05
Iter: 1161 loss: 2.16764838e-05
Iter: 1162 loss: 2.16755034e-05
Iter: 1163 loss: 2.16647859e-05
Iter: 1164 loss: 2.17278266e-05
Iter: 1165 loss: 2.16634e-05
Iter: 1166 loss: 2.16545559e-05
Iter: 1167 loss: 2.17666129e-05
Iter: 1168 loss: 2.16544649e-05
Iter: 1169 loss: 2.16496082e-05
Iter: 1170 loss: 2.16529097e-05
Iter: 1171 loss: 2.16466196e-05
Iter: 1172 loss: 2.16399112e-05
Iter: 1173 loss: 2.16307017e-05
Iter: 1174 loss: 2.16302105e-05
Iter: 1175 loss: 2.16193257e-05
Iter: 1176 loss: 2.16544067e-05
Iter: 1177 loss: 2.16162334e-05
Iter: 1178 loss: 2.16097342e-05
Iter: 1179 loss: 2.16095686e-05
Iter: 1180 loss: 2.16034314e-05
Iter: 1181 loss: 2.15939926e-05
Iter: 1182 loss: 2.1593798e-05
Iter: 1183 loss: 2.15852797e-05
Iter: 1184 loss: 2.16246917e-05
Iter: 1185 loss: 2.15837172e-05
Iter: 1186 loss: 2.15749315e-05
Iter: 1187 loss: 2.16129665e-05
Iter: 1188 loss: 2.15731561e-05
Iter: 1189 loss: 2.15652435e-05
Iter: 1190 loss: 2.15713044e-05
Iter: 1191 loss: 2.15603541e-05
Iter: 1192 loss: 2.15520631e-05
Iter: 1193 loss: 2.16351509e-05
Iter: 1194 loss: 2.15518412e-05
Iter: 1195 loss: 2.1544749e-05
Iter: 1196 loss: 2.15400669e-05
Iter: 1197 loss: 2.15373911e-05
Iter: 1198 loss: 2.15306227e-05
Iter: 1199 loss: 2.15868204e-05
Iter: 1200 loss: 2.15302425e-05
Iter: 1201 loss: 2.15230084e-05
Iter: 1202 loss: 2.15548862e-05
Iter: 1203 loss: 2.15215332e-05
Iter: 1204 loss: 2.15164218e-05
Iter: 1205 loss: 2.15185028e-05
Iter: 1206 loss: 2.15129112e-05
Iter: 1207 loss: 2.15058062e-05
Iter: 1208 loss: 2.1508582e-05
Iter: 1209 loss: 2.1500884e-05
Iter: 1210 loss: 2.14913507e-05
Iter: 1211 loss: 2.14910506e-05
Iter: 1212 loss: 2.14836928e-05
Iter: 1213 loss: 2.14780666e-05
Iter: 1214 loss: 2.14770607e-05
Iter: 1215 loss: 2.14706561e-05
Iter: 1216 loss: 2.1463311e-05
Iter: 1217 loss: 2.14624815e-05
Iter: 1218 loss: 2.14537286e-05
Iter: 1219 loss: 2.1472335e-05
Iter: 1220 loss: 2.14502288e-05
Iter: 1221 loss: 2.14426509e-05
Iter: 1222 loss: 2.15085856e-05
Iter: 1223 loss: 2.1442298e-05
Iter: 1224 loss: 2.14351239e-05
Iter: 1225 loss: 2.14409538e-05
Iter: 1226 loss: 2.14307365e-05
Iter: 1227 loss: 2.14246065e-05
Iter: 1228 loss: 2.14831434e-05
Iter: 1229 loss: 2.14242482e-05
Iter: 1230 loss: 2.14184365e-05
Iter: 1231 loss: 2.14103893e-05
Iter: 1232 loss: 2.140992e-05
Iter: 1233 loss: 2.14023894e-05
Iter: 1234 loss: 2.14566589e-05
Iter: 1235 loss: 2.14017982e-05
Iter: 1236 loss: 2.13954627e-05
Iter: 1237 loss: 2.14774154e-05
Iter: 1238 loss: 2.139547e-05
Iter: 1239 loss: 2.13911972e-05
Iter: 1240 loss: 2.13838157e-05
Iter: 1241 loss: 2.13837684e-05
Iter: 1242 loss: 2.13765306e-05
Iter: 1243 loss: 2.14382126e-05
Iter: 1244 loss: 2.13760904e-05
Iter: 1245 loss: 2.13697313e-05
Iter: 1246 loss: 2.13591447e-05
Iter: 1247 loss: 2.13591065e-05
Iter: 1248 loss: 2.1350419e-05
Iter: 1249 loss: 2.14491283e-05
Iter: 1250 loss: 2.1350359e-05
Iter: 1251 loss: 2.13426865e-05
Iter: 1252 loss: 2.1399137e-05
Iter: 1253 loss: 2.13420753e-05
Iter: 1254 loss: 2.13365875e-05
Iter: 1255 loss: 2.13304902e-05
Iter: 1256 loss: 2.13297099e-05
Iter: 1257 loss: 2.13223357e-05
Iter: 1258 loss: 2.13498715e-05
Iter: 1259 loss: 2.13205385e-05
Iter: 1260 loss: 2.13124549e-05
Iter: 1261 loss: 2.13599087e-05
Iter: 1262 loss: 2.13113653e-05
Iter: 1263 loss: 2.13057028e-05
Iter: 1264 loss: 2.13180574e-05
Iter: 1265 loss: 2.13034909e-05
Iter: 1266 loss: 2.12968062e-05
Iter: 1267 loss: 2.13075928e-05
Iter: 1268 loss: 2.12936066e-05
Iter: 1269 loss: 2.12861742e-05
Iter: 1270 loss: 2.12855157e-05
Iter: 1271 loss: 2.12799241e-05
Iter: 1272 loss: 2.12721152e-05
Iter: 1273 loss: 2.13257463e-05
Iter: 1274 loss: 2.12712421e-05
Iter: 1275 loss: 2.12624909e-05
Iter: 1276 loss: 2.13164894e-05
Iter: 1277 loss: 2.12613941e-05
Iter: 1278 loss: 2.12576306e-05
Iter: 1279 loss: 2.12545674e-05
Iter: 1280 loss: 2.12534032e-05
Iter: 1281 loss: 2.12462928e-05
Iter: 1282 loss: 2.12633931e-05
Iter: 1283 loss: 2.12436444e-05
Iter: 1284 loss: 2.12361574e-05
Iter: 1285 loss: 2.12408777e-05
Iter: 1286 loss: 2.12313644e-05
Iter: 1287 loss: 2.12231207e-05
Iter: 1288 loss: 2.12228806e-05
Iter: 1289 loss: 2.12165196e-05
Iter: 1290 loss: 2.12153682e-05
Iter: 1291 loss: 2.12114319e-05
Iter: 1292 loss: 2.12072027e-05
Iter: 1293 loss: 2.11987099e-05
Iter: 1294 loss: 2.13607964e-05
Iter: 1295 loss: 2.11985862e-05
Iter: 1296 loss: 2.119079e-05
Iter: 1297 loss: 2.12491868e-05
Iter: 1298 loss: 2.1190137e-05
Iter: 1299 loss: 2.11836705e-05
Iter: 1300 loss: 2.12238356e-05
Iter: 1301 loss: 2.11829301e-05
Iter: 1302 loss: 2.11774732e-05
Iter: 1303 loss: 2.11692441e-05
Iter: 1304 loss: 2.11690876e-05
Iter: 1305 loss: 2.11602346e-05
Iter: 1306 loss: 2.11822571e-05
Iter: 1307 loss: 2.11571751e-05
Iter: 1308 loss: 2.11530605e-05
Iter: 1309 loss: 2.11515417e-05
Iter: 1310 loss: 2.11476145e-05
Iter: 1311 loss: 2.11405513e-05
Iter: 1312 loss: 2.11405677e-05
Iter: 1313 loss: 2.1134163e-05
Iter: 1314 loss: 2.11574352e-05
Iter: 1315 loss: 2.11324805e-05
Iter: 1316 loss: 2.11243623e-05
Iter: 1317 loss: 2.11324314e-05
Iter: 1318 loss: 2.11199258e-05
Iter: 1319 loss: 2.11126899e-05
Iter: 1320 loss: 2.11175247e-05
Iter: 1321 loss: 2.11082952e-05
Iter: 1322 loss: 2.11003135e-05
Iter: 1323 loss: 2.12026116e-05
Iter: 1324 loss: 2.1100197e-05
Iter: 1325 loss: 2.10946564e-05
Iter: 1326 loss: 2.11123788e-05
Iter: 1327 loss: 2.10930302e-05
Iter: 1328 loss: 2.10875605e-05
Iter: 1329 loss: 2.11030456e-05
Iter: 1330 loss: 2.10857652e-05
Iter: 1331 loss: 2.1080119e-05
Iter: 1332 loss: 2.10747894e-05
Iter: 1333 loss: 2.1073407e-05
Iter: 1334 loss: 2.10677281e-05
Iter: 1335 loss: 2.10676808e-05
Iter: 1336 loss: 2.10623239e-05
Iter: 1337 loss: 2.10542548e-05
Iter: 1338 loss: 2.10541293e-05
Iter: 1339 loss: 2.10459402e-05
Iter: 1340 loss: 2.10757262e-05
Iter: 1341 loss: 2.10438629e-05
Iter: 1342 loss: 2.10368416e-05
Iter: 1343 loss: 2.10716789e-05
Iter: 1344 loss: 2.10357e-05
Iter: 1345 loss: 2.10285016e-05
Iter: 1346 loss: 2.11010974e-05
Iter: 1347 loss: 2.10282215e-05
Iter: 1348 loss: 2.10249982e-05
Iter: 1349 loss: 2.10170147e-05
Iter: 1350 loss: 2.10885719e-05
Iter: 1351 loss: 2.1015876e-05
Iter: 1352 loss: 2.10071066e-05
Iter: 1353 loss: 2.10872313e-05
Iter: 1354 loss: 2.10067919e-05
Iter: 1355 loss: 2.09994778e-05
Iter: 1356 loss: 2.10257076e-05
Iter: 1357 loss: 2.09976461e-05
Iter: 1358 loss: 2.09917143e-05
Iter: 1359 loss: 2.09931532e-05
Iter: 1360 loss: 2.09873888e-05
Iter: 1361 loss: 2.09809696e-05
Iter: 1362 loss: 2.10751714e-05
Iter: 1363 loss: 2.09810059e-05
Iter: 1364 loss: 2.09762038e-05
Iter: 1365 loss: 2.09824284e-05
Iter: 1366 loss: 2.09737664e-05
Iter: 1367 loss: 2.09673e-05
Iter: 1368 loss: 2.09755326e-05
Iter: 1369 loss: 2.0963831e-05
Iter: 1370 loss: 2.09582158e-05
Iter: 1371 loss: 2.09714926e-05
Iter: 1372 loss: 2.09561767e-05
Iter: 1373 loss: 2.09497666e-05
Iter: 1374 loss: 2.09785394e-05
Iter: 1375 loss: 2.09483896e-05
Iter: 1376 loss: 2.09427744e-05
Iter: 1377 loss: 2.09364298e-05
Iter: 1378 loss: 2.09355312e-05
Iter: 1379 loss: 2.09278769e-05
Iter: 1380 loss: 2.09399623e-05
Iter: 1381 loss: 2.09242698e-05
Iter: 1382 loss: 2.09182399e-05
Iter: 1383 loss: 2.09182035e-05
Iter: 1384 loss: 2.09129739e-05
Iter: 1385 loss: 2.0955842e-05
Iter: 1386 loss: 2.09126119e-05
Iter: 1387 loss: 2.09095888e-05
Iter: 1388 loss: 2.09021382e-05
Iter: 1389 loss: 2.09792815e-05
Iter: 1390 loss: 2.09012214e-05
Iter: 1391 loss: 2.08946403e-05
Iter: 1392 loss: 2.09849168e-05
Iter: 1393 loss: 2.08946931e-05
Iter: 1394 loss: 2.08886086e-05
Iter: 1395 loss: 2.08872516e-05
Iter: 1396 loss: 2.08834208e-05
Iter: 1397 loss: 2.0876083e-05
Iter: 1398 loss: 2.09093669e-05
Iter: 1399 loss: 2.0874515e-05
Iter: 1400 loss: 2.08690508e-05
Iter: 1401 loss: 2.09168775e-05
Iter: 1402 loss: 2.08686506e-05
Iter: 1403 loss: 2.08636702e-05
Iter: 1404 loss: 2.08704478e-05
Iter: 1405 loss: 2.08610691e-05
Iter: 1406 loss: 2.08547899e-05
Iter: 1407 loss: 2.087744e-05
Iter: 1408 loss: 2.08531892e-05
Iter: 1409 loss: 2.08493893e-05
Iter: 1410 loss: 2.08539932e-05
Iter: 1411 loss: 2.08474485e-05
Iter: 1412 loss: 2.08415622e-05
Iter: 1413 loss: 2.0848307e-05
Iter: 1414 loss: 2.08383572e-05
Iter: 1415 loss: 2.08322817e-05
Iter: 1416 loss: 2.08344172e-05
Iter: 1417 loss: 2.08280617e-05
Iter: 1418 loss: 2.08207366e-05
Iter: 1419 loss: 2.08236488e-05
Iter: 1420 loss: 2.08157217e-05
Iter: 1421 loss: 2.08108249e-05
Iter: 1422 loss: 2.08106067e-05
Iter: 1423 loss: 2.08045076e-05
Iter: 1424 loss: 2.08144666e-05
Iter: 1425 loss: 2.08017809e-05
Iter: 1426 loss: 2.0797137e-05
Iter: 1427 loss: 2.07944577e-05
Iter: 1428 loss: 2.0792515e-05
Iter: 1429 loss: 2.07866833e-05
Iter: 1430 loss: 2.08233123e-05
Iter: 1431 loss: 2.07859684e-05
Iter: 1432 loss: 2.07801277e-05
Iter: 1433 loss: 2.07838857e-05
Iter: 1434 loss: 2.0776386e-05
Iter: 1435 loss: 2.07701269e-05
Iter: 1436 loss: 2.07830744e-05
Iter: 1437 loss: 2.07676039e-05
Iter: 1438 loss: 2.07607e-05
Iter: 1439 loss: 2.08188831e-05
Iter: 1440 loss: 2.07601952e-05
Iter: 1441 loss: 2.07555131e-05
Iter: 1442 loss: 2.07760695e-05
Iter: 1443 loss: 2.07544199e-05
Iter: 1444 loss: 2.07496905e-05
Iter: 1445 loss: 2.07500798e-05
Iter: 1446 loss: 2.0745998e-05
Iter: 1447 loss: 2.07397206e-05
Iter: 1448 loss: 2.07562261e-05
Iter: 1449 loss: 2.07377343e-05
Iter: 1450 loss: 2.0730844e-05
Iter: 1451 loss: 2.07545454e-05
Iter: 1452 loss: 2.07290868e-05
Iter: 1453 loss: 2.07239718e-05
Iter: 1454 loss: 2.0714604e-05
Iter: 1455 loss: 2.09339851e-05
Iter: 1456 loss: 2.07146568e-05
Iter: 1457 loss: 2.07055182e-05
Iter: 1458 loss: 2.07681987e-05
Iter: 1459 loss: 2.07046614e-05
Iter: 1460 loss: 2.06966761e-05
Iter: 1461 loss: 2.07151934e-05
Iter: 1462 loss: 2.06937129e-05
Iter: 1463 loss: 2.06936238e-05
Iter: 1464 loss: 2.06904842e-05
Iter: 1465 loss: 2.0687643e-05
Iter: 1466 loss: 2.06809273e-05
Iter: 1467 loss: 2.07604244e-05
Iter: 1468 loss: 2.06803597e-05
Iter: 1469 loss: 2.06741734e-05
Iter: 1470 loss: 2.07400626e-05
Iter: 1471 loss: 2.06739278e-05
Iter: 1472 loss: 2.0669575e-05
Iter: 1473 loss: 2.06682962e-05
Iter: 1474 loss: 2.06654859e-05
Iter: 1475 loss: 2.06581972e-05
Iter: 1476 loss: 2.06669938e-05
Iter: 1477 loss: 2.06543027e-05
Iter: 1478 loss: 2.06484783e-05
Iter: 1479 loss: 2.0648491e-05
Iter: 1480 loss: 2.06438563e-05
Iter: 1481 loss: 2.06539717e-05
Iter: 1482 loss: 2.06420445e-05
Iter: 1483 loss: 2.06364402e-05
Iter: 1484 loss: 2.06456371e-05
Iter: 1485 loss: 2.06339064e-05
Iter: 1486 loss: 2.0629428e-05
Iter: 1487 loss: 2.06266632e-05
Iter: 1488 loss: 2.06249151e-05
Iter: 1489 loss: 2.06182231e-05
Iter: 1490 loss: 2.06801578e-05
Iter: 1491 loss: 2.06178629e-05
Iter: 1492 loss: 2.06118093e-05
Iter: 1493 loss: 2.06144214e-05
Iter: 1494 loss: 2.06075638e-05
Iter: 1495 loss: 2.0601432e-05
Iter: 1496 loss: 2.05968245e-05
Iter: 1497 loss: 2.05946235e-05
Iter: 1498 loss: 2.05924862e-05
Iter: 1499 loss: 2.05905053e-05
Iter: 1500 loss: 2.05857468e-05
Iter: 1501 loss: 2.05861797e-05
Iter: 1502 loss: 2.05823108e-05
Iter: 1503 loss: 2.05775723e-05
Iter: 1504 loss: 2.05702272e-05
Iter: 1505 loss: 2.05701253e-05
Iter: 1506 loss: 2.05623601e-05
Iter: 1507 loss: 2.05961187e-05
Iter: 1508 loss: 2.05608449e-05
Iter: 1509 loss: 2.05540164e-05
Iter: 1510 loss: 2.06250188e-05
Iter: 1511 loss: 2.05537399e-05
Iter: 1512 loss: 2.05486904e-05
Iter: 1513 loss: 2.05600882e-05
Iter: 1514 loss: 2.05467841e-05
Iter: 1515 loss: 2.05420438e-05
Iter: 1516 loss: 2.0585052e-05
Iter: 1517 loss: 2.05418291e-05
Iter: 1518 loss: 2.05377783e-05
Iter: 1519 loss: 2.05300166e-05
Iter: 1520 loss: 2.06933746e-05
Iter: 1521 loss: 2.05298747e-05
Iter: 1522 loss: 2.05223678e-05
Iter: 1523 loss: 2.05528631e-05
Iter: 1524 loss: 2.05206816e-05
Iter: 1525 loss: 2.05134056e-05
Iter: 1526 loss: 2.05641409e-05
Iter: 1527 loss: 2.05128199e-05
Iter: 1528 loss: 2.05078395e-05
Iter: 1529 loss: 2.05139495e-05
Iter: 1530 loss: 2.05052529e-05
Iter: 1531 loss: 2.04994e-05
Iter: 1532 loss: 2.05260221e-05
Iter: 1533 loss: 2.04983426e-05
Iter: 1534 loss: 2.04930147e-05
Iter: 1535 loss: 2.04977405e-05
Iter: 1536 loss: 2.04899534e-05
Iter: 1537 loss: 2.04860989e-05
Iter: 1538 loss: 2.04858152e-05
Iter: 1539 loss: 2.04832468e-05
Iter: 1540 loss: 2.04760527e-05
Iter: 1541 loss: 2.05029537e-05
Iter: 1542 loss: 2.04728713e-05
Iter: 1543 loss: 2.04648313e-05
Iter: 1544 loss: 2.05271172e-05
Iter: 1545 loss: 2.04642547e-05
Iter: 1546 loss: 2.04575917e-05
Iter: 1547 loss: 2.04532989e-05
Iter: 1548 loss: 2.04506141e-05
Iter: 1549 loss: 2.04459939e-05
Iter: 1550 loss: 2.0445299e-05
Iter: 1551 loss: 2.04398311e-05
Iter: 1552 loss: 2.04519583e-05
Iter: 1553 loss: 2.04376302e-05
Iter: 1554 loss: 2.04329481e-05
Iter: 1555 loss: 2.04420685e-05
Iter: 1556 loss: 2.04310127e-05
Iter: 1557 loss: 2.04259668e-05
Iter: 1558 loss: 2.04346652e-05
Iter: 1559 loss: 2.04237294e-05
Iter: 1560 loss: 2.04182979e-05
Iter: 1561 loss: 2.04147291e-05
Iter: 1562 loss: 2.041263e-05
Iter: 1563 loss: 2.04060816e-05
Iter: 1564 loss: 2.04719436e-05
Iter: 1565 loss: 2.04058797e-05
Iter: 1566 loss: 2.03995533e-05
Iter: 1567 loss: 2.04151856e-05
Iter: 1568 loss: 2.0397445e-05
Iter: 1569 loss: 2.03927339e-05
Iter: 1570 loss: 2.04446533e-05
Iter: 1571 loss: 2.03926447e-05
Iter: 1572 loss: 2.03889176e-05
Iter: 1573 loss: 2.03948293e-05
Iter: 1574 loss: 2.03870277e-05
Iter: 1575 loss: 2.03832333e-05
Iter: 1576 loss: 2.03854324e-05
Iter: 1577 loss: 2.03808486e-05
Iter: 1578 loss: 2.03750024e-05
Iter: 1579 loss: 2.03732416e-05
Iter: 1580 loss: 2.03696763e-05
Iter: 1581 loss: 2.03632426e-05
Iter: 1582 loss: 2.03655036e-05
Iter: 1583 loss: 2.03586897e-05
Iter: 1584 loss: 2.03504296e-05
Iter: 1585 loss: 2.03950076e-05
Iter: 1586 loss: 2.03491727e-05
Iter: 1587 loss: 2.03431155e-05
Iter: 1588 loss: 2.03664404e-05
Iter: 1589 loss: 2.03415948e-05
Iter: 1590 loss: 2.03354903e-05
Iter: 1591 loss: 2.04157732e-05
Iter: 1592 loss: 2.03355303e-05
Iter: 1593 loss: 2.0332147e-05
Iter: 1594 loss: 2.03319723e-05
Iter: 1595 loss: 2.03294148e-05
Iter: 1596 loss: 2.0323434e-05
Iter: 1597 loss: 2.03241598e-05
Iter: 1598 loss: 2.03188501e-05
Iter: 1599 loss: 2.03128984e-05
Iter: 1600 loss: 2.03285799e-05
Iter: 1601 loss: 2.03108139e-05
Iter: 1602 loss: 2.03050531e-05
Iter: 1603 loss: 2.0318832e-05
Iter: 1604 loss: 2.03028649e-05
Iter: 1605 loss: 2.02965075e-05
Iter: 1606 loss: 2.03469208e-05
Iter: 1607 loss: 2.02960618e-05
Iter: 1608 loss: 2.02917763e-05
Iter: 1609 loss: 2.02959818e-05
Iter: 1610 loss: 2.0289317e-05
Iter: 1611 loss: 2.02835945e-05
Iter: 1612 loss: 2.03273939e-05
Iter: 1613 loss: 2.02831143e-05
Iter: 1614 loss: 2.02793035e-05
Iter: 1615 loss: 2.02914071e-05
Iter: 1616 loss: 2.02781594e-05
Iter: 1617 loss: 2.02744122e-05
Iter: 1618 loss: 2.02686497e-05
Iter: 1619 loss: 2.02685897e-05
Iter: 1620 loss: 2.02613919e-05
Iter: 1621 loss: 2.02732281e-05
Iter: 1622 loss: 2.02581432e-05
Iter: 1623 loss: 2.02510655e-05
Iter: 1624 loss: 2.02571282e-05
Iter: 1625 loss: 2.02469564e-05
Iter: 1626 loss: 2.02431092e-05
Iter: 1627 loss: 2.02421543e-05
Iter: 1628 loss: 2.02376395e-05
Iter: 1629 loss: 2.02408701e-05
Iter: 1630 loss: 2.02348347e-05
Iter: 1631 loss: 2.02297924e-05
Iter: 1632 loss: 2.02354386e-05
Iter: 1633 loss: 2.02271767e-05
Iter: 1634 loss: 2.02203446e-05
Iter: 1635 loss: 2.02293904e-05
Iter: 1636 loss: 2.02169213e-05
Iter: 1637 loss: 2.02115752e-05
Iter: 1638 loss: 2.02134e-05
Iter: 1639 loss: 2.02078954e-05
Iter: 1640 loss: 2.02011925e-05
Iter: 1641 loss: 2.02750907e-05
Iter: 1642 loss: 2.02009578e-05
Iter: 1643 loss: 2.01961011e-05
Iter: 1644 loss: 2.02089013e-05
Iter: 1645 loss: 2.01945222e-05
Iter: 1646 loss: 2.0190444e-05
Iter: 1647 loss: 2.02274641e-05
Iter: 1648 loss: 2.01902312e-05
Iter: 1649 loss: 2.01865223e-05
Iter: 1650 loss: 2.01867115e-05
Iter: 1651 loss: 2.01835319e-05
Iter: 1652 loss: 2.0178757e-05
Iter: 1653 loss: 2.01937582e-05
Iter: 1654 loss: 2.01774274e-05
Iter: 1655 loss: 2.0173251e-05
Iter: 1656 loss: 2.01666862e-05
Iter: 1657 loss: 2.01666153e-05
Iter: 1658 loss: 2.01590519e-05
Iter: 1659 loss: 2.01898e-05
Iter: 1660 loss: 2.01572984e-05
Iter: 1661 loss: 2.01500807e-05
Iter: 1662 loss: 2.01603052e-05
Iter: 1663 loss: 2.0146399e-05
Iter: 1664 loss: 2.01452167e-05
Iter: 1665 loss: 2.01428957e-05
Iter: 1666 loss: 2.01395924e-05
Iter: 1667 loss: 2.01345774e-05
Iter: 1668 loss: 2.0134521e-05
Iter: 1669 loss: 2.01286148e-05
Iter: 1670 loss: 2.01602707e-05
Iter: 1671 loss: 2.01276744e-05
Iter: 1672 loss: 2.01224975e-05
Iter: 1673 loss: 2.01240891e-05
Iter: 1674 loss: 2.01187e-05
Iter: 1675 loss: 2.0112504e-05
Iter: 1676 loss: 2.01194198e-05
Iter: 1677 loss: 2.01090479e-05
Iter: 1678 loss: 2.01043895e-05
Iter: 1679 loss: 2.01044422e-05
Iter: 1680 loss: 2.01003531e-05
Iter: 1681 loss: 2.00974318e-05
Iter: 1682 loss: 2.00960585e-05
Iter: 1683 loss: 2.00915201e-05
Iter: 1684 loss: 2.00913855e-05
Iter: 1685 loss: 2.0088466e-05
Iter: 1686 loss: 2.00892573e-05
Iter: 1687 loss: 2.00863651e-05
Iter: 1688 loss: 2.00821e-05
Iter: 1689 loss: 2.00816285e-05
Iter: 1690 loss: 2.00786126e-05
Iter: 1691 loss: 2.00725499e-05
Iter: 1692 loss: 2.00763934e-05
Iter: 1693 loss: 2.00687482e-05
Iter: 1694 loss: 2.00619761e-05
Iter: 1695 loss: 2.00629238e-05
Iter: 1696 loss: 2.00568538e-05
Iter: 1697 loss: 2.0049456e-05
Iter: 1698 loss: 2.01244475e-05
Iter: 1699 loss: 2.00491486e-05
Iter: 1700 loss: 2.00445484e-05
Iter: 1701 loss: 2.00444938e-05
Iter: 1702 loss: 2.00408285e-05
Iter: 1703 loss: 2.0035166e-05
Iter: 1704 loss: 2.00350405e-05
Iter: 1705 loss: 2.00300474e-05
Iter: 1706 loss: 2.00831673e-05
Iter: 1707 loss: 2.00299583e-05
Iter: 1708 loss: 2.002568e-05
Iter: 1709 loss: 2.00194936e-05
Iter: 1710 loss: 2.00192844e-05
Iter: 1711 loss: 2.00131e-05
Iter: 1712 loss: 2.0055908e-05
Iter: 1713 loss: 2.00125651e-05
Iter: 1714 loss: 2.00067097e-05
Iter: 1715 loss: 2.00377108e-05
Iter: 1716 loss: 2.00058748e-05
Iter: 1717 loss: 2.00011527e-05
Iter: 1718 loss: 2.00145587e-05
Iter: 1719 loss: 1.99996539e-05
Iter: 1720 loss: 1.99948699e-05
Iter: 1721 loss: 2.00355316e-05
Iter: 1722 loss: 1.99947499e-05
Iter: 1723 loss: 1.99916831e-05
Iter: 1724 loss: 1.9989322e-05
Iter: 1725 loss: 1.99884907e-05
Iter: 1726 loss: 1.99825645e-05
Iter: 1727 loss: 1.99867172e-05
Iter: 1728 loss: 1.99789265e-05
Iter: 1729 loss: 1.99733149e-05
Iter: 1730 loss: 1.99737333e-05
Iter: 1731 loss: 1.99691312e-05
Iter: 1732 loss: 1.99611895e-05
Iter: 1733 loss: 1.99885908e-05
Iter: 1734 loss: 1.9959054e-05
Iter: 1735 loss: 1.99534079e-05
Iter: 1736 loss: 1.99985407e-05
Iter: 1737 loss: 1.9952975e-05
Iter: 1738 loss: 1.99466594e-05
Iter: 1739 loss: 1.99785572e-05
Iter: 1740 loss: 1.99455881e-05
Iter: 1741 loss: 1.99423048e-05
Iter: 1742 loss: 1.99399947e-05
Iter: 1743 loss: 1.9938765e-05
Iter: 1744 loss: 1.99328606e-05
Iter: 1745 loss: 1.9958381e-05
Iter: 1746 loss: 1.99317692e-05
Iter: 1747 loss: 1.99267852e-05
Iter: 1748 loss: 1.9928113e-05
Iter: 1749 loss: 1.99233291e-05
Iter: 1750 loss: 1.9917552e-05
Iter: 1751 loss: 1.99340975e-05
Iter: 1752 loss: 1.99158312e-05
Iter: 1753 loss: 1.99093265e-05
Iter: 1754 loss: 1.99591886e-05
Iter: 1755 loss: 1.99088445e-05
Iter: 1756 loss: 1.99054357e-05
Iter: 1757 loss: 1.99277274e-05
Iter: 1758 loss: 1.99050519e-05
Iter: 1759 loss: 1.99011665e-05
Iter: 1760 loss: 1.98998496e-05
Iter: 1761 loss: 1.98975285e-05
Iter: 1762 loss: 1.98928828e-05
Iter: 1763 loss: 1.99102469e-05
Iter: 1764 loss: 1.98917878e-05
Iter: 1765 loss: 1.98875205e-05
Iter: 1766 loss: 1.98856105e-05
Iter: 1767 loss: 1.98834314e-05
Iter: 1768 loss: 1.98768103e-05
Iter: 1769 loss: 1.9878973e-05
Iter: 1770 loss: 1.98721646e-05
Iter: 1771 loss: 1.98644557e-05
Iter: 1772 loss: 1.98979869e-05
Iter: 1773 loss: 1.98629932e-05
Iter: 1774 loss: 1.98593989e-05
Iter: 1775 loss: 1.98586749e-05
Iter: 1776 loss: 1.98552552e-05
Iter: 1777 loss: 1.98509842e-05
Iter: 1778 loss: 1.98506259e-05
Iter: 1779 loss: 1.98460111e-05
Iter: 1780 loss: 1.9855921e-05
Iter: 1781 loss: 1.98443049e-05
Iter: 1782 loss: 1.98379239e-05
Iter: 1783 loss: 1.98530397e-05
Iter: 1784 loss: 1.98355483e-05
Iter: 1785 loss: 1.98305806e-05
Iter: 1786 loss: 1.98304879e-05
Iter: 1787 loss: 1.98264843e-05
Iter: 1788 loss: 1.98204907e-05
Iter: 1789 loss: 1.98977796e-05
Iter: 1790 loss: 1.98203488e-05
Iter: 1791 loss: 1.98156304e-05
Iter: 1792 loss: 1.98299222e-05
Iter: 1793 loss: 1.98140769e-05
Iter: 1794 loss: 1.98100042e-05
Iter: 1795 loss: 1.98442194e-05
Iter: 1796 loss: 1.98098114e-05
Iter: 1797 loss: 1.9806319e-05
Iter: 1798 loss: 1.97997251e-05
Iter: 1799 loss: 1.99371825e-05
Iter: 1800 loss: 1.97997069e-05
Iter: 1801 loss: 1.97933732e-05
Iter: 1802 loss: 1.98597118e-05
Iter: 1803 loss: 1.97931731e-05
Iter: 1804 loss: 1.97887966e-05
Iter: 1805 loss: 1.97825575e-05
Iter: 1806 loss: 1.97823847e-05
Iter: 1807 loss: 1.97745539e-05
Iter: 1808 loss: 1.98081e-05
Iter: 1809 loss: 1.97728859e-05
Iter: 1810 loss: 1.9767198e-05
Iter: 1811 loss: 1.97974077e-05
Iter: 1812 loss: 1.97663576e-05
Iter: 1813 loss: 1.97594891e-05
Iter: 1814 loss: 1.97991012e-05
Iter: 1815 loss: 1.97587069e-05
Iter: 1816 loss: 1.97552217e-05
Iter: 1817 loss: 1.97479822e-05
Iter: 1818 loss: 1.98669986e-05
Iter: 1819 loss: 1.97477493e-05
Iter: 1820 loss: 1.97398858e-05
Iter: 1821 loss: 1.97767986e-05
Iter: 1822 loss: 1.97384e-05
Iter: 1823 loss: 1.97317895e-05
Iter: 1824 loss: 1.97468071e-05
Iter: 1825 loss: 1.97290756e-05
Iter: 1826 loss: 1.97243062e-05
Iter: 1827 loss: 1.97241061e-05
Iter: 1828 loss: 1.97210629e-05
Iter: 1829 loss: 1.97254722e-05
Iter: 1830 loss: 1.97194549e-05
Iter: 1831 loss: 1.97145891e-05
Iter: 1832 loss: 1.97151712e-05
Iter: 1833 loss: 1.97107765e-05
Iter: 1834 loss: 1.97052977e-05
Iter: 1835 loss: 1.97129502e-05
Iter: 1836 loss: 1.97025438e-05
Iter: 1837 loss: 1.9696563e-05
Iter: 1838 loss: 1.97168883e-05
Iter: 1839 loss: 1.96949877e-05
Iter: 1840 loss: 1.96890687e-05
Iter: 1841 loss: 1.96881265e-05
Iter: 1842 loss: 1.96839756e-05
Iter: 1843 loss: 1.96780693e-05
Iter: 1844 loss: 1.97082354e-05
Iter: 1845 loss: 1.9676907e-05
Iter: 1846 loss: 1.96706023e-05
Iter: 1847 loss: 1.97098161e-05
Iter: 1848 loss: 1.96697656e-05
Iter: 1849 loss: 1.96655456e-05
Iter: 1850 loss: 1.96954697e-05
Iter: 1851 loss: 1.96652109e-05
Iter: 1852 loss: 1.96614765e-05
Iter: 1853 loss: 1.96588844e-05
Iter: 1854 loss: 1.9657502e-05
Iter: 1855 loss: 1.9651714e-05
Iter: 1856 loss: 1.96448127e-05
Iter: 1857 loss: 1.9644016e-05
Iter: 1858 loss: 1.96354558e-05
Iter: 1859 loss: 1.96704204e-05
Iter: 1860 loss: 1.96334804e-05
Iter: 1861 loss: 1.9625908e-05
Iter: 1862 loss: 1.96742949e-05
Iter: 1863 loss: 1.96250312e-05
Iter: 1864 loss: 1.96216679e-05
Iter: 1865 loss: 1.96210349e-05
Iter: 1866 loss: 1.96183319e-05
Iter: 1867 loss: 1.96109686e-05
Iter: 1868 loss: 1.96651818e-05
Iter: 1869 loss: 1.96094e-05
Iter: 1870 loss: 1.96022884e-05
Iter: 1871 loss: 1.9671952e-05
Iter: 1872 loss: 1.96021028e-05
Iter: 1873 loss: 1.95957928e-05
Iter: 1874 loss: 1.96007932e-05
Iter: 1875 loss: 1.95920711e-05
Iter: 1876 loss: 1.9585219e-05
Iter: 1877 loss: 1.9597057e-05
Iter: 1878 loss: 1.9582274e-05
Iter: 1879 loss: 1.95768589e-05
Iter: 1880 loss: 1.96413675e-05
Iter: 1881 loss: 1.95767716e-05
Iter: 1882 loss: 1.95716384e-05
Iter: 1883 loss: 1.9581541e-05
Iter: 1884 loss: 1.95695466e-05
Iter: 1885 loss: 1.95648936e-05
Iter: 1886 loss: 1.96011206e-05
Iter: 1887 loss: 1.95646608e-05
Iter: 1888 loss: 1.95612447e-05
Iter: 1889 loss: 1.95551384e-05
Iter: 1890 loss: 1.97054687e-05
Iter: 1891 loss: 1.95550892e-05
Iter: 1892 loss: 1.95491084e-05
Iter: 1893 loss: 1.96226538e-05
Iter: 1894 loss: 1.95490629e-05
Iter: 1895 loss: 1.95443899e-05
Iter: 1896 loss: 1.95443208e-05
Iter: 1897 loss: 1.95404973e-05
Iter: 1898 loss: 1.95344765e-05
Iter: 1899 loss: 1.95321481e-05
Iter: 1900 loss: 1.95287739e-05
Iter: 1901 loss: 1.95236844e-05
Iter: 1902 loss: 1.95236316e-05
Iter: 1903 loss: 1.95181219e-05
Iter: 1904 loss: 1.95365392e-05
Iter: 1905 loss: 1.9516574e-05
Iter: 1906 loss: 1.95127e-05
Iter: 1907 loss: 1.9505942e-05
Iter: 1908 loss: 1.95059401e-05
Iter: 1909 loss: 1.94986387e-05
Iter: 1910 loss: 1.95165067e-05
Iter: 1911 loss: 1.9496114e-05
Iter: 1912 loss: 1.94886579e-05
Iter: 1913 loss: 1.95645571e-05
Iter: 1914 loss: 1.94885251e-05
Iter: 1915 loss: 1.9483643e-05
Iter: 1916 loss: 1.94802269e-05
Iter: 1917 loss: 1.94785935e-05
Iter: 1918 loss: 1.94732638e-05
Iter: 1919 loss: 1.94732711e-05
Iter: 1920 loss: 1.9468127e-05
Iter: 1921 loss: 1.94784388e-05
Iter: 1922 loss: 1.94659715e-05
Iter: 1923 loss: 1.9462128e-05
Iter: 1924 loss: 1.9459585e-05
Iter: 1925 loss: 1.94582135e-05
Iter: 1926 loss: 1.94511886e-05
Iter: 1927 loss: 1.94853164e-05
Iter: 1928 loss: 1.94499789e-05
Iter: 1929 loss: 1.94448075e-05
Iter: 1930 loss: 1.94392233e-05
Iter: 1931 loss: 1.94384211e-05
Iter: 1932 loss: 1.94305e-05
Iter: 1933 loss: 1.94891418e-05
Iter: 1934 loss: 1.94299391e-05
Iter: 1935 loss: 1.94270706e-05
Iter: 1936 loss: 1.94263102e-05
Iter: 1937 loss: 1.94235818e-05
Iter: 1938 loss: 1.94171989e-05
Iter: 1939 loss: 1.95015236e-05
Iter: 1940 loss: 1.94168279e-05
Iter: 1941 loss: 1.94107633e-05
Iter: 1942 loss: 1.94948079e-05
Iter: 1943 loss: 1.94107452e-05
Iter: 1944 loss: 1.94070708e-05
Iter: 1945 loss: 1.94023778e-05
Iter: 1946 loss: 1.94020831e-05
Iter: 1947 loss: 1.93947108e-05
Iter: 1948 loss: 1.93986089e-05
Iter: 1949 loss: 1.9389885e-05
Iter: 1950 loss: 1.93820706e-05
Iter: 1951 loss: 1.94371041e-05
Iter: 1952 loss: 1.93813921e-05
Iter: 1953 loss: 1.93735741e-05
Iter: 1954 loss: 1.9411189e-05
Iter: 1955 loss: 1.93722462e-05
Iter: 1956 loss: 1.93676169e-05
Iter: 1957 loss: 1.94302174e-05
Iter: 1958 loss: 1.93675151e-05
Iter: 1959 loss: 1.93638134e-05
Iter: 1960 loss: 1.93562773e-05
Iter: 1961 loss: 1.95043103e-05
Iter: 1962 loss: 1.93562519e-05
Iter: 1963 loss: 1.93490232e-05
Iter: 1964 loss: 1.93680498e-05
Iter: 1965 loss: 1.93466494e-05
Iter: 1966 loss: 1.93402611e-05
Iter: 1967 loss: 1.93792148e-05
Iter: 1968 loss: 1.93394135e-05
Iter: 1969 loss: 1.93326305e-05
Iter: 1970 loss: 1.93464712e-05
Iter: 1971 loss: 1.93299929e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script80
+ '[' -r STOP.script80 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2 /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2
+ date
Mon Nov  2 13:33:09 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d03b50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d026b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d026bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d039ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d02ebd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d00fb510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d01f68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d0119950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d020f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d006ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d01d6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d01cbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d01ad488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d01ad510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d00e3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d00e3b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f238477a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f238477ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23846569d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d007c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d007ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f238474ee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23845d26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23845e4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23845e4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23846d7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23844cd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23844ef730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23844ef2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23844fe488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23d015c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23845367b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2384536620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2384539510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23844349d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f238471e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.001927574
test_loss: 0.0019294589
train_loss: 0.0007254556
test_loss: 0.00085252296
train_loss: 0.00055059086
test_loss: 0.0005537315
train_loss: 0.00030750208
test_loss: 0.00040305016
train_loss: 0.00026389235
test_loss: 0.00028627474
train_loss: 0.000195939
test_loss: 0.00025110823
train_loss: 0.00019950092
test_loss: 0.00024159053
train_loss: 0.00013202774
test_loss: 0.00018483514
train_loss: 0.00014321716
test_loss: 0.00015509581
train_loss: 0.00015009737
test_loss: 0.00015455093
train_loss: 0.00013985384
test_loss: 0.0001482611
train_loss: 0.00011204232
test_loss: 0.0001271879
train_loss: 0.00010038138
test_loss: 0.00012007605
train_loss: 8.96389e-05
test_loss: 0.00011027464
train_loss: 8.4476174e-05
test_loss: 0.000108747816
train_loss: 8.3254454e-05
test_loss: 0.00010229623
train_loss: 8.519142e-05
test_loss: 0.0001013904
train_loss: 8.0528465e-05
test_loss: 9.6426025e-05
train_loss: 7.800466e-05
test_loss: 9.21408e-05
train_loss: 7.0964605e-05
test_loss: 9.344711e-05
train_loss: 7.978045e-05
test_loss: 8.632733e-05
train_loss: 7.4363576e-05
test_loss: 8.558925e-05
train_loss: 6.944736e-05
test_loss: 8.3696905e-05
train_loss: 7.374011e-05
test_loss: 8.2220344e-05
train_loss: 6.394825e-05
test_loss: 7.7940516e-05
train_loss: 6.462156e-05
test_loss: 8.097978e-05
train_loss: 7.094567e-05
test_loss: 7.7086566e-05
train_loss: 6.0093924e-05
test_loss: 7.62184e-05
train_loss: 5.911471e-05
test_loss: 7.6100616e-05
train_loss: 6.732219e-05
test_loss: 7.4764444e-05
train_loss: 6.5737026e-05
test_loss: 7.427972e-05
train_loss: 5.431485e-05
test_loss: 7.278368e-05
train_loss: 5.9048136e-05
test_loss: 7.1603245e-05
train_loss: 5.9268314e-05
test_loss: 7.15423e-05
train_loss: 5.939663e-05
test_loss: 7.06203e-05
train_loss: 5.773136e-05
test_loss: 7.1066104e-05
train_loss: 5.0154704e-05
test_loss: 7.02403e-05
train_loss: 5.023845e-05
test_loss: 6.920311e-05
train_loss: 5.261094e-05
test_loss: 6.867145e-05
train_loss: 5.3820397e-05
test_loss: 6.897658e-05
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eab375510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eab42ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eab472b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eab3bd7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eab3bd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e82644378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eab322d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e825907b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e825ab268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e825bf620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e8263ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e825638c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e82563a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e82582400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e82582048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c443510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e825d6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e825d6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c465950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c359620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c359158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c37d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c37d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c3de620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c3de400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c3391e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c2d3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c2fe7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c2fe8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c2ab950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c1d3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c206620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c2060d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c272730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c219ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5c1192f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.20235808e-05
Iter: 2 loss: 5.14420572e-05
Iter: 3 loss: 5.12125116e-05
Iter: 4 loss: 5.0809e-05
Iter: 5 loss: 5.08076555e-05
Iter: 6 loss: 5.06243632e-05
Iter: 7 loss: 5.0342067e-05
Iter: 8 loss: 5.03378324e-05
Iter: 9 loss: 5.01014e-05
Iter: 10 loss: 5.10111458e-05
Iter: 11 loss: 5.00458627e-05
Iter: 12 loss: 4.98213558e-05
Iter: 13 loss: 5.02405856e-05
Iter: 14 loss: 4.97261281e-05
Iter: 15 loss: 4.94996239e-05
Iter: 16 loss: 5.06867764e-05
Iter: 17 loss: 4.94645137e-05
Iter: 18 loss: 4.92851614e-05
Iter: 19 loss: 4.93263287e-05
Iter: 20 loss: 4.91532046e-05
Iter: 21 loss: 4.89510167e-05
Iter: 22 loss: 5.03703559e-05
Iter: 23 loss: 4.89325976e-05
Iter: 24 loss: 4.8779093e-05
Iter: 25 loss: 4.87646321e-05
Iter: 26 loss: 4.86518147e-05
Iter: 27 loss: 4.84568882e-05
Iter: 28 loss: 4.95287168e-05
Iter: 29 loss: 4.84287339e-05
Iter: 30 loss: 4.82590367e-05
Iter: 31 loss: 4.85527853e-05
Iter: 32 loss: 4.8183414e-05
Iter: 33 loss: 4.80194867e-05
Iter: 34 loss: 4.84676057e-05
Iter: 35 loss: 4.7965772e-05
Iter: 36 loss: 4.78441361e-05
Iter: 37 loss: 4.94353044e-05
Iter: 38 loss: 4.7843263e-05
Iter: 39 loss: 4.77388967e-05
Iter: 40 loss: 4.86790959e-05
Iter: 41 loss: 4.77339599e-05
Iter: 42 loss: 4.76798814e-05
Iter: 43 loss: 4.75584566e-05
Iter: 44 loss: 4.92197287e-05
Iter: 45 loss: 4.75516426e-05
Iter: 46 loss: 4.74435074e-05
Iter: 47 loss: 4.8025664e-05
Iter: 48 loss: 4.74272929e-05
Iter: 49 loss: 4.73268774e-05
Iter: 50 loss: 4.7787129e-05
Iter: 51 loss: 4.73078217e-05
Iter: 52 loss: 4.72342617e-05
Iter: 53 loss: 4.75041634e-05
Iter: 54 loss: 4.72161591e-05
Iter: 55 loss: 4.71451713e-05
Iter: 56 loss: 4.71830353e-05
Iter: 57 loss: 4.7098114e-05
Iter: 58 loss: 4.70143568e-05
Iter: 59 loss: 4.7395697e-05
Iter: 60 loss: 4.6998317e-05
Iter: 61 loss: 4.69260776e-05
Iter: 62 loss: 4.69734186e-05
Iter: 63 loss: 4.68804974e-05
Iter: 64 loss: 4.67845603e-05
Iter: 65 loss: 4.71094972e-05
Iter: 66 loss: 4.67587197e-05
Iter: 67 loss: 4.66782e-05
Iter: 68 loss: 4.68700528e-05
Iter: 69 loss: 4.66490601e-05
Iter: 70 loss: 4.6575e-05
Iter: 71 loss: 4.6919864e-05
Iter: 72 loss: 4.65614794e-05
Iter: 73 loss: 4.65202393e-05
Iter: 74 loss: 4.65198136e-05
Iter: 75 loss: 4.64716868e-05
Iter: 76 loss: 4.64697841e-05
Iter: 77 loss: 4.64326295e-05
Iter: 78 loss: 4.63898541e-05
Iter: 79 loss: 4.63576071e-05
Iter: 80 loss: 4.63435717e-05
Iter: 81 loss: 4.62808312e-05
Iter: 82 loss: 4.64854929e-05
Iter: 83 loss: 4.62634598e-05
Iter: 84 loss: 4.62057396e-05
Iter: 85 loss: 4.64815166e-05
Iter: 86 loss: 4.61952804e-05
Iter: 87 loss: 4.61490854e-05
Iter: 88 loss: 4.63862234e-05
Iter: 89 loss: 4.61414857e-05
Iter: 90 loss: 4.60984484e-05
Iter: 91 loss: 4.60733463e-05
Iter: 92 loss: 4.60552401e-05
Iter: 93 loss: 4.6009347e-05
Iter: 94 loss: 4.63998622e-05
Iter: 95 loss: 4.6006764e-05
Iter: 96 loss: 4.59620642e-05
Iter: 97 loss: 4.59704497e-05
Iter: 98 loss: 4.59287548e-05
Iter: 99 loss: 4.58775321e-05
Iter: 100 loss: 4.59997e-05
Iter: 101 loss: 4.58588765e-05
Iter: 102 loss: 4.58028662e-05
Iter: 103 loss: 4.60791634e-05
Iter: 104 loss: 4.57932401e-05
Iter: 105 loss: 4.57529168e-05
Iter: 106 loss: 4.57821297e-05
Iter: 107 loss: 4.57280694e-05
Iter: 108 loss: 4.57290807e-05
Iter: 109 loss: 4.5706729e-05
Iter: 110 loss: 4.56873458e-05
Iter: 111 loss: 4.56475536e-05
Iter: 112 loss: 4.63489778e-05
Iter: 113 loss: 4.56470152e-05
Iter: 114 loss: 4.56095222e-05
Iter: 115 loss: 4.56329872e-05
Iter: 116 loss: 4.5585748e-05
Iter: 117 loss: 4.55431218e-05
Iter: 118 loss: 4.57313581e-05
Iter: 119 loss: 4.55345689e-05
Iter: 120 loss: 4.54974797e-05
Iter: 121 loss: 4.5620327e-05
Iter: 122 loss: 4.5487126e-05
Iter: 123 loss: 4.54505e-05
Iter: 124 loss: 4.56242233e-05
Iter: 125 loss: 4.54436085e-05
Iter: 126 loss: 4.54113397e-05
Iter: 127 loss: 4.5437715e-05
Iter: 128 loss: 4.53917164e-05
Iter: 129 loss: 4.53560715e-05
Iter: 130 loss: 4.53948778e-05
Iter: 131 loss: 4.53365319e-05
Iter: 132 loss: 4.52902095e-05
Iter: 133 loss: 4.55335685e-05
Iter: 134 loss: 4.52832137e-05
Iter: 135 loss: 4.52531604e-05
Iter: 136 loss: 4.53047905e-05
Iter: 137 loss: 4.52399108e-05
Iter: 138 loss: 4.51985616e-05
Iter: 139 loss: 4.52532404e-05
Iter: 140 loss: 4.51777305e-05
Iter: 141 loss: 4.51456508e-05
Iter: 142 loss: 4.52842432e-05
Iter: 143 loss: 4.51390297e-05
Iter: 144 loss: 4.51174128e-05
Iter: 145 loss: 4.51157939e-05
Iter: 146 loss: 4.5099514e-05
Iter: 147 loss: 4.50713269e-05
Iter: 148 loss: 4.50713887e-05
Iter: 149 loss: 4.50450316e-05
Iter: 150 loss: 4.50236657e-05
Iter: 151 loss: 4.50160587e-05
Iter: 152 loss: 4.49803629e-05
Iter: 153 loss: 4.52854802e-05
Iter: 154 loss: 4.49784238e-05
Iter: 155 loss: 4.49473155e-05
Iter: 156 loss: 4.50201405e-05
Iter: 157 loss: 4.49359904e-05
Iter: 158 loss: 4.49098879e-05
Iter: 159 loss: 4.51423184e-05
Iter: 160 loss: 4.49087383e-05
Iter: 161 loss: 4.48870414e-05
Iter: 162 loss: 4.48703504e-05
Iter: 163 loss: 4.48634601e-05
Iter: 164 loss: 4.48297214e-05
Iter: 165 loss: 4.49587023e-05
Iter: 166 loss: 4.48216597e-05
Iter: 167 loss: 4.47905404e-05
Iter: 168 loss: 4.48708961e-05
Iter: 169 loss: 4.47798629e-05
Iter: 170 loss: 4.47500352e-05
Iter: 171 loss: 4.48262581e-05
Iter: 172 loss: 4.4739656e-05
Iter: 173 loss: 4.47084967e-05
Iter: 174 loss: 4.48364481e-05
Iter: 175 loss: 4.47016537e-05
Iter: 176 loss: 4.46776176e-05
Iter: 177 loss: 4.47358689e-05
Iter: 178 loss: 4.46691e-05
Iter: 179 loss: 4.46540143e-05
Iter: 180 loss: 4.4651475e-05
Iter: 181 loss: 4.46420563e-05
Iter: 182 loss: 4.4617329e-05
Iter: 183 loss: 4.48085157e-05
Iter: 184 loss: 4.46123886e-05
Iter: 185 loss: 4.45807236e-05
Iter: 186 loss: 4.45951482e-05
Iter: 187 loss: 4.4559245e-05
Iter: 188 loss: 4.45299738e-05
Iter: 189 loss: 4.48830615e-05
Iter: 190 loss: 4.45297737e-05
Iter: 191 loss: 4.45055121e-05
Iter: 192 loss: 4.44992111e-05
Iter: 193 loss: 4.44837242e-05
Iter: 194 loss: 4.44610523e-05
Iter: 195 loss: 4.4460794e-05
Iter: 196 loss: 4.44416146e-05
Iter: 197 loss: 4.44153629e-05
Iter: 198 loss: 4.44141733e-05
Iter: 199 loss: 4.43857207e-05
Iter: 200 loss: 4.46222839e-05
Iter: 201 loss: 4.43841382e-05
Iter: 202 loss: 4.43611934e-05
Iter: 203 loss: 4.43815443e-05
Iter: 204 loss: 4.43479657e-05
Iter: 205 loss: 4.4320921e-05
Iter: 206 loss: 4.44148754e-05
Iter: 207 loss: 4.43139143e-05
Iter: 208 loss: 4.42889286e-05
Iter: 209 loss: 4.43631034e-05
Iter: 210 loss: 4.42810924e-05
Iter: 211 loss: 4.42578967e-05
Iter: 212 loss: 4.44313046e-05
Iter: 213 loss: 4.42562232e-05
Iter: 214 loss: 4.42345809e-05
Iter: 215 loss: 4.44109319e-05
Iter: 216 loss: 4.42332093e-05
Iter: 217 loss: 4.42227902e-05
Iter: 218 loss: 4.41968878e-05
Iter: 219 loss: 4.44383404e-05
Iter: 220 loss: 4.41930169e-05
Iter: 221 loss: 4.41631419e-05
Iter: 222 loss: 4.42897253e-05
Iter: 223 loss: 4.41567354e-05
Iter: 224 loss: 4.41331795e-05
Iter: 225 loss: 4.41817137e-05
Iter: 226 loss: 4.41234333e-05
Iter: 227 loss: 4.4093631e-05
Iter: 228 loss: 4.41821903e-05
Iter: 229 loss: 4.40845979e-05
Iter: 230 loss: 4.40629083e-05
Iter: 231 loss: 4.42812889e-05
Iter: 232 loss: 4.40623335e-05
Iter: 233 loss: 4.40434742e-05
Iter: 234 loss: 4.40453587e-05
Iter: 235 loss: 4.40288059e-05
Iter: 236 loss: 4.40042386e-05
Iter: 237 loss: 4.40730219e-05
Iter: 238 loss: 4.39961295e-05
Iter: 239 loss: 4.39720825e-05
Iter: 240 loss: 4.39929572e-05
Iter: 241 loss: 4.39579744e-05
Iter: 242 loss: 4.39323267e-05
Iter: 243 loss: 4.40879303e-05
Iter: 244 loss: 4.39293071e-05
Iter: 245 loss: 4.39056748e-05
Iter: 246 loss: 4.39489086e-05
Iter: 247 loss: 4.38955976e-05
Iter: 248 loss: 4.38773714e-05
Iter: 249 loss: 4.38771785e-05
Iter: 250 loss: 4.38583666e-05
Iter: 251 loss: 4.38683237e-05
Iter: 252 loss: 4.38459319e-05
Iter: 253 loss: 4.38321513e-05
Iter: 254 loss: 4.38266361e-05
Iter: 255 loss: 4.38193747e-05
Iter: 256 loss: 4.3797183e-05
Iter: 257 loss: 4.37906638e-05
Iter: 258 loss: 4.37773342e-05
Iter: 259 loss: 4.37533054e-05
Iter: 260 loss: 4.39740106e-05
Iter: 261 loss: 4.37523777e-05
Iter: 262 loss: 4.37286471e-05
Iter: 263 loss: 4.37171475e-05
Iter: 264 loss: 4.37056078e-05
Iter: 265 loss: 4.36835071e-05
Iter: 266 loss: 4.40212607e-05
Iter: 267 loss: 4.36834453e-05
Iter: 268 loss: 4.36611081e-05
Iter: 269 loss: 4.36706141e-05
Iter: 270 loss: 4.36458067e-05
Iter: 271 loss: 4.36237824e-05
Iter: 272 loss: 4.37047274e-05
Iter: 273 loss: 4.36186128e-05
Iter: 274 loss: 4.35957918e-05
Iter: 275 loss: 4.36007249e-05
Iter: 276 loss: 4.35788679e-05
Iter: 277 loss: 4.3552689e-05
Iter: 278 loss: 4.3718479e-05
Iter: 279 loss: 4.35496404e-05
Iter: 280 loss: 4.35298607e-05
Iter: 281 loss: 4.3594926e-05
Iter: 282 loss: 4.3524291e-05
Iter: 283 loss: 4.35099028e-05
Iter: 284 loss: 4.35094626e-05
Iter: 285 loss: 4.34983303e-05
Iter: 286 loss: 4.34855756e-05
Iter: 287 loss: 4.3484004e-05
Iter: 288 loss: 4.34670947e-05
Iter: 289 loss: 4.34538233e-05
Iter: 290 loss: 4.34485664e-05
Iter: 291 loss: 4.34212852e-05
Iter: 292 loss: 4.34526373e-05
Iter: 293 loss: 4.34066569e-05
Iter: 294 loss: 4.33801833e-05
Iter: 295 loss: 4.36434493e-05
Iter: 296 loss: 4.33794448e-05
Iter: 297 loss: 4.33579444e-05
Iter: 298 loss: 4.33837704e-05
Iter: 299 loss: 4.33468194e-05
Iter: 300 loss: 4.33254245e-05
Iter: 301 loss: 4.33918649e-05
Iter: 302 loss: 4.33189743e-05
Iter: 303 loss: 4.32927663e-05
Iter: 304 loss: 4.34154354e-05
Iter: 305 loss: 4.32878187e-05
Iter: 306 loss: 4.32705892e-05
Iter: 307 loss: 4.32627094e-05
Iter: 308 loss: 4.32540764e-05
Iter: 309 loss: 4.32283377e-05
Iter: 310 loss: 4.33697714e-05
Iter: 311 loss: 4.32245215e-05
Iter: 312 loss: 4.32022498e-05
Iter: 313 loss: 4.32108536e-05
Iter: 314 loss: 4.31866465e-05
Iter: 315 loss: 4.31701483e-05
Iter: 316 loss: 4.31691224e-05
Iter: 317 loss: 4.31529043e-05
Iter: 318 loss: 4.32213928e-05
Iter: 319 loss: 4.31495428e-05
Iter: 320 loss: 4.31385342e-05
Iter: 321 loss: 4.31238223e-05
Iter: 322 loss: 4.31232e-05
Iter: 323 loss: 4.31013104e-05
Iter: 324 loss: 4.31109293e-05
Iter: 325 loss: 4.30864457e-05
Iter: 326 loss: 4.30626824e-05
Iter: 327 loss: 4.31439221e-05
Iter: 328 loss: 4.30561122e-05
Iter: 329 loss: 4.30305226e-05
Iter: 330 loss: 4.30642249e-05
Iter: 331 loss: 4.3017295e-05
Iter: 332 loss: 4.29909487e-05
Iter: 333 loss: 4.31299122e-05
Iter: 334 loss: 4.29865977e-05
Iter: 335 loss: 4.2963431e-05
Iter: 336 loss: 4.30486325e-05
Iter: 337 loss: 4.29577485e-05
Iter: 338 loss: 4.29371539e-05
Iter: 339 loss: 4.30668879e-05
Iter: 340 loss: 4.2934691e-05
Iter: 341 loss: 4.29183274e-05
Iter: 342 loss: 4.29170832e-05
Iter: 343 loss: 4.29048123e-05
Iter: 344 loss: 4.28815292e-05
Iter: 345 loss: 4.29545689e-05
Iter: 346 loss: 4.28747444e-05
Iter: 347 loss: 4.28521707e-05
Iter: 348 loss: 4.28572748e-05
Iter: 349 loss: 4.28356361e-05
Iter: 350 loss: 4.28419407e-05
Iter: 351 loss: 4.2825839e-05
Iter: 352 loss: 4.2816042e-05
Iter: 353 loss: 4.28017083e-05
Iter: 354 loss: 4.28012499e-05
Iter: 355 loss: 4.27825908e-05
Iter: 356 loss: 4.27915402e-05
Iter: 357 loss: 4.27700652e-05
Iter: 358 loss: 4.27489977e-05
Iter: 359 loss: 4.27866107e-05
Iter: 360 loss: 4.27398118e-05
Iter: 361 loss: 4.27163104e-05
Iter: 362 loss: 4.27733394e-05
Iter: 363 loss: 4.27078849e-05
Iter: 364 loss: 4.26840161e-05
Iter: 365 loss: 4.27352497e-05
Iter: 366 loss: 4.26745828e-05
Iter: 367 loss: 4.26524821e-05
Iter: 368 loss: 4.27397681e-05
Iter: 369 loss: 4.26474289e-05
Iter: 370 loss: 4.26231709e-05
Iter: 371 loss: 4.26565057e-05
Iter: 372 loss: 4.26110928e-05
Iter: 373 loss: 4.25893522e-05
Iter: 374 loss: 4.2857977e-05
Iter: 375 loss: 4.25889193e-05
Iter: 376 loss: 4.2570904e-05
Iter: 377 loss: 4.25566759e-05
Iter: 378 loss: 4.2551259e-05
Iter: 379 loss: 4.25278849e-05
Iter: 380 loss: 4.2642856e-05
Iter: 381 loss: 4.25237668e-05
Iter: 382 loss: 4.25018225e-05
Iter: 383 loss: 4.25497346e-05
Iter: 384 loss: 4.24932514e-05
Iter: 385 loss: 4.24923637e-05
Iter: 386 loss: 4.24846876e-05
Iter: 387 loss: 4.24770515e-05
Iter: 388 loss: 4.24588397e-05
Iter: 389 loss: 4.2661155e-05
Iter: 390 loss: 4.24571081e-05
Iter: 391 loss: 4.2434629e-05
Iter: 392 loss: 4.24431273e-05
Iter: 393 loss: 4.24190221e-05
Iter: 394 loss: 4.23989404e-05
Iter: 395 loss: 4.26312617e-05
Iter: 396 loss: 4.23986749e-05
Iter: 397 loss: 4.23808597e-05
Iter: 398 loss: 4.23555612e-05
Iter: 399 loss: 4.23546189e-05
Iter: 400 loss: 4.23295642e-05
Iter: 401 loss: 4.25649814e-05
Iter: 402 loss: 4.23286328e-05
Iter: 403 loss: 4.23076963e-05
Iter: 404 loss: 4.23501e-05
Iter: 405 loss: 4.22993326e-05
Iter: 406 loss: 4.22767371e-05
Iter: 407 loss: 4.23319216e-05
Iter: 408 loss: 4.22685698e-05
Iter: 409 loss: 4.22503763e-05
Iter: 410 loss: 4.24144273e-05
Iter: 411 loss: 4.22497251e-05
Iter: 412 loss: 4.22313e-05
Iter: 413 loss: 4.22321536e-05
Iter: 414 loss: 4.22167723e-05
Iter: 415 loss: 4.21962759e-05
Iter: 416 loss: 4.22308367e-05
Iter: 417 loss: 4.21870936e-05
Iter: 418 loss: 4.2163294e-05
Iter: 419 loss: 4.22764497e-05
Iter: 420 loss: 4.2159314e-05
Iter: 421 loss: 4.2147647e-05
Iter: 422 loss: 4.2145708e-05
Iter: 423 loss: 4.21367149e-05
Iter: 424 loss: 4.21141303e-05
Iter: 425 loss: 4.23233978e-05
Iter: 426 loss: 4.21110635e-05
Iter: 427 loss: 4.20873293e-05
Iter: 428 loss: 4.21596487e-05
Iter: 429 loss: 4.20804245e-05
Iter: 430 loss: 4.20590368e-05
Iter: 431 loss: 4.21566e-05
Iter: 432 loss: 4.20548567e-05
Iter: 433 loss: 4.20336364e-05
Iter: 434 loss: 4.20467222e-05
Iter: 435 loss: 4.20200086e-05
Iter: 436 loss: 4.19941825e-05
Iter: 437 loss: 4.20618089e-05
Iter: 438 loss: 4.19854405e-05
Iter: 439 loss: 4.19608878e-05
Iter: 440 loss: 4.20535725e-05
Iter: 441 loss: 4.1954856e-05
Iter: 442 loss: 4.19299904e-05
Iter: 443 loss: 4.19731514e-05
Iter: 444 loss: 4.19193129e-05
Iter: 445 loss: 4.1893818e-05
Iter: 446 loss: 4.2015261e-05
Iter: 447 loss: 4.18895506e-05
Iter: 448 loss: 4.18662748e-05
Iter: 449 loss: 4.19896096e-05
Iter: 450 loss: 4.18627642e-05
Iter: 451 loss: 4.18456075e-05
Iter: 452 loss: 4.18399322e-05
Iter: 453 loss: 4.18300406e-05
Iter: 454 loss: 4.18050549e-05
Iter: 455 loss: 4.19648932e-05
Iter: 456 loss: 4.18022682e-05
Iter: 457 loss: 4.17850206e-05
Iter: 458 loss: 4.1784846e-05
Iter: 459 loss: 4.17773117e-05
Iter: 460 loss: 4.17556439e-05
Iter: 461 loss: 4.1839492e-05
Iter: 462 loss: 4.17465635e-05
Iter: 463 loss: 4.17168812e-05
Iter: 464 loss: 4.19481366e-05
Iter: 465 loss: 4.17148913e-05
Iter: 466 loss: 4.16942312e-05
Iter: 467 loss: 4.16884322e-05
Iter: 468 loss: 4.16759e-05
Iter: 469 loss: 4.16493785e-05
Iter: 470 loss: 4.20039432e-05
Iter: 471 loss: 4.16493058e-05
Iter: 472 loss: 4.16310504e-05
Iter: 473 loss: 4.16267e-05
Iter: 474 loss: 4.16149705e-05
Iter: 475 loss: 4.15876857e-05
Iter: 476 loss: 4.16568364e-05
Iter: 477 loss: 4.15782088e-05
Iter: 478 loss: 4.15533141e-05
Iter: 479 loss: 4.1741514e-05
Iter: 480 loss: 4.15514078e-05
Iter: 481 loss: 4.15309078e-05
Iter: 482 loss: 4.15469258e-05
Iter: 483 loss: 4.15185314e-05
Iter: 484 loss: 4.14951064e-05
Iter: 485 loss: 4.172833e-05
Iter: 486 loss: 4.14942406e-05
Iter: 487 loss: 4.14766764e-05
Iter: 488 loss: 4.14636052e-05
Iter: 489 loss: 4.14577553e-05
Iter: 490 loss: 4.1437248e-05
Iter: 491 loss: 4.1670366e-05
Iter: 492 loss: 4.1437e-05
Iter: 493 loss: 4.1424857e-05
Iter: 494 loss: 4.14246315e-05
Iter: 495 loss: 4.14161841e-05
Iter: 496 loss: 4.13924099e-05
Iter: 497 loss: 4.1533247e-05
Iter: 498 loss: 4.1386018e-05
Iter: 499 loss: 4.13604866e-05
Iter: 500 loss: 4.14839e-05
Iter: 501 loss: 4.1356172e-05
Iter: 502 loss: 4.13335074e-05
Iter: 503 loss: 4.13470516e-05
Iter: 504 loss: 4.13190792e-05
Iter: 505 loss: 4.12879817e-05
Iter: 506 loss: 4.1425832e-05
Iter: 507 loss: 4.12816844e-05
Iter: 508 loss: 4.12531626e-05
Iter: 509 loss: 4.13493435e-05
Iter: 510 loss: 4.12453592e-05
Iter: 511 loss: 4.12227018e-05
Iter: 512 loss: 4.13268099e-05
Iter: 513 loss: 4.12184745e-05
Iter: 514 loss: 4.11964793e-05
Iter: 515 loss: 4.11965157e-05
Iter: 516 loss: 4.11785077e-05
Iter: 517 loss: 4.11523e-05
Iter: 518 loss: 4.12629815e-05
Iter: 519 loss: 4.11467663e-05
Iter: 520 loss: 4.11253641e-05
Iter: 521 loss: 4.13286325e-05
Iter: 522 loss: 4.11244546e-05
Iter: 523 loss: 4.1105428e-05
Iter: 524 loss: 4.11031142e-05
Iter: 525 loss: 4.10893881e-05
Iter: 526 loss: 4.10692373e-05
Iter: 527 loss: 4.1244115e-05
Iter: 528 loss: 4.10678331e-05
Iter: 529 loss: 4.10494977e-05
Iter: 530 loss: 4.12141089e-05
Iter: 531 loss: 4.10488283e-05
Iter: 532 loss: 4.10359789e-05
Iter: 533 loss: 4.1013278e-05
Iter: 534 loss: 4.15813192e-05
Iter: 535 loss: 4.10131397e-05
Iter: 536 loss: 4.09916902e-05
Iter: 537 loss: 4.09902314e-05
Iter: 538 loss: 4.09741733e-05
Iter: 539 loss: 4.09426175e-05
Iter: 540 loss: 4.10824468e-05
Iter: 541 loss: 4.09364147e-05
Iter: 542 loss: 4.09081476e-05
Iter: 543 loss: 4.09905406e-05
Iter: 544 loss: 4.08994019e-05
Iter: 545 loss: 4.08688065e-05
Iter: 546 loss: 4.09790737e-05
Iter: 547 loss: 4.08609703e-05
Iter: 548 loss: 4.08371634e-05
Iter: 549 loss: 4.091546e-05
Iter: 550 loss: 4.08304695e-05
Iter: 551 loss: 4.08053456e-05
Iter: 552 loss: 4.08739288e-05
Iter: 553 loss: 4.07971784e-05
Iter: 554 loss: 4.07731713e-05
Iter: 555 loss: 4.07967455e-05
Iter: 556 loss: 4.07594853e-05
Iter: 557 loss: 4.07322041e-05
Iter: 558 loss: 4.08855485e-05
Iter: 559 loss: 4.0728286e-05
Iter: 560 loss: 4.07031403e-05
Iter: 561 loss: 4.08024061e-05
Iter: 562 loss: 4.06973195e-05
Iter: 563 loss: 4.06796316e-05
Iter: 564 loss: 4.08202541e-05
Iter: 565 loss: 4.06784238e-05
Iter: 566 loss: 4.06619947e-05
Iter: 567 loss: 4.07517473e-05
Iter: 568 loss: 4.06594045e-05
Iter: 569 loss: 4.06457111e-05
Iter: 570 loss: 4.06174513e-05
Iter: 571 loss: 4.11129731e-05
Iter: 572 loss: 4.06169274e-05
Iter: 573 loss: 4.05917199e-05
Iter: 574 loss: 4.06232793e-05
Iter: 575 loss: 4.05784085e-05
Iter: 576 loss: 4.05483661e-05
Iter: 577 loss: 4.07310508e-05
Iter: 578 loss: 4.05447208e-05
Iter: 579 loss: 4.05199971e-05
Iter: 580 loss: 4.05311694e-05
Iter: 581 loss: 4.05032551e-05
Iter: 582 loss: 4.0472758e-05
Iter: 583 loss: 4.06701583e-05
Iter: 584 loss: 4.04693419e-05
Iter: 585 loss: 4.04465463e-05
Iter: 586 loss: 4.05393439e-05
Iter: 587 loss: 4.04412676e-05
Iter: 588 loss: 4.04190287e-05
Iter: 589 loss: 4.04568673e-05
Iter: 590 loss: 4.04089042e-05
Iter: 591 loss: 4.0382376e-05
Iter: 592 loss: 4.04386228e-05
Iter: 593 loss: 4.03720805e-05
Iter: 594 loss: 4.03480662e-05
Iter: 595 loss: 4.0440711e-05
Iter: 596 loss: 4.03422309e-05
Iter: 597 loss: 4.03199883e-05
Iter: 598 loss: 4.04711282e-05
Iter: 599 loss: 4.03178637e-05
Iter: 600 loss: 4.03035956e-05
Iter: 601 loss: 4.04599123e-05
Iter: 602 loss: 4.03033264e-05
Iter: 603 loss: 4.02891601e-05
Iter: 604 loss: 4.02814912e-05
Iter: 605 loss: 4.02750775e-05
Iter: 606 loss: 4.02585e-05
Iter: 607 loss: 4.02586811e-05
Iter: 608 loss: 4.02455116e-05
Iter: 609 loss: 4.0224535e-05
Iter: 610 loss: 4.02510486e-05
Iter: 611 loss: 4.02137084e-05
Iter: 612 loss: 4.0186198e-05
Iter: 613 loss: 4.0245679e-05
Iter: 614 loss: 4.0175335e-05
Iter: 615 loss: 4.0151448e-05
Iter: 616 loss: 4.02208752e-05
Iter: 617 loss: 4.01438519e-05
Iter: 618 loss: 4.01175857e-05
Iter: 619 loss: 4.02258665e-05
Iter: 620 loss: 4.01118741e-05
Iter: 621 loss: 4.0089315e-05
Iter: 622 loss: 4.00977951e-05
Iter: 623 loss: 4.00738136e-05
Iter: 624 loss: 4.00430872e-05
Iter: 625 loss: 4.02821825e-05
Iter: 626 loss: 4.00406643e-05
Iter: 627 loss: 4.00194804e-05
Iter: 628 loss: 4.00582467e-05
Iter: 629 loss: 4.00103308e-05
Iter: 630 loss: 3.99861237e-05
Iter: 631 loss: 4.00434365e-05
Iter: 632 loss: 3.9977298e-05
Iter: 633 loss: 3.99545679e-05
Iter: 634 loss: 4.01456491e-05
Iter: 635 loss: 3.99531709e-05
Iter: 636 loss: 3.99379933e-05
Iter: 637 loss: 4.01490179e-05
Iter: 638 loss: 3.99376295e-05
Iter: 639 loss: 3.99281562e-05
Iter: 640 loss: 3.99130586e-05
Iter: 641 loss: 3.99130295e-05
Iter: 642 loss: 3.98900665e-05
Iter: 643 loss: 3.98949742e-05
Iter: 644 loss: 3.98729935e-05
Iter: 645 loss: 3.98498451e-05
Iter: 646 loss: 3.98525153e-05
Iter: 647 loss: 3.98317679e-05
Iter: 648 loss: 3.98002412e-05
Iter: 649 loss: 3.99864221e-05
Iter: 650 loss: 3.97962212e-05
Iter: 651 loss: 3.97687909e-05
Iter: 652 loss: 3.98163393e-05
Iter: 653 loss: 3.97567819e-05
Iter: 654 loss: 3.97273252e-05
Iter: 655 loss: 3.98437551e-05
Iter: 656 loss: 3.9720464e-05
Iter: 657 loss: 3.96959476e-05
Iter: 658 loss: 3.97493495e-05
Iter: 659 loss: 3.96863834e-05
Iter: 660 loss: 3.96601754e-05
Iter: 661 loss: 3.97866315e-05
Iter: 662 loss: 3.96556425e-05
Iter: 663 loss: 3.96341784e-05
Iter: 664 loss: 3.97283184e-05
Iter: 665 loss: 3.96298637e-05
Iter: 666 loss: 3.96099713e-05
Iter: 667 loss: 3.96219366e-05
Iter: 668 loss: 3.95973184e-05
Iter: 669 loss: 3.95798743e-05
Iter: 670 loss: 3.95790303e-05
Iter: 671 loss: 3.95636307e-05
Iter: 672 loss: 3.95804309e-05
Iter: 673 loss: 3.95549941e-05
Iter: 674 loss: 3.95397583e-05
Iter: 675 loss: 3.95335737e-05
Iter: 676 loss: 3.95255411e-05
Iter: 677 loss: 3.95005954e-05
Iter: 678 loss: 3.95294628e-05
Iter: 679 loss: 3.94875169e-05
Iter: 680 loss: 3.94631024e-05
Iter: 681 loss: 3.95382522e-05
Iter: 682 loss: 3.94558738e-05
Iter: 683 loss: 3.942884e-05
Iter: 684 loss: 3.94450472e-05
Iter: 685 loss: 3.9411505e-05
Iter: 686 loss: 3.93828959e-05
Iter: 687 loss: 3.94456729e-05
Iter: 688 loss: 3.93719274e-05
Iter: 689 loss: 3.93437513e-05
Iter: 690 loss: 3.95381503e-05
Iter: 691 loss: 3.9341e-05
Iter: 692 loss: 3.93180308e-05
Iter: 693 loss: 3.93450609e-05
Iter: 694 loss: 3.93058144e-05
Iter: 695 loss: 3.92772636e-05
Iter: 696 loss: 3.93823502e-05
Iter: 697 loss: 3.92704897e-05
Iter: 698 loss: 3.92472757e-05
Iter: 699 loss: 3.9346e-05
Iter: 700 loss: 3.92426e-05
Iter: 701 loss: 3.92186121e-05
Iter: 702 loss: 3.93042465e-05
Iter: 703 loss: 3.92128422e-05
Iter: 704 loss: 3.91980939e-05
Iter: 705 loss: 3.91976e-05
Iter: 706 loss: 3.91859212e-05
Iter: 707 loss: 3.91671565e-05
Iter: 708 loss: 3.91670619e-05
Iter: 709 loss: 3.91467838e-05
Iter: 710 loss: 3.92148359e-05
Iter: 711 loss: 3.91412686e-05
Iter: 712 loss: 3.91224676e-05
Iter: 713 loss: 3.91480608e-05
Iter: 714 loss: 3.91131471e-05
Iter: 715 loss: 3.90909045e-05
Iter: 716 loss: 3.9120423e-05
Iter: 717 loss: 3.90796304e-05
Iter: 718 loss: 3.90535351e-05
Iter: 719 loss: 3.90828209e-05
Iter: 720 loss: 3.90396672e-05
Iter: 721 loss: 3.90126952e-05
Iter: 722 loss: 3.91092362e-05
Iter: 723 loss: 3.90058194e-05
Iter: 724 loss: 3.89774e-05
Iter: 725 loss: 3.90822861e-05
Iter: 726 loss: 3.89704401e-05
Iter: 727 loss: 3.8945891e-05
Iter: 728 loss: 3.89654197e-05
Iter: 729 loss: 3.89311681e-05
Iter: 730 loss: 3.89049819e-05
Iter: 731 loss: 3.90956775e-05
Iter: 732 loss: 3.890281e-05
Iter: 733 loss: 3.88808694e-05
Iter: 734 loss: 3.89479828e-05
Iter: 735 loss: 3.88743138e-05
Iter: 736 loss: 3.88577973e-05
Iter: 737 loss: 3.90870555e-05
Iter: 738 loss: 3.88575281e-05
Iter: 739 loss: 3.88418557e-05
Iter: 740 loss: 3.88802509e-05
Iter: 741 loss: 3.88361514e-05
Iter: 742 loss: 3.88230328e-05
Iter: 743 loss: 3.88212247e-05
Iter: 744 loss: 3.88117624e-05
Iter: 745 loss: 3.87923319e-05
Iter: 746 loss: 3.87778418e-05
Iter: 747 loss: 3.87716318e-05
Iter: 748 loss: 3.87479085e-05
Iter: 749 loss: 3.91140638e-05
Iter: 750 loss: 3.87479049e-05
Iter: 751 loss: 3.87323526e-05
Iter: 752 loss: 3.87137261e-05
Iter: 753 loss: 3.87119726e-05
Iter: 754 loss: 3.86815846e-05
Iter: 755 loss: 3.88035514e-05
Iter: 756 loss: 3.86747342e-05
Iter: 757 loss: 3.86500324e-05
Iter: 758 loss: 3.86935135e-05
Iter: 759 loss: 3.86392931e-05
Iter: 760 loss: 3.861203e-05
Iter: 761 loss: 3.8731574e-05
Iter: 762 loss: 3.86065149e-05
Iter: 763 loss: 3.85858257e-05
Iter: 764 loss: 3.86262109e-05
Iter: 765 loss: 3.85771782e-05
Iter: 766 loss: 3.85513158e-05
Iter: 767 loss: 3.86173851e-05
Iter: 768 loss: 3.85425083e-05
Iter: 769 loss: 3.85209314e-05
Iter: 770 loss: 3.86565807e-05
Iter: 771 loss: 3.85185813e-05
Iter: 772 loss: 3.8503742e-05
Iter: 773 loss: 3.85036401e-05
Iter: 774 loss: 3.84917294e-05
Iter: 775 loss: 3.8484086e-05
Iter: 776 loss: 3.84794366e-05
Iter: 777 loss: 3.84619e-05
Iter: 778 loss: 3.84562081e-05
Iter: 779 loss: 3.8445949e-05
Iter: 780 loss: 3.84207888e-05
Iter: 781 loss: 3.85285093e-05
Iter: 782 loss: 3.84156338e-05
Iter: 783 loss: 3.83931219e-05
Iter: 784 loss: 3.84364466e-05
Iter: 785 loss: 3.83837541e-05
Iter: 786 loss: 3.83625e-05
Iter: 787 loss: 3.84608757e-05
Iter: 788 loss: 3.83584556e-05
Iter: 789 loss: 3.83378e-05
Iter: 790 loss: 3.83271727e-05
Iter: 791 loss: 3.83174847e-05
Iter: 792 loss: 3.82923608e-05
Iter: 793 loss: 3.83650622e-05
Iter: 794 loss: 3.8284481e-05
Iter: 795 loss: 3.82548424e-05
Iter: 796 loss: 3.83881452e-05
Iter: 797 loss: 3.82491053e-05
Iter: 798 loss: 3.82257494e-05
Iter: 799 loss: 3.82526705e-05
Iter: 800 loss: 3.82133694e-05
Iter: 801 loss: 3.81873106e-05
Iter: 802 loss: 3.82828366e-05
Iter: 803 loss: 3.81809587e-05
Iter: 804 loss: 3.81625723e-05
Iter: 805 loss: 3.84561354e-05
Iter: 806 loss: 3.8162525e-05
Iter: 807 loss: 3.81451682e-05
Iter: 808 loss: 3.82188955e-05
Iter: 809 loss: 3.81414575e-05
Iter: 810 loss: 3.81291211e-05
Iter: 811 loss: 3.81176105e-05
Iter: 812 loss: 3.81147038e-05
Iter: 813 loss: 3.80936399e-05
Iter: 814 loss: 3.81525315e-05
Iter: 815 loss: 3.80867496e-05
Iter: 816 loss: 3.80656784e-05
Iter: 817 loss: 3.80961937e-05
Iter: 818 loss: 3.80552265e-05
Iter: 819 loss: 3.80342462e-05
Iter: 820 loss: 3.81360296e-05
Iter: 821 loss: 3.80304737e-05
Iter: 822 loss: 3.80098427e-05
Iter: 823 loss: 3.80303027e-05
Iter: 824 loss: 3.79981648e-05
Iter: 825 loss: 3.79751582e-05
Iter: 826 loss: 3.8031736e-05
Iter: 827 loss: 3.7967111e-05
Iter: 828 loss: 3.79447229e-05
Iter: 829 loss: 3.79878038e-05
Iter: 830 loss: 3.79352132e-05
Iter: 831 loss: 3.79092e-05
Iter: 832 loss: 3.79594894e-05
Iter: 833 loss: 3.78984332e-05
Iter: 834 loss: 3.78707773e-05
Iter: 835 loss: 3.7933969e-05
Iter: 836 loss: 3.78603872e-05
Iter: 837 loss: 3.7833961e-05
Iter: 838 loss: 3.80204219e-05
Iter: 839 loss: 3.78314326e-05
Iter: 840 loss: 3.78193e-05
Iter: 841 loss: 3.78185723e-05
Iter: 842 loss: 3.78055338e-05
Iter: 843 loss: 3.77894976e-05
Iter: 844 loss: 3.77880788e-05
Iter: 845 loss: 3.77701544e-05
Iter: 846 loss: 3.78092518e-05
Iter: 847 loss: 3.77631295e-05
Iter: 848 loss: 3.77440556e-05
Iter: 849 loss: 3.7801663e-05
Iter: 850 loss: 3.77382894e-05
Iter: 851 loss: 3.77185679e-05
Iter: 852 loss: 3.77208125e-05
Iter: 853 loss: 3.77034703e-05
Iter: 854 loss: 3.76800745e-05
Iter: 855 loss: 3.7855516e-05
Iter: 856 loss: 3.76782409e-05
Iter: 857 loss: 3.76600583e-05
Iter: 858 loss: 3.769e-05
Iter: 859 loss: 3.7651851e-05
Iter: 860 loss: 3.76302924e-05
Iter: 861 loss: 3.76462267e-05
Iter: 862 loss: 3.76171411e-05
Iter: 863 loss: 3.75952295e-05
Iter: 864 loss: 3.76622447e-05
Iter: 865 loss: 3.75887175e-05
Iter: 866 loss: 3.75650052e-05
Iter: 867 loss: 3.76329263e-05
Iter: 868 loss: 3.75576419e-05
Iter: 869 loss: 3.75356685e-05
Iter: 870 loss: 3.75779273e-05
Iter: 871 loss: 3.75269083e-05
Iter: 872 loss: 3.75045784e-05
Iter: 873 loss: 3.76706412e-05
Iter: 874 loss: 3.75027885e-05
Iter: 875 loss: 3.74866868e-05
Iter: 876 loss: 3.7486654e-05
Iter: 877 loss: 3.74784649e-05
Iter: 878 loss: 3.74599804e-05
Iter: 879 loss: 3.771099e-05
Iter: 880 loss: 3.7458929e-05
Iter: 881 loss: 3.74363626e-05
Iter: 882 loss: 3.75329073e-05
Iter: 883 loss: 3.74317315e-05
Iter: 884 loss: 3.74106312e-05
Iter: 885 loss: 3.74505e-05
Iter: 886 loss: 3.74015872e-05
Iter: 887 loss: 3.737917e-05
Iter: 888 loss: 3.74606461e-05
Iter: 889 loss: 3.73737639e-05
Iter: 890 loss: 3.73563162e-05
Iter: 891 loss: 3.7373502e-05
Iter: 892 loss: 3.73463554e-05
Iter: 893 loss: 3.73249786e-05
Iter: 894 loss: 3.74653355e-05
Iter: 895 loss: 3.73225703e-05
Iter: 896 loss: 3.73059593e-05
Iter: 897 loss: 3.72964969e-05
Iter: 898 loss: 3.728913e-05
Iter: 899 loss: 3.7266138e-05
Iter: 900 loss: 3.73292714e-05
Iter: 901 loss: 3.72584655e-05
Iter: 902 loss: 3.72353315e-05
Iter: 903 loss: 3.73766743e-05
Iter: 904 loss: 3.72326067e-05
Iter: 905 loss: 3.72129871e-05
Iter: 906 loss: 3.72204086e-05
Iter: 907 loss: 3.71991409e-05
Iter: 908 loss: 3.71977367e-05
Iter: 909 loss: 3.71884526e-05
Iter: 910 loss: 3.71783681e-05
Iter: 911 loss: 3.71685164e-05
Iter: 912 loss: 3.71663918e-05
Iter: 913 loss: 3.71532e-05
Iter: 914 loss: 3.71403439e-05
Iter: 915 loss: 3.71375572e-05
Iter: 916 loss: 3.71167407e-05
Iter: 917 loss: 3.73569419e-05
Iter: 918 loss: 3.71163624e-05
Iter: 919 loss: 3.71033711e-05
Iter: 920 loss: 3.71174792e-05
Iter: 921 loss: 3.70965e-05
Iter: 922 loss: 3.70774796e-05
Iter: 923 loss: 3.70855414e-05
Iter: 924 loss: 3.70643829e-05
Iter: 925 loss: 3.70434354e-05
Iter: 926 loss: 3.71605238e-05
Iter: 927 loss: 3.70405032e-05
Iter: 928 loss: 3.702088e-05
Iter: 929 loss: 3.70559719e-05
Iter: 930 loss: 3.70122543e-05
Iter: 931 loss: 3.69908157e-05
Iter: 932 loss: 3.7024176e-05
Iter: 933 loss: 3.69808622e-05
Iter: 934 loss: 3.69585323e-05
Iter: 935 loss: 3.69836525e-05
Iter: 936 loss: 3.69464433e-05
Iter: 937 loss: 3.69200934e-05
Iter: 938 loss: 3.70265843e-05
Iter: 939 loss: 3.69142363e-05
Iter: 940 loss: 3.68927504e-05
Iter: 941 loss: 3.69818081e-05
Iter: 942 loss: 3.68882793e-05
Iter: 943 loss: 3.68739493e-05
Iter: 944 loss: 3.68730907e-05
Iter: 945 loss: 3.68628207e-05
Iter: 946 loss: 3.68481269e-05
Iter: 947 loss: 3.68475085e-05
Iter: 948 loss: 3.6832429e-05
Iter: 949 loss: 3.68414658e-05
Iter: 950 loss: 3.68228066e-05
Iter: 951 loss: 3.68034962e-05
Iter: 952 loss: 3.69481204e-05
Iter: 953 loss: 3.68019319e-05
Iter: 954 loss: 3.67854082e-05
Iter: 955 loss: 3.68094406e-05
Iter: 956 loss: 3.67773318e-05
Iter: 957 loss: 3.67603607e-05
Iter: 958 loss: 3.67662651e-05
Iter: 959 loss: 3.67485627e-05
Iter: 960 loss: 3.67261928e-05
Iter: 961 loss: 3.68658e-05
Iter: 962 loss: 3.67233806e-05
Iter: 963 loss: 3.67046741e-05
Iter: 964 loss: 3.6733647e-05
Iter: 965 loss: 3.66959212e-05
Iter: 966 loss: 3.66754684e-05
Iter: 967 loss: 3.67217071e-05
Iter: 968 loss: 3.66677414e-05
Iter: 969 loss: 3.66471795e-05
Iter: 970 loss: 3.66877321e-05
Iter: 971 loss: 3.66386412e-05
Iter: 972 loss: 3.66181957e-05
Iter: 973 loss: 3.66507738e-05
Iter: 974 loss: 3.66087115e-05
Iter: 975 loss: 3.6594487e-05
Iter: 976 loss: 3.6594116e-05
Iter: 977 loss: 3.65785527e-05
Iter: 978 loss: 3.66206114e-05
Iter: 979 loss: 3.65736341e-05
Iter: 980 loss: 3.65603919e-05
Iter: 981 loss: 3.65456071e-05
Iter: 982 loss: 3.6543468e-05
Iter: 983 loss: 3.65256055e-05
Iter: 984 loss: 3.6534635e-05
Iter: 985 loss: 3.65133637e-05
Iter: 986 loss: 3.64932093e-05
Iter: 987 loss: 3.67388238e-05
Iter: 988 loss: 3.64928928e-05
Iter: 989 loss: 3.64772131e-05
Iter: 990 loss: 3.65004817e-05
Iter: 991 loss: 3.64697771e-05
Iter: 992 loss: 3.6450896e-05
Iter: 993 loss: 3.64827538e-05
Iter: 994 loss: 3.6442776e-05
Iter: 995 loss: 3.64242151e-05
Iter: 996 loss: 3.64243897e-05
Iter: 997 loss: 3.64093503e-05
Iter: 998 loss: 3.63886938e-05
Iter: 999 loss: 3.66712811e-05
Iter: 1000 loss: 3.63885847e-05
Iter: 1001 loss: 3.63720974e-05
Iter: 1002 loss: 3.63783e-05
Iter: 1003 loss: 3.63604559e-05
Iter: 1004 loss: 3.63405197e-05
Iter: 1005 loss: 3.6350677e-05
Iter: 1006 loss: 3.63272338e-05
Iter: 1007 loss: 3.63050422e-05
Iter: 1008 loss: 3.65165251e-05
Iter: 1009 loss: 3.63042e-05
Iter: 1010 loss: 3.62909195e-05
Iter: 1011 loss: 3.63818835e-05
Iter: 1012 loss: 3.62896644e-05
Iter: 1013 loss: 3.62726714e-05
Iter: 1014 loss: 3.63068393e-05
Iter: 1015 loss: 3.6265792e-05
Iter: 1016 loss: 3.62545179e-05
Iter: 1017 loss: 3.62344144e-05
Iter: 1018 loss: 3.67349749e-05
Iter: 1019 loss: 3.62343235e-05
Iter: 1020 loss: 3.62112405e-05
Iter: 1021 loss: 3.63466315e-05
Iter: 1022 loss: 3.62082792e-05
Iter: 1023 loss: 3.61912098e-05
Iter: 1024 loss: 3.6293517e-05
Iter: 1025 loss: 3.61891434e-05
Iter: 1026 loss: 3.61716957e-05
Iter: 1027 loss: 3.62018509e-05
Iter: 1028 loss: 3.61641e-05
Iter: 1029 loss: 3.61456187e-05
Iter: 1030 loss: 3.61620769e-05
Iter: 1031 loss: 3.61348502e-05
Iter: 1032 loss: 3.61153652e-05
Iter: 1033 loss: 3.62071187e-05
Iter: 1034 loss: 3.61118218e-05
Iter: 1035 loss: 3.6093581e-05
Iter: 1036 loss: 3.61233506e-05
Iter: 1037 loss: 3.60854319e-05
Iter: 1038 loss: 3.60637969e-05
Iter: 1039 loss: 3.6165904e-05
Iter: 1040 loss: 3.60598387e-05
Iter: 1041 loss: 3.60435297e-05
Iter: 1042 loss: 3.60454214e-05
Iter: 1043 loss: 3.60310587e-05
Iter: 1044 loss: 3.60097911e-05
Iter: 1045 loss: 3.61031853e-05
Iter: 1046 loss: 3.60056e-05
Iter: 1047 loss: 3.59927799e-05
Iter: 1048 loss: 3.59925398e-05
Iter: 1049 loss: 3.59786791e-05
Iter: 1050 loss: 3.59777114e-05
Iter: 1051 loss: 3.59671503e-05
Iter: 1052 loss: 3.5954472e-05
Iter: 1053 loss: 3.59426558e-05
Iter: 1054 loss: 3.59395635e-05
Iter: 1055 loss: 3.59192709e-05
Iter: 1056 loss: 3.59405312e-05
Iter: 1057 loss: 3.59076039e-05
Iter: 1058 loss: 3.588869e-05
Iter: 1059 loss: 3.61320854e-05
Iter: 1060 loss: 3.58885e-05
Iter: 1061 loss: 3.58713223e-05
Iter: 1062 loss: 3.58972393e-05
Iter: 1063 loss: 3.58632751e-05
Iter: 1064 loss: 3.5843419e-05
Iter: 1065 loss: 3.58837497e-05
Iter: 1066 loss: 3.58353027e-05
Iter: 1067 loss: 3.58152756e-05
Iter: 1068 loss: 3.58207035e-05
Iter: 1069 loss: 3.5800469e-05
Iter: 1070 loss: 3.57758072e-05
Iter: 1071 loss: 3.59214173e-05
Iter: 1072 loss: 3.5772704e-05
Iter: 1073 loss: 3.57517565e-05
Iter: 1074 loss: 3.58645448e-05
Iter: 1075 loss: 3.57486788e-05
Iter: 1076 loss: 3.57316167e-05
Iter: 1077 loss: 3.57238678e-05
Iter: 1078 loss: 3.57152676e-05
Iter: 1079 loss: 3.56930323e-05
Iter: 1080 loss: 3.58540892e-05
Iter: 1081 loss: 3.5691206e-05
Iter: 1082 loss: 3.56792734e-05
Iter: 1083 loss: 3.56790115e-05
Iter: 1084 loss: 3.56666824e-05
Iter: 1085 loss: 3.56512755e-05
Iter: 1086 loss: 3.56500896e-05
Iter: 1087 loss: 3.56361707e-05
Iter: 1088 loss: 3.56375785e-05
Iter: 1089 loss: 3.56257224e-05
Iter: 1090 loss: 3.5602905e-05
Iter: 1091 loss: 3.56105847e-05
Iter: 1092 loss: 3.55870288e-05
Iter: 1093 loss: 3.55649063e-05
Iter: 1094 loss: 3.58400357e-05
Iter: 1095 loss: 3.55646844e-05
Iter: 1096 loss: 3.55490956e-05
Iter: 1097 loss: 3.56097153e-05
Iter: 1098 loss: 3.5545374e-05
Iter: 1099 loss: 3.55282245e-05
Iter: 1100 loss: 3.55449556e-05
Iter: 1101 loss: 3.55184238e-05
Iter: 1102 loss: 3.55001685e-05
Iter: 1103 loss: 3.55246411e-05
Iter: 1104 loss: 3.5491059e-05
Iter: 1105 loss: 3.54706499e-05
Iter: 1106 loss: 3.5521829e-05
Iter: 1107 loss: 3.54634612e-05
Iter: 1108 loss: 3.5441044e-05
Iter: 1109 loss: 3.55612538e-05
Iter: 1110 loss: 3.54378e-05
Iter: 1111 loss: 3.54210861e-05
Iter: 1112 loss: 3.54764343e-05
Iter: 1113 loss: 3.54165677e-05
Iter: 1114 loss: 3.53988071e-05
Iter: 1115 loss: 3.53928481e-05
Iter: 1116 loss: 3.53826581e-05
Iter: 1117 loss: 3.53842297e-05
Iter: 1118 loss: 3.5371897e-05
Iter: 1119 loss: 3.53644791e-05
Iter: 1120 loss: 3.53458818e-05
Iter: 1121 loss: 3.55180382e-05
Iter: 1122 loss: 3.53431496e-05
Iter: 1123 loss: 3.53226023e-05
Iter: 1124 loss: 3.53756404e-05
Iter: 1125 loss: 3.5315541e-05
Iter: 1126 loss: 3.52958341e-05
Iter: 1127 loss: 3.53353316e-05
Iter: 1128 loss: 3.52877323e-05
Iter: 1129 loss: 3.52661664e-05
Iter: 1130 loss: 3.53310388e-05
Iter: 1131 loss: 3.52594943e-05
Iter: 1132 loss: 3.5240857e-05
Iter: 1133 loss: 3.53627911e-05
Iter: 1134 loss: 3.52385869e-05
Iter: 1135 loss: 3.52216375e-05
Iter: 1136 loss: 3.52851494e-05
Iter: 1137 loss: 3.52174793e-05
Iter: 1138 loss: 3.52010502e-05
Iter: 1139 loss: 3.5197103e-05
Iter: 1140 loss: 3.51866111e-05
Iter: 1141 loss: 3.51639901e-05
Iter: 1142 loss: 3.52163261e-05
Iter: 1143 loss: 3.51557646e-05
Iter: 1144 loss: 3.51349445e-05
Iter: 1145 loss: 3.52481475e-05
Iter: 1146 loss: 3.5131914e-05
Iter: 1147 loss: 3.51130875e-05
Iter: 1148 loss: 3.51494455e-05
Iter: 1149 loss: 3.51051058e-05
Iter: 1150 loss: 3.50880073e-05
Iter: 1151 loss: 3.52368806e-05
Iter: 1152 loss: 3.50873452e-05
Iter: 1153 loss: 3.50738701e-05
Iter: 1154 loss: 3.50925657e-05
Iter: 1155 loss: 3.50674709e-05
Iter: 1156 loss: 3.50509872e-05
Iter: 1157 loss: 3.52391289e-05
Iter: 1158 loss: 3.50508926e-05
Iter: 1159 loss: 3.50430018e-05
Iter: 1160 loss: 3.50255759e-05
Iter: 1161 loss: 3.52638199e-05
Iter: 1162 loss: 3.50246191e-05
Iter: 1163 loss: 3.50039627e-05
Iter: 1164 loss: 3.50088667e-05
Iter: 1165 loss: 3.49889269e-05
Iter: 1166 loss: 3.4966426e-05
Iter: 1167 loss: 3.51841736e-05
Iter: 1168 loss: 3.49655784e-05
Iter: 1169 loss: 3.49464353e-05
Iter: 1170 loss: 3.49586589e-05
Iter: 1171 loss: 3.49341026e-05
Iter: 1172 loss: 3.49165202e-05
Iter: 1173 loss: 3.51104682e-05
Iter: 1174 loss: 3.49162183e-05
Iter: 1175 loss: 3.49007641e-05
Iter: 1176 loss: 3.49429e-05
Iter: 1177 loss: 3.48955691e-05
Iter: 1178 loss: 3.48820031e-05
Iter: 1179 loss: 3.48717949e-05
Iter: 1180 loss: 3.48670328e-05
Iter: 1181 loss: 3.48461763e-05
Iter: 1182 loss: 3.49766124e-05
Iter: 1183 loss: 3.48438261e-05
Iter: 1184 loss: 3.48267931e-05
Iter: 1185 loss: 3.48441099e-05
Iter: 1186 loss: 3.48173853e-05
Iter: 1187 loss: 3.47985915e-05
Iter: 1188 loss: 3.49221518e-05
Iter: 1189 loss: 3.47966597e-05
Iter: 1190 loss: 3.47803107e-05
Iter: 1191 loss: 3.48335452e-05
Iter: 1192 loss: 3.47757559e-05
Iter: 1193 loss: 3.47636924e-05
Iter: 1194 loss: 3.49120128e-05
Iter: 1195 loss: 3.47635e-05
Iter: 1196 loss: 3.47522109e-05
Iter: 1197 loss: 3.47696805e-05
Iter: 1198 loss: 3.47468667e-05
Iter: 1199 loss: 3.47362511e-05
Iter: 1200 loss: 3.47184505e-05
Iter: 1201 loss: 3.47181667e-05
Iter: 1202 loss: 3.46989e-05
Iter: 1203 loss: 3.47751738e-05
Iter: 1204 loss: 3.46943e-05
Iter: 1205 loss: 3.46759698e-05
Iter: 1206 loss: 3.46776724e-05
Iter: 1207 loss: 3.46618035e-05
Iter: 1208 loss: 3.46405795e-05
Iter: 1209 loss: 3.4786488e-05
Iter: 1210 loss: 3.46385641e-05
Iter: 1211 loss: 3.4619603e-05
Iter: 1212 loss: 3.47213572e-05
Iter: 1213 loss: 3.4616678e-05
Iter: 1214 loss: 3.45994849e-05
Iter: 1215 loss: 3.46589077e-05
Iter: 1216 loss: 3.45947919e-05
Iter: 1217 loss: 3.45805565e-05
Iter: 1218 loss: 3.45921726e-05
Iter: 1219 loss: 3.457188e-05
Iter: 1220 loss: 3.45542539e-05
Iter: 1221 loss: 3.45678709e-05
Iter: 1222 loss: 3.45434055e-05
Iter: 1223 loss: 3.45242e-05
Iter: 1224 loss: 3.45903391e-05
Iter: 1225 loss: 3.45191729e-05
Iter: 1226 loss: 3.44998844e-05
Iter: 1227 loss: 3.4591736e-05
Iter: 1228 loss: 3.44962245e-05
Iter: 1229 loss: 3.44853397e-05
Iter: 1230 loss: 3.44855071e-05
Iter: 1231 loss: 3.44737746e-05
Iter: 1232 loss: 3.44766959e-05
Iter: 1233 loss: 3.44652362e-05
Iter: 1234 loss: 3.44539076e-05
Iter: 1235 loss: 3.4465229e-05
Iter: 1236 loss: 3.44476321e-05
Iter: 1237 loss: 3.44331711e-05
Iter: 1238 loss: 3.44504951e-05
Iter: 1239 loss: 3.44259934e-05
Iter: 1240 loss: 3.44093714e-05
Iter: 1241 loss: 3.44295258e-05
Iter: 1242 loss: 3.44007276e-05
Iter: 1243 loss: 3.43829488e-05
Iter: 1244 loss: 3.44192485e-05
Iter: 1245 loss: 3.43755964e-05
Iter: 1246 loss: 3.43577485e-05
Iter: 1247 loss: 3.44125583e-05
Iter: 1248 loss: 3.43523534e-05
Iter: 1249 loss: 3.43351567e-05
Iter: 1250 loss: 3.44482869e-05
Iter: 1251 loss: 3.43335814e-05
Iter: 1252 loss: 3.4319597e-05
Iter: 1253 loss: 3.43771026e-05
Iter: 1254 loss: 3.43165375e-05
Iter: 1255 loss: 3.43036372e-05
Iter: 1256 loss: 3.43008578e-05
Iter: 1257 loss: 3.42923668e-05
Iter: 1258 loss: 3.42743442e-05
Iter: 1259 loss: 3.43100473e-05
Iter: 1260 loss: 3.42669955e-05
Iter: 1261 loss: 3.42471e-05
Iter: 1262 loss: 3.42830681e-05
Iter: 1263 loss: 3.42385829e-05
Iter: 1264 loss: 3.42219828e-05
Iter: 1265 loss: 3.44194486e-05
Iter: 1266 loss: 3.42218627e-05
Iter: 1267 loss: 3.42088679e-05
Iter: 1268 loss: 3.43592183e-05
Iter: 1269 loss: 3.42087806e-05
Iter: 1270 loss: 3.42007479e-05
Iter: 1271 loss: 3.41823834e-05
Iter: 1272 loss: 3.44090586e-05
Iter: 1273 loss: 3.41809136e-05
Iter: 1274 loss: 3.41635496e-05
Iter: 1275 loss: 3.41892e-05
Iter: 1276 loss: 3.41553896e-05
Iter: 1277 loss: 3.41393243e-05
Iter: 1278 loss: 3.43899e-05
Iter: 1279 loss: 3.41392697e-05
Iter: 1280 loss: 3.41266459e-05
Iter: 1281 loss: 3.41274863e-05
Iter: 1282 loss: 3.41167251e-05
Iter: 1283 loss: 3.40993502e-05
Iter: 1284 loss: 3.41392079e-05
Iter: 1285 loss: 3.4092609e-05
Iter: 1286 loss: 3.40761e-05
Iter: 1287 loss: 3.40918705e-05
Iter: 1288 loss: 3.40666884e-05
Iter: 1289 loss: 3.4050885e-05
Iter: 1290 loss: 3.42498206e-05
Iter: 1291 loss: 3.40506667e-05
Iter: 1292 loss: 3.40366678e-05
Iter: 1293 loss: 3.40521037e-05
Iter: 1294 loss: 3.40291153e-05
Iter: 1295 loss: 3.4014025e-05
Iter: 1296 loss: 3.40298429e-05
Iter: 1297 loss: 3.40054103e-05
Iter: 1298 loss: 3.39878825e-05
Iter: 1299 loss: 3.40557235e-05
Iter: 1300 loss: 3.39835606e-05
Iter: 1301 loss: 3.39699327e-05
Iter: 1302 loss: 3.39870276e-05
Iter: 1303 loss: 3.39629551e-05
Iter: 1304 loss: 3.39583275e-05
Iter: 1305 loss: 3.39539947e-05
Iter: 1306 loss: 3.39481412e-05
Iter: 1307 loss: 3.39355392e-05
Iter: 1308 loss: 3.41367595e-05
Iter: 1309 loss: 3.39350518e-05
Iter: 1310 loss: 3.39215039e-05
Iter: 1311 loss: 3.39281978e-05
Iter: 1312 loss: 3.39125654e-05
Iter: 1313 loss: 3.38942555e-05
Iter: 1314 loss: 3.39286926e-05
Iter: 1315 loss: 3.38865393e-05
Iter: 1316 loss: 3.38674727e-05
Iter: 1317 loss: 3.39796679e-05
Iter: 1318 loss: 3.38649697e-05
Iter: 1319 loss: 3.38495483e-05
Iter: 1320 loss: 3.38997052e-05
Iter: 1321 loss: 3.384499e-05
Iter: 1322 loss: 3.38307145e-05
Iter: 1323 loss: 3.39213e-05
Iter: 1324 loss: 3.38290447e-05
Iter: 1325 loss: 3.38168575e-05
Iter: 1326 loss: 3.38027676e-05
Iter: 1327 loss: 3.38010359e-05
Iter: 1328 loss: 3.37890378e-05
Iter: 1329 loss: 3.37885685e-05
Iter: 1330 loss: 3.37783204e-05
Iter: 1331 loss: 3.37714082e-05
Iter: 1332 loss: 3.37678e-05
Iter: 1333 loss: 3.37529309e-05
Iter: 1334 loss: 3.38092177e-05
Iter: 1335 loss: 3.37492456e-05
Iter: 1336 loss: 3.37352139e-05
Iter: 1337 loss: 3.37409765e-05
Iter: 1338 loss: 3.37252532e-05
Iter: 1339 loss: 3.37178353e-05
Iter: 1340 loss: 3.37157835e-05
Iter: 1341 loss: 3.37050733e-05
Iter: 1342 loss: 3.36970588e-05
Iter: 1343 loss: 3.36936137e-05
Iter: 1344 loss: 3.36806152e-05
Iter: 1345 loss: 3.36868106e-05
Iter: 1346 loss: 3.36718876e-05
Iter: 1347 loss: 3.36576486e-05
Iter: 1348 loss: 3.36852609e-05
Iter: 1349 loss: 3.36517187e-05
Iter: 1350 loss: 3.3634904e-05
Iter: 1351 loss: 3.36521407e-05
Iter: 1352 loss: 3.36253979e-05
Iter: 1353 loss: 3.36100857e-05
Iter: 1354 loss: 3.37298043e-05
Iter: 1355 loss: 3.36091543e-05
Iter: 1356 loss: 3.35959267e-05
Iter: 1357 loss: 3.36578669e-05
Iter: 1358 loss: 3.35936893e-05
Iter: 1359 loss: 3.35805598e-05
Iter: 1360 loss: 3.358908e-05
Iter: 1361 loss: 3.35723234e-05
Iter: 1362 loss: 3.35579789e-05
Iter: 1363 loss: 3.36189842e-05
Iter: 1364 loss: 3.35549448e-05
Iter: 1365 loss: 3.35410041e-05
Iter: 1366 loss: 3.35844525e-05
Iter: 1367 loss: 3.35370969e-05
Iter: 1368 loss: 3.35233563e-05
Iter: 1369 loss: 3.35271543e-05
Iter: 1370 loss: 3.35133591e-05
Iter: 1371 loss: 3.34982e-05
Iter: 1372 loss: 3.35390141e-05
Iter: 1373 loss: 3.34932083e-05
Iter: 1374 loss: 3.34853503e-05
Iter: 1375 loss: 3.34838624e-05
Iter: 1376 loss: 3.34748183e-05
Iter: 1377 loss: 3.34618926e-05
Iter: 1378 loss: 3.34616561e-05
Iter: 1379 loss: 3.34492361e-05
Iter: 1380 loss: 3.34630167e-05
Iter: 1381 loss: 3.34426913e-05
Iter: 1382 loss: 3.34286e-05
Iter: 1383 loss: 3.34567158e-05
Iter: 1384 loss: 3.34228098e-05
Iter: 1385 loss: 3.34080651e-05
Iter: 1386 loss: 3.34344222e-05
Iter: 1387 loss: 3.34018041e-05
Iter: 1388 loss: 3.33837415e-05
Iter: 1389 loss: 3.34342476e-05
Iter: 1390 loss: 3.33778116e-05
Iter: 1391 loss: 3.33633325e-05
Iter: 1392 loss: 3.34778597e-05
Iter: 1393 loss: 3.33624448e-05
Iter: 1394 loss: 3.33499484e-05
Iter: 1395 loss: 3.33992502e-05
Iter: 1396 loss: 3.33470525e-05
Iter: 1397 loss: 3.33361604e-05
Iter: 1398 loss: 3.33460448e-05
Iter: 1399 loss: 3.33297794e-05
Iter: 1400 loss: 3.3316097e-05
Iter: 1401 loss: 3.33478747e-05
Iter: 1402 loss: 3.33106182e-05
Iter: 1403 loss: 3.32959316e-05
Iter: 1404 loss: 3.33584758e-05
Iter: 1405 loss: 3.32928685e-05
Iter: 1406 loss: 3.32807613e-05
Iter: 1407 loss: 3.32782656e-05
Iter: 1408 loss: 3.32702e-05
Iter: 1409 loss: 3.32634154e-05
Iter: 1410 loss: 3.32608397e-05
Iter: 1411 loss: 3.32520976e-05
Iter: 1412 loss: 3.32443124e-05
Iter: 1413 loss: 3.32420095e-05
Iter: 1414 loss: 3.32319614e-05
Iter: 1415 loss: 3.32163909e-05
Iter: 1416 loss: 3.32162526e-05
Iter: 1417 loss: 3.31984702e-05
Iter: 1418 loss: 3.33667413e-05
Iter: 1419 loss: 3.3197648e-05
Iter: 1420 loss: 3.31835472e-05
Iter: 1421 loss: 3.3176424e-05
Iter: 1422 loss: 3.31698866e-05
Iter: 1423 loss: 3.31499978e-05
Iter: 1424 loss: 3.32659838e-05
Iter: 1425 loss: 3.31473202e-05
Iter: 1426 loss: 3.31316478e-05
Iter: 1427 loss: 3.32878335e-05
Iter: 1428 loss: 3.3131193e-05
Iter: 1429 loss: 3.31206211e-05
Iter: 1430 loss: 3.31644042e-05
Iter: 1431 loss: 3.31185911e-05
Iter: 1432 loss: 3.31070878e-05
Iter: 1433 loss: 3.3111497e-05
Iter: 1434 loss: 3.30992698e-05
Iter: 1435 loss: 3.30882176e-05
Iter: 1436 loss: 3.31144038e-05
Iter: 1437 loss: 3.30841358e-05
Iter: 1438 loss: 3.30705188e-05
Iter: 1439 loss: 3.31238116e-05
Iter: 1440 loss: 3.30672701e-05
Iter: 1441 loss: 3.30570329e-05
Iter: 1442 loss: 3.30793155e-05
Iter: 1443 loss: 3.30530238e-05
Iter: 1444 loss: 3.30422336e-05
Iter: 1445 loss: 3.31909469e-05
Iter: 1446 loss: 3.30422845e-05
Iter: 1447 loss: 3.30353032e-05
Iter: 1448 loss: 3.30261682e-05
Iter: 1449 loss: 3.30257753e-05
Iter: 1450 loss: 3.30142793e-05
Iter: 1451 loss: 3.30028051e-05
Iter: 1452 loss: 3.30003168e-05
Iter: 1453 loss: 3.29843315e-05
Iter: 1454 loss: 3.31167757e-05
Iter: 1455 loss: 3.29833456e-05
Iter: 1456 loss: 3.29689428e-05
Iter: 1457 loss: 3.29763861e-05
Iter: 1458 loss: 3.29591567e-05
Iter: 1459 loss: 3.29431023e-05
Iter: 1460 loss: 3.30547773e-05
Iter: 1461 loss: 3.29415852e-05
Iter: 1462 loss: 3.29277136e-05
Iter: 1463 loss: 3.29489194e-05
Iter: 1464 loss: 3.29212126e-05
Iter: 1465 loss: 3.29085233e-05
Iter: 1466 loss: 3.29085451e-05
Iter: 1467 loss: 3.29002069e-05
Iter: 1468 loss: 3.28896494e-05
Iter: 1469 loss: 3.28890383e-05
Iter: 1470 loss: 3.28732567e-05
Iter: 1471 loss: 3.29409668e-05
Iter: 1472 loss: 3.28700626e-05
Iter: 1473 loss: 3.28563183e-05
Iter: 1474 loss: 3.29109462e-05
Iter: 1475 loss: 3.2853226e-05
Iter: 1476 loss: 3.28428214e-05
Iter: 1477 loss: 3.29289e-05
Iter: 1478 loss: 3.28422684e-05
Iter: 1479 loss: 3.28318e-05
Iter: 1480 loss: 3.28600763e-05
Iter: 1481 loss: 3.28283822e-05
Iter: 1482 loss: 3.2820295e-05
Iter: 1483 loss: 3.28130482e-05
Iter: 1484 loss: 3.28111055e-05
Iter: 1485 loss: 3.27983798e-05
Iter: 1486 loss: 3.28020251e-05
Iter: 1487 loss: 3.27892922e-05
Iter: 1488 loss: 3.2775104e-05
Iter: 1489 loss: 3.28610076e-05
Iter: 1490 loss: 3.27733869e-05
Iter: 1491 loss: 3.2759548e-05
Iter: 1492 loss: 3.27618109e-05
Iter: 1493 loss: 3.27492344e-05
Iter: 1494 loss: 3.27328635e-05
Iter: 1495 loss: 3.27728048e-05
Iter: 1496 loss: 3.27272e-05
Iter: 1497 loss: 3.27133312e-05
Iter: 1498 loss: 3.29024515e-05
Iter: 1499 loss: 3.27133384e-05
Iter: 1500 loss: 3.2701726e-05
Iter: 1501 loss: 3.27431553e-05
Iter: 1502 loss: 3.26986556e-05
Iter: 1503 loss: 3.26890658e-05
Iter: 1504 loss: 3.26960835e-05
Iter: 1505 loss: 3.26828e-05
Iter: 1506 loss: 3.2670825e-05
Iter: 1507 loss: 3.26930603e-05
Iter: 1508 loss: 3.26655827e-05
Iter: 1509 loss: 3.26538502e-05
Iter: 1510 loss: 3.27368543e-05
Iter: 1511 loss: 3.26529625e-05
Iter: 1512 loss: 3.26438931e-05
Iter: 1513 loss: 3.27345624e-05
Iter: 1514 loss: 3.26437221e-05
Iter: 1515 loss: 3.26359077e-05
Iter: 1516 loss: 3.26329973e-05
Iter: 1517 loss: 3.26286754e-05
Iter: 1518 loss: 3.26190529e-05
Iter: 1519 loss: 3.261253e-05
Iter: 1520 loss: 3.26089394e-05
Iter: 1521 loss: 3.25956862e-05
Iter: 1522 loss: 3.26274967e-05
Iter: 1523 loss: 3.25908695e-05
Iter: 1524 loss: 3.25761284e-05
Iter: 1525 loss: 3.26338413e-05
Iter: 1526 loss: 3.25726069e-05
Iter: 1527 loss: 3.25593319e-05
Iter: 1528 loss: 3.25740402e-05
Iter: 1529 loss: 3.25522706e-05
Iter: 1530 loss: 3.25384681e-05
Iter: 1531 loss: 3.25960282e-05
Iter: 1532 loss: 3.25354631e-05
Iter: 1533 loss: 3.25220899e-05
Iter: 1534 loss: 3.25527581e-05
Iter: 1535 loss: 3.25171641e-05
Iter: 1536 loss: 3.25058609e-05
Iter: 1537 loss: 3.25058609e-05
Iter: 1538 loss: 3.24985158e-05
Iter: 1539 loss: 3.24846696e-05
Iter: 1540 loss: 3.27861562e-05
Iter: 1541 loss: 3.24845896e-05
Iter: 1542 loss: 3.24714347e-05
Iter: 1543 loss: 3.26044465e-05
Iter: 1544 loss: 3.24710636e-05
Iter: 1545 loss: 3.24619577e-05
Iter: 1546 loss: 3.25209912e-05
Iter: 1547 loss: 3.24609064e-05
Iter: 1548 loss: 3.24512148e-05
Iter: 1549 loss: 3.24880893e-05
Iter: 1550 loss: 3.24492066e-05
Iter: 1551 loss: 3.2441254e-05
Iter: 1552 loss: 3.24397624e-05
Iter: 1553 loss: 3.24345e-05
Iter: 1554 loss: 3.24249559e-05
Iter: 1555 loss: 3.24288885e-05
Iter: 1556 loss: 3.24183566e-05
Iter: 1557 loss: 3.24056055e-05
Iter: 1558 loss: 3.24349821e-05
Iter: 1559 loss: 3.24008e-05
Iter: 1560 loss: 3.23882305e-05
Iter: 1561 loss: 3.24058565e-05
Iter: 1562 loss: 3.23820277e-05
Iter: 1563 loss: 3.23678287e-05
Iter: 1564 loss: 3.24324428e-05
Iter: 1565 loss: 3.23648856e-05
Iter: 1566 loss: 3.235244e-05
Iter: 1567 loss: 3.23846325e-05
Iter: 1568 loss: 3.23482236e-05
Iter: 1569 loss: 3.23363929e-05
Iter: 1570 loss: 3.2357937e-05
Iter: 1571 loss: 3.23312343e-05
Iter: 1572 loss: 3.2319731e-05
Iter: 1573 loss: 3.23197237e-05
Iter: 1574 loss: 3.23123968e-05
Iter: 1575 loss: 3.23099484e-05
Iter: 1576 loss: 3.23058048e-05
Iter: 1577 loss: 3.22946798e-05
Iter: 1578 loss: 3.23067288e-05
Iter: 1579 loss: 3.22888591e-05
Iter: 1580 loss: 3.22820597e-05
Iter: 1581 loss: 3.22815868e-05
Iter: 1582 loss: 3.22743617e-05
Iter: 1583 loss: 3.22763808e-05
Iter: 1584 loss: 3.22690539e-05
Iter: 1585 loss: 3.22615888e-05
Iter: 1586 loss: 3.22636042e-05
Iter: 1587 loss: 3.22560809e-05
Iter: 1588 loss: 3.22448686e-05
Iter: 1589 loss: 3.22593551e-05
Iter: 1590 loss: 3.22390879e-05
Iter: 1591 loss: 3.22288688e-05
Iter: 1592 loss: 3.22249107e-05
Iter: 1593 loss: 3.22193082e-05
Iter: 1594 loss: 3.22048909e-05
Iter: 1595 loss: 3.23376917e-05
Iter: 1596 loss: 3.22041742e-05
Iter: 1597 loss: 3.21924308e-05
Iter: 1598 loss: 3.21952539e-05
Iter: 1599 loss: 3.21839507e-05
Iter: 1600 loss: 3.21717889e-05
Iter: 1601 loss: 3.22473279e-05
Iter: 1602 loss: 3.21703046e-05
Iter: 1603 loss: 3.21580592e-05
Iter: 1604 loss: 3.21712432e-05
Iter: 1605 loss: 3.21513289e-05
Iter: 1606 loss: 3.2142525e-05
Iter: 1607 loss: 3.21422631e-05
Iter: 1608 loss: 3.21350308e-05
Iter: 1609 loss: 3.21224616e-05
Iter: 1610 loss: 3.21224143e-05
Iter: 1611 loss: 3.21113912e-05
Iter: 1612 loss: 3.22143569e-05
Iter: 1613 loss: 3.21107582e-05
Iter: 1614 loss: 3.21044245e-05
Iter: 1615 loss: 3.21045518e-05
Iter: 1616 loss: 3.20987747e-05
Iter: 1617 loss: 3.20872859e-05
Iter: 1618 loss: 3.22942578e-05
Iter: 1619 loss: 3.20870167e-05
Iter: 1620 loss: 3.20759646e-05
Iter: 1621 loss: 3.21373955e-05
Iter: 1622 loss: 3.20744657e-05
Iter: 1623 loss: 3.20646577e-05
Iter: 1624 loss: 3.20723448e-05
Iter: 1625 loss: 3.2058746e-05
Iter: 1626 loss: 3.20467298e-05
Iter: 1627 loss: 3.20577128e-05
Iter: 1628 loss: 3.20399085e-05
Iter: 1629 loss: 3.20252911e-05
Iter: 1630 loss: 3.20816325e-05
Iter: 1631 loss: 3.20218714e-05
Iter: 1632 loss: 3.20105464e-05
Iter: 1633 loss: 3.2035874e-05
Iter: 1634 loss: 3.20062391e-05
Iter: 1635 loss: 3.19940882e-05
Iter: 1636 loss: 3.20350118e-05
Iter: 1637 loss: 3.19909341e-05
Iter: 1638 loss: 3.19792562e-05
Iter: 1639 loss: 3.20197578e-05
Iter: 1640 loss: 3.19763276e-05
Iter: 1641 loss: 3.19664e-05
Iter: 1642 loss: 3.20639156e-05
Iter: 1643 loss: 3.1966e-05
Iter: 1644 loss: 3.1957672e-05
Iter: 1645 loss: 3.19621904e-05
Iter: 1646 loss: 3.19522478e-05
Iter: 1647 loss: 3.19430692e-05
Iter: 1648 loss: 3.19536884e-05
Iter: 1649 loss: 3.1937805e-05
Iter: 1650 loss: 3.19305036e-05
Iter: 1651 loss: 3.19298852e-05
Iter: 1652 loss: 3.19258215e-05
Iter: 1653 loss: 3.19165774e-05
Iter: 1654 loss: 3.20449617e-05
Iter: 1655 loss: 3.19162173e-05
Iter: 1656 loss: 3.19048195e-05
Iter: 1657 loss: 3.19339888e-05
Iter: 1658 loss: 3.19009487e-05
Iter: 1659 loss: 3.18898165e-05
Iter: 1660 loss: 3.19397732e-05
Iter: 1661 loss: 3.18875791e-05
Iter: 1662 loss: 3.18766688e-05
Iter: 1663 loss: 3.18772218e-05
Iter: 1664 loss: 3.18680759e-05
Iter: 1665 loss: 3.18555e-05
Iter: 1666 loss: 3.18879102e-05
Iter: 1667 loss: 3.18512102e-05
Iter: 1668 loss: 3.18371167e-05
Iter: 1669 loss: 3.18604471e-05
Iter: 1670 loss: 3.18307066e-05
Iter: 1671 loss: 3.18172388e-05
Iter: 1672 loss: 3.19192404e-05
Iter: 1673 loss: 3.18162492e-05
Iter: 1674 loss: 3.18055027e-05
Iter: 1675 loss: 3.18237726e-05
Iter: 1676 loss: 3.18005114e-05
Iter: 1677 loss: 3.17907252e-05
Iter: 1678 loss: 3.19339633e-05
Iter: 1679 loss: 3.17907907e-05
Iter: 1680 loss: 3.17832892e-05
Iter: 1681 loss: 3.17755912e-05
Iter: 1682 loss: 3.17741506e-05
Iter: 1683 loss: 3.17658742e-05
Iter: 1684 loss: 3.18940838e-05
Iter: 1685 loss: 3.17658414e-05
Iter: 1686 loss: 3.17573868e-05
Iter: 1687 loss: 3.17743834e-05
Iter: 1688 loss: 3.17539671e-05
Iter: 1689 loss: 3.17467e-05
Iter: 1690 loss: 3.17364065e-05
Iter: 1691 loss: 3.17360245e-05
Iter: 1692 loss: 3.1723117e-05
Iter: 1693 loss: 3.17756567e-05
Iter: 1694 loss: 3.17201484e-05
Iter: 1695 loss: 3.17098966e-05
Iter: 1696 loss: 3.17701961e-05
Iter: 1697 loss: 3.17086451e-05
Iter: 1698 loss: 3.16988808e-05
Iter: 1699 loss: 3.17024133e-05
Iter: 1700 loss: 3.16921578e-05
Iter: 1701 loss: 3.16816477e-05
Iter: 1702 loss: 3.16905425e-05
Iter: 1703 loss: 3.16755686e-05
Iter: 1704 loss: 3.1661395e-05
Iter: 1705 loss: 3.17142876e-05
Iter: 1706 loss: 3.165831e-05
Iter: 1707 loss: 3.16449295e-05
Iter: 1708 loss: 3.16774604e-05
Iter: 1709 loss: 3.16403275e-05
Iter: 1710 loss: 3.1629359e-05
Iter: 1711 loss: 3.17077574e-05
Iter: 1712 loss: 3.16286423e-05
Iter: 1713 loss: 3.16174373e-05
Iter: 1714 loss: 3.16595324e-05
Iter: 1715 loss: 3.16146434e-05
Iter: 1716 loss: 3.16064034e-05
Iter: 1717 loss: 3.16238766e-05
Iter: 1718 loss: 3.16033e-05
Iter: 1719 loss: 3.15969446e-05
Iter: 1720 loss: 3.16748519e-05
Iter: 1721 loss: 3.15968136e-05
Iter: 1722 loss: 3.15896032e-05
Iter: 1723 loss: 3.1578631e-05
Iter: 1724 loss: 3.15784819e-05
Iter: 1725 loss: 3.15695725e-05
Iter: 1726 loss: 3.15874713e-05
Iter: 1727 loss: 3.15657162e-05
Iter: 1728 loss: 3.15544094e-05
Iter: 1729 loss: 3.15588768e-05
Iter: 1730 loss: 3.1546464e-05
Iter: 1731 loss: 3.15368889e-05
Iter: 1732 loss: 3.1536827e-05
Iter: 1733 loss: 3.15295474e-05
Iter: 1734 loss: 3.15182697e-05
Iter: 1735 loss: 3.15179641e-05
Iter: 1736 loss: 3.15039615e-05
Iter: 1737 loss: 3.15657453e-05
Iter: 1738 loss: 3.15011421e-05
Iter: 1739 loss: 3.14885e-05
Iter: 1740 loss: 3.15288271e-05
Iter: 1741 loss: 3.14850076e-05
Iter: 1742 loss: 3.14736244e-05
Iter: 1743 loss: 3.14997742e-05
Iter: 1744 loss: 3.14694189e-05
Iter: 1745 loss: 3.14599674e-05
Iter: 1746 loss: 3.15618126e-05
Iter: 1747 loss: 3.14597492e-05
Iter: 1748 loss: 3.14502977e-05
Iter: 1749 loss: 3.14645367e-05
Iter: 1750 loss: 3.14455974e-05
Iter: 1751 loss: 3.14377503e-05
Iter: 1752 loss: 3.14606477e-05
Iter: 1753 loss: 3.14351637e-05
Iter: 1754 loss: 3.14259669e-05
Iter: 1755 loss: 3.15028883e-05
Iter: 1756 loss: 3.14254503e-05
Iter: 1757 loss: 3.14194585e-05
Iter: 1758 loss: 3.14096615e-05
Iter: 1759 loss: 3.14096833e-05
Iter: 1760 loss: 3.13985656e-05
Iter: 1761 loss: 3.1417745e-05
Iter: 1762 loss: 3.13936362e-05
Iter: 1763 loss: 3.13832352e-05
Iter: 1764 loss: 3.14327262e-05
Iter: 1765 loss: 3.13813434e-05
Iter: 1766 loss: 3.13717319e-05
Iter: 1767 loss: 3.14107238e-05
Iter: 1768 loss: 3.13696255e-05
Iter: 1769 loss: 3.13609635e-05
Iter: 1770 loss: 3.13716664e-05
Iter: 1771 loss: 3.13563323e-05
Iter: 1772 loss: 3.13464916e-05
Iter: 1773 loss: 3.1362677e-05
Iter: 1774 loss: 3.13418204e-05
Iter: 1775 loss: 3.1331263e-05
Iter: 1776 loss: 3.13512173e-05
Iter: 1777 loss: 3.13267738e-05
Iter: 1778 loss: 3.13146629e-05
Iter: 1779 loss: 3.13412747e-05
Iter: 1780 loss: 3.13102355e-05
Iter: 1781 loss: 3.13015917e-05
Iter: 1782 loss: 3.13014316e-05
Iter: 1783 loss: 3.12938937e-05
Iter: 1784 loss: 3.12920711e-05
Iter: 1785 loss: 3.12874727e-05
Iter: 1786 loss: 3.12794073e-05
Iter: 1787 loss: 3.1372354e-05
Iter: 1788 loss: 3.12792363e-05
Iter: 1789 loss: 3.12714874e-05
Iter: 1790 loss: 3.12837292e-05
Iter: 1791 loss: 3.12678167e-05
Iter: 1792 loss: 3.12612465e-05
Iter: 1793 loss: 3.12479679e-05
Iter: 1794 loss: 3.15000798e-05
Iter: 1795 loss: 3.12479096e-05
Iter: 1796 loss: 3.12338962e-05
Iter: 1797 loss: 3.13212549e-05
Iter: 1798 loss: 3.12323755e-05
Iter: 1799 loss: 3.12208467e-05
Iter: 1800 loss: 3.12548582e-05
Iter: 1801 loss: 3.12172597e-05
Iter: 1802 loss: 3.12062875e-05
Iter: 1803 loss: 3.12622215e-05
Iter: 1804 loss: 3.12046832e-05
Iter: 1805 loss: 3.11954864e-05
Iter: 1806 loss: 3.12099219e-05
Iter: 1807 loss: 3.11912881e-05
Iter: 1808 loss: 3.11811455e-05
Iter: 1809 loss: 3.12011325e-05
Iter: 1810 loss: 3.11772092e-05
Iter: 1811 loss: 3.11674266e-05
Iter: 1812 loss: 3.11688746e-05
Iter: 1813 loss: 3.1160147e-05
Iter: 1814 loss: 3.1147134e-05
Iter: 1815 loss: 3.12583652e-05
Iter: 1816 loss: 3.11465919e-05
Iter: 1817 loss: 3.11386721e-05
Iter: 1818 loss: 3.12232551e-05
Iter: 1819 loss: 3.11383264e-05
Iter: 1820 loss: 3.11310287e-05
Iter: 1821 loss: 3.11260337e-05
Iter: 1822 loss: 3.11232179e-05
Iter: 1823 loss: 3.11178956e-05
Iter: 1824 loss: 3.11170079e-05
Iter: 1825 loss: 3.1112264e-05
Iter: 1826 loss: 3.11023105e-05
Iter: 1827 loss: 3.12885422e-05
Iter: 1828 loss: 3.11021213e-05
Iter: 1829 loss: 3.10927571e-05
Iter: 1830 loss: 3.11083495e-05
Iter: 1831 loss: 3.10882824e-05
Iter: 1832 loss: 3.10778923e-05
Iter: 1833 loss: 3.10914111e-05
Iter: 1834 loss: 3.1072479e-05
Iter: 1835 loss: 3.10618707e-05
Iter: 1836 loss: 3.11371368e-05
Iter: 1837 loss: 3.10609103e-05
Iter: 1838 loss: 3.10505347e-05
Iter: 1839 loss: 3.10634241e-05
Iter: 1840 loss: 3.10451105e-05
Iter: 1841 loss: 3.10345386e-05
Iter: 1842 loss: 3.10901523e-05
Iter: 1843 loss: 3.10330215e-05
Iter: 1844 loss: 3.10234173e-05
Iter: 1845 loss: 3.10212163e-05
Iter: 1846 loss: 3.10151e-05
Iter: 1847 loss: 3.10037e-05
Iter: 1848 loss: 3.10477226e-05
Iter: 1849 loss: 3.10010291e-05
Iter: 1850 loss: 3.09902534e-05
Iter: 1851 loss: 3.10396717e-05
Iter: 1852 loss: 3.09882053e-05
Iter: 1853 loss: 3.09784518e-05
Iter: 1854 loss: 3.10637624e-05
Iter: 1855 loss: 3.0978e-05
Iter: 1856 loss: 3.09710522e-05
Iter: 1857 loss: 3.09828829e-05
Iter: 1858 loss: 3.09676761e-05
Iter: 1859 loss: 3.09606621e-05
Iter: 1860 loss: 3.1035117e-05
Iter: 1861 loss: 3.09604293e-05
Iter: 1862 loss: 3.09554162e-05
Iter: 1863 loss: 3.09440329e-05
Iter: 1864 loss: 3.1096024e-05
Iter: 1865 loss: 3.09433271e-05
Iter: 1866 loss: 3.09319112e-05
Iter: 1867 loss: 3.09433235e-05
Iter: 1868 loss: 3.09256357e-05
Iter: 1869 loss: 3.09133575e-05
Iter: 1870 loss: 3.10212417e-05
Iter: 1871 loss: 3.091279e-05
Iter: 1872 loss: 3.09033567e-05
Iter: 1873 loss: 3.09145871e-05
Iter: 1874 loss: 3.08984672e-05
Iter: 1875 loss: 3.08872513e-05
Iter: 1876 loss: 3.09599563e-05
Iter: 1877 loss: 3.08860253e-05
Iter: 1878 loss: 3.08773742e-05
Iter: 1879 loss: 3.08833951e-05
Iter: 1880 loss: 3.08719464e-05
Iter: 1881 loss: 3.08608614e-05
Iter: 1882 loss: 3.08872586e-05
Iter: 1883 loss: 3.08569652e-05
Iter: 1884 loss: 3.08469134e-05
Iter: 1885 loss: 3.08705203e-05
Iter: 1886 loss: 3.08430208e-05
Iter: 1887 loss: 3.08315139e-05
Iter: 1888 loss: 3.08789058e-05
Iter: 1889 loss: 3.08290328e-05
Iter: 1890 loss: 3.08195122e-05
Iter: 1891 loss: 3.09355382e-05
Iter: 1892 loss: 3.08193048e-05
Iter: 1893 loss: 3.08145827e-05
Iter: 1894 loss: 3.08302406e-05
Iter: 1895 loss: 3.0813273e-05
Iter: 1896 loss: 3.08063718e-05
Iter: 1897 loss: 3.08042981e-05
Iter: 1898 loss: 3.08003109e-05
Iter: 1899 loss: 3.07923547e-05
Iter: 1900 loss: 3.07877053e-05
Iter: 1901 loss: 3.07842201e-05
Iter: 1902 loss: 3.07731571e-05
Iter: 1903 loss: 3.08112903e-05
Iter: 1904 loss: 3.07703376e-05
Iter: 1905 loss: 3.07597293e-05
Iter: 1906 loss: 3.07683731e-05
Iter: 1907 loss: 3.07535593e-05
Iter: 1908 loss: 3.07406808e-05
Iter: 1909 loss: 3.08286835e-05
Iter: 1910 loss: 3.07395167e-05
Iter: 1911 loss: 3.07284427e-05
Iter: 1912 loss: 3.07575756e-05
Iter: 1913 loss: 3.07248556e-05
Iter: 1914 loss: 3.07132505e-05
Iter: 1915 loss: 3.07395239e-05
Iter: 1916 loss: 3.07089322e-05
Iter: 1917 loss: 3.06990478e-05
Iter: 1918 loss: 3.07105583e-05
Iter: 1919 loss: 3.06937945e-05
Iter: 1920 loss: 3.06813636e-05
Iter: 1921 loss: 3.0734147e-05
Iter: 1922 loss: 3.06789698e-05
Iter: 1923 loss: 3.06698967e-05
Iter: 1924 loss: 3.07373266e-05
Iter: 1925 loss: 3.06693692e-05
Iter: 1926 loss: 3.06600195e-05
Iter: 1927 loss: 3.06831644e-05
Iter: 1928 loss: 3.06567599e-05
Iter: 1929 loss: 3.06502407e-05
Iter: 1930 loss: 3.07285554e-05
Iter: 1931 loss: 3.06501097e-05
Iter: 1932 loss: 3.06448856e-05
Iter: 1933 loss: 3.06384536e-05
Iter: 1934 loss: 3.06379188e-05
Iter: 1935 loss: 3.06293041e-05
Iter: 1936 loss: 3.06243164e-05
Iter: 1937 loss: 3.06207257e-05
Iter: 1938 loss: 3.06087168e-05
Iter: 1939 loss: 3.06566326e-05
Iter: 1940 loss: 3.0606061e-05
Iter: 1941 loss: 3.05941212e-05
Iter: 1942 loss: 3.06215952e-05
Iter: 1943 loss: 3.05898066e-05
Iter: 1944 loss: 3.05786234e-05
Iter: 1945 loss: 3.06209586e-05
Iter: 1946 loss: 3.05760041e-05
Iter: 1947 loss: 3.05647409e-05
Iter: 1948 loss: 3.06240472e-05
Iter: 1949 loss: 3.05627364e-05
Iter: 1950 loss: 3.05538342e-05
Iter: 1951 loss: 3.05609e-05
Iter: 1952 loss: 3.05482172e-05
Iter: 1953 loss: 3.05373615e-05
Iter: 1954 loss: 3.0563504e-05
Iter: 1955 loss: 3.0533356e-05
Iter: 1956 loss: 3.05221693e-05
Iter: 1957 loss: 3.05664798e-05
Iter: 1958 loss: 3.0519619e-05
Iter: 1959 loss: 3.0512565e-05
Iter: 1960 loss: 3.06191869e-05
Iter: 1961 loss: 3.05125141e-05
Iter: 1962 loss: 3.0505722e-05
Iter: 1963 loss: 3.05085487e-05
Iter: 1964 loss: 3.05012836e-05
Iter: 1965 loss: 3.04931418e-05
Iter: 1966 loss: 3.05664798e-05
Iter: 1967 loss: 3.04927144e-05
Iter: 1968 loss: 3.04879832e-05
Iter: 1969 loss: 3.04802197e-05
Iter: 1970 loss: 3.04800014e-05
Iter: 1971 loss: 3.04696514e-05
Iter: 1972 loss: 3.04829427e-05
Iter: 1973 loss: 3.04643781e-05
Iter: 1974 loss: 3.04525456e-05
Iter: 1975 loss: 3.04571913e-05
Iter: 1976 loss: 3.04442619e-05
Iter: 1977 loss: 3.04320529e-05
Iter: 1978 loss: 3.0521649e-05
Iter: 1979 loss: 3.04310488e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script80
+ '[' -r STOP.script80 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2.4
+ date
Mon Nov  2 14:32:52 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d04161e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d0433ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d0433b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d0433268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d04a42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d048ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d02068c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d022b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d022b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d02061e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d03f4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d02a16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d02a1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d0189b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d02de9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d02f5c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d02d7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d02e09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d00b2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d00b2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d00a4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d00e39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d0147730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d0171158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d0171d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb379fcf378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d02709d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3800dc488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3800d7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3800a6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d01151e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb379ec82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb379ec8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb379ec8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3d006a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb379e84d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.14887619
test_loss: 0.16604927
train_loss: 236.71725
test_loss: 329.37042
train_loss: 193.70209
test_loss: 1690.7751
train_loss: 0.708036
test_loss: 0.6773152
train_loss: 0.733369
test_loss: 0.6770594
train_loss: 0.7327697
test_loss: 0.6770169
train_loss: 0.6841029
test_loss: 0.6773686
train_loss: 0.6520259
test_loss: 0.67758197
train_loss: 0.67267895
test_loss: 0.67684513
train_loss: 0.7191262
test_loss: 0.6766347
train_loss: 0.66030544
test_loss: 0.67717147
train_loss: 0.73437226
test_loss: 0.67761916
train_loss: 0.667009
test_loss: 0.6774315
train_loss: 0.73303413
test_loss: 0.6772457
train_loss: 0.645095
test_loss: 0.6779861
train_loss: 0.74716794
test_loss: 0.6777585
train_loss: 0.5910086
test_loss: 0.67678463
train_loss: 0.62001586
test_loss: 0.67808354
train_loss: 0.6551424
test_loss: 0.677716
train_loss: 0.63585734
test_loss: 0.6771632
train_loss: 0.68368924
test_loss: 0.67764306
train_loss: 0.6789628
test_loss: 0.67750037
train_loss: 0.6456542
test_loss: 0.67736167
train_loss: 0.7179088
test_loss: 0.6775714
train_loss: 0.53806174
test_loss: 0.67704326
train_loss: 0.7717737
test_loss: 0.67717016
train_loss: 0.72626173
test_loss: 0.6765684
train_loss: 0.70197165
test_loss: 0.67703646
train_loss: 0.6539355
test_loss: 0.67735124
train_loss: 0.6668321
test_loss: 0.6771188
train_loss: 0.67959166
test_loss: 0.6769305
train_loss: 0.6182732
test_loss: 0.6770134
train_loss: 0.6711952
test_loss: 0.67817056
train_loss: 0.6271263
test_loss: 0.67719775
train_loss: 0.6296921
test_loss: 0.6775355
train_loss: 0.7309593
test_loss: 0.6766743
train_loss: 0.66230947
test_loss: 0.6772662
train_loss: 0.7300334
test_loss: 0.6777757
train_loss: 0.7754361
test_loss: 0.67723423
train_loss: 0.6769567
test_loss: 0.6771927
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff83827d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff83827d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff83821bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff80f51a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff80f51e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff80f51e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff80f4848c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff80f49d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff80f49d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff80f4b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff80f4b8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e83248c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e8324488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e82fbd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff80f4439d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff80f429d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff80f420378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e8256d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e82c58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e82c57b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e824f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e824f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e820a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e81b9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e81b9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e80e99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e81188c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e8139510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e8139400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e8154598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e8060620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e80891e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e802c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e804fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d00df8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d0120378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.678854108
Iter: 2 loss: 0.806606889
Iter: 3 loss: 0.551992774
Iter: 4 loss: 0.308382064
Iter: 5 loss: 1.52248764
Iter: 6 loss: 0.260037154
Iter: 7 loss: 0.160347939
Iter: 8 loss: 0.732223
Iter: 9 loss: 0.147787213
Iter: 10 loss: 0.0958432928
Iter: 11 loss: 0.795877457
Iter: 12 loss: 0.0939860344
Iter: 13 loss: 0.0904284641
Iter: 14 loss: 0.0758821145
Iter: 15 loss: 0.0661103949
Iter: 16 loss: 0.163016781
Iter: 17 loss: 0.0659824163
Iter: 18 loss: 0.0470637716
Iter: 19 loss: 0.569228649
Iter: 20 loss: 0.0470450372
Iter: 21 loss: 0.0363156311
Iter: 22 loss: 0.151327938
Iter: 23 loss: 0.0361866094
Iter: 24 loss: 0.0293794833
Iter: 25 loss: 0.0616820902
Iter: 26 loss: 0.0277922619
Iter: 27 loss: 0.0251529813
Iter: 28 loss: 0.0393210128
Iter: 29 loss: 0.0249254033
Iter: 30 loss: 0.0214590691
Iter: 31 loss: 0.0289473161
Iter: 32 loss: 0.0202669948
Iter: 33 loss: 0.0178904012
Iter: 34 loss: 0.0342322513
Iter: 35 loss: 0.0176709797
Iter: 36 loss: 0.0166207924
Iter: 37 loss: 0.0225177538
Iter: 38 loss: 0.0164872482
Iter: 39 loss: 0.0159085523
Iter: 40 loss: 0.015458894
Iter: 41 loss: 0.0152945817
Iter: 42 loss: 0.01463232
Iter: 43 loss: 0.0198926367
Iter: 44 loss: 0.0145883588
Iter: 45 loss: 0.0141971055
Iter: 46 loss: 0.0153543521
Iter: 47 loss: 0.0140744597
Iter: 48 loss: 0.0137550384
Iter: 49 loss: 0.0148555432
Iter: 50 loss: 0.0136806406
Iter: 51 loss: 0.013447661
Iter: 52 loss: 0.0142953694
Iter: 53 loss: 0.0133948885
Iter: 54 loss: 0.0132587403
Iter: 55 loss: 0.013311388
Iter: 56 loss: 0.0131641449
Iter: 57 loss: 0.0130347302
Iter: 58 loss: 0.014023684
Iter: 59 loss: 0.0130250342
Iter: 60 loss: 0.0129407523
Iter: 61 loss: 0.0129977632
Iter: 62 loss: 0.0128907636
Iter: 63 loss: 0.0127313286
Iter: 64 loss: 0.012995543
Iter: 65 loss: 0.0126607921
Iter: 66 loss: 0.0125340503
Iter: 67 loss: 0.0127071869
Iter: 68 loss: 0.0124737937
Iter: 69 loss: 0.0123572107
Iter: 70 loss: 0.0128981695
Iter: 71 loss: 0.0123348609
Iter: 72 loss: 0.0121996682
Iter: 73 loss: 0.0123848841
Iter: 74 loss: 0.0121352952
Iter: 75 loss: 0.0120170405
Iter: 76 loss: 0.0129213948
Iter: 77 loss: 0.0120023564
Iter: 78 loss: 0.0119280331
Iter: 79 loss: 0.0121939545
Iter: 80 loss: 0.0119092464
Iter: 81 loss: 0.0117930025
Iter: 82 loss: 0.0121105006
Iter: 83 loss: 0.0117451157
Iter: 84 loss: 0.0116364639
Iter: 85 loss: 0.0117653552
Iter: 86 loss: 0.0115800621
Iter: 87 loss: 0.0114728566
Iter: 88 loss: 0.0117684137
Iter: 89 loss: 0.0114271929
Iter: 90 loss: 0.0113487048
Iter: 91 loss: 0.0113437138
Iter: 92 loss: 0.0112696718
Iter: 93 loss: 0.0116019137
Iter: 94 loss: 0.0112531465
Iter: 95 loss: 0.0111321453
Iter: 96 loss: 0.0118029602
Iter: 97 loss: 0.0111139137
Iter: 98 loss: 0.0110215042
Iter: 99 loss: 0.0112905279
Iter: 100 loss: 0.0109957531
Iter: 101 loss: 0.0109261135
Iter: 102 loss: 0.0112109529
Iter: 103 loss: 0.0109044965
Iter: 104 loss: 0.0108119287
Iter: 105 loss: 0.0109270951
Iter: 106 loss: 0.0107623246
Iter: 107 loss: 0.0106508248
Iter: 108 loss: 0.0114298556
Iter: 109 loss: 0.0106405877
Iter: 110 loss: 0.0105659775
Iter: 111 loss: 0.0117124869
Iter: 112 loss: 0.0105659552
Iter: 113 loss: 0.010511932
Iter: 114 loss: 0.0105398614
Iter: 115 loss: 0.0104744984
Iter: 116 loss: 0.0103654861
Iter: 117 loss: 0.0106409416
Iter: 118 loss: 0.0103318654
Iter: 119 loss: 0.0102626905
Iter: 120 loss: 0.0102567021
Iter: 121 loss: 0.0102160908
Iter: 122 loss: 0.0103027103
Iter: 123 loss: 0.0102001019
Iter: 124 loss: 0.0101503506
Iter: 125 loss: 0.0101301661
Iter: 126 loss: 0.0101014078
Iter: 127 loss: 0.0100401714
Iter: 128 loss: 0.0102411322
Iter: 129 loss: 0.0100243669
Iter: 130 loss: 0.0099632917
Iter: 131 loss: 0.0104188137
Iter: 132 loss: 0.00995332468
Iter: 133 loss: 0.00990917
Iter: 134 loss: 0.00996188074
Iter: 135 loss: 0.0098859733
Iter: 136 loss: 0.00982645
Iter: 137 loss: 0.00988824479
Iter: 138 loss: 0.00979147106
Iter: 139 loss: 0.00975255296
Iter: 140 loss: 0.00974538643
Iter: 141 loss: 0.00968312
Iter: 142 loss: 0.0100586275
Iter: 143 loss: 0.00967530161
Iter: 144 loss: 0.0096306093
Iter: 145 loss: 0.0101901088
Iter: 146 loss: 0.00962865446
Iter: 147 loss: 0.00959333312
Iter: 148 loss: 0.00965842418
Iter: 149 loss: 0.00957835559
Iter: 150 loss: 0.00953544304
Iter: 151 loss: 0.00953508541
Iter: 152 loss: 0.00949441642
Iter: 153 loss: 0.00951084308
Iter: 154 loss: 0.00946725439
Iter: 155 loss: 0.00941488892
Iter: 156 loss: 0.0095674647
Iter: 157 loss: 0.00939901825
Iter: 158 loss: 0.00934556127
Iter: 159 loss: 0.00993705261
Iter: 160 loss: 0.0093440488
Iter: 161 loss: 0.00931284763
Iter: 162 loss: 0.00928586815
Iter: 163 loss: 0.00927743
Iter: 164 loss: 0.00922915339
Iter: 165 loss: 0.00928888284
Iter: 166 loss: 0.00920315739
Iter: 167 loss: 0.0091597978
Iter: 168 loss: 0.00956996065
Iter: 169 loss: 0.00915833376
Iter: 170 loss: 0.00912913308
Iter: 171 loss: 0.00912866835
Iter: 172 loss: 0.00908156391
Iter: 173 loss: 0.00935989153
Iter: 174 loss: 0.00907547772
Iter: 175 loss: 0.00904329307
Iter: 176 loss: 0.0090309307
Iter: 177 loss: 0.00901237316
Iter: 178 loss: 0.0089694513
Iter: 179 loss: 0.00908841379
Iter: 180 loss: 0.00895588472
Iter: 181 loss: 0.00892690383
Iter: 182 loss: 0.00896554254
Iter: 183 loss: 0.0089115072
Iter: 184 loss: 0.00889236107
Iter: 185 loss: 0.00893634
Iter: 186 loss: 0.00888554659
Iter: 187 loss: 0.00885601
Iter: 188 loss: 0.00916550122
Iter: 189 loss: 0.0088538155
Iter: 190 loss: 0.00881622545
Iter: 191 loss: 0.00893235952
Iter: 192 loss: 0.008804949
Iter: 193 loss: 0.00876074657
Iter: 194 loss: 0.0090947561
Iter: 195 loss: 0.00875639
Iter: 196 loss: 0.00871661212
Iter: 197 loss: 0.00871645566
Iter: 198 loss: 0.00868865848
Iter: 199 loss: 0.00879468303
Iter: 200 loss: 0.00868023746
Iter: 201 loss: 0.00865815207
Iter: 202 loss: 0.00872518122
Iter: 203 loss: 0.00865165517
Iter: 204 loss: 0.00861840323
Iter: 205 loss: 0.0086519476
Iter: 206 loss: 0.00859875791
Iter: 207 loss: 0.00856673159
Iter: 208 loss: 0.00881448
Iter: 209 loss: 0.00856474228
Iter: 210 loss: 0.00854127668
Iter: 211 loss: 0.00854107272
Iter: 212 loss: 0.0085248407
Iter: 213 loss: 0.00855542161
Iter: 214 loss: 0.00851783
Iter: 215 loss: 0.00849661231
Iter: 216 loss: 0.00855167396
Iter: 217 loss: 0.0084894523
Iter: 218 loss: 0.00846775435
Iter: 219 loss: 0.00849703
Iter: 220 loss: 0.00845701061
Iter: 221 loss: 0.00842939876
Iter: 222 loss: 0.00853131
Iter: 223 loss: 0.00842076167
Iter: 224 loss: 0.00839044712
Iter: 225 loss: 0.00867256429
Iter: 226 loss: 0.00838931464
Iter: 227 loss: 0.00837422162
Iter: 228 loss: 0.00849029236
Iter: 229 loss: 0.00837254338
Iter: 230 loss: 0.00836103
Iter: 231 loss: 0.00837465934
Iter: 232 loss: 0.00835531
Iter: 233 loss: 0.00834066607
Iter: 234 loss: 0.0083262641
Iter: 235 loss: 0.00832301751
Iter: 236 loss: 0.00829814
Iter: 237 loss: 0.00833598711
Iter: 238 loss: 0.00828654505
Iter: 239 loss: 0.00826339703
Iter: 240 loss: 0.00858840905
Iter: 241 loss: 0.00826272741
Iter: 242 loss: 0.00824353751
Iter: 243 loss: 0.00830122
Iter: 244 loss: 0.00823779684
Iter: 245 loss: 0.00821538828
Iter: 246 loss: 0.00824089069
Iter: 247 loss: 0.00820298214
Iter: 248 loss: 0.00817814562
Iter: 249 loss: 0.00817497261
Iter: 250 loss: 0.00816217251
Iter: 251 loss: 0.00824128278
Iter: 252 loss: 0.00816081557
Iter: 253 loss: 0.00815020502
Iter: 254 loss: 0.00816482492
Iter: 255 loss: 0.0081441775
Iter: 256 loss: 0.00812736154
Iter: 257 loss: 0.00811639428
Iter: 258 loss: 0.0081099011
Iter: 259 loss: 0.00807883404
Iter: 260 loss: 0.00848592538
Iter: 261 loss: 0.00807867385
Iter: 262 loss: 0.00807491317
Iter: 263 loss: 0.0080619473
Iter: 264 loss: 0.0080438368
Iter: 265 loss: 0.00811747834
Iter: 266 loss: 0.00803979766
Iter: 267 loss: 0.00801536255
Iter: 268 loss: 0.00810262
Iter: 269 loss: 0.00800714828
Iter: 270 loss: 0.00799169857
Iter: 271 loss: 0.00812569447
Iter: 272 loss: 0.00799089298
Iter: 273 loss: 0.00797664747
Iter: 274 loss: 0.00798622705
Iter: 275 loss: 0.00796697
Iter: 276 loss: 0.00794503838
Iter: 277 loss: 0.00797522441
Iter: 278 loss: 0.00793444738
Iter: 279 loss: 0.00791320484
Iter: 280 loss: 0.00796537567
Iter: 281 loss: 0.00790504832
Iter: 282 loss: 0.00787744857
Iter: 283 loss: 0.00807381235
Iter: 284 loss: 0.00787488464
Iter: 285 loss: 0.00785285048
Iter: 286 loss: 0.00795236602
Iter: 287 loss: 0.00784886815
Iter: 288 loss: 0.00783841871
Iter: 289 loss: 0.00787586905
Iter: 290 loss: 0.00783483684
Iter: 291 loss: 0.0078173615
Iter: 292 loss: 0.00786602125
Iter: 293 loss: 0.00781188486
Iter: 294 loss: 0.00778814219
Iter: 295 loss: 0.00793624111
Iter: 296 loss: 0.00778467488
Iter: 297 loss: 0.00776516879
Iter: 298 loss: 0.00783243496
Iter: 299 loss: 0.00776006188
Iter: 300 loss: 0.00774109643
Iter: 301 loss: 0.00784785394
Iter: 302 loss: 0.00773830758
Iter: 303 loss: 0.00771927461
Iter: 304 loss: 0.00782094151
Iter: 305 loss: 0.00771624595
Iter: 306 loss: 0.00770271569
Iter: 307 loss: 0.00773752201
Iter: 308 loss: 0.00769819
Iter: 309 loss: 0.00768456608
Iter: 310 loss: 0.00775524508
Iter: 311 loss: 0.00768111646
Iter: 312 loss: 0.00766173564
Iter: 313 loss: 0.00770355668
Iter: 314 loss: 0.007654218
Iter: 315 loss: 0.00763441715
Iter: 316 loss: 0.00766198337
Iter: 317 loss: 0.00762377772
Iter: 318 loss: 0.00760610076
Iter: 319 loss: 0.00766228139
Iter: 320 loss: 0.00760171423
Iter: 321 loss: 0.00757269282
Iter: 322 loss: 0.00769143086
Iter: 323 loss: 0.00756535772
Iter: 324 loss: 0.00756434118
Iter: 325 loss: 0.00753985438
Iter: 326 loss: 0.00752421562
Iter: 327 loss: 0.00757596735
Iter: 328 loss: 0.00752017368
Iter: 329 loss: 0.00749157276
Iter: 330 loss: 0.00760639599
Iter: 331 loss: 0.00748202112
Iter: 332 loss: 0.00745325955
Iter: 333 loss: 0.00745316409
Iter: 334 loss: 0.00743273273
Iter: 335 loss: 0.00743266754
Iter: 336 loss: 0.00742049888
Iter: 337 loss: 0.00742751034
Iter: 338 loss: 0.00741255563
Iter: 339 loss: 0.00738710118
Iter: 340 loss: 0.00768286688
Iter: 341 loss: 0.00738521107
Iter: 342 loss: 0.00736789
Iter: 343 loss: 0.00758769
Iter: 344 loss: 0.00736775761
Iter: 345 loss: 0.00735398196
Iter: 346 loss: 0.00741547067
Iter: 347 loss: 0.00735062826
Iter: 348 loss: 0.007337844
Iter: 349 loss: 0.00734728342
Iter: 350 loss: 0.00733003486
Iter: 351 loss: 0.00731341913
Iter: 352 loss: 0.00734348176
Iter: 353 loss: 0.00730568683
Iter: 354 loss: 0.00728544639
Iter: 355 loss: 0.00729176961
Iter: 356 loss: 0.00727102067
Iter: 357 loss: 0.0072527742
Iter: 358 loss: 0.0073820306
Iter: 359 loss: 0.00725094
Iter: 360 loss: 0.00724152476
Iter: 361 loss: 0.00723944418
Iter: 362 loss: 0.00723327138
Iter: 363 loss: 0.00721577555
Iter: 364 loss: 0.00727958232
Iter: 365 loss: 0.00721174479
Iter: 366 loss: 0.00719217677
Iter: 367 loss: 0.00770143885
Iter: 368 loss: 0.00719213393
Iter: 369 loss: 0.00717834244
Iter: 370 loss: 0.00717796478
Iter: 371 loss: 0.00716580311
Iter: 372 loss: 0.0072094854
Iter: 373 loss: 0.00716264453
Iter: 374 loss: 0.0071518328
Iter: 375 loss: 0.00713998545
Iter: 376 loss: 0.00713800918
Iter: 377 loss: 0.00712437
Iter: 378 loss: 0.00719693583
Iter: 379 loss: 0.00712227449
Iter: 380 loss: 0.00711193308
Iter: 381 loss: 0.00711248675
Iter: 382 loss: 0.00710372208
Iter: 383 loss: 0.00709080324
Iter: 384 loss: 0.00717955455
Iter: 385 loss: 0.00708921207
Iter: 386 loss: 0.0070738229
Iter: 387 loss: 0.00723927282
Iter: 388 loss: 0.00707367947
Iter: 389 loss: 0.00705840625
Iter: 390 loss: 0.00705653243
Iter: 391 loss: 0.00703879399
Iter: 392 loss: 0.007138249
Iter: 393 loss: 0.00703570154
Iter: 394 loss: 0.00702693965
Iter: 395 loss: 0.00703487685
Iter: 396 loss: 0.00702202786
Iter: 397 loss: 0.00699856924
Iter: 398 loss: 0.00707213767
Iter: 399 loss: 0.00698996754
Iter: 400 loss: 0.0069706
Iter: 401 loss: 0.00715129357
Iter: 402 loss: 0.00696950639
Iter: 403 loss: 0.00695498427
Iter: 404 loss: 0.00710342079
Iter: 405 loss: 0.00695463503
Iter: 406 loss: 0.00694175903
Iter: 407 loss: 0.00695887627
Iter: 408 loss: 0.00693495292
Iter: 409 loss: 0.00691511761
Iter: 410 loss: 0.0069437474
Iter: 411 loss: 0.00690580066
Iter: 412 loss: 0.00688157603
Iter: 413 loss: 0.00688112853
Iter: 414 loss: 0.00686802156
Iter: 415 loss: 0.00701711513
Iter: 416 loss: 0.00686780224
Iter: 417 loss: 0.00685169268
Iter: 418 loss: 0.006887211
Iter: 419 loss: 0.00684562838
Iter: 420 loss: 0.00682987878
Iter: 421 loss: 0.00685897935
Iter: 422 loss: 0.00682230806
Iter: 423 loss: 0.00681322161
Iter: 424 loss: 0.0068118833
Iter: 425 loss: 0.00680554379
Iter: 426 loss: 0.00680161314
Iter: 427 loss: 0.00679898169
Iter: 428 loss: 0.00678472826
Iter: 429 loss: 0.00678623794
Iter: 430 loss: 0.00677407626
Iter: 431 loss: 0.00676938286
Iter: 432 loss: 0.00675303815
Iter: 433 loss: 0.00673537189
Iter: 434 loss: 0.00682566268
Iter: 435 loss: 0.00673246942
Iter: 436 loss: 0.00671637338
Iter: 437 loss: 0.00681419531
Iter: 438 loss: 0.00671435334
Iter: 439 loss: 0.00670134369
Iter: 440 loss: 0.00669546658
Iter: 441 loss: 0.00668902695
Iter: 442 loss: 0.00667064078
Iter: 443 loss: 0.006829408
Iter: 444 loss: 0.00666818395
Iter: 445 loss: 0.00665653404
Iter: 446 loss: 0.00669148285
Iter: 447 loss: 0.00665302668
Iter: 448 loss: 0.00663876161
Iter: 449 loss: 0.00663324632
Iter: 450 loss: 0.00662491
Iter: 451 loss: 0.00660591107
Iter: 452 loss: 0.006760085
Iter: 453 loss: 0.00660488848
Iter: 454 loss: 0.00659358222
Iter: 455 loss: 0.00664077792
Iter: 456 loss: 0.00659109745
Iter: 457 loss: 0.00657955045
Iter: 458 loss: 0.0066060368
Iter: 459 loss: 0.00657457439
Iter: 460 loss: 0.00655175187
Iter: 461 loss: 0.0067344089
Iter: 462 loss: 0.00655021425
Iter: 463 loss: 0.00653601624
Iter: 464 loss: 0.00656330399
Iter: 465 loss: 0.00652903831
Iter: 466 loss: 0.0065082279
Iter: 467 loss: 0.00659857551
Iter: 468 loss: 0.00650426093
Iter: 469 loss: 0.00649096631
Iter: 470 loss: 0.0065522315
Iter: 471 loss: 0.00648835674
Iter: 472 loss: 0.00647588074
Iter: 473 loss: 0.00651602726
Iter: 474 loss: 0.00647228118
Iter: 475 loss: 0.00645977398
Iter: 476 loss: 0.00643955823
Iter: 477 loss: 0.00643943483
Iter: 478 loss: 0.00641066115
Iter: 479 loss: 0.00749084726
Iter: 480 loss: 0.00641065417
Iter: 481 loss: 0.00639325147
Iter: 482 loss: 0.0064331647
Iter: 483 loss: 0.00638700882
Iter: 484 loss: 0.00636475952
Iter: 485 loss: 0.00639792811
Iter: 486 loss: 0.00635272823
Iter: 487 loss: 0.00632837322
Iter: 488 loss: 0.00647427235
Iter: 489 loss: 0.00632558856
Iter: 490 loss: 0.00630452577
Iter: 491 loss: 0.00640695682
Iter: 492 loss: 0.00630081072
Iter: 493 loss: 0.00628305692
Iter: 494 loss: 0.00672624633
Iter: 495 loss: 0.0062829433
Iter: 496 loss: 0.00626045046
Iter: 497 loss: 0.00642931089
Iter: 498 loss: 0.00625894777
Iter: 499 loss: 0.0062391609
Iter: 500 loss: 0.0063284263
Iter: 501 loss: 0.00623401906
Iter: 502 loss: 0.00621595327
Iter: 503 loss: 0.00645114388
Iter: 504 loss: 0.00621582
Iter: 505 loss: 0.00620002858
Iter: 506 loss: 0.00628529675
Iter: 507 loss: 0.00619773287
Iter: 508 loss: 0.0061856932
Iter: 509 loss: 0.00620772224
Iter: 510 loss: 0.00617993763
Iter: 511 loss: 0.00616703369
Iter: 512 loss: 0.00620739767
Iter: 513 loss: 0.00616359524
Iter: 514 loss: 0.0061514657
Iter: 515 loss: 0.00618662732
Iter: 516 loss: 0.00614737347
Iter: 517 loss: 0.00613549724
Iter: 518 loss: 0.00616060058
Iter: 519 loss: 0.00613085
Iter: 520 loss: 0.00611216575
Iter: 521 loss: 0.00615031738
Iter: 522 loss: 0.00610474683
Iter: 523 loss: 0.00608786661
Iter: 524 loss: 0.00631690957
Iter: 525 loss: 0.00608723704
Iter: 526 loss: 0.00607775338
Iter: 527 loss: 0.00607235078
Iter: 528 loss: 0.0060586147
Iter: 529 loss: 0.00609846367
Iter: 530 loss: 0.00605440093
Iter: 531 loss: 0.00604124786
Iter: 532 loss: 0.00608642865
Iter: 533 loss: 0.00603761896
Iter: 534 loss: 0.00602875371
Iter: 535 loss: 0.00603307225
Iter: 536 loss: 0.00602323841
Iter: 537 loss: 0.00600337377
Iter: 538 loss: 0.00603222055
Iter: 539 loss: 0.00599220861
Iter: 540 loss: 0.00597865926
Iter: 541 loss: 0.00597604224
Iter: 542 loss: 0.00596653903
Iter: 543 loss: 0.00595303625
Iter: 544 loss: 0.00595101528
Iter: 545 loss: 0.00593819097
Iter: 546 loss: 0.0060021407
Iter: 547 loss: 0.00593612855
Iter: 548 loss: 0.00592465792
Iter: 549 loss: 0.00600562803
Iter: 550 loss: 0.00592274126
Iter: 551 loss: 0.00590696
Iter: 552 loss: 0.00596996397
Iter: 553 loss: 0.00590332691
Iter: 554 loss: 0.00588290393
Iter: 555 loss: 0.00598720368
Iter: 556 loss: 0.00587964058
Iter: 557 loss: 0.00586494245
Iter: 558 loss: 0.00593862729
Iter: 559 loss: 0.00586205348
Iter: 560 loss: 0.00585099682
Iter: 561 loss: 0.00585966092
Iter: 562 loss: 0.0058445055
Iter: 563 loss: 0.00582965836
Iter: 564 loss: 0.00591357751
Iter: 565 loss: 0.00582657941
Iter: 566 loss: 0.00581728155
Iter: 567 loss: 0.00583491661
Iter: 568 loss: 0.00581338676
Iter: 569 loss: 0.00580305513
Iter: 570 loss: 0.00591359288
Iter: 571 loss: 0.00580282928
Iter: 572 loss: 0.00578863546
Iter: 573 loss: 0.00580012798
Iter: 574 loss: 0.00577993691
Iter: 575 loss: 0.00576003315
Iter: 576 loss: 0.00586599298
Iter: 577 loss: 0.00575657096
Iter: 578 loss: 0.00574287865
Iter: 579 loss: 0.00574160926
Iter: 580 loss: 0.00573288603
Iter: 581 loss: 0.00572611392
Iter: 582 loss: 0.00572319608
Iter: 583 loss: 0.00570613425
Iter: 584 loss: 0.00574593619
Iter: 585 loss: 0.00570032094
Iter: 586 loss: 0.00568378437
Iter: 587 loss: 0.00593248662
Iter: 588 loss: 0.00568333594
Iter: 589 loss: 0.0056742914
Iter: 590 loss: 0.00566906
Iter: 591 loss: 0.00566519937
Iter: 592 loss: 0.00565169845
Iter: 593 loss: 0.0058044563
Iter: 594 loss: 0.00565145
Iter: 595 loss: 0.00563613139
Iter: 596 loss: 0.00568493549
Iter: 597 loss: 0.00563190412
Iter: 598 loss: 0.00561933871
Iter: 599 loss: 0.00563556328
Iter: 600 loss: 0.00561290607
Iter: 601 loss: 0.0055989651
Iter: 602 loss: 0.00567119103
Iter: 603 loss: 0.00559630711
Iter: 604 loss: 0.00558152143
Iter: 605 loss: 0.00561696338
Iter: 606 loss: 0.00557626877
Iter: 607 loss: 0.00555827795
Iter: 608 loss: 0.00567822624
Iter: 609 loss: 0.00555507559
Iter: 610 loss: 0.00554261077
Iter: 611 loss: 0.00560795423
Iter: 612 loss: 0.00554070342
Iter: 613 loss: 0.00553163327
Iter: 614 loss: 0.00552361738
Iter: 615 loss: 0.00552096916
Iter: 616 loss: 0.00550154224
Iter: 617 loss: 0.00557257887
Iter: 618 loss: 0.00549676269
Iter: 619 loss: 0.00548138097
Iter: 620 loss: 0.00548769534
Iter: 621 loss: 0.00547071593
Iter: 622 loss: 0.00545150787
Iter: 623 loss: 0.00548657496
Iter: 624 loss: 0.00544323353
Iter: 625 loss: 0.00542783039
Iter: 626 loss: 0.00546211656
Iter: 627 loss: 0.00542244781
Iter: 628 loss: 0.00540441927
Iter: 629 loss: 0.00545051554
Iter: 630 loss: 0.00539624132
Iter: 631 loss: 0.00538046658
Iter: 632 loss: 0.0054217237
Iter: 633 loss: 0.00537508726
Iter: 634 loss: 0.00536130695
Iter: 635 loss: 0.00536114071
Iter: 636 loss: 0.00534402858
Iter: 637 loss: 0.00539740734
Iter: 638 loss: 0.00533907721
Iter: 639 loss: 0.00532367732
Iter: 640 loss: 0.00537604187
Iter: 641 loss: 0.00531818764
Iter: 642 loss: 0.00530425087
Iter: 643 loss: 0.00533788558
Iter: 644 loss: 0.0052993
Iter: 645 loss: 0.00527999457
Iter: 646 loss: 0.00527949817
Iter: 647 loss: 0.00525121531
Iter: 648 loss: 0.00541514112
Iter: 649 loss: 0.00524760224
Iter: 650 loss: 0.0052303616
Iter: 651 loss: 0.0052293567
Iter: 652 loss: 0.00521099614
Iter: 653 loss: 0.00533868885
Iter: 654 loss: 0.00520918146
Iter: 655 loss: 0.00519677345
Iter: 656 loss: 0.00520934351
Iter: 657 loss: 0.00518998923
Iter: 658 loss: 0.00517713744
Iter: 659 loss: 0.00530685252
Iter: 660 loss: 0.00517594721
Iter: 661 loss: 0.00515821
Iter: 662 loss: 0.00521937804
Iter: 663 loss: 0.00515390094
Iter: 664 loss: 0.00513154268
Iter: 665 loss: 0.00519430917
Iter: 666 loss: 0.00512391189
Iter: 667 loss: 0.00510188378
Iter: 668 loss: 0.00532156369
Iter: 669 loss: 0.00510119647
Iter: 670 loss: 0.00509097334
Iter: 671 loss: 0.00509478152
Iter: 672 loss: 0.00508395443
Iter: 673 loss: 0.00507027935
Iter: 674 loss: 0.00512898061
Iter: 675 loss: 0.00506666675
Iter: 676 loss: 0.00505652791
Iter: 677 loss: 0.00504359975
Iter: 678 loss: 0.00504263164
Iter: 679 loss: 0.00502015883
Iter: 680 loss: 0.00516138878
Iter: 681 loss: 0.00501700491
Iter: 682 loss: 0.00499790674
Iter: 683 loss: 0.00499786809
Iter: 684 loss: 0.00498547498
Iter: 685 loss: 0.0049854517
Iter: 686 loss: 0.00497163553
Iter: 687 loss: 0.00501695462
Iter: 688 loss: 0.00496764574
Iter: 689 loss: 0.00495562516
Iter: 690 loss: 0.00497424137
Iter: 691 loss: 0.00494997948
Iter: 692 loss: 0.00493675098
Iter: 693 loss: 0.00493913516
Iter: 694 loss: 0.00492623542
Iter: 695 loss: 0.0049096006
Iter: 696 loss: 0.00499424897
Iter: 697 loss: 0.00490711909
Iter: 698 loss: 0.00489374436
Iter: 699 loss: 0.00492474902
Iter: 700 loss: 0.00488791382
Iter: 701 loss: 0.00487106107
Iter: 702 loss: 0.00499412511
Iter: 703 loss: 0.00486958493
Iter: 704 loss: 0.00485000201
Iter: 705 loss: 0.00490633538
Iter: 706 loss: 0.00484372862
Iter: 707 loss: 0.00483090524
Iter: 708 loss: 0.00484772213
Iter: 709 loss: 0.00482385512
Iter: 710 loss: 0.00480908481
Iter: 711 loss: 0.00484159449
Iter: 712 loss: 0.00480395136
Iter: 713 loss: 0.00478525786
Iter: 714 loss: 0.00483859796
Iter: 715 loss: 0.00477815699
Iter: 716 loss: 0.00476300064
Iter: 717 loss: 0.00476294616
Iter: 718 loss: 0.0047489875
Iter: 719 loss: 0.00495278556
Iter: 720 loss: 0.00474896748
Iter: 721 loss: 0.00473851152
Iter: 722 loss: 0.00472715031
Iter: 723 loss: 0.00472540455
Iter: 724 loss: 0.00470972
Iter: 725 loss: 0.00477435905
Iter: 726 loss: 0.00470591849
Iter: 727 loss: 0.00468831323
Iter: 728 loss: 0.00480176136
Iter: 729 loss: 0.00468648458
Iter: 730 loss: 0.0046570315
Iter: 731 loss: 0.0047849738
Iter: 732 loss: 0.00465065194
Iter: 733 loss: 0.00462542847
Iter: 734 loss: 0.00488166418
Iter: 735 loss: 0.00462450925
Iter: 736 loss: 0.00460620597
Iter: 737 loss: 0.00468499633
Iter: 738 loss: 0.00460257195
Iter: 739 loss: 0.00459343102
Iter: 740 loss: 0.00459195254
Iter: 741 loss: 0.00458539743
Iter: 742 loss: 0.00458902027
Iter: 743 loss: 0.00458109425
Iter: 744 loss: 0.00457063504
Iter: 745 loss: 0.00457415264
Iter: 746 loss: 0.00456284173
Iter: 747 loss: 0.00454659108
Iter: 748 loss: 0.00457781367
Iter: 749 loss: 0.00454024039
Iter: 750 loss: 0.00451916642
Iter: 751 loss: 0.0045734914
Iter: 752 loss: 0.00451185228
Iter: 753 loss: 0.00450076815
Iter: 754 loss: 0.00449825265
Iter: 755 loss: 0.00448921882
Iter: 756 loss: 0.00447848253
Iter: 757 loss: 0.00447734864
Iter: 758 loss: 0.00446098857
Iter: 759 loss: 0.00450760778
Iter: 760 loss: 0.00445423741
Iter: 761 loss: 0.00443766918
Iter: 762 loss: 0.00447017234
Iter: 763 loss: 0.00443093712
Iter: 764 loss: 0.00441397168
Iter: 765 loss: 0.00443430757
Iter: 766 loss: 0.00440440467
Iter: 767 loss: 0.00439025462
Iter: 768 loss: 0.0044515105
Iter: 769 loss: 0.00438739732
Iter: 770 loss: 0.00437336788
Iter: 771 loss: 0.00438446505
Iter: 772 loss: 0.00436363136
Iter: 773 loss: 0.004347587
Iter: 774 loss: 0.00439870637
Iter: 775 loss: 0.00434311107
Iter: 776 loss: 0.00433182484
Iter: 777 loss: 0.00436957553
Iter: 778 loss: 0.00432868954
Iter: 779 loss: 0.00431623356
Iter: 780 loss: 0.00434538
Iter: 781 loss: 0.00431163702
Iter: 782 loss: 0.00429618
Iter: 783 loss: 0.00429258449
Iter: 784 loss: 0.00428290386
Iter: 785 loss: 0.0042656865
Iter: 786 loss: 0.00448106881
Iter: 787 loss: 0.00426502433
Iter: 788 loss: 0.00425682589
Iter: 789 loss: 0.00426811539
Iter: 790 loss: 0.00425271131
Iter: 791 loss: 0.0042395778
Iter: 792 loss: 0.00422666967
Iter: 793 loss: 0.00422373414
Iter: 794 loss: 0.004218271
Iter: 795 loss: 0.00421251077
Iter: 796 loss: 0.00419062935
Iter: 797 loss: 0.00436010119
Iter: 798 loss: 0.00418854551
Iter: 799 loss: 0.00417437032
Iter: 800 loss: 0.00420905184
Iter: 801 loss: 0.0041691279
Iter: 802 loss: 0.00415344583
Iter: 803 loss: 0.00419257488
Iter: 804 loss: 0.00414788118
Iter: 805 loss: 0.00412737951
Iter: 806 loss: 0.00419323379
Iter: 807 loss: 0.00412110705
Iter: 808 loss: 0.00409196364
Iter: 809 loss: 0.00425421447
Iter: 810 loss: 0.00408761296
Iter: 811 loss: 0.00406789314
Iter: 812 loss: 0.00438121147
Iter: 813 loss: 0.00406787824
Iter: 814 loss: 0.0040511759
Iter: 815 loss: 0.00416913722
Iter: 816 loss: 0.00404959638
Iter: 817 loss: 0.00403776206
Iter: 818 loss: 0.00405687466
Iter: 819 loss: 0.00403232034
Iter: 820 loss: 0.00403363397
Iter: 821 loss: 0.00402643112
Iter: 822 loss: 0.00402274728
Iter: 823 loss: 0.0040253792
Iter: 824 loss: 0.00402041804
Iter: 825 loss: 0.00400841236
Iter: 826 loss: 0.00400295481
Iter: 827 loss: 0.00399697199
Iter: 828 loss: 0.00397917163
Iter: 829 loss: 0.00401649624
Iter: 830 loss: 0.00397185329
Iter: 831 loss: 0.00395369
Iter: 832 loss: 0.00396452146
Iter: 833 loss: 0.00394201418
Iter: 834 loss: 0.00392079446
Iter: 835 loss: 0.00403432455
Iter: 836 loss: 0.00391741423
Iter: 837 loss: 0.00390045322
Iter: 838 loss: 0.00399601087
Iter: 839 loss: 0.00389776868
Iter: 840 loss: 0.00387942349
Iter: 841 loss: 0.00391185703
Iter: 842 loss: 0.00387152843
Iter: 843 loss: 0.00384658109
Iter: 844 loss: 0.00392464362
Iter: 845 loss: 0.00383892329
Iter: 846 loss: 0.00381633732
Iter: 847 loss: 0.00391142303
Iter: 848 loss: 0.00381163321
Iter: 849 loss: 0.00379101234
Iter: 850 loss: 0.00384868961
Iter: 851 loss: 0.00378444418
Iter: 852 loss: 0.00376248406
Iter: 853 loss: 0.00384555012
Iter: 854 loss: 0.00375721976
Iter: 855 loss: 0.00374169229
Iter: 856 loss: 0.00374164269
Iter: 857 loss: 0.00373053225
Iter: 858 loss: 0.00372660602
Iter: 859 loss: 0.00372029468
Iter: 860 loss: 0.00373281515
Iter: 861 loss: 0.00371167529
Iter: 862 loss: 0.00370526733
Iter: 863 loss: 0.00369983213
Iter: 864 loss: 0.00369800301
Iter: 865 loss: 0.00368060684
Iter: 866 loss: 0.00368481781
Iter: 867 loss: 0.00366796553
Iter: 868 loss: 0.00364957517
Iter: 869 loss: 0.00375414453
Iter: 870 loss: 0.00364705967
Iter: 871 loss: 0.00363224326
Iter: 872 loss: 0.00365509
Iter: 873 loss: 0.00362502458
Iter: 874 loss: 0.00360502
Iter: 875 loss: 0.00365164038
Iter: 876 loss: 0.00359783415
Iter: 877 loss: 0.00358359236
Iter: 878 loss: 0.0036108864
Iter: 879 loss: 0.00357674295
Iter: 880 loss: 0.00356178661
Iter: 881 loss: 0.00362217613
Iter: 882 loss: 0.00355863688
Iter: 883 loss: 0.00354290358
Iter: 884 loss: 0.00359618245
Iter: 885 loss: 0.00353861554
Iter: 886 loss: 0.00352353393
Iter: 887 loss: 0.00354662677
Iter: 888 loss: 0.00351642305
Iter: 889 loss: 0.00350419758
Iter: 890 loss: 0.0036234986
Iter: 891 loss: 0.00350373588
Iter: 892 loss: 0.00349498959
Iter: 893 loss: 0.00348279299
Iter: 894 loss: 0.00348222535
Iter: 895 loss: 0.00346578052
Iter: 896 loss: 0.00349796261
Iter: 897 loss: 0.00345917302
Iter: 898 loss: 0.00344323646
Iter: 899 loss: 0.00374762202
Iter: 900 loss: 0.00344323344
Iter: 901 loss: 0.00342609035
Iter: 902 loss: 0.00342603331
Iter: 903 loss: 0.00341635058
Iter: 904 loss: 0.00343508576
Iter: 905 loss: 0.00341248745
Iter: 906 loss: 0.00339560793
Iter: 907 loss: 0.00343934027
Iter: 908 loss: 0.00338878389
Iter: 909 loss: 0.00337209
Iter: 910 loss: 0.00342794601
Iter: 911 loss: 0.00336745312
Iter: 912 loss: 0.00334780058
Iter: 913 loss: 0.00342429522
Iter: 914 loss: 0.00334337284
Iter: 915 loss: 0.00332261901
Iter: 916 loss: 0.00342791784
Iter: 917 loss: 0.00331912236
Iter: 918 loss: 0.00330405775
Iter: 919 loss: 0.00330376439
Iter: 920 loss: 0.00328651071
Iter: 921 loss: 0.00333552
Iter: 922 loss: 0.00328092091
Iter: 923 loss: 0.00326558016
Iter: 924 loss: 0.00345415669
Iter: 925 loss: 0.00326542882
Iter: 926 loss: 0.00325529277
Iter: 927 loss: 0.00324032106
Iter: 928 loss: 0.00323979743
Iter: 929 loss: 0.00322240568
Iter: 930 loss: 0.00322232209
Iter: 931 loss: 0.00321067031
Iter: 932 loss: 0.00323348306
Iter: 933 loss: 0.00320592569
Iter: 934 loss: 0.00319355959
Iter: 935 loss: 0.00319700199
Iter: 936 loss: 0.00318478933
Iter: 937 loss: 0.00316356379
Iter: 938 loss: 0.00328678172
Iter: 939 loss: 0.00316063105
Iter: 940 loss: 0.00314558926
Iter: 941 loss: 0.00316584436
Iter: 942 loss: 0.00313787791
Iter: 943 loss: 0.00311755715
Iter: 944 loss: 0.00317631
Iter: 945 loss: 0.00311129214
Iter: 946 loss: 0.0030931083
Iter: 947 loss: 0.00321538793
Iter: 948 loss: 0.00309134112
Iter: 949 loss: 0.00307523762
Iter: 950 loss: 0.00311600277
Iter: 951 loss: 0.00306966272
Iter: 952 loss: 0.00305224862
Iter: 953 loss: 0.00331376819
Iter: 954 loss: 0.00305195665
Iter: 955 loss: 0.00303846877
Iter: 956 loss: 0.00312611
Iter: 957 loss: 0.00303720403
Iter: 958 loss: 0.00302643212
Iter: 959 loss: 0.00305289356
Iter: 960 loss: 0.00302225258
Iter: 961 loss: 0.00301111559
Iter: 962 loss: 0.00307396613
Iter: 963 loss: 0.00300960499
Iter: 964 loss: 0.00299894135
Iter: 965 loss: 0.00299082929
Iter: 966 loss: 0.00298738224
Iter: 967 loss: 0.00297111878
Iter: 968 loss: 0.00301010185
Iter: 969 loss: 0.00296403421
Iter: 970 loss: 0.00294370623
Iter: 971 loss: 0.0031626015
Iter: 972 loss: 0.00294334022
Iter: 973 loss: 0.0029250104
Iter: 974 loss: 0.00292266579
Iter: 975 loss: 0.00291005289
Iter: 976 loss: 0.00295759202
Iter: 977 loss: 0.00290655391
Iter: 978 loss: 0.00289618759
Iter: 979 loss: 0.00297246827
Iter: 980 loss: 0.00289548491
Iter: 981 loss: 0.00288551254
Iter: 982 loss: 0.00288615446
Iter: 983 loss: 0.00287733297
Iter: 984 loss: 0.00286126439
Iter: 985 loss: 0.00285702245
Iter: 986 loss: 0.00284704473
Iter: 987 loss: 0.00282288715
Iter: 988 loss: 0.00290428987
Iter: 989 loss: 0.00281595392
Iter: 990 loss: 0.002798056
Iter: 991 loss: 0.00283590984
Iter: 992 loss: 0.00279101497
Iter: 993 loss: 0.00277263438
Iter: 994 loss: 0.00291823409
Iter: 995 loss: 0.0027704204
Iter: 996 loss: 0.00275406102
Iter: 997 loss: 0.00296973577
Iter: 998 loss: 0.00275399606
Iter: 999 loss: 0.00273726
Iter: 1000 loss: 0.00292076147
Iter: 1001 loss: 0.00273693306
Iter: 1002 loss: 0.0027235602
Iter: 1003 loss: 0.00274950173
Iter: 1004 loss: 0.00271776109
Iter: 1005 loss: 0.0027050646
Iter: 1006 loss: 0.00279390183
Iter: 1007 loss: 0.00270407856
Iter: 1008 loss: 0.00269371551
Iter: 1009 loss: 0.00270094723
Iter: 1010 loss: 0.00268680113
Iter: 1011 loss: 0.00266290456
Iter: 1012 loss: 0.00274651917
Iter: 1013 loss: 0.00265642675
Iter: 1014 loss: 0.00265058596
Iter: 1015 loss: 0.0026411945
Iter: 1016 loss: 0.0026259555
Iter: 1017 loss: 0.00265616458
Iter: 1018 loss: 0.0026196281
Iter: 1019 loss: 0.00260294648
Iter: 1020 loss: 0.00263122376
Iter: 1021 loss: 0.00259486912
Iter: 1022 loss: 0.00257903617
Iter: 1023 loss: 0.00260186894
Iter: 1024 loss: 0.00257124892
Iter: 1025 loss: 0.00255382014
Iter: 1026 loss: 0.0026143752
Iter: 1027 loss: 0.00254927063
Iter: 1028 loss: 0.00253195409
Iter: 1029 loss: 0.00263293
Iter: 1030 loss: 0.00252939807
Iter: 1031 loss: 0.00252022455
Iter: 1032 loss: 0.00251985854
Iter: 1033 loss: 0.00251296326
Iter: 1034 loss: 0.0025076312
Iter: 1035 loss: 0.00250527449
Iter: 1036 loss: 0.00248540798
Iter: 1037 loss: 0.00254055136
Iter: 1038 loss: 0.00247889664
Iter: 1039 loss: 0.0024647559
Iter: 1040 loss: 0.00246211048
Iter: 1041 loss: 0.00244708941
Iter: 1042 loss: 0.00247982144
Iter: 1043 loss: 0.00244118902
Iter: 1044 loss: 0.00242706132
Iter: 1045 loss: 0.00243980857
Iter: 1046 loss: 0.00241895253
Iter: 1047 loss: 0.0024039431
Iter: 1048 loss: 0.00246484368
Iter: 1049 loss: 0.0023999
Iter: 1050 loss: 0.00238211406
Iter: 1051 loss: 0.00248151505
Iter: 1052 loss: 0.00237982767
Iter: 1053 loss: 0.00235986081
Iter: 1054 loss: 0.00235986104
Iter: 1055 loss: 0.00234196428
Iter: 1056 loss: 0.00247531664
Iter: 1057 loss: 0.00234011561
Iter: 1058 loss: 0.0023312103
Iter: 1059 loss: 0.00239419262
Iter: 1060 loss: 0.00233045779
Iter: 1061 loss: 0.00232239184
Iter: 1062 loss: 0.00231523602
Iter: 1063 loss: 0.00231299712
Iter: 1064 loss: 0.00229101442
Iter: 1065 loss: 0.00255522458
Iter: 1066 loss: 0.00229063071
Iter: 1067 loss: 0.00227549113
Iter: 1068 loss: 0.00245312974
Iter: 1069 loss: 0.00227532908
Iter: 1070 loss: 0.00226339418
Iter: 1071 loss: 0.00229586195
Iter: 1072 loss: 0.00225944165
Iter: 1073 loss: 0.00224802
Iter: 1074 loss: 0.00236455584
Iter: 1075 loss: 0.00224770117
Iter: 1076 loss: 0.00223917235
Iter: 1077 loss: 0.00228107208
Iter: 1078 loss: 0.00223765103
Iter: 1079 loss: 0.00223006122
Iter: 1080 loss: 0.00222094706
Iter: 1081 loss: 0.00222012633
Iter: 1082 loss: 0.00220468943
Iter: 1083 loss: 0.00226269383
Iter: 1084 loss: 0.00219998229
Iter: 1085 loss: 0.00219064113
Iter: 1086 loss: 0.00218467321
Iter: 1087 loss: 0.00218106946
Iter: 1088 loss: 0.00216335244
Iter: 1089 loss: 0.00224987534
Iter: 1090 loss: 0.00216032937
Iter: 1091 loss: 0.00214616815
Iter: 1092 loss: 0.00225208537
Iter: 1093 loss: 0.00214501331
Iter: 1094 loss: 0.00213457481
Iter: 1095 loss: 0.00214796048
Iter: 1096 loss: 0.00212911144
Iter: 1097 loss: 0.00211928901
Iter: 1098 loss: 0.00211827876
Iter: 1099 loss: 0.00211111922
Iter: 1100 loss: 0.00209922
Iter: 1101 loss: 0.0021103574
Iter: 1102 loss: 0.00209186971
Iter: 1103 loss: 0.00207604514
Iter: 1104 loss: 0.00216625817
Iter: 1105 loss: 0.00207420439
Iter: 1106 loss: 0.00206256239
Iter: 1107 loss: 0.00206052721
Iter: 1108 loss: 0.0020478433
Iter: 1109 loss: 0.00215237727
Iter: 1110 loss: 0.00204703934
Iter: 1111 loss: 0.00203831913
Iter: 1112 loss: 0.00205746759
Iter: 1113 loss: 0.00203504926
Iter: 1114 loss: 0.00202729763
Iter: 1115 loss: 0.00202806434
Iter: 1116 loss: 0.00202127476
Iter: 1117 loss: 0.00201055524
Iter: 1118 loss: 0.00204531988
Iter: 1119 loss: 0.0020075459
Iter: 1120 loss: 0.00199626083
Iter: 1121 loss: 0.0020185574
Iter: 1122 loss: 0.0019915977
Iter: 1123 loss: 0.00197811075
Iter: 1124 loss: 0.00205197092
Iter: 1125 loss: 0.00197544
Iter: 1126 loss: 0.00196591346
Iter: 1127 loss: 0.00199904293
Iter: 1128 loss: 0.00196352927
Iter: 1129 loss: 0.0019556
Iter: 1130 loss: 0.00198530452
Iter: 1131 loss: 0.0019537
Iter: 1132 loss: 0.00194550364
Iter: 1133 loss: 0.0019514953
Iter: 1134 loss: 0.00194047531
Iter: 1135 loss: 0.00192615343
Iter: 1136 loss: 0.00193451764
Iter: 1137 loss: 0.0019168905
Iter: 1138 loss: 0.00190103624
Iter: 1139 loss: 0.00197622599
Iter: 1140 loss: 0.00189794169
Iter: 1141 loss: 0.00188482064
Iter: 1142 loss: 0.00203443412
Iter: 1143 loss: 0.00188463
Iter: 1144 loss: 0.00187703723
Iter: 1145 loss: 0.00187645445
Iter: 1146 loss: 0.00186764158
Iter: 1147 loss: 0.00189974078
Iter: 1148 loss: 0.00186514715
Iter: 1149 loss: 0.001858545
Iter: 1150 loss: 0.00186637184
Iter: 1151 loss: 0.00185503322
Iter: 1152 loss: 0.00184584572
Iter: 1153 loss: 0.00186815334
Iter: 1154 loss: 0.00184255862
Iter: 1155 loss: 0.00183293689
Iter: 1156 loss: 0.00192452758
Iter: 1157 loss: 0.00183255482
Iter: 1158 loss: 0.00182348653
Iter: 1159 loss: 0.0018449903
Iter: 1160 loss: 0.00182028615
Iter: 1161 loss: 0.00181271124
Iter: 1162 loss: 0.00185694429
Iter: 1163 loss: 0.00181166222
Iter: 1164 loss: 0.00180740049
Iter: 1165 loss: 0.00180327101
Iter: 1166 loss: 0.00180232804
Iter: 1167 loss: 0.00179163727
Iter: 1168 loss: 0.00182105124
Iter: 1169 loss: 0.00178820768
Iter: 1170 loss: 0.00177771144
Iter: 1171 loss: 0.00181878102
Iter: 1172 loss: 0.00177503272
Iter: 1173 loss: 0.00176246837
Iter: 1174 loss: 0.0018043943
Iter: 1175 loss: 0.00175907963
Iter: 1176 loss: 0.00175146968
Iter: 1177 loss: 0.00175047095
Iter: 1178 loss: 0.00174263027
Iter: 1179 loss: 0.0018044177
Iter: 1180 loss: 0.00174208125
Iter: 1181 loss: 0.0017344415
Iter: 1182 loss: 0.00182094472
Iter: 1183 loss: 0.00173428841
Iter: 1184 loss: 0.00172930304
Iter: 1185 loss: 0.00172559754
Iter: 1186 loss: 0.00172393199
Iter: 1187 loss: 0.00171805941
Iter: 1188 loss: 0.00173603371
Iter: 1189 loss: 0.00171603053
Iter: 1190 loss: 0.00170934037
Iter: 1191 loss: 0.00171671389
Iter: 1192 loss: 0.00170570333
Iter: 1193 loss: 0.00169895869
Iter: 1194 loss: 0.00176075171
Iter: 1195 loss: 0.00169866253
Iter: 1196 loss: 0.00169291138
Iter: 1197 loss: 0.00169649639
Iter: 1198 loss: 0.00168922287
Iter: 1199 loss: 0.00168374006
Iter: 1200 loss: 0.00168308278
Iter: 1201 loss: 0.00167914457
Iter: 1202 loss: 0.00167333649
Iter: 1203 loss: 0.00167326978
Iter: 1204 loss: 0.00166540197
Iter: 1205 loss: 0.00167091051
Iter: 1206 loss: 0.00166043686
Iter: 1207 loss: 0.00165117066
Iter: 1208 loss: 0.00166092848
Iter: 1209 loss: 0.00164608832
Iter: 1210 loss: 0.00163637171
Iter: 1211 loss: 0.0016732337
Iter: 1212 loss: 0.00163383433
Iter: 1213 loss: 0.00162820774
Iter: 1214 loss: 0.00164533569
Iter: 1215 loss: 0.00162654312
Iter: 1216 loss: 0.00162022305
Iter: 1217 loss: 0.00162753789
Iter: 1218 loss: 0.00161677529
Iter: 1219 loss: 0.00161000679
Iter: 1220 loss: 0.00161409564
Iter: 1221 loss: 0.00160566648
Iter: 1222 loss: 0.00159889378
Iter: 1223 loss: 0.00160402758
Iter: 1224 loss: 0.00159473671
Iter: 1225 loss: 0.00158588425
Iter: 1226 loss: 0.0016007917
Iter: 1227 loss: 0.00158191309
Iter: 1228 loss: 0.00157595053
Iter: 1229 loss: 0.00157538988
Iter: 1230 loss: 0.00156911439
Iter: 1231 loss: 0.00157352164
Iter: 1232 loss: 0.00156523171
Iter: 1233 loss: 0.00155952107
Iter: 1234 loss: 0.00157340849
Iter: 1235 loss: 0.00155727169
Iter: 1236 loss: 0.00155305327
Iter: 1237 loss: 0.00155584922
Iter: 1238 loss: 0.00155040319
Iter: 1239 loss: 0.00154551491
Iter: 1240 loss: 0.00154873426
Iter: 1241 loss: 0.00154240034
Iter: 1242 loss: 0.00153382716
Iter: 1243 loss: 0.00161066977
Iter: 1244 loss: 0.00153342029
Iter: 1245 loss: 0.00152633572
Iter: 1246 loss: 0.0015825649
Iter: 1247 loss: 0.00152581383
Iter: 1248 loss: 0.0015202557
Iter: 1249 loss: 0.00152856251
Iter: 1250 loss: 0.00151756499
Iter: 1251 loss: 0.00151174329
Iter: 1252 loss: 0.00152263395
Iter: 1253 loss: 0.00150935072
Iter: 1254 loss: 0.00150395161
Iter: 1255 loss: 0.00151135423
Iter: 1256 loss: 0.00150124019
Iter: 1257 loss: 0.00149570405
Iter: 1258 loss: 0.00149147725
Iter: 1259 loss: 0.00148971193
Iter: 1260 loss: 0.00148141407
Iter: 1261 loss: 0.00150971767
Iter: 1262 loss: 0.00147919846
Iter: 1263 loss: 0.00147330726
Iter: 1264 loss: 0.00153469853
Iter: 1265 loss: 0.00147315115
Iter: 1266 loss: 0.00146729499
Iter: 1267 loss: 0.00148450944
Iter: 1268 loss: 0.00146546168
Iter: 1269 loss: 0.00145979878
Iter: 1270 loss: 0.00147783197
Iter: 1271 loss: 0.00145821972
Iter: 1272 loss: 0.00145299162
Iter: 1273 loss: 0.00146875624
Iter: 1274 loss: 0.00145143154
Iter: 1275 loss: 0.00144661218
Iter: 1276 loss: 0.0014418253
Iter: 1277 loss: 0.00144081772
Iter: 1278 loss: 0.00143376947
Iter: 1279 loss: 0.00148529583
Iter: 1280 loss: 0.00143316272
Iter: 1281 loss: 0.00142753124
Iter: 1282 loss: 0.00145098125
Iter: 1283 loss: 0.00142633729
Iter: 1284 loss: 0.00142166845
Iter: 1285 loss: 0.00144634349
Iter: 1286 loss: 0.00142094505
Iter: 1287 loss: 0.0014160634
Iter: 1288 loss: 0.00143972994
Iter: 1289 loss: 0.00141522067
Iter: 1290 loss: 0.00141131552
Iter: 1291 loss: 0.00141451147
Iter: 1292 loss: 0.00140896626
Iter: 1293 loss: 0.00140430022
Iter: 1294 loss: 0.00140082883
Iter: 1295 loss: 0.00139928819
Iter: 1296 loss: 0.00139188289
Iter: 1297 loss: 0.00139492715
Iter: 1298 loss: 0.00138675922
Iter: 1299 loss: 0.00138381706
Iter: 1300 loss: 0.00138198817
Iter: 1301 loss: 0.00137743657
Iter: 1302 loss: 0.00137458113
Iter: 1303 loss: 0.00137271127
Iter: 1304 loss: 0.00136497756
Iter: 1305 loss: 0.00144895329
Iter: 1306 loss: 0.00136482203
Iter: 1307 loss: 0.00136060687
Iter: 1308 loss: 0.00135658099
Iter: 1309 loss: 0.00135561044
Iter: 1310 loss: 0.00134970201
Iter: 1311 loss: 0.00136627548
Iter: 1312 loss: 0.0013478063
Iter: 1313 loss: 0.00134169846
Iter: 1314 loss: 0.0013548272
Iter: 1315 loss: 0.00133931905
Iter: 1316 loss: 0.00133373961
Iter: 1317 loss: 0.00139004679
Iter: 1318 loss: 0.00133358804
Iter: 1319 loss: 0.00132933795
Iter: 1320 loss: 0.00133761019
Iter: 1321 loss: 0.00132754422
Iter: 1322 loss: 0.00132254011
Iter: 1323 loss: 0.00134938699
Iter: 1324 loss: 0.00132179377
Iter: 1325 loss: 0.00131852226
Iter: 1326 loss: 0.00131622632
Iter: 1327 loss: 0.00131503399
Iter: 1328 loss: 0.00131080172
Iter: 1329 loss: 0.00131526194
Iter: 1330 loss: 0.00130847911
Iter: 1331 loss: 0.00130364089
Iter: 1332 loss: 0.00131008448
Iter: 1333 loss: 0.00130108779
Iter: 1334 loss: 0.00129655655
Iter: 1335 loss: 0.00131472026
Iter: 1336 loss: 0.00129553443
Iter: 1337 loss: 0.001290799
Iter: 1338 loss: 0.00130328094
Iter: 1339 loss: 0.00128920702
Iter: 1340 loss: 0.00128461479
Iter: 1341 loss: 0.00129771663
Iter: 1342 loss: 0.00128312781
Iter: 1343 loss: 0.00127741671
Iter: 1344 loss: 0.00128323562
Iter: 1345 loss: 0.00127424672
Iter: 1346 loss: 0.00126864575
Iter: 1347 loss: 0.00128472818
Iter: 1348 loss: 0.00126688299
Iter: 1349 loss: 0.0012600777
Iter: 1350 loss: 0.0012940201
Iter: 1351 loss: 0.00125889445
Iter: 1352 loss: 0.00125232129
Iter: 1353 loss: 0.00125227775
Iter: 1354 loss: 0.00124851963
Iter: 1355 loss: 0.00126227667
Iter: 1356 loss: 0.0012476
Iter: 1357 loss: 0.0012430707
Iter: 1358 loss: 0.00124176871
Iter: 1359 loss: 0.00123894145
Iter: 1360 loss: 0.00123488193
Iter: 1361 loss: 0.00124923838
Iter: 1362 loss: 0.00123385026
Iter: 1363 loss: 0.00123018562
Iter: 1364 loss: 0.00122823077
Iter: 1365 loss: 0.00122656859
Iter: 1366 loss: 0.00122063351
Iter: 1367 loss: 0.00124043087
Iter: 1368 loss: 0.00121901231
Iter: 1369 loss: 0.00121496338
Iter: 1370 loss: 0.00121496373
Iter: 1371 loss: 0.00121166033
Iter: 1372 loss: 0.00121022842
Iter: 1373 loss: 0.00120853272
Iter: 1374 loss: 0.00120363501
Iter: 1375 loss: 0.00123254769
Iter: 1376 loss: 0.00120300637
Iter: 1377 loss: 0.00119936187
Iter: 1378 loss: 0.00123520091
Iter: 1379 loss: 0.00119923474
Iter: 1380 loss: 0.0011966551
Iter: 1381 loss: 0.00119135133
Iter: 1382 loss: 0.00128617208
Iter: 1383 loss: 0.00119124888
Iter: 1384 loss: 0.00118482299
Iter: 1385 loss: 0.00120529032
Iter: 1386 loss: 0.00118296908
Iter: 1387 loss: 0.00117811374
Iter: 1388 loss: 0.00121525198
Iter: 1389 loss: 0.00117776159
Iter: 1390 loss: 0.00117535796
Iter: 1391 loss: 0.00117535761
Iter: 1392 loss: 0.00117270439
Iter: 1393 loss: 0.00117791293
Iter: 1394 loss: 0.00117161078
Iter: 1395 loss: 0.00116957794
Iter: 1396 loss: 0.00116858189
Iter: 1397 loss: 0.00116759329
Iter: 1398 loss: 0.0011636382
Iter: 1399 loss: 0.00116507057
Iter: 1400 loss: 0.00116086518
Iter: 1401 loss: 0.00115620659
Iter: 1402 loss: 0.00116871973
Iter: 1403 loss: 0.00115466211
Iter: 1404 loss: 0.00114952424
Iter: 1405 loss: 0.00117934379
Iter: 1406 loss: 0.00114885019
Iter: 1407 loss: 0.00114488543
Iter: 1408 loss: 0.00115613232
Iter: 1409 loss: 0.00114362047
Iter: 1410 loss: 0.00113977538
Iter: 1411 loss: 0.00114300009
Iter: 1412 loss: 0.00113750226
Iter: 1413 loss: 0.0011334609
Iter: 1414 loss: 0.00113345217
Iter: 1415 loss: 0.00113027007
Iter: 1416 loss: 0.00112885621
Iter: 1417 loss: 0.00112722605
Iter: 1418 loss: 0.00112378318
Iter: 1419 loss: 0.00112448668
Iter: 1420 loss: 0.00112126081
Iter: 1421 loss: 0.0011175063
Iter: 1422 loss: 0.00112417282
Iter: 1423 loss: 0.00111584459
Iter: 1424 loss: 0.0011121924
Iter: 1425 loss: 0.00111038052
Iter: 1426 loss: 0.00110866898
Iter: 1427 loss: 0.00110294856
Iter: 1428 loss: 0.00117690396
Iter: 1429 loss: 0.0011028985
Iter: 1430 loss: 0.00109867367
Iter: 1431 loss: 0.0011130115
Iter: 1432 loss: 0.00109754165
Iter: 1433 loss: 0.00109330739
Iter: 1434 loss: 0.00110436557
Iter: 1435 loss: 0.00109186838
Iter: 1436 loss: 0.00108805113
Iter: 1437 loss: 0.00110424112
Iter: 1438 loss: 0.00108723785
Iter: 1439 loss: 0.00108397903
Iter: 1440 loss: 0.00109624083
Iter: 1441 loss: 0.00108318741
Iter: 1442 loss: 0.00108008157
Iter: 1443 loss: 0.00108483876
Iter: 1444 loss: 0.00107860984
Iter: 1445 loss: 0.00107518025
Iter: 1446 loss: 0.00108139974
Iter: 1447 loss: 0.00107369176
Iter: 1448 loss: 0.00107066578
Iter: 1449 loss: 0.00107065635
Iter: 1450 loss: 0.00106898171
Iter: 1451 loss: 0.00107659807
Iter: 1452 loss: 0.00106866052
Iter: 1453 loss: 0.00106720044
Iter: 1454 loss: 0.00106581626
Iter: 1455 loss: 0.00106547843
Iter: 1456 loss: 0.00106297445
Iter: 1457 loss: 0.0010639485
Iter: 1458 loss: 0.00106123858
Iter: 1459 loss: 0.00105852238
Iter: 1460 loss: 0.00108291779
Iter: 1461 loss: 0.00105839327
Iter: 1462 loss: 0.00105608196
Iter: 1463 loss: 0.00105466612
Iter: 1464 loss: 0.00105372979
Iter: 1465 loss: 0.00104979356
Iter: 1466 loss: 0.001066325
Iter: 1467 loss: 0.00104894675
Iter: 1468 loss: 0.00104485382
Iter: 1469 loss: 0.00105543144
Iter: 1470 loss: 0.00104344939
Iter: 1471 loss: 0.00103906216
Iter: 1472 loss: 0.00106003811
Iter: 1473 loss: 0.00103826169
Iter: 1474 loss: 0.0010346052
Iter: 1475 loss: 0.00105020241
Iter: 1476 loss: 0.00103383325
Iter: 1477 loss: 0.00103086699
Iter: 1478 loss: 0.00103369157
Iter: 1479 loss: 0.00102917268
Iter: 1480 loss: 0.00102650328
Iter: 1481 loss: 0.00106250565
Iter: 1482 loss: 0.00102649466
Iter: 1483 loss: 0.00102436473
Iter: 1484 loss: 0.00103614968
Iter: 1485 loss: 0.0010240603
Iter: 1486 loss: 0.00102211675
Iter: 1487 loss: 0.00102052721
Iter: 1488 loss: 0.00101994665
Iter: 1489 loss: 0.00101719401
Iter: 1490 loss: 0.00101902732
Iter: 1491 loss: 0.00101546152
Iter: 1492 loss: 0.00101232459
Iter: 1493 loss: 0.00102605741
Iter: 1494 loss: 0.00101167988
Iter: 1495 loss: 0.00100843317
Iter: 1496 loss: 0.00101489376
Iter: 1497 loss: 0.00100710464
Iter: 1498 loss: 0.00100385072
Iter: 1499 loss: 0.00101232482
Iter: 1500 loss: 0.00100274466
Iter: 1501 loss: 0.000999829848
Iter: 1502 loss: 0.00101958588
Iter: 1503 loss: 0.000999538694
Iter: 1504 loss: 0.000996720279
Iter: 1505 loss: 0.00100294454
Iter: 1506 loss: 0.00099563715
Iter: 1507 loss: 0.000992473681
Iter: 1508 loss: 0.00100013614
Iter: 1509 loss: 0.000991341658
Iter: 1510 loss: 0.000988395652
Iter: 1511 loss: 0.000996927731
Iter: 1512 loss: 0.000987479929
Iter: 1513 loss: 0.000984766753
Iter: 1514 loss: 0.000991374487
Iter: 1515 loss: 0.000983778387
Iter: 1516 loss: 0.000981254852
Iter: 1517 loss: 0.00098121
Iter: 1518 loss: 0.000979646342
Iter: 1519 loss: 0.000978188356
Iter: 1520 loss: 0.000977815478
Iter: 1521 loss: 0.000975405623
Iter: 1522 loss: 0.000975029892
Iter: 1523 loss: 0.000973356946
Iter: 1524 loss: 0.000969908433
Iter: 1525 loss: 0.000978272292
Iter: 1526 loss: 0.000968660461
Iter: 1527 loss: 0.000965240062
Iter: 1528 loss: 0.000988046871
Iter: 1529 loss: 0.000964895822
Iter: 1530 loss: 0.000961839687
Iter: 1531 loss: 0.000959412311
Iter: 1532 loss: 0.000958471443
Iter: 1533 loss: 0.000953761279
Iter: 1534 loss: 0.000972284353
Iter: 1535 loss: 0.000952677685
Iter: 1536 loss: 0.000948951463
Iter: 1537 loss: 0.000978506403
Iter: 1538 loss: 0.000948702043
Iter: 1539 loss: 0.000945109408
Iter: 1540 loss: 0.000947319844
Iter: 1541 loss: 0.000942815328
Iter: 1542 loss: 0.000938664773
Iter: 1543 loss: 0.000960762729
Iter: 1544 loss: 0.000938007375
Iter: 1545 loss: 0.000934681215
Iter: 1546 loss: 0.000934239593
Iter: 1547 loss: 0.000931883231
Iter: 1548 loss: 0.0009277481
Iter: 1549 loss: 0.000949740759
Iter: 1550 loss: 0.000927111716
Iter: 1551 loss: 0.000927629881
Iter: 1552 loss: 0.000926026667
Iter: 1553 loss: 0.000924988301
Iter: 1554 loss: 0.0009224643
Iter: 1555 loss: 0.000948687608
Iter: 1556 loss: 0.000922177103
Iter: 1557 loss: 0.000918275677
Iter: 1558 loss: 0.000929173781
Iter: 1559 loss: 0.000916998717
Iter: 1560 loss: 0.000913364
Iter: 1561 loss: 0.000924451277
Iter: 1562 loss: 0.000912288
Iter: 1563 loss: 0.000909126422
Iter: 1564 loss: 0.000932851457
Iter: 1565 loss: 0.000908879447
Iter: 1566 loss: 0.000906400732
Iter: 1567 loss: 0.000907731766
Iter: 1568 loss: 0.000904778892
Iter: 1569 loss: 0.000901208725
Iter: 1570 loss: 0.000904115266
Iter: 1571 loss: 0.00089905964
Iter: 1572 loss: 0.000895498903
Iter: 1573 loss: 0.00090213184
Iter: 1574 loss: 0.000893955817
Iter: 1575 loss: 0.000889971503
Iter: 1576 loss: 0.000939497375
Iter: 1577 loss: 0.00088993425
Iter: 1578 loss: 0.000887299422
Iter: 1579 loss: 0.000886043184
Iter: 1580 loss: 0.000884748937
Iter: 1581 loss: 0.000881658518
Iter: 1582 loss: 0.000885633868
Iter: 1583 loss: 0.000880074571
Iter: 1584 loss: 0.000876478502
Iter: 1585 loss: 0.000903751
Iter: 1586 loss: 0.000876208069
Iter: 1587 loss: 0.000872742501
Iter: 1588 loss: 0.000908936083
Iter: 1589 loss: 0.000872649834
Iter: 1590 loss: 0.000871015945
Iter: 1591 loss: 0.000867149327
Iter: 1592 loss: 0.000911073643
Iter: 1593 loss: 0.000866793911
Iter: 1594 loss: 0.000862666348
Iter: 1595 loss: 0.000886405
Iter: 1596 loss: 0.000862096553
Iter: 1597 loss: 0.000858947635
Iter: 1598 loss: 0.000863164081
Iter: 1599 loss: 0.000857369392
Iter: 1600 loss: 0.000854224199
Iter: 1601 loss: 0.000874845893
Iter: 1602 loss: 0.000853898353
Iter: 1603 loss: 0.000851174467
Iter: 1604 loss: 0.000853020872
Iter: 1605 loss: 0.000849466771
Iter: 1606 loss: 0.000845337519
Iter: 1607 loss: 0.00085269555
Iter: 1608 loss: 0.000843497925
Iter: 1609 loss: 0.000840383349
Iter: 1610 loss: 0.000866066199
Iter: 1611 loss: 0.000840198831
Iter: 1612 loss: 0.000838135951
Iter: 1613 loss: 0.000844894792
Iter: 1614 loss: 0.00083756051
Iter: 1615 loss: 0.000835104496
Iter: 1616 loss: 0.000832645
Iter: 1617 loss: 0.000832146208
Iter: 1618 loss: 0.000828194665
Iter: 1619 loss: 0.000844959577
Iter: 1620 loss: 0.000827358395
Iter: 1621 loss: 0.000824348885
Iter: 1622 loss: 0.0008312508
Iter: 1623 loss: 0.000823218899
Iter: 1624 loss: 0.00082275673
Iter: 1625 loss: 0.000821770926
Iter: 1626 loss: 0.000820366258
Iter: 1627 loss: 0.000817856868
Iter: 1628 loss: 0.000882263179
Iter: 1629 loss: 0.000817855587
Iter: 1630 loss: 0.000815382577
Iter: 1631 loss: 0.000815844396
Iter: 1632 loss: 0.000813529361
Iter: 1633 loss: 0.000810152153
Iter: 1634 loss: 0.000836764812
Iter: 1635 loss: 0.000809907448
Iter: 1636 loss: 0.000807658653
Iter: 1637 loss: 0.000811938662
Iter: 1638 loss: 0.000806715
Iter: 1639 loss: 0.000803682313
Iter: 1640 loss: 0.000808173208
Iter: 1641 loss: 0.000802218448
Iter: 1642 loss: 0.000799044268
Iter: 1643 loss: 0.000820100773
Iter: 1644 loss: 0.000798710273
Iter: 1645 loss: 0.000796880107
Iter: 1646 loss: 0.000815066043
Iter: 1647 loss: 0.000796819339
Iter: 1648 loss: 0.00079540041
Iter: 1649 loss: 0.000793497602
Iter: 1650 loss: 0.000793394516
Iter: 1651 loss: 0.000790624879
Iter: 1652 loss: 0.000792712905
Iter: 1653 loss: 0.000788928126
Iter: 1654 loss: 0.000785380485
Iter: 1655 loss: 0.000797597691
Iter: 1656 loss: 0.000784427277
Iter: 1657 loss: 0.000781491282
Iter: 1658 loss: 0.000793285784
Iter: 1659 loss: 0.000780826958
Iter: 1660 loss: 0.000780829811
Iter: 1661 loss: 0.000779355061
Iter: 1662 loss: 0.000777908484
Iter: 1663 loss: 0.000776649918
Iter: 1664 loss: 0.00077626
Iter: 1665 loss: 0.000774329295
Iter: 1666 loss: 0.000775669585
Iter: 1667 loss: 0.000773124048
Iter: 1668 loss: 0.00077083928
Iter: 1669 loss: 0.000772504
Iter: 1670 loss: 0.000769412844
Iter: 1671 loss: 0.000766813231
Iter: 1672 loss: 0.000774044311
Iter: 1673 loss: 0.000765980803
Iter: 1674 loss: 0.00076374755
Iter: 1675 loss: 0.000774316723
Iter: 1676 loss: 0.0007633348
Iter: 1677 loss: 0.000761178439
Iter: 1678 loss: 0.000761559699
Iter: 1679 loss: 0.000759555143
Iter: 1680 loss: 0.000756922062
Iter: 1681 loss: 0.00076891447
Iter: 1682 loss: 0.000756456
Iter: 1683 loss: 0.000754418143
Iter: 1684 loss: 0.000756525551
Iter: 1685 loss: 0.000753276865
Iter: 1686 loss: 0.00075065461
Iter: 1687 loss: 0.000753849163
Iter: 1688 loss: 0.000749279221
Iter: 1689 loss: 0.000746145961
Iter: 1690 loss: 0.000758071837
Iter: 1691 loss: 0.000745394093
Iter: 1692 loss: 0.000743258453
Iter: 1693 loss: 0.000755044923
Iter: 1694 loss: 0.000742949778
Iter: 1695 loss: 0.000741104595
Iter: 1696 loss: 0.000765561534
Iter: 1697 loss: 0.000741091266
Iter: 1698 loss: 0.000739632058
Iter: 1699 loss: 0.000738572911
Iter: 1700 loss: 0.000738071278
Iter: 1701 loss: 0.000735924114
Iter: 1702 loss: 0.000738490548
Iter: 1703 loss: 0.000734765781
Iter: 1704 loss: 0.000732217799
Iter: 1705 loss: 0.000733835157
Iter: 1706 loss: 0.000730606
Iter: 1707 loss: 0.000727508
Iter: 1708 loss: 0.000745809521
Iter: 1709 loss: 0.000727103383
Iter: 1710 loss: 0.000724997255
Iter: 1711 loss: 0.000734741159
Iter: 1712 loss: 0.000724606041
Iter: 1713 loss: 0.000722571276
Iter: 1714 loss: 0.000723493518
Iter: 1715 loss: 0.000721193617
Iter: 1716 loss: 0.00071838533
Iter: 1717 loss: 0.000735356589
Iter: 1718 loss: 0.000718035735
Iter: 1719 loss: 0.000716036186
Iter: 1720 loss: 0.000717615068
Iter: 1721 loss: 0.000714820926
Iter: 1722 loss: 0.000711956294
Iter: 1723 loss: 0.000713979243
Iter: 1724 loss: 0.000710162101
Iter: 1725 loss: 0.000707329134
Iter: 1726 loss: 0.000722678553
Iter: 1727 loss: 0.000706907944
Iter: 1728 loss: 0.000705866143
Iter: 1729 loss: 0.000705506769
Iter: 1730 loss: 0.000703945407
Iter: 1731 loss: 0.000706717779
Iter: 1732 loss: 0.000703246682
Iter: 1733 loss: 0.000701679382
Iter: 1734 loss: 0.000700042816
Iter: 1735 loss: 0.000699752
Iter: 1736 loss: 0.000697544077
Iter: 1737 loss: 0.000709838234
Iter: 1738 loss: 0.000697223179
Iter: 1739 loss: 0.000695187598
Iter: 1740 loss: 0.000696571078
Iter: 1741 loss: 0.000693902839
Iter: 1742 loss: 0.000691394089
Iter: 1743 loss: 0.000701688288
Iter: 1744 loss: 0.00069084554
Iter: 1745 loss: 0.000689028879
Iter: 1746 loss: 0.000699096185
Iter: 1747 loss: 0.000688765838
Iter: 1748 loss: 0.000686866697
Iter: 1749 loss: 0.00068436889
Iter: 1750 loss: 0.000684211147
Iter: 1751 loss: 0.000681294245
Iter: 1752 loss: 0.000710398774
Iter: 1753 loss: 0.000681201462
Iter: 1754 loss: 0.000678221346
Iter: 1755 loss: 0.00069724489
Iter: 1756 loss: 0.000677904405
Iter: 1757 loss: 0.000675628311
Iter: 1758 loss: 0.000679622521
Iter: 1759 loss: 0.00067460537
Iter: 1760 loss: 0.000672567869
Iter: 1761 loss: 0.000673895585
Iter: 1762 loss: 0.000671286834
Iter: 1763 loss: 0.000669100555
Iter: 1764 loss: 0.000688651926
Iter: 1765 loss: 0.000668999855
Iter: 1766 loss: 0.000667741173
Iter: 1767 loss: 0.000667599263
Iter: 1768 loss: 0.000666898908
Iter: 1769 loss: 0.000665292609
Iter: 1770 loss: 0.000685744337
Iter: 1771 loss: 0.000665179221
Iter: 1772 loss: 0.00066335022
Iter: 1773 loss: 0.000663807034
Iter: 1774 loss: 0.000662017264
Iter: 1775 loss: 0.000660123536
Iter: 1776 loss: 0.0006720128
Iter: 1777 loss: 0.000659901067
Iter: 1778 loss: 0.000658044824
Iter: 1779 loss: 0.000660744088
Iter: 1780 loss: 0.000657140685
Iter: 1781 loss: 0.000655095151
Iter: 1782 loss: 0.00066148804
Iter: 1783 loss: 0.000654485135
Iter: 1784 loss: 0.000652322
Iter: 1785 loss: 0.000660689664
Iter: 1786 loss: 0.000651813752
Iter: 1787 loss: 0.000649957685
Iter: 1788 loss: 0.000651120616
Iter: 1789 loss: 0.000648772693
Iter: 1790 loss: 0.00064693595
Iter: 1791 loss: 0.000659093843
Iter: 1792 loss: 0.000646747
Iter: 1793 loss: 0.000645019521
Iter: 1794 loss: 0.00064587855
Iter: 1795 loss: 0.000643865264
Iter: 1796 loss: 0.000641861116
Iter: 1797 loss: 0.000648898887
Iter: 1798 loss: 0.000641335384
Iter: 1799 loss: 0.000639417791
Iter: 1800 loss: 0.000640893471
Iter: 1801 loss: 0.000638253056
Iter: 1802 loss: 0.000636573415
Iter: 1803 loss: 0.000661057
Iter: 1804 loss: 0.000636570621
Iter: 1805 loss: 0.00063459581
Iter: 1806 loss: 0.000638821512
Iter: 1807 loss: 0.000633816
Iter: 1808 loss: 0.000632741896
Iter: 1809 loss: 0.000632095034
Iter: 1810 loss: 0.000631648116
Iter: 1811 loss: 0.000629211834
Iter: 1812 loss: 0.000631393457
Iter: 1813 loss: 0.000627797446
Iter: 1814 loss: 0.000625434623
Iter: 1815 loss: 0.000648490677
Iter: 1816 loss: 0.000625346787
Iter: 1817 loss: 0.000623454049
Iter: 1818 loss: 0.000625723449
Iter: 1819 loss: 0.000622451829
Iter: 1820 loss: 0.000620105828
Iter: 1821 loss: 0.00062461535
Iter: 1822 loss: 0.000619119965
Iter: 1823 loss: 0.000616820529
Iter: 1824 loss: 0.000640712446
Iter: 1825 loss: 0.000616756559
Iter: 1826 loss: 0.000615627621
Iter: 1827 loss: 0.0006204698
Iter: 1828 loss: 0.00061538897
Iter: 1829 loss: 0.000614165037
Iter: 1830 loss: 0.000613447162
Iter: 1831 loss: 0.000612936565
Iter: 1832 loss: 0.000610968564
Iter: 1833 loss: 0.000612092088
Iter: 1834 loss: 0.000609683106
Iter: 1835 loss: 0.000607623602
Iter: 1836 loss: 0.000611627649
Iter: 1837 loss: 0.000606774411
Iter: 1838 loss: 0.000608169357
Iter: 1839 loss: 0.000605983892
Iter: 1840 loss: 0.000605509791
Iter: 1841 loss: 0.000604248256
Iter: 1842 loss: 0.000613061537
Iter: 1843 loss: 0.000603967812
Iter: 1844 loss: 0.000601827342
Iter: 1845 loss: 0.000602485146
Iter: 1846 loss: 0.000600275234
Iter: 1847 loss: 0.000597746694
Iter: 1848 loss: 0.000621123123
Iter: 1849 loss: 0.000597636856
Iter: 1850 loss: 0.000595239049
Iter: 1851 loss: 0.000601041247
Iter: 1852 loss: 0.000594378333
Iter: 1853 loss: 0.00059281697
Iter: 1854 loss: 0.000601554755
Iter: 1855 loss: 0.000592589611
Iter: 1856 loss: 0.000590945478
Iter: 1857 loss: 0.000590645883
Iter: 1858 loss: 0.000589538948
Iter: 1859 loss: 0.000587789691
Iter: 1860 loss: 0.000602597429
Iter: 1861 loss: 0.000587688584
Iter: 1862 loss: 0.000586278271
Iter: 1863 loss: 0.000587689632
Iter: 1864 loss: 0.000585484668
Iter: 1865 loss: 0.000583742105
Iter: 1866 loss: 0.000585690374
Iter: 1867 loss: 0.000582809211
Iter: 1868 loss: 0.000580880325
Iter: 1869 loss: 0.000586594746
Iter: 1870 loss: 0.000580289052
Iter: 1871 loss: 0.000580967288
Iter: 1872 loss: 0.000579382409
Iter: 1873 loss: 0.000578692125
Iter: 1874 loss: 0.000579326646
Iter: 1875 loss: 0.000578276231
Iter: 1876 loss: 0.00057633681
Iter: 1877 loss: 0.000577767962
Iter: 1878 loss: 0.000575145124
Iter: 1879 loss: 0.00057265535
Iter: 1880 loss: 0.000577606959
Iter: 1881 loss: 0.000571634853
Iter: 1882 loss: 0.000569486525
Iter: 1883 loss: 0.000597375445
Iter: 1884 loss: 0.000569471624
Iter: 1885 loss: 0.000568067946
Iter: 1886 loss: 0.000574782316
Iter: 1887 loss: 0.000567816664
Iter: 1888 loss: 0.000566468865
Iter: 1889 loss: 0.000567652751
Iter: 1890 loss: 0.000565679162
Iter: 1891 loss: 0.00056379661
Iter: 1892 loss: 0.000563605281
Iter: 1893 loss: 0.000562219473
Iter: 1894 loss: 0.000560249144
Iter: 1895 loss: 0.00056355464
Iter: 1896 loss: 0.000559363863
Iter: 1897 loss: 0.000557817
Iter: 1898 loss: 0.000582814275
Iter: 1899 loss: 0.000557817286
Iter: 1900 loss: 0.000556747545
Iter: 1901 loss: 0.000559929525
Iter: 1902 loss: 0.000556424493
Iter: 1903 loss: 0.000555513136
Iter: 1904 loss: 0.000564322
Iter: 1905 loss: 0.000555482926
Iter: 1906 loss: 0.000554553466
Iter: 1907 loss: 0.000554235769
Iter: 1908 loss: 0.000553702121
Iter: 1909 loss: 0.000552725163
Iter: 1910 loss: 0.000551519
Iter: 1911 loss: 0.000551413628
Iter: 1912 loss: 0.000549148303
Iter: 1913 loss: 0.000548049
Iter: 1914 loss: 0.000546956202
Iter: 1915 loss: 0.000544148905
Iter: 1916 loss: 0.000565771654
Iter: 1917 loss: 0.000543940696
Iter: 1918 loss: 0.000541578338
Iter: 1919 loss: 0.000552101294
Iter: 1920 loss: 0.000541109068
Iter: 1921 loss: 0.000539048575
Iter: 1922 loss: 0.00055617094
Iter: 1923 loss: 0.000538923778
Iter: 1924 loss: 0.000537722255
Iter: 1925 loss: 0.000540660345
Iter: 1926 loss: 0.000537296059
Iter: 1927 loss: 0.000535793
Iter: 1928 loss: 0.000537576852
Iter: 1929 loss: 0.000534985506
Iter: 1930 loss: 0.000533754239
Iter: 1931 loss: 0.000534628402
Iter: 1932 loss: 0.00053299393
Iter: 1933 loss: 0.000531588914
Iter: 1934 loss: 0.000535849249
Iter: 1935 loss: 0.000531162892
Iter: 1936 loss: 0.000529886223
Iter: 1937 loss: 0.000539348344
Iter: 1938 loss: 0.000529780169
Iter: 1939 loss: 0.000528628181
Iter: 1940 loss: 0.000539864239
Iter: 1941 loss: 0.000528589066
Iter: 1942 loss: 0.000528043834
Iter: 1943 loss: 0.000527027179
Iter: 1944 loss: 0.000550351047
Iter: 1945 loss: 0.000527025142
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script80
+ '[' -r STOP.script80 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2.8
+ date
Mon Nov  2 15:33:31 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35403e3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35403e3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3540507d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f354040c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35403fdd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35401cb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f354040ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35402448c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3540241378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35402417b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3540241ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3540103d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3540127048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35400adb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3540310268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35402f3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3540317598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3540317e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f4780c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35402be620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35402e99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35402e9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f4757f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f45ed840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f45edae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f4618378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35400d0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35400d08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f46f3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f4710730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f354015dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f4592158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f45921e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f469eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f46899d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34f45bbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.008374706
test_loss: 0.009737847
train_loss: 0.005530763
test_loss: 0.0059044072
train_loss: 0.0038408
test_loss: 0.0044582644
train_loss: 0.0029480893
test_loss: 0.0040617674
train_loss: 0.0024354816
test_loss: 0.0031962888
train_loss: 0.0025337718
test_loss: 0.002680368
train_loss: 0.0020029622
test_loss: 0.0023507783
train_loss: 0.0013962032
test_loss: 0.0020482622
train_loss: 0.0011526694
test_loss: 0.001937421
train_loss: 0.0010922214
test_loss: 0.0016431918
train_loss: 0.00090339384
test_loss: 0.0015612934
train_loss: 0.0011265771
test_loss: 0.0015332961
train_loss: 0.00068855955
test_loss: 0.001470597
train_loss: 0.0009840474
test_loss: 0.0013282676
train_loss: 0.0006721443
test_loss: 0.0012984116
train_loss: 0.0007106182
test_loss: 0.0012365526
train_loss: 0.0007053524
test_loss: 0.0011763035
train_loss: 0.0006201049
test_loss: 0.0011510665
train_loss: 0.0005223083
test_loss: 0.0011056715
train_loss: 0.0007373282
test_loss: 0.0011070546
train_loss: 0.00066799484
test_loss: 0.0010525307
train_loss: 0.00074916263
test_loss: 0.0009937341
train_loss: 0.00043724506
test_loss: 0.0009951289
train_loss: 0.00046419678
test_loss: 0.0009443108
train_loss: 0.00047868016
test_loss: 0.0009535438
train_loss: 0.0005065861
test_loss: 0.00093211833
train_loss: 0.0005512457
test_loss: 0.0009158006
train_loss: 0.00046336433
test_loss: 0.00091578614
train_loss: 0.0005002166
test_loss: 0.0008869043
train_loss: 0.00044152004
test_loss: 0.0008945402
train_loss: 0.0004299872
test_loss: 0.0008879027
train_loss: 0.00037718762
test_loss: 0.00086861517
train_loss: 0.00040887544
test_loss: 0.00085937406
train_loss: 0.0003566015
test_loss: 0.0008602651
train_loss: 0.00039840757
test_loss: 0.00084487454
train_loss: 0.00042747176
test_loss: 0.00084182486
train_loss: 0.00035583886
test_loss: 0.0008422693
train_loss: 0.00041504443
test_loss: 0.0008396764
train_loss: 0.0003284877
test_loss: 0.0008352813
train_loss: 0.00032719344
test_loss: 0.00083405647
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838855fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838851cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838851c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838851cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83884c77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83884c6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8388455e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83884c7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83884349d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838841f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838838f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83883eeea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83883e5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838835ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838830c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83882fed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838831f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838831fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838823a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838823a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83882e36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83882e3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83881d79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838818fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838818f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83881302f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838816f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8388147840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8388147158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838821b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83880e9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f838803d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83880572f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8388074730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83880b19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8370211268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000358794787
Iter: 2 loss: 0.000362474297
Iter: 3 loss: 0.000352109404
Iter: 4 loss: 0.0003479718
Iter: 5 loss: 0.000369099493
Iter: 6 loss: 0.000347323949
Iter: 7 loss: 0.000343653199
Iter: 8 loss: 0.000348546513
Iter: 9 loss: 0.00034179003
Iter: 10 loss: 0.000339145598
Iter: 11 loss: 0.000341014966
Iter: 12 loss: 0.000337516249
Iter: 13 loss: 0.000334106851
Iter: 14 loss: 0.000342422456
Iter: 15 loss: 0.000332865107
Iter: 16 loss: 0.000329390488
Iter: 17 loss: 0.000341777719
Iter: 18 loss: 0.000328513095
Iter: 19 loss: 0.000325775793
Iter: 20 loss: 0.000330912182
Iter: 21 loss: 0.000324608729
Iter: 22 loss: 0.000321760512
Iter: 23 loss: 0.000330509705
Iter: 24 loss: 0.000320919149
Iter: 25 loss: 0.000318697945
Iter: 26 loss: 0.000329135626
Iter: 27 loss: 0.000318291917
Iter: 28 loss: 0.000316336111
Iter: 29 loss: 0.000316730526
Iter: 30 loss: 0.00031488
Iter: 31 loss: 0.000312432065
Iter: 32 loss: 0.000323339918
Iter: 33 loss: 0.000311947952
Iter: 34 loss: 0.000309856725
Iter: 35 loss: 0.000315322832
Iter: 36 loss: 0.000309147785
Iter: 37 loss: 0.000307361712
Iter: 38 loss: 0.000313681056
Iter: 39 loss: 0.000306899514
Iter: 40 loss: 0.000306216418
Iter: 41 loss: 0.000305965717
Iter: 42 loss: 0.000305247842
Iter: 43 loss: 0.000304471119
Iter: 44 loss: 0.000304350455
Iter: 45 loss: 0.000303227658
Iter: 46 loss: 0.000305431604
Iter: 47 loss: 0.000302765635
Iter: 48 loss: 0.00030157881
Iter: 49 loss: 0.000304502493
Iter: 50 loss: 0.000301156892
Iter: 51 loss: 0.000300013111
Iter: 52 loss: 0.000302920584
Iter: 53 loss: 0.000299619918
Iter: 54 loss: 0.00029850431
Iter: 55 loss: 0.000303122826
Iter: 56 loss: 0.000298261875
Iter: 57 loss: 0.000297281455
Iter: 58 loss: 0.000298019819
Iter: 59 loss: 0.000296682
Iter: 60 loss: 0.000295594102
Iter: 61 loss: 0.000298314495
Iter: 62 loss: 0.000295213598
Iter: 63 loss: 0.000293968478
Iter: 64 loss: 0.000298023
Iter: 65 loss: 0.000293618243
Iter: 66 loss: 0.00029256905
Iter: 67 loss: 0.000295060629
Iter: 68 loss: 0.000292187469
Iter: 69 loss: 0.000291193486
Iter: 70 loss: 0.000294257799
Iter: 71 loss: 0.000290901051
Iter: 72 loss: 0.000289807242
Iter: 73 loss: 0.000290921191
Iter: 74 loss: 0.000289197604
Iter: 75 loss: 0.000288828393
Iter: 76 loss: 0.000288649404
Iter: 77 loss: 0.000288072217
Iter: 78 loss: 0.000287441071
Iter: 79 loss: 0.000287346309
Iter: 80 loss: 0.000286621391
Iter: 81 loss: 0.00028787757
Iter: 82 loss: 0.000286300841
Iter: 83 loss: 0.000285477552
Iter: 84 loss: 0.000288728246
Iter: 85 loss: 0.000285289716
Iter: 86 loss: 0.000284535083
Iter: 87 loss: 0.00028606478
Iter: 88 loss: 0.000284230278
Iter: 89 loss: 0.000283529254
Iter: 90 loss: 0.000285385642
Iter: 91 loss: 0.000283292728
Iter: 92 loss: 0.00028254214
Iter: 93 loss: 0.00028502912
Iter: 94 loss: 0.000282335444
Iter: 95 loss: 0.000281647313
Iter: 96 loss: 0.000281660032
Iter: 97 loss: 0.000281099783
Iter: 98 loss: 0.000280288747
Iter: 99 loss: 0.000285049347
Iter: 100 loss: 0.000280182256
Iter: 101 loss: 0.000279449858
Iter: 102 loss: 0.000281403947
Iter: 103 loss: 0.000279205153
Iter: 104 loss: 0.000278481893
Iter: 105 loss: 0.000279245287
Iter: 106 loss: 0.000278082181
Iter: 107 loss: 0.000277269457
Iter: 108 loss: 0.000280208973
Iter: 109 loss: 0.000277065381
Iter: 110 loss: 0.000276488019
Iter: 111 loss: 0.000276487874
Iter: 112 loss: 0.000275928236
Iter: 113 loss: 0.000276616629
Iter: 114 loss: 0.000275635655
Iter: 115 loss: 0.000275195402
Iter: 116 loss: 0.0002745823
Iter: 117 loss: 0.000274556398
Iter: 118 loss: 0.000273725484
Iter: 119 loss: 0.000279963424
Iter: 120 loss: 0.000273659243
Iter: 121 loss: 0.000273074984
Iter: 122 loss: 0.000276208331
Iter: 123 loss: 0.000272987469
Iter: 124 loss: 0.000272460486
Iter: 125 loss: 0.000272421021
Iter: 126 loss: 0.000272026577
Iter: 127 loss: 0.000271402532
Iter: 128 loss: 0.000274167483
Iter: 129 loss: 0.000271279423
Iter: 130 loss: 0.000270590535
Iter: 131 loss: 0.000271790894
Iter: 132 loss: 0.000270284654
Iter: 133 loss: 0.000269680051
Iter: 134 loss: 0.000270308024
Iter: 135 loss: 0.000269344542
Iter: 136 loss: 0.000268594973
Iter: 137 loss: 0.000271708035
Iter: 138 loss: 0.000268433098
Iter: 139 loss: 0.000267819036
Iter: 140 loss: 0.000269669
Iter: 141 loss: 0.000267632888
Iter: 142 loss: 0.000266998482
Iter: 143 loss: 0.000268928
Iter: 144 loss: 0.000266808318
Iter: 145 loss: 0.000266396091
Iter: 146 loss: 0.000266395917
Iter: 147 loss: 0.000265962241
Iter: 148 loss: 0.000265857496
Iter: 149 loss: 0.000265581941
Iter: 150 loss: 0.000265078124
Iter: 151 loss: 0.000265428913
Iter: 152 loss: 0.000264765404
Iter: 153 loss: 0.000264229864
Iter: 154 loss: 0.000264735951
Iter: 155 loss: 0.000263922528
Iter: 156 loss: 0.000263305556
Iter: 157 loss: 0.000270053424
Iter: 158 loss: 0.000263292633
Iter: 159 loss: 0.000262846763
Iter: 160 loss: 0.000263126567
Iter: 161 loss: 0.000262562477
Iter: 162 loss: 0.000262017769
Iter: 163 loss: 0.000262701127
Iter: 164 loss: 0.000261735957
Iter: 165 loss: 0.000261217268
Iter: 166 loss: 0.000266074319
Iter: 167 loss: 0.000261195237
Iter: 168 loss: 0.000260770525
Iter: 169 loss: 0.000260518049
Iter: 170 loss: 0.000260341214
Iter: 171 loss: 0.000259771652
Iter: 172 loss: 0.000262488611
Iter: 173 loss: 0.000259669148
Iter: 174 loss: 0.000259148539
Iter: 175 loss: 0.000260286819
Iter: 176 loss: 0.000258947315
Iter: 177 loss: 0.000258430606
Iter: 178 loss: 0.000261603331
Iter: 179 loss: 0.000258368498
Iter: 180 loss: 0.000258003536
Iter: 181 loss: 0.000263054826
Iter: 182 loss: 0.000258002372
Iter: 183 loss: 0.0002576939
Iter: 184 loss: 0.000257238
Iter: 185 loss: 0.000257226522
Iter: 186 loss: 0.000256706349
Iter: 187 loss: 0.00025806186
Iter: 188 loss: 0.000256529194
Iter: 189 loss: 0.000256018946
Iter: 190 loss: 0.000256643631
Iter: 191 loss: 0.000255751831
Iter: 192 loss: 0.000255222549
Iter: 193 loss: 0.000260399
Iter: 194 loss: 0.000255203515
Iter: 195 loss: 0.000254773826
Iter: 196 loss: 0.000255280756
Iter: 197 loss: 0.000254545681
Iter: 198 loss: 0.000254055543
Iter: 199 loss: 0.000254235521
Iter: 200 loss: 0.000253712526
Iter: 201 loss: 0.000253169273
Iter: 202 loss: 0.0002573682
Iter: 203 loss: 0.000253129285
Iter: 204 loss: 0.000252646714
Iter: 205 loss: 0.000253073318
Iter: 206 loss: 0.000252364494
Iter: 207 loss: 0.000251813792
Iter: 208 loss: 0.000253607897
Iter: 209 loss: 0.000251659279
Iter: 210 loss: 0.000251211401
Iter: 211 loss: 0.000251749763
Iter: 212 loss: 0.000250975194
Iter: 213 loss: 0.000250603713
Iter: 214 loss: 0.000250602199
Iter: 215 loss: 0.000250249635
Iter: 216 loss: 0.000251172896
Iter: 217 loss: 0.000250129553
Iter: 218 loss: 0.00024982309
Iter: 219 loss: 0.000249742647
Iter: 220 loss: 0.00024955158
Iter: 221 loss: 0.00024913653
Iter: 222 loss: 0.000249699166
Iter: 223 loss: 0.000248928438
Iter: 224 loss: 0.000248424651
Iter: 225 loss: 0.000249902485
Iter: 226 loss: 0.000248269527
Iter: 227 loss: 0.000247860153
Iter: 228 loss: 0.000251091958
Iter: 229 loss: 0.000247831398
Iter: 230 loss: 0.00024741856
Iter: 231 loss: 0.000247360964
Iter: 232 loss: 0.000247070479
Iter: 233 loss: 0.000246525742
Iter: 234 loss: 0.000247463846
Iter: 235 loss: 0.000246282842
Iter: 236 loss: 0.00024573665
Iter: 237 loss: 0.000248687196
Iter: 238 loss: 0.000245654257
Iter: 239 loss: 0.000245145871
Iter: 240 loss: 0.000246144395
Iter: 241 loss: 0.000244935392
Iter: 242 loss: 0.000244443305
Iter: 243 loss: 0.000245777133
Iter: 244 loss: 0.000244280789
Iter: 245 loss: 0.000243799499
Iter: 246 loss: 0.000244590949
Iter: 247 loss: 0.000243578252
Iter: 248 loss: 0.000243399758
Iter: 249 loss: 0.000243322356
Iter: 250 loss: 0.000243077287
Iter: 251 loss: 0.000242753013
Iter: 252 loss: 0.000242733484
Iter: 253 loss: 0.000242309849
Iter: 254 loss: 0.00024276058
Iter: 255 loss: 0.000242076305
Iter: 256 loss: 0.00024157326
Iter: 257 loss: 0.000242922484
Iter: 258 loss: 0.000241405927
Iter: 259 loss: 0.000240960013
Iter: 260 loss: 0.000243138202
Iter: 261 loss: 0.000240881782
Iter: 262 loss: 0.000240455352
Iter: 263 loss: 0.000241521775
Iter: 264 loss: 0.000240305468
Iter: 265 loss: 0.00023984554
Iter: 266 loss: 0.000241333793
Iter: 267 loss: 0.000239715271
Iter: 268 loss: 0.000239347573
Iter: 269 loss: 0.000239531146
Iter: 270 loss: 0.000239101762
Iter: 271 loss: 0.000238651774
Iter: 272 loss: 0.000240915295
Iter: 273 loss: 0.000238576627
Iter: 274 loss: 0.000238182954
Iter: 275 loss: 0.000238600289
Iter: 276 loss: 0.000237965811
Iter: 277 loss: 0.000237481567
Iter: 278 loss: 0.000240278852
Iter: 279 loss: 0.000237416752
Iter: 280 loss: 0.000237049506
Iter: 281 loss: 0.000237059096
Iter: 282 loss: 0.000236757274
Iter: 283 loss: 0.00023656433
Iter: 284 loss: 0.000236469234
Iter: 285 loss: 0.000236270062
Iter: 286 loss: 0.00023583087
Iter: 287 loss: 0.000242322945
Iter: 288 loss: 0.000235810381
Iter: 289 loss: 0.000235346408
Iter: 290 loss: 0.000236431733
Iter: 291 loss: 0.000235175801
Iter: 292 loss: 0.000234696912
Iter: 293 loss: 0.000236012886
Iter: 294 loss: 0.000234540392
Iter: 295 loss: 0.000234065388
Iter: 296 loss: 0.00023757774
Iter: 297 loss: 0.000234026229
Iter: 298 loss: 0.000233656421
Iter: 299 loss: 0.0002344401
Iter: 300 loss: 0.000233510233
Iter: 301 loss: 0.000233074141
Iter: 302 loss: 0.000233871688
Iter: 303 loss: 0.000232885635
Iter: 304 loss: 0.000232480379
Iter: 305 loss: 0.000233907878
Iter: 306 loss: 0.000232375518
Iter: 307 loss: 0.000231974365
Iter: 308 loss: 0.000232314604
Iter: 309 loss: 0.000231736136
Iter: 310 loss: 0.000231283397
Iter: 311 loss: 0.00023304738
Iter: 312 loss: 0.000231179409
Iter: 313 loss: 0.000230726029
Iter: 314 loss: 0.000232203107
Iter: 315 loss: 0.000230598365
Iter: 316 loss: 0.000230199163
Iter: 317 loss: 0.000231300801
Iter: 318 loss: 0.000230068981
Iter: 319 loss: 0.000229801095
Iter: 320 loss: 0.00022979769
Iter: 321 loss: 0.000229540135
Iter: 322 loss: 0.000229315803
Iter: 323 loss: 0.000229246856
Iter: 324 loss: 0.000228886507
Iter: 325 loss: 0.000229083089
Iter: 326 loss: 0.000228650169
Iter: 327 loss: 0.000228206118
Iter: 328 loss: 0.0002297928
Iter: 329 loss: 0.000228092744
Iter: 330 loss: 0.000227665747
Iter: 331 loss: 0.000228365898
Iter: 332 loss: 0.000227470358
Iter: 333 loss: 0.000226994161
Iter: 334 loss: 0.000228329751
Iter: 335 loss: 0.000226842385
Iter: 336 loss: 0.000226486707
Iter: 337 loss: 0.000226485732
Iter: 338 loss: 0.000226216172
Iter: 339 loss: 0.000226124335
Iter: 340 loss: 0.000225970231
Iter: 341 loss: 0.000225565775
Iter: 342 loss: 0.000226160453
Iter: 343 loss: 0.000225370502
Iter: 344 loss: 0.000224960066
Iter: 345 loss: 0.000225879689
Iter: 346 loss: 0.000224804098
Iter: 347 loss: 0.000224340532
Iter: 348 loss: 0.000226771066
Iter: 349 loss: 0.00022426722
Iter: 350 loss: 0.000223918178
Iter: 351 loss: 0.000224853065
Iter: 352 loss: 0.000223802141
Iter: 353 loss: 0.000223466894
Iter: 354 loss: 0.000224439485
Iter: 355 loss: 0.000223362338
Iter: 356 loss: 0.00022299077
Iter: 357 loss: 0.000227054334
Iter: 358 loss: 0.000222982009
Iter: 359 loss: 0.000222779025
Iter: 360 loss: 0.000222410585
Iter: 361 loss: 0.000231287791
Iter: 362 loss: 0.000222410657
Iter: 363 loss: 0.000221984315
Iter: 364 loss: 0.000223405863
Iter: 365 loss: 0.000221867711
Iter: 366 loss: 0.00022147024
Iter: 367 loss: 0.000222244125
Iter: 368 loss: 0.000221305061
Iter: 369 loss: 0.000220858463
Iter: 370 loss: 0.000222608855
Iter: 371 loss: 0.000220754941
Iter: 372 loss: 0.000220366113
Iter: 373 loss: 0.000221701281
Iter: 374 loss: 0.000220262416
Iter: 375 loss: 0.000219889014
Iter: 376 loss: 0.000222604707
Iter: 377 loss: 0.00021985748
Iter: 378 loss: 0.000219576585
Iter: 379 loss: 0.000219665118
Iter: 380 loss: 0.000219376103
Iter: 381 loss: 0.000218972709
Iter: 382 loss: 0.000219538735
Iter: 383 loss: 0.000218773203
Iter: 384 loss: 0.000218372792
Iter: 385 loss: 0.000219369133
Iter: 386 loss: 0.00021823241
Iter: 387 loss: 0.000217831344
Iter: 388 loss: 0.000219386813
Iter: 389 loss: 0.00021773795
Iter: 390 loss: 0.000217384688
Iter: 391 loss: 0.000218456145
Iter: 392 loss: 0.000217279361
Iter: 393 loss: 0.000216934102
Iter: 394 loss: 0.000219010573
Iter: 395 loss: 0.000216890476
Iter: 396 loss: 0.000216599452
Iter: 397 loss: 0.000219190217
Iter: 398 loss: 0.000216585875
Iter: 399 loss: 0.000216367378
Iter: 400 loss: 0.000215928943
Iter: 401 loss: 0.000224375544
Iter: 402 loss: 0.00021592289
Iter: 403 loss: 0.000215481487
Iter: 404 loss: 0.000216841421
Iter: 405 loss: 0.000215351087
Iter: 406 loss: 0.000214932355
Iter: 407 loss: 0.000216527726
Iter: 408 loss: 0.000214832951
Iter: 409 loss: 0.000214460859
Iter: 410 loss: 0.000215869339
Iter: 411 loss: 0.000214372238
Iter: 412 loss: 0.000213958294
Iter: 413 loss: 0.000215071777
Iter: 414 loss: 0.000213820604
Iter: 415 loss: 0.000213446037
Iter: 416 loss: 0.000216372
Iter: 417 loss: 0.000213419436
Iter: 418 loss: 0.000213176609
Iter: 419 loss: 0.000213124207
Iter: 420 loss: 0.00021296482
Iter: 421 loss: 0.000212587795
Iter: 422 loss: 0.000213043473
Iter: 423 loss: 0.000212390354
Iter: 424 loss: 0.000211985331
Iter: 425 loss: 0.00021359243
Iter: 426 loss: 0.00021189297
Iter: 427 loss: 0.000211506922
Iter: 428 loss: 0.000212912651
Iter: 429 loss: 0.000211410777
Iter: 430 loss: 0.00021110347
Iter: 431 loss: 0.000213122694
Iter: 432 loss: 0.000211070583
Iter: 433 loss: 0.000210795086
Iter: 434 loss: 0.000212711268
Iter: 435 loss: 0.000210769416
Iter: 436 loss: 0.000210528277
Iter: 437 loss: 0.000210311176
Iter: 438 loss: 0.000210250058
Iter: 439 loss: 0.00020991081
Iter: 440 loss: 0.0002104179
Iter: 441 loss: 0.000209748629
Iter: 442 loss: 0.0002093699
Iter: 443 loss: 0.000210164668
Iter: 444 loss: 0.000209218706
Iter: 445 loss: 0.000208823592
Iter: 446 loss: 0.000209935941
Iter: 447 loss: 0.000208697238
Iter: 448 loss: 0.000208363272
Iter: 449 loss: 0.000210249622
Iter: 450 loss: 0.000208317884
Iter: 451 loss: 0.000208050958
Iter: 452 loss: 0.000210006518
Iter: 453 loss: 0.000208028257
Iter: 454 loss: 0.000207775622
Iter: 455 loss: 0.000207684439
Iter: 456 loss: 0.000207543315
Iter: 457 loss: 0.00020720766
Iter: 458 loss: 0.00020789601
Iter: 459 loss: 0.000207072386
Iter: 460 loss: 0.000206717334
Iter: 461 loss: 0.000208251062
Iter: 462 loss: 0.000206645578
Iter: 463 loss: 0.00020635454
Iter: 464 loss: 0.000206877798
Iter: 465 loss: 0.000206228375
Iter: 466 loss: 0.000205904362
Iter: 467 loss: 0.000207677454
Iter: 468 loss: 0.000205857257
Iter: 469 loss: 0.000205586985
Iter: 470 loss: 0.000208568148
Iter: 471 loss: 0.000205581746
Iter: 472 loss: 0.000205406948
Iter: 473 loss: 0.000205218617
Iter: 474 loss: 0.000205189455
Iter: 475 loss: 0.000204871234
Iter: 476 loss: 0.000205541292
Iter: 477 loss: 0.000204745651
Iter: 478 loss: 0.000204457843
Iter: 479 loss: 0.000204599899
Iter: 480 loss: 0.000204265132
Iter: 481 loss: 0.000203886899
Iter: 482 loss: 0.000205386503
Iter: 483 loss: 0.000203800475
Iter: 484 loss: 0.00020346258
Iter: 485 loss: 0.000203967211
Iter: 486 loss: 0.000203300326
Iter: 487 loss: 0.000203044518
Iter: 488 loss: 0.000203042116
Iter: 489 loss: 0.000202824216
Iter: 490 loss: 0.000202782539
Iter: 491 loss: 0.000202636831
Iter: 492 loss: 0.000202294716
Iter: 493 loss: 0.000202776442
Iter: 494 loss: 0.000202125535
Iter: 495 loss: 0.000201815885
Iter: 496 loss: 0.000202954456
Iter: 497 loss: 0.000201738963
Iter: 498 loss: 0.0002014247
Iter: 499 loss: 0.000201762305
Iter: 500 loss: 0.000201252464
Iter: 501 loss: 0.000200934184
Iter: 502 loss: 0.000203800591
Iter: 503 loss: 0.000200919603
Iter: 504 loss: 0.000200690178
Iter: 505 loss: 0.000203095318
Iter: 506 loss: 0.000200684648
Iter: 507 loss: 0.000200488808
Iter: 508 loss: 0.000200319482
Iter: 509 loss: 0.000200266295
Iter: 510 loss: 0.000199980888
Iter: 511 loss: 0.000200249
Iter: 512 loss: 0.000199817194
Iter: 513 loss: 0.000199474045
Iter: 514 loss: 0.000200776674
Iter: 515 loss: 0.000199392583
Iter: 516 loss: 0.000199088419
Iter: 517 loss: 0.000199638307
Iter: 518 loss: 0.000198956637
Iter: 519 loss: 0.000198633323
Iter: 520 loss: 0.000199461254
Iter: 521 loss: 0.00019852219
Iter: 522 loss: 0.000198224632
Iter: 523 loss: 0.000199173781
Iter: 524 loss: 0.000198139489
Iter: 525 loss: 0.000197860354
Iter: 526 loss: 0.000200890907
Iter: 527 loss: 0.000197854737
Iter: 528 loss: 0.000197658024
Iter: 529 loss: 0.000197523535
Iter: 530 loss: 0.000197452187
Iter: 531 loss: 0.000197125686
Iter: 532 loss: 0.00019753458
Iter: 533 loss: 0.000196956462
Iter: 534 loss: 0.000196604582
Iter: 535 loss: 0.000197858506
Iter: 536 loss: 0.000196514651
Iter: 537 loss: 0.000196210312
Iter: 538 loss: 0.000198250753
Iter: 539 loss: 0.000196179302
Iter: 540 loss: 0.000195923611
Iter: 541 loss: 0.000198330585
Iter: 542 loss: 0.000195913133
Iter: 543 loss: 0.00019571642
Iter: 544 loss: 0.000195765941
Iter: 545 loss: 0.000195572429
Iter: 546 loss: 0.000195362256
Iter: 547 loss: 0.000195256871
Iter: 548 loss: 0.000195158558
Iter: 549 loss: 0.000194836291
Iter: 550 loss: 0.000196798472
Iter: 551 loss: 0.00019479786
Iter: 552 loss: 0.00019453287
Iter: 553 loss: 0.000194986496
Iter: 554 loss: 0.000194413893
Iter: 555 loss: 0.000194114808
Iter: 556 loss: 0.000194636392
Iter: 557 loss: 0.000193983316
Iter: 558 loss: 0.000193691521
Iter: 559 loss: 0.00019506838
Iter: 560 loss: 0.000193638378
Iter: 561 loss: 0.000193375425
Iter: 562 loss: 0.000195577741
Iter: 563 loss: 0.000193360233
Iter: 564 loss: 0.000193147716
Iter: 565 loss: 0.000192970678
Iter: 566 loss: 0.000192910258
Iter: 567 loss: 0.000192618332
Iter: 568 loss: 0.000193658547
Iter: 569 loss: 0.000192543375
Iter: 570 loss: 0.000192246822
Iter: 571 loss: 0.000192736406
Iter: 572 loss: 0.000192111416
Iter: 573 loss: 0.00019184433
Iter: 574 loss: 0.000194593013
Iter: 575 loss: 0.000191836472
Iter: 576 loss: 0.000191597675
Iter: 577 loss: 0.000193174739
Iter: 578 loss: 0.000191572646
Iter: 579 loss: 0.000191394065
Iter: 580 loss: 0.000191237719
Iter: 581 loss: 0.000191189625
Iter: 582 loss: 0.000190906561
Iter: 583 loss: 0.000191730884
Iter: 584 loss: 0.000190818901
Iter: 585 loss: 0.000190552106
Iter: 586 loss: 0.00019126531
Iter: 587 loss: 0.000190462946
Iter: 588 loss: 0.000190163351
Iter: 589 loss: 0.000190761843
Iter: 590 loss: 0.000190040286
Iter: 591 loss: 0.000189759885
Iter: 592 loss: 0.000190503473
Iter: 593 loss: 0.000189666171
Iter: 594 loss: 0.0001893843
Iter: 595 loss: 0.000190956373
Iter: 596 loss: 0.000189344428
Iter: 597 loss: 0.000189090119
Iter: 598 loss: 0.000190294697
Iter: 599 loss: 0.000189043756
Iter: 600 loss: 0.000188837483
Iter: 601 loss: 0.000189023849
Iter: 602 loss: 0.000188717328
Iter: 603 loss: 0.00018847434
Iter: 604 loss: 0.000188675025
Iter: 605 loss: 0.000188329403
Iter: 606 loss: 0.000188032413
Iter: 607 loss: 0.000188648919
Iter: 608 loss: 0.000187914
Iter: 609 loss: 0.000187723024
Iter: 610 loss: 0.00018772058
Iter: 611 loss: 0.000187520578
Iter: 612 loss: 0.000187737372
Iter: 613 loss: 0.00018741173
Iter: 614 loss: 0.000187217403
Iter: 615 loss: 0.000187210171
Iter: 616 loss: 0.000187059486
Iter: 617 loss: 0.000186805206
Iter: 618 loss: 0.000188129896
Iter: 619 loss: 0.000186765377
Iter: 620 loss: 0.000186509627
Iter: 621 loss: 0.000186788966
Iter: 622 loss: 0.000186370089
Iter: 623 loss: 0.000186088902
Iter: 624 loss: 0.000186931196
Iter: 625 loss: 0.000186004065
Iter: 626 loss: 0.000185718658
Iter: 627 loss: 0.000186694233
Iter: 628 loss: 0.000185642304
Iter: 629 loss: 0.000185406883
Iter: 630 loss: 0.000187039666
Iter: 631 loss: 0.000185384299
Iter: 632 loss: 0.00018516212
Iter: 633 loss: 0.000185645695
Iter: 634 loss: 0.000185076424
Iter: 635 loss: 0.000184857665
Iter: 636 loss: 0.000185296667
Iter: 637 loss: 0.000184767923
Iter: 638 loss: 0.000184553326
Iter: 639 loss: 0.000184663775
Iter: 640 loss: 0.000184410455
Iter: 641 loss: 0.000184179866
Iter: 642 loss: 0.000185309211
Iter: 643 loss: 0.000184139586
Iter: 644 loss: 0.000183935364
Iter: 645 loss: 0.00018637038
Iter: 646 loss: 0.000183932629
Iter: 647 loss: 0.000183772936
Iter: 648 loss: 0.000183761396
Iter: 649 loss: 0.00018364127
Iter: 650 loss: 0.000183457945
Iter: 651 loss: 0.000183396478
Iter: 652 loss: 0.000183290831
Iter: 653 loss: 0.000183006894
Iter: 654 loss: 0.000184357632
Iter: 655 loss: 0.000182955584
Iter: 656 loss: 0.000182703938
Iter: 657 loss: 0.000183503842
Iter: 658 loss: 0.000182631746
Iter: 659 loss: 0.000182384363
Iter: 660 loss: 0.000183084208
Iter: 661 loss: 0.000182306117
Iter: 662 loss: 0.000182064789
Iter: 663 loss: 0.000182594522
Iter: 664 loss: 0.000181971889
Iter: 665 loss: 0.00018175444
Iter: 666 loss: 0.000183319818
Iter: 667 loss: 0.000181734533
Iter: 668 loss: 0.000181508396
Iter: 669 loss: 0.000181744806
Iter: 670 loss: 0.000181383308
Iter: 671 loss: 0.000181164505
Iter: 672 loss: 0.000181541458
Iter: 673 loss: 0.000181067502
Iter: 674 loss: 0.00018083959
Iter: 675 loss: 0.000181338633
Iter: 676 loss: 0.000180751449
Iter: 677 loss: 0.000180525967
Iter: 678 loss: 0.000181796466
Iter: 679 loss: 0.000180494681
Iter: 680 loss: 0.000180277595
Iter: 681 loss: 0.000181854048
Iter: 682 loss: 0.000180259522
Iter: 683 loss: 0.000180099378
Iter: 684 loss: 0.000179994633
Iter: 685 loss: 0.0001799327
Iter: 686 loss: 0.000179709503
Iter: 687 loss: 0.00017975393
Iter: 688 loss: 0.000179543218
Iter: 689 loss: 0.000179276045
Iter: 690 loss: 0.000180337098
Iter: 691 loss: 0.000179215829
Iter: 692 loss: 0.000178945
Iter: 693 loss: 0.000180199073
Iter: 694 loss: 0.000178894246
Iter: 695 loss: 0.000178687565
Iter: 696 loss: 0.000179657975
Iter: 697 loss: 0.000178649148
Iter: 698 loss: 0.000178449875
Iter: 699 loss: 0.000178427057
Iter: 700 loss: 0.000178283255
Iter: 701 loss: 0.000178084374
Iter: 702 loss: 0.000178084098
Iter: 703 loss: 0.000177911395
Iter: 704 loss: 0.000177902708
Iter: 705 loss: 0.000177770649
Iter: 706 loss: 0.000177541253
Iter: 707 loss: 0.000177714319
Iter: 708 loss: 0.000177400943
Iter: 709 loss: 0.000177156937
Iter: 710 loss: 0.000178198286
Iter: 711 loss: 0.000177106354
Iter: 712 loss: 0.000176893533
Iter: 713 loss: 0.000178262
Iter: 714 loss: 0.000176869886
Iter: 715 loss: 0.000176659494
Iter: 716 loss: 0.000177817332
Iter: 717 loss: 0.000176628833
Iter: 718 loss: 0.000176481321
Iter: 719 loss: 0.000176366419
Iter: 720 loss: 0.000176319954
Iter: 721 loss: 0.000176090238
Iter: 722 loss: 0.000176225847
Iter: 723 loss: 0.000175941357
Iter: 724 loss: 0.000175643858
Iter: 725 loss: 0.000176961083
Iter: 726 loss: 0.00017558501
Iter: 727 loss: 0.000175348338
Iter: 728 loss: 0.000176044763
Iter: 729 loss: 0.00017527584
Iter: 730 loss: 0.00017504752
Iter: 731 loss: 0.000175990441
Iter: 732 loss: 0.000174997956
Iter: 733 loss: 0.000174774817
Iter: 734 loss: 0.000175198031
Iter: 735 loss: 0.000174681132
Iter: 736 loss: 0.000174468136
Iter: 737 loss: 0.000175857655
Iter: 738 loss: 0.00017444542
Iter: 739 loss: 0.000174264307
Iter: 740 loss: 0.00017483093
Iter: 741 loss: 0.000174211935
Iter: 742 loss: 0.000174030632
Iter: 743 loss: 0.000174216781
Iter: 744 loss: 0.000173928711
Iter: 745 loss: 0.000173715423
Iter: 746 loss: 0.000173987064
Iter: 747 loss: 0.000173605455
Iter: 748 loss: 0.000173392313
Iter: 749 loss: 0.000174364191
Iter: 750 loss: 0.000173351451
Iter: 751 loss: 0.000173155946
Iter: 752 loss: 0.000175565714
Iter: 753 loss: 0.000173154083
Iter: 754 loss: 0.000173037712
Iter: 755 loss: 0.000172821325
Iter: 756 loss: 0.000177758746
Iter: 757 loss: 0.000172820932
Iter: 758 loss: 0.000172585656
Iter: 759 loss: 0.00017341807
Iter: 760 loss: 0.000172524684
Iter: 761 loss: 0.000172290587
Iter: 762 loss: 0.000173103821
Iter: 763 loss: 0.00017222864
Iter: 764 loss: 0.000172023
Iter: 765 loss: 0.000172389817
Iter: 766 loss: 0.000171932101
Iter: 767 loss: 0.000171680382
Iter: 768 loss: 0.000172123313
Iter: 769 loss: 0.000171569627
Iter: 770 loss: 0.00017134
Iter: 771 loss: 0.000172998087
Iter: 772 loss: 0.000171319829
Iter: 773 loss: 0.000171127118
Iter: 774 loss: 0.000171673499
Iter: 775 loss: 0.000171066262
Iter: 776 loss: 0.000170899933
Iter: 777 loss: 0.000171729684
Iter: 778 loss: 0.000170872547
Iter: 779 loss: 0.000170694388
Iter: 780 loss: 0.000170841842
Iter: 781 loss: 0.000170588159
Iter: 782 loss: 0.000170395535
Iter: 783 loss: 0.000170916159
Iter: 784 loss: 0.000170331332
Iter: 785 loss: 0.000170154584
Iter: 786 loss: 0.000170420099
Iter: 787 loss: 0.000170069965
Iter: 788 loss: 0.000169889652
Iter: 789 loss: 0.000169889565
Iter: 790 loss: 0.000169764127
Iter: 791 loss: 0.000169671664
Iter: 792 loss: 0.000169629362
Iter: 793 loss: 0.000169443156
Iter: 794 loss: 0.000169482984
Iter: 795 loss: 0.000169305466
Iter: 796 loss: 0.000169064879
Iter: 797 loss: 0.000169551757
Iter: 798 loss: 0.00016896741
Iter: 799 loss: 0.000168703235
Iter: 800 loss: 0.000169721243
Iter: 801 loss: 0.000168641956
Iter: 802 loss: 0.000168384606
Iter: 803 loss: 0.000169242558
Iter: 804 loss: 0.000168314815
Iter: 805 loss: 0.000168086961
Iter: 806 loss: 0.000168950311
Iter: 807 loss: 0.000168032508
Iter: 808 loss: 0.000167825201
Iter: 809 loss: 0.000168242303
Iter: 810 loss: 0.000167740975
Iter: 811 loss: 0.000167518301
Iter: 812 loss: 0.000168747734
Iter: 813 loss: 0.00016748601
Iter: 814 loss: 0.000167289196
Iter: 815 loss: 0.000168145823
Iter: 816 loss: 0.000167249615
Iter: 817 loss: 0.000167096
Iter: 818 loss: 0.000167155871
Iter: 819 loss: 0.000166989586
Iter: 820 loss: 0.00016680744
Iter: 821 loss: 0.000167948427
Iter: 822 loss: 0.000166785394
Iter: 823 loss: 0.000166659855
Iter: 824 loss: 0.000167642749
Iter: 825 loss: 0.0001666506
Iter: 826 loss: 0.000166512647
Iter: 827 loss: 0.000166432612
Iter: 828 loss: 0.000166373662
Iter: 829 loss: 0.000166190701
Iter: 830 loss: 0.000166203477
Iter: 831 loss: 0.000166047656
Iter: 832 loss: 0.000165829915
Iter: 833 loss: 0.000166468642
Iter: 834 loss: 0.000165762438
Iter: 835 loss: 0.000165557256
Iter: 836 loss: 0.000166718979
Iter: 837 loss: 0.000165528792
Iter: 838 loss: 0.000165332895
Iter: 839 loss: 0.000165547885
Iter: 840 loss: 0.000165226287
Iter: 841 loss: 0.00016502486
Iter: 842 loss: 0.000165888487
Iter: 843 loss: 0.000164983619
Iter: 844 loss: 0.000164797908
Iter: 845 loss: 0.000165387435
Iter: 846 loss: 0.000164744677
Iter: 847 loss: 0.000164581841
Iter: 848 loss: 0.000165617
Iter: 849 loss: 0.00016456384
Iter: 850 loss: 0.000164413184
Iter: 851 loss: 0.00016470236
Iter: 852 loss: 0.000164349767
Iter: 853 loss: 0.000164182158
Iter: 854 loss: 0.000164412311
Iter: 855 loss: 0.000164098223
Iter: 856 loss: 0.000163928358
Iter: 857 loss: 0.000164512312
Iter: 858 loss: 0.000163883044
Iter: 859 loss: 0.000163703138
Iter: 860 loss: 0.000165097023
Iter: 861 loss: 0.000163689896
Iter: 862 loss: 0.00016354835
Iter: 863 loss: 0.000163533579
Iter: 864 loss: 0.000163431047
Iter: 865 loss: 0.000163276156
Iter: 866 loss: 0.000163360019
Iter: 867 loss: 0.000163174642
Iter: 868 loss: 0.000162958459
Iter: 869 loss: 0.000163589983
Iter: 870 loss: 0.000162891607
Iter: 871 loss: 0.000162695651
Iter: 872 loss: 0.000163069271
Iter: 873 loss: 0.000162612851
Iter: 874 loss: 0.000162401149
Iter: 875 loss: 0.000162813929
Iter: 876 loss: 0.000162312936
Iter: 877 loss: 0.000162087483
Iter: 878 loss: 0.000162652839
Iter: 879 loss: 0.000162008422
Iter: 880 loss: 0.000161802425
Iter: 881 loss: 0.000163468721
Iter: 882 loss: 0.000161789183
Iter: 883 loss: 0.000161623204
Iter: 884 loss: 0.000162128286
Iter: 885 loss: 0.000161573422
Iter: 886 loss: 0.000161403121
Iter: 887 loss: 0.000162071825
Iter: 888 loss: 0.000161363831
Iter: 889 loss: 0.00016120488
Iter: 890 loss: 0.000161420845
Iter: 891 loss: 0.000161125237
Iter: 892 loss: 0.00016097122
Iter: 893 loss: 0.000161532575
Iter: 894 loss: 0.000160933239
Iter: 895 loss: 0.000160760785
Iter: 896 loss: 0.000161864155
Iter: 897 loss: 0.000160741329
Iter: 898 loss: 0.000160628173
Iter: 899 loss: 0.000160568161
Iter: 900 loss: 0.000160517346
Iter: 901 loss: 0.000160343945
Iter: 902 loss: 0.000160653464
Iter: 903 loss: 0.000160267897
Iter: 904 loss: 0.000160083378
Iter: 905 loss: 0.000160498865
Iter: 906 loss: 0.000160013704
Iter: 907 loss: 0.000159839401
Iter: 908 loss: 0.00016008463
Iter: 909 loss: 0.000159753457
Iter: 910 loss: 0.000159548072
Iter: 911 loss: 0.000160623837
Iter: 912 loss: 0.00015951565
Iter: 913 loss: 0.000159344156
Iter: 914 loss: 0.000159519695
Iter: 915 loss: 0.000159248331
Iter: 916 loss: 0.000159056217
Iter: 917 loss: 0.000160001509
Iter: 918 loss: 0.000159023504
Iter: 919 loss: 0.000158854673
Iter: 920 loss: 0.000159585354
Iter: 921 loss: 0.000158820738
Iter: 922 loss: 0.000158654846
Iter: 923 loss: 0.000159409145
Iter: 924 loss: 0.00015862257
Iter: 925 loss: 0.000158486044
Iter: 926 loss: 0.000158604264
Iter: 927 loss: 0.000158405397
Iter: 928 loss: 0.000158267125
Iter: 929 loss: 0.000159460353
Iter: 930 loss: 0.000158259572
Iter: 931 loss: 0.000158130628
Iter: 932 loss: 0.000158435883
Iter: 933 loss: 0.000158083771
Iter: 934 loss: 0.000157960065
Iter: 935 loss: 0.000157976232
Iter: 936 loss: 0.000157865725
Iter: 937 loss: 0.000157709277
Iter: 938 loss: 0.000157814749
Iter: 939 loss: 0.000157610702
Iter: 940 loss: 0.00015742697
Iter: 941 loss: 0.000158032752
Iter: 942 loss: 0.000157376024
Iter: 943 loss: 0.000157187562
Iter: 944 loss: 0.000157929957
Iter: 945 loss: 0.000157144299
Iter: 946 loss: 0.000156981725
Iter: 947 loss: 0.000157052244
Iter: 948 loss: 0.000156870505
Iter: 949 loss: 0.00015666752
Iter: 950 loss: 0.000157471659
Iter: 951 loss: 0.000156620779
Iter: 952 loss: 0.000156446011
Iter: 953 loss: 0.000157090893
Iter: 954 loss: 0.000156402937
Iter: 955 loss: 0.000156252558
Iter: 956 loss: 0.000157285598
Iter: 957 loss: 0.000156238297
Iter: 958 loss: 0.000156106515
Iter: 959 loss: 0.000156544411
Iter: 960 loss: 0.000156070761
Iter: 961 loss: 0.000155936374
Iter: 962 loss: 0.000156041497
Iter: 963 loss: 0.000155855218
Iter: 964 loss: 0.00015573774
Iter: 965 loss: 0.000157216098
Iter: 966 loss: 0.000155736765
Iter: 967 loss: 0.000155627582
Iter: 968 loss: 0.000155638409
Iter: 969 loss: 0.000155542715
Iter: 970 loss: 0.000155403221
Iter: 971 loss: 0.000155482965
Iter: 972 loss: 0.000155312184
Iter: 973 loss: 0.000155145797
Iter: 974 loss: 0.00015550814
Iter: 975 loss: 0.000155081041
Iter: 976 loss: 0.000154908019
Iter: 977 loss: 0.000155369198
Iter: 978 loss: 0.000154850306
Iter: 979 loss: 0.000154678331
Iter: 980 loss: 0.000155199232
Iter: 981 loss: 0.000154627458
Iter: 982 loss: 0.00015446727
Iter: 983 loss: 0.000154507201
Iter: 984 loss: 0.000154350404
Iter: 985 loss: 0.000154148918
Iter: 986 loss: 0.000156077105
Iter: 987 loss: 0.000154140638
Iter: 988 loss: 0.000153990535
Iter: 989 loss: 0.000153980422
Iter: 990 loss: 0.000153867586
Iter: 991 loss: 0.000153712
Iter: 992 loss: 0.000155313319
Iter: 993 loss: 0.00015370769
Iter: 994 loss: 0.000153584027
Iter: 995 loss: 0.000154273279
Iter: 996 loss: 0.000153566652
Iter: 997 loss: 0.000153447298
Iter: 998 loss: 0.000153423171
Iter: 999 loss: 0.000153344008
Iter: 1000 loss: 0.00015324296
Iter: 1001 loss: 0.000153239962
Iter: 1002 loss: 0.000153153698
Iter: 1003 loss: 0.000153043453
Iter: 1004 loss: 0.000153035624
Iter: 1005 loss: 0.000152898981
Iter: 1006 loss: 0.000153274086
Iter: 1007 loss: 0.000152854307
Iter: 1008 loss: 0.000152696331
Iter: 1009 loss: 0.000152796856
Iter: 1010 loss: 0.000152596345
Iter: 1011 loss: 0.000152425026
Iter: 1012 loss: 0.000153068191
Iter: 1013 loss: 0.00015238332
Iter: 1014 loss: 0.000152224326
Iter: 1015 loss: 0.000152724358
Iter: 1016 loss: 0.000152178502
Iter: 1017 loss: 0.000152011024
Iter: 1018 loss: 0.00015222888
Iter: 1019 loss: 0.00015192623
Iter: 1020 loss: 0.000151753542
Iter: 1021 loss: 0.000152272027
Iter: 1022 loss: 0.00015170133
Iter: 1023 loss: 0.000151531392
Iter: 1024 loss: 0.000152169116
Iter: 1025 loss: 0.000151490123
Iter: 1026 loss: 0.000151326502
Iter: 1027 loss: 0.000151941102
Iter: 1028 loss: 0.000151287066
Iter: 1029 loss: 0.000151153974
Iter: 1030 loss: 0.000152223292
Iter: 1031 loss: 0.000151145272
Iter: 1032 loss: 0.0001510242
Iter: 1033 loss: 0.000151089538
Iter: 1034 loss: 0.000150944092
Iter: 1035 loss: 0.000150842854
Iter: 1036 loss: 0.000152232446
Iter: 1037 loss: 0.000150842476
Iter: 1038 loss: 0.000150751337
Iter: 1039 loss: 0.000150648601
Iter: 1040 loss: 0.000150634907
Iter: 1041 loss: 0.000150498483
Iter: 1042 loss: 0.000150641397
Iter: 1043 loss: 0.000150423264
Iter: 1044 loss: 0.000150260224
Iter: 1045 loss: 0.000150939537
Iter: 1046 loss: 0.000150225358
Iter: 1047 loss: 0.000150083462
Iter: 1048 loss: 0.000150459935
Iter: 1049 loss: 0.000150035543
Iter: 1050 loss: 0.000149885469
Iter: 1051 loss: 0.000150190172
Iter: 1052 loss: 0.000149824948
Iter: 1053 loss: 0.000149672545
Iter: 1054 loss: 0.000149823725
Iter: 1055 loss: 0.000149586442
Iter: 1056 loss: 0.000149407599
Iter: 1057 loss: 0.000150249631
Iter: 1058 loss: 0.000149374333
Iter: 1059 loss: 0.000149211584
Iter: 1060 loss: 0.000149701635
Iter: 1061 loss: 0.000149162661
Iter: 1062 loss: 0.000148997875
Iter: 1063 loss: 0.000149468047
Iter: 1064 loss: 0.000148945721
Iter: 1065 loss: 0.000148810665
Iter: 1066 loss: 0.000150522683
Iter: 1067 loss: 0.000148809399
Iter: 1068 loss: 0.000148701714
Iter: 1069 loss: 0.000148664
Iter: 1070 loss: 0.000148603736
Iter: 1071 loss: 0.000148468811
Iter: 1072 loss: 0.000150109772
Iter: 1073 loss: 0.000148467472
Iter: 1074 loss: 0.000148369028
Iter: 1075 loss: 0.000148255334
Iter: 1076 loss: 0.000148242252
Iter: 1077 loss: 0.000148088977
Iter: 1078 loss: 0.000148552906
Iter: 1079 loss: 0.000148042629
Iter: 1080 loss: 0.000147885294
Iter: 1081 loss: 0.000148117644
Iter: 1082 loss: 0.000147808925
Iter: 1083 loss: 0.000147637737
Iter: 1084 loss: 0.000148126725
Iter: 1085 loss: 0.000147584156
Iter: 1086 loss: 0.000147424289
Iter: 1087 loss: 0.000148209307
Iter: 1088 loss: 0.00014739664
Iter: 1089 loss: 0.000147245242
Iter: 1090 loss: 0.00014738238
Iter: 1091 loss: 0.000147156476
Iter: 1092 loss: 0.000146992184
Iter: 1093 loss: 0.000147479703
Iter: 1094 loss: 0.000146942315
Iter: 1095 loss: 0.000146775346
Iter: 1096 loss: 0.000147096071
Iter: 1097 loss: 0.000146705148
Iter: 1098 loss: 0.000146555481
Iter: 1099 loss: 0.000148019375
Iter: 1100 loss: 0.000146551014
Iter: 1101 loss: 0.000146427526
Iter: 1102 loss: 0.000146975217
Iter: 1103 loss: 0.000146403094
Iter: 1104 loss: 0.000146289021
Iter: 1105 loss: 0.000146472739
Iter: 1106 loss: 0.000146236067
Iter: 1107 loss: 0.000146119448
Iter: 1108 loss: 0.000146901
Iter: 1109 loss: 0.000146107777
Iter: 1110 loss: 0.000146007878
Iter: 1111 loss: 0.000145913742
Iter: 1112 loss: 0.000145890313
Iter: 1113 loss: 0.000145732658
Iter: 1114 loss: 0.000146169114
Iter: 1115 loss: 0.000145681493
Iter: 1116 loss: 0.000145520011
Iter: 1117 loss: 0.00014568145
Iter: 1118 loss: 0.000145429105
Iter: 1119 loss: 0.000145255181
Iter: 1120 loss: 0.000146050676
Iter: 1121 loss: 0.000145221624
Iter: 1122 loss: 0.000145082551
Iter: 1123 loss: 0.000145828599
Iter: 1124 loss: 0.00014506164
Iter: 1125 loss: 0.00014493175
Iter: 1126 loss: 0.000145047277
Iter: 1127 loss: 0.000144856225
Iter: 1128 loss: 0.000144704551
Iter: 1129 loss: 0.000144947844
Iter: 1130 loss: 0.000144634803
Iter: 1131 loss: 0.000144463775
Iter: 1132 loss: 0.00014518082
Iter: 1133 loss: 0.000144427177
Iter: 1134 loss: 0.000144276855
Iter: 1135 loss: 0.000145084719
Iter: 1136 loss: 0.000144254343
Iter: 1137 loss: 0.000144144491
Iter: 1138 loss: 0.000145235404
Iter: 1139 loss: 0.000144140911
Iter: 1140 loss: 0.000144055433
Iter: 1141 loss: 0.000144031685
Iter: 1142 loss: 0.000143979807
Iter: 1143 loss: 0.000143860103
Iter: 1144 loss: 0.000144787104
Iter: 1145 loss: 0.000143851212
Iter: 1146 loss: 0.00014376262
Iter: 1147 loss: 0.000143665908
Iter: 1148 loss: 0.000143651298
Iter: 1149 loss: 0.000143508689
Iter: 1150 loss: 0.00014396105
Iter: 1151 loss: 0.0001434679
Iter: 1152 loss: 0.00014332305
Iter: 1153 loss: 0.000143456738
Iter: 1154 loss: 0.000143238838
Iter: 1155 loss: 0.000143088226
Iter: 1156 loss: 0.000144238773
Iter: 1157 loss: 0.000143076468
Iter: 1158 loss: 0.000142943332
Iter: 1159 loss: 0.000143075245
Iter: 1160 loss: 0.000142868026
Iter: 1161 loss: 0.000142730176
Iter: 1162 loss: 0.000143084559
Iter: 1163 loss: 0.000142682882
Iter: 1164 loss: 0.000142534933
Iter: 1165 loss: 0.000143198704
Iter: 1166 loss: 0.000142506236
Iter: 1167 loss: 0.000142367469
Iter: 1168 loss: 0.000142534467
Iter: 1169 loss: 0.000142294128
Iter: 1170 loss: 0.000142146877
Iter: 1171 loss: 0.000143038589
Iter: 1172 loss: 0.00014212844
Iter: 1173 loss: 0.000141992787
Iter: 1174 loss: 0.000142929377
Iter: 1175 loss: 0.00014197969
Iter: 1176 loss: 0.000141896016
Iter: 1177 loss: 0.000141931727
Iter: 1178 loss: 0.000141838478
Iter: 1179 loss: 0.000141705765
Iter: 1180 loss: 0.00014212924
Iter: 1181 loss: 0.00014166758
Iter: 1182 loss: 0.000141570374
Iter: 1183 loss: 0.000141511118
Iter: 1184 loss: 0.000141471784
Iter: 1185 loss: 0.000141321652
Iter: 1186 loss: 0.000142020668
Iter: 1187 loss: 0.000141293582
Iter: 1188 loss: 0.000141155702
Iter: 1189 loss: 0.000141209734
Iter: 1190 loss: 0.000141059514
Iter: 1191 loss: 0.000140890043
Iter: 1192 loss: 0.000141758035
Iter: 1193 loss: 0.000140862801
Iter: 1194 loss: 0.000140718214
Iter: 1195 loss: 0.000141331155
Iter: 1196 loss: 0.000140687771
Iter: 1197 loss: 0.000140564516
Iter: 1198 loss: 0.000140640535
Iter: 1199 loss: 0.000140485063
Iter: 1200 loss: 0.000140318094
Iter: 1201 loss: 0.000141124765
Iter: 1202 loss: 0.000140288816
Iter: 1203 loss: 0.000140168733
Iter: 1204 loss: 0.000140411459
Iter: 1205 loss: 0.000140120435
Iter: 1206 loss: 0.000139989424
Iter: 1207 loss: 0.000141158118
Iter: 1208 loss: 0.000139982963
Iter: 1209 loss: 0.000139877884
Iter: 1210 loss: 0.000140133925
Iter: 1211 loss: 0.000139840355
Iter: 1212 loss: 0.000139740339
Iter: 1213 loss: 0.000139901065
Iter: 1214 loss: 0.000139693788
Iter: 1215 loss: 0.000139575626
Iter: 1216 loss: 0.000139969867
Iter: 1217 loss: 0.00013954319
Iter: 1218 loss: 0.000139438082
Iter: 1219 loss: 0.000139448064
Iter: 1220 loss: 0.000139356896
Iter: 1221 loss: 0.000139208249
Iter: 1222 loss: 0.000139353797
Iter: 1223 loss: 0.000139123644
Iter: 1224 loss: 0.000138982694
Iter: 1225 loss: 0.000139489275
Iter: 1226 loss: 0.00013894726
Iter: 1227 loss: 0.000138805306
Iter: 1228 loss: 0.000139317519
Iter: 1229 loss: 0.000138769814
Iter: 1230 loss: 0.000138636271
Iter: 1231 loss: 0.00013901385
Iter: 1232 loss: 0.000138593663
Iter: 1233 loss: 0.000138466436
Iter: 1234 loss: 0.000139087759
Iter: 1235 loss: 0.000138444622
Iter: 1236 loss: 0.000138336138
Iter: 1237 loss: 0.000138345
Iter: 1238 loss: 0.000138252013
Iter: 1239 loss: 0.000138102885
Iter: 1240 loss: 0.000139051903
Iter: 1241 loss: 0.000138085219
Iter: 1242 loss: 0.000137973169
Iter: 1243 loss: 0.000139263066
Iter: 1244 loss: 0.000137971321
Iter: 1245 loss: 0.000137895317
Iter: 1246 loss: 0.000137909592
Iter: 1247 loss: 0.000137838244
Iter: 1248 loss: 0.000137730298
Iter: 1249 loss: 0.000138005184
Iter: 1250 loss: 0.000137693
Iter: 1251 loss: 0.000137581985
Iter: 1252 loss: 0.000137909665
Iter: 1253 loss: 0.000137547584
Iter: 1254 loss: 0.000137444891
Iter: 1255 loss: 0.000137499243
Iter: 1256 loss: 0.00013737753
Iter: 1257 loss: 0.000137245806
Iter: 1258 loss: 0.000137321214
Iter: 1259 loss: 0.000137160037
Iter: 1260 loss: 0.000137015959
Iter: 1261 loss: 0.000137461102
Iter: 1262 loss: 0.000136973817
Iter: 1263 loss: 0.000136838
Iter: 1264 loss: 0.000137564915
Iter: 1265 loss: 0.00013681686
Iter: 1266 loss: 0.000136691233
Iter: 1267 loss: 0.000137013223
Iter: 1268 loss: 0.000136648217
Iter: 1269 loss: 0.000136520364
Iter: 1270 loss: 0.000136843213
Iter: 1271 loss: 0.000136475966
Iter: 1272 loss: 0.000136343064
Iter: 1273 loss: 0.000136619317
Iter: 1274 loss: 0.000136290648
Iter: 1275 loss: 0.000136158764
Iter: 1276 loss: 0.000137242721
Iter: 1277 loss: 0.000136150687
Iter: 1278 loss: 0.000136048053
Iter: 1279 loss: 0.0001366785
Iter: 1280 loss: 0.000136035713
Iter: 1281 loss: 0.000135949434
Iter: 1282 loss: 0.00013589632
Iter: 1283 loss: 0.000135861745
Iter: 1284 loss: 0.000135750422
Iter: 1285 loss: 0.000136765942
Iter: 1286 loss: 0.000135745446
Iter: 1287 loss: 0.000135656068
Iter: 1288 loss: 0.000135655311
Iter: 1289 loss: 0.000135583789
Iter: 1290 loss: 0.000135452137
Iter: 1291 loss: 0.000135642244
Iter: 1292 loss: 0.00013538792
Iter: 1293 loss: 0.000135259848
Iter: 1294 loss: 0.000135561015
Iter: 1295 loss: 0.000135212351
Iter: 1296 loss: 0.000135073962
Iter: 1297 loss: 0.000135612
Iter: 1298 loss: 0.000135041701
Iter: 1299 loss: 0.000134908943
Iter: 1300 loss: 0.000134990303
Iter: 1301 loss: 0.000134823975
Iter: 1302 loss: 0.000134659262
Iter: 1303 loss: 0.000135024166
Iter: 1304 loss: 0.000134596732
Iter: 1305 loss: 0.000134439717
Iter: 1306 loss: 0.000135254289
Iter: 1307 loss: 0.000134414833
Iter: 1308 loss: 0.000134289847
Iter: 1309 loss: 0.000135016424
Iter: 1310 loss: 0.000134273258
Iter: 1311 loss: 0.000134169954
Iter: 1312 loss: 0.000134693721
Iter: 1313 loss: 0.000134152855
Iter: 1314 loss: 0.000134042682
Iter: 1315 loss: 0.000134381582
Iter: 1316 loss: 0.000134010392
Iter: 1317 loss: 0.000133916226
Iter: 1318 loss: 0.00013402538
Iter: 1319 loss: 0.000133865426
Iter: 1320 loss: 0.000133769063
Iter: 1321 loss: 0.000134143149
Iter: 1322 loss: 0.000133746114
Iter: 1323 loss: 0.000133636961
Iter: 1324 loss: 0.00013376071
Iter: 1325 loss: 0.000133577647
Iter: 1326 loss: 0.0001334556
Iter: 1327 loss: 0.000133538502
Iter: 1328 loss: 0.000133379217
Iter: 1329 loss: 0.000133233232
Iter: 1330 loss: 0.000133435795
Iter: 1331 loss: 0.000133161724
Iter: 1332 loss: 0.000133032503
Iter: 1333 loss: 0.000134415008
Iter: 1334 loss: 0.000133029302
Iter: 1335 loss: 0.000132918547
Iter: 1336 loss: 0.000132861591
Iter: 1337 loss: 0.00013281015
Iter: 1338 loss: 0.00013265069
Iter: 1339 loss: 0.000133161389
Iter: 1340 loss: 0.000132605026
Iter: 1341 loss: 0.000132451591
Iter: 1342 loss: 0.00013279618
Iter: 1343 loss: 0.000132393019
Iter: 1344 loss: 0.000132259214
Iter: 1345 loss: 0.000133341702
Iter: 1346 loss: 0.000132250454
Iter: 1347 loss: 0.0001321444
Iter: 1348 loss: 0.000133185415
Iter: 1349 loss: 0.000132140354
Iter: 1350 loss: 0.000132055517
Iter: 1351 loss: 0.000132153771
Iter: 1352 loss: 0.000132010609
Iter: 1353 loss: 0.000131917739
Iter: 1354 loss: 0.000132001325
Iter: 1355 loss: 0.000131863606
Iter: 1356 loss: 0.00013175793
Iter: 1357 loss: 0.000132307614
Iter: 1358 loss: 0.000131740904
Iter: 1359 loss: 0.00013164
Iter: 1360 loss: 0.000131802866
Iter: 1361 loss: 0.000131593872
Iter: 1362 loss: 0.000131490699
Iter: 1363 loss: 0.000131471679
Iter: 1364 loss: 0.000131402456
Iter: 1365 loss: 0.000131268491
Iter: 1366 loss: 0.000131867826
Iter: 1367 loss: 0.000131242268
Iter: 1368 loss: 0.000131126828
Iter: 1369 loss: 0.000131446737
Iter: 1370 loss: 0.000131089677
Iter: 1371 loss: 0.00013096015
Iter: 1372 loss: 0.000131279317
Iter: 1373 loss: 0.000130913919
Iter: 1374 loss: 0.000130787463
Iter: 1375 loss: 0.000131043023
Iter: 1376 loss: 0.000130736385
Iter: 1377 loss: 0.000130601111
Iter: 1378 loss: 0.000130816916
Iter: 1379 loss: 0.000130538436
Iter: 1380 loss: 0.000130413348
Iter: 1381 loss: 0.000131185021
Iter: 1382 loss: 0.000130398432
Iter: 1383 loss: 0.000130293076
Iter: 1384 loss: 0.000131535358
Iter: 1385 loss: 0.000130291184
Iter: 1386 loss: 0.000130214758
Iter: 1387 loss: 0.000130259694
Iter: 1388 loss: 0.000130165601
Iter: 1389 loss: 0.000130074652
Iter: 1390 loss: 0.00013010019
Iter: 1391 loss: 0.000130008935
Iter: 1392 loss: 0.00012990291
Iter: 1393 loss: 0.000130963046
Iter: 1394 loss: 0.000129899447
Iter: 1395 loss: 0.000129815686
Iter: 1396 loss: 0.000129773543
Iter: 1397 loss: 0.000129733933
Iter: 1398 loss: 0.000129614666
Iter: 1399 loss: 0.000130228931
Iter: 1400 loss: 0.000129595923
Iter: 1401 loss: 0.000129494452
Iter: 1402 loss: 0.000129470674
Iter: 1403 loss: 0.000129405322
Iter: 1404 loss: 0.00012926507
Iter: 1405 loss: 0.000130059096
Iter: 1406 loss: 0.000129245309
Iter: 1407 loss: 0.000129125008
Iter: 1408 loss: 0.000129219625
Iter: 1409 loss: 0.000129052263
Iter: 1410 loss: 0.000128902902
Iter: 1411 loss: 0.000129325257
Iter: 1412 loss: 0.000128855376
Iter: 1413 loss: 0.000128717104
Iter: 1414 loss: 0.000128986576
Iter: 1415 loss: 0.00012865942
Iter: 1416 loss: 0.000128539948
Iter: 1417 loss: 0.000129467531
Iter: 1418 loss: 0.000128530854
Iter: 1419 loss: 0.000128419269
Iter: 1420 loss: 0.000129462802
Iter: 1421 loss: 0.000128415137
Iter: 1422 loss: 0.000128339758
Iter: 1423 loss: 0.000128284271
Iter: 1424 loss: 0.000128259242
Iter: 1425 loss: 0.000128147993
Iter: 1426 loss: 0.000128634099
Iter: 1427 loss: 0.00012812567
Iter: 1428 loss: 0.000128022366
Iter: 1429 loss: 0.000128395957
Iter: 1430 loss: 0.000127996289
Iter: 1431 loss: 0.00012789623
Iter: 1432 loss: 0.000128033556
Iter: 1433 loss: 0.000127846681
Iter: 1434 loss: 0.0001277353
Iter: 1435 loss: 0.000127847117
Iter: 1436 loss: 0.000127673411
Iter: 1437 loss: 0.000127538908
Iter: 1438 loss: 0.000127815627
Iter: 1439 loss: 0.000127484585
Iter: 1440 loss: 0.000127349806
Iter: 1441 loss: 0.000127878273
Iter: 1442 loss: 0.000127318664
Iter: 1443 loss: 0.000127188032
Iter: 1444 loss: 0.000127436288
Iter: 1445 loss: 0.000127133535
Iter: 1446 loss: 0.000127012347
Iter: 1447 loss: 0.000127511536
Iter: 1448 loss: 0.000126985848
Iter: 1449 loss: 0.000126863262
Iter: 1450 loss: 0.000127051811
Iter: 1451 loss: 0.000126805564
Iter: 1452 loss: 0.000126678264
Iter: 1453 loss: 0.000127300023
Iter: 1454 loss: 0.000126656116
Iter: 1455 loss: 0.000126569677
Iter: 1456 loss: 0.000127573454
Iter: 1457 loss: 0.000126568251
Iter: 1458 loss: 0.00012648647
Iter: 1459 loss: 0.000126536877
Iter: 1460 loss: 0.000126433719
Iter: 1461 loss: 0.00012634843
Iter: 1462 loss: 0.000126272062
Iter: 1463 loss: 0.000126250248
Iter: 1464 loss: 0.00012614498
Iter: 1465 loss: 0.000127773819
Iter: 1466 loss: 0.000126144936
Iter: 1467 loss: 0.000126052095
Iter: 1468 loss: 0.000126186409
Iter: 1469 loss: 0.000126006969
Iter: 1470 loss: 0.000125905965
Iter: 1471 loss: 0.000125876715
Iter: 1472 loss: 0.000125815612
Iter: 1473 loss: 0.000125683087
Iter: 1474 loss: 0.000126029787
Iter: 1475 loss: 0.000125638864
Iter: 1476 loss: 0.000125507097
Iter: 1477 loss: 0.000126392973
Iter: 1478 loss: 0.000125494334
Iter: 1479 loss: 0.000125385923
Iter: 1480 loss: 0.000125489256
Iter: 1481 loss: 0.000125323946
Iter: 1482 loss: 0.000125196151
Iter: 1483 loss: 0.00012579652
Iter: 1484 loss: 0.000125172752
Iter: 1485 loss: 0.000125054
Iter: 1486 loss: 0.000125153601
Iter: 1487 loss: 0.000124983868
Iter: 1488 loss: 0.000124872313
Iter: 1489 loss: 0.000125525854
Iter: 1490 loss: 0.000124858183
Iter: 1491 loss: 0.000124761136
Iter: 1492 loss: 0.000125750652
Iter: 1493 loss: 0.000124758226
Iter: 1494 loss: 0.00012467659
Iter: 1495 loss: 0.000124645274
Iter: 1496 loss: 0.000124600279
Iter: 1497 loss: 0.000124491853
Iter: 1498 loss: 0.000124628074
Iter: 1499 loss: 0.000124435508
Iter: 1500 loss: 0.000124329876
Iter: 1501 loss: 0.000125034188
Iter: 1502 loss: 0.000124319136
Iter: 1503 loss: 0.00012421573
Iter: 1504 loss: 0.000124542785
Iter: 1505 loss: 0.000124186045
Iter: 1506 loss: 0.00012409428
Iter: 1507 loss: 0.000124112674
Iter: 1508 loss: 0.000124026032
Iter: 1509 loss: 0.000123898877
Iter: 1510 loss: 0.000124318671
Iter: 1511 loss: 0.000123864025
Iter: 1512 loss: 0.000123754857
Iter: 1513 loss: 0.00012404038
Iter: 1514 loss: 0.000123717764
Iter: 1515 loss: 0.000123611564
Iter: 1516 loss: 0.000123760226
Iter: 1517 loss: 0.000123559948
Iter: 1518 loss: 0.000123440521
Iter: 1519 loss: 0.000123807287
Iter: 1520 loss: 0.000123405771
Iter: 1521 loss: 0.000123288948
Iter: 1522 loss: 0.000123436083
Iter: 1523 loss: 0.000123228616
Iter: 1524 loss: 0.000123134247
Iter: 1525 loss: 0.00012313397
Iter: 1526 loss: 0.000123060861
Iter: 1527 loss: 0.0001233863
Iter: 1528 loss: 0.000123045946
Iter: 1529 loss: 0.000122972138
Iter: 1530 loss: 0.000123071964
Iter: 1531 loss: 0.000122935147
Iter: 1532 loss: 0.000122856276
Iter: 1533 loss: 0.000122770929
Iter: 1534 loss: 0.00012275773
Iter: 1535 loss: 0.000122644662
Iter: 1536 loss: 0.000123540129
Iter: 1537 loss: 0.000122636702
Iter: 1538 loss: 0.00012254648
Iter: 1539 loss: 0.000123049598
Iter: 1540 loss: 0.000122533384
Iter: 1541 loss: 0.000122448706
Iter: 1542 loss: 0.000122520083
Iter: 1543 loss: 0.000122398
Iter: 1544 loss: 0.00012230486
Iter: 1545 loss: 0.000122429221
Iter: 1546 loss: 0.000122257668
Iter: 1547 loss: 0.000122155732
Iter: 1548 loss: 0.000122360798
Iter: 1549 loss: 0.00012211423
Iter: 1550 loss: 0.000121995916
Iter: 1551 loss: 0.000122165467
Iter: 1552 loss: 0.000121938545
Iter: 1553 loss: 0.000121820995
Iter: 1554 loss: 0.000122137571
Iter: 1555 loss: 0.000121781821
Iter: 1556 loss: 0.000121666824
Iter: 1557 loss: 0.00012254942
Iter: 1558 loss: 0.000121657918
Iter: 1559 loss: 0.000121566933
Iter: 1560 loss: 0.000121777673
Iter: 1561 loss: 0.000121533318
Iter: 1562 loss: 0.000121450932
Iter: 1563 loss: 0.000122344602
Iter: 1564 loss: 0.000121449688
Iter: 1565 loss: 0.000121373581
Iter: 1566 loss: 0.000121365549
Iter: 1567 loss: 0.000121310455
Iter: 1568 loss: 0.000121222714
Iter: 1569 loss: 0.000121280325
Iter: 1570 loss: 0.000121167213
Iter: 1571 loss: 0.000121058052
Iter: 1572 loss: 0.000121347228
Iter: 1573 loss: 0.000121021643
Iter: 1574 loss: 0.000120928118
Iter: 1575 loss: 0.0001218611
Iter: 1576 loss: 0.000120924771
Iter: 1577 loss: 0.000120841934
Iter: 1578 loss: 0.000120865559
Iter: 1579 loss: 0.000120783232
Iter: 1580 loss: 0.000120684927
Iter: 1581 loss: 0.000120875826
Iter: 1582 loss: 0.000120643963
Iter: 1583 loss: 0.000120531433
Iter: 1584 loss: 0.000120838231
Iter: 1585 loss: 0.000120494849
Iter: 1586 loss: 0.000120392564
Iter: 1587 loss: 0.000120544282
Iter: 1588 loss: 0.000120343393
Iter: 1589 loss: 0.000120223165
Iter: 1590 loss: 0.000120485536
Iter: 1591 loss: 0.000120177174
Iter: 1592 loss: 0.000120045435
Iter: 1593 loss: 0.000120344441
Iter: 1594 loss: 0.000119996068
Iter: 1595 loss: 0.000119878176
Iter: 1596 loss: 0.000120406359
Iter: 1597 loss: 0.000119855264
Iter: 1598 loss: 0.000119770433
Iter: 1599 loss: 0.000120876561
Iter: 1600 loss: 0.000119770222
Iter: 1601 loss: 0.000119695775
Iter: 1602 loss: 0.000119858305
Iter: 1603 loss: 0.000119667267
Iter: 1604 loss: 0.000119596429
Iter: 1605 loss: 0.000119498894
Iter: 1606 loss: 0.000119493976
Iter: 1607 loss: 0.00011937094
Iter: 1608 loss: 0.000119544398
Iter: 1609 loss: 0.000119310309
Iter: 1610 loss: 0.000119176912
Iter: 1611 loss: 0.000120080731
Iter: 1612 loss: 0.000119163044
Iter: 1613 loss: 0.00011906193
Iter: 1614 loss: 0.000120225028
Iter: 1615 loss: 0.000119060525
Iter: 1616 loss: 0.000118987271
Iter: 1617 loss: 0.000119024291
Iter: 1618 loss: 0.000118938609
Iter: 1619 loss: 0.000118849377
Iter: 1620 loss: 0.000119153447
Iter: 1621 loss: 0.000118825119
Iter: 1622 loss: 0.00011873008
Iter: 1623 loss: 0.000118710275
Iter: 1624 loss: 0.000118647265
Iter: 1625 loss: 0.000118534372
Iter: 1626 loss: 0.000118934586
Iter: 1627 loss: 0.000118505188
Iter: 1628 loss: 0.000118396289
Iter: 1629 loss: 0.000118725176
Iter: 1630 loss: 0.000118364042
Iter: 1631 loss: 0.000118264987
Iter: 1632 loss: 0.000118738251
Iter: 1633 loss: 0.00011824684
Iter: 1634 loss: 0.000118160417
Iter: 1635 loss: 0.000118508658
Iter: 1636 loss: 0.000118140764
Iter: 1637 loss: 0.000118062642
Iter: 1638 loss: 0.000118682823
Iter: 1639 loss: 0.000118056952
Iter: 1640 loss: 0.000117983698
Iter: 1641 loss: 0.000117941963
Iter: 1642 loss: 0.00011791088
Iter: 1643 loss: 0.000117825577
Iter: 1644 loss: 0.000118026321
Iter: 1645 loss: 0.000117794079
Iter: 1646 loss: 0.000117705553
Iter: 1647 loss: 0.000117841424
Iter: 1648 loss: 0.000117663614
Iter: 1649 loss: 0.000117555617
Iter: 1650 loss: 0.000117737582
Iter: 1651 loss: 0.000117506977
Iter: 1652 loss: 0.000117406787
Iter: 1653 loss: 0.000118011521
Iter: 1654 loss: 0.000117394855
Iter: 1655 loss: 0.000117313582
Iter: 1656 loss: 0.000117968782
Iter: 1657 loss: 0.000117308125
Iter: 1658 loss: 0.000117237141
Iter: 1659 loss: 0.000117204967
Iter: 1660 loss: 0.000117169315
Iter: 1661 loss: 0.000117076183
Iter: 1662 loss: 0.00011712547
Iter: 1663 loss: 0.00011701417
Iter: 1664 loss: 0.000116898984
Iter: 1665 loss: 0.000117597097
Iter: 1666 loss: 0.000116884898
Iter: 1667 loss: 0.000116789626
Iter: 1668 loss: 0.000117163137
Iter: 1669 loss: 0.000116767878
Iter: 1670 loss: 0.000116682801
Iter: 1671 loss: 0.00011687474
Iter: 1672 loss: 0.000116650204
Iter: 1673 loss: 0.000116569332
Iter: 1674 loss: 0.000116810363
Iter: 1675 loss: 0.000116544368
Iter: 1676 loss: 0.000116466268
Iter: 1677 loss: 0.000117454445
Iter: 1678 loss: 0.000116465621
Iter: 1679 loss: 0.000116415016
Iter: 1680 loss: 0.000116327326
Iter: 1681 loss: 0.000116327094
Iter: 1682 loss: 0.000116226118
Iter: 1683 loss: 0.000116646559
Iter: 1684 loss: 0.000116204523
Iter: 1685 loss: 0.000116111652
Iter: 1686 loss: 0.000116088806
Iter: 1687 loss: 0.000116030758
Iter: 1688 loss: 0.00011590989
Iter: 1689 loss: 0.000116583251
Iter: 1690 loss: 0.000115892995
Iter: 1691 loss: 0.000115782044
Iter: 1692 loss: 0.000116357143
Iter: 1693 loss: 0.000115764793
Iter: 1694 loss: 0.000115680508
Iter: 1695 loss: 0.000116006762
Iter: 1696 loss: 0.000115661067
Iter: 1697 loss: 0.0001155871
Iter: 1698 loss: 0.000116144685
Iter: 1699 loss: 0.000115581584
Iter: 1700 loss: 0.00011552537
Iter: 1701 loss: 0.000115477895
Iter: 1702 loss: 0.0001154622
Iter: 1703 loss: 0.00011537048
Iter: 1704 loss: 0.000115767631
Iter: 1705 loss: 0.000115351446
Iter: 1706 loss: 0.000115261297
Iter: 1707 loss: 0.000115353243
Iter: 1708 loss: 0.000115211049
Iter: 1709 loss: 0.00011511326
Iter: 1710 loss: 0.000115288727
Iter: 1711 loss: 0.000115071263
Iter: 1712 loss: 0.00011496208
Iter: 1713 loss: 0.000115345829
Iter: 1714 loss: 0.000114933704
Iter: 1715 loss: 0.00011486224
Iter: 1716 loss: 0.000114860261
Iter: 1717 loss: 0.000114801973
Iter: 1718 loss: 0.000114738104
Iter: 1719 loss: 0.000114729017
Iter: 1720 loss: 0.000114648778
Iter: 1721 loss: 0.000114788454
Iter: 1722 loss: 0.000114613817
Iter: 1723 loss: 0.000114523093
Iter: 1724 loss: 0.00011489974
Iter: 1725 loss: 0.000114503622
Iter: 1726 loss: 0.000114414004
Iter: 1727 loss: 0.000114512426
Iter: 1728 loss: 0.00011436487
Iter: 1729 loss: 0.00011427523
Iter: 1730 loss: 0.000114409391
Iter: 1731 loss: 0.000114232207
Iter: 1732 loss: 0.000114126233
Iter: 1733 loss: 0.000114611161
Iter: 1734 loss: 0.000114105991
Iter: 1735 loss: 0.000114026174
Iter: 1736 loss: 0.000114640083
Iter: 1737 loss: 0.000114020178
Iter: 1738 loss: 0.00011394648
Iter: 1739 loss: 0.000114119321
Iter: 1740 loss: 0.00011391881
Iter: 1741 loss: 0.000113845817
Iter: 1742 loss: 0.000113783914
Iter: 1743 loss: 0.000113763417
Iter: 1744 loss: 0.000113656119
Iter: 1745 loss: 0.000114354691
Iter: 1746 loss: 0.000113644375
Iter: 1747 loss: 0.000113556162
Iter: 1748 loss: 0.000113647882
Iter: 1749 loss: 0.000113507711
Iter: 1750 loss: 0.000113408969
Iter: 1751 loss: 0.000114006391
Iter: 1752 loss: 0.000113397276
Iter: 1753 loss: 0.00011330808
Iter: 1754 loss: 0.000113394075
Iter: 1755 loss: 0.000113257382
Iter: 1756 loss: 0.000113189737
Iter: 1757 loss: 0.000113188027
Iter: 1758 loss: 0.000113131224
Iter: 1759 loss: 0.000113090195
Iter: 1760 loss: 0.000113070942
Iter: 1761 loss: 0.000112986047
Iter: 1762 loss: 0.0001130448
Iter: 1763 loss: 0.000112933303
Iter: 1764 loss: 0.000112832233
Iter: 1765 loss: 0.000113053247
Iter: 1766 loss: 0.000112793365
Iter: 1767 loss: 0.000112695488
Iter: 1768 loss: 0.000112917769
Iter: 1769 loss: 0.000112659211
Iter: 1770 loss: 0.000112558628
Iter: 1771 loss: 0.000113059468
Iter: 1772 loss: 0.000112541275
Iter: 1773 loss: 0.000112450842
Iter: 1774 loss: 0.000112866677
Iter: 1775 loss: 0.000112433372
Iter: 1776 loss: 0.000112352966
Iter: 1777 loss: 0.000112408059
Iter: 1778 loss: 0.000112302252
Iter: 1779 loss: 0.000112212991
Iter: 1780 loss: 0.000113257316
Iter: 1781 loss: 0.000112211681
Iter: 1782 loss: 0.000112146314
Iter: 1783 loss: 0.000112076683
Iter: 1784 loss: 0.000112065441
Iter: 1785 loss: 0.000111973568
Iter: 1786 loss: 0.000112286711
Iter: 1787 loss: 0.000111948844
Iter: 1788 loss: 0.000111853711
Iter: 1789 loss: 0.000112059352
Iter: 1790 loss: 0.000111816611
Iter: 1791 loss: 0.000111736314
Iter: 1792 loss: 0.000112630994
Iter: 1793 loss: 0.000111734873
Iter: 1794 loss: 0.000111662404
Iter: 1795 loss: 0.000111945323
Iter: 1796 loss: 0.000111645422
Iter: 1797 loss: 0.000111590234
Iter: 1798 loss: 0.000111528876
Iter: 1799 loss: 0.000111520108
Iter: 1800 loss: 0.000111423331
Iter: 1801 loss: 0.000111671499
Iter: 1802 loss: 0.000111390516
Iter: 1803 loss: 0.000111298214
Iter: 1804 loss: 0.000111565605
Iter: 1805 loss: 0.000111269852
Iter: 1806 loss: 0.00011117832
Iter: 1807 loss: 0.000111542184
Iter: 1808 loss: 0.000111156565
Iter: 1809 loss: 0.000111067493
Iter: 1810 loss: 0.000111263085
Iter: 1811 loss: 0.000111032939
Iter: 1812 loss: 0.000110947
Iter: 1813 loss: 0.000111127381
Iter: 1814 loss: 0.000110913221
Iter: 1815 loss: 0.000110837871
Iter: 1816 loss: 0.000111612841
Iter: 1817 loss: 0.000110836059
Iter: 1818 loss: 0.000110767462
Iter: 1819 loss: 0.000110826346
Iter: 1820 loss: 0.000110726833
Iter: 1821 loss: 0.000110646375
Iter: 1822 loss: 0.000110715308
Iter: 1823 loss: 0.000110599009
Iter: 1824 loss: 0.000110507499
Iter: 1825 loss: 0.000110627399
Iter: 1826 loss: 0.000110461326
Iter: 1827 loss: 0.000110375928
Iter: 1828 loss: 0.000110932597
Iter: 1829 loss: 0.000110367037
Iter: 1830 loss: 0.000110291
Iter: 1831 loss: 0.000111107576
Iter: 1832 loss: 0.000110289271
Iter: 1833 loss: 0.000110237917
Iter: 1834 loss: 0.000110191751
Iter: 1835 loss: 0.000110179157
Iter: 1836 loss: 0.000110098103
Iter: 1837 loss: 0.000110110501
Iter: 1838 loss: 0.000110037035
Iter: 1839 loss: 0.000109941793
Iter: 1840 loss: 0.000110476612
Iter: 1841 loss: 0.00010992834
Iter: 1842 loss: 0.000109838118
Iter: 1843 loss: 0.000110073343
Iter: 1844 loss: 0.000109808083
Iter: 1845 loss: 0.000109715416
Iter: 1846 loss: 0.00010985187
Iter: 1847 loss: 0.00010967064
Iter: 1848 loss: 0.000109570559
Iter: 1849 loss: 0.00011025503
Iter: 1850 loss: 0.000109561166
Iter: 1851 loss: 0.000109487053
Iter: 1852 loss: 0.000109531611
Iter: 1853 loss: 0.00010943909
Iter: 1854 loss: 0.000109363551
Iter: 1855 loss: 0.000110516856
Iter: 1856 loss: 0.000109363704
Iter: 1857 loss: 0.000109311019
Iter: 1858 loss: 0.000109272383
Iter: 1859 loss: 0.000109254142
Iter: 1860 loss: 0.000109176079
Iter: 1861 loss: 0.000109297398
Iter: 1862 loss: 0.000109138913
Iter: 1863 loss: 0.000109047978
Iter: 1864 loss: 0.000109656168
Iter: 1865 loss: 0.00010903849
Iter: 1866 loss: 0.00010898013
Iter: 1867 loss: 0.000109243396
Iter: 1868 loss: 0.000108968925
Iter: 1869 loss: 0.00010890713
Iter: 1870 loss: 0.000109177126
Iter: 1871 loss: 0.000108894943
Iter: 1872 loss: 0.000108844877
Iter: 1873 loss: 0.000108789784
Iter: 1874 loss: 0.000108781656
Iter: 1875 loss: 0.000108692271
Iter: 1876 loss: 0.000108854991
Iter: 1877 loss: 0.000108654116
Iter: 1878 loss: 0.000108561311
Iter: 1879 loss: 0.000108765416
Iter: 1880 loss: 0.000108525266
Iter: 1881 loss: 0.00010843057
Iter: 1882 loss: 0.000108578097
Iter: 1883 loss: 0.000108386317
Iter: 1884 loss: 0.000108280365
Iter: 1885 loss: 0.000108979548
Iter: 1886 loss: 0.000108269378
Iter: 1887 loss: 0.000108185734
Iter: 1888 loss: 0.000108332912
Iter: 1889 loss: 0.000108149157
Iter: 1890 loss: 0.000108077613
Iter: 1891 loss: 0.000109154797
Iter: 1892 loss: 0.0001080777
Iter: 1893 loss: 0.000108023363
Iter: 1894 loss: 0.000107992164
Iter: 1895 loss: 0.00010796875
Iter: 1896 loss: 0.000107888525
Iter: 1897 loss: 0.000108127439
Iter: 1898 loss: 0.000107864289
Iter: 1899 loss: 0.000107790576
Iter: 1900 loss: 0.000107943182
Iter: 1901 loss: 0.000107761072
Iter: 1902 loss: 0.000107683722
Iter: 1903 loss: 0.000108404842
Iter: 1904 loss: 0.000107680404
Iter: 1905 loss: 0.000107622473
Iter: 1906 loss: 0.000107836582
Iter: 1907 loss: 0.000107608008
Iter: 1908 loss: 0.000107552107
Iter: 1909 loss: 0.000107505388
Iter: 1910 loss: 0.000107489323
Iter: 1911 loss: 0.000107408428
Iter: 1912 loss: 0.000107559201
Iter: 1913 loss: 0.0001073737
Iter: 1914 loss: 0.00010729357
Iter: 1915 loss: 0.000107496846
Iter: 1916 loss: 0.000107264954
Iter: 1917 loss: 0.000107178246
Iter: 1918 loss: 0.000107364147
Iter: 1919 loss: 0.00010714386
Iter: 1920 loss: 0.000107055006
Iter: 1921 loss: 0.000107581523
Iter: 1922 loss: 0.000107043816
Iter: 1923 loss: 0.000106971362
Iter: 1924 loss: 0.000107183056
Iter: 1925 loss: 0.000106948501
Iter: 1926 loss: 0.00010688023
Iter: 1927 loss: 0.000107006737
Iter: 1928 loss: 0.000106851432
Iter: 1929 loss: 0.000106781867
Iter: 1930 loss: 0.000107349057
Iter: 1931 loss: 0.000106777727
Iter: 1932 loss: 0.000106714768
Iter: 1933 loss: 0.000106741783
Iter: 1934 loss: 0.000106672131
Iter: 1935 loss: 0.000106598047
Iter: 1936 loss: 0.000106693005
Iter: 1937 loss: 0.000106559834
Iter: 1938 loss: 0.000106486012
Iter: 1939 loss: 0.000106985171
Iter: 1940 loss: 0.000106478765
Iter: 1941 loss: 0.000106416526
Iter: 1942 loss: 0.000106939151
Iter: 1943 loss: 0.000106413288
Iter: 1944 loss: 0.000106367093
Iter: 1945 loss: 0.000106286934
Iter: 1946 loss: 0.000106286672
Iter: 1947 loss: 0.000106206237
Iter: 1948 loss: 0.000106486943
Iter: 1949 loss: 0.000106185209
Iter: 1950 loss: 0.000106105072
Iter: 1951 loss: 0.000106433239
Iter: 1952 loss: 0.000106086984
Iter: 1953 loss: 0.000106014289
Iter: 1954 loss: 0.000106194901
Iter: 1955 loss: 0.000105987987
Iter: 1956 loss: 0.000105911073
Iter: 1957 loss: 0.000106088592
Iter: 1958 loss: 0.000105882704
Iter: 1959 loss: 0.000105807601
Iter: 1960 loss: 0.000105922125
Iter: 1961 loss: 0.000105771876
Iter: 1962 loss: 0.000105688014
Iter: 1963 loss: 0.000105884887
Iter: 1964 loss: 0.000105657338
Iter: 1965 loss: 0.000105593805
Iter: 1966 loss: 0.000106360385
Iter: 1967 loss: 0.00010559323
Iter: 1968 loss: 0.000105526567
Iter: 1969 loss: 0.000105553663
Iter: 1970 loss: 0.00010548062
Iter: 1971 loss: 0.000105417741
Iter: 1972 loss: 0.000105464329
Iter: 1973 loss: 0.00010537928
Iter: 1974 loss: 0.00010531336
Iter: 1975 loss: 0.000106113512
Iter: 1976 loss: 0.000105312472
Iter: 1977 loss: 0.000105259598
Iter: 1978 loss: 0.000105499763
Iter: 1979 loss: 0.000105249856
Iter: 1980 loss: 0.000105204679
Iter: 1981 loss: 0.000105136511
Iter: 1982 loss: 0.000105134895
Iter: 1983 loss: 0.000105050975
Iter: 1984 loss: 0.00010522065
Iter: 1985 loss: 0.00010501693
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script80
+ '[' -r STOP.script80 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi3 /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi3
+ date
Mon Nov  2 16:35:02 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MSE
Processing model: 300_300_300_1
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 119, in worker
    result = (True, func(*args, **kwds))
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 47, in starmapstar
    return list(itertools.starmap(args[0], args[1]))
  File "/home/mrdouglas/Manifold/hypersurface_tf.py", line 204, in solve_poly
    c_solved = polyroots(coeff)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/mpmath/calculus/polynomials.py", line 196, in polyroots
    % maxsteps)
mpmath.libmp.libhyper.NoConvergence: Didn't converge in maxsteps=50 steps.
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "biholoNN_train.py", line 65, in <module>
    HS_test = Hypersurface(Z, f, n_pairs)
  File "/home/mrdouglas/Manifold/hypersurface_tf.py", line 31, in __init__
    self.points = self.__solve_points(n_pairs)
  File "/home/mrdouglas/Manifold/hypersurface_tf.py", line 227, in __solve_points
    for zpair in zpairs])):
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 274, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 644, in get
    raise self._value
mpmath.libmp.libhyper.NoConvergence: Didn't converge in maxsteps=50 steps.
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi3/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output81/f1_psi2_phi3/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc80a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc8268c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc826ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc8260d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc745d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc72a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc6908c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc6f4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc6d36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc6d3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc60a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b9439e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b941c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7bc6446a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b93d6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b93f0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b940b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b940b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b934b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b934b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b935a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b9386c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b92737b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b92cba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b92cb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b92db8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b922d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b9243620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b9243400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b91d0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b91769d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b9114488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b9114268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b9142c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b90f3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd7b91c9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 90, in <module>
    model = tf.keras.models.load_model(load_path, compile=False)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py", line 186, in load_model
    loader_impl.parse_saved_model(filepath)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py", line 113, in parse_saved_model
    constants.SAVED_MODEL_FILENAME_PB))
OSError: SavedModel file does not exist at: /home/mrdouglas/Manifold/experiments.final/output80/f1_psi2_phi3/300_300_300_1/{saved_model.pbtxt|saved_model.pb}
++ basename experiments.final/script80
+ '[' -r STOP.script80 ']'
