+ RUN=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ LAYERS=300_300_300_1
+ case $RUN in
+ PSI=3
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f2
+ case $fn in
+ OPT=--alpha
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0
+ date
Wed Nov  4 10:46:41 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f2 --psi 3 --alpha 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e304add08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e304ad730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30500840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e3050df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30483730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e303c1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e303c11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e303c1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e303afd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e303f9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e303f92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30331d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30331620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30331ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e3033a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30234f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e301d5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e301ff840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e301d5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30174950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e302b7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e301249d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e300f1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30100f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30105730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30300840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30300620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30068a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30068bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30029bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e30029ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ddc7dac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ddc7da6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ddc7578c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e301a1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3e301b4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.005255385
test_loss: 0.0053481697
train_loss: 0.0044807
test_loss: 0.004337728
train_loss: 0.0038790093
test_loss: 0.0040441905
train_loss: 0.004681156
test_loss: 0.004181132
train_loss: 0.0038243579
test_loss: 0.0038508286
train_loss: 0.003866023
test_loss: 0.0038969656
train_loss: 0.0035951934
test_loss: 0.0038441082
train_loss: 0.0035453243
test_loss: 0.0035560064
train_loss: 0.003416251
test_loss: 0.0035649547
train_loss: 0.0034980264
test_loss: 0.003473211
train_loss: 0.0030808668
test_loss: 0.0034780859
train_loss: 0.0032506003
test_loss: 0.0034829325
train_loss: 0.003314805
test_loss: 0.0033836775
train_loss: 0.003025405
test_loss: 0.0034504384
train_loss: 0.0033831366
test_loss: 0.0034790738
train_loss: 0.0032041087
test_loss: 0.003430084
train_loss: 0.0032757632
test_loss: 0.0032869498
train_loss: 0.0031365717
test_loss: 0.0033102084
train_loss: 0.003091374
test_loss: 0.003420374
train_loss: 0.0027983752
test_loss: 0.0031964239
train_loss: 0.0028926248
test_loss: 0.0032528804
train_loss: 0.003337529
test_loss: 0.0033164993
train_loss: 0.0028985622
test_loss: 0.0032050146
train_loss: 0.0028624805
test_loss: 0.0032218266
train_loss: 0.002939431
test_loss: 0.0033764103
train_loss: 0.0029470033
test_loss: 0.003139107
train_loss: 0.0027776866
test_loss: 0.0030627656
train_loss: 0.0027141664
test_loss: 0.0031146065
train_loss: 0.0027902615
test_loss: 0.0031474198
train_loss: 0.0029908472
test_loss: 0.0032367078
train_loss: 0.0028491644
test_loss: 0.0031874476
train_loss: 0.0028712917
test_loss: 0.0030811958
train_loss: 0.0028361236
test_loss: 0.0032199349
train_loss: 0.0027419282
test_loss: 0.0030403475
train_loss: 0.0027558822
test_loss: 0.003022398
train_loss: 0.002687201
test_loss: 0.002998862
train_loss: 0.0026989446
test_loss: 0.0030582335
train_loss: 0.0027585241
test_loss: 0.003109524
train_loss: 0.0027681123
test_loss: 0.003139973
train_loss: 0.0026884896
test_loss: 0.0029302137
train_loss: 0.002676916
test_loss: 0.0030034971
train_loss: 0.0027619316
test_loss: 0.0029649923
train_loss: 0.0027689193
test_loss: 0.003078336
train_loss: 0.0026534337
test_loss: 0.0031371159
train_loss: 0.002874229
test_loss: 0.0031403243
train_loss: 0.00274214
test_loss: 0.0032124738
train_loss: 0.0027623673
test_loss: 0.0029884828
train_loss: 0.0026248293
test_loss: 0.003016551
train_loss: 0.0028349515
test_loss: 0.003099707
train_loss: 0.0026054082
test_loss: 0.0029124024
train_loss: 0.0027058776
test_loss: 0.0029866074
train_loss: 0.0026371868
test_loss: 0.0031174629
train_loss: 0.0026152988
test_loss: 0.0030387323
train_loss: 0.0026370678
test_loss: 0.0030429284
train_loss: 0.002762488
test_loss: 0.0031343147
train_loss: 0.0026458958
test_loss: 0.0029772054
train_loss: 0.002527989
test_loss: 0.002866697
train_loss: 0.0027539872
test_loss: 0.0031852387
train_loss: 0.002893929
test_loss: 0.0032932935
train_loss: 0.002469387
test_loss: 0.0028922837
train_loss: 0.002765393
test_loss: 0.0029874793
train_loss: 0.0026514095
test_loss: 0.0029187882
train_loss: 0.0026313083
test_loss: 0.002886395
train_loss: 0.0026584116
test_loss: 0.0028905144
train_loss: 0.002547782
test_loss: 0.0029720515
train_loss: 0.0024246334
test_loss: 0.0028291908
train_loss: 0.0025316216
test_loss: 0.0028523398
train_loss: 0.0025266553
test_loss: 0.0028315992
train_loss: 0.00249578
test_loss: 0.0028989976
train_loss: 0.002359964
test_loss: 0.002935228
train_loss: 0.0024101278
test_loss: 0.003120855
train_loss: 0.0026553317
test_loss: 0.002963765
train_loss: 0.0023772416
test_loss: 0.0028790534
train_loss: 0.0024219656
test_loss: 0.0027465257
train_loss: 0.0026220225
test_loss: 0.0028720573
train_loss: 0.0025269093
test_loss: 0.0030044077
train_loss: 0.0025455263
test_loss: 0.0030457797
train_loss: 0.002441904
test_loss: 0.0029622905
train_loss: 0.0025230148
test_loss: 0.002794988
train_loss: 0.002395944
test_loss: 0.0028268076
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0/300_300_300_1 --optimizer lbfgs --function f2 --psi 3 --alpha 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47658c9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47658c37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47658c9d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47659e2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4765911268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4765911950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4765886ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47658866a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47658867b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f476581c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4765828488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4765847bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47657f68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47657a60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4765775950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47659b82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4765712598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f476571b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47656e8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47656bb048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47656bb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f476566f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47656156a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4765636f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f476563b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f471096c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4710937ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f476563b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47108e7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47108e7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47108e9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f471089ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f471089e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f46ec128620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f46ec0d3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f46ec0f8d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 9.936688e-06
Iter: 2 loss: 8.78508763e-06
Iter: 3 loss: 8.58507883e-06
Iter: 4 loss: 8.07390279e-06
Iter: 5 loss: 7.42411112e-06
Iter: 6 loss: 7.37524806e-06
Iter: 7 loss: 6.91112973e-06
Iter: 8 loss: 1.22207e-05
Iter: 9 loss: 6.90332308e-06
Iter: 10 loss: 6.39920609e-06
Iter: 11 loss: 6.50183119e-06
Iter: 12 loss: 6.02526688e-06
Iter: 13 loss: 5.70754219e-06
Iter: 14 loss: 1.02151525e-05
Iter: 15 loss: 5.70684824e-06
Iter: 16 loss: 5.50749e-06
Iter: 17 loss: 5.89045248e-06
Iter: 18 loss: 5.42373664e-06
Iter: 19 loss: 5.17315402e-06
Iter: 20 loss: 4.80196377e-06
Iter: 21 loss: 4.79319715e-06
Iter: 22 loss: 4.41799057e-06
Iter: 23 loss: 5.43218175e-06
Iter: 24 loss: 4.29407e-06
Iter: 25 loss: 4.29112424e-06
Iter: 26 loss: 4.18514401e-06
Iter: 27 loss: 4.08747474e-06
Iter: 28 loss: 3.93591154e-06
Iter: 29 loss: 3.93382197e-06
Iter: 30 loss: 3.75869104e-06
Iter: 31 loss: 3.45743251e-06
Iter: 32 loss: 3.45716262e-06
Iter: 33 loss: 3.17873673e-06
Iter: 34 loss: 5.66700146e-06
Iter: 35 loss: 3.16531032e-06
Iter: 36 loss: 2.98118925e-06
Iter: 37 loss: 3.80264828e-06
Iter: 38 loss: 2.94491565e-06
Iter: 39 loss: 2.93034964e-06
Iter: 40 loss: 2.86637533e-06
Iter: 41 loss: 2.82750716e-06
Iter: 42 loss: 2.74220065e-06
Iter: 43 loss: 4.00197086e-06
Iter: 44 loss: 2.73845831e-06
Iter: 45 loss: 2.72563375e-06
Iter: 46 loss: 2.70095461e-06
Iter: 47 loss: 2.66522034e-06
Iter: 48 loss: 2.56859948e-06
Iter: 49 loss: 3.19204264e-06
Iter: 50 loss: 2.54436281e-06
Iter: 51 loss: 2.51564961e-06
Iter: 52 loss: 2.48552351e-06
Iter: 53 loss: 2.45047136e-06
Iter: 54 loss: 2.39390079e-06
Iter: 55 loss: 2.39349902e-06
Iter: 56 loss: 2.32985826e-06
Iter: 57 loss: 2.35398852e-06
Iter: 58 loss: 2.28554518e-06
Iter: 59 loss: 2.26061593e-06
Iter: 60 loss: 2.24858286e-06
Iter: 61 loss: 2.22061499e-06
Iter: 62 loss: 2.21688356e-06
Iter: 63 loss: 2.19713957e-06
Iter: 64 loss: 2.15455702e-06
Iter: 65 loss: 2.22079962e-06
Iter: 66 loss: 2.1345063e-06
Iter: 67 loss: 2.095441e-06
Iter: 68 loss: 2.07702806e-06
Iter: 69 loss: 2.05786637e-06
Iter: 70 loss: 2.01245894e-06
Iter: 71 loss: 2.35309699e-06
Iter: 72 loss: 2.0088512e-06
Iter: 73 loss: 1.97161307e-06
Iter: 74 loss: 2.51442611e-06
Iter: 75 loss: 1.97157306e-06
Iter: 76 loss: 1.95539815e-06
Iter: 77 loss: 1.91786467e-06
Iter: 78 loss: 2.38505072e-06
Iter: 79 loss: 1.91501113e-06
Iter: 80 loss: 1.88534796e-06
Iter: 81 loss: 1.88195497e-06
Iter: 82 loss: 1.86484874e-06
Iter: 83 loss: 1.83570069e-06
Iter: 84 loss: 1.83565021e-06
Iter: 85 loss: 1.82101689e-06
Iter: 86 loss: 1.81760049e-06
Iter: 87 loss: 1.80720508e-06
Iter: 88 loss: 1.77749803e-06
Iter: 89 loss: 1.91395475e-06
Iter: 90 loss: 1.76675917e-06
Iter: 91 loss: 1.73455055e-06
Iter: 92 loss: 1.99812621e-06
Iter: 93 loss: 1.73253932e-06
Iter: 94 loss: 1.71946601e-06
Iter: 95 loss: 1.71554416e-06
Iter: 96 loss: 1.70808164e-06
Iter: 97 loss: 1.69482348e-06
Iter: 98 loss: 2.02434444e-06
Iter: 99 loss: 1.69480813e-06
Iter: 100 loss: 1.67304904e-06
Iter: 101 loss: 1.68488305e-06
Iter: 102 loss: 1.65869722e-06
Iter: 103 loss: 1.63849745e-06
Iter: 104 loss: 1.67477219e-06
Iter: 105 loss: 1.62968684e-06
Iter: 106 loss: 1.618743e-06
Iter: 107 loss: 1.61785545e-06
Iter: 108 loss: 1.60436309e-06
Iter: 109 loss: 1.5883038e-06
Iter: 110 loss: 1.5865902e-06
Iter: 111 loss: 1.57369971e-06
Iter: 112 loss: 1.69229986e-06
Iter: 113 loss: 1.57314571e-06
Iter: 114 loss: 1.55731368e-06
Iter: 115 loss: 1.56351757e-06
Iter: 116 loss: 1.54635086e-06
Iter: 117 loss: 1.53982648e-06
Iter: 118 loss: 1.59414867e-06
Iter: 119 loss: 1.53946394e-06
Iter: 120 loss: 1.53145993e-06
Iter: 121 loss: 1.51765835e-06
Iter: 122 loss: 1.51764198e-06
Iter: 123 loss: 1.50303526e-06
Iter: 124 loss: 1.51566678e-06
Iter: 125 loss: 1.49447692e-06
Iter: 126 loss: 1.48711433e-06
Iter: 127 loss: 1.48658137e-06
Iter: 128 loss: 1.47703622e-06
Iter: 129 loss: 1.46541151e-06
Iter: 130 loss: 1.46430966e-06
Iter: 131 loss: 1.45293143e-06
Iter: 132 loss: 1.51822508e-06
Iter: 133 loss: 1.45141007e-06
Iter: 134 loss: 1.44135697e-06
Iter: 135 loss: 1.44814589e-06
Iter: 136 loss: 1.43503905e-06
Iter: 137 loss: 1.42587623e-06
Iter: 138 loss: 1.45066576e-06
Iter: 139 loss: 1.42286171e-06
Iter: 140 loss: 1.41808459e-06
Iter: 141 loss: 1.41713394e-06
Iter: 142 loss: 1.41290184e-06
Iter: 143 loss: 1.40137195e-06
Iter: 144 loss: 1.473765e-06
Iter: 145 loss: 1.39838869e-06
Iter: 146 loss: 1.4032687e-06
Iter: 147 loss: 1.39320161e-06
Iter: 148 loss: 1.39032124e-06
Iter: 149 loss: 1.38191376e-06
Iter: 150 loss: 1.41557416e-06
Iter: 151 loss: 1.37844677e-06
Iter: 152 loss: 1.37235077e-06
Iter: 153 loss: 1.37127415e-06
Iter: 154 loss: 1.36568838e-06
Iter: 155 loss: 1.35992946e-06
Iter: 156 loss: 1.35887899e-06
Iter: 157 loss: 1.35434675e-06
Iter: 158 loss: 1.35606047e-06
Iter: 159 loss: 1.35109758e-06
Iter: 160 loss: 1.34291463e-06
Iter: 161 loss: 1.39825124e-06
Iter: 162 loss: 1.34205925e-06
Iter: 163 loss: 1.33786853e-06
Iter: 164 loss: 1.33128265e-06
Iter: 165 loss: 1.33117362e-06
Iter: 166 loss: 1.3241679e-06
Iter: 167 loss: 1.34168613e-06
Iter: 168 loss: 1.32174728e-06
Iter: 169 loss: 1.31360366e-06
Iter: 170 loss: 1.35989342e-06
Iter: 171 loss: 1.31247759e-06
Iter: 172 loss: 1.30837475e-06
Iter: 173 loss: 1.35665334e-06
Iter: 174 loss: 1.30829176e-06
Iter: 175 loss: 1.30413832e-06
Iter: 176 loss: 1.30256376e-06
Iter: 177 loss: 1.30020271e-06
Iter: 178 loss: 1.29689761e-06
Iter: 179 loss: 1.31855313e-06
Iter: 180 loss: 1.29651971e-06
Iter: 181 loss: 1.29191869e-06
Iter: 182 loss: 1.28993361e-06
Iter: 183 loss: 1.28758052e-06
Iter: 184 loss: 1.28293391e-06
Iter: 185 loss: 1.29059117e-06
Iter: 186 loss: 1.28083218e-06
Iter: 187 loss: 1.27529393e-06
Iter: 188 loss: 1.32575383e-06
Iter: 189 loss: 1.27505609e-06
Iter: 190 loss: 1.27270232e-06
Iter: 191 loss: 1.26625571e-06
Iter: 192 loss: 1.30358387e-06
Iter: 193 loss: 1.2643967e-06
Iter: 194 loss: 1.26472401e-06
Iter: 195 loss: 1.26118766e-06
Iter: 196 loss: 1.25802262e-06
Iter: 197 loss: 1.25616134e-06
Iter: 198 loss: 1.25486383e-06
Iter: 199 loss: 1.25158294e-06
Iter: 200 loss: 1.2476246e-06
Iter: 201 loss: 1.24725238e-06
Iter: 202 loss: 1.24238159e-06
Iter: 203 loss: 1.30400622e-06
Iter: 204 loss: 1.24232815e-06
Iter: 205 loss: 1.23842176e-06
Iter: 206 loss: 1.24118162e-06
Iter: 207 loss: 1.2359526e-06
Iter: 208 loss: 1.23407119e-06
Iter: 209 loss: 1.23344682e-06
Iter: 210 loss: 1.23168866e-06
Iter: 211 loss: 1.22657571e-06
Iter: 212 loss: 1.24798657e-06
Iter: 213 loss: 1.22454094e-06
Iter: 214 loss: 1.22303902e-06
Iter: 215 loss: 1.22152323e-06
Iter: 216 loss: 1.21847427e-06
Iter: 217 loss: 1.21663516e-06
Iter: 218 loss: 1.21535527e-06
Iter: 219 loss: 1.21290668e-06
Iter: 220 loss: 1.212798e-06
Iter: 221 loss: 1.21091e-06
Iter: 222 loss: 1.2081307e-06
Iter: 223 loss: 1.20805134e-06
Iter: 224 loss: 1.20639697e-06
Iter: 225 loss: 1.20203e-06
Iter: 226 loss: 1.23489326e-06
Iter: 227 loss: 1.2011958e-06
Iter: 228 loss: 1.19724382e-06
Iter: 229 loss: 1.2052634e-06
Iter: 230 loss: 1.19566198e-06
Iter: 231 loss: 1.19343633e-06
Iter: 232 loss: 1.19314814e-06
Iter: 233 loss: 1.19061701e-06
Iter: 234 loss: 1.19203025e-06
Iter: 235 loss: 1.18892945e-06
Iter: 236 loss: 1.18732771e-06
Iter: 237 loss: 1.18480648e-06
Iter: 238 loss: 1.18474361e-06
Iter: 239 loss: 1.18174057e-06
Iter: 240 loss: 1.20875688e-06
Iter: 241 loss: 1.18160199e-06
Iter: 242 loss: 1.17887896e-06
Iter: 243 loss: 1.1880104e-06
Iter: 244 loss: 1.1781085e-06
Iter: 245 loss: 1.17567072e-06
Iter: 246 loss: 1.20224774e-06
Iter: 247 loss: 1.17561717e-06
Iter: 248 loss: 1.17417039e-06
Iter: 249 loss: 1.17148113e-06
Iter: 250 loss: 1.23421967e-06
Iter: 251 loss: 1.17145385e-06
Iter: 252 loss: 1.17083914e-06
Iter: 253 loss: 1.17006164e-06
Iter: 254 loss: 1.16896183e-06
Iter: 255 loss: 1.1662305e-06
Iter: 256 loss: 1.18876721e-06
Iter: 257 loss: 1.16572153e-06
Iter: 258 loss: 1.16516287e-06
Iter: 259 loss: 1.16461138e-06
Iter: 260 loss: 1.16347439e-06
Iter: 261 loss: 1.16115871e-06
Iter: 262 loss: 1.20167124e-06
Iter: 263 loss: 1.16111664e-06
Iter: 264 loss: 1.15861167e-06
Iter: 265 loss: 1.16646765e-06
Iter: 266 loss: 1.15789874e-06
Iter: 267 loss: 1.15526905e-06
Iter: 268 loss: 1.16402316e-06
Iter: 269 loss: 1.15452951e-06
Iter: 270 loss: 1.15302282e-06
Iter: 271 loss: 1.15154216e-06
Iter: 272 loss: 1.15121929e-06
Iter: 273 loss: 1.14884642e-06
Iter: 274 loss: 1.15878913e-06
Iter: 275 loss: 1.1483221e-06
Iter: 276 loss: 1.14635e-06
Iter: 277 loss: 1.15663329e-06
Iter: 278 loss: 1.14603642e-06
Iter: 279 loss: 1.14443367e-06
Iter: 280 loss: 1.16063984e-06
Iter: 281 loss: 1.14438421e-06
Iter: 282 loss: 1.14281102e-06
Iter: 283 loss: 1.14226191e-06
Iter: 284 loss: 1.14138368e-06
Iter: 285 loss: 1.13975227e-06
Iter: 286 loss: 1.14687089e-06
Iter: 287 loss: 1.13944725e-06
Iter: 288 loss: 1.13729925e-06
Iter: 289 loss: 1.13988722e-06
Iter: 290 loss: 1.13613407e-06
Iter: 291 loss: 1.13473914e-06
Iter: 292 loss: 1.13505246e-06
Iter: 293 loss: 1.1336881e-06
Iter: 294 loss: 1.13251167e-06
Iter: 295 loss: 1.13246165e-06
Iter: 296 loss: 1.13171154e-06
Iter: 297 loss: 1.1298348e-06
Iter: 298 loss: 1.14458919e-06
Iter: 299 loss: 1.12946225e-06
Iter: 300 loss: 1.12815042e-06
Iter: 301 loss: 1.12793964e-06
Iter: 302 loss: 1.12696557e-06
Iter: 303 loss: 1.12530643e-06
Iter: 304 loss: 1.16731894e-06
Iter: 305 loss: 1.12529949e-06
Iter: 306 loss: 1.1232504e-06
Iter: 307 loss: 1.12181192e-06
Iter: 308 loss: 1.12108046e-06
Iter: 309 loss: 1.11882082e-06
Iter: 310 loss: 1.11881923e-06
Iter: 311 loss: 1.11754423e-06
Iter: 312 loss: 1.12668192e-06
Iter: 313 loss: 1.11745749e-06
Iter: 314 loss: 1.11624581e-06
Iter: 315 loss: 1.1212876e-06
Iter: 316 loss: 1.11597456e-06
Iter: 317 loss: 1.11480176e-06
Iter: 318 loss: 1.1136483e-06
Iter: 319 loss: 1.11342126e-06
Iter: 320 loss: 1.1117861e-06
Iter: 321 loss: 1.11177815e-06
Iter: 322 loss: 1.11098393e-06
Iter: 323 loss: 1.10885946e-06
Iter: 324 loss: 1.12759903e-06
Iter: 325 loss: 1.10857229e-06
Iter: 326 loss: 1.10731207e-06
Iter: 327 loss: 1.10712961e-06
Iter: 328 loss: 1.10574092e-06
Iter: 329 loss: 1.10600035e-06
Iter: 330 loss: 1.10466749e-06
Iter: 331 loss: 1.10359974e-06
Iter: 332 loss: 1.10434189e-06
Iter: 333 loss: 1.10290864e-06
Iter: 334 loss: 1.10109454e-06
Iter: 335 loss: 1.11266695e-06
Iter: 336 loss: 1.10088786e-06
Iter: 337 loss: 1.10000758e-06
Iter: 338 loss: 1.09775453e-06
Iter: 339 loss: 1.11606096e-06
Iter: 340 loss: 1.09732594e-06
Iter: 341 loss: 1.09437087e-06
Iter: 342 loss: 1.11027543e-06
Iter: 343 loss: 1.09389111e-06
Iter: 344 loss: 1.09299799e-06
Iter: 345 loss: 1.09276414e-06
Iter: 346 loss: 1.09192229e-06
Iter: 347 loss: 1.0937772e-06
Iter: 348 loss: 1.09160146e-06
Iter: 349 loss: 1.09042969e-06
Iter: 350 loss: 1.09037308e-06
Iter: 351 loss: 1.08946199e-06
Iter: 352 loss: 1.08844495e-06
Iter: 353 loss: 1.10372889e-06
Iter: 354 loss: 1.0884512e-06
Iter: 355 loss: 1.08759446e-06
Iter: 356 loss: 1.08652114e-06
Iter: 357 loss: 1.08642178e-06
Iter: 358 loss: 1.08510494e-06
Iter: 359 loss: 1.08349514e-06
Iter: 360 loss: 1.08332392e-06
Iter: 361 loss: 1.08211304e-06
Iter: 362 loss: 1.08171241e-06
Iter: 363 loss: 1.08087238e-06
Iter: 364 loss: 1.07878e-06
Iter: 365 loss: 1.09937798e-06
Iter: 366 loss: 1.07851406e-06
Iter: 367 loss: 1.07835751e-06
Iter: 368 loss: 1.07768972e-06
Iter: 369 loss: 1.07687401e-06
Iter: 370 loss: 1.07540143e-06
Iter: 371 loss: 1.10975043e-06
Iter: 372 loss: 1.07539415e-06
Iter: 373 loss: 1.07377673e-06
Iter: 374 loss: 1.07231494e-06
Iter: 375 loss: 1.07194842e-06
Iter: 376 loss: 1.07062067e-06
Iter: 377 loss: 1.08718132e-06
Iter: 378 loss: 1.07062715e-06
Iter: 379 loss: 1.06929417e-06
Iter: 380 loss: 1.06972607e-06
Iter: 381 loss: 1.06833e-06
Iter: 382 loss: 1.06682523e-06
Iter: 383 loss: 1.08435734e-06
Iter: 384 loss: 1.06681478e-06
Iter: 385 loss: 1.06584343e-06
Iter: 386 loss: 1.06813263e-06
Iter: 387 loss: 1.06550351e-06
Iter: 388 loss: 1.06428297e-06
Iter: 389 loss: 1.06639254e-06
Iter: 390 loss: 1.0637882e-06
Iter: 391 loss: 1.06266486e-06
Iter: 392 loss: 1.0603892e-06
Iter: 393 loss: 1.10110057e-06
Iter: 394 loss: 1.06038112e-06
Iter: 395 loss: 1.05994172e-06
Iter: 396 loss: 1.05951722e-06
Iter: 397 loss: 1.0586192e-06
Iter: 398 loss: 1.05780077e-06
Iter: 399 loss: 1.05759261e-06
Iter: 400 loss: 1.05625679e-06
Iter: 401 loss: 1.05576964e-06
Iter: 402 loss: 1.05501624e-06
Iter: 403 loss: 1.05472918e-06
Iter: 404 loss: 1.05418189e-06
Iter: 405 loss: 1.05353274e-06
Iter: 406 loss: 1.05170011e-06
Iter: 407 loss: 1.06219954e-06
Iter: 408 loss: 1.05121035e-06
Iter: 409 loss: 1.04951084e-06
Iter: 410 loss: 1.05784204e-06
Iter: 411 loss: 1.0491982e-06
Iter: 412 loss: 1.04817525e-06
Iter: 413 loss: 1.04815877e-06
Iter: 414 loss: 1.04726246e-06
Iter: 415 loss: 1.04897856e-06
Iter: 416 loss: 1.0468259e-06
Iter: 417 loss: 1.0459737e-06
Iter: 418 loss: 1.05099275e-06
Iter: 419 loss: 1.04586115e-06
Iter: 420 loss: 1.04511719e-06
Iter: 421 loss: 1.04690548e-06
Iter: 422 loss: 1.04486401e-06
Iter: 423 loss: 1.04380388e-06
Iter: 424 loss: 1.04235153e-06
Iter: 425 loss: 1.04230367e-06
Iter: 426 loss: 1.04108699e-06
Iter: 427 loss: 1.04274795e-06
Iter: 428 loss: 1.04048706e-06
Iter: 429 loss: 1.03977766e-06
Iter: 430 loss: 1.03966136e-06
Iter: 431 loss: 1.03897378e-06
Iter: 432 loss: 1.03779632e-06
Iter: 433 loss: 1.03779848e-06
Iter: 434 loss: 1.03692514e-06
Iter: 435 loss: 1.03987907e-06
Iter: 436 loss: 1.03672483e-06
Iter: 437 loss: 1.03555021e-06
Iter: 438 loss: 1.04248943e-06
Iter: 439 loss: 1.03539992e-06
Iter: 440 loss: 1.03479147e-06
Iter: 441 loss: 1.03325328e-06
Iter: 442 loss: 1.04772994e-06
Iter: 443 loss: 1.03305285e-06
Iter: 444 loss: 1.03120055e-06
Iter: 445 loss: 1.03977379e-06
Iter: 446 loss: 1.03083835e-06
Iter: 447 loss: 1.03070897e-06
Iter: 448 loss: 1.03028219e-06
Iter: 449 loss: 1.02976992e-06
Iter: 450 loss: 1.02928163e-06
Iter: 451 loss: 1.02916761e-06
Iter: 452 loss: 1.02843262e-06
Iter: 453 loss: 1.03698267e-06
Iter: 454 loss: 1.02841727e-06
Iter: 455 loss: 1.02792455e-06
Iter: 456 loss: 1.02832564e-06
Iter: 457 loss: 1.02758577e-06
Iter: 458 loss: 1.02693207e-06
Iter: 459 loss: 1.02626836e-06
Iter: 460 loss: 1.02612023e-06
Iter: 461 loss: 1.02506715e-06
Iter: 462 loss: 1.02637682e-06
Iter: 463 loss: 1.02454351e-06
Iter: 464 loss: 1.02352681e-06
Iter: 465 loss: 1.02350691e-06
Iter: 466 loss: 1.0230882e-06
Iter: 467 loss: 1.02200829e-06
Iter: 468 loss: 1.03283935e-06
Iter: 469 loss: 1.02186868e-06
Iter: 470 loss: 1.02137551e-06
Iter: 471 loss: 1.02124659e-06
Iter: 472 loss: 1.02056651e-06
Iter: 473 loss: 1.01988849e-06
Iter: 474 loss: 1.01972705e-06
Iter: 475 loss: 1.01898854e-06
Iter: 476 loss: 1.01814408e-06
Iter: 477 loss: 1.01803528e-06
Iter: 478 loss: 1.01696435e-06
Iter: 479 loss: 1.02935064e-06
Iter: 480 loss: 1.01694991e-06
Iter: 481 loss: 1.01599574e-06
Iter: 482 loss: 1.02453077e-06
Iter: 483 loss: 1.01596424e-06
Iter: 484 loss: 1.01550427e-06
Iter: 485 loss: 1.01618969e-06
Iter: 486 loss: 1.01531145e-06
Iter: 487 loss: 1.01463752e-06
Iter: 488 loss: 1.01628041e-06
Iter: 489 loss: 1.01439321e-06
Iter: 490 loss: 1.0138333e-06
Iter: 491 loss: 1.01481783e-06
Iter: 492 loss: 1.01360934e-06
Iter: 493 loss: 1.01308194e-06
Iter: 494 loss: 1.01248293e-06
Iter: 495 loss: 1.01239505e-06
Iter: 496 loss: 1.01162618e-06
Iter: 497 loss: 1.02280342e-06
Iter: 498 loss: 1.01163528e-06
Iter: 499 loss: 1.01081821e-06
Iter: 500 loss: 1.01034789e-06
Iter: 501 loss: 1.00998625e-06
Iter: 502 loss: 1.00918419e-06
Iter: 503 loss: 1.00858949e-06
Iter: 504 loss: 1.00831312e-06
Iter: 505 loss: 1.00796262e-06
Iter: 506 loss: 1.00762009e-06
Iter: 507 loss: 1.00733916e-06
Iter: 508 loss: 1.00656985e-06
Iter: 509 loss: 1.01202158e-06
Iter: 510 loss: 1.00644775e-06
Iter: 511 loss: 1.00541388e-06
Iter: 512 loss: 1.006096e-06
Iter: 513 loss: 1.00479656e-06
Iter: 514 loss: 1.00470265e-06
Iter: 515 loss: 1.00417378e-06
Iter: 516 loss: 1.00370755e-06
Iter: 517 loss: 1.00258944e-06
Iter: 518 loss: 1.01694991e-06
Iter: 519 loss: 1.00254044e-06
Iter: 520 loss: 1.00178954e-06
Iter: 521 loss: 1.0017136e-06
Iter: 522 loss: 1.00123907e-06
Iter: 523 loss: 1.00078e-06
Iter: 524 loss: 1.00067632e-06
Iter: 525 loss: 9.99924737e-07
Iter: 526 loss: 1.0014727e-06
Iter: 527 loss: 9.99568e-07
Iter: 528 loss: 9.98882342e-07
Iter: 529 loss: 1.00083957e-06
Iter: 530 loss: 9.98690894e-07
Iter: 531 loss: 9.97920097e-07
Iter: 532 loss: 1.00359182e-06
Iter: 533 loss: 9.97826419e-07
Iter: 534 loss: 9.97330517e-07
Iter: 535 loss: 9.96009703e-07
Iter: 536 loss: 1.00447357e-06
Iter: 537 loss: 9.95691494e-07
Iter: 538 loss: 9.95617711e-07
Iter: 539 loss: 9.95030859e-07
Iter: 540 loss: 9.94342713e-07
Iter: 541 loss: 9.93638423e-07
Iter: 542 loss: 9.93474941e-07
Iter: 543 loss: 9.92641503e-07
Iter: 544 loss: 9.91921524e-07
Iter: 545 loss: 9.91705292e-07
Iter: 546 loss: 9.9078e-07
Iter: 547 loss: 1.0001088e-06
Iter: 548 loss: 9.90756462e-07
Iter: 549 loss: 9.8992507e-07
Iter: 550 loss: 9.98773658e-07
Iter: 551 loss: 9.89919272e-07
Iter: 552 loss: 9.89467e-07
Iter: 553 loss: 9.89246e-07
Iter: 554 loss: 9.89048431e-07
Iter: 555 loss: 9.88053216e-07
Iter: 556 loss: 9.89980208e-07
Iter: 557 loss: 9.87631552e-07
Iter: 558 loss: 9.86995133e-07
Iter: 559 loss: 9.87467274e-07
Iter: 560 loss: 9.8660189e-07
Iter: 561 loss: 9.85669203e-07
Iter: 562 loss: 9.86209557e-07
Iter: 563 loss: 9.85057909e-07
Iter: 564 loss: 9.84401822e-07
Iter: 565 loss: 9.84401e-07
Iter: 566 loss: 9.83808377e-07
Iter: 567 loss: 9.83367272e-07
Iter: 568 loss: 9.83194809e-07
Iter: 569 loss: 9.82486881e-07
Iter: 570 loss: 9.8164378e-07
Iter: 571 loss: 9.81593416e-07
Iter: 572 loss: 9.8174678e-07
Iter: 573 loss: 9.81072617e-07
Iter: 574 loss: 9.80718482e-07
Iter: 575 loss: 9.79786819e-07
Iter: 576 loss: 9.84629651e-07
Iter: 577 loss: 9.79450306e-07
Iter: 578 loss: 9.78270918e-07
Iter: 579 loss: 9.81557719e-07
Iter: 580 loss: 9.77858463e-07
Iter: 581 loss: 9.76832553e-07
Iter: 582 loss: 9.78160642e-07
Iter: 583 loss: 9.76304364e-07
Iter: 584 loss: 9.7658608e-07
Iter: 585 loss: 9.75801e-07
Iter: 586 loss: 9.75432158e-07
Iter: 587 loss: 9.75131684e-07
Iter: 588 loss: 9.75059493e-07
Iter: 589 loss: 9.74323711e-07
Iter: 590 loss: 9.76789124e-07
Iter: 591 loss: 9.74126806e-07
Iter: 592 loss: 9.7350653e-07
Iter: 593 loss: 9.72794624e-07
Iter: 594 loss: 9.72728458e-07
Iter: 595 loss: 9.71939e-07
Iter: 596 loss: 9.83420591e-07
Iter: 597 loss: 9.71932536e-07
Iter: 598 loss: 9.71431405e-07
Iter: 599 loss: 9.73255283e-07
Iter: 600 loss: 9.71311351e-07
Iter: 601 loss: 9.70686642e-07
Iter: 602 loss: 9.69684606e-07
Iter: 603 loss: 9.69690745e-07
Iter: 604 loss: 9.68968607e-07
Iter: 605 loss: 9.72562475e-07
Iter: 606 loss: 9.68881295e-07
Iter: 607 loss: 9.67994538e-07
Iter: 608 loss: 9.71782583e-07
Iter: 609 loss: 9.67799792e-07
Iter: 610 loss: 9.67273e-07
Iter: 611 loss: 9.66511607e-07
Iter: 612 loss: 9.66507059e-07
Iter: 613 loss: 9.65503e-07
Iter: 614 loss: 9.64714786e-07
Iter: 615 loss: 9.644192e-07
Iter: 616 loss: 9.63323373e-07
Iter: 617 loss: 9.75934768e-07
Iter: 618 loss: 9.6329e-07
Iter: 619 loss: 9.6301153e-07
Iter: 620 loss: 9.62816e-07
Iter: 621 loss: 9.62506874e-07
Iter: 622 loss: 9.61808837e-07
Iter: 623 loss: 9.74489581e-07
Iter: 624 loss: 9.61821115e-07
Iter: 625 loss: 9.61072942e-07
Iter: 626 loss: 9.70709152e-07
Iter: 627 loss: 9.61086471e-07
Iter: 628 loss: 9.60688567e-07
Iter: 629 loss: 9.59692102e-07
Iter: 630 loss: 9.68539325e-07
Iter: 631 loss: 9.59525892e-07
Iter: 632 loss: 9.59080808e-07
Iter: 633 loss: 9.58923e-07
Iter: 634 loss: 9.58341e-07
Iter: 635 loss: 9.57665407e-07
Iter: 636 loss: 9.57587758e-07
Iter: 637 loss: 9.56802523e-07
Iter: 638 loss: 9.63832917e-07
Iter: 639 loss: 9.56782515e-07
Iter: 640 loss: 9.56324925e-07
Iter: 641 loss: 9.56048098e-07
Iter: 642 loss: 9.55844826e-07
Iter: 643 loss: 9.55384735e-07
Iter: 644 loss: 9.5536393e-07
Iter: 645 loss: 9.55043788e-07
Iter: 646 loss: 9.54258667e-07
Iter: 647 loss: 9.61390811e-07
Iter: 648 loss: 9.54130428e-07
Iter: 649 loss: 9.53185349e-07
Iter: 650 loss: 9.53743e-07
Iter: 651 loss: 9.52554331e-07
Iter: 652 loss: 9.51492552e-07
Iter: 653 loss: 9.59013846e-07
Iter: 654 loss: 9.514e-07
Iter: 655 loss: 9.50425033e-07
Iter: 656 loss: 9.50430149e-07
Iter: 657 loss: 9.50094261e-07
Iter: 658 loss: 9.49944592e-07
Iter: 659 loss: 9.49770254e-07
Iter: 660 loss: 9.49087735e-07
Iter: 661 loss: 9.51139498e-07
Iter: 662 loss: 9.48860304e-07
Iter: 663 loss: 9.48481556e-07
Iter: 664 loss: 9.47752937e-07
Iter: 665 loss: 9.65821187e-07
Iter: 666 loss: 9.4775686e-07
Iter: 667 loss: 9.47202807e-07
Iter: 668 loss: 9.4718223e-07
Iter: 669 loss: 9.46527223e-07
Iter: 670 loss: 9.45432248e-07
Iter: 671 loss: 9.45420197e-07
Iter: 672 loss: 9.44555723e-07
Iter: 673 loss: 9.50901722e-07
Iter: 674 loss: 9.44466478e-07
Iter: 675 loss: 9.43787597e-07
Iter: 676 loss: 9.47922842e-07
Iter: 677 loss: 9.43683347e-07
Iter: 678 loss: 9.43101e-07
Iter: 679 loss: 9.4565678e-07
Iter: 680 loss: 9.4294046e-07
Iter: 681 loss: 9.42505494e-07
Iter: 682 loss: 9.42056147e-07
Iter: 683 loss: 9.41957921e-07
Iter: 684 loss: 9.41318717e-07
Iter: 685 loss: 9.40065092e-07
Iter: 686 loss: 9.65096092e-07
Iter: 687 loss: 9.40034909e-07
Iter: 688 loss: 9.40522114e-07
Iter: 689 loss: 9.39461529e-07
Iter: 690 loss: 9.3881647e-07
Iter: 691 loss: 9.38079154e-07
Iter: 692 loss: 9.37987409e-07
Iter: 693 loss: 9.37343657e-07
Iter: 694 loss: 9.44312035e-07
Iter: 695 loss: 9.37321e-07
Iter: 696 loss: 9.3675493e-07
Iter: 697 loss: 9.37196262e-07
Iter: 698 loss: 9.36384481e-07
Iter: 699 loss: 9.35948492e-07
Iter: 700 loss: 9.35393587e-07
Iter: 701 loss: 9.3537983e-07
Iter: 702 loss: 9.35016033e-07
Iter: 703 loss: 9.34857894e-07
Iter: 704 loss: 9.34574359e-07
Iter: 705 loss: 9.33778722e-07
Iter: 706 loss: 9.40373297e-07
Iter: 707 loss: 9.33626779e-07
Iter: 708 loss: 9.32365083e-07
Iter: 709 loss: 9.3623089e-07
Iter: 710 loss: 9.3195689e-07
Iter: 711 loss: 9.31358386e-07
Iter: 712 loss: 9.31270392e-07
Iter: 713 loss: 9.3076369e-07
Iter: 714 loss: 9.32051364e-07
Iter: 715 loss: 9.30550868e-07
Iter: 716 loss: 9.30126e-07
Iter: 717 loss: 9.3003041e-07
Iter: 718 loss: 9.2974085e-07
Iter: 719 loss: 9.29038094e-07
Iter: 720 loss: 9.28315387e-07
Iter: 721 loss: 9.28193742e-07
Iter: 722 loss: 9.27662882e-07
Iter: 723 loss: 9.27601491e-07
Iter: 724 loss: 9.2688947e-07
Iter: 725 loss: 9.2754982e-07
Iter: 726 loss: 9.26455755e-07
Iter: 727 loss: 9.25865e-07
Iter: 728 loss: 9.26039434e-07
Iter: 729 loss: 9.25459062e-07
Iter: 730 loss: 9.24454525e-07
Iter: 731 loss: 9.30240958e-07
Iter: 732 loss: 9.24352207e-07
Iter: 733 loss: 9.23936568e-07
Iter: 734 loss: 9.23119501e-07
Iter: 735 loss: 9.37481843e-07
Iter: 736 loss: 9.23076243e-07
Iter: 737 loss: 9.23070218e-07
Iter: 738 loss: 9.22726599e-07
Iter: 739 loss: 9.22410322e-07
Iter: 740 loss: 9.21638787e-07
Iter: 741 loss: 9.30702413e-07
Iter: 742 loss: 9.216281e-07
Iter: 743 loss: 9.20897776e-07
Iter: 744 loss: 9.23220398e-07
Iter: 745 loss: 9.20644652e-07
Iter: 746 loss: 9.19984416e-07
Iter: 747 loss: 9.24813e-07
Iter: 748 loss: 9.19903584e-07
Iter: 749 loss: 9.19439401e-07
Iter: 750 loss: 9.19188153e-07
Iter: 751 loss: 9.19000968e-07
Iter: 752 loss: 9.18225112e-07
Iter: 753 loss: 9.20312971e-07
Iter: 754 loss: 9.17946409e-07
Iter: 755 loss: 9.17448858e-07
Iter: 756 loss: 9.18615228e-07
Iter: 757 loss: 9.17300554e-07
Iter: 758 loss: 9.16998601e-07
Iter: 759 loss: 9.1693903e-07
Iter: 760 loss: 9.16688236e-07
Iter: 761 loss: 9.161854e-07
Iter: 762 loss: 9.25372092e-07
Iter: 763 loss: 9.16176305e-07
Iter: 764 loss: 9.15758733e-07
Iter: 765 loss: 9.15721046e-07
Iter: 766 loss: 9.15466615e-07
Iter: 767 loss: 9.14716622e-07
Iter: 768 loss: 9.20963828e-07
Iter: 769 loss: 9.14598786e-07
Iter: 770 loss: 9.14070313e-07
Iter: 771 loss: 9.1407e-07
Iter: 772 loss: 9.1352922e-07
Iter: 773 loss: 9.13563724e-07
Iter: 774 loss: 9.13067367e-07
Iter: 775 loss: 9.12572943e-07
Iter: 776 loss: 9.13166332e-07
Iter: 777 loss: 9.12298788e-07
Iter: 778 loss: 9.11880079e-07
Iter: 779 loss: 9.11873826e-07
Iter: 780 loss: 9.11545669e-07
Iter: 781 loss: 9.11043173e-07
Iter: 782 loss: 9.1105278e-07
Iter: 783 loss: 9.10453764e-07
Iter: 784 loss: 9.14894429e-07
Iter: 785 loss: 9.10408176e-07
Iter: 786 loss: 9.09954224e-07
Iter: 787 loss: 9.09825474e-07
Iter: 788 loss: 9.09533753e-07
Iter: 789 loss: 9.09236178e-07
Iter: 790 loss: 9.09213782e-07
Iter: 791 loss: 9.08779441e-07
Iter: 792 loss: 9.08274728e-07
Iter: 793 loss: 9.08249092e-07
Iter: 794 loss: 9.07824926e-07
Iter: 795 loss: 9.14053487e-07
Iter: 796 loss: 9.07837432e-07
Iter: 797 loss: 9.07419519e-07
Iter: 798 loss: 9.06927767e-07
Iter: 799 loss: 9.06895536e-07
Iter: 800 loss: 9.06377068e-07
Iter: 801 loss: 9.06259402e-07
Iter: 802 loss: 9.05955972e-07
Iter: 803 loss: 9.05280217e-07
Iter: 804 loss: 9.05269928e-07
Iter: 805 loss: 9.04909143e-07
Iter: 806 loss: 9.04114813e-07
Iter: 807 loss: 9.17323121e-07
Iter: 808 loss: 9.04077467e-07
Iter: 809 loss: 9.03822752e-07
Iter: 810 loss: 9.03718615e-07
Iter: 811 loss: 9.0336664e-07
Iter: 812 loss: 9.03012676e-07
Iter: 813 loss: 9.02933664e-07
Iter: 814 loss: 9.02403144e-07
Iter: 815 loss: 9.04009653e-07
Iter: 816 loss: 9.02230624e-07
Iter: 817 loss: 9.01661622e-07
Iter: 818 loss: 9.01927592e-07
Iter: 819 loss: 9.01294811e-07
Iter: 820 loss: 9.00655778e-07
Iter: 821 loss: 9.04454623e-07
Iter: 822 loss: 9.00585405e-07
Iter: 823 loss: 8.99960241e-07
Iter: 824 loss: 9.03602881e-07
Iter: 825 loss: 8.99900328e-07
Iter: 826 loss: 8.995396e-07
Iter: 827 loss: 8.99838142e-07
Iter: 828 loss: 8.99331837e-07
Iter: 829 loss: 8.98792678e-07
Iter: 830 loss: 9.00421242e-07
Iter: 831 loss: 8.98637268e-07
Iter: 832 loss: 8.98259316e-07
Iter: 833 loss: 8.97650864e-07
Iter: 834 loss: 8.97635516e-07
Iter: 835 loss: 8.97381426e-07
Iter: 836 loss: 8.97267341e-07
Iter: 837 loss: 8.96923439e-07
Iter: 838 loss: 8.96319193e-07
Iter: 839 loss: 9.09823598e-07
Iter: 840 loss: 8.9631294e-07
Iter: 841 loss: 8.95813571e-07
Iter: 842 loss: 8.98175358e-07
Iter: 843 loss: 8.95702442e-07
Iter: 844 loss: 8.95153619e-07
Iter: 845 loss: 8.99140275e-07
Iter: 846 loss: 8.95100584e-07
Iter: 847 loss: 8.94746051e-07
Iter: 848 loss: 8.94509583e-07
Iter: 849 loss: 8.94389245e-07
Iter: 850 loss: 8.93672564e-07
Iter: 851 loss: 8.94354343e-07
Iter: 852 loss: 8.93294498e-07
Iter: 853 loss: 8.92661944e-07
Iter: 854 loss: 8.95267135e-07
Iter: 855 loss: 8.92525577e-07
Iter: 856 loss: 8.92041669e-07
Iter: 857 loss: 8.92028311e-07
Iter: 858 loss: 8.91778427e-07
Iter: 859 loss: 8.91229774e-07
Iter: 860 loss: 9.01730971e-07
Iter: 861 loss: 8.91227842e-07
Iter: 862 loss: 8.90946865e-07
Iter: 863 loss: 8.90892409e-07
Iter: 864 loss: 8.90636613e-07
Iter: 865 loss: 8.89889122e-07
Iter: 866 loss: 8.93362085e-07
Iter: 867 loss: 8.89595185e-07
Iter: 868 loss: 8.89124237e-07
Iter: 869 loss: 8.89049829e-07
Iter: 870 loss: 8.88528e-07
Iter: 871 loss: 8.88447119e-07
Iter: 872 loss: 8.8811953e-07
Iter: 873 loss: 8.87622832e-07
Iter: 874 loss: 8.88345596e-07
Iter: 875 loss: 8.87417855e-07
Iter: 876 loss: 8.87044621e-07
Iter: 877 loss: 8.87036e-07
Iter: 878 loss: 8.86740736e-07
Iter: 879 loss: 8.86323903e-07
Iter: 880 loss: 8.86306282e-07
Iter: 881 loss: 8.85667362e-07
Iter: 882 loss: 8.86026328e-07
Iter: 883 loss: 8.85315e-07
Iter: 884 loss: 8.84661745e-07
Iter: 885 loss: 8.86520695e-07
Iter: 886 loss: 8.84478425e-07
Iter: 887 loss: 8.84009921e-07
Iter: 888 loss: 8.83990765e-07
Iter: 889 loss: 8.8366204e-07
Iter: 890 loss: 8.82853385e-07
Iter: 891 loss: 8.92523246e-07
Iter: 892 loss: 8.82806148e-07
Iter: 893 loss: 8.8238005e-07
Iter: 894 loss: 8.82320592e-07
Iter: 895 loss: 8.81916321e-07
Iter: 896 loss: 8.81289566e-07
Iter: 897 loss: 8.8129184e-07
Iter: 898 loss: 8.80809409e-07
Iter: 899 loss: 8.86190037e-07
Iter: 900 loss: 8.80799234e-07
Iter: 901 loss: 8.80297478e-07
Iter: 902 loss: 8.81027177e-07
Iter: 903 loss: 8.80089374e-07
Iter: 904 loss: 8.79628715e-07
Iter: 905 loss: 8.79317668e-07
Iter: 906 loss: 8.79146114e-07
Iter: 907 loss: 8.78630146e-07
Iter: 908 loss: 8.8657589e-07
Iter: 909 loss: 8.78643e-07
Iter: 910 loss: 8.78130209e-07
Iter: 911 loss: 8.78084734e-07
Iter: 912 loss: 8.77667958e-07
Iter: 913 loss: 8.77082471e-07
Iter: 914 loss: 8.77270736e-07
Iter: 915 loss: 8.76631816e-07
Iter: 916 loss: 8.75957e-07
Iter: 917 loss: 8.77116065e-07
Iter: 918 loss: 8.7564041e-07
Iter: 919 loss: 8.75333683e-07
Iter: 920 loss: 8.75271496e-07
Iter: 921 loss: 8.74952832e-07
Iter: 922 loss: 8.74314367e-07
Iter: 923 loss: 8.87556666e-07
Iter: 924 loss: 8.743271e-07
Iter: 925 loss: 8.73829663e-07
Iter: 926 loss: 8.80996765e-07
Iter: 927 loss: 8.7382233e-07
Iter: 928 loss: 8.73260319e-07
Iter: 929 loss: 8.72787439e-07
Iter: 930 loss: 8.72658e-07
Iter: 931 loss: 8.72032274e-07
Iter: 932 loss: 8.72171483e-07
Iter: 933 loss: 8.71543079e-07
Iter: 934 loss: 8.70792746e-07
Iter: 935 loss: 8.70798e-07
Iter: 936 loss: 8.70288943e-07
Iter: 937 loss: 8.69644396e-07
Iter: 938 loss: 8.69615292e-07
Iter: 939 loss: 8.68930954e-07
Iter: 940 loss: 8.74199714e-07
Iter: 941 loss: 8.68891107e-07
Iter: 942 loss: 8.68267307e-07
Iter: 943 loss: 8.71095779e-07
Iter: 944 loss: 8.68151e-07
Iter: 945 loss: 8.67690801e-07
Iter: 946 loss: 8.67358835e-07
Iter: 947 loss: 8.67214794e-07
Iter: 948 loss: 8.66541541e-07
Iter: 949 loss: 8.67088147e-07
Iter: 950 loss: 8.6619167e-07
Iter: 951 loss: 8.65502045e-07
Iter: 952 loss: 8.74048737e-07
Iter: 953 loss: 8.65526e-07
Iter: 954 loss: 8.64885124e-07
Iter: 955 loss: 8.66269488e-07
Iter: 956 loss: 8.64643e-07
Iter: 957 loss: 8.64265303e-07
Iter: 958 loss: 8.64777405e-07
Iter: 959 loss: 8.64065e-07
Iter: 960 loss: 8.63556465e-07
Iter: 961 loss: 8.65765855e-07
Iter: 962 loss: 8.63403784e-07
Iter: 963 loss: 8.62995194e-07
Iter: 964 loss: 8.62345189e-07
Iter: 965 loss: 8.6234752e-07
Iter: 966 loss: 8.61855824e-07
Iter: 967 loss: 8.61838544e-07
Iter: 968 loss: 8.61409717e-07
Iter: 969 loss: 8.61062119e-07
Iter: 970 loss: 8.60878686e-07
Iter: 971 loss: 8.60387559e-07
Iter: 972 loss: 8.61768399e-07
Iter: 973 loss: 8.60219131e-07
Iter: 974 loss: 8.59692932e-07
Iter: 975 loss: 8.6447875e-07
Iter: 976 loss: 8.59674401e-07
Iter: 977 loss: 8.59266265e-07
Iter: 978 loss: 8.59034628e-07
Iter: 979 loss: 8.58841304e-07
Iter: 980 loss: 8.58296744e-07
Iter: 981 loss: 8.58144858e-07
Iter: 982 loss: 8.57800728e-07
Iter: 983 loss: 8.5713782e-07
Iter: 984 loss: 8.61860201e-07
Iter: 985 loss: 8.57116333e-07
Iter: 986 loss: 8.56708425e-07
Iter: 987 loss: 8.5671e-07
Iter: 988 loss: 8.56391807e-07
Iter: 989 loss: 8.55827238e-07
Iter: 990 loss: 8.67001631e-07
Iter: 991 loss: 8.55836504e-07
Iter: 992 loss: 8.55448434e-07
Iter: 993 loss: 8.55422627e-07
Iter: 994 loss: 8.55107e-07
Iter: 995 loss: 8.54544624e-07
Iter: 996 loss: 8.6713203e-07
Iter: 997 loss: 8.54543714e-07
Iter: 998 loss: 8.53976758e-07
Iter: 999 loss: 8.5512022e-07
Iter: 1000 loss: 8.53830556e-07
Iter: 1001 loss: 8.53542417e-07
Iter: 1002 loss: 8.5346e-07
Iter: 1003 loss: 8.53250924e-07
Iter: 1004 loss: 8.52706648e-07
Iter: 1005 loss: 8.59621082e-07
Iter: 1006 loss: 8.52661344e-07
Iter: 1007 loss: 8.52199832e-07
Iter: 1008 loss: 8.5650754e-07
Iter: 1009 loss: 8.52135372e-07
Iter: 1010 loss: 8.51825121e-07
Iter: 1011 loss: 8.55302e-07
Iter: 1012 loss: 8.51827281e-07
Iter: 1013 loss: 8.51558809e-07
Iter: 1014 loss: 8.51216043e-07
Iter: 1015 loss: 8.5119774e-07
Iter: 1016 loss: 8.50763911e-07
Iter: 1017 loss: 8.51607865e-07
Iter: 1018 loss: 8.50609581e-07
Iter: 1019 loss: 8.50255e-07
Iter: 1020 loss: 8.50746119e-07
Iter: 1021 loss: 8.50062804e-07
Iter: 1022 loss: 8.49773073e-07
Iter: 1023 loss: 8.4974954e-07
Iter: 1024 loss: 8.49572245e-07
Iter: 1025 loss: 8.49265e-07
Iter: 1026 loss: 8.49259095e-07
Iter: 1027 loss: 8.48913942e-07
Iter: 1028 loss: 8.5359045e-07
Iter: 1029 loss: 8.48918262e-07
Iter: 1030 loss: 8.48639388e-07
Iter: 1031 loss: 8.48245e-07
Iter: 1032 loss: 8.48224886e-07
Iter: 1033 loss: 8.47803221e-07
Iter: 1034 loss: 8.48728348e-07
Iter: 1035 loss: 8.47643776e-07
Iter: 1036 loss: 8.47213641e-07
Iter: 1037 loss: 8.53866e-07
Iter: 1038 loss: 8.47217507e-07
Iter: 1039 loss: 8.4694318e-07
Iter: 1040 loss: 8.46516059e-07
Iter: 1041 loss: 8.46524927e-07
Iter: 1042 loss: 8.46105877e-07
Iter: 1043 loss: 8.46770263e-07
Iter: 1044 loss: 8.45892373e-07
Iter: 1045 loss: 8.4528989e-07
Iter: 1046 loss: 8.49231753e-07
Iter: 1047 loss: 8.45225543e-07
Iter: 1048 loss: 8.44799104e-07
Iter: 1049 loss: 8.4517535e-07
Iter: 1050 loss: 8.44541091e-07
Iter: 1051 loss: 8.44114311e-07
Iter: 1052 loss: 8.45507316e-07
Iter: 1053 loss: 8.43990392e-07
Iter: 1054 loss: 8.43535588e-07
Iter: 1055 loss: 8.44573606e-07
Iter: 1056 loss: 8.43374e-07
Iter: 1057 loss: 8.42753593e-07
Iter: 1058 loss: 8.43890803e-07
Iter: 1059 loss: 8.42455449e-07
Iter: 1060 loss: 8.42051e-07
Iter: 1061 loss: 8.43263308e-07
Iter: 1062 loss: 8.41942949e-07
Iter: 1063 loss: 8.41389806e-07
Iter: 1064 loss: 8.4197444e-07
Iter: 1065 loss: 8.41068072e-07
Iter: 1066 loss: 8.40509585e-07
Iter: 1067 loss: 8.39816551e-07
Iter: 1068 loss: 8.39772156e-07
Iter: 1069 loss: 8.39108111e-07
Iter: 1070 loss: 8.39131076e-07
Iter: 1071 loss: 8.38481583e-07
Iter: 1072 loss: 8.39359814e-07
Iter: 1073 loss: 8.38169967e-07
Iter: 1074 loss: 8.37697883e-07
Iter: 1075 loss: 8.38039853e-07
Iter: 1076 loss: 8.37447431e-07
Iter: 1077 loss: 8.37095058e-07
Iter: 1078 loss: 8.37108246e-07
Iter: 1079 loss: 8.36762638e-07
Iter: 1080 loss: 8.36336199e-07
Iter: 1081 loss: 8.36324318e-07
Iter: 1082 loss: 8.35699609e-07
Iter: 1083 loss: 8.38896653e-07
Iter: 1084 loss: 8.35618e-07
Iter: 1085 loss: 8.35156925e-07
Iter: 1086 loss: 8.36349045e-07
Iter: 1087 loss: 8.35035507e-07
Iter: 1088 loss: 8.34503112e-07
Iter: 1089 loss: 8.36988079e-07
Iter: 1090 loss: 8.34416653e-07
Iter: 1091 loss: 8.3393229e-07
Iter: 1092 loss: 8.33199124e-07
Iter: 1093 loss: 8.3319685e-07
Iter: 1094 loss: 8.32826345e-07
Iter: 1095 loss: 8.32701062e-07
Iter: 1096 loss: 8.32424121e-07
Iter: 1097 loss: 8.31962666e-07
Iter: 1098 loss: 8.3195755e-07
Iter: 1099 loss: 8.31428281e-07
Iter: 1100 loss: 8.31533839e-07
Iter: 1101 loss: 8.31013836e-07
Iter: 1102 loss: 8.30716886e-07
Iter: 1103 loss: 8.30615363e-07
Iter: 1104 loss: 8.30345357e-07
Iter: 1105 loss: 8.29989176e-07
Iter: 1106 loss: 8.29974738e-07
Iter: 1107 loss: 8.29540909e-07
Iter: 1108 loss: 8.29658518e-07
Iter: 1109 loss: 8.2920053e-07
Iter: 1110 loss: 8.28866575e-07
Iter: 1111 loss: 8.28808254e-07
Iter: 1112 loss: 8.28513123e-07
Iter: 1113 loss: 8.28172176e-07
Iter: 1114 loss: 8.28119141e-07
Iter: 1115 loss: 8.27710323e-07
Iter: 1116 loss: 8.31825616e-07
Iter: 1117 loss: 8.27703616e-07
Iter: 1118 loss: 8.27324357e-07
Iter: 1119 loss: 8.27950771e-07
Iter: 1120 loss: 8.27180884e-07
Iter: 1121 loss: 8.26683163e-07
Iter: 1122 loss: 8.26587097e-07
Iter: 1123 loss: 8.26291853e-07
Iter: 1124 loss: 8.25769462e-07
Iter: 1125 loss: 8.27855e-07
Iter: 1126 loss: 8.25658844e-07
Iter: 1127 loss: 8.25055054e-07
Iter: 1128 loss: 8.26568112e-07
Iter: 1129 loss: 8.24831318e-07
Iter: 1130 loss: 8.24475421e-07
Iter: 1131 loss: 8.24320352e-07
Iter: 1132 loss: 8.24136578e-07
Iter: 1133 loss: 8.23714231e-07
Iter: 1134 loss: 8.27166502e-07
Iter: 1135 loss: 8.23701384e-07
Iter: 1136 loss: 8.23185189e-07
Iter: 1137 loss: 8.23205937e-07
Iter: 1138 loss: 8.22773302e-07
Iter: 1139 loss: 8.2229792e-07
Iter: 1140 loss: 8.22643642e-07
Iter: 1141 loss: 8.22017057e-07
Iter: 1142 loss: 8.21533604e-07
Iter: 1143 loss: 8.23849746e-07
Iter: 1144 loss: 8.21458684e-07
Iter: 1145 loss: 8.20898663e-07
Iter: 1146 loss: 8.24686708e-07
Iter: 1147 loss: 8.20857395e-07
Iter: 1148 loss: 8.20537707e-07
Iter: 1149 loss: 8.20436128e-07
Iter: 1150 loss: 8.20238768e-07
Iter: 1151 loss: 8.19764864e-07
Iter: 1152 loss: 8.23599294e-07
Iter: 1153 loss: 8.19733884e-07
Iter: 1154 loss: 8.19357069e-07
Iter: 1155 loss: 8.19916181e-07
Iter: 1156 loss: 8.19152319e-07
Iter: 1157 loss: 8.18769308e-07
Iter: 1158 loss: 8.18549779e-07
Iter: 1159 loss: 8.18366743e-07
Iter: 1160 loss: 8.17861462e-07
Iter: 1161 loss: 8.25703751e-07
Iter: 1162 loss: 8.17868226e-07
Iter: 1163 loss: 8.17562864e-07
Iter: 1164 loss: 8.17112e-07
Iter: 1165 loss: 8.17087198e-07
Iter: 1166 loss: 8.16663771e-07
Iter: 1167 loss: 8.17738169e-07
Iter: 1168 loss: 8.16471356e-07
Iter: 1169 loss: 8.16086185e-07
Iter: 1170 loss: 8.16078e-07
Iter: 1171 loss: 8.15766043e-07
Iter: 1172 loss: 8.14959833e-07
Iter: 1173 loss: 8.21765298e-07
Iter: 1174 loss: 8.14834152e-07
Iter: 1175 loss: 8.14184091e-07
Iter: 1176 loss: 8.21641152e-07
Iter: 1177 loss: 8.14150383e-07
Iter: 1178 loss: 8.13814836e-07
Iter: 1179 loss: 8.13813813e-07
Iter: 1180 loss: 8.13501572e-07
Iter: 1181 loss: 8.12947405e-07
Iter: 1182 loss: 8.12939447e-07
Iter: 1183 loss: 8.12609073e-07
Iter: 1184 loss: 8.12599524e-07
Iter: 1185 loss: 8.12323094e-07
Iter: 1186 loss: 8.12694736e-07
Iter: 1187 loss: 8.12181781e-07
Iter: 1188 loss: 8.11827135e-07
Iter: 1189 loss: 8.11711459e-07
Iter: 1190 loss: 8.11510859e-07
Iter: 1191 loss: 8.11016889e-07
Iter: 1192 loss: 8.13412953e-07
Iter: 1193 loss: 8.10946574e-07
Iter: 1194 loss: 8.10341817e-07
Iter: 1195 loss: 8.10060669e-07
Iter: 1196 loss: 8.09758149e-07
Iter: 1197 loss: 8.09163225e-07
Iter: 1198 loss: 8.09457106e-07
Iter: 1199 loss: 8.08775553e-07
Iter: 1200 loss: 8.08592574e-07
Iter: 1201 loss: 8.08477637e-07
Iter: 1202 loss: 8.08185177e-07
Iter: 1203 loss: 8.07871e-07
Iter: 1204 loss: 8.07813194e-07
Iter: 1205 loss: 8.07354e-07
Iter: 1206 loss: 8.0675261e-07
Iter: 1207 loss: 8.06715434e-07
Iter: 1208 loss: 8.06278706e-07
Iter: 1209 loss: 8.06212142e-07
Iter: 1210 loss: 8.05837033e-07
Iter: 1211 loss: 8.07007609e-07
Iter: 1212 loss: 8.05707714e-07
Iter: 1213 loss: 8.0543208e-07
Iter: 1214 loss: 8.05718969e-07
Iter: 1215 loss: 8.05267575e-07
Iter: 1216 loss: 8.0486825e-07
Iter: 1217 loss: 8.06901312e-07
Iter: 1218 loss: 8.04820218e-07
Iter: 1219 loss: 8.04531e-07
Iter: 1220 loss: 8.04901049e-07
Iter: 1221 loss: 8.04388947e-07
Iter: 1222 loss: 8.04092451e-07
Iter: 1223 loss: 8.04760248e-07
Iter: 1224 loss: 8.03997693e-07
Iter: 1225 loss: 8.03621106e-07
Iter: 1226 loss: 8.04321076e-07
Iter: 1227 loss: 8.03455919e-07
Iter: 1228 loss: 8.03052728e-07
Iter: 1229 loss: 8.02156251e-07
Iter: 1230 loss: 8.16794056e-07
Iter: 1231 loss: 8.02134537e-07
Iter: 1232 loss: 8.01600379e-07
Iter: 1233 loss: 8.01585202e-07
Iter: 1234 loss: 8.01092483e-07
Iter: 1235 loss: 8.04500701e-07
Iter: 1236 loss: 8.01039505e-07
Iter: 1237 loss: 8.00805481e-07
Iter: 1238 loss: 8.00478347e-07
Iter: 1239 loss: 8.00479768e-07
Iter: 1240 loss: 8.00080784e-07
Iter: 1241 loss: 8.02877707e-07
Iter: 1242 loss: 8.00059865e-07
Iter: 1243 loss: 7.99688564e-07
Iter: 1244 loss: 8.02067234e-07
Iter: 1245 loss: 7.99641498e-07
Iter: 1246 loss: 7.9938809e-07
Iter: 1247 loss: 7.99609609e-07
Iter: 1248 loss: 7.99219151e-07
Iter: 1249 loss: 7.98927601e-07
Iter: 1250 loss: 8.01432407e-07
Iter: 1251 loss: 7.98873202e-07
Iter: 1252 loss: 7.98625081e-07
Iter: 1253 loss: 7.984587e-07
Iter: 1254 loss: 7.98344388e-07
Iter: 1255 loss: 7.97942846e-07
Iter: 1256 loss: 7.99462e-07
Iter: 1257 loss: 7.97854682e-07
Iter: 1258 loss: 7.97536472e-07
Iter: 1259 loss: 8.00348118e-07
Iter: 1260 loss: 7.97509301e-07
Iter: 1261 loss: 7.97341045e-07
Iter: 1262 loss: 7.97069788e-07
Iter: 1263 loss: 7.97071152e-07
Iter: 1264 loss: 7.96747258e-07
Iter: 1265 loss: 7.96819222e-07
Iter: 1266 loss: 7.96479753e-07
Iter: 1267 loss: 7.96161771e-07
Iter: 1268 loss: 7.96139147e-07
Iter: 1269 loss: 7.95931555e-07
Iter: 1270 loss: 7.95446283e-07
Iter: 1271 loss: 8.01289e-07
Iter: 1272 loss: 7.95400581e-07
Iter: 1273 loss: 7.9490286e-07
Iter: 1274 loss: 7.97295513e-07
Iter: 1275 loss: 7.94814468e-07
Iter: 1276 loss: 7.94596303e-07
Iter: 1277 loss: 7.94560947e-07
Iter: 1278 loss: 7.94388825e-07
Iter: 1279 loss: 7.94306175e-07
Iter: 1280 loss: 7.94226594e-07
Iter: 1281 loss: 7.94011044e-07
Iter: 1282 loss: 7.95966798e-07
Iter: 1283 loss: 7.94002858e-07
Iter: 1284 loss: 7.93816412e-07
Iter: 1285 loss: 7.93802201e-07
Iter: 1286 loss: 7.93658273e-07
Iter: 1287 loss: 7.9337417e-07
Iter: 1288 loss: 7.9397995e-07
Iter: 1289 loss: 7.93265826e-07
Iter: 1290 loss: 7.9295495e-07
Iter: 1291 loss: 7.94218067e-07
Iter: 1292 loss: 7.92904245e-07
Iter: 1293 loss: 7.92617527e-07
Iter: 1294 loss: 7.92338085e-07
Iter: 1295 loss: 7.92248443e-07
Iter: 1296 loss: 7.91949788e-07
Iter: 1297 loss: 7.92702622e-07
Iter: 1298 loss: 7.91816e-07
Iter: 1299 loss: 7.91667105e-07
Iter: 1300 loss: 7.91637717e-07
Iter: 1301 loss: 7.91448429e-07
Iter: 1302 loss: 7.91009711e-07
Iter: 1303 loss: 7.96715199e-07
Iter: 1304 loss: 7.91008517e-07
Iter: 1305 loss: 7.9060294e-07
Iter: 1306 loss: 7.91519597e-07
Iter: 1307 loss: 7.90450486e-07
Iter: 1308 loss: 7.9012807e-07
Iter: 1309 loss: 7.9012716e-07
Iter: 1310 loss: 7.89806677e-07
Iter: 1311 loss: 7.89853857e-07
Iter: 1312 loss: 7.8960494e-07
Iter: 1313 loss: 7.89385467e-07
Iter: 1314 loss: 7.92966489e-07
Iter: 1315 loss: 7.89390185e-07
Iter: 1316 loss: 7.89201408e-07
Iter: 1317 loss: 7.89455157e-07
Iter: 1318 loss: 7.89115e-07
Iter: 1319 loss: 7.88959369e-07
Iter: 1320 loss: 7.89160481e-07
Iter: 1321 loss: 7.88878936e-07
Iter: 1322 loss: 7.88702835e-07
Iter: 1323 loss: 7.89450496e-07
Iter: 1324 loss: 7.88665261e-07
Iter: 1325 loss: 7.88445902e-07
Iter: 1326 loss: 7.88348473e-07
Iter: 1327 loss: 7.88244961e-07
Iter: 1328 loss: 7.87925501e-07
Iter: 1329 loss: 7.87655608e-07
Iter: 1330 loss: 7.87555223e-07
Iter: 1331 loss: 7.87377076e-07
Iter: 1332 loss: 7.87327338e-07
Iter: 1333 loss: 7.8706e-07
Iter: 1334 loss: 7.87218141e-07
Iter: 1335 loss: 7.86900728e-07
Iter: 1336 loss: 7.86682506e-07
Iter: 1337 loss: 7.86454336e-07
Iter: 1338 loss: 7.86416422e-07
Iter: 1339 loss: 7.8628e-07
Iter: 1340 loss: 7.86239127e-07
Iter: 1341 loss: 7.86101964e-07
Iter: 1342 loss: 7.86268515e-07
Iter: 1343 loss: 7.85999532e-07
Iter: 1344 loss: 7.85838893e-07
Iter: 1345 loss: 7.86180124e-07
Iter: 1346 loss: 7.85792963e-07
Iter: 1347 loss: 7.85503858e-07
Iter: 1348 loss: 7.85667964e-07
Iter: 1349 loss: 7.85319173e-07
Iter: 1350 loss: 7.85037514e-07
Iter: 1351 loss: 7.85821214e-07
Iter: 1352 loss: 7.84915e-07
Iter: 1353 loss: 7.84684801e-07
Iter: 1354 loss: 7.86406758e-07
Iter: 1355 loss: 7.84650865e-07
Iter: 1356 loss: 7.84431279e-07
Iter: 1357 loss: 7.84577651e-07
Iter: 1358 loss: 7.84309918e-07
Iter: 1359 loss: 7.84091526e-07
Iter: 1360 loss: 7.83943506e-07
Iter: 1361 loss: 7.83860116e-07
Iter: 1362 loss: 7.83613132e-07
Iter: 1363 loss: 7.8571162e-07
Iter: 1364 loss: 7.8359443e-07
Iter: 1365 loss: 7.83344e-07
Iter: 1366 loss: 7.85041834e-07
Iter: 1367 loss: 7.83307087e-07
Iter: 1368 loss: 7.83170151e-07
Iter: 1369 loss: 7.82792654e-07
Iter: 1370 loss: 7.87156353e-07
Iter: 1371 loss: 7.82755819e-07
Iter: 1372 loss: 7.82379402e-07
Iter: 1373 loss: 7.86074907e-07
Iter: 1374 loss: 7.82357631e-07
Iter: 1375 loss: 7.82025609e-07
Iter: 1376 loss: 7.84744941e-07
Iter: 1377 loss: 7.82009749e-07
Iter: 1378 loss: 7.81844562e-07
Iter: 1379 loss: 7.82457846e-07
Iter: 1380 loss: 7.81803806e-07
Iter: 1381 loss: 7.81618837e-07
Iter: 1382 loss: 7.81944038e-07
Iter: 1383 loss: 7.81528058e-07
Iter: 1384 loss: 7.81348547e-07
Iter: 1385 loss: 7.81346159e-07
Iter: 1386 loss: 7.81185918e-07
Iter: 1387 loss: 7.80935352e-07
Iter: 1388 loss: 7.82169082e-07
Iter: 1389 loss: 7.80862308e-07
Iter: 1390 loss: 7.80611401e-07
Iter: 1391 loss: 7.81082065e-07
Iter: 1392 loss: 7.80475716e-07
Iter: 1393 loss: 7.80245159e-07
Iter: 1394 loss: 7.80114533e-07
Iter: 1395 loss: 7.80006417e-07
Iter: 1396 loss: 7.79657171e-07
Iter: 1397 loss: 7.80280516e-07
Iter: 1398 loss: 7.79516199e-07
Iter: 1399 loss: 7.79330662e-07
Iter: 1400 loss: 7.79266657e-07
Iter: 1401 loss: 7.79144671e-07
Iter: 1402 loss: 7.78738865e-07
Iter: 1403 loss: 7.82054087e-07
Iter: 1404 loss: 7.78676281e-07
Iter: 1405 loss: 7.78281787e-07
Iter: 1406 loss: 7.81103154e-07
Iter: 1407 loss: 7.78251206e-07
Iter: 1408 loss: 7.77959599e-07
Iter: 1409 loss: 7.81499068e-07
Iter: 1410 loss: 7.77947093e-07
Iter: 1411 loss: 7.77707555e-07
Iter: 1412 loss: 7.77614162e-07
Iter: 1413 loss: 7.77506443e-07
Iter: 1414 loss: 7.77172318e-07
Iter: 1415 loss: 7.8065824e-07
Iter: 1416 loss: 7.77162825e-07
Iter: 1417 loss: 7.76962e-07
Iter: 1418 loss: 7.7706261e-07
Iter: 1419 loss: 7.7686741e-07
Iter: 1420 loss: 7.76642139e-07
Iter: 1421 loss: 7.77408e-07
Iter: 1422 loss: 7.76586603e-07
Iter: 1423 loss: 7.76354909e-07
Iter: 1424 loss: 7.76717343e-07
Iter: 1425 loss: 7.7622451e-07
Iter: 1426 loss: 7.75956437e-07
Iter: 1427 loss: 7.75806484e-07
Iter: 1428 loss: 7.75703256e-07
Iter: 1429 loss: 7.75365834e-07
Iter: 1430 loss: 7.76040338e-07
Iter: 1431 loss: 7.75221565e-07
Iter: 1432 loss: 7.75054104e-07
Iter: 1433 loss: 7.75028184e-07
Iter: 1434 loss: 7.74840828e-07
Iter: 1435 loss: 7.74448495e-07
Iter: 1436 loss: 7.81983e-07
Iter: 1437 loss: 7.74445368e-07
Iter: 1438 loss: 7.74124828e-07
Iter: 1439 loss: 7.74995101e-07
Iter: 1440 loss: 7.74045247e-07
Iter: 1441 loss: 7.73805198e-07
Iter: 1442 loss: 7.73803322e-07
Iter: 1443 loss: 7.73575664e-07
Iter: 1444 loss: 7.73431395e-07
Iter: 1445 loss: 7.73367503e-07
Iter: 1446 loss: 7.73078796e-07
Iter: 1447 loss: 7.77100354e-07
Iter: 1448 loss: 7.73089425e-07
Iter: 1449 loss: 7.72893486e-07
Iter: 1450 loss: 7.72685894e-07
Iter: 1451 loss: 7.72644398e-07
Iter: 1452 loss: 7.72363e-07
Iter: 1453 loss: 7.74276e-07
Iter: 1454 loss: 7.72322323e-07
Iter: 1455 loss: 7.72083865e-07
Iter: 1456 loss: 7.73033662e-07
Iter: 1457 loss: 7.72041346e-07
Iter: 1458 loss: 7.71849e-07
Iter: 1459 loss: 7.71580517e-07
Iter: 1460 loss: 7.71580403e-07
Iter: 1461 loss: 7.71214616e-07
Iter: 1462 loss: 7.71463249e-07
Iter: 1463 loss: 7.70961e-07
Iter: 1464 loss: 7.7081296e-07
Iter: 1465 loss: 7.70741963e-07
Iter: 1466 loss: 7.70518454e-07
Iter: 1467 loss: 7.70305917e-07
Iter: 1468 loss: 7.70261238e-07
Iter: 1469 loss: 7.6997793e-07
Iter: 1470 loss: 7.69836902e-07
Iter: 1471 loss: 7.69685357e-07
Iter: 1472 loss: 7.69430812e-07
Iter: 1473 loss: 7.6941393e-07
Iter: 1474 loss: 7.69142673e-07
Iter: 1475 loss: 7.6910942e-07
Iter: 1476 loss: 7.68934854e-07
Iter: 1477 loss: 7.6867434e-07
Iter: 1478 loss: 7.72016676e-07
Iter: 1479 loss: 7.68667746e-07
Iter: 1480 loss: 7.6847067e-07
Iter: 1481 loss: 7.68346467e-07
Iter: 1482 loss: 7.68255802e-07
Iter: 1483 loss: 7.67976076e-07
Iter: 1484 loss: 7.69290352e-07
Iter: 1485 loss: 7.67917754e-07
Iter: 1486 loss: 7.6768049e-07
Iter: 1487 loss: 7.68714472e-07
Iter: 1488 loss: 7.67652068e-07
Iter: 1489 loss: 7.67446409e-07
Iter: 1490 loss: 7.67383597e-07
Iter: 1491 loss: 7.67273491e-07
Iter: 1492 loss: 7.67002916e-07
Iter: 1493 loss: 7.66914525e-07
Iter: 1494 loss: 7.6676389e-07
Iter: 1495 loss: 7.66427434e-07
Iter: 1496 loss: 7.69969574e-07
Iter: 1497 loss: 7.66442326e-07
Iter: 1498 loss: 7.66057497e-07
Iter: 1499 loss: 7.67273946e-07
Iter: 1500 loss: 7.65942673e-07
Iter: 1501 loss: 7.65776463e-07
Iter: 1502 loss: 7.65531411e-07
Iter: 1503 loss: 7.65506684e-07
Iter: 1504 loss: 7.65280163e-07
Iter: 1505 loss: 7.69061671e-07
Iter: 1506 loss: 7.65263451e-07
Iter: 1507 loss: 7.65012715e-07
Iter: 1508 loss: 7.6576e-07
Iter: 1509 loss: 7.64933873e-07
Iter: 1510 loss: 7.64810807e-07
Iter: 1511 loss: 7.65823415e-07
Iter: 1512 loss: 7.64785341e-07
Iter: 1513 loss: 7.64644085e-07
Iter: 1514 loss: 7.64617141e-07
Iter: 1515 loss: 7.64493109e-07
Iter: 1516 loss: 7.64315e-07
Iter: 1517 loss: 7.64597644e-07
Iter: 1518 loss: 7.64203e-07
Iter: 1519 loss: 7.63952869e-07
Iter: 1520 loss: 7.64864126e-07
Iter: 1521 loss: 7.6389432e-07
Iter: 1522 loss: 7.63622211e-07
Iter: 1523 loss: 7.63699461e-07
Iter: 1524 loss: 7.63417745e-07
Iter: 1525 loss: 7.63156947e-07
Iter: 1526 loss: 7.63314802e-07
Iter: 1527 loss: 7.63007449e-07
Iter: 1528 loss: 7.62778768e-07
Iter: 1529 loss: 7.64157278e-07
Iter: 1530 loss: 7.62738e-07
Iter: 1531 loss: 7.62499724e-07
Iter: 1532 loss: 7.64158699e-07
Iter: 1533 loss: 7.62476304e-07
Iter: 1534 loss: 7.62297077e-07
Iter: 1535 loss: 7.61932597e-07
Iter: 1536 loss: 7.67409e-07
Iter: 1537 loss: 7.61931688e-07
Iter: 1538 loss: 7.61621607e-07
Iter: 1539 loss: 7.66138101e-07
Iter: 1540 loss: 7.61619276e-07
Iter: 1541 loss: 7.6137303e-07
Iter: 1542 loss: 7.62528373e-07
Iter: 1543 loss: 7.61309252e-07
Iter: 1544 loss: 7.6110382e-07
Iter: 1545 loss: 7.61907359e-07
Iter: 1546 loss: 7.61057436e-07
Iter: 1547 loss: 7.60864737e-07
Iter: 1548 loss: 7.61256615e-07
Iter: 1549 loss: 7.60770774e-07
Iter: 1550 loss: 7.60598937e-07
Iter: 1551 loss: 7.60739908e-07
Iter: 1552 loss: 7.60495652e-07
Iter: 1553 loss: 7.60282e-07
Iter: 1554 loss: 7.61464548e-07
Iter: 1555 loss: 7.60237924e-07
Iter: 1556 loss: 7.60034254e-07
Iter: 1557 loss: 7.60100647e-07
Iter: 1558 loss: 7.59893396e-07
Iter: 1559 loss: 7.59606962e-07
Iter: 1560 loss: 7.59316038e-07
Iter: 1561 loss: 7.59283523e-07
Iter: 1562 loss: 7.58863109e-07
Iter: 1563 loss: 7.61206252e-07
Iter: 1564 loss: 7.58808255e-07
Iter: 1565 loss: 7.58612657e-07
Iter: 1566 loss: 7.58588385e-07
Iter: 1567 loss: 7.58456622e-07
Iter: 1568 loss: 7.58120336e-07
Iter: 1569 loss: 7.6147893e-07
Iter: 1570 loss: 7.58078045e-07
Iter: 1571 loss: 7.57716521e-07
Iter: 1572 loss: 7.59267664e-07
Iter: 1573 loss: 7.57619659e-07
Iter: 1574 loss: 7.5731748e-07
Iter: 1575 loss: 7.57306395e-07
Iter: 1576 loss: 7.571183e-07
Iter: 1577 loss: 7.57118244e-07
Iter: 1578 loss: 7.56970962e-07
Iter: 1579 loss: 7.56620807e-07
Iter: 1580 loss: 7.57735904e-07
Iter: 1581 loss: 7.56530767e-07
Iter: 1582 loss: 7.5625735e-07
Iter: 1583 loss: 7.56255758e-07
Iter: 1584 loss: 7.56036286e-07
Iter: 1585 loss: 7.55734504e-07
Iter: 1586 loss: 7.58990723e-07
Iter: 1587 loss: 7.55710346e-07
Iter: 1588 loss: 7.55494511e-07
Iter: 1589 loss: 7.55746e-07
Iter: 1590 loss: 7.55371616e-07
Iter: 1591 loss: 7.55131055e-07
Iter: 1592 loss: 7.5497195e-07
Iter: 1593 loss: 7.54873668e-07
Iter: 1594 loss: 7.54483665e-07
Iter: 1595 loss: 7.54515781e-07
Iter: 1596 loss: 7.54180519e-07
Iter: 1597 loss: 7.53907443e-07
Iter: 1598 loss: 7.53829909e-07
Iter: 1599 loss: 7.53569623e-07
Iter: 1600 loss: 7.53153188e-07
Iter: 1601 loss: 7.53148925e-07
Iter: 1602 loss: 7.5275716e-07
Iter: 1603 loss: 7.53056156e-07
Iter: 1604 loss: 7.52499204e-07
Iter: 1605 loss: 7.52246365e-07
Iter: 1606 loss: 7.52202595e-07
Iter: 1607 loss: 7.51994662e-07
Iter: 1608 loss: 7.51994207e-07
Iter: 1609 loss: 7.51826065e-07
Iter: 1610 loss: 7.51552591e-07
Iter: 1611 loss: 7.54508676e-07
Iter: 1612 loss: 7.51551738e-07
Iter: 1613 loss: 7.51383e-07
Iter: 1614 loss: 7.51022128e-07
Iter: 1615 loss: 7.57193789e-07
Iter: 1616 loss: 7.50991774e-07
Iter: 1617 loss: 7.5052759e-07
Iter: 1618 loss: 7.5380683e-07
Iter: 1619 loss: 7.50475863e-07
Iter: 1620 loss: 7.50073127e-07
Iter: 1621 loss: 7.51154175e-07
Iter: 1622 loss: 7.49944e-07
Iter: 1623 loss: 7.49605874e-07
Iter: 1624 loss: 7.49422441e-07
Iter: 1625 loss: 7.49268338e-07
Iter: 1626 loss: 7.4878119e-07
Iter: 1627 loss: 7.48876e-07
Iter: 1628 loss: 7.48448201e-07
Iter: 1629 loss: 7.48190246e-07
Iter: 1630 loss: 7.48102082e-07
Iter: 1631 loss: 7.47797969e-07
Iter: 1632 loss: 7.47561728e-07
Iter: 1633 loss: 7.47426611e-07
Iter: 1634 loss: 7.47073273e-07
Iter: 1635 loss: 7.46832086e-07
Iter: 1636 loss: 7.46689352e-07
Iter: 1637 loss: 7.46409228e-07
Iter: 1638 loss: 7.46344881e-07
Iter: 1639 loss: 7.4603463e-07
Iter: 1640 loss: 7.45746831e-07
Iter: 1641 loss: 7.45655029e-07
Iter: 1642 loss: 7.45311411e-07
Iter: 1643 loss: 7.45295779e-07
Iter: 1644 loss: 7.4506147e-07
Iter: 1645 loss: 7.44710803e-07
Iter: 1646 loss: 7.446871e-07
Iter: 1647 loss: 7.44306533e-07
Iter: 1648 loss: 7.46425712e-07
Iter: 1649 loss: 7.44232e-07
Iter: 1650 loss: 7.43830128e-07
Iter: 1651 loss: 7.44757472e-07
Iter: 1652 loss: 7.43675059e-07
Iter: 1653 loss: 7.43195073e-07
Iter: 1654 loss: 7.43027726e-07
Iter: 1655 loss: 7.42802627e-07
Iter: 1656 loss: 7.42211114e-07
Iter: 1657 loss: 7.42761927e-07
Iter: 1658 loss: 7.41903e-07
Iter: 1659 loss: 7.41492556e-07
Iter: 1660 loss: 7.45814532e-07
Iter: 1661 loss: 7.41467602e-07
Iter: 1662 loss: 7.40983751e-07
Iter: 1663 loss: 7.42479301e-07
Iter: 1664 loss: 7.40921223e-07
Iter: 1665 loss: 7.40548785e-07
Iter: 1666 loss: 7.39941e-07
Iter: 1667 loss: 7.54434609e-07
Iter: 1668 loss: 7.39925667e-07
Iter: 1669 loss: 7.39444715e-07
Iter: 1670 loss: 7.39453128e-07
Iter: 1671 loss: 7.3901731e-07
Iter: 1672 loss: 7.40389851e-07
Iter: 1673 loss: 7.38866504e-07
Iter: 1674 loss: 7.38522544e-07
Iter: 1675 loss: 7.40372059e-07
Iter: 1676 loss: 7.38479571e-07
Iter: 1677 loss: 7.38095309e-07
Iter: 1678 loss: 7.38071549e-07
Iter: 1679 loss: 7.37766641e-07
Iter: 1680 loss: 7.3743422e-07
Iter: 1681 loss: 7.38763845e-07
Iter: 1682 loss: 7.37377036e-07
Iter: 1683 loss: 7.36999311e-07
Iter: 1684 loss: 7.38629694e-07
Iter: 1685 loss: 7.36952359e-07
Iter: 1686 loss: 7.36644e-07
Iter: 1687 loss: 7.36541381e-07
Iter: 1688 loss: 7.36369259e-07
Iter: 1689 loss: 7.35937306e-07
Iter: 1690 loss: 7.36164452e-07
Iter: 1691 loss: 7.35661388e-07
Iter: 1692 loss: 7.35226479e-07
Iter: 1693 loss: 7.36201287e-07
Iter: 1694 loss: 7.35055778e-07
Iter: 1695 loss: 7.3479714e-07
Iter: 1696 loss: 7.34726484e-07
Iter: 1697 loss: 7.34551463e-07
Iter: 1698 loss: 7.34014748e-07
Iter: 1699 loss: 7.37475091e-07
Iter: 1700 loss: 7.33895149e-07
Iter: 1701 loss: 7.33250715e-07
Iter: 1702 loss: 7.36154902e-07
Iter: 1703 loss: 7.33106845e-07
Iter: 1704 loss: 7.32902436e-07
Iter: 1705 loss: 7.3279142e-07
Iter: 1706 loss: 7.32613501e-07
Iter: 1707 loss: 7.32444278e-07
Iter: 1708 loss: 7.3239687e-07
Iter: 1709 loss: 7.31982652e-07
Iter: 1710 loss: 7.33496222e-07
Iter: 1711 loss: 7.31891873e-07
Iter: 1712 loss: 7.31634543e-07
Iter: 1713 loss: 7.31498176e-07
Iter: 1714 loss: 7.31353623e-07
Iter: 1715 loss: 7.31058208e-07
Iter: 1716 loss: 7.35363415e-07
Iter: 1717 loss: 7.31047749e-07
Iter: 1718 loss: 7.30818e-07
Iter: 1719 loss: 7.30817931e-07
Iter: 1720 loss: 7.3062e-07
Iter: 1721 loss: 7.30302588e-07
Iter: 1722 loss: 7.30104773e-07
Iter: 1723 loss: 7.29933049e-07
Iter: 1724 loss: 7.29513204e-07
Iter: 1725 loss: 7.30575834e-07
Iter: 1726 loss: 7.29325166e-07
Iter: 1727 loss: 7.29193857e-07
Iter: 1728 loss: 7.2911655e-07
Iter: 1729 loss: 7.28918394e-07
Iter: 1730 loss: 7.28635314e-07
Iter: 1731 loss: 7.28621444e-07
Iter: 1732 loss: 7.28340808e-07
Iter: 1733 loss: 7.28175621e-07
Iter: 1734 loss: 7.28076884e-07
Iter: 1735 loss: 7.27821828e-07
Iter: 1736 loss: 7.27779423e-07
Iter: 1737 loss: 7.27533802e-07
Iter: 1738 loss: 7.27219458e-07
Iter: 1739 loss: 7.27184215e-07
Iter: 1740 loss: 7.27079964e-07
Iter: 1741 loss: 7.27022723e-07
Iter: 1742 loss: 7.26882945e-07
Iter: 1743 loss: 7.26534324e-07
Iter: 1744 loss: 7.3121646e-07
Iter: 1745 loss: 7.26508176e-07
Iter: 1746 loss: 7.26236e-07
Iter: 1747 loss: 7.26995495e-07
Iter: 1748 loss: 7.26159783e-07
Iter: 1749 loss: 7.25967823e-07
Iter: 1750 loss: 7.25955147e-07
Iter: 1751 loss: 7.25796e-07
Iter: 1752 loss: 7.25465839e-07
Iter: 1753 loss: 7.30289059e-07
Iter: 1754 loss: 7.25441282e-07
Iter: 1755 loss: 7.25032066e-07
Iter: 1756 loss: 7.26197e-07
Iter: 1757 loss: 7.24875406e-07
Iter: 1758 loss: 7.24573511e-07
Iter: 1759 loss: 7.27777e-07
Iter: 1760 loss: 7.24554525e-07
Iter: 1761 loss: 7.24274059e-07
Iter: 1762 loss: 7.25812129e-07
Iter: 1763 loss: 7.24249674e-07
Iter: 1764 loss: 7.24137067e-07
Iter: 1765 loss: 7.2388292e-07
Iter: 1766 loss: 7.23897642e-07
Iter: 1767 loss: 7.23607172e-07
Iter: 1768 loss: 7.24305551e-07
Iter: 1769 loss: 7.23507753e-07
Iter: 1770 loss: 7.2311741e-07
Iter: 1771 loss: 7.27354063e-07
Iter: 1772 loss: 7.23102744e-07
Iter: 1773 loss: 7.22976552e-07
Iter: 1774 loss: 7.2277237e-07
Iter: 1775 loss: 7.22767595e-07
Iter: 1776 loss: 7.22393622e-07
Iter: 1777 loss: 7.25242614e-07
Iter: 1778 loss: 7.22350933e-07
Iter: 1779 loss: 7.22185632e-07
Iter: 1780 loss: 7.2191915e-07
Iter: 1781 loss: 7.21920117e-07
Iter: 1782 loss: 7.21675349e-07
Iter: 1783 loss: 7.2432681e-07
Iter: 1784 loss: 7.21676088e-07
Iter: 1785 loss: 7.21486117e-07
Iter: 1786 loss: 7.2274247e-07
Iter: 1787 loss: 7.21473384e-07
Iter: 1788 loss: 7.21346112e-07
Iter: 1789 loss: 7.21053766e-07
Iter: 1790 loss: 7.251233e-07
Iter: 1791 loss: 7.21021479e-07
Iter: 1792 loss: 7.20597427e-07
Iter: 1793 loss: 7.21643346e-07
Iter: 1794 loss: 7.20464698e-07
Iter: 1795 loss: 7.20262e-07
Iter: 1796 loss: 7.20250114e-07
Iter: 1797 loss: 7.20022e-07
Iter: 1798 loss: 7.20107892e-07
Iter: 1799 loss: 7.1988768e-07
Iter: 1800 loss: 7.19721413e-07
Iter: 1801 loss: 7.19410139e-07
Iter: 1802 loss: 7.19389448e-07
Iter: 1803 loss: 7.19135755e-07
Iter: 1804 loss: 7.22506115e-07
Iter: 1805 loss: 7.1912973e-07
Iter: 1806 loss: 7.18957494e-07
Iter: 1807 loss: 7.21698484e-07
Iter: 1808 loss: 7.18963122e-07
Iter: 1809 loss: 7.18848923e-07
Iter: 1810 loss: 7.1851548e-07
Iter: 1811 loss: 7.2012881e-07
Iter: 1812 loss: 7.1839122e-07
Iter: 1813 loss: 7.18654746e-07
Iter: 1814 loss: 7.18264516e-07
Iter: 1815 loss: 7.18138608e-07
Iter: 1816 loss: 7.1790214e-07
Iter: 1817 loss: 7.21644824e-07
Iter: 1818 loss: 7.17908051e-07
Iter: 1819 loss: 7.17699209e-07
Iter: 1820 loss: 7.18057038e-07
Iter: 1821 loss: 7.17604735e-07
Iter: 1822 loss: 7.17384637e-07
Iter: 1823 loss: 7.19477498e-07
Iter: 1824 loss: 7.17392311e-07
Iter: 1825 loss: 7.17146463e-07
Iter: 1826 loss: 7.1746814e-07
Iter: 1827 loss: 7.1706171e-07
Iter: 1828 loss: 7.16787611e-07
Iter: 1829 loss: 7.16794602e-07
Iter: 1830 loss: 7.16602813e-07
Iter: 1831 loss: 7.16301e-07
Iter: 1832 loss: 7.16779596e-07
Iter: 1833 loss: 7.16196325e-07
Iter: 1834 loss: 7.15893293e-07
Iter: 1835 loss: 7.16047111e-07
Iter: 1836 loss: 7.15677857e-07
Iter: 1837 loss: 7.15757324e-07
Iter: 1838 loss: 7.15544843e-07
Iter: 1839 loss: 7.15462761e-07
Iter: 1840 loss: 7.15272449e-07
Iter: 1841 loss: 7.17194382e-07
Iter: 1842 loss: 7.15236354e-07
Iter: 1843 loss: 7.15014437e-07
Iter: 1844 loss: 7.16099e-07
Iter: 1845 loss: 7.14990563e-07
Iter: 1846 loss: 7.14717373e-07
Iter: 1847 loss: 7.16094519e-07
Iter: 1848 loss: 7.14667806e-07
Iter: 1849 loss: 7.1451143e-07
Iter: 1850 loss: 7.15017734e-07
Iter: 1851 loss: 7.14463681e-07
Iter: 1852 loss: 7.14264388e-07
Iter: 1853 loss: 7.14235171e-07
Iter: 1854 loss: 7.1410966e-07
Iter: 1855 loss: 7.13957434e-07
Iter: 1856 loss: 7.14096814e-07
Iter: 1857 loss: 7.13853296e-07
Iter: 1858 loss: 7.13664292e-07
Iter: 1859 loss: 7.15214753e-07
Iter: 1860 loss: 7.13628879e-07
Iter: 1861 loss: 7.13489158e-07
Iter: 1862 loss: 7.1350297e-07
Iter: 1863 loss: 7.1336683e-07
Iter: 1864 loss: 7.13142526e-07
Iter: 1865 loss: 7.13126099e-07
Iter: 1866 loss: 7.12956364e-07
Iter: 1867 loss: 7.12663e-07
Iter: 1868 loss: 7.1251128e-07
Iter: 1869 loss: 7.1236127e-07
Iter: 1870 loss: 7.12236783e-07
Iter: 1871 loss: 7.12188125e-07
Iter: 1872 loss: 7.11956545e-07
Iter: 1873 loss: 7.12015719e-07
Iter: 1874 loss: 7.11789937e-07
Iter: 1875 loss: 7.11610767e-07
Iter: 1876 loss: 7.11282439e-07
Iter: 1877 loss: 7.1938365e-07
Iter: 1878 loss: 7.11291477e-07
Iter: 1879 loss: 7.11039434e-07
Iter: 1880 loss: 7.10963377e-07
Iter: 1881 loss: 7.10812174e-07
Iter: 1882 loss: 7.10531822e-07
Iter: 1883 loss: 7.1054e-07
Iter: 1884 loss: 7.10161203e-07
Iter: 1885 loss: 7.13272868e-07
Iter: 1886 loss: 7.10131303e-07
Iter: 1887 loss: 7.09895346e-07
Iter: 1888 loss: 7.0951927e-07
Iter: 1889 loss: 7.09512278e-07
Iter: 1890 loss: 7.09097094e-07
Iter: 1891 loss: 7.14114776e-07
Iter: 1892 loss: 7.09076858e-07
Iter: 1893 loss: 7.08902121e-07
Iter: 1894 loss: 7.08707148e-07
Iter: 1895 loss: 7.08671166e-07
Iter: 1896 loss: 7.08289861e-07
Iter: 1897 loss: 7.10299219e-07
Iter: 1898 loss: 7.08205278e-07
Iter: 1899 loss: 7.07812887e-07
Iter: 1900 loss: 7.08954701e-07
Iter: 1901 loss: 7.07680158e-07
Iter: 1902 loss: 7.074309e-07
Iter: 1903 loss: 7.08661219e-07
Iter: 1904 loss: 7.07371839e-07
Iter: 1905 loss: 7.07051299e-07
Iter: 1906 loss: 7.08242055e-07
Iter: 1907 loss: 7.06990477e-07
Iter: 1908 loss: 7.06744345e-07
Iter: 1909 loss: 7.06492926e-07
Iter: 1910 loss: 7.06475078e-07
Iter: 1911 loss: 7.06149308e-07
Iter: 1912 loss: 7.08267635e-07
Iter: 1913 loss: 7.06088258e-07
Iter: 1914 loss: 7.05735033e-07
Iter: 1915 loss: 7.08194307e-07
Iter: 1916 loss: 7.0569115e-07
Iter: 1917 loss: 7.0553557e-07
Iter: 1918 loss: 7.05878051e-07
Iter: 1919 loss: 7.05477873e-07
Iter: 1920 loss: 7.05212528e-07
Iter: 1921 loss: 7.04829745e-07
Iter: 1922 loss: 7.04814e-07
Iter: 1923 loss: 7.04601575e-07
Iter: 1924 loss: 7.07387585e-07
Iter: 1925 loss: 7.04615e-07
Iter: 1926 loss: 7.04414674e-07
Iter: 1927 loss: 7.04468164e-07
Iter: 1928 loss: 7.04283536e-07
Iter: 1929 loss: 7.04076797e-07
Iter: 1930 loss: 7.03802471e-07
Iter: 1931 loss: 7.03773594e-07
Iter: 1932 loss: 7.03550086e-07
Iter: 1933 loss: 7.03541332e-07
Iter: 1934 loss: 7.03348235e-07
Iter: 1935 loss: 7.03181968e-07
Iter: 1936 loss: 7.03134901e-07
Iter: 1937 loss: 7.02935324e-07
Iter: 1938 loss: 7.02920431e-07
Iter: 1939 loss: 7.02750754e-07
Iter: 1940 loss: 7.02412876e-07
Iter: 1941 loss: 7.0875933e-07
Iter: 1942 loss: 7.02417765e-07
Iter: 1943 loss: 7.02043906e-07
Iter: 1944 loss: 7.02197951e-07
Iter: 1945 loss: 7.01822159e-07
Iter: 1946 loss: 7.01679483e-07
Iter: 1947 loss: 7.01624344e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0.4
+ date
Wed Nov  4 11:48:13 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0/300_300_300_1 --function f2 --psi 3 --alpha 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2372652158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23726521e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c694ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23725c72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23725c7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23725c6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c611b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c64f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c64f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23725e79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23725e7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f237260bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f237260b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c49b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c4d6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c4d66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c4d68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c4827b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c5bf378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c482950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c482598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c3c8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c362ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c4342f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c434b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c4288c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c3052f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c305730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c2ae0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c2ae488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c3fc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c3fc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c3ea8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c3eabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c22be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f234c22bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.023028567
test_loss: 0.023016615
train_loss: 0.013300905
test_loss: 0.014133399
train_loss: 0.00996177
test_loss: 0.011204668
train_loss: 0.009216845
test_loss: 0.009763627
train_loss: 0.007581125
test_loss: 0.008747208
train_loss: 0.006973063
test_loss: 0.00832803
train_loss: 0.006603843
test_loss: 0.007590427
train_loss: 0.0063704783
test_loss: 0.0075025978
train_loss: 0.006272984
test_loss: 0.0071254615
train_loss: 0.0058902064
test_loss: 0.0068908758
train_loss: 0.005999065
test_loss: 0.0068056136
train_loss: 0.005456456
test_loss: 0.0067201206
train_loss: 0.005113501
test_loss: 0.006350251
train_loss: 0.0048452453
test_loss: 0.0063404036
train_loss: 0.004926811
test_loss: 0.006305012
train_loss: 0.0049945973
test_loss: 0.0060654227
train_loss: 0.004944706
test_loss: 0.0059881303
train_loss: 0.0046956623
test_loss: 0.006022413
train_loss: 0.004746448
test_loss: 0.00588211
train_loss: 0.004524101
test_loss: 0.0058094296
train_loss: 0.004551762
test_loss: 0.0054645864
train_loss: 0.004650032
test_loss: 0.00557419
train_loss: 0.0044365684
test_loss: 0.005662454
train_loss: 0.0042776936
test_loss: 0.0054643573
train_loss: 0.0045577525
test_loss: 0.0058120964
train_loss: 0.004289507
test_loss: 0.005434526
train_loss: 0.0041956147
test_loss: 0.005205151
train_loss: 0.0040330673
test_loss: 0.005299474
train_loss: 0.003995472
test_loss: 0.005169767
train_loss: 0.004178827
test_loss: 0.005796137
train_loss: 0.0043720496
test_loss: 0.005145402
train_loss: 0.00381352
test_loss: 0.005204232
train_loss: 0.004208342
test_loss: 0.005585633
train_loss: 0.0039236294
test_loss: 0.0051220865
train_loss: 0.0041809427
test_loss: 0.0053275838
train_loss: 0.0039949254
test_loss: 0.0050644847
train_loss: 0.004037068
test_loss: 0.005074733
train_loss: 0.0038628555
test_loss: 0.0051471256
train_loss: 0.0038124816
test_loss: 0.0050676023
train_loss: 0.0037082569
test_loss: 0.005217325
train_loss: 0.0043399604
test_loss: 0.0051168716
train_loss: 0.0037144832
test_loss: 0.005014611
train_loss: 0.0037005243
test_loss: 0.0050997376
train_loss: 0.0039157267
test_loss: 0.0049998835
train_loss: 0.0038008909
test_loss: 0.0049193227
train_loss: 0.0040504094
test_loss: 0.005036521
train_loss: 0.0037376946
test_loss: 0.004927411
train_loss: 0.004278397
test_loss: 0.004914435
train_loss: 0.0037551576
test_loss: 0.004902942
train_loss: 0.0035165357
test_loss: 0.004711156
train_loss: 0.0036800317
test_loss: 0.004912657
train_loss: 0.0036830618
test_loss: 0.004915709
train_loss: 0.0035519453
test_loss: 0.0048237154
train_loss: 0.0038862326
test_loss: 0.0048465556
train_loss: 0.0039056398
test_loss: 0.0049154637
train_loss: 0.003731208
test_loss: 0.004940692
train_loss: 0.0034925984
test_loss: 0.0046842615
train_loss: 0.0037426346
test_loss: 0.0048481
train_loss: 0.003775288
test_loss: 0.0047317217
train_loss: 0.0038282173
test_loss: 0.0047520245
train_loss: 0.003522024
test_loss: 0.004923644
train_loss: 0.0036796124
test_loss: 0.0047746864
train_loss: 0.003713276
test_loss: 0.0047904165
train_loss: 0.0034233488
test_loss: 0.0045952704
train_loss: 0.0036377185
test_loss: 0.004811239
train_loss: 0.0035371806
test_loss: 0.0047020623
train_loss: 0.0034727454
test_loss: 0.004574555
train_loss: 0.0035280026
test_loss: 0.0048846435
train_loss: 0.0033558288
test_loss: 0.004632933
train_loss: 0.0033253822
test_loss: 0.004467214
train_loss: 0.003354304
test_loss: 0.0047367886
train_loss: 0.0036579696
test_loss: 0.0046509043
train_loss: 0.0033562658
test_loss: 0.0045721284
train_loss: 0.0035222939
test_loss: 0.004499958
train_loss: 0.0033577634
test_loss: 0.004533363
train_loss: 0.0032243077
test_loss: 0.004611101
train_loss: 0.0036277901
test_loss: 0.0047908314
train_loss: 0.0033265036
test_loss: 0.0045143333
train_loss: 0.0034330466
test_loss: 0.0046437765
train_loss: 0.0033662687
test_loss: 0.004648659
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.4/300_300_300_1 --optimizer lbfgs --function f2 --psi 3 --alpha 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5432fa4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5432fd7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5432fd7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5432e5f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5432e5fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5432e8d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5432e8db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54206ca488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54206cb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5420680598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54206cb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54206b41e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54206b4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f542069c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f542069c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53f41b27b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53f41748c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53f4174c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53f40ca1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53f41747b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53f4174e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53f4093950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b01a16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b019bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b019bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b0175730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54206e6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b01822f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b01828c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b0093620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b0093c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b0093950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b0056c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b0077e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b0037a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53b0038a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.06427612e-05
Iter: 2 loss: 1.79222516e-05
Iter: 3 loss: 5.0863604e-05
Iter: 4 loss: 1.78907e-05
Iter: 5 loss: 1.61819062e-05
Iter: 6 loss: 1.7100534e-05
Iter: 7 loss: 1.50572432e-05
Iter: 8 loss: 1.39249669e-05
Iter: 9 loss: 1.70724597e-05
Iter: 10 loss: 1.35591545e-05
Iter: 11 loss: 1.22468491e-05
Iter: 12 loss: 2.04971147e-05
Iter: 13 loss: 1.20959976e-05
Iter: 14 loss: 1.14535351e-05
Iter: 15 loss: 1.26011792e-05
Iter: 16 loss: 1.11748832e-05
Iter: 17 loss: 1.05826675e-05
Iter: 18 loss: 1.1136287e-05
Iter: 19 loss: 1.02413096e-05
Iter: 20 loss: 9.63003e-06
Iter: 21 loss: 1.71149732e-05
Iter: 22 loss: 9.62338e-06
Iter: 23 loss: 9.22780055e-06
Iter: 24 loss: 9.07213143e-06
Iter: 25 loss: 8.85951522e-06
Iter: 26 loss: 8.57086707e-06
Iter: 27 loss: 9.72708767e-06
Iter: 28 loss: 8.50594643e-06
Iter: 29 loss: 8.14243504e-06
Iter: 30 loss: 1.0069145e-05
Iter: 31 loss: 8.08650293e-06
Iter: 32 loss: 7.8482044e-06
Iter: 33 loss: 7.36661059e-06
Iter: 34 loss: 1.624996e-05
Iter: 35 loss: 7.35923277e-06
Iter: 36 loss: 6.91074274e-06
Iter: 37 loss: 8.69289943e-06
Iter: 38 loss: 6.8091822e-06
Iter: 39 loss: 6.86111844e-06
Iter: 40 loss: 6.63188621e-06
Iter: 41 loss: 6.51182e-06
Iter: 42 loss: 6.29113674e-06
Iter: 43 loss: 1.14213553e-05
Iter: 44 loss: 6.29102942e-06
Iter: 45 loss: 6.20575065e-06
Iter: 46 loss: 6.18864578e-06
Iter: 47 loss: 6.09104609e-06
Iter: 48 loss: 5.99276518e-06
Iter: 49 loss: 5.97326516e-06
Iter: 50 loss: 5.83285282e-06
Iter: 51 loss: 6.1980254e-06
Iter: 52 loss: 5.78498384e-06
Iter: 53 loss: 5.63220738e-06
Iter: 54 loss: 6.01415923e-06
Iter: 55 loss: 5.5785863e-06
Iter: 56 loss: 5.41757072e-06
Iter: 57 loss: 6.45448927e-06
Iter: 58 loss: 5.39999655e-06
Iter: 59 loss: 5.31714613e-06
Iter: 60 loss: 5.24605912e-06
Iter: 61 loss: 5.22330765e-06
Iter: 62 loss: 5.14493695e-06
Iter: 63 loss: 5.14479871e-06
Iter: 64 loss: 5.06382639e-06
Iter: 65 loss: 5.16138425e-06
Iter: 66 loss: 5.02121975e-06
Iter: 67 loss: 4.96891062e-06
Iter: 68 loss: 4.8812617e-06
Iter: 69 loss: 4.8809834e-06
Iter: 70 loss: 4.77115782e-06
Iter: 71 loss: 5.24307416e-06
Iter: 72 loss: 4.74852459e-06
Iter: 73 loss: 4.72511783e-06
Iter: 74 loss: 4.69843508e-06
Iter: 75 loss: 4.67040081e-06
Iter: 76 loss: 4.60812907e-06
Iter: 77 loss: 5.48527714e-06
Iter: 78 loss: 4.60480032e-06
Iter: 79 loss: 4.5866218e-06
Iter: 80 loss: 4.57193528e-06
Iter: 81 loss: 4.54750261e-06
Iter: 82 loss: 4.49041318e-06
Iter: 83 loss: 5.18593151e-06
Iter: 84 loss: 4.48588116e-06
Iter: 85 loss: 4.43038516e-06
Iter: 86 loss: 4.6631485e-06
Iter: 87 loss: 4.4186072e-06
Iter: 88 loss: 4.36255596e-06
Iter: 89 loss: 4.81839925e-06
Iter: 90 loss: 4.35880838e-06
Iter: 91 loss: 4.31497892e-06
Iter: 92 loss: 4.41392285e-06
Iter: 93 loss: 4.29855754e-06
Iter: 94 loss: 4.25943927e-06
Iter: 95 loss: 4.23202709e-06
Iter: 96 loss: 4.21808545e-06
Iter: 97 loss: 4.195369e-06
Iter: 98 loss: 4.1906942e-06
Iter: 99 loss: 4.16434523e-06
Iter: 100 loss: 4.13850239e-06
Iter: 101 loss: 4.13287853e-06
Iter: 102 loss: 4.10174789e-06
Iter: 103 loss: 4.06591244e-06
Iter: 104 loss: 4.06155687e-06
Iter: 105 loss: 4.01248508e-06
Iter: 106 loss: 4.40919621e-06
Iter: 107 loss: 4.00920544e-06
Iter: 108 loss: 3.99831288e-06
Iter: 109 loss: 3.99064629e-06
Iter: 110 loss: 3.97719032e-06
Iter: 111 loss: 3.94480958e-06
Iter: 112 loss: 4.29768488e-06
Iter: 113 loss: 3.94137896e-06
Iter: 114 loss: 3.93624259e-06
Iter: 115 loss: 3.92613674e-06
Iter: 116 loss: 3.91377898e-06
Iter: 117 loss: 3.88578383e-06
Iter: 118 loss: 4.25918188e-06
Iter: 119 loss: 3.88398348e-06
Iter: 120 loss: 3.85562e-06
Iter: 121 loss: 3.95220286e-06
Iter: 122 loss: 3.84801842e-06
Iter: 123 loss: 3.81575774e-06
Iter: 124 loss: 3.98890506e-06
Iter: 125 loss: 3.81097971e-06
Iter: 126 loss: 3.79105268e-06
Iter: 127 loss: 3.8422686e-06
Iter: 128 loss: 3.78425329e-06
Iter: 129 loss: 3.7636546e-06
Iter: 130 loss: 3.77557899e-06
Iter: 131 loss: 3.75036529e-06
Iter: 132 loss: 3.73380067e-06
Iter: 133 loss: 3.92077118e-06
Iter: 134 loss: 3.73354919e-06
Iter: 135 loss: 3.71551755e-06
Iter: 136 loss: 3.7199145e-06
Iter: 137 loss: 3.70235148e-06
Iter: 138 loss: 3.68608562e-06
Iter: 139 loss: 3.65631536e-06
Iter: 140 loss: 4.35556e-06
Iter: 141 loss: 3.6562858e-06
Iter: 142 loss: 3.62553396e-06
Iter: 143 loss: 3.89009e-06
Iter: 144 loss: 3.62383525e-06
Iter: 145 loss: 3.61575712e-06
Iter: 146 loss: 3.61124034e-06
Iter: 147 loss: 3.60297645e-06
Iter: 148 loss: 3.58457237e-06
Iter: 149 loss: 3.84312898e-06
Iter: 150 loss: 3.58360535e-06
Iter: 151 loss: 3.57598037e-06
Iter: 152 loss: 3.57263593e-06
Iter: 153 loss: 3.5647663e-06
Iter: 154 loss: 3.54659051e-06
Iter: 155 loss: 3.77892115e-06
Iter: 156 loss: 3.54529061e-06
Iter: 157 loss: 3.52842835e-06
Iter: 158 loss: 3.59191517e-06
Iter: 159 loss: 3.52432653e-06
Iter: 160 loss: 3.50857408e-06
Iter: 161 loss: 3.72691738e-06
Iter: 162 loss: 3.50853861e-06
Iter: 163 loss: 3.49906304e-06
Iter: 164 loss: 3.48886624e-06
Iter: 165 loss: 3.48720687e-06
Iter: 166 loss: 3.47039713e-06
Iter: 167 loss: 3.54828489e-06
Iter: 168 loss: 3.46721708e-06
Iter: 169 loss: 3.45719741e-06
Iter: 170 loss: 3.56356941e-06
Iter: 171 loss: 3.45695412e-06
Iter: 172 loss: 3.44672e-06
Iter: 173 loss: 3.43802731e-06
Iter: 174 loss: 3.4352222e-06
Iter: 175 loss: 3.42143494e-06
Iter: 176 loss: 3.41377654e-06
Iter: 177 loss: 3.40772294e-06
Iter: 178 loss: 3.39071e-06
Iter: 179 loss: 3.43666147e-06
Iter: 180 loss: 3.38518316e-06
Iter: 181 loss: 3.37688812e-06
Iter: 182 loss: 3.3738454e-06
Iter: 183 loss: 3.36788162e-06
Iter: 184 loss: 3.35721143e-06
Iter: 185 loss: 3.62237665e-06
Iter: 186 loss: 3.35721961e-06
Iter: 187 loss: 3.35053164e-06
Iter: 188 loss: 3.34965375e-06
Iter: 189 loss: 3.34401943e-06
Iter: 190 loss: 3.32798163e-06
Iter: 191 loss: 3.40464385e-06
Iter: 192 loss: 3.32232867e-06
Iter: 193 loss: 3.30743228e-06
Iter: 194 loss: 3.48514732e-06
Iter: 195 loss: 3.30719831e-06
Iter: 196 loss: 3.29380646e-06
Iter: 197 loss: 3.39124563e-06
Iter: 198 loss: 3.29263071e-06
Iter: 199 loss: 3.28553324e-06
Iter: 200 loss: 3.2820833e-06
Iter: 201 loss: 3.27866519e-06
Iter: 202 loss: 3.26865916e-06
Iter: 203 loss: 3.32970376e-06
Iter: 204 loss: 3.26742361e-06
Iter: 205 loss: 3.26010468e-06
Iter: 206 loss: 3.31260094e-06
Iter: 207 loss: 3.25942869e-06
Iter: 208 loss: 3.25212613e-06
Iter: 209 loss: 3.24441635e-06
Iter: 210 loss: 3.24303346e-06
Iter: 211 loss: 3.23237259e-06
Iter: 212 loss: 3.22624646e-06
Iter: 213 loss: 3.2216974e-06
Iter: 214 loss: 3.2075709e-06
Iter: 215 loss: 3.31377828e-06
Iter: 216 loss: 3.20643403e-06
Iter: 217 loss: 3.20157619e-06
Iter: 218 loss: 3.19985406e-06
Iter: 219 loss: 3.19658625e-06
Iter: 220 loss: 3.18841057e-06
Iter: 221 loss: 3.27246153e-06
Iter: 222 loss: 3.187434e-06
Iter: 223 loss: 3.18017896e-06
Iter: 224 loss: 3.17984745e-06
Iter: 225 loss: 3.17521335e-06
Iter: 226 loss: 3.16396495e-06
Iter: 227 loss: 3.28345e-06
Iter: 228 loss: 3.16276146e-06
Iter: 229 loss: 3.15205102e-06
Iter: 230 loss: 3.19477795e-06
Iter: 231 loss: 3.14956492e-06
Iter: 232 loss: 3.13719761e-06
Iter: 233 loss: 3.24595862e-06
Iter: 234 loss: 3.13658666e-06
Iter: 235 loss: 3.13057853e-06
Iter: 236 loss: 3.12493421e-06
Iter: 237 loss: 3.12359589e-06
Iter: 238 loss: 3.11705276e-06
Iter: 239 loss: 3.11706754e-06
Iter: 240 loss: 3.11161466e-06
Iter: 241 loss: 3.11989788e-06
Iter: 242 loss: 3.1090558e-06
Iter: 243 loss: 3.10148198e-06
Iter: 244 loss: 3.10078212e-06
Iter: 245 loss: 3.09522466e-06
Iter: 246 loss: 3.08700601e-06
Iter: 247 loss: 3.08630661e-06
Iter: 248 loss: 3.08023709e-06
Iter: 249 loss: 3.07209257e-06
Iter: 250 loss: 3.17341073e-06
Iter: 251 loss: 3.07205482e-06
Iter: 252 loss: 3.06411175e-06
Iter: 253 loss: 3.10651285e-06
Iter: 254 loss: 3.06288393e-06
Iter: 255 loss: 3.05898948e-06
Iter: 256 loss: 3.05743674e-06
Iter: 257 loss: 3.0551837e-06
Iter: 258 loss: 3.04889454e-06
Iter: 259 loss: 3.11298481e-06
Iter: 260 loss: 3.04865625e-06
Iter: 261 loss: 3.04531682e-06
Iter: 262 loss: 3.03650586e-06
Iter: 263 loss: 3.09790721e-06
Iter: 264 loss: 3.03454294e-06
Iter: 265 loss: 3.0249189e-06
Iter: 266 loss: 3.13194664e-06
Iter: 267 loss: 3.02479566e-06
Iter: 268 loss: 3.01602e-06
Iter: 269 loss: 3.073269e-06
Iter: 270 loss: 3.01503223e-06
Iter: 271 loss: 3.01022533e-06
Iter: 272 loss: 3.00059719e-06
Iter: 273 loss: 3.18237608e-06
Iter: 274 loss: 3.00053625e-06
Iter: 275 loss: 2.9966061e-06
Iter: 276 loss: 2.99469548e-06
Iter: 277 loss: 2.99059798e-06
Iter: 278 loss: 2.99493649e-06
Iter: 279 loss: 2.98830628e-06
Iter: 280 loss: 2.98402347e-06
Iter: 281 loss: 2.98783107e-06
Iter: 282 loss: 2.98147415e-06
Iter: 283 loss: 2.97593874e-06
Iter: 284 loss: 2.96784333e-06
Iter: 285 loss: 2.96758617e-06
Iter: 286 loss: 2.96464714e-06
Iter: 287 loss: 2.96294502e-06
Iter: 288 loss: 2.95784184e-06
Iter: 289 loss: 2.95642121e-06
Iter: 290 loss: 2.95331256e-06
Iter: 291 loss: 2.94825168e-06
Iter: 292 loss: 2.96841199e-06
Iter: 293 loss: 2.94712891e-06
Iter: 294 loss: 2.94054507e-06
Iter: 295 loss: 2.95331688e-06
Iter: 296 loss: 2.93785411e-06
Iter: 297 loss: 2.93421022e-06
Iter: 298 loss: 2.92750042e-06
Iter: 299 loss: 3.07815549e-06
Iter: 300 loss: 2.9274911e-06
Iter: 301 loss: 2.92216032e-06
Iter: 302 loss: 2.92197728e-06
Iter: 303 loss: 2.91632136e-06
Iter: 304 loss: 2.91732727e-06
Iter: 305 loss: 2.91213018e-06
Iter: 306 loss: 2.90708749e-06
Iter: 307 loss: 2.90458092e-06
Iter: 308 loss: 2.90214371e-06
Iter: 309 loss: 2.89829904e-06
Iter: 310 loss: 2.89736818e-06
Iter: 311 loss: 2.89410309e-06
Iter: 312 loss: 2.89037393e-06
Iter: 313 loss: 2.8898819e-06
Iter: 314 loss: 2.88397541e-06
Iter: 315 loss: 2.89714899e-06
Iter: 316 loss: 2.8818165e-06
Iter: 317 loss: 2.87580679e-06
Iter: 318 loss: 2.87486159e-06
Iter: 319 loss: 2.87065131e-06
Iter: 320 loss: 2.8669906e-06
Iter: 321 loss: 2.86619161e-06
Iter: 322 loss: 2.86156023e-06
Iter: 323 loss: 2.85576152e-06
Iter: 324 loss: 2.85528608e-06
Iter: 325 loss: 2.85172109e-06
Iter: 326 loss: 2.8515924e-06
Iter: 327 loss: 2.84818134e-06
Iter: 328 loss: 2.84237376e-06
Iter: 329 loss: 2.84234602e-06
Iter: 330 loss: 2.83693726e-06
Iter: 331 loss: 2.83120812e-06
Iter: 332 loss: 2.83021586e-06
Iter: 333 loss: 2.82555175e-06
Iter: 334 loss: 2.8251095e-06
Iter: 335 loss: 2.81920484e-06
Iter: 336 loss: 2.82450151e-06
Iter: 337 loss: 2.81572375e-06
Iter: 338 loss: 2.81071743e-06
Iter: 339 loss: 2.81074404e-06
Iter: 340 loss: 2.80671179e-06
Iter: 341 loss: 2.80503355e-06
Iter: 342 loss: 2.80385143e-06
Iter: 343 loss: 2.8012837e-06
Iter: 344 loss: 2.79443839e-06
Iter: 345 loss: 2.84422663e-06
Iter: 346 loss: 2.79302731e-06
Iter: 347 loss: 2.78671973e-06
Iter: 348 loss: 2.84367661e-06
Iter: 349 loss: 2.78639345e-06
Iter: 350 loss: 2.78070797e-06
Iter: 351 loss: 2.79245296e-06
Iter: 352 loss: 2.77843765e-06
Iter: 353 loss: 2.77403933e-06
Iter: 354 loss: 2.83325608e-06
Iter: 355 loss: 2.77403296e-06
Iter: 356 loss: 2.76942e-06
Iter: 357 loss: 2.763255e-06
Iter: 358 loss: 2.7628912e-06
Iter: 359 loss: 2.75902175e-06
Iter: 360 loss: 2.75901016e-06
Iter: 361 loss: 2.75495358e-06
Iter: 362 loss: 2.75093e-06
Iter: 363 loss: 2.7501419e-06
Iter: 364 loss: 2.74473337e-06
Iter: 365 loss: 2.73855585e-06
Iter: 366 loss: 2.73774117e-06
Iter: 367 loss: 2.73748492e-06
Iter: 368 loss: 2.73405794e-06
Iter: 369 loss: 2.73118917e-06
Iter: 370 loss: 2.72443913e-06
Iter: 371 loss: 2.8097727e-06
Iter: 372 loss: 2.72396142e-06
Iter: 373 loss: 2.71944941e-06
Iter: 374 loss: 2.78159723e-06
Iter: 375 loss: 2.71943736e-06
Iter: 376 loss: 2.71561885e-06
Iter: 377 loss: 2.74079071e-06
Iter: 378 loss: 2.71519139e-06
Iter: 379 loss: 2.71281442e-06
Iter: 380 loss: 2.70707392e-06
Iter: 381 loss: 2.76765604e-06
Iter: 382 loss: 2.70631517e-06
Iter: 383 loss: 2.700609e-06
Iter: 384 loss: 2.76199944e-06
Iter: 385 loss: 2.70047713e-06
Iter: 386 loss: 2.69498696e-06
Iter: 387 loss: 2.70672126e-06
Iter: 388 loss: 2.69284828e-06
Iter: 389 loss: 2.68734357e-06
Iter: 390 loss: 2.74962804e-06
Iter: 391 loss: 2.68724079e-06
Iter: 392 loss: 2.68415465e-06
Iter: 393 loss: 2.67996256e-06
Iter: 394 loss: 2.67974588e-06
Iter: 395 loss: 2.67660016e-06
Iter: 396 loss: 2.67637984e-06
Iter: 397 loss: 2.67350583e-06
Iter: 398 loss: 2.66800021e-06
Iter: 399 loss: 2.78261632e-06
Iter: 400 loss: 2.66800157e-06
Iter: 401 loss: 2.66322604e-06
Iter: 402 loss: 2.67788914e-06
Iter: 403 loss: 2.66178245e-06
Iter: 404 loss: 2.65599647e-06
Iter: 405 loss: 2.70278224e-06
Iter: 406 loss: 2.65565791e-06
Iter: 407 loss: 2.65272e-06
Iter: 408 loss: 2.64785649e-06
Iter: 409 loss: 2.64781488e-06
Iter: 410 loss: 2.64564278e-06
Iter: 411 loss: 2.6450075e-06
Iter: 412 loss: 2.6422008e-06
Iter: 413 loss: 2.63858692e-06
Iter: 414 loss: 2.63831134e-06
Iter: 415 loss: 2.63421043e-06
Iter: 416 loss: 2.62972594e-06
Iter: 417 loss: 2.62897811e-06
Iter: 418 loss: 2.62389835e-06
Iter: 419 loss: 2.623819e-06
Iter: 420 loss: 2.61958917e-06
Iter: 421 loss: 2.63798643e-06
Iter: 422 loss: 2.61868468e-06
Iter: 423 loss: 2.61438913e-06
Iter: 424 loss: 2.62804383e-06
Iter: 425 loss: 2.61314426e-06
Iter: 426 loss: 2.61007563e-06
Iter: 427 loss: 2.61273135e-06
Iter: 428 loss: 2.60838488e-06
Iter: 429 loss: 2.60432853e-06
Iter: 430 loss: 2.62925687e-06
Iter: 431 loss: 2.60374145e-06
Iter: 432 loss: 2.6013729e-06
Iter: 433 loss: 2.59633e-06
Iter: 434 loss: 2.67824862e-06
Iter: 435 loss: 2.59614058e-06
Iter: 436 loss: 2.59246917e-06
Iter: 437 loss: 2.59234594e-06
Iter: 438 loss: 2.58820955e-06
Iter: 439 loss: 2.58438649e-06
Iter: 440 loss: 2.58345199e-06
Iter: 441 loss: 2.57893e-06
Iter: 442 loss: 2.58405362e-06
Iter: 443 loss: 2.57652209e-06
Iter: 444 loss: 2.57378042e-06
Iter: 445 loss: 2.57358533e-06
Iter: 446 loss: 2.57024476e-06
Iter: 447 loss: 2.56374324e-06
Iter: 448 loss: 2.69669226e-06
Iter: 449 loss: 2.56371391e-06
Iter: 450 loss: 2.55858845e-06
Iter: 451 loss: 2.56373551e-06
Iter: 452 loss: 2.55574037e-06
Iter: 453 loss: 2.54956058e-06
Iter: 454 loss: 2.56748331e-06
Iter: 455 loss: 2.54764245e-06
Iter: 456 loss: 2.54573706e-06
Iter: 457 loss: 2.54418455e-06
Iter: 458 loss: 2.54243582e-06
Iter: 459 loss: 2.54052634e-06
Iter: 460 loss: 2.54025053e-06
Iter: 461 loss: 2.53707981e-06
Iter: 462 loss: 2.55211307e-06
Iter: 463 loss: 2.53644566e-06
Iter: 464 loss: 2.53358303e-06
Iter: 465 loss: 2.54104816e-06
Iter: 466 loss: 2.53263124e-06
Iter: 467 loss: 2.52956397e-06
Iter: 468 loss: 2.52590098e-06
Iter: 469 loss: 2.5255672e-06
Iter: 470 loss: 2.52127256e-06
Iter: 471 loss: 2.53892404e-06
Iter: 472 loss: 2.52036398e-06
Iter: 473 loss: 2.515902e-06
Iter: 474 loss: 2.56129692e-06
Iter: 475 loss: 2.51575761e-06
Iter: 476 loss: 2.5131726e-06
Iter: 477 loss: 2.50707035e-06
Iter: 478 loss: 2.57772717e-06
Iter: 479 loss: 2.50657195e-06
Iter: 480 loss: 2.50430548e-06
Iter: 481 loss: 2.50350422e-06
Iter: 482 loss: 2.50035782e-06
Iter: 483 loss: 2.5022016e-06
Iter: 484 loss: 2.49833556e-06
Iter: 485 loss: 2.49506797e-06
Iter: 486 loss: 2.48873243e-06
Iter: 487 loss: 2.60955744e-06
Iter: 488 loss: 2.48858487e-06
Iter: 489 loss: 2.48252491e-06
Iter: 490 loss: 2.50382413e-06
Iter: 491 loss: 2.48098627e-06
Iter: 492 loss: 2.47680259e-06
Iter: 493 loss: 2.47673938e-06
Iter: 494 loss: 2.47260914e-06
Iter: 495 loss: 2.48695073e-06
Iter: 496 loss: 2.47146363e-06
Iter: 497 loss: 2.46928607e-06
Iter: 498 loss: 2.46721947e-06
Iter: 499 loss: 2.46668219e-06
Iter: 500 loss: 2.46280115e-06
Iter: 501 loss: 2.50948915e-06
Iter: 502 loss: 2.46281525e-06
Iter: 503 loss: 2.46071522e-06
Iter: 504 loss: 2.455502e-06
Iter: 505 loss: 2.50550056e-06
Iter: 506 loss: 2.45481647e-06
Iter: 507 loss: 2.45029833e-06
Iter: 508 loss: 2.50206131e-06
Iter: 509 loss: 2.45018896e-06
Iter: 510 loss: 2.44636885e-06
Iter: 511 loss: 2.4631554e-06
Iter: 512 loss: 2.44561352e-06
Iter: 513 loss: 2.4426663e-06
Iter: 514 loss: 2.4680603e-06
Iter: 515 loss: 2.44250191e-06
Iter: 516 loss: 2.44012563e-06
Iter: 517 loss: 2.43711179e-06
Iter: 518 loss: 2.43689601e-06
Iter: 519 loss: 2.43404793e-06
Iter: 520 loss: 2.45329693e-06
Iter: 521 loss: 2.4337362e-06
Iter: 522 loss: 2.43004092e-06
Iter: 523 loss: 2.43718478e-06
Iter: 524 loss: 2.42861665e-06
Iter: 525 loss: 2.4258909e-06
Iter: 526 loss: 2.42110605e-06
Iter: 527 loss: 2.42109854e-06
Iter: 528 loss: 2.41724229e-06
Iter: 529 loss: 2.47065282e-06
Iter: 530 loss: 2.41724888e-06
Iter: 531 loss: 2.413517e-06
Iter: 532 loss: 2.4398496e-06
Iter: 533 loss: 2.41312409e-06
Iter: 534 loss: 2.41125872e-06
Iter: 535 loss: 2.40767235e-06
Iter: 536 loss: 2.48690685e-06
Iter: 537 loss: 2.40767213e-06
Iter: 538 loss: 2.40471763e-06
Iter: 539 loss: 2.40452209e-06
Iter: 540 loss: 2.40267855e-06
Iter: 541 loss: 2.39772316e-06
Iter: 542 loss: 2.44018565e-06
Iter: 543 loss: 2.39694691e-06
Iter: 544 loss: 2.39110477e-06
Iter: 545 loss: 2.40616714e-06
Iter: 546 loss: 2.38912412e-06
Iter: 547 loss: 2.38673124e-06
Iter: 548 loss: 2.38639177e-06
Iter: 549 loss: 2.38384973e-06
Iter: 550 loss: 2.38615962e-06
Iter: 551 loss: 2.38236839e-06
Iter: 552 loss: 2.37914082e-06
Iter: 553 loss: 2.38443272e-06
Iter: 554 loss: 2.37771474e-06
Iter: 555 loss: 2.37426229e-06
Iter: 556 loss: 2.37689574e-06
Iter: 557 loss: 2.37212112e-06
Iter: 558 loss: 2.36870937e-06
Iter: 559 loss: 2.36869892e-06
Iter: 560 loss: 2.36665755e-06
Iter: 561 loss: 2.36200913e-06
Iter: 562 loss: 2.42896704e-06
Iter: 563 loss: 2.36179767e-06
Iter: 564 loss: 2.3573798e-06
Iter: 565 loss: 2.36435426e-06
Iter: 566 loss: 2.35527136e-06
Iter: 567 loss: 2.35616562e-06
Iter: 568 loss: 2.35331299e-06
Iter: 569 loss: 2.35182529e-06
Iter: 570 loss: 2.34786194e-06
Iter: 571 loss: 2.37552285e-06
Iter: 572 loss: 2.34699746e-06
Iter: 573 loss: 2.34335084e-06
Iter: 574 loss: 2.39711312e-06
Iter: 575 loss: 2.3433895e-06
Iter: 576 loss: 2.33953779e-06
Iter: 577 loss: 2.34142044e-06
Iter: 578 loss: 2.33695164e-06
Iter: 579 loss: 2.33443e-06
Iter: 580 loss: 2.33102742e-06
Iter: 581 loss: 2.33086575e-06
Iter: 582 loss: 2.32603315e-06
Iter: 583 loss: 2.34129084e-06
Iter: 584 loss: 2.32470984e-06
Iter: 585 loss: 2.32033858e-06
Iter: 586 loss: 2.33976493e-06
Iter: 587 loss: 2.3195239e-06
Iter: 588 loss: 2.31674449e-06
Iter: 589 loss: 2.31662762e-06
Iter: 590 loss: 2.31462127e-06
Iter: 591 loss: 2.31043555e-06
Iter: 592 loss: 2.37234872e-06
Iter: 593 loss: 2.31026888e-06
Iter: 594 loss: 2.30820251e-06
Iter: 595 loss: 2.3077464e-06
Iter: 596 loss: 2.30552951e-06
Iter: 597 loss: 2.3034404e-06
Iter: 598 loss: 2.3029429e-06
Iter: 599 loss: 2.30040473e-06
Iter: 600 loss: 2.31889521e-06
Iter: 601 loss: 2.30022624e-06
Iter: 602 loss: 2.29780835e-06
Iter: 603 loss: 2.303163e-06
Iter: 604 loss: 2.29693933e-06
Iter: 605 loss: 2.29359466e-06
Iter: 606 loss: 2.29208194e-06
Iter: 607 loss: 2.29035e-06
Iter: 608 loss: 2.28706381e-06
Iter: 609 loss: 2.29303851e-06
Iter: 610 loss: 2.28566432e-06
Iter: 611 loss: 2.28227782e-06
Iter: 612 loss: 2.32847265e-06
Iter: 613 loss: 2.28225804e-06
Iter: 614 loss: 2.28040017e-06
Iter: 615 loss: 2.27619512e-06
Iter: 616 loss: 2.33825267e-06
Iter: 617 loss: 2.27603482e-06
Iter: 618 loss: 2.27216765e-06
Iter: 619 loss: 2.27994269e-06
Iter: 620 loss: 2.27060173e-06
Iter: 621 loss: 2.26607108e-06
Iter: 622 loss: 2.2755471e-06
Iter: 623 loss: 2.26423708e-06
Iter: 624 loss: 2.26251814e-06
Iter: 625 loss: 2.26184738e-06
Iter: 626 loss: 2.2594138e-06
Iter: 627 loss: 2.25778967e-06
Iter: 628 loss: 2.25688404e-06
Iter: 629 loss: 2.25474309e-06
Iter: 630 loss: 2.2672416e-06
Iter: 631 loss: 2.25449594e-06
Iter: 632 loss: 2.25180474e-06
Iter: 633 loss: 2.2535994e-06
Iter: 634 loss: 2.25002168e-06
Iter: 635 loss: 2.24739756e-06
Iter: 636 loss: 2.24523887e-06
Iter: 637 loss: 2.24440942e-06
Iter: 638 loss: 2.24305086e-06
Iter: 639 loss: 2.24224323e-06
Iter: 640 loss: 2.24049154e-06
Iter: 641 loss: 2.23738789e-06
Iter: 642 loss: 2.2374029e-06
Iter: 643 loss: 2.23372604e-06
Iter: 644 loss: 2.25007807e-06
Iter: 645 loss: 2.23301959e-06
Iter: 646 loss: 2.23057941e-06
Iter: 647 loss: 2.24713358e-06
Iter: 648 loss: 2.23031884e-06
Iter: 649 loss: 2.22793142e-06
Iter: 650 loss: 2.23294842e-06
Iter: 651 loss: 2.22691187e-06
Iter: 652 loss: 2.22505696e-06
Iter: 653 loss: 2.22004564e-06
Iter: 654 loss: 2.26031534e-06
Iter: 655 loss: 2.2191316e-06
Iter: 656 loss: 2.21372602e-06
Iter: 657 loss: 2.25620306e-06
Iter: 658 loss: 2.21328355e-06
Iter: 659 loss: 2.21151e-06
Iter: 660 loss: 2.21108439e-06
Iter: 661 loss: 2.2088584e-06
Iter: 662 loss: 2.20664788e-06
Iter: 663 loss: 2.20613538e-06
Iter: 664 loss: 2.20387437e-06
Iter: 665 loss: 2.21639903e-06
Iter: 666 loss: 2.20354195e-06
Iter: 667 loss: 2.20091738e-06
Iter: 668 loss: 2.20888251e-06
Iter: 669 loss: 2.20014044e-06
Iter: 670 loss: 2.19811386e-06
Iter: 671 loss: 2.19338949e-06
Iter: 672 loss: 2.24973383e-06
Iter: 673 loss: 2.19301137e-06
Iter: 674 loss: 2.19349158e-06
Iter: 675 loss: 2.19077901e-06
Iter: 676 loss: 2.18883406e-06
Iter: 677 loss: 2.18632749e-06
Iter: 678 loss: 2.18618948e-06
Iter: 679 loss: 2.18373725e-06
Iter: 680 loss: 2.19563071e-06
Iter: 681 loss: 2.18329615e-06
Iter: 682 loss: 2.18074388e-06
Iter: 683 loss: 2.18831156e-06
Iter: 684 loss: 2.18000355e-06
Iter: 685 loss: 2.17782417e-06
Iter: 686 loss: 2.1851713e-06
Iter: 687 loss: 2.17712454e-06
Iter: 688 loss: 2.17492766e-06
Iter: 689 loss: 2.17119282e-06
Iter: 690 loss: 2.17117213e-06
Iter: 691 loss: 2.16672424e-06
Iter: 692 loss: 2.17482e-06
Iter: 693 loss: 2.16485796e-06
Iter: 694 loss: 2.16007311e-06
Iter: 695 loss: 2.17275738e-06
Iter: 696 loss: 2.15853402e-06
Iter: 697 loss: 2.15503178e-06
Iter: 698 loss: 2.17671641e-06
Iter: 699 loss: 2.15463774e-06
Iter: 700 loss: 2.15219188e-06
Iter: 701 loss: 2.15215414e-06
Iter: 702 loss: 2.15033742e-06
Iter: 703 loss: 2.14720922e-06
Iter: 704 loss: 2.14721922e-06
Iter: 705 loss: 2.14553484e-06
Iter: 706 loss: 2.14548459e-06
Iter: 707 loss: 2.14351758e-06
Iter: 708 loss: 2.13980684e-06
Iter: 709 loss: 2.21656092e-06
Iter: 710 loss: 2.13979592e-06
Iter: 711 loss: 2.13645694e-06
Iter: 712 loss: 2.14257511e-06
Iter: 713 loss: 2.13496969e-06
Iter: 714 loss: 2.13329326e-06
Iter: 715 loss: 2.13309954e-06
Iter: 716 loss: 2.13109684e-06
Iter: 717 loss: 2.12939949e-06
Iter: 718 loss: 2.12888267e-06
Iter: 719 loss: 2.12655209e-06
Iter: 720 loss: 2.13766293e-06
Iter: 721 loss: 2.12615487e-06
Iter: 722 loss: 2.12369105e-06
Iter: 723 loss: 2.12961686e-06
Iter: 724 loss: 2.1228193e-06
Iter: 725 loss: 2.12042528e-06
Iter: 726 loss: 2.12421719e-06
Iter: 727 loss: 2.11932252e-06
Iter: 728 loss: 2.11661381e-06
Iter: 729 loss: 2.11354859e-06
Iter: 730 loss: 2.11319184e-06
Iter: 731 loss: 2.10955159e-06
Iter: 732 loss: 2.11614224e-06
Iter: 733 loss: 2.10795315e-06
Iter: 734 loss: 2.10380767e-06
Iter: 735 loss: 2.12299847e-06
Iter: 736 loss: 2.10297412e-06
Iter: 737 loss: 2.09941027e-06
Iter: 738 loss: 2.09940754e-06
Iter: 739 loss: 2.09785321e-06
Iter: 740 loss: 2.09418795e-06
Iter: 741 loss: 2.13700037e-06
Iter: 742 loss: 2.09395694e-06
Iter: 743 loss: 2.09246787e-06
Iter: 744 loss: 2.09160862e-06
Iter: 745 loss: 2.09012751e-06
Iter: 746 loss: 2.08677e-06
Iter: 747 loss: 2.13191834e-06
Iter: 748 loss: 2.08658298e-06
Iter: 749 loss: 2.08336814e-06
Iter: 750 loss: 2.08567462e-06
Iter: 751 loss: 2.08135589e-06
Iter: 752 loss: 2.07975e-06
Iter: 753 loss: 2.07922767e-06
Iter: 754 loss: 2.07684138e-06
Iter: 755 loss: 2.0731286e-06
Iter: 756 loss: 2.07305698e-06
Iter: 757 loss: 2.07113908e-06
Iter: 758 loss: 2.07107155e-06
Iter: 759 loss: 2.06929167e-06
Iter: 760 loss: 2.06676259e-06
Iter: 761 loss: 2.06663572e-06
Iter: 762 loss: 2.06377376e-06
Iter: 763 loss: 2.07404469e-06
Iter: 764 loss: 2.06300956e-06
Iter: 765 loss: 2.06041796e-06
Iter: 766 loss: 2.06718732e-06
Iter: 767 loss: 2.05952028e-06
Iter: 768 loss: 2.0567943e-06
Iter: 769 loss: 2.05659217e-06
Iter: 770 loss: 2.05456149e-06
Iter: 771 loss: 2.05266497e-06
Iter: 772 loss: 2.05221158e-06
Iter: 773 loss: 2.05064975e-06
Iter: 774 loss: 2.04712774e-06
Iter: 775 loss: 2.09376458e-06
Iter: 776 loss: 2.04694311e-06
Iter: 777 loss: 2.04532307e-06
Iter: 778 loss: 2.04496246e-06
Iter: 779 loss: 2.0430216e-06
Iter: 780 loss: 2.03952891e-06
Iter: 781 loss: 2.12380246e-06
Iter: 782 loss: 2.03955096e-06
Iter: 783 loss: 2.03607351e-06
Iter: 784 loss: 2.0364173e-06
Iter: 785 loss: 2.03343416e-06
Iter: 786 loss: 2.03312356e-06
Iter: 787 loss: 2.03132458e-06
Iter: 788 loss: 2.02956835e-06
Iter: 789 loss: 2.02616911e-06
Iter: 790 loss: 2.10048893e-06
Iter: 791 loss: 2.02615456e-06
Iter: 792 loss: 2.02413094e-06
Iter: 793 loss: 2.02414412e-06
Iter: 794 loss: 2.02194951e-06
Iter: 795 loss: 2.02089382e-06
Iter: 796 loss: 2.01984949e-06
Iter: 797 loss: 2.01763351e-06
Iter: 798 loss: 2.02227693e-06
Iter: 799 loss: 2.01673174e-06
Iter: 800 loss: 2.01417151e-06
Iter: 801 loss: 2.01942953e-06
Iter: 802 loss: 2.01313878e-06
Iter: 803 loss: 2.01038961e-06
Iter: 804 loss: 2.01065927e-06
Iter: 805 loss: 2.00821614e-06
Iter: 806 loss: 2.0057812e-06
Iter: 807 loss: 2.00552859e-06
Iter: 808 loss: 2.00408113e-06
Iter: 809 loss: 2.00094428e-06
Iter: 810 loss: 2.04772687e-06
Iter: 811 loss: 2.00081513e-06
Iter: 812 loss: 1.99892202e-06
Iter: 813 loss: 1.99878787e-06
Iter: 814 loss: 1.99675628e-06
Iter: 815 loss: 1.99453666e-06
Iter: 816 loss: 1.99416331e-06
Iter: 817 loss: 1.99176907e-06
Iter: 818 loss: 1.99221085e-06
Iter: 819 loss: 1.98999987e-06
Iter: 820 loss: 1.98846419e-06
Iter: 821 loss: 1.98802286e-06
Iter: 822 loss: 1.98639373e-06
Iter: 823 loss: 1.98307703e-06
Iter: 824 loss: 2.04343223e-06
Iter: 825 loss: 1.98302246e-06
Iter: 826 loss: 1.9811564e-06
Iter: 827 loss: 1.98106682e-06
Iter: 828 loss: 1.9791803e-06
Iter: 829 loss: 1.97853069e-06
Iter: 830 loss: 1.97742042e-06
Iter: 831 loss: 1.9754707e-06
Iter: 832 loss: 1.97395548e-06
Iter: 833 loss: 1.97336476e-06
Iter: 834 loss: 1.96997053e-06
Iter: 835 loss: 1.9944182e-06
Iter: 836 loss: 1.96963674e-06
Iter: 837 loss: 1.96733163e-06
Iter: 838 loss: 1.97352301e-06
Iter: 839 loss: 1.96654423e-06
Iter: 840 loss: 1.96362748e-06
Iter: 841 loss: 1.97946383e-06
Iter: 842 loss: 1.96315113e-06
Iter: 843 loss: 1.96136125e-06
Iter: 844 loss: 1.95873372e-06
Iter: 845 loss: 1.95863186e-06
Iter: 846 loss: 1.9579461e-06
Iter: 847 loss: 1.95713392e-06
Iter: 848 loss: 1.95611233e-06
Iter: 849 loss: 1.95356552e-06
Iter: 850 loss: 1.98295129e-06
Iter: 851 loss: 1.95334746e-06
Iter: 852 loss: 1.95076859e-06
Iter: 853 loss: 1.96249266e-06
Iter: 854 loss: 1.95023313e-06
Iter: 855 loss: 1.94766699e-06
Iter: 856 loss: 1.98078783e-06
Iter: 857 loss: 1.94763788e-06
Iter: 858 loss: 1.94652216e-06
Iter: 859 loss: 1.94415929e-06
Iter: 860 loss: 1.9874019e-06
Iter: 861 loss: 1.94416202e-06
Iter: 862 loss: 1.9416168e-06
Iter: 863 loss: 1.98157204e-06
Iter: 864 loss: 1.94163295e-06
Iter: 865 loss: 1.94006043e-06
Iter: 866 loss: 1.93704159e-06
Iter: 867 loss: 1.99890428e-06
Iter: 868 loss: 1.93702272e-06
Iter: 869 loss: 1.93422466e-06
Iter: 870 loss: 1.93979849e-06
Iter: 871 loss: 1.93310598e-06
Iter: 872 loss: 1.93015853e-06
Iter: 873 loss: 1.95269126e-06
Iter: 874 loss: 1.92990728e-06
Iter: 875 loss: 1.9273175e-06
Iter: 876 loss: 1.93021515e-06
Iter: 877 loss: 1.92592597e-06
Iter: 878 loss: 1.92419589e-06
Iter: 879 loss: 1.92404787e-06
Iter: 880 loss: 1.92259176e-06
Iter: 881 loss: 1.91973845e-06
Iter: 882 loss: 1.97448981e-06
Iter: 883 loss: 1.91970776e-06
Iter: 884 loss: 1.91729941e-06
Iter: 885 loss: 1.94022505e-06
Iter: 886 loss: 1.91723166e-06
Iter: 887 loss: 1.91461208e-06
Iter: 888 loss: 1.91983418e-06
Iter: 889 loss: 1.91355639e-06
Iter: 890 loss: 1.91192748e-06
Iter: 891 loss: 1.90959463e-06
Iter: 892 loss: 1.90954506e-06
Iter: 893 loss: 1.90852597e-06
Iter: 894 loss: 1.90773858e-06
Iter: 895 loss: 1.9064928e-06
Iter: 896 loss: 1.90344792e-06
Iter: 897 loss: 1.93600681e-06
Iter: 898 loss: 1.90314677e-06
Iter: 899 loss: 1.90128651e-06
Iter: 900 loss: 1.9013105e-06
Iter: 901 loss: 1.8992273e-06
Iter: 902 loss: 1.90065839e-06
Iter: 903 loss: 1.89798448e-06
Iter: 904 loss: 1.89614491e-06
Iter: 905 loss: 1.89440402e-06
Iter: 906 loss: 1.89392199e-06
Iter: 907 loss: 1.89144305e-06
Iter: 908 loss: 1.90836886e-06
Iter: 909 loss: 1.89122329e-06
Iter: 910 loss: 1.88856052e-06
Iter: 911 loss: 1.88688853e-06
Iter: 912 loss: 1.88586591e-06
Iter: 913 loss: 1.88376714e-06
Iter: 914 loss: 1.88362321e-06
Iter: 915 loss: 1.88170907e-06
Iter: 916 loss: 1.88350725e-06
Iter: 917 loss: 1.8804858e-06
Iter: 918 loss: 1.87864794e-06
Iter: 919 loss: 1.8759863e-06
Iter: 920 loss: 1.87591809e-06
Iter: 921 loss: 1.87501882e-06
Iter: 922 loss: 1.87412297e-06
Iter: 923 loss: 1.87281648e-06
Iter: 924 loss: 1.86991326e-06
Iter: 925 loss: 1.90894434e-06
Iter: 926 loss: 1.86974e-06
Iter: 927 loss: 1.86670604e-06
Iter: 928 loss: 1.8755552e-06
Iter: 929 loss: 1.86576017e-06
Iter: 930 loss: 1.86450893e-06
Iter: 931 loss: 1.86398756e-06
Iter: 932 loss: 1.86302304e-06
Iter: 933 loss: 1.86064983e-06
Iter: 934 loss: 1.88175352e-06
Iter: 935 loss: 1.86027046e-06
Iter: 936 loss: 1.85864451e-06
Iter: 937 loss: 1.85865042e-06
Iter: 938 loss: 1.85681438e-06
Iter: 939 loss: 1.85489068e-06
Iter: 940 loss: 1.85453473e-06
Iter: 941 loss: 1.85195836e-06
Iter: 942 loss: 1.85284216e-06
Iter: 943 loss: 1.85016256e-06
Iter: 944 loss: 1.84713986e-06
Iter: 945 loss: 1.88413469e-06
Iter: 946 loss: 1.84716e-06
Iter: 947 loss: 1.84530563e-06
Iter: 948 loss: 1.85339923e-06
Iter: 949 loss: 1.84489977e-06
Iter: 950 loss: 1.84285773e-06
Iter: 951 loss: 1.84507576e-06
Iter: 952 loss: 1.84176611e-06
Iter: 953 loss: 1.83990596e-06
Iter: 954 loss: 1.8384826e-06
Iter: 955 loss: 1.83785119e-06
Iter: 956 loss: 1.83692384e-06
Iter: 957 loss: 1.83650013e-06
Iter: 958 loss: 1.8351418e-06
Iter: 959 loss: 1.832e-06
Iter: 960 loss: 1.86646207e-06
Iter: 961 loss: 1.83172369e-06
Iter: 962 loss: 1.82870508e-06
Iter: 963 loss: 1.84419332e-06
Iter: 964 loss: 1.82823055e-06
Iter: 965 loss: 1.82665349e-06
Iter: 966 loss: 1.82645613e-06
Iter: 967 loss: 1.82554368e-06
Iter: 968 loss: 1.82306871e-06
Iter: 969 loss: 1.83792906e-06
Iter: 970 loss: 1.82233509e-06
Iter: 971 loss: 1.82091094e-06
Iter: 972 loss: 1.82066822e-06
Iter: 973 loss: 1.81888743e-06
Iter: 974 loss: 1.818236e-06
Iter: 975 loss: 1.81732821e-06
Iter: 976 loss: 1.81549899e-06
Iter: 977 loss: 1.81515054e-06
Iter: 978 loss: 1.81393034e-06
Iter: 979 loss: 1.81148505e-06
Iter: 980 loss: 1.84210421e-06
Iter: 981 loss: 1.81145833e-06
Iter: 982 loss: 1.80983807e-06
Iter: 983 loss: 1.8182202e-06
Iter: 984 loss: 1.80959353e-06
Iter: 985 loss: 1.80818802e-06
Iter: 986 loss: 1.80742109e-06
Iter: 987 loss: 1.80677648e-06
Iter: 988 loss: 1.80484972e-06
Iter: 989 loss: 1.80440088e-06
Iter: 990 loss: 1.80314169e-06
Iter: 991 loss: 1.80240841e-06
Iter: 992 loss: 1.80188204e-06
Iter: 993 loss: 1.80070117e-06
Iter: 994 loss: 1.79823405e-06
Iter: 995 loss: 1.83402415e-06
Iter: 996 loss: 1.79811821e-06
Iter: 997 loss: 1.79583787e-06
Iter: 998 loss: 1.8084645e-06
Iter: 999 loss: 1.7954718e-06
Iter: 1000 loss: 1.79292749e-06
Iter: 1001 loss: 1.8131708e-06
Iter: 1002 loss: 1.79276583e-06
Iter: 1003 loss: 1.79165795e-06
Iter: 1004 loss: 1.78932123e-06
Iter: 1005 loss: 1.82729138e-06
Iter: 1006 loss: 1.78924597e-06
Iter: 1007 loss: 1.78849893e-06
Iter: 1008 loss: 1.78797779e-06
Iter: 1009 loss: 1.78682956e-06
Iter: 1010 loss: 1.78438324e-06
Iter: 1011 loss: 1.82381848e-06
Iter: 1012 loss: 1.78429673e-06
Iter: 1013 loss: 1.78195535e-06
Iter: 1014 loss: 1.79484152e-06
Iter: 1015 loss: 1.7816991e-06
Iter: 1016 loss: 1.77959259e-06
Iter: 1017 loss: 1.79923109e-06
Iter: 1018 loss: 1.77946913e-06
Iter: 1019 loss: 1.77799245e-06
Iter: 1020 loss: 1.78074174e-06
Iter: 1021 loss: 1.77732613e-06
Iter: 1022 loss: 1.77571451e-06
Iter: 1023 loss: 1.77438164e-06
Iter: 1024 loss: 1.77391689e-06
Iter: 1025 loss: 1.77204583e-06
Iter: 1026 loss: 1.78595849e-06
Iter: 1027 loss: 1.77183142e-06
Iter: 1028 loss: 1.77011907e-06
Iter: 1029 loss: 1.78509163e-06
Iter: 1030 loss: 1.77005177e-06
Iter: 1031 loss: 1.76892081e-06
Iter: 1032 loss: 1.76619028e-06
Iter: 1033 loss: 1.7957575e-06
Iter: 1034 loss: 1.76592698e-06
Iter: 1035 loss: 1.76557614e-06
Iter: 1036 loss: 1.76452249e-06
Iter: 1037 loss: 1.76333015e-06
Iter: 1038 loss: 1.76070546e-06
Iter: 1039 loss: 1.80801715e-06
Iter: 1040 loss: 1.76068738e-06
Iter: 1041 loss: 1.7586317e-06
Iter: 1042 loss: 1.76495621e-06
Iter: 1043 loss: 1.75798436e-06
Iter: 1044 loss: 1.75587354e-06
Iter: 1045 loss: 1.7813577e-06
Iter: 1046 loss: 1.75582886e-06
Iter: 1047 loss: 1.7546364e-06
Iter: 1048 loss: 1.75229525e-06
Iter: 1049 loss: 1.79700828e-06
Iter: 1050 loss: 1.75222829e-06
Iter: 1051 loss: 1.75049058e-06
Iter: 1052 loss: 1.75048069e-06
Iter: 1053 loss: 1.74863135e-06
Iter: 1054 loss: 1.75082721e-06
Iter: 1055 loss: 1.74768184e-06
Iter: 1056 loss: 1.74552667e-06
Iter: 1057 loss: 1.74812328e-06
Iter: 1058 loss: 1.74439629e-06
Iter: 1059 loss: 1.74242109e-06
Iter: 1060 loss: 1.74213687e-06
Iter: 1061 loss: 1.74076536e-06
Iter: 1062 loss: 1.73870524e-06
Iter: 1063 loss: 1.75512525e-06
Iter: 1064 loss: 1.73860599e-06
Iter: 1065 loss: 1.73649846e-06
Iter: 1066 loss: 1.74813363e-06
Iter: 1067 loss: 1.73621038e-06
Iter: 1068 loss: 1.73479771e-06
Iter: 1069 loss: 1.73244507e-06
Iter: 1070 loss: 1.73244132e-06
Iter: 1071 loss: 1.73211902e-06
Iter: 1072 loss: 1.73119815e-06
Iter: 1073 loss: 1.73019771e-06
Iter: 1074 loss: 1.72764067e-06
Iter: 1075 loss: 1.7462944e-06
Iter: 1076 loss: 1.72710577e-06
Iter: 1077 loss: 1.72464911e-06
Iter: 1078 loss: 1.74473314e-06
Iter: 1079 loss: 1.72445903e-06
Iter: 1080 loss: 1.72236571e-06
Iter: 1081 loss: 1.74205616e-06
Iter: 1082 loss: 1.7223e-06
Iter: 1083 loss: 1.72122827e-06
Iter: 1084 loss: 1.71890747e-06
Iter: 1085 loss: 1.7535308e-06
Iter: 1086 loss: 1.71878798e-06
Iter: 1087 loss: 1.71682655e-06
Iter: 1088 loss: 1.74166496e-06
Iter: 1089 loss: 1.71679119e-06
Iter: 1090 loss: 1.71471788e-06
Iter: 1091 loss: 1.72191562e-06
Iter: 1092 loss: 1.71418219e-06
Iter: 1093 loss: 1.71256761e-06
Iter: 1094 loss: 1.71399586e-06
Iter: 1095 loss: 1.71169609e-06
Iter: 1096 loss: 1.70975557e-06
Iter: 1097 loss: 1.71006513e-06
Iter: 1098 loss: 1.70827536e-06
Iter: 1099 loss: 1.70609019e-06
Iter: 1100 loss: 1.71435795e-06
Iter: 1101 loss: 1.70559383e-06
Iter: 1102 loss: 1.70335863e-06
Iter: 1103 loss: 1.72964883e-06
Iter: 1104 loss: 1.70334158e-06
Iter: 1105 loss: 1.70223393e-06
Iter: 1106 loss: 1.69997224e-06
Iter: 1107 loss: 1.74137062e-06
Iter: 1108 loss: 1.69993837e-06
Iter: 1109 loss: 1.69942e-06
Iter: 1110 loss: 1.69865734e-06
Iter: 1111 loss: 1.69772829e-06
Iter: 1112 loss: 1.69556654e-06
Iter: 1113 loss: 1.72363957e-06
Iter: 1114 loss: 1.6953303e-06
Iter: 1115 loss: 1.69352916e-06
Iter: 1116 loss: 1.704243e-06
Iter: 1117 loss: 1.69328371e-06
Iter: 1118 loss: 1.69129521e-06
Iter: 1119 loss: 1.70212991e-06
Iter: 1120 loss: 1.69095779e-06
Iter: 1121 loss: 1.68975589e-06
Iter: 1122 loss: 1.68753172e-06
Iter: 1123 loss: 1.73955368e-06
Iter: 1124 loss: 1.68754912e-06
Iter: 1125 loss: 1.68538418e-06
Iter: 1126 loss: 1.70326598e-06
Iter: 1127 loss: 1.68526356e-06
Iter: 1128 loss: 1.68307429e-06
Iter: 1129 loss: 1.69549025e-06
Iter: 1130 loss: 1.68277927e-06
Iter: 1131 loss: 1.68124632e-06
Iter: 1132 loss: 1.68042845e-06
Iter: 1133 loss: 1.67966675e-06
Iter: 1134 loss: 1.67751159e-06
Iter: 1135 loss: 1.68495433e-06
Iter: 1136 loss: 1.6769568e-06
Iter: 1137 loss: 1.67512439e-06
Iter: 1138 loss: 1.67672385e-06
Iter: 1139 loss: 1.67404426e-06
Iter: 1140 loss: 1.67225198e-06
Iter: 1141 loss: 1.67224812e-06
Iter: 1142 loss: 1.67105725e-06
Iter: 1143 loss: 1.66845621e-06
Iter: 1144 loss: 1.70489056e-06
Iter: 1145 loss: 1.66836969e-06
Iter: 1146 loss: 1.66798327e-06
Iter: 1147 loss: 1.66718723e-06
Iter: 1148 loss: 1.66598443e-06
Iter: 1149 loss: 1.663033e-06
Iter: 1150 loss: 1.69205305e-06
Iter: 1151 loss: 1.66266568e-06
Iter: 1152 loss: 1.66011955e-06
Iter: 1153 loss: 1.67529015e-06
Iter: 1154 loss: 1.65976837e-06
Iter: 1155 loss: 1.65817482e-06
Iter: 1156 loss: 1.65817994e-06
Iter: 1157 loss: 1.65730319e-06
Iter: 1158 loss: 1.65496715e-06
Iter: 1159 loss: 1.67103872e-06
Iter: 1160 loss: 1.65441759e-06
Iter: 1161 loss: 1.65230529e-06
Iter: 1162 loss: 1.68553106e-06
Iter: 1163 loss: 1.65228153e-06
Iter: 1164 loss: 1.65031179e-06
Iter: 1165 loss: 1.65969072e-06
Iter: 1166 loss: 1.64994844e-06
Iter: 1167 loss: 1.64850564e-06
Iter: 1168 loss: 1.64750713e-06
Iter: 1169 loss: 1.64699986e-06
Iter: 1170 loss: 1.64463779e-06
Iter: 1171 loss: 1.65431595e-06
Iter: 1172 loss: 1.64417656e-06
Iter: 1173 loss: 1.64231255e-06
Iter: 1174 loss: 1.64716766e-06
Iter: 1175 loss: 1.6417215e-06
Iter: 1176 loss: 1.63975392e-06
Iter: 1177 loss: 1.65645599e-06
Iter: 1178 loss: 1.63963489e-06
Iter: 1179 loss: 1.63851814e-06
Iter: 1180 loss: 1.63615107e-06
Iter: 1181 loss: 1.6797344e-06
Iter: 1182 loss: 1.63611969e-06
Iter: 1183 loss: 1.63538618e-06
Iter: 1184 loss: 1.6347152e-06
Iter: 1185 loss: 1.63367258e-06
Iter: 1186 loss: 1.63145637e-06
Iter: 1187 loss: 1.662964e-06
Iter: 1188 loss: 1.63131801e-06
Iter: 1189 loss: 1.62974857e-06
Iter: 1190 loss: 1.64331459e-06
Iter: 1191 loss: 1.62969627e-06
Iter: 1192 loss: 1.6276806e-06
Iter: 1193 loss: 1.62848914e-06
Iter: 1194 loss: 1.62630658e-06
Iter: 1195 loss: 1.6248265e-06
Iter: 1196 loss: 1.62341348e-06
Iter: 1197 loss: 1.62312313e-06
Iter: 1198 loss: 1.62059064e-06
Iter: 1199 loss: 1.6250865e-06
Iter: 1200 loss: 1.61944388e-06
Iter: 1201 loss: 1.61877972e-06
Iter: 1202 loss: 1.6178808e-06
Iter: 1203 loss: 1.6171316e-06
Iter: 1204 loss: 1.61516903e-06
Iter: 1205 loss: 1.63006075e-06
Iter: 1206 loss: 1.61481148e-06
Iter: 1207 loss: 1.61240973e-06
Iter: 1208 loss: 1.61677917e-06
Iter: 1209 loss: 1.61136813e-06
Iter: 1210 loss: 1.61037531e-06
Iter: 1211 loss: 1.60990317e-06
Iter: 1212 loss: 1.60883951e-06
Iter: 1213 loss: 1.60727768e-06
Iter: 1214 loss: 1.60724812e-06
Iter: 1215 loss: 1.6055958e-06
Iter: 1216 loss: 1.61897174e-06
Iter: 1217 loss: 1.60550519e-06
Iter: 1218 loss: 1.60396348e-06
Iter: 1219 loss: 1.6086924e-06
Iter: 1220 loss: 1.60354659e-06
Iter: 1221 loss: 1.60197146e-06
Iter: 1222 loss: 1.60300692e-06
Iter: 1223 loss: 1.60099353e-06
Iter: 1224 loss: 1.59968044e-06
Iter: 1225 loss: 1.59864794e-06
Iter: 1226 loss: 1.59823708e-06
Iter: 1227 loss: 1.59635238e-06
Iter: 1228 loss: 1.61240416e-06
Iter: 1229 loss: 1.59626495e-06
Iter: 1230 loss: 1.59398621e-06
Iter: 1231 loss: 1.59865647e-06
Iter: 1232 loss: 1.5930849e-06
Iter: 1233 loss: 1.59174112e-06
Iter: 1234 loss: 1.59016167e-06
Iter: 1235 loss: 1.5900082e-06
Iter: 1236 loss: 1.58757211e-06
Iter: 1237 loss: 1.58961234e-06
Iter: 1238 loss: 1.58610283e-06
Iter: 1239 loss: 1.58439059e-06
Iter: 1240 loss: 1.58435478e-06
Iter: 1241 loss: 1.58267198e-06
Iter: 1242 loss: 1.59279602e-06
Iter: 1243 loss: 1.5824553e-06
Iter: 1244 loss: 1.58136265e-06
Iter: 1245 loss: 1.57906902e-06
Iter: 1246 loss: 1.61312e-06
Iter: 1247 loss: 1.57898614e-06
Iter: 1248 loss: 1.57809825e-06
Iter: 1249 loss: 1.5775795e-06
Iter: 1250 loss: 1.57649254e-06
Iter: 1251 loss: 1.5749356e-06
Iter: 1252 loss: 1.57489262e-06
Iter: 1253 loss: 1.57349837e-06
Iter: 1254 loss: 1.58782086e-06
Iter: 1255 loss: 1.57347677e-06
Iter: 1256 loss: 1.57217028e-06
Iter: 1257 loss: 1.5759357e-06
Iter: 1258 loss: 1.57178636e-06
Iter: 1259 loss: 1.57040597e-06
Iter: 1260 loss: 1.57092518e-06
Iter: 1261 loss: 1.56942076e-06
Iter: 1262 loss: 1.56798546e-06
Iter: 1263 loss: 1.56726833e-06
Iter: 1264 loss: 1.56654301e-06
Iter: 1265 loss: 1.56560736e-06
Iter: 1266 loss: 1.56542364e-06
Iter: 1267 loss: 1.56433487e-06
Iter: 1268 loss: 1.5626232e-06
Iter: 1269 loss: 1.56260467e-06
Iter: 1270 loss: 1.5608498e-06
Iter: 1271 loss: 1.56131989e-06
Iter: 1272 loss: 1.5596313e-06
Iter: 1273 loss: 1.55813791e-06
Iter: 1274 loss: 1.55811563e-06
Iter: 1275 loss: 1.55641931e-06
Iter: 1276 loss: 1.55945611e-06
Iter: 1277 loss: 1.55568125e-06
Iter: 1278 loss: 1.55434293e-06
Iter: 1279 loss: 1.55289899e-06
Iter: 1280 loss: 1.5527055e-06
Iter: 1281 loss: 1.55174178e-06
Iter: 1282 loss: 1.55141095e-06
Iter: 1283 loss: 1.55057523e-06
Iter: 1284 loss: 1.54879069e-06
Iter: 1285 loss: 1.57692602e-06
Iter: 1286 loss: 1.54873192e-06
Iter: 1287 loss: 1.54738086e-06
Iter: 1288 loss: 1.54737222e-06
Iter: 1289 loss: 1.5460696e-06
Iter: 1290 loss: 1.54621739e-06
Iter: 1291 loss: 1.54502743e-06
Iter: 1292 loss: 1.54343e-06
Iter: 1293 loss: 1.54651798e-06
Iter: 1294 loss: 1.54272209e-06
Iter: 1295 loss: 1.54117924e-06
Iter: 1296 loss: 1.54025497e-06
Iter: 1297 loss: 1.53958752e-06
Iter: 1298 loss: 1.53857093e-06
Iter: 1299 loss: 1.53828523e-06
Iter: 1300 loss: 1.53720589e-06
Iter: 1301 loss: 1.5351909e-06
Iter: 1302 loss: 1.58158343e-06
Iter: 1303 loss: 1.53518909e-06
Iter: 1304 loss: 1.53347344e-06
Iter: 1305 loss: 1.5351095e-06
Iter: 1306 loss: 1.53246287e-06
Iter: 1307 loss: 1.53157e-06
Iter: 1308 loss: 1.53133806e-06
Iter: 1309 loss: 1.53027702e-06
Iter: 1310 loss: 1.52860207e-06
Iter: 1311 loss: 1.52854511e-06
Iter: 1312 loss: 1.52708958e-06
Iter: 1313 loss: 1.53121141e-06
Iter: 1314 loss: 1.52659823e-06
Iter: 1315 loss: 1.52507027e-06
Iter: 1316 loss: 1.54444399e-06
Iter: 1317 loss: 1.52504936e-06
Iter: 1318 loss: 1.52415839e-06
Iter: 1319 loss: 1.52265466e-06
Iter: 1320 loss: 1.52266989e-06
Iter: 1321 loss: 1.52184384e-06
Iter: 1322 loss: 1.52158179e-06
Iter: 1323 loss: 1.52085147e-06
Iter: 1324 loss: 1.51917129e-06
Iter: 1325 loss: 1.54312647e-06
Iter: 1326 loss: 1.51908375e-06
Iter: 1327 loss: 1.51700101e-06
Iter: 1328 loss: 1.52763664e-06
Iter: 1329 loss: 1.51669519e-06
Iter: 1330 loss: 1.51502638e-06
Iter: 1331 loss: 1.51696702e-06
Iter: 1332 loss: 1.5141153e-06
Iter: 1333 loss: 1.51267568e-06
Iter: 1334 loss: 1.51263703e-06
Iter: 1335 loss: 1.51169638e-06
Iter: 1336 loss: 1.50952542e-06
Iter: 1337 loss: 1.54268446e-06
Iter: 1338 loss: 1.509453e-06
Iter: 1339 loss: 1.50728624e-06
Iter: 1340 loss: 1.5106325e-06
Iter: 1341 loss: 1.50628921e-06
Iter: 1342 loss: 1.50502547e-06
Iter: 1343 loss: 1.50495669e-06
Iter: 1344 loss: 1.50334654e-06
Iter: 1345 loss: 1.50225219e-06
Iter: 1346 loss: 1.50164328e-06
Iter: 1347 loss: 1.49995822e-06
Iter: 1348 loss: 1.49918344e-06
Iter: 1349 loss: 1.49834125e-06
Iter: 1350 loss: 1.49741868e-06
Iter: 1351 loss: 1.49706216e-06
Iter: 1352 loss: 1.49586856e-06
Iter: 1353 loss: 1.49491939e-06
Iter: 1354 loss: 1.49452615e-06
Iter: 1355 loss: 1.49352513e-06
Iter: 1356 loss: 1.50587834e-06
Iter: 1357 loss: 1.49348352e-06
Iter: 1358 loss: 1.49241828e-06
Iter: 1359 loss: 1.49256903e-06
Iter: 1360 loss: 1.49157245e-06
Iter: 1361 loss: 1.49047855e-06
Iter: 1362 loss: 1.48968093e-06
Iter: 1363 loss: 1.48927666e-06
Iter: 1364 loss: 1.48717072e-06
Iter: 1365 loss: 1.49714197e-06
Iter: 1366 loss: 1.48684671e-06
Iter: 1367 loss: 1.48574327e-06
Iter: 1368 loss: 1.50244136e-06
Iter: 1369 loss: 1.48573395e-06
Iter: 1370 loss: 1.48473327e-06
Iter: 1371 loss: 1.48328627e-06
Iter: 1372 loss: 1.4832292e-06
Iter: 1373 loss: 1.48156437e-06
Iter: 1374 loss: 1.48043466e-06
Iter: 1375 loss: 1.4797813e-06
Iter: 1376 loss: 1.47798187e-06
Iter: 1377 loss: 1.50408425e-06
Iter: 1378 loss: 1.47797709e-06
Iter: 1379 loss: 1.47637297e-06
Iter: 1380 loss: 1.49044774e-06
Iter: 1381 loss: 1.47628066e-06
Iter: 1382 loss: 1.47535138e-06
Iter: 1383 loss: 1.47312221e-06
Iter: 1384 loss: 1.49831249e-06
Iter: 1385 loss: 1.47290507e-06
Iter: 1386 loss: 1.47063861e-06
Iter: 1387 loss: 1.48013601e-06
Iter: 1388 loss: 1.47024457e-06
Iter: 1389 loss: 1.46863454e-06
Iter: 1390 loss: 1.47984599e-06
Iter: 1391 loss: 1.46849777e-06
Iter: 1392 loss: 1.46690809e-06
Iter: 1393 loss: 1.48057711e-06
Iter: 1394 loss: 1.46681634e-06
Iter: 1395 loss: 1.46588854e-06
Iter: 1396 loss: 1.46405637e-06
Iter: 1397 loss: 1.49818186e-06
Iter: 1398 loss: 1.46400771e-06
Iter: 1399 loss: 1.46309469e-06
Iter: 1400 loss: 1.46285947e-06
Iter: 1401 loss: 1.46178627e-06
Iter: 1402 loss: 1.4617965e-06
Iter: 1403 loss: 1.46086222e-06
Iter: 1404 loss: 1.45955221e-06
Iter: 1405 loss: 1.46246498e-06
Iter: 1406 loss: 1.45904596e-06
Iter: 1407 loss: 1.45775414e-06
Iter: 1408 loss: 1.46212551e-06
Iter: 1409 loss: 1.45740705e-06
Iter: 1410 loss: 1.45595811e-06
Iter: 1411 loss: 1.46215041e-06
Iter: 1412 loss: 1.45570698e-06
Iter: 1413 loss: 1.45477088e-06
Iter: 1414 loss: 1.45242871e-06
Iter: 1415 loss: 1.47533569e-06
Iter: 1416 loss: 1.45209128e-06
Iter: 1417 loss: 1.45019976e-06
Iter: 1418 loss: 1.4742825e-06
Iter: 1419 loss: 1.45017384e-06
Iter: 1420 loss: 1.44934859e-06
Iter: 1421 loss: 1.44925866e-06
Iter: 1422 loss: 1.44857904e-06
Iter: 1423 loss: 1.4467405e-06
Iter: 1424 loss: 1.45977731e-06
Iter: 1425 loss: 1.44632179e-06
Iter: 1426 loss: 1.4446407e-06
Iter: 1427 loss: 1.45567856e-06
Iter: 1428 loss: 1.44443527e-06
Iter: 1429 loss: 1.44343653e-06
Iter: 1430 loss: 1.44337423e-06
Iter: 1431 loss: 1.44262424e-06
Iter: 1432 loss: 1.44069543e-06
Iter: 1433 loss: 1.45670265e-06
Iter: 1434 loss: 1.44034789e-06
Iter: 1435 loss: 1.43891134e-06
Iter: 1436 loss: 1.43892203e-06
Iter: 1437 loss: 1.43742056e-06
Iter: 1438 loss: 1.4405706e-06
Iter: 1439 loss: 1.43683224e-06
Iter: 1440 loss: 1.43593661e-06
Iter: 1441 loss: 1.43497562e-06
Iter: 1442 loss: 1.43487068e-06
Iter: 1443 loss: 1.43342209e-06
Iter: 1444 loss: 1.4533033e-06
Iter: 1445 loss: 1.43341276e-06
Iter: 1446 loss: 1.43239345e-06
Iter: 1447 loss: 1.43236684e-06
Iter: 1448 loss: 1.43153966e-06
Iter: 1449 loss: 1.43019406e-06
Iter: 1450 loss: 1.43285e-06
Iter: 1451 loss: 1.42957208e-06
Iter: 1452 loss: 1.42829799e-06
Iter: 1453 loss: 1.42836234e-06
Iter: 1454 loss: 1.4272124e-06
Iter: 1455 loss: 1.42600652e-06
Iter: 1456 loss: 1.42592012e-06
Iter: 1457 loss: 1.42518388e-06
Iter: 1458 loss: 1.42328122e-06
Iter: 1459 loss: 1.44377179e-06
Iter: 1460 loss: 1.42313138e-06
Iter: 1461 loss: 1.4215542e-06
Iter: 1462 loss: 1.4296362e-06
Iter: 1463 loss: 1.42127647e-06
Iter: 1464 loss: 1.41962278e-06
Iter: 1465 loss: 1.43668353e-06
Iter: 1466 loss: 1.41955957e-06
Iter: 1467 loss: 1.41856572e-06
Iter: 1468 loss: 1.41680994e-06
Iter: 1469 loss: 1.45968352e-06
Iter: 1470 loss: 1.41681403e-06
Iter: 1471 loss: 1.41666862e-06
Iter: 1472 loss: 1.4159865e-06
Iter: 1473 loss: 1.41532246e-06
Iter: 1474 loss: 1.4136823e-06
Iter: 1475 loss: 1.42789838e-06
Iter: 1476 loss: 1.41336716e-06
Iter: 1477 loss: 1.41161809e-06
Iter: 1478 loss: 1.41766657e-06
Iter: 1479 loss: 1.41115913e-06
Iter: 1480 loss: 1.40981183e-06
Iter: 1481 loss: 1.40978e-06
Iter: 1482 loss: 1.40905786e-06
Iter: 1483 loss: 1.40790064e-06
Iter: 1484 loss: 1.40790462e-06
Iter: 1485 loss: 1.40629709e-06
Iter: 1486 loss: 1.41082467e-06
Iter: 1487 loss: 1.40581187e-06
Iter: 1488 loss: 1.40430643e-06
Iter: 1489 loss: 1.40661803e-06
Iter: 1490 loss: 1.40361067e-06
Iter: 1491 loss: 1.40291331e-06
Iter: 1492 loss: 1.40285101e-06
Iter: 1493 loss: 1.40205111e-06
Iter: 1494 loss: 1.40058773e-06
Iter: 1495 loss: 1.4299635e-06
Iter: 1496 loss: 1.40056693e-06
Iter: 1497 loss: 1.39909616e-06
Iter: 1498 loss: 1.40031034e-06
Iter: 1499 loss: 1.3982326e-06
Iter: 1500 loss: 1.39759e-06
Iter: 1501 loss: 1.39729855e-06
Iter: 1502 loss: 1.39644294e-06
Iter: 1503 loss: 1.39456756e-06
Iter: 1504 loss: 1.41898113e-06
Iter: 1505 loss: 1.39441772e-06
Iter: 1506 loss: 1.39408985e-06
Iter: 1507 loss: 1.39363942e-06
Iter: 1508 loss: 1.39285885e-06
Iter: 1509 loss: 1.3914148e-06
Iter: 1510 loss: 1.42447186e-06
Iter: 1511 loss: 1.39138865e-06
Iter: 1512 loss: 1.39014935e-06
Iter: 1513 loss: 1.39003919e-06
Iter: 1514 loss: 1.38904886e-06
Iter: 1515 loss: 1.38824316e-06
Iter: 1516 loss: 1.38807195e-06
Iter: 1517 loss: 1.38707765e-06
Iter: 1518 loss: 1.38547898e-06
Iter: 1519 loss: 1.38547193e-06
Iter: 1520 loss: 1.38405051e-06
Iter: 1521 loss: 1.38454959e-06
Iter: 1522 loss: 1.38301925e-06
Iter: 1523 loss: 1.38144514e-06
Iter: 1524 loss: 1.39378699e-06
Iter: 1525 loss: 1.38128416e-06
Iter: 1526 loss: 1.37994289e-06
Iter: 1527 loss: 1.3841593e-06
Iter: 1528 loss: 1.37953589e-06
Iter: 1529 loss: 1.37861014e-06
Iter: 1530 loss: 1.38325981e-06
Iter: 1531 loss: 1.37844063e-06
Iter: 1532 loss: 1.3771039e-06
Iter: 1533 loss: 1.37849656e-06
Iter: 1534 loss: 1.3763713e-06
Iter: 1535 loss: 1.37525581e-06
Iter: 1536 loss: 1.37453549e-06
Iter: 1537 loss: 1.37407551e-06
Iter: 1538 loss: 1.37331767e-06
Iter: 1539 loss: 1.3730928e-06
Iter: 1540 loss: 1.37235793e-06
Iter: 1541 loss: 1.37074449e-06
Iter: 1542 loss: 1.39562462e-06
Iter: 1543 loss: 1.37071299e-06
Iter: 1544 loss: 1.37010511e-06
Iter: 1545 loss: 1.36992912e-06
Iter: 1546 loss: 1.36915105e-06
Iter: 1547 loss: 1.36796518e-06
Iter: 1548 loss: 1.36794756e-06
Iter: 1549 loss: 1.36666063e-06
Iter: 1550 loss: 1.36466201e-06
Iter: 1551 loss: 1.36460494e-06
Iter: 1552 loss: 1.36216931e-06
Iter: 1553 loss: 1.37595225e-06
Iter: 1554 loss: 1.36184701e-06
Iter: 1555 loss: 1.36142387e-06
Iter: 1556 loss: 1.36077097e-06
Iter: 1557 loss: 1.36025847e-06
Iter: 1558 loss: 1.35897903e-06
Iter: 1559 loss: 1.37456982e-06
Iter: 1560 loss: 1.35893038e-06
Iter: 1561 loss: 1.35750304e-06
Iter: 1562 loss: 1.35881123e-06
Iter: 1563 loss: 1.35675464e-06
Iter: 1564 loss: 1.35577511e-06
Iter: 1565 loss: 1.3557227e-06
Iter: 1566 loss: 1.35466712e-06
Iter: 1567 loss: 1.355265e-06
Iter: 1568 loss: 1.35398602e-06
Iter: 1569 loss: 1.3524816e-06
Iter: 1570 loss: 1.35499158e-06
Iter: 1571 loss: 1.35182154e-06
Iter: 1572 loss: 1.3505985e-06
Iter: 1573 loss: 1.35186815e-06
Iter: 1574 loss: 1.34990705e-06
Iter: 1575 loss: 1.3491524e-06
Iter: 1576 loss: 1.34912239e-06
Iter: 1577 loss: 1.348458e-06
Iter: 1578 loss: 1.3470526e-06
Iter: 1579 loss: 1.36904305e-06
Iter: 1580 loss: 1.34701747e-06
Iter: 1581 loss: 1.34616766e-06
Iter: 1582 loss: 1.34611673e-06
Iter: 1583 loss: 1.34517268e-06
Iter: 1584 loss: 1.34380059e-06
Iter: 1585 loss: 1.34374591e-06
Iter: 1586 loss: 1.34247102e-06
Iter: 1587 loss: 1.34126208e-06
Iter: 1588 loss: 1.34097081e-06
Iter: 1589 loss: 1.33884271e-06
Iter: 1590 loss: 1.34784318e-06
Iter: 1591 loss: 1.33837057e-06
Iter: 1592 loss: 1.33697984e-06
Iter: 1593 loss: 1.34715492e-06
Iter: 1594 loss: 1.33681442e-06
Iter: 1595 loss: 1.33542153e-06
Iter: 1596 loss: 1.34801053e-06
Iter: 1597 loss: 1.33532558e-06
Iter: 1598 loss: 1.33443245e-06
Iter: 1599 loss: 1.33267736e-06
Iter: 1600 loss: 1.36911467e-06
Iter: 1601 loss: 1.33268077e-06
Iter: 1602 loss: 1.33148092e-06
Iter: 1603 loss: 1.33151059e-06
Iter: 1604 loss: 1.3302e-06
Iter: 1605 loss: 1.33119545e-06
Iter: 1606 loss: 1.32945274e-06
Iter: 1607 loss: 1.32850437e-06
Iter: 1608 loss: 1.33086178e-06
Iter: 1609 loss: 1.3282189e-06
Iter: 1610 loss: 1.32726746e-06
Iter: 1611 loss: 1.33098604e-06
Iter: 1612 loss: 1.32700063e-06
Iter: 1613 loss: 1.32599416e-06
Iter: 1614 loss: 1.32977834e-06
Iter: 1615 loss: 1.32577111e-06
Iter: 1616 loss: 1.32478749e-06
Iter: 1617 loss: 1.32393097e-06
Iter: 1618 loss: 1.32364266e-06
Iter: 1619 loss: 1.32278865e-06
Iter: 1620 loss: 1.32268428e-06
Iter: 1621 loss: 1.32209243e-06
Iter: 1622 loss: 1.32045284e-06
Iter: 1623 loss: 1.33077083e-06
Iter: 1624 loss: 1.32002333e-06
Iter: 1625 loss: 1.31810202e-06
Iter: 1626 loss: 1.32156561e-06
Iter: 1627 loss: 1.31726802e-06
Iter: 1628 loss: 1.31520505e-06
Iter: 1629 loss: 1.32017362e-06
Iter: 1630 loss: 1.31442948e-06
Iter: 1631 loss: 1.31370621e-06
Iter: 1632 loss: 1.31337072e-06
Iter: 1633 loss: 1.31219986e-06
Iter: 1634 loss: 1.31076717e-06
Iter: 1635 loss: 1.310686e-06
Iter: 1636 loss: 1.30918966e-06
Iter: 1637 loss: 1.31044703e-06
Iter: 1638 loss: 1.30834269e-06
Iter: 1639 loss: 1.30687044e-06
Iter: 1640 loss: 1.31291154e-06
Iter: 1641 loss: 1.30655633e-06
Iter: 1642 loss: 1.30541389e-06
Iter: 1643 loss: 1.30538535e-06
Iter: 1644 loss: 1.3047935e-06
Iter: 1645 loss: 1.30327805e-06
Iter: 1646 loss: 1.31856905e-06
Iter: 1647 loss: 1.30307308e-06
Iter: 1648 loss: 1.30174749e-06
Iter: 1649 loss: 1.32079231e-06
Iter: 1650 loss: 1.30174737e-06
Iter: 1651 loss: 1.3003787e-06
Iter: 1652 loss: 1.30382796e-06
Iter: 1653 loss: 1.29991918e-06
Iter: 1654 loss: 1.2989658e-06
Iter: 1655 loss: 1.30270053e-06
Iter: 1656 loss: 1.29873092e-06
Iter: 1657 loss: 1.29798946e-06
Iter: 1658 loss: 1.30052456e-06
Iter: 1659 loss: 1.29777425e-06
Iter: 1660 loss: 1.2968485e-06
Iter: 1661 loss: 1.29817136e-06
Iter: 1662 loss: 1.29638363e-06
Iter: 1663 loss: 1.29556861e-06
Iter: 1664 loss: 1.29395607e-06
Iter: 1665 loss: 1.32327523e-06
Iter: 1666 loss: 1.29390673e-06
Iter: 1667 loss: 1.29208e-06
Iter: 1668 loss: 1.30031231e-06
Iter: 1669 loss: 1.29177079e-06
Iter: 1670 loss: 1.29119894e-06
Iter: 1671 loss: 1.29085436e-06
Iter: 1672 loss: 1.29018031e-06
Iter: 1673 loss: 1.28850206e-06
Iter: 1674 loss: 1.30355727e-06
Iter: 1675 loss: 1.2882756e-06
Iter: 1676 loss: 1.28661407e-06
Iter: 1677 loss: 1.28981947e-06
Iter: 1678 loss: 1.2859017e-06
Iter: 1679 loss: 1.28576676e-06
Iter: 1680 loss: 1.2851026e-06
Iter: 1681 loss: 1.28441e-06
Iter: 1682 loss: 1.28264139e-06
Iter: 1683 loss: 1.29790874e-06
Iter: 1684 loss: 1.2823308e-06
Iter: 1685 loss: 1.28078307e-06
Iter: 1686 loss: 1.29377554e-06
Iter: 1687 loss: 1.28070701e-06
Iter: 1688 loss: 1.27965939e-06
Iter: 1689 loss: 1.29612636e-06
Iter: 1690 loss: 1.27964108e-06
Iter: 1691 loss: 1.27914166e-06
Iter: 1692 loss: 1.27819499e-06
Iter: 1693 loss: 1.27819021e-06
Iter: 1694 loss: 1.27700469e-06
Iter: 1695 loss: 1.29071623e-06
Iter: 1696 loss: 1.2770148e-06
Iter: 1697 loss: 1.27620456e-06
Iter: 1698 loss: 1.27649719e-06
Iter: 1699 loss: 1.27558837e-06
Iter: 1700 loss: 1.2745827e-06
Iter: 1701 loss: 1.27477324e-06
Iter: 1702 loss: 1.27383851e-06
Iter: 1703 loss: 1.27237172e-06
Iter: 1704 loss: 1.27177623e-06
Iter: 1705 loss: 1.27105409e-06
Iter: 1706 loss: 1.27077897e-06
Iter: 1707 loss: 1.27015585e-06
Iter: 1708 loss: 1.26939358e-06
Iter: 1709 loss: 1.26801365e-06
Iter: 1710 loss: 1.30096009e-06
Iter: 1711 loss: 1.26800978e-06
Iter: 1712 loss: 1.26664941e-06
Iter: 1713 loss: 1.26675582e-06
Iter: 1714 loss: 1.2655978e-06
Iter: 1715 loss: 1.26531643e-06
Iter: 1716 loss: 1.26480154e-06
Iter: 1717 loss: 1.26406599e-06
Iter: 1718 loss: 1.26286409e-06
Iter: 1719 loss: 1.26285295e-06
Iter: 1720 loss: 1.26167663e-06
Iter: 1721 loss: 1.2619264e-06
Iter: 1722 loss: 1.26078726e-06
Iter: 1723 loss: 1.25995621e-06
Iter: 1724 loss: 1.25978715e-06
Iter: 1725 loss: 1.2592825e-06
Iter: 1726 loss: 1.25815018e-06
Iter: 1727 loss: 1.27364638e-06
Iter: 1728 loss: 1.2580922e-06
Iter: 1729 loss: 1.25725524e-06
Iter: 1730 loss: 1.25718066e-06
Iter: 1731 loss: 1.25644692e-06
Iter: 1732 loss: 1.25506347e-06
Iter: 1733 loss: 1.28510715e-06
Iter: 1734 loss: 1.25505289e-06
Iter: 1735 loss: 1.25365318e-06
Iter: 1736 loss: 1.26075156e-06
Iter: 1737 loss: 1.2533817e-06
Iter: 1738 loss: 1.25221482e-06
Iter: 1739 loss: 1.25221129e-06
Iter: 1740 loss: 1.25125905e-06
Iter: 1741 loss: 1.25047245e-06
Iter: 1742 loss: 1.2503483e-06
Iter: 1743 loss: 1.24958092e-06
Iter: 1744 loss: 1.24842484e-06
Iter: 1745 loss: 1.24838743e-06
Iter: 1746 loss: 1.24768826e-06
Iter: 1747 loss: 1.2476653e-06
Iter: 1748 loss: 1.24684948e-06
Iter: 1749 loss: 1.24747589e-06
Iter: 1750 loss: 1.24637745e-06
Iter: 1751 loss: 1.24543601e-06
Iter: 1752 loss: 1.24406461e-06
Iter: 1753 loss: 1.2440164e-06
Iter: 1754 loss: 1.24335497e-06
Iter: 1755 loss: 1.24313067e-06
Iter: 1756 loss: 1.24223675e-06
Iter: 1757 loss: 1.24266114e-06
Iter: 1758 loss: 1.241625e-06
Iter: 1759 loss: 1.24083658e-06
Iter: 1760 loss: 1.2420187e-06
Iter: 1761 loss: 1.2404812e-06
Iter: 1762 loss: 1.23922791e-06
Iter: 1763 loss: 1.24363032e-06
Iter: 1764 loss: 1.2389346e-06
Iter: 1765 loss: 1.23821701e-06
Iter: 1766 loss: 1.23761015e-06
Iter: 1767 loss: 1.23741211e-06
Iter: 1768 loss: 1.23611483e-06
Iter: 1769 loss: 1.24153075e-06
Iter: 1770 loss: 1.23586938e-06
Iter: 1771 loss: 1.23461109e-06
Iter: 1772 loss: 1.23405925e-06
Iter: 1773 loss: 1.23336758e-06
Iter: 1774 loss: 1.23272628e-06
Iter: 1775 loss: 1.23243149e-06
Iter: 1776 loss: 1.23166535e-06
Iter: 1777 loss: 1.23060022e-06
Iter: 1778 loss: 1.23057248e-06
Iter: 1779 loss: 1.22955635e-06
Iter: 1780 loss: 1.23171026e-06
Iter: 1781 loss: 1.22911047e-06
Iter: 1782 loss: 1.22780762e-06
Iter: 1783 loss: 1.24006965e-06
Iter: 1784 loss: 1.22777703e-06
Iter: 1785 loss: 1.22706012e-06
Iter: 1786 loss: 1.22548931e-06
Iter: 1787 loss: 1.24549e-06
Iter: 1788 loss: 1.22536062e-06
Iter: 1789 loss: 1.2248579e-06
Iter: 1790 loss: 1.22454276e-06
Iter: 1791 loss: 1.22366089e-06
Iter: 1792 loss: 1.22222366e-06
Iter: 1793 loss: 1.22222934e-06
Iter: 1794 loss: 1.22108031e-06
Iter: 1795 loss: 1.22428253e-06
Iter: 1796 loss: 1.22067149e-06
Iter: 1797 loss: 1.21971971e-06
Iter: 1798 loss: 1.21973142e-06
Iter: 1799 loss: 1.21910534e-06
Iter: 1800 loss: 1.21772541e-06
Iter: 1801 loss: 1.23214068e-06
Iter: 1802 loss: 1.2175509e-06
Iter: 1803 loss: 1.21596759e-06
Iter: 1804 loss: 1.22828374e-06
Iter: 1805 loss: 1.21589517e-06
Iter: 1806 loss: 1.2146354e-06
Iter: 1807 loss: 1.21588812e-06
Iter: 1808 loss: 1.21390917e-06
Iter: 1809 loss: 1.2130522e-06
Iter: 1810 loss: 1.21304936e-06
Iter: 1811 loss: 1.21222433e-06
Iter: 1812 loss: 1.21148173e-06
Iter: 1813 loss: 1.21124958e-06
Iter: 1814 loss: 1.21047674e-06
Iter: 1815 loss: 1.21899893e-06
Iter: 1816 loss: 1.21042558e-06
Iter: 1817 loss: 1.20959135e-06
Iter: 1818 loss: 1.20977347e-06
Iter: 1819 loss: 1.20898176e-06
Iter: 1820 loss: 1.20800757e-06
Iter: 1821 loss: 1.20679806e-06
Iter: 1822 loss: 1.2067419e-06
Iter: 1823 loss: 1.20580034e-06
Iter: 1824 loss: 1.20575248e-06
Iter: 1825 loss: 1.20477023e-06
Iter: 1826 loss: 1.20669802e-06
Iter: 1827 loss: 1.20437642e-06
Iter: 1828 loss: 1.2036636e-06
Iter: 1829 loss: 1.20306959e-06
Iter: 1830 loss: 1.20287837e-06
Iter: 1831 loss: 1.20162213e-06
Iter: 1832 loss: 1.21582525e-06
Iter: 1833 loss: 1.20158938e-06
Iter: 1834 loss: 1.20106142e-06
Iter: 1835 loss: 1.19981917e-06
Iter: 1836 loss: 1.21687265e-06
Iter: 1837 loss: 1.19975948e-06
Iter: 1838 loss: 1.19832316e-06
Iter: 1839 loss: 1.20283994e-06
Iter: 1840 loss: 1.1978758e-06
Iter: 1841 loss: 1.19629647e-06
Iter: 1842 loss: 1.20686127e-06
Iter: 1843 loss: 1.19611673e-06
Iter: 1844 loss: 1.19532842e-06
Iter: 1845 loss: 1.19702281e-06
Iter: 1846 loss: 1.19499146e-06
Iter: 1847 loss: 1.19391768e-06
Iter: 1848 loss: 1.2019342e-06
Iter: 1849 loss: 1.19384731e-06
Iter: 1850 loss: 1.19321385e-06
Iter: 1851 loss: 1.19204856e-06
Iter: 1852 loss: 1.22043355e-06
Iter: 1853 loss: 1.19205743e-06
Iter: 1854 loss: 1.19130721e-06
Iter: 1855 loss: 1.19120909e-06
Iter: 1856 loss: 1.1905563e-06
Iter: 1857 loss: 1.18901153e-06
Iter: 1858 loss: 1.20775007e-06
Iter: 1859 loss: 1.18890307e-06
Iter: 1860 loss: 1.18743208e-06
Iter: 1861 loss: 1.19052652e-06
Iter: 1862 loss: 1.18684579e-06
Iter: 1863 loss: 1.1856099e-06
Iter: 1864 loss: 1.19050151e-06
Iter: 1865 loss: 1.18531125e-06
Iter: 1866 loss: 1.18460275e-06
Iter: 1867 loss: 1.18453977e-06
Iter: 1868 loss: 1.18382331e-06
Iter: 1869 loss: 1.18263983e-06
Iter: 1870 loss: 1.18264302e-06
Iter: 1871 loss: 1.18178e-06
Iter: 1872 loss: 1.18175922e-06
Iter: 1873 loss: 1.18087e-06
Iter: 1874 loss: 1.18062655e-06
Iter: 1875 loss: 1.18007006e-06
Iter: 1876 loss: 1.17917477e-06
Iter: 1877 loss: 1.17910895e-06
Iter: 1878 loss: 1.17848413e-06
Iter: 1879 loss: 1.1774066e-06
Iter: 1880 loss: 1.18732419e-06
Iter: 1881 loss: 1.17735749e-06
Iter: 1882 loss: 1.17668424e-06
Iter: 1883 loss: 1.17847958e-06
Iter: 1884 loss: 1.17645538e-06
Iter: 1885 loss: 1.17549882e-06
Iter: 1886 loss: 1.17855029e-06
Iter: 1887 loss: 1.17525917e-06
Iter: 1888 loss: 1.17451953e-06
Iter: 1889 loss: 1.17339914e-06
Iter: 1890 loss: 1.17337754e-06
Iter: 1891 loss: 1.17326772e-06
Iter: 1892 loss: 1.17280592e-06
Iter: 1893 loss: 1.17233708e-06
Iter: 1894 loss: 1.17124489e-06
Iter: 1895 loss: 1.18288528e-06
Iter: 1896 loss: 1.17114541e-06
Iter: 1897 loss: 1.17001082e-06
Iter: 1898 loss: 1.17211619e-06
Iter: 1899 loss: 1.16951389e-06
Iter: 1900 loss: 1.16896899e-06
Iter: 1901 loss: 1.16880881e-06
Iter: 1902 loss: 1.16829858e-06
Iter: 1903 loss: 1.16719411e-06
Iter: 1904 loss: 1.18417256e-06
Iter: 1905 loss: 1.16715421e-06
Iter: 1906 loss: 1.16646402e-06
Iter: 1907 loss: 1.16637977e-06
Iter: 1908 loss: 1.16577405e-06
Iter: 1909 loss: 1.16434751e-06
Iter: 1910 loss: 1.18333799e-06
Iter: 1911 loss: 1.16426963e-06
Iter: 1912 loss: 1.16319916e-06
Iter: 1913 loss: 1.16523779e-06
Iter: 1914 loss: 1.16274532e-06
Iter: 1915 loss: 1.16179763e-06
Iter: 1916 loss: 1.17401828e-06
Iter: 1917 loss: 1.16179103e-06
Iter: 1918 loss: 1.16094236e-06
Iter: 1919 loss: 1.16421779e-06
Iter: 1920 loss: 1.1607525e-06
Iter: 1921 loss: 1.15986836e-06
Iter: 1922 loss: 1.16010597e-06
Iter: 1923 loss: 1.15923137e-06
Iter: 1924 loss: 1.15829766e-06
Iter: 1925 loss: 1.15755233e-06
Iter: 1926 loss: 1.15721218e-06
Iter: 1927 loss: 1.15587159e-06
Iter: 1928 loss: 1.16211618e-06
Iter: 1929 loss: 1.15561988e-06
Iter: 1930 loss: 1.1548525e-06
Iter: 1931 loss: 1.15477803e-06
Iter: 1932 loss: 1.15429111e-06
Iter: 1933 loss: 1.15311855e-06
Iter: 1934 loss: 1.16449473e-06
Iter: 1935 loss: 1.15296143e-06
Iter: 1936 loss: 1.15170906e-06
Iter: 1937 loss: 1.15661192e-06
Iter: 1938 loss: 1.15143825e-06
Iter: 1939 loss: 1.15046544e-06
Iter: 1940 loss: 1.15044236e-06
Iter: 1941 loss: 1.14987347e-06
Iter: 1942 loss: 1.14884926e-06
Iter: 1943 loss: 1.14885052e-06
Iter: 1944 loss: 1.1479641e-06
Iter: 1945 loss: 1.14793693e-06
Iter: 1946 loss: 1.14749264e-06
Iter: 1947 loss: 1.14630473e-06
Iter: 1948 loss: 1.15385865e-06
Iter: 1949 loss: 1.14600402e-06
Iter: 1950 loss: 1.1446574e-06
Iter: 1951 loss: 1.15500052e-06
Iter: 1952 loss: 1.14456475e-06
Iter: 1953 loss: 1.14338741e-06
Iter: 1954 loss: 1.1546897e-06
Iter: 1955 loss: 1.14333125e-06
Iter: 1956 loss: 1.1426248e-06
Iter: 1957 loss: 1.14121281e-06
Iter: 1958 loss: 1.16963952e-06
Iter: 1959 loss: 1.14121417e-06
Iter: 1960 loss: 1.13999249e-06
Iter: 1961 loss: 1.15086095e-06
Iter: 1962 loss: 1.13989313e-06
Iter: 1963 loss: 1.13878662e-06
Iter: 1964 loss: 1.14111936e-06
Iter: 1965 loss: 1.13832e-06
Iter: 1966 loss: 1.13748456e-06
Iter: 1967 loss: 1.14998011e-06
Iter: 1968 loss: 1.13750752e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0.8
+ date
Wed Nov  4 12:44:39 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.4/300_300_300_1 --function f2 --psi 3 --alpha 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f675e255d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67381c2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67381c2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f673817e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f673817ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67381bb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f673810bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f673812d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f673812d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6738224730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6738224d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f673827a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f673827abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67380470d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f673827a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f672027d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f672027dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f672023a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67201852f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67201850d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67201c2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f672017aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67380e5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67200cb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67200cbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f672008d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6720088a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f673827aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67200e7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6720127598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6720200730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67202001e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66d4785598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66d477af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67200278c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67200338c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.014527563
test_loss: 0.015478441
train_loss: 0.01011528
test_loss: 0.011183752
train_loss: 0.008341687
test_loss: 0.0096134385
train_loss: 0.007596555
test_loss: 0.009258611
train_loss: 0.0074040606
test_loss: 0.008789629
train_loss: 0.0066273264
test_loss: 0.008342476
train_loss: 0.0066237617
test_loss: 0.008119785
train_loss: 0.0062971413
test_loss: 0.008006139
train_loss: 0.0057807537
test_loss: 0.0075361826
train_loss: 0.0061199935
test_loss: 0.007898646
train_loss: 0.0057215663
test_loss: 0.0076240967
train_loss: 0.005755078
test_loss: 0.007313138
train_loss: 0.0057054837
test_loss: 0.0072885747
train_loss: 0.005834844
test_loss: 0.0074712476
train_loss: 0.0057037934
test_loss: 0.0072422875
train_loss: 0.005834179
test_loss: 0.007385262
train_loss: 0.0052746907
test_loss: 0.007240259
train_loss: 0.005551517
test_loss: 0.007061446
train_loss: 0.005462523
test_loss: 0.006892616
train_loss: 0.005735217
test_loss: 0.007117928
train_loss: 0.0052714404
test_loss: 0.0068346285
train_loss: 0.00506578
test_loss: 0.0068572783
train_loss: 0.005201045
test_loss: 0.007035868
train_loss: 0.005087133
test_loss: 0.006745479
train_loss: 0.005328941
test_loss: 0.006717938
train_loss: 0.005023086
test_loss: 0.0067729224
train_loss: 0.00507084
test_loss: 0.006860389
train_loss: 0.005316527
test_loss: 0.0068239477
train_loss: 0.0050657825
test_loss: 0.006727257
train_loss: 0.005197334
test_loss: 0.006648363
train_loss: 0.004997738
test_loss: 0.0067157145
train_loss: 0.0047686677
test_loss: 0.006550806
train_loss: 0.004855674
test_loss: 0.0065300236
train_loss: 0.0047906996
test_loss: 0.0066514467
train_loss: 0.0052214777
test_loss: 0.0066375765
train_loss: 0.00476674
test_loss: 0.00641851
train_loss: 0.0052570496
test_loss: 0.0065090987
train_loss: 0.004727527
test_loss: 0.006471301
train_loss: 0.004530204
test_loss: 0.0063511757
train_loss: 0.00479143
test_loss: 0.0065282974
train_loss: 0.0045234505
test_loss: 0.0064716414
train_loss: 0.004739001
test_loss: 0.0063830405
train_loss: 0.004486169
test_loss: 0.0065548527
train_loss: 0.0048263436
test_loss: 0.006522067
train_loss: 0.004842302
test_loss: 0.0064376616
train_loss: 0.0046341866
test_loss: 0.0063478383
train_loss: 0.004761854
test_loss: 0.006667272
train_loss: 0.0047148685
test_loss: 0.006443016
train_loss: 0.004943399
test_loss: 0.0064518955
train_loss: 0.0052195783
test_loss: 0.0063666226
train_loss: 0.004710256
test_loss: 0.0063339258
train_loss: 0.004430343
test_loss: 0.006142962
train_loss: 0.0047571547
test_loss: 0.006244959
train_loss: 0.0042885477
test_loss: 0.006135606
train_loss: 0.0043556765
test_loss: 0.0062332707
train_loss: 0.004365023
test_loss: 0.0061548445
train_loss: 0.0045189345
test_loss: 0.0062377844
train_loss: 0.004377803
test_loss: 0.006556739
train_loss: 0.00424904
test_loss: 0.006370168
train_loss: 0.0045729745
test_loss: 0.0064318124
train_loss: 0.004860458
test_loss: 0.0064595677
train_loss: 0.0042830235
test_loss: 0.0064044846
train_loss: 0.0043567354
test_loss: 0.006152926
train_loss: 0.004562568
test_loss: 0.006388609
train_loss: 0.0045802323
test_loss: 0.0062871804
train_loss: 0.0045064255
test_loss: 0.006165319
train_loss: 0.0043165516
test_loss: 0.0063095363
train_loss: 0.0043638204
test_loss: 0.0061025885
train_loss: 0.004320255
test_loss: 0.006244544
train_loss: 0.0043222597
test_loss: 0.006195309
train_loss: 0.004328264
test_loss: 0.006219794
train_loss: 0.004215685
test_loss: 0.0062114024
train_loss: 0.0045349747
test_loss: 0.0061996332
train_loss: 0.0041753245
test_loss: 0.006214873
train_loss: 0.0043095816
test_loss: 0.0062284498
train_loss: 0.004264007
test_loss: 0.0060989787
train_loss: 0.004372424
test_loss: 0.0061855046
train_loss: 0.004388759
test_loss: 0.006034567
train_loss: 0.0044811685
test_loss: 0.006217318
train_loss: 0.003962989
test_loss: 0.0061810017
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.8/300_300_300_1 --optimizer lbfgs --function f2 --psi 3 --alpha 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55f54f1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55f54f1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55f53e12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55f5382510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55f53828c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55f53992f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55f5382400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55f52fb378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55f530a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4fd18c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4fd12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55f530a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4f7b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4f5d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4f72730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4f72ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4f228c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4eea620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4e7f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4eea6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4eeaa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4e57ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4e15ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4dc4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4dc4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4df26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4d9c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4d9c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4d59598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4d596a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4d59400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4d260d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4cf4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4f06158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4cf4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55d4c3f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.51957715e-05
Iter: 2 loss: 2.90609951e-05
Iter: 3 loss: 9.51558322e-05
Iter: 4 loss: 2.89185555e-05
Iter: 5 loss: 2.54385559e-05
Iter: 6 loss: 2.96167345e-05
Iter: 7 loss: 2.35792468e-05
Iter: 8 loss: 2.14365209e-05
Iter: 9 loss: 3.13395431e-05
Iter: 10 loss: 2.1042295e-05
Iter: 11 loss: 1.90857882e-05
Iter: 12 loss: 2.63333714e-05
Iter: 13 loss: 1.86046818e-05
Iter: 14 loss: 1.74392644e-05
Iter: 15 loss: 2.19050144e-05
Iter: 16 loss: 1.71596639e-05
Iter: 17 loss: 1.62795895e-05
Iter: 18 loss: 1.78898263e-05
Iter: 19 loss: 1.59017636e-05
Iter: 20 loss: 1.51145941e-05
Iter: 21 loss: 2.31761969e-05
Iter: 22 loss: 1.50920851e-05
Iter: 23 loss: 1.44653231e-05
Iter: 24 loss: 1.3891382e-05
Iter: 25 loss: 1.37381248e-05
Iter: 26 loss: 1.30589588e-05
Iter: 27 loss: 1.65229467e-05
Iter: 28 loss: 1.29477603e-05
Iter: 29 loss: 1.23335085e-05
Iter: 30 loss: 1.79456219e-05
Iter: 31 loss: 1.23059253e-05
Iter: 32 loss: 1.19299375e-05
Iter: 33 loss: 1.13529695e-05
Iter: 34 loss: 1.134389e-05
Iter: 35 loss: 1.0737398e-05
Iter: 36 loss: 1.41087294e-05
Iter: 37 loss: 1.0650695e-05
Iter: 38 loss: 1.05330419e-05
Iter: 39 loss: 1.03844759e-05
Iter: 40 loss: 1.01939449e-05
Iter: 41 loss: 9.82810707e-06
Iter: 42 loss: 1.75149908e-05
Iter: 43 loss: 9.82607162e-06
Iter: 44 loss: 9.5840478e-06
Iter: 45 loss: 9.5840378e-06
Iter: 46 loss: 9.35836215e-06
Iter: 47 loss: 9.65183062e-06
Iter: 48 loss: 9.24344e-06
Iter: 49 loss: 9.03868204e-06
Iter: 50 loss: 9.54438474e-06
Iter: 51 loss: 8.96594793e-06
Iter: 52 loss: 8.76276863e-06
Iter: 53 loss: 9.45154352e-06
Iter: 54 loss: 8.7082708e-06
Iter: 55 loss: 8.53491838e-06
Iter: 56 loss: 9.83116843e-06
Iter: 57 loss: 8.52121684e-06
Iter: 58 loss: 8.39956465e-06
Iter: 59 loss: 8.23321625e-06
Iter: 60 loss: 8.22541188e-06
Iter: 61 loss: 8.0901309e-06
Iter: 62 loss: 8.09007906e-06
Iter: 63 loss: 7.97048597e-06
Iter: 64 loss: 8.22550373e-06
Iter: 65 loss: 7.92368e-06
Iter: 66 loss: 7.82304323e-06
Iter: 67 loss: 7.6917122e-06
Iter: 68 loss: 7.68323662e-06
Iter: 69 loss: 7.53106e-06
Iter: 70 loss: 8.30446515e-06
Iter: 71 loss: 7.506153e-06
Iter: 72 loss: 7.41390204e-06
Iter: 73 loss: 7.40874748e-06
Iter: 74 loss: 7.34365403e-06
Iter: 75 loss: 7.239566e-06
Iter: 76 loss: 7.23869925e-06
Iter: 77 loss: 7.15032229e-06
Iter: 78 loss: 8.47998672e-06
Iter: 79 loss: 7.15029091e-06
Iter: 80 loss: 7.07754316e-06
Iter: 81 loss: 7.16073691e-06
Iter: 82 loss: 7.03856904e-06
Iter: 83 loss: 6.9711723e-06
Iter: 84 loss: 7.01800172e-06
Iter: 85 loss: 6.92901267e-06
Iter: 86 loss: 6.83662483e-06
Iter: 87 loss: 7.42117072e-06
Iter: 88 loss: 6.82618838e-06
Iter: 89 loss: 6.76191485e-06
Iter: 90 loss: 6.99142265e-06
Iter: 91 loss: 6.74541116e-06
Iter: 92 loss: 6.68470466e-06
Iter: 93 loss: 6.64612162e-06
Iter: 94 loss: 6.62222192e-06
Iter: 95 loss: 6.56612247e-06
Iter: 96 loss: 7.42238672e-06
Iter: 97 loss: 6.56612565e-06
Iter: 98 loss: 6.51008759e-06
Iter: 99 loss: 6.50914444e-06
Iter: 100 loss: 6.46519584e-06
Iter: 101 loss: 6.40490543e-06
Iter: 102 loss: 6.39293557e-06
Iter: 103 loss: 6.35288097e-06
Iter: 104 loss: 6.28580847e-06
Iter: 105 loss: 6.69587371e-06
Iter: 106 loss: 6.27771124e-06
Iter: 107 loss: 6.22742164e-06
Iter: 108 loss: 6.2273416e-06
Iter: 109 loss: 6.20201126e-06
Iter: 110 loss: 6.16652505e-06
Iter: 111 loss: 6.16520811e-06
Iter: 112 loss: 6.12395161e-06
Iter: 113 loss: 6.62278444e-06
Iter: 114 loss: 6.12352233e-06
Iter: 115 loss: 6.08938217e-06
Iter: 116 loss: 6.07246966e-06
Iter: 117 loss: 6.05618879e-06
Iter: 118 loss: 6.01717875e-06
Iter: 119 loss: 6.20798892e-06
Iter: 120 loss: 6.01041847e-06
Iter: 121 loss: 5.97023245e-06
Iter: 122 loss: 6.11376663e-06
Iter: 123 loss: 5.95987331e-06
Iter: 124 loss: 5.92452761e-06
Iter: 125 loss: 5.97922417e-06
Iter: 126 loss: 5.90799755e-06
Iter: 127 loss: 5.86998749e-06
Iter: 128 loss: 5.93606e-06
Iter: 129 loss: 5.85319549e-06
Iter: 130 loss: 5.82042776e-06
Iter: 131 loss: 6.04978413e-06
Iter: 132 loss: 5.81731592e-06
Iter: 133 loss: 5.78033496e-06
Iter: 134 loss: 5.77072569e-06
Iter: 135 loss: 5.74751675e-06
Iter: 136 loss: 5.70906377e-06
Iter: 137 loss: 5.69994427e-06
Iter: 138 loss: 5.67528423e-06
Iter: 139 loss: 5.63791264e-06
Iter: 140 loss: 6.20771789e-06
Iter: 141 loss: 5.6379231e-06
Iter: 142 loss: 5.60520402e-06
Iter: 143 loss: 5.81267386e-06
Iter: 144 loss: 5.60155331e-06
Iter: 145 loss: 5.58238162e-06
Iter: 146 loss: 5.56800023e-06
Iter: 147 loss: 5.56174064e-06
Iter: 148 loss: 5.53931295e-06
Iter: 149 loss: 5.53924201e-06
Iter: 150 loss: 5.52258598e-06
Iter: 151 loss: 5.49158494e-06
Iter: 152 loss: 6.18610557e-06
Iter: 153 loss: 5.491499e-06
Iter: 154 loss: 5.46833144e-06
Iter: 155 loss: 5.46824322e-06
Iter: 156 loss: 5.44682962e-06
Iter: 157 loss: 5.45473267e-06
Iter: 158 loss: 5.43192073e-06
Iter: 159 loss: 5.40541e-06
Iter: 160 loss: 5.46671708e-06
Iter: 161 loss: 5.39550911e-06
Iter: 162 loss: 5.36857169e-06
Iter: 163 loss: 5.42390262e-06
Iter: 164 loss: 5.35772506e-06
Iter: 165 loss: 5.33545699e-06
Iter: 166 loss: 5.58289685e-06
Iter: 167 loss: 5.33494858e-06
Iter: 168 loss: 5.31686965e-06
Iter: 169 loss: 5.30330499e-06
Iter: 170 loss: 5.29742601e-06
Iter: 171 loss: 5.27100929e-06
Iter: 172 loss: 5.28879673e-06
Iter: 173 loss: 5.25446785e-06
Iter: 174 loss: 5.23455265e-06
Iter: 175 loss: 5.54797907e-06
Iter: 176 loss: 5.23449717e-06
Iter: 177 loss: 5.21233869e-06
Iter: 178 loss: 5.23391236e-06
Iter: 179 loss: 5.19968262e-06
Iter: 180 loss: 5.18237312e-06
Iter: 181 loss: 5.19799232e-06
Iter: 182 loss: 5.17233184e-06
Iter: 183 loss: 5.15041211e-06
Iter: 184 loss: 5.35555819e-06
Iter: 185 loss: 5.14944531e-06
Iter: 186 loss: 5.13709438e-06
Iter: 187 loss: 5.11441613e-06
Iter: 188 loss: 5.6422632e-06
Iter: 189 loss: 5.11441385e-06
Iter: 190 loss: 5.09344682e-06
Iter: 191 loss: 5.09332676e-06
Iter: 192 loss: 5.07681216e-06
Iter: 193 loss: 5.07971527e-06
Iter: 194 loss: 5.06450397e-06
Iter: 195 loss: 5.04522041e-06
Iter: 196 loss: 5.086692e-06
Iter: 197 loss: 5.03779302e-06
Iter: 198 loss: 5.01678915e-06
Iter: 199 loss: 5.09767142e-06
Iter: 200 loss: 5.01191698e-06
Iter: 201 loss: 4.99625139e-06
Iter: 202 loss: 5.13005443e-06
Iter: 203 loss: 4.99538237e-06
Iter: 204 loss: 4.98336794e-06
Iter: 205 loss: 4.96359144e-06
Iter: 206 loss: 4.96345092e-06
Iter: 207 loss: 4.94011738e-06
Iter: 208 loss: 5.01635714e-06
Iter: 209 loss: 4.93358357e-06
Iter: 210 loss: 4.92332219e-06
Iter: 211 loss: 4.92182653e-06
Iter: 212 loss: 4.91108585e-06
Iter: 213 loss: 4.89493868e-06
Iter: 214 loss: 4.89465128e-06
Iter: 215 loss: 4.87849547e-06
Iter: 216 loss: 4.97571637e-06
Iter: 217 loss: 4.87647958e-06
Iter: 218 loss: 4.85910368e-06
Iter: 219 loss: 4.91023502e-06
Iter: 220 loss: 4.85382361e-06
Iter: 221 loss: 4.84233897e-06
Iter: 222 loss: 4.83156055e-06
Iter: 223 loss: 4.82901578e-06
Iter: 224 loss: 4.81187226e-06
Iter: 225 loss: 4.81183e-06
Iter: 226 loss: 4.79939354e-06
Iter: 227 loss: 4.78646598e-06
Iter: 228 loss: 4.78408492e-06
Iter: 229 loss: 4.76883952e-06
Iter: 230 loss: 4.89101058e-06
Iter: 231 loss: 4.76775222e-06
Iter: 232 loss: 4.75435536e-06
Iter: 233 loss: 4.78152106e-06
Iter: 234 loss: 4.74890476e-06
Iter: 235 loss: 4.73346927e-06
Iter: 236 loss: 4.79046957e-06
Iter: 237 loss: 4.72955662e-06
Iter: 238 loss: 4.7172839e-06
Iter: 239 loss: 4.71459089e-06
Iter: 240 loss: 4.70647774e-06
Iter: 241 loss: 4.69156794e-06
Iter: 242 loss: 4.7303256e-06
Iter: 243 loss: 4.68649614e-06
Iter: 244 loss: 4.6740488e-06
Iter: 245 loss: 4.67396876e-06
Iter: 246 loss: 4.66558595e-06
Iter: 247 loss: 4.65088215e-06
Iter: 248 loss: 4.65087487e-06
Iter: 249 loss: 4.63729975e-06
Iter: 250 loss: 4.81543657e-06
Iter: 251 loss: 4.63722836e-06
Iter: 252 loss: 4.62449862e-06
Iter: 253 loss: 4.62279331e-06
Iter: 254 loss: 4.61372838e-06
Iter: 255 loss: 4.59976491e-06
Iter: 256 loss: 4.61367563e-06
Iter: 257 loss: 4.59176408e-06
Iter: 258 loss: 4.57989699e-06
Iter: 259 loss: 4.57950318e-06
Iter: 260 loss: 4.57219903e-06
Iter: 261 loss: 4.55694226e-06
Iter: 262 loss: 4.82370478e-06
Iter: 263 loss: 4.55657391e-06
Iter: 264 loss: 4.54353631e-06
Iter: 265 loss: 4.73196633e-06
Iter: 266 loss: 4.54353903e-06
Iter: 267 loss: 4.53279472e-06
Iter: 268 loss: 4.55439567e-06
Iter: 269 loss: 4.52844415e-06
Iter: 270 loss: 4.5171937e-06
Iter: 271 loss: 4.55114696e-06
Iter: 272 loss: 4.51384176e-06
Iter: 273 loss: 4.50370408e-06
Iter: 274 loss: 4.50214384e-06
Iter: 275 loss: 4.49521394e-06
Iter: 276 loss: 4.48362289e-06
Iter: 277 loss: 4.55226564e-06
Iter: 278 loss: 4.48224546e-06
Iter: 279 loss: 4.46985041e-06
Iter: 280 loss: 4.53692428e-06
Iter: 281 loss: 4.46791546e-06
Iter: 282 loss: 4.45993192e-06
Iter: 283 loss: 4.44992429e-06
Iter: 284 loss: 4.44905891e-06
Iter: 285 loss: 4.43831868e-06
Iter: 286 loss: 4.43825184e-06
Iter: 287 loss: 4.42990859e-06
Iter: 288 loss: 4.41753582e-06
Iter: 289 loss: 4.41724205e-06
Iter: 290 loss: 4.40305848e-06
Iter: 291 loss: 4.46564172e-06
Iter: 292 loss: 4.4001863e-06
Iter: 293 loss: 4.38728512e-06
Iter: 294 loss: 4.51865117e-06
Iter: 295 loss: 4.38683583e-06
Iter: 296 loss: 4.37972358e-06
Iter: 297 loss: 4.36629534e-06
Iter: 298 loss: 4.67182235e-06
Iter: 299 loss: 4.36627806e-06
Iter: 300 loss: 4.3551272e-06
Iter: 301 loss: 4.35504262e-06
Iter: 302 loss: 4.34536923e-06
Iter: 303 loss: 4.35873881e-06
Iter: 304 loss: 4.34053163e-06
Iter: 305 loss: 4.33087098e-06
Iter: 306 loss: 4.35725906e-06
Iter: 307 loss: 4.32775914e-06
Iter: 308 loss: 4.31775334e-06
Iter: 309 loss: 4.31811532e-06
Iter: 310 loss: 4.30979435e-06
Iter: 311 loss: 4.30064074e-06
Iter: 312 loss: 4.41733027e-06
Iter: 313 loss: 4.30055525e-06
Iter: 314 loss: 4.29058218e-06
Iter: 315 loss: 4.29581405e-06
Iter: 316 loss: 4.28386738e-06
Iter: 317 loss: 4.27481791e-06
Iter: 318 loss: 4.28243129e-06
Iter: 319 loss: 4.26948236e-06
Iter: 320 loss: 4.25865392e-06
Iter: 321 loss: 4.36491655e-06
Iter: 322 loss: 4.25824146e-06
Iter: 323 loss: 4.24982318e-06
Iter: 324 loss: 4.23732126e-06
Iter: 325 loss: 4.23709298e-06
Iter: 326 loss: 4.22907169e-06
Iter: 327 loss: 4.22903531e-06
Iter: 328 loss: 4.22080893e-06
Iter: 329 loss: 4.22190124e-06
Iter: 330 loss: 4.21453569e-06
Iter: 331 loss: 4.20482775e-06
Iter: 332 loss: 4.20730339e-06
Iter: 333 loss: 4.19779826e-06
Iter: 334 loss: 4.18929267e-06
Iter: 335 loss: 4.1892954e-06
Iter: 336 loss: 4.18175614e-06
Iter: 337 loss: 4.18236414e-06
Iter: 338 loss: 4.17588444e-06
Iter: 339 loss: 4.16720559e-06
Iter: 340 loss: 4.1922508e-06
Iter: 341 loss: 4.16452121e-06
Iter: 342 loss: 4.15539307e-06
Iter: 343 loss: 4.16770035e-06
Iter: 344 loss: 4.15074464e-06
Iter: 345 loss: 4.14403212e-06
Iter: 346 loss: 4.14403212e-06
Iter: 347 loss: 4.13730231e-06
Iter: 348 loss: 4.12991949e-06
Iter: 349 loss: 4.12887084e-06
Iter: 350 loss: 4.11922974e-06
Iter: 351 loss: 4.1358835e-06
Iter: 352 loss: 4.11502e-06
Iter: 353 loss: 4.1032431e-06
Iter: 354 loss: 4.20719789e-06
Iter: 355 loss: 4.10266739e-06
Iter: 356 loss: 4.09666654e-06
Iter: 357 loss: 4.08654478e-06
Iter: 358 loss: 4.08654432e-06
Iter: 359 loss: 4.07815241e-06
Iter: 360 loss: 4.07787593e-06
Iter: 361 loss: 4.07044035e-06
Iter: 362 loss: 4.06759182e-06
Iter: 363 loss: 4.06349591e-06
Iter: 364 loss: 4.05460742e-06
Iter: 365 loss: 4.0633131e-06
Iter: 366 loss: 4.04964248e-06
Iter: 367 loss: 4.04102229e-06
Iter: 368 loss: 4.04103275e-06
Iter: 369 loss: 4.03420518e-06
Iter: 370 loss: 4.02733531e-06
Iter: 371 loss: 4.0259788e-06
Iter: 372 loss: 4.01659463e-06
Iter: 373 loss: 4.07350672e-06
Iter: 374 loss: 4.01549778e-06
Iter: 375 loss: 4.00714907e-06
Iter: 376 loss: 4.01550778e-06
Iter: 377 loss: 4.00241515e-06
Iter: 378 loss: 3.99533883e-06
Iter: 379 loss: 3.99526743e-06
Iter: 380 loss: 3.9905367e-06
Iter: 381 loss: 3.98201e-06
Iter: 382 loss: 4.19430853e-06
Iter: 383 loss: 3.98198563e-06
Iter: 384 loss: 3.97318126e-06
Iter: 385 loss: 4.04329057e-06
Iter: 386 loss: 3.97261101e-06
Iter: 387 loss: 3.96373e-06
Iter: 388 loss: 3.97895838e-06
Iter: 389 loss: 3.95964e-06
Iter: 390 loss: 3.95254028e-06
Iter: 391 loss: 3.94513972e-06
Iter: 392 loss: 3.9438537e-06
Iter: 393 loss: 3.93601749e-06
Iter: 394 loss: 3.93531218e-06
Iter: 395 loss: 3.9299639e-06
Iter: 396 loss: 3.92113952e-06
Iter: 397 loss: 3.92105721e-06
Iter: 398 loss: 3.9118122e-06
Iter: 399 loss: 3.95360257e-06
Iter: 400 loss: 3.91006097e-06
Iter: 401 loss: 3.90183641e-06
Iter: 402 loss: 3.98831662e-06
Iter: 403 loss: 3.90164905e-06
Iter: 404 loss: 3.89612524e-06
Iter: 405 loss: 3.88897388e-06
Iter: 406 loss: 3.88851458e-06
Iter: 407 loss: 3.8795165e-06
Iter: 408 loss: 3.9446677e-06
Iter: 409 loss: 3.87874479e-06
Iter: 410 loss: 3.87150249e-06
Iter: 411 loss: 3.89461275e-06
Iter: 412 loss: 3.86946704e-06
Iter: 413 loss: 3.86179136e-06
Iter: 414 loss: 3.89921297e-06
Iter: 415 loss: 3.86035754e-06
Iter: 416 loss: 3.85469866e-06
Iter: 417 loss: 3.85006388e-06
Iter: 418 loss: 3.84838495e-06
Iter: 419 loss: 3.84124905e-06
Iter: 420 loss: 3.92987386e-06
Iter: 421 loss: 3.84122495e-06
Iter: 422 loss: 3.834381e-06
Iter: 423 loss: 3.82883172e-06
Iter: 424 loss: 3.82692906e-06
Iter: 425 loss: 3.81837708e-06
Iter: 426 loss: 3.83528868e-06
Iter: 427 loss: 3.81479731e-06
Iter: 428 loss: 3.80657252e-06
Iter: 429 loss: 3.80656365e-06
Iter: 430 loss: 3.80238384e-06
Iter: 431 loss: 3.79301264e-06
Iter: 432 loss: 3.92801758e-06
Iter: 433 loss: 3.7926261e-06
Iter: 434 loss: 3.78446248e-06
Iter: 435 loss: 3.9023821e-06
Iter: 436 loss: 3.78444793e-06
Iter: 437 loss: 3.77654123e-06
Iter: 438 loss: 3.79499988e-06
Iter: 439 loss: 3.7736886e-06
Iter: 440 loss: 3.76712615e-06
Iter: 441 loss: 3.76395337e-06
Iter: 442 loss: 3.7607324e-06
Iter: 443 loss: 3.75185709e-06
Iter: 444 loss: 3.83205588e-06
Iter: 445 loss: 3.75148397e-06
Iter: 446 loss: 3.74500769e-06
Iter: 447 loss: 3.77952802e-06
Iter: 448 loss: 3.74404954e-06
Iter: 449 loss: 3.73785883e-06
Iter: 450 loss: 3.74539059e-06
Iter: 451 loss: 3.7346606e-06
Iter: 452 loss: 3.72871409e-06
Iter: 453 loss: 3.73006242e-06
Iter: 454 loss: 3.7242653e-06
Iter: 455 loss: 3.7174334e-06
Iter: 456 loss: 3.80987649e-06
Iter: 457 loss: 3.71745091e-06
Iter: 458 loss: 3.71256829e-06
Iter: 459 loss: 3.70498969e-06
Iter: 460 loss: 3.70490102e-06
Iter: 461 loss: 3.69736131e-06
Iter: 462 loss: 3.76112393e-06
Iter: 463 loss: 3.69685631e-06
Iter: 464 loss: 3.68898827e-06
Iter: 465 loss: 3.71005672e-06
Iter: 466 loss: 3.68636233e-06
Iter: 467 loss: 3.68086216e-06
Iter: 468 loss: 3.67689586e-06
Iter: 469 loss: 3.67498342e-06
Iter: 470 loss: 3.67000985e-06
Iter: 471 loss: 3.67003031e-06
Iter: 472 loss: 3.66436734e-06
Iter: 473 loss: 3.65752794e-06
Iter: 474 loss: 3.65682808e-06
Iter: 475 loss: 3.64846574e-06
Iter: 476 loss: 3.67197117e-06
Iter: 477 loss: 3.64580478e-06
Iter: 478 loss: 3.63847175e-06
Iter: 479 loss: 3.70969474e-06
Iter: 480 loss: 3.63829054e-06
Iter: 481 loss: 3.6317e-06
Iter: 482 loss: 3.64342986e-06
Iter: 483 loss: 3.628885e-06
Iter: 484 loss: 3.62131027e-06
Iter: 485 loss: 3.62896981e-06
Iter: 486 loss: 3.6170693e-06
Iter: 487 loss: 3.61112916e-06
Iter: 488 loss: 3.64221751e-06
Iter: 489 loss: 3.61021011e-06
Iter: 490 loss: 3.60422359e-06
Iter: 491 loss: 3.62265132e-06
Iter: 492 loss: 3.60232e-06
Iter: 493 loss: 3.59670867e-06
Iter: 494 loss: 3.58913803e-06
Iter: 495 loss: 3.58872126e-06
Iter: 496 loss: 3.58419538e-06
Iter: 497 loss: 3.5832586e-06
Iter: 498 loss: 3.57872364e-06
Iter: 499 loss: 3.57345857e-06
Iter: 500 loss: 3.5728217e-06
Iter: 501 loss: 3.56574856e-06
Iter: 502 loss: 3.5725725e-06
Iter: 503 loss: 3.56167357e-06
Iter: 504 loss: 3.55647148e-06
Iter: 505 loss: 3.55623251e-06
Iter: 506 loss: 3.55119619e-06
Iter: 507 loss: 3.54187705e-06
Iter: 508 loss: 3.75770333e-06
Iter: 509 loss: 3.54187e-06
Iter: 510 loss: 3.53336168e-06
Iter: 511 loss: 3.57700242e-06
Iter: 512 loss: 3.53200699e-06
Iter: 513 loss: 3.52517873e-06
Iter: 514 loss: 3.61468574e-06
Iter: 515 loss: 3.52516327e-06
Iter: 516 loss: 3.52057396e-06
Iter: 517 loss: 3.52315897e-06
Iter: 518 loss: 3.51752e-06
Iter: 519 loss: 3.5112057e-06
Iter: 520 loss: 3.51581275e-06
Iter: 521 loss: 3.50740765e-06
Iter: 522 loss: 3.50135952e-06
Iter: 523 loss: 3.55092629e-06
Iter: 524 loss: 3.50102755e-06
Iter: 525 loss: 3.49506217e-06
Iter: 526 loss: 3.49625248e-06
Iter: 527 loss: 3.49065908e-06
Iter: 528 loss: 3.48414733e-06
Iter: 529 loss: 3.48938e-06
Iter: 530 loss: 3.48036406e-06
Iter: 531 loss: 3.47343303e-06
Iter: 532 loss: 3.57534145e-06
Iter: 533 loss: 3.47338892e-06
Iter: 534 loss: 3.46883439e-06
Iter: 535 loss: 3.46351339e-06
Iter: 536 loss: 3.46295133e-06
Iter: 537 loss: 3.45628018e-06
Iter: 538 loss: 3.47171431e-06
Iter: 539 loss: 3.4537793e-06
Iter: 540 loss: 3.44723321e-06
Iter: 541 loss: 3.53747282e-06
Iter: 542 loss: 3.44717137e-06
Iter: 543 loss: 3.44274576e-06
Iter: 544 loss: 3.43652118e-06
Iter: 545 loss: 3.43633178e-06
Iter: 546 loss: 3.42961039e-06
Iter: 547 loss: 3.46620982e-06
Iter: 548 loss: 3.42862063e-06
Iter: 549 loss: 3.42142289e-06
Iter: 550 loss: 3.46671959e-06
Iter: 551 loss: 3.42066892e-06
Iter: 552 loss: 3.41661644e-06
Iter: 553 loss: 3.42067119e-06
Iter: 554 loss: 3.41432542e-06
Iter: 555 loss: 3.40886254e-06
Iter: 556 loss: 3.41188252e-06
Iter: 557 loss: 3.40533802e-06
Iter: 558 loss: 3.39954227e-06
Iter: 559 loss: 3.46264892e-06
Iter: 560 loss: 3.39942449e-06
Iter: 561 loss: 3.39486905e-06
Iter: 562 loss: 3.39004055e-06
Iter: 563 loss: 3.38912514e-06
Iter: 564 loss: 3.38241125e-06
Iter: 565 loss: 3.41409623e-06
Iter: 566 loss: 3.38123618e-06
Iter: 567 loss: 3.37415872e-06
Iter: 568 loss: 3.41255782e-06
Iter: 569 loss: 3.37312918e-06
Iter: 570 loss: 3.36856465e-06
Iter: 571 loss: 3.36318431e-06
Iter: 572 loss: 3.36262951e-06
Iter: 573 loss: 3.35696313e-06
Iter: 574 loss: 3.41304258e-06
Iter: 575 loss: 3.35677487e-06
Iter: 576 loss: 3.35051072e-06
Iter: 577 loss: 3.36096491e-06
Iter: 578 loss: 3.34766946e-06
Iter: 579 loss: 3.34198967e-06
Iter: 580 loss: 3.3407307e-06
Iter: 581 loss: 3.33694925e-06
Iter: 582 loss: 3.33158368e-06
Iter: 583 loss: 3.33164553e-06
Iter: 584 loss: 3.32639479e-06
Iter: 585 loss: 3.32732725e-06
Iter: 586 loss: 3.32245668e-06
Iter: 587 loss: 3.31671936e-06
Iter: 588 loss: 3.32609966e-06
Iter: 589 loss: 3.31413275e-06
Iter: 590 loss: 3.30753574e-06
Iter: 591 loss: 3.33975959e-06
Iter: 592 loss: 3.30633884e-06
Iter: 593 loss: 3.30183047e-06
Iter: 594 loss: 3.33193202e-06
Iter: 595 loss: 3.30140892e-06
Iter: 596 loss: 3.29767499e-06
Iter: 597 loss: 3.29217528e-06
Iter: 598 loss: 3.29211844e-06
Iter: 599 loss: 3.28677402e-06
Iter: 600 loss: 3.36056701e-06
Iter: 601 loss: 3.2867415e-06
Iter: 602 loss: 3.2814321e-06
Iter: 603 loss: 3.28156875e-06
Iter: 604 loss: 3.27721023e-06
Iter: 605 loss: 3.27201e-06
Iter: 606 loss: 3.26741929e-06
Iter: 607 loss: 3.26598024e-06
Iter: 608 loss: 3.26068857e-06
Iter: 609 loss: 3.2601879e-06
Iter: 610 loss: 3.25578549e-06
Iter: 611 loss: 3.25758333e-06
Iter: 612 loss: 3.25263954e-06
Iter: 613 loss: 3.24820576e-06
Iter: 614 loss: 3.24786811e-06
Iter: 615 loss: 3.24444818e-06
Iter: 616 loss: 3.24076109e-06
Iter: 617 loss: 3.24032771e-06
Iter: 618 loss: 3.23728091e-06
Iter: 619 loss: 3.23133281e-06
Iter: 620 loss: 3.35314576e-06
Iter: 621 loss: 3.23132372e-06
Iter: 622 loss: 3.22526375e-06
Iter: 623 loss: 3.27633302e-06
Iter: 624 loss: 3.22491701e-06
Iter: 625 loss: 3.2199921e-06
Iter: 626 loss: 3.23560744e-06
Iter: 627 loss: 3.2186224e-06
Iter: 628 loss: 3.21376751e-06
Iter: 629 loss: 3.22310143e-06
Iter: 630 loss: 3.21158245e-06
Iter: 631 loss: 3.20662912e-06
Iter: 632 loss: 3.20865638e-06
Iter: 633 loss: 3.20321897e-06
Iter: 634 loss: 3.19896708e-06
Iter: 635 loss: 3.19897094e-06
Iter: 636 loss: 3.19500532e-06
Iter: 637 loss: 3.18804018e-06
Iter: 638 loss: 3.18807042e-06
Iter: 639 loss: 3.18167145e-06
Iter: 640 loss: 3.2062112e-06
Iter: 641 loss: 3.18015168e-06
Iter: 642 loss: 3.17463923e-06
Iter: 643 loss: 3.24670464e-06
Iter: 644 loss: 3.17456511e-06
Iter: 645 loss: 3.17022204e-06
Iter: 646 loss: 3.16365731e-06
Iter: 647 loss: 3.16360365e-06
Iter: 648 loss: 3.15825309e-06
Iter: 649 loss: 3.21157927e-06
Iter: 650 loss: 3.15808393e-06
Iter: 651 loss: 3.15242187e-06
Iter: 652 loss: 3.17039598e-06
Iter: 653 loss: 3.15090892e-06
Iter: 654 loss: 3.14650106e-06
Iter: 655 loss: 3.14456656e-06
Iter: 656 loss: 3.14230874e-06
Iter: 657 loss: 3.13751752e-06
Iter: 658 loss: 3.19522246e-06
Iter: 659 loss: 3.13745113e-06
Iter: 660 loss: 3.13333248e-06
Iter: 661 loss: 3.13663918e-06
Iter: 662 loss: 3.13087185e-06
Iter: 663 loss: 3.12571729e-06
Iter: 664 loss: 3.13747341e-06
Iter: 665 loss: 3.12380257e-06
Iter: 666 loss: 3.11874874e-06
Iter: 667 loss: 3.12543307e-06
Iter: 668 loss: 3.11620852e-06
Iter: 669 loss: 3.11106851e-06
Iter: 670 loss: 3.17106173e-06
Iter: 671 loss: 3.11101621e-06
Iter: 672 loss: 3.10765654e-06
Iter: 673 loss: 3.10148516e-06
Iter: 674 loss: 3.25174506e-06
Iter: 675 loss: 3.10146766e-06
Iter: 676 loss: 3.09489837e-06
Iter: 677 loss: 3.13661303e-06
Iter: 678 loss: 3.09419806e-06
Iter: 679 loss: 3.08843664e-06
Iter: 680 loss: 3.13734427e-06
Iter: 681 loss: 3.08805079e-06
Iter: 682 loss: 3.08461085e-06
Iter: 683 loss: 3.07737173e-06
Iter: 684 loss: 3.19459832e-06
Iter: 685 loss: 3.0771273e-06
Iter: 686 loss: 3.07516689e-06
Iter: 687 loss: 3.07313803e-06
Iter: 688 loss: 3.06978154e-06
Iter: 689 loss: 3.06569518e-06
Iter: 690 loss: 3.0653473e-06
Iter: 691 loss: 3.06037737e-06
Iter: 692 loss: 3.07202527e-06
Iter: 693 loss: 3.05854383e-06
Iter: 694 loss: 3.05333219e-06
Iter: 695 loss: 3.09617826e-06
Iter: 696 loss: 3.05302979e-06
Iter: 697 loss: 3.04907167e-06
Iter: 698 loss: 3.05042113e-06
Iter: 699 loss: 3.04633249e-06
Iter: 700 loss: 3.04097739e-06
Iter: 701 loss: 3.05514186e-06
Iter: 702 loss: 3.03925503e-06
Iter: 703 loss: 3.03479305e-06
Iter: 704 loss: 3.05566209e-06
Iter: 705 loss: 3.03400293e-06
Iter: 706 loss: 3.02923149e-06
Iter: 707 loss: 3.03789875e-06
Iter: 708 loss: 3.02711305e-06
Iter: 709 loss: 3.02278841e-06
Iter: 710 loss: 3.02099306e-06
Iter: 711 loss: 3.01857654e-06
Iter: 712 loss: 3.0146166e-06
Iter: 713 loss: 3.01470436e-06
Iter: 714 loss: 3.01066484e-06
Iter: 715 loss: 3.00923239e-06
Iter: 716 loss: 3.00705824e-06
Iter: 717 loss: 3.00259899e-06
Iter: 718 loss: 3.006714e-06
Iter: 719 loss: 3.00000534e-06
Iter: 720 loss: 2.99564022e-06
Iter: 721 loss: 2.99562907e-06
Iter: 722 loss: 2.99264684e-06
Iter: 723 loss: 2.98654868e-06
Iter: 724 loss: 3.09985853e-06
Iter: 725 loss: 2.98645318e-06
Iter: 726 loss: 2.98135546e-06
Iter: 727 loss: 3.03481147e-06
Iter: 728 loss: 2.9812e-06
Iter: 729 loss: 2.97606289e-06
Iter: 730 loss: 2.98556324e-06
Iter: 731 loss: 2.97381894e-06
Iter: 732 loss: 2.96956432e-06
Iter: 733 loss: 2.98238865e-06
Iter: 734 loss: 2.96815892e-06
Iter: 735 loss: 2.96377038e-06
Iter: 736 loss: 2.97037423e-06
Iter: 737 loss: 2.96172038e-06
Iter: 738 loss: 2.9571147e-06
Iter: 739 loss: 2.98929035e-06
Iter: 740 loss: 2.9566736e-06
Iter: 741 loss: 2.95270502e-06
Iter: 742 loss: 2.95329141e-06
Iter: 743 loss: 2.94961615e-06
Iter: 744 loss: 2.94508027e-06
Iter: 745 loss: 2.94835399e-06
Iter: 746 loss: 2.94219399e-06
Iter: 747 loss: 2.9381215e-06
Iter: 748 loss: 2.93816652e-06
Iter: 749 loss: 2.93502853e-06
Iter: 750 loss: 2.9301525e-06
Iter: 751 loss: 2.93008861e-06
Iter: 752 loss: 2.92571849e-06
Iter: 753 loss: 2.96616e-06
Iter: 754 loss: 2.9254918e-06
Iter: 755 loss: 2.92080313e-06
Iter: 756 loss: 2.93309222e-06
Iter: 757 loss: 2.91920765e-06
Iter: 758 loss: 2.91527908e-06
Iter: 759 loss: 2.90979506e-06
Iter: 760 loss: 2.90955927e-06
Iter: 761 loss: 2.90582216e-06
Iter: 762 loss: 2.90541539e-06
Iter: 763 loss: 2.90168373e-06
Iter: 764 loss: 2.89830678e-06
Iter: 765 loss: 2.89737682e-06
Iter: 766 loss: 2.89246782e-06
Iter: 767 loss: 2.9228554e-06
Iter: 768 loss: 2.89187165e-06
Iter: 769 loss: 2.88748424e-06
Iter: 770 loss: 2.8983045e-06
Iter: 771 loss: 2.8860054e-06
Iter: 772 loss: 2.88163187e-06
Iter: 773 loss: 2.90477169e-06
Iter: 774 loss: 2.88093042e-06
Iter: 775 loss: 2.87754733e-06
Iter: 776 loss: 2.87439934e-06
Iter: 777 loss: 2.87364583e-06
Iter: 778 loss: 2.86935665e-06
Iter: 779 loss: 2.90859862e-06
Iter: 780 loss: 2.86916588e-06
Iter: 781 loss: 2.86507066e-06
Iter: 782 loss: 2.87768262e-06
Iter: 783 loss: 2.86383874e-06
Iter: 784 loss: 2.86022714e-06
Iter: 785 loss: 2.85740589e-06
Iter: 786 loss: 2.85631609e-06
Iter: 787 loss: 2.85411693e-06
Iter: 788 loss: 2.8535826e-06
Iter: 789 loss: 2.8510517e-06
Iter: 790 loss: 2.84551243e-06
Iter: 791 loss: 2.93269954e-06
Iter: 792 loss: 2.84536259e-06
Iter: 793 loss: 2.8398847e-06
Iter: 794 loss: 2.85947272e-06
Iter: 795 loss: 2.8385532e-06
Iter: 796 loss: 2.83416102e-06
Iter: 797 loss: 2.83418285e-06
Iter: 798 loss: 2.83128111e-06
Iter: 799 loss: 2.826642e-06
Iter: 800 loss: 2.82657948e-06
Iter: 801 loss: 2.82185692e-06
Iter: 802 loss: 2.87991929e-06
Iter: 803 loss: 2.82172323e-06
Iter: 804 loss: 2.81832808e-06
Iter: 805 loss: 2.82963015e-06
Iter: 806 loss: 2.81742359e-06
Iter: 807 loss: 2.81387975e-06
Iter: 808 loss: 2.81667e-06
Iter: 809 loss: 2.8117297e-06
Iter: 810 loss: 2.80731138e-06
Iter: 811 loss: 2.80666336e-06
Iter: 812 loss: 2.80351901e-06
Iter: 813 loss: 2.80000472e-06
Iter: 814 loss: 2.799904e-06
Iter: 815 loss: 2.7965043e-06
Iter: 816 loss: 2.79286542e-06
Iter: 817 loss: 2.79229835e-06
Iter: 818 loss: 2.78795142e-06
Iter: 819 loss: 2.80600739e-06
Iter: 820 loss: 2.78700804e-06
Iter: 821 loss: 2.78312e-06
Iter: 822 loss: 2.82932297e-06
Iter: 823 loss: 2.78308403e-06
Iter: 824 loss: 2.78076595e-06
Iter: 825 loss: 2.77627964e-06
Iter: 826 loss: 2.87446846e-06
Iter: 827 loss: 2.776261e-06
Iter: 828 loss: 2.77137201e-06
Iter: 829 loss: 2.79000665e-06
Iter: 830 loss: 2.77016807e-06
Iter: 831 loss: 2.76496462e-06
Iter: 832 loss: 2.81450912e-06
Iter: 833 loss: 2.76480546e-06
Iter: 834 loss: 2.76217042e-06
Iter: 835 loss: 2.75886214e-06
Iter: 836 loss: 2.75861294e-06
Iter: 837 loss: 2.75454795e-06
Iter: 838 loss: 2.80514587e-06
Iter: 839 loss: 2.75456387e-06
Iter: 840 loss: 2.75131015e-06
Iter: 841 loss: 2.75568209e-06
Iter: 842 loss: 2.74980084e-06
Iter: 843 loss: 2.74632794e-06
Iter: 844 loss: 2.75538014e-06
Iter: 845 loss: 2.74512831e-06
Iter: 846 loss: 2.74178137e-06
Iter: 847 loss: 2.73787668e-06
Iter: 848 loss: 2.73739943e-06
Iter: 849 loss: 2.73247292e-06
Iter: 850 loss: 2.73237e-06
Iter: 851 loss: 2.72893567e-06
Iter: 852 loss: 2.72609395e-06
Iter: 853 loss: 2.72512216e-06
Iter: 854 loss: 2.72144962e-06
Iter: 855 loss: 2.75604225e-06
Iter: 856 loss: 2.72127431e-06
Iter: 857 loss: 2.71710178e-06
Iter: 858 loss: 2.7203464e-06
Iter: 859 loss: 2.71461022e-06
Iter: 860 loss: 2.71116096e-06
Iter: 861 loss: 2.70948499e-06
Iter: 862 loss: 2.70773944e-06
Iter: 863 loss: 2.70351666e-06
Iter: 864 loss: 2.74763352e-06
Iter: 865 loss: 2.70335977e-06
Iter: 866 loss: 2.69934253e-06
Iter: 867 loss: 2.71172735e-06
Iter: 868 loss: 2.69806947e-06
Iter: 869 loss: 2.69514817e-06
Iter: 870 loss: 2.69233033e-06
Iter: 871 loss: 2.69169868e-06
Iter: 872 loss: 2.68755412e-06
Iter: 873 loss: 2.75059688e-06
Iter: 874 loss: 2.68757321e-06
Iter: 875 loss: 2.68407439e-06
Iter: 876 loss: 2.68319582e-06
Iter: 877 loss: 2.68111535e-06
Iter: 878 loss: 2.67666474e-06
Iter: 879 loss: 2.69809925e-06
Iter: 880 loss: 2.6758687e-06
Iter: 881 loss: 2.67239739e-06
Iter: 882 loss: 2.67264409e-06
Iter: 883 loss: 2.66966936e-06
Iter: 884 loss: 2.6652192e-06
Iter: 885 loss: 2.72147554e-06
Iter: 886 loss: 2.66517645e-06
Iter: 887 loss: 2.66216148e-06
Iter: 888 loss: 2.65930794e-06
Iter: 889 loss: 2.65860081e-06
Iter: 890 loss: 2.65558651e-06
Iter: 891 loss: 2.6555258e-06
Iter: 892 loss: 2.65256722e-06
Iter: 893 loss: 2.64850496e-06
Iter: 894 loss: 2.64827167e-06
Iter: 895 loss: 2.64384e-06
Iter: 896 loss: 2.647776e-06
Iter: 897 loss: 2.6411592e-06
Iter: 898 loss: 2.63811262e-06
Iter: 899 loss: 2.63786296e-06
Iter: 900 loss: 2.63517722e-06
Iter: 901 loss: 2.63250331e-06
Iter: 902 loss: 2.6319392e-06
Iter: 903 loss: 2.62784579e-06
Iter: 904 loss: 2.63640413e-06
Iter: 905 loss: 2.6262353e-06
Iter: 906 loss: 2.62303047e-06
Iter: 907 loss: 2.62297044e-06
Iter: 908 loss: 2.62073718e-06
Iter: 909 loss: 2.61706737e-06
Iter: 910 loss: 2.61701393e-06
Iter: 911 loss: 2.61166883e-06
Iter: 912 loss: 2.63108609e-06
Iter: 913 loss: 2.61033165e-06
Iter: 914 loss: 2.60588558e-06
Iter: 915 loss: 2.62669664e-06
Iter: 916 loss: 2.60508023e-06
Iter: 917 loss: 2.60125444e-06
Iter: 918 loss: 2.6239868e-06
Iter: 919 loss: 2.60074421e-06
Iter: 920 loss: 2.597905e-06
Iter: 921 loss: 2.59411354e-06
Iter: 922 loss: 2.59382705e-06
Iter: 923 loss: 2.59066314e-06
Iter: 924 loss: 2.59038711e-06
Iter: 925 loss: 2.58805903e-06
Iter: 926 loss: 2.58342629e-06
Iter: 927 loss: 2.67147539e-06
Iter: 928 loss: 2.58341333e-06
Iter: 929 loss: 2.57919623e-06
Iter: 930 loss: 2.59444914e-06
Iter: 931 loss: 2.57811553e-06
Iter: 932 loss: 2.57508373e-06
Iter: 933 loss: 2.57507372e-06
Iter: 934 loss: 2.5728973e-06
Iter: 935 loss: 2.56851808e-06
Iter: 936 loss: 2.64791811e-06
Iter: 937 loss: 2.56841417e-06
Iter: 938 loss: 2.56421731e-06
Iter: 939 loss: 2.60165166e-06
Iter: 940 loss: 2.56406133e-06
Iter: 941 loss: 2.5600948e-06
Iter: 942 loss: 2.57484089e-06
Iter: 943 loss: 2.55906161e-06
Iter: 944 loss: 2.55548412e-06
Iter: 945 loss: 2.5543045e-06
Iter: 946 loss: 2.55220539e-06
Iter: 947 loss: 2.54780707e-06
Iter: 948 loss: 2.58022646e-06
Iter: 949 loss: 2.547451e-06
Iter: 950 loss: 2.54412498e-06
Iter: 951 loss: 2.55630403e-06
Iter: 952 loss: 2.54328916e-06
Iter: 953 loss: 2.53989788e-06
Iter: 954 loss: 2.54823863e-06
Iter: 955 loss: 2.53875396e-06
Iter: 956 loss: 2.53559256e-06
Iter: 957 loss: 2.53582812e-06
Iter: 958 loss: 2.53314693e-06
Iter: 959 loss: 2.52924838e-06
Iter: 960 loss: 2.58789782e-06
Iter: 961 loss: 2.52921564e-06
Iter: 962 loss: 2.52734662e-06
Iter: 963 loss: 2.52281779e-06
Iter: 964 loss: 2.5746026e-06
Iter: 965 loss: 2.52242012e-06
Iter: 966 loss: 2.51783831e-06
Iter: 967 loss: 2.56297517e-06
Iter: 968 loss: 2.51767847e-06
Iter: 969 loss: 2.51410029e-06
Iter: 970 loss: 2.54546649e-06
Iter: 971 loss: 2.51386837e-06
Iter: 972 loss: 2.51147821e-06
Iter: 973 loss: 2.50668131e-06
Iter: 974 loss: 2.59330454e-06
Iter: 975 loss: 2.50659514e-06
Iter: 976 loss: 2.50340281e-06
Iter: 977 loss: 2.50324229e-06
Iter: 978 loss: 2.50013295e-06
Iter: 979 loss: 2.50023777e-06
Iter: 980 loss: 2.49767663e-06
Iter: 981 loss: 2.49389223e-06
Iter: 982 loss: 2.50012545e-06
Iter: 983 loss: 2.49216919e-06
Iter: 984 loss: 2.48831566e-06
Iter: 985 loss: 2.51142137e-06
Iter: 986 loss: 2.48780452e-06
Iter: 987 loss: 2.484574e-06
Iter: 988 loss: 2.49586719e-06
Iter: 989 loss: 2.48377091e-06
Iter: 990 loss: 2.48029323e-06
Iter: 991 loss: 2.48268952e-06
Iter: 992 loss: 2.47810976e-06
Iter: 993 loss: 2.47479647e-06
Iter: 994 loss: 2.49628943e-06
Iter: 995 loss: 2.4745566e-06
Iter: 996 loss: 2.47110393e-06
Iter: 997 loss: 2.47638764e-06
Iter: 998 loss: 2.46947047e-06
Iter: 999 loss: 2.46652644e-06
Iter: 1000 loss: 2.46316495e-06
Iter: 1001 loss: 2.462765e-06
Iter: 1002 loss: 2.45915589e-06
Iter: 1003 loss: 2.51017855e-06
Iter: 1004 loss: 2.45911542e-06
Iter: 1005 loss: 2.45556657e-06
Iter: 1006 loss: 2.46310265e-06
Iter: 1007 loss: 2.45412798e-06
Iter: 1008 loss: 2.4514361e-06
Iter: 1009 loss: 2.44866942e-06
Iter: 1010 loss: 2.44819171e-06
Iter: 1011 loss: 2.44469902e-06
Iter: 1012 loss: 2.44463945e-06
Iter: 1013 loss: 2.44173953e-06
Iter: 1014 loss: 2.43923296e-06
Iter: 1015 loss: 2.43841941e-06
Iter: 1016 loss: 2.43472414e-06
Iter: 1017 loss: 2.44926923e-06
Iter: 1018 loss: 2.43393447e-06
Iter: 1019 loss: 2.42991155e-06
Iter: 1020 loss: 2.44361809e-06
Iter: 1021 loss: 2.42883016e-06
Iter: 1022 loss: 2.42504939e-06
Iter: 1023 loss: 2.44159492e-06
Iter: 1024 loss: 2.42434339e-06
Iter: 1025 loss: 2.42124315e-06
Iter: 1026 loss: 2.42409419e-06
Iter: 1027 loss: 2.41955104e-06
Iter: 1028 loss: 2.41667476e-06
Iter: 1029 loss: 2.44984267e-06
Iter: 1030 loss: 2.41665816e-06
Iter: 1031 loss: 2.41423527e-06
Iter: 1032 loss: 2.41189286e-06
Iter: 1033 loss: 2.41133785e-06
Iter: 1034 loss: 2.40784448e-06
Iter: 1035 loss: 2.40962117e-06
Iter: 1036 loss: 2.40543022e-06
Iter: 1037 loss: 2.40356326e-06
Iter: 1038 loss: 2.40314398e-06
Iter: 1039 loss: 2.40107556e-06
Iter: 1040 loss: 2.39730571e-06
Iter: 1041 loss: 2.4826e-06
Iter: 1042 loss: 2.39725523e-06
Iter: 1043 loss: 2.3936168e-06
Iter: 1044 loss: 2.41433077e-06
Iter: 1045 loss: 2.39314772e-06
Iter: 1046 loss: 2.38976872e-06
Iter: 1047 loss: 2.41547082e-06
Iter: 1048 loss: 2.38950656e-06
Iter: 1049 loss: 2.38711482e-06
Iter: 1050 loss: 2.38379198e-06
Iter: 1051 loss: 2.38361645e-06
Iter: 1052 loss: 2.38058396e-06
Iter: 1053 loss: 2.4242197e-06
Iter: 1054 loss: 2.3805751e-06
Iter: 1055 loss: 2.37795848e-06
Iter: 1056 loss: 2.3825437e-06
Iter: 1057 loss: 2.37679524e-06
Iter: 1058 loss: 2.37380527e-06
Iter: 1059 loss: 2.38042e-06
Iter: 1060 loss: 2.3726675e-06
Iter: 1061 loss: 2.36970277e-06
Iter: 1062 loss: 2.37653148e-06
Iter: 1063 loss: 2.3686448e-06
Iter: 1064 loss: 2.36526057e-06
Iter: 1065 loss: 2.38591701e-06
Iter: 1066 loss: 2.36495748e-06
Iter: 1067 loss: 2.36259302e-06
Iter: 1068 loss: 2.35896505e-06
Iter: 1069 loss: 2.3589655e-06
Iter: 1070 loss: 2.35463e-06
Iter: 1071 loss: 2.37320751e-06
Iter: 1072 loss: 2.35376592e-06
Iter: 1073 loss: 2.35051425e-06
Iter: 1074 loss: 2.35053153e-06
Iter: 1075 loss: 2.34866457e-06
Iter: 1076 loss: 2.34529512e-06
Iter: 1077 loss: 2.34533923e-06
Iter: 1078 loss: 2.3424891e-06
Iter: 1079 loss: 2.37971972e-06
Iter: 1080 loss: 2.34252366e-06
Iter: 1081 loss: 2.33965943e-06
Iter: 1082 loss: 2.34234722e-06
Iter: 1083 loss: 2.33807214e-06
Iter: 1084 loss: 2.33543301e-06
Iter: 1085 loss: 2.3350442e-06
Iter: 1086 loss: 2.33335413e-06
Iter: 1087 loss: 2.33003038e-06
Iter: 1088 loss: 2.36945925e-06
Iter: 1089 loss: 2.32998605e-06
Iter: 1090 loss: 2.3272903e-06
Iter: 1091 loss: 2.32976663e-06
Iter: 1092 loss: 2.32580373e-06
Iter: 1093 loss: 2.32269122e-06
Iter: 1094 loss: 2.33173705e-06
Iter: 1095 loss: 2.32179354e-06
Iter: 1096 loss: 2.31926742e-06
Iter: 1097 loss: 2.33642504e-06
Iter: 1098 loss: 2.3190405e-06
Iter: 1099 loss: 2.31651961e-06
Iter: 1100 loss: 2.31643753e-06
Iter: 1101 loss: 2.31448189e-06
Iter: 1102 loss: 2.31139688e-06
Iter: 1103 loss: 2.31046283e-06
Iter: 1104 loss: 2.30864771e-06
Iter: 1105 loss: 2.30590376e-06
Iter: 1106 loss: 2.30589785e-06
Iter: 1107 loss: 2.30313799e-06
Iter: 1108 loss: 2.30385081e-06
Iter: 1109 loss: 2.30121941e-06
Iter: 1110 loss: 2.29803118e-06
Iter: 1111 loss: 2.29804391e-06
Iter: 1112 loss: 2.29557054e-06
Iter: 1113 loss: 2.29274565e-06
Iter: 1114 loss: 2.29264674e-06
Iter: 1115 loss: 2.29025909e-06
Iter: 1116 loss: 2.28628983e-06
Iter: 1117 loss: 2.28626095e-06
Iter: 1118 loss: 2.28250383e-06
Iter: 1119 loss: 2.30664978e-06
Iter: 1120 loss: 2.28216049e-06
Iter: 1121 loss: 2.27891542e-06
Iter: 1122 loss: 2.3030932e-06
Iter: 1123 loss: 2.27859391e-06
Iter: 1124 loss: 2.27644296e-06
Iter: 1125 loss: 2.27838359e-06
Iter: 1126 loss: 2.27513397e-06
Iter: 1127 loss: 2.27236933e-06
Iter: 1128 loss: 2.27971555e-06
Iter: 1129 loss: 2.2714346e-06
Iter: 1130 loss: 2.26873067e-06
Iter: 1131 loss: 2.28694489e-06
Iter: 1132 loss: 2.26848761e-06
Iter: 1133 loss: 2.26631e-06
Iter: 1134 loss: 2.26450811e-06
Iter: 1135 loss: 2.26392353e-06
Iter: 1136 loss: 2.26099564e-06
Iter: 1137 loss: 2.26569978e-06
Iter: 1138 loss: 2.25963549e-06
Iter: 1139 loss: 2.25703661e-06
Iter: 1140 loss: 2.25705071e-06
Iter: 1141 loss: 2.25512554e-06
Iter: 1142 loss: 2.25214694e-06
Iter: 1143 loss: 2.25207282e-06
Iter: 1144 loss: 2.24869291e-06
Iter: 1145 loss: 2.26285169e-06
Iter: 1146 loss: 2.24792552e-06
Iter: 1147 loss: 2.24417249e-06
Iter: 1148 loss: 2.26544239e-06
Iter: 1149 loss: 2.24366136e-06
Iter: 1150 loss: 2.24111864e-06
Iter: 1151 loss: 2.23920961e-06
Iter: 1152 loss: 2.23842267e-06
Iter: 1153 loss: 2.23591951e-06
Iter: 1154 loss: 2.27060696e-06
Iter: 1155 loss: 2.23588154e-06
Iter: 1156 loss: 2.2333802e-06
Iter: 1157 loss: 2.23521465e-06
Iter: 1158 loss: 2.23183065e-06
Iter: 1159 loss: 2.22936092e-06
Iter: 1160 loss: 2.2378963e-06
Iter: 1161 loss: 2.22865037e-06
Iter: 1162 loss: 2.22629387e-06
Iter: 1163 loss: 2.23768393e-06
Iter: 1164 loss: 2.2258962e-06
Iter: 1165 loss: 2.22352287e-06
Iter: 1166 loss: 2.22710582e-06
Iter: 1167 loss: 2.22247172e-06
Iter: 1168 loss: 2.2197778e-06
Iter: 1169 loss: 2.21639812e-06
Iter: 1170 loss: 2.21616847e-06
Iter: 1171 loss: 2.21326536e-06
Iter: 1172 loss: 2.21326627e-06
Iter: 1173 loss: 2.2107547e-06
Iter: 1174 loss: 2.21838354e-06
Iter: 1175 loss: 2.20997072e-06
Iter: 1176 loss: 2.20765924e-06
Iter: 1177 loss: 2.20483253e-06
Iter: 1178 loss: 2.20463198e-06
Iter: 1179 loss: 2.20224183e-06
Iter: 1180 loss: 2.20216407e-06
Iter: 1181 loss: 2.19991875e-06
Iter: 1182 loss: 2.19854019e-06
Iter: 1183 loss: 2.1976432e-06
Iter: 1184 loss: 2.19466665e-06
Iter: 1185 loss: 2.19686308e-06
Iter: 1186 loss: 2.19282197e-06
Iter: 1187 loss: 2.1905405e-06
Iter: 1188 loss: 2.19049161e-06
Iter: 1189 loss: 2.18847981e-06
Iter: 1190 loss: 2.18679656e-06
Iter: 1191 loss: 2.18623654e-06
Iter: 1192 loss: 2.18369405e-06
Iter: 1193 loss: 2.20265383e-06
Iter: 1194 loss: 2.18351806e-06
Iter: 1195 loss: 2.18106288e-06
Iter: 1196 loss: 2.18577543e-06
Iter: 1197 loss: 2.18001242e-06
Iter: 1198 loss: 2.17720844e-06
Iter: 1199 loss: 2.18115838e-06
Iter: 1200 loss: 2.17576599e-06
Iter: 1201 loss: 2.17315392e-06
Iter: 1202 loss: 2.17147681e-06
Iter: 1203 loss: 2.17045522e-06
Iter: 1204 loss: 2.16771514e-06
Iter: 1205 loss: 2.16763988e-06
Iter: 1206 loss: 2.16505578e-06
Iter: 1207 loss: 2.16559556e-06
Iter: 1208 loss: 2.16316221e-06
Iter: 1209 loss: 2.16069384e-06
Iter: 1210 loss: 2.16269154e-06
Iter: 1211 loss: 2.159215e-06
Iter: 1212 loss: 2.1563319e-06
Iter: 1213 loss: 2.18998639e-06
Iter: 1214 loss: 2.15631849e-06
Iter: 1215 loss: 2.15409045e-06
Iter: 1216 loss: 2.1520616e-06
Iter: 1217 loss: 2.15146702e-06
Iter: 1218 loss: 2.14877946e-06
Iter: 1219 loss: 2.15677255e-06
Iter: 1220 loss: 2.14791748e-06
Iter: 1221 loss: 2.14525153e-06
Iter: 1222 loss: 2.17848742e-06
Iter: 1223 loss: 2.14524516e-06
Iter: 1224 loss: 2.1435178e-06
Iter: 1225 loss: 2.1418598e-06
Iter: 1226 loss: 2.14146371e-06
Iter: 1227 loss: 2.13903468e-06
Iter: 1228 loss: 2.16371586e-06
Iter: 1229 loss: 2.13892349e-06
Iter: 1230 loss: 2.13664453e-06
Iter: 1231 loss: 2.13806243e-06
Iter: 1232 loss: 2.13519388e-06
Iter: 1233 loss: 2.13260773e-06
Iter: 1234 loss: 2.13966223e-06
Iter: 1235 loss: 2.13177077e-06
Iter: 1236 loss: 2.12950727e-06
Iter: 1237 loss: 2.12880423e-06
Iter: 1238 loss: 2.12750911e-06
Iter: 1239 loss: 2.12537361e-06
Iter: 1240 loss: 2.12530313e-06
Iter: 1241 loss: 2.12336795e-06
Iter: 1242 loss: 2.12012674e-06
Iter: 1243 loss: 2.12010127e-06
Iter: 1244 loss: 2.11714041e-06
Iter: 1245 loss: 2.13407702e-06
Iter: 1246 loss: 2.11678844e-06
Iter: 1247 loss: 2.11388578e-06
Iter: 1248 loss: 2.13383237e-06
Iter: 1249 loss: 2.11362476e-06
Iter: 1250 loss: 2.11183965e-06
Iter: 1251 loss: 2.10917574e-06
Iter: 1252 loss: 2.10910275e-06
Iter: 1253 loss: 2.10595454e-06
Iter: 1254 loss: 2.12463146e-06
Iter: 1255 loss: 2.10559574e-06
Iter: 1256 loss: 2.10284975e-06
Iter: 1257 loss: 2.12707391e-06
Iter: 1258 loss: 2.10272628e-06
Iter: 1259 loss: 2.10094754e-06
Iter: 1260 loss: 2.09851305e-06
Iter: 1261 loss: 2.0983889e-06
Iter: 1262 loss: 2.09585346e-06
Iter: 1263 loss: 2.09586597e-06
Iter: 1264 loss: 2.09411519e-06
Iter: 1265 loss: 2.09413292e-06
Iter: 1266 loss: 2.09271502e-06
Iter: 1267 loss: 2.09061159e-06
Iter: 1268 loss: 2.09544669e-06
Iter: 1269 loss: 2.0897935e-06
Iter: 1270 loss: 2.08735582e-06
Iter: 1271 loss: 2.087133e-06
Iter: 1272 loss: 2.08539768e-06
Iter: 1273 loss: 2.0831003e-06
Iter: 1274 loss: 2.08304323e-06
Iter: 1275 loss: 2.08143251e-06
Iter: 1276 loss: 2.07870494e-06
Iter: 1277 loss: 2.07872108e-06
Iter: 1278 loss: 2.07584981e-06
Iter: 1279 loss: 2.09258951e-06
Iter: 1280 loss: 2.07544554e-06
Iter: 1281 loss: 2.07229277e-06
Iter: 1282 loss: 2.08527945e-06
Iter: 1283 loss: 2.07158587e-06
Iter: 1284 loss: 2.06959612e-06
Iter: 1285 loss: 2.06788559e-06
Iter: 1286 loss: 2.06735831e-06
Iter: 1287 loss: 2.06472e-06
Iter: 1288 loss: 2.0906964e-06
Iter: 1289 loss: 2.06458913e-06
Iter: 1290 loss: 2.06185678e-06
Iter: 1291 loss: 2.06761365e-06
Iter: 1292 loss: 2.06076243e-06
Iter: 1293 loss: 2.05851302e-06
Iter: 1294 loss: 2.060719e-06
Iter: 1295 loss: 2.05726406e-06
Iter: 1296 loss: 2.05508604e-06
Iter: 1297 loss: 2.08617439e-06
Iter: 1298 loss: 2.05507672e-06
Iter: 1299 loss: 2.05353786e-06
Iter: 1300 loss: 2.05139372e-06
Iter: 1301 loss: 2.051303e-06
Iter: 1302 loss: 2.04865296e-06
Iter: 1303 loss: 2.0641005e-06
Iter: 1304 loss: 2.04829166e-06
Iter: 1305 loss: 2.04608477e-06
Iter: 1306 loss: 2.0479888e-06
Iter: 1307 loss: 2.04476123e-06
Iter: 1308 loss: 2.04220237e-06
Iter: 1309 loss: 2.0717107e-06
Iter: 1310 loss: 2.04216167e-06
Iter: 1311 loss: 2.04066987e-06
Iter: 1312 loss: 2.03779837e-06
Iter: 1313 loss: 2.10686949e-06
Iter: 1314 loss: 2.03782542e-06
Iter: 1315 loss: 2.03494074e-06
Iter: 1316 loss: 2.05911169e-06
Iter: 1317 loss: 2.03481818e-06
Iter: 1318 loss: 2.03177729e-06
Iter: 1319 loss: 2.04056505e-06
Iter: 1320 loss: 2.0309144e-06
Iter: 1321 loss: 2.02873116e-06
Iter: 1322 loss: 2.02634806e-06
Iter: 1323 loss: 2.02603269e-06
Iter: 1324 loss: 2.02389265e-06
Iter: 1325 loss: 2.02388242e-06
Iter: 1326 loss: 2.0216944e-06
Iter: 1327 loss: 2.02238562e-06
Iter: 1328 loss: 2.02015985e-06
Iter: 1329 loss: 2.01798139e-06
Iter: 1330 loss: 2.02323918e-06
Iter: 1331 loss: 2.01718e-06
Iter: 1332 loss: 2.0152288e-06
Iter: 1333 loss: 2.03920513e-06
Iter: 1334 loss: 2.01522926e-06
Iter: 1335 loss: 2.01376588e-06
Iter: 1336 loss: 2.01082867e-06
Iter: 1337 loss: 2.06558707e-06
Iter: 1338 loss: 2.01081252e-06
Iter: 1339 loss: 2.00762975e-06
Iter: 1340 loss: 2.03079844e-06
Iter: 1341 loss: 2.0074e-06
Iter: 1342 loss: 2.00502882e-06
Iter: 1343 loss: 2.01292141e-06
Iter: 1344 loss: 2.00439217e-06
Iter: 1345 loss: 2.00210411e-06
Iter: 1346 loss: 2.01605872e-06
Iter: 1347 loss: 2.00178465e-06
Iter: 1348 loss: 1.99998431e-06
Iter: 1349 loss: 1.99769806e-06
Iter: 1350 loss: 1.99747683e-06
Iter: 1351 loss: 1.99523492e-06
Iter: 1352 loss: 2.01434455e-06
Iter: 1353 loss: 1.99508941e-06
Iter: 1354 loss: 1.99237729e-06
Iter: 1355 loss: 1.99486635e-06
Iter: 1356 loss: 1.99080796e-06
Iter: 1357 loss: 1.9884335e-06
Iter: 1358 loss: 1.98913722e-06
Iter: 1359 loss: 1.98674616e-06
Iter: 1360 loss: 1.98475846e-06
Iter: 1361 loss: 1.98471844e-06
Iter: 1362 loss: 1.98270459e-06
Iter: 1363 loss: 1.98284192e-06
Iter: 1364 loss: 1.98113025e-06
Iter: 1365 loss: 1.97905456e-06
Iter: 1366 loss: 1.98266e-06
Iter: 1367 loss: 1.97810482e-06
Iter: 1368 loss: 1.97548206e-06
Iter: 1369 loss: 1.99697752e-06
Iter: 1370 loss: 1.97531767e-06
Iter: 1371 loss: 1.97376585e-06
Iter: 1372 loss: 1.97203622e-06
Iter: 1373 loss: 1.97176e-06
Iter: 1374 loss: 1.96897804e-06
Iter: 1375 loss: 1.97931922e-06
Iter: 1376 loss: 1.96833525e-06
Iter: 1377 loss: 1.96579708e-06
Iter: 1378 loss: 1.98330576e-06
Iter: 1379 loss: 1.96566816e-06
Iter: 1380 loss: 1.96376982e-06
Iter: 1381 loss: 1.97069426e-06
Iter: 1382 loss: 1.96328756e-06
Iter: 1383 loss: 1.96158885e-06
Iter: 1384 loss: 1.95924804e-06
Iter: 1385 loss: 1.95913049e-06
Iter: 1386 loss: 1.95694406e-06
Iter: 1387 loss: 1.98912e-06
Iter: 1388 loss: 1.95694201e-06
Iter: 1389 loss: 1.95459188e-06
Iter: 1390 loss: 1.95586381e-06
Iter: 1391 loss: 1.95305711e-06
Iter: 1392 loss: 1.95090388e-06
Iter: 1393 loss: 1.9506308e-06
Iter: 1394 loss: 1.94913173e-06
Iter: 1395 loss: 1.94709014e-06
Iter: 1396 loss: 1.94711583e-06
Iter: 1397 loss: 1.94506515e-06
Iter: 1398 loss: 1.94445875e-06
Iter: 1399 loss: 1.94315703e-06
Iter: 1400 loss: 1.94095196e-06
Iter: 1401 loss: 1.94730796e-06
Iter: 1402 loss: 1.94028962e-06
Iter: 1403 loss: 1.93747337e-06
Iter: 1404 loss: 1.95080429e-06
Iter: 1405 loss: 1.93701567e-06
Iter: 1406 loss: 1.93519804e-06
Iter: 1407 loss: 1.93311757e-06
Iter: 1408 loss: 1.93285655e-06
Iter: 1409 loss: 1.92991206e-06
Iter: 1410 loss: 1.95256052e-06
Iter: 1411 loss: 1.92967559e-06
Iter: 1412 loss: 1.92757579e-06
Iter: 1413 loss: 1.94039058e-06
Iter: 1414 loss: 1.92726611e-06
Iter: 1415 loss: 1.9254544e-06
Iter: 1416 loss: 1.92955417e-06
Iter: 1417 loss: 1.92480934e-06
Iter: 1418 loss: 1.92306e-06
Iter: 1419 loss: 1.92169068e-06
Iter: 1420 loss: 1.92114499e-06
Iter: 1421 loss: 1.91869049e-06
Iter: 1422 loss: 1.94123209e-06
Iter: 1423 loss: 1.91858203e-06
Iter: 1424 loss: 1.91602408e-06
Iter: 1425 loss: 1.91938761e-06
Iter: 1426 loss: 1.9147592e-06
Iter: 1427 loss: 1.91267918e-06
Iter: 1428 loss: 1.91082972e-06
Iter: 1429 loss: 1.91028084e-06
Iter: 1430 loss: 1.90806236e-06
Iter: 1431 loss: 1.90809487e-06
Iter: 1432 loss: 1.90572484e-06
Iter: 1433 loss: 1.90902119e-06
Iter: 1434 loss: 1.90458172e-06
Iter: 1435 loss: 1.90292531e-06
Iter: 1436 loss: 1.90509513e-06
Iter: 1437 loss: 1.9020838e-06
Iter: 1438 loss: 1.8997257e-06
Iter: 1439 loss: 1.91233539e-06
Iter: 1440 loss: 1.89938123e-06
Iter: 1441 loss: 1.89771265e-06
Iter: 1442 loss: 1.89532329e-06
Iter: 1443 loss: 1.8952378e-06
Iter: 1444 loss: 1.89255661e-06
Iter: 1445 loss: 1.91432218e-06
Iter: 1446 loss: 1.89244065e-06
Iter: 1447 loss: 1.8900854e-06
Iter: 1448 loss: 1.9036861e-06
Iter: 1449 loss: 1.88988679e-06
Iter: 1450 loss: 1.88826573e-06
Iter: 1451 loss: 1.89180776e-06
Iter: 1452 loss: 1.88764056e-06
Iter: 1453 loss: 1.88579577e-06
Iter: 1454 loss: 1.88391823e-06
Iter: 1455 loss: 1.8835799e-06
Iter: 1456 loss: 1.88132378e-06
Iter: 1457 loss: 1.91122308e-06
Iter: 1458 loss: 1.88132367e-06
Iter: 1459 loss: 1.8792731e-06
Iter: 1460 loss: 1.88314345e-06
Iter: 1461 loss: 1.87842284e-06
Iter: 1462 loss: 1.87644105e-06
Iter: 1463 loss: 1.87432693e-06
Iter: 1464 loss: 1.8739554e-06
Iter: 1465 loss: 1.87125443e-06
Iter: 1466 loss: 1.88413776e-06
Iter: 1467 loss: 1.87072339e-06
Iter: 1468 loss: 1.86873558e-06
Iter: 1469 loss: 1.86871046e-06
Iter: 1470 loss: 1.86733701e-06
Iter: 1471 loss: 1.86478303e-06
Iter: 1472 loss: 1.92111793e-06
Iter: 1473 loss: 1.864816e-06
Iter: 1474 loss: 1.86330726e-06
Iter: 1475 loss: 1.86309978e-06
Iter: 1476 loss: 1.86167063e-06
Iter: 1477 loss: 1.85846795e-06
Iter: 1478 loss: 1.90351841e-06
Iter: 1479 loss: 1.85831755e-06
Iter: 1480 loss: 1.85564738e-06
Iter: 1481 loss: 1.87373826e-06
Iter: 1482 loss: 1.85533565e-06
Iter: 1483 loss: 1.85324848e-06
Iter: 1484 loss: 1.87538922e-06
Iter: 1485 loss: 1.85315184e-06
Iter: 1486 loss: 1.8515891e-06
Iter: 1487 loss: 1.85236183e-06
Iter: 1488 loss: 1.85048498e-06
Iter: 1489 loss: 1.84840133e-06
Iter: 1490 loss: 1.85188208e-06
Iter: 1491 loss: 1.84747557e-06
Iter: 1492 loss: 1.84539169e-06
Iter: 1493 loss: 1.8511048e-06
Iter: 1494 loss: 1.84470696e-06
Iter: 1495 loss: 1.84272244e-06
Iter: 1496 loss: 1.86088869e-06
Iter: 1497 loss: 1.84260921e-06
Iter: 1498 loss: 1.84124735e-06
Iter: 1499 loss: 1.8385424e-06
Iter: 1500 loss: 1.8933049e-06
Iter: 1501 loss: 1.83857094e-06
Iter: 1502 loss: 1.83555233e-06
Iter: 1503 loss: 1.84634848e-06
Iter: 1504 loss: 1.83469228e-06
Iter: 1505 loss: 1.83337249e-06
Iter: 1506 loss: 1.83314137e-06
Iter: 1507 loss: 1.8317719e-06
Iter: 1508 loss: 1.82961548e-06
Iter: 1509 loss: 1.82958979e-06
Iter: 1510 loss: 1.82765552e-06
Iter: 1511 loss: 1.84912778e-06
Iter: 1512 loss: 1.82761516e-06
Iter: 1513 loss: 1.82567351e-06
Iter: 1514 loss: 1.82639201e-06
Iter: 1515 loss: 1.82430745e-06
Iter: 1516 loss: 1.82251802e-06
Iter: 1517 loss: 1.82052588e-06
Iter: 1518 loss: 1.82024496e-06
Iter: 1519 loss: 1.81859536e-06
Iter: 1520 loss: 1.81838459e-06
Iter: 1521 loss: 1.81668611e-06
Iter: 1522 loss: 1.81627445e-06
Iter: 1523 loss: 1.81522546e-06
Iter: 1524 loss: 1.81306552e-06
Iter: 1525 loss: 1.81847952e-06
Iter: 1526 loss: 1.8123169e-06
Iter: 1527 loss: 1.8099995e-06
Iter: 1528 loss: 1.81504743e-06
Iter: 1529 loss: 1.80912548e-06
Iter: 1530 loss: 1.80747315e-06
Iter: 1531 loss: 1.82704957e-06
Iter: 1532 loss: 1.80744837e-06
Iter: 1533 loss: 1.805956e-06
Iter: 1534 loss: 1.80404163e-06
Iter: 1535 loss: 1.80393681e-06
Iter: 1536 loss: 1.80147845e-06
Iter: 1537 loss: 1.8052441e-06
Iter: 1538 loss: 1.80038649e-06
Iter: 1539 loss: 1.79876463e-06
Iter: 1540 loss: 1.79872859e-06
Iter: 1541 loss: 1.79704193e-06
Iter: 1542 loss: 1.79617314e-06
Iter: 1543 loss: 1.79540666e-06
Iter: 1544 loss: 1.79327196e-06
Iter: 1545 loss: 1.80110817e-06
Iter: 1546 loss: 1.79272683e-06
Iter: 1547 loss: 1.79022697e-06
Iter: 1548 loss: 1.7986539e-06
Iter: 1549 loss: 1.78954701e-06
Iter: 1550 loss: 1.7877303e-06
Iter: 1551 loss: 1.78551954e-06
Iter: 1552 loss: 1.7852874e-06
Iter: 1553 loss: 1.78358948e-06
Iter: 1554 loss: 1.78354549e-06
Iter: 1555 loss: 1.78173309e-06
Iter: 1556 loss: 1.78262076e-06
Iter: 1557 loss: 1.78044161e-06
Iter: 1558 loss: 1.77865695e-06
Iter: 1559 loss: 1.78155233e-06
Iter: 1560 loss: 1.77783318e-06
Iter: 1561 loss: 1.77574066e-06
Iter: 1562 loss: 1.78358482e-06
Iter: 1563 loss: 1.77516506e-06
Iter: 1564 loss: 1.77325433e-06
Iter: 1565 loss: 1.78079426e-06
Iter: 1566 loss: 1.77277923e-06
Iter: 1567 loss: 1.77059701e-06
Iter: 1568 loss: 1.77167794e-06
Iter: 1569 loss: 1.76914261e-06
Iter: 1570 loss: 1.76700041e-06
Iter: 1571 loss: 1.76681988e-06
Iter: 1572 loss: 1.76527669e-06
Iter: 1573 loss: 1.76365222e-06
Iter: 1574 loss: 1.76348976e-06
Iter: 1575 loss: 1.76185074e-06
Iter: 1576 loss: 1.76104891e-06
Iter: 1577 loss: 1.76026992e-06
Iter: 1578 loss: 1.75836556e-06
Iter: 1579 loss: 1.76967387e-06
Iter: 1580 loss: 1.75814182e-06
Iter: 1581 loss: 1.7560975e-06
Iter: 1582 loss: 1.7577986e-06
Iter: 1583 loss: 1.75489754e-06
Iter: 1584 loss: 1.75288255e-06
Iter: 1585 loss: 1.7522575e-06
Iter: 1586 loss: 1.75112132e-06
Iter: 1587 loss: 1.74956983e-06
Iter: 1588 loss: 1.74948582e-06
Iter: 1589 loss: 1.74799118e-06
Iter: 1590 loss: 1.74702279e-06
Iter: 1591 loss: 1.74646095e-06
Iter: 1592 loss: 1.74467698e-06
Iter: 1593 loss: 1.74933871e-06
Iter: 1594 loss: 1.74408865e-06
Iter: 1595 loss: 1.74207685e-06
Iter: 1596 loss: 1.75091111e-06
Iter: 1597 loss: 1.74167053e-06
Iter: 1598 loss: 1.73982585e-06
Iter: 1599 loss: 1.74482102e-06
Iter: 1600 loss: 1.73928368e-06
Iter: 1601 loss: 1.73718968e-06
Iter: 1602 loss: 1.73775561e-06
Iter: 1603 loss: 1.73569617e-06
Iter: 1604 loss: 1.7333042e-06
Iter: 1605 loss: 1.73517219e-06
Iter: 1606 loss: 1.73186072e-06
Iter: 1607 loss: 1.73081e-06
Iter: 1608 loss: 1.73038734e-06
Iter: 1609 loss: 1.72921193e-06
Iter: 1610 loss: 1.72717421e-06
Iter: 1611 loss: 1.72719842e-06
Iter: 1612 loss: 1.7252778e-06
Iter: 1613 loss: 1.75186108e-06
Iter: 1614 loss: 1.72526165e-06
Iter: 1615 loss: 1.72363434e-06
Iter: 1616 loss: 1.72286616e-06
Iter: 1617 loss: 1.72203818e-06
Iter: 1618 loss: 1.72001933e-06
Iter: 1619 loss: 1.72054411e-06
Iter: 1620 loss: 1.71857937e-06
Iter: 1621 loss: 1.71638533e-06
Iter: 1622 loss: 1.71639783e-06
Iter: 1623 loss: 1.71491274e-06
Iter: 1624 loss: 1.71393947e-06
Iter: 1625 loss: 1.71335546e-06
Iter: 1626 loss: 1.71157956e-06
Iter: 1627 loss: 1.71691204e-06
Iter: 1628 loss: 1.71104352e-06
Iter: 1629 loss: 1.70894953e-06
Iter: 1630 loss: 1.71820579e-06
Iter: 1631 loss: 1.70848398e-06
Iter: 1632 loss: 1.70693875e-06
Iter: 1633 loss: 1.71118711e-06
Iter: 1634 loss: 1.70639112e-06
Iter: 1635 loss: 1.70448527e-06
Iter: 1636 loss: 1.70340911e-06
Iter: 1637 loss: 1.7026673e-06
Iter: 1638 loss: 1.70023964e-06
Iter: 1639 loss: 1.70895419e-06
Iter: 1640 loss: 1.69960686e-06
Iter: 1641 loss: 1.69739e-06
Iter: 1642 loss: 1.72685645e-06
Iter: 1643 loss: 1.69734653e-06
Iter: 1644 loss: 1.69599423e-06
Iter: 1645 loss: 1.69457212e-06
Iter: 1646 loss: 1.69431598e-06
Iter: 1647 loss: 1.69261102e-06
Iter: 1648 loss: 1.69261966e-06
Iter: 1649 loss: 1.69159932e-06
Iter: 1650 loss: 1.68940335e-06
Iter: 1651 loss: 1.73279636e-06
Iter: 1652 loss: 1.689388e-06
Iter: 1653 loss: 1.68716201e-06
Iter: 1654 loss: 1.69834937e-06
Iter: 1655 loss: 1.6867433e-06
Iter: 1656 loss: 1.68436259e-06
Iter: 1657 loss: 1.70133706e-06
Iter: 1658 loss: 1.68414147e-06
Iter: 1659 loss: 1.68252825e-06
Iter: 1660 loss: 1.68075303e-06
Iter: 1661 loss: 1.6804637e-06
Iter: 1662 loss: 1.67853511e-06
Iter: 1663 loss: 1.70181329e-06
Iter: 1664 loss: 1.6785425e-06
Iter: 1665 loss: 1.67672965e-06
Iter: 1666 loss: 1.67717201e-06
Iter: 1667 loss: 1.67541498e-06
Iter: 1668 loss: 1.67320877e-06
Iter: 1669 loss: 1.6830312e-06
Iter: 1670 loss: 1.67281826e-06
Iter: 1671 loss: 1.67097733e-06
Iter: 1672 loss: 1.67266296e-06
Iter: 1673 loss: 1.66991822e-06
Iter: 1674 loss: 1.6681729e-06
Iter: 1675 loss: 1.67602991e-06
Iter: 1676 loss: 1.66780444e-06
Iter: 1677 loss: 1.66566554e-06
Iter: 1678 loss: 1.67426765e-06
Iter: 1679 loss: 1.66522284e-06
Iter: 1680 loss: 1.66395466e-06
Iter: 1681 loss: 1.66450752e-06
Iter: 1682 loss: 1.66311258e-06
Iter: 1683 loss: 1.66140012e-06
Iter: 1684 loss: 1.67413225e-06
Iter: 1685 loss: 1.66125665e-06
Iter: 1686 loss: 1.66007499e-06
Iter: 1687 loss: 1.65779784e-06
Iter: 1688 loss: 1.7013532e-06
Iter: 1689 loss: 1.65780045e-06
Iter: 1690 loss: 1.65611857e-06
Iter: 1691 loss: 1.65609424e-06
Iter: 1692 loss: 1.6545016e-06
Iter: 1693 loss: 1.65721235e-06
Iter: 1694 loss: 1.65379788e-06
Iter: 1695 loss: 1.6523727e-06
Iter: 1696 loss: 1.65061101e-06
Iter: 1697 loss: 1.65048664e-06
Iter: 1698 loss: 1.64846062e-06
Iter: 1699 loss: 1.64843595e-06
Iter: 1700 loss: 1.64685753e-06
Iter: 1701 loss: 1.64688902e-06
Iter: 1702 loss: 1.64559049e-06
Iter: 1703 loss: 1.64365235e-06
Iter: 1704 loss: 1.65079803e-06
Iter: 1705 loss: 1.64315441e-06
Iter: 1706 loss: 1.64131484e-06
Iter: 1707 loss: 1.64270182e-06
Iter: 1708 loss: 1.64019571e-06
Iter: 1709 loss: 1.63840969e-06
Iter: 1710 loss: 1.65846609e-06
Iter: 1711 loss: 1.63839309e-06
Iter: 1712 loss: 1.63666641e-06
Iter: 1713 loss: 1.6375252e-06
Iter: 1714 loss: 1.63544178e-06
Iter: 1715 loss: 1.63411642e-06
Iter: 1716 loss: 1.63932646e-06
Iter: 1717 loss: 1.63378729e-06
Iter: 1718 loss: 1.63194545e-06
Iter: 1719 loss: 1.63319032e-06
Iter: 1720 loss: 1.63082791e-06
Iter: 1721 loss: 1.62915512e-06
Iter: 1722 loss: 1.62778883e-06
Iter: 1723 loss: 1.62724609e-06
Iter: 1724 loss: 1.62591141e-06
Iter: 1725 loss: 1.62570313e-06
Iter: 1726 loss: 1.62424749e-06
Iter: 1727 loss: 1.6225747e-06
Iter: 1728 loss: 1.62249012e-06
Iter: 1729 loss: 1.62042306e-06
Iter: 1730 loss: 1.62316292e-06
Iter: 1731 loss: 1.61939795e-06
Iter: 1732 loss: 1.6174165e-06
Iter: 1733 loss: 1.64791265e-06
Iter: 1734 loss: 1.6173941e-06
Iter: 1735 loss: 1.61590958e-06
Iter: 1736 loss: 1.61494609e-06
Iter: 1737 loss: 1.61433434e-06
Iter: 1738 loss: 1.61247794e-06
Iter: 1739 loss: 1.62317315e-06
Iter: 1740 loss: 1.61215564e-06
Iter: 1741 loss: 1.61046046e-06
Iter: 1742 loss: 1.61200546e-06
Iter: 1743 loss: 1.6094267e-06
Iter: 1744 loss: 1.60764944e-06
Iter: 1745 loss: 1.63113077e-06
Iter: 1746 loss: 1.60764739e-06
Iter: 1747 loss: 1.60628235e-06
Iter: 1748 loss: 1.60510194e-06
Iter: 1749 loss: 1.60465765e-06
Iter: 1750 loss: 1.6032775e-06
Iter: 1751 loss: 1.62042647e-06
Iter: 1752 loss: 1.60327613e-06
Iter: 1753 loss: 1.60188358e-06
Iter: 1754 loss: 1.60081663e-06
Iter: 1755 loss: 1.60038906e-06
Iter: 1756 loss: 1.59859155e-06
Iter: 1757 loss: 1.6003462e-06
Iter: 1758 loss: 1.59755609e-06
Iter: 1759 loss: 1.59661386e-06
Iter: 1760 loss: 1.59639308e-06
Iter: 1761 loss: 1.59545777e-06
Iter: 1762 loss: 1.59327146e-06
Iter: 1763 loss: 1.61477e-06
Iter: 1764 loss: 1.59296678e-06
Iter: 1765 loss: 1.59063052e-06
Iter: 1766 loss: 1.60522427e-06
Iter: 1767 loss: 1.59030037e-06
Iter: 1768 loss: 1.58846524e-06
Iter: 1769 loss: 1.61236801e-06
Iter: 1770 loss: 1.58844705e-06
Iter: 1771 loss: 1.58728812e-06
Iter: 1772 loss: 1.58572766e-06
Iter: 1773 loss: 1.58559612e-06
Iter: 1774 loss: 1.58343687e-06
Iter: 1775 loss: 1.59291017e-06
Iter: 1776 loss: 1.58296632e-06
Iter: 1777 loss: 1.58083049e-06
Iter: 1778 loss: 1.58940315e-06
Iter: 1779 loss: 1.58040302e-06
Iter: 1780 loss: 1.57878503e-06
Iter: 1781 loss: 1.5945518e-06
Iter: 1782 loss: 1.57874047e-06
Iter: 1783 loss: 1.57763702e-06
Iter: 1784 loss: 1.57592171e-06
Iter: 1785 loss: 1.5758892e-06
Iter: 1786 loss: 1.57456839e-06
Iter: 1787 loss: 1.57451416e-06
Iter: 1788 loss: 1.57347074e-06
Iter: 1789 loss: 1.57236332e-06
Iter: 1790 loss: 1.5721439e-06
Iter: 1791 loss: 1.57056718e-06
Iter: 1792 loss: 1.57151112e-06
Iter: 1793 loss: 1.56960652e-06
Iter: 1794 loss: 1.56819056e-06
Iter: 1795 loss: 1.56816805e-06
Iter: 1796 loss: 1.56690339e-06
Iter: 1797 loss: 1.56471015e-06
Iter: 1798 loss: 1.56472845e-06
Iter: 1799 loss: 1.56251588e-06
Iter: 1800 loss: 1.5654557e-06
Iter: 1801 loss: 1.56138913e-06
Iter: 1802 loss: 1.56037026e-06
Iter: 1803 loss: 1.56000567e-06
Iter: 1804 loss: 1.55911255e-06
Iter: 1805 loss: 1.55721318e-06
Iter: 1806 loss: 1.59196907e-06
Iter: 1807 loss: 1.55720363e-06
Iter: 1808 loss: 1.55529278e-06
Iter: 1809 loss: 1.5649814e-06
Iter: 1810 loss: 1.55500129e-06
Iter: 1811 loss: 1.55305702e-06
Iter: 1812 loss: 1.56360466e-06
Iter: 1813 loss: 1.55278337e-06
Iter: 1814 loss: 1.55134103e-06
Iter: 1815 loss: 1.56171291e-06
Iter: 1816 loss: 1.55122927e-06
Iter: 1817 loss: 1.55028113e-06
Iter: 1818 loss: 1.54866837e-06
Iter: 1819 loss: 1.54869804e-06
Iter: 1820 loss: 1.54712188e-06
Iter: 1821 loss: 1.57170894e-06
Iter: 1822 loss: 1.54711131e-06
Iter: 1823 loss: 1.54576128e-06
Iter: 1824 loss: 1.54454403e-06
Iter: 1825 loss: 1.54421207e-06
Iter: 1826 loss: 1.54238523e-06
Iter: 1827 loss: 1.54473582e-06
Iter: 1828 loss: 1.54145187e-06
Iter: 1829 loss: 1.54005374e-06
Iter: 1830 loss: 1.54000577e-06
Iter: 1831 loss: 1.53867904e-06
Iter: 1832 loss: 1.53701512e-06
Iter: 1833 loss: 1.53688597e-06
Iter: 1834 loss: 1.53512872e-06
Iter: 1835 loss: 1.53818769e-06
Iter: 1836 loss: 1.53437327e-06
Iter: 1837 loss: 1.5330063e-06
Iter: 1838 loss: 1.53300459e-06
Iter: 1839 loss: 1.53193514e-06
Iter: 1840 loss: 1.52968983e-06
Iter: 1841 loss: 1.56446242e-06
Iter: 1842 loss: 1.52961127e-06
Iter: 1843 loss: 1.52731991e-06
Iter: 1844 loss: 1.54028339e-06
Iter: 1845 loss: 1.52704706e-06
Iter: 1846 loss: 1.52514326e-06
Iter: 1847 loss: 1.54780946e-06
Iter: 1848 loss: 1.5251037e-06
Iter: 1849 loss: 1.52391567e-06
Iter: 1850 loss: 1.52603025e-06
Iter: 1851 loss: 1.52340817e-06
Iter: 1852 loss: 1.52211021e-06
Iter: 1853 loss: 1.52274799e-06
Iter: 1854 loss: 1.52125926e-06
Iter: 1855 loss: 1.52005339e-06
Iter: 1856 loss: 1.53194787e-06
Iter: 1857 loss: 1.5200128e-06
Iter: 1858 loss: 1.51877032e-06
Iter: 1859 loss: 1.51747361e-06
Iter: 1860 loss: 1.51726033e-06
Iter: 1861 loss: 1.51557947e-06
Iter: 1862 loss: 1.51681138e-06
Iter: 1863 loss: 1.51451127e-06
Iter: 1864 loss: 1.51360291e-06
Iter: 1865 loss: 1.51329778e-06
Iter: 1866 loss: 1.51245445e-06
Iter: 1867 loss: 1.51083566e-06
Iter: 1868 loss: 1.54567181e-06
Iter: 1869 loss: 1.51084373e-06
Iter: 1870 loss: 1.50903588e-06
Iter: 1871 loss: 1.51421068e-06
Iter: 1872 loss: 1.50847336e-06
Iter: 1873 loss: 1.50686242e-06
Iter: 1874 loss: 1.52760458e-06
Iter: 1875 loss: 1.50678216e-06
Iter: 1876 loss: 1.50552978e-06
Iter: 1877 loss: 1.50302878e-06
Iter: 1878 loss: 1.55120608e-06
Iter: 1879 loss: 1.5029799e-06
Iter: 1880 loss: 1.50043343e-06
Iter: 1881 loss: 1.51430254e-06
Iter: 1882 loss: 1.5000827e-06
Iter: 1883 loss: 1.49871676e-06
Iter: 1884 loss: 1.4986058e-06
Iter: 1885 loss: 1.49774939e-06
Iter: 1886 loss: 1.49689345e-06
Iter: 1887 loss: 1.4967236e-06
Iter: 1888 loss: 1.49522702e-06
Iter: 1889 loss: 1.50038272e-06
Iter: 1890 loss: 1.49475477e-06
Iter: 1891 loss: 1.49344601e-06
Iter: 1892 loss: 1.49837842e-06
Iter: 1893 loss: 1.49307527e-06
Iter: 1894 loss: 1.49148718e-06
Iter: 1895 loss: 1.49305902e-06
Iter: 1896 loss: 1.49056848e-06
Iter: 1897 loss: 1.48922152e-06
Iter: 1898 loss: 1.48791298e-06
Iter: 1899 loss: 1.4875352e-06
Iter: 1900 loss: 1.48667073e-06
Iter: 1901 loss: 1.48624781e-06
Iter: 1902 loss: 1.48534173e-06
Iter: 1903 loss: 1.48348022e-06
Iter: 1904 loss: 1.51573431e-06
Iter: 1905 loss: 1.48345521e-06
Iter: 1906 loss: 1.48163031e-06
Iter: 1907 loss: 1.48811182e-06
Iter: 1908 loss: 1.48113736e-06
Iter: 1909 loss: 1.47933235e-06
Iter: 1910 loss: 1.49902212e-06
Iter: 1911 loss: 1.47927108e-06
Iter: 1912 loss: 1.47793673e-06
Iter: 1913 loss: 1.47605988e-06
Iter: 1914 loss: 1.47598712e-06
Iter: 1915 loss: 1.47386163e-06
Iter: 1916 loss: 1.48212871e-06
Iter: 1917 loss: 1.47338596e-06
Iter: 1918 loss: 1.47215178e-06
Iter: 1919 loss: 1.47206492e-06
Iter: 1920 loss: 1.4712009e-06
Iter: 1921 loss: 1.46955767e-06
Iter: 1922 loss: 1.50536152e-06
Iter: 1923 loss: 1.46960144e-06
Iter: 1924 loss: 1.46768105e-06
Iter: 1925 loss: 1.4810164e-06
Iter: 1926 loss: 1.46749801e-06
Iter: 1927 loss: 1.46584989e-06
Iter: 1928 loss: 1.47010587e-06
Iter: 1929 loss: 1.46527464e-06
Iter: 1930 loss: 1.46376658e-06
Iter: 1931 loss: 1.4686816e-06
Iter: 1932 loss: 1.46335e-06
Iter: 1933 loss: 1.46214063e-06
Iter: 1934 loss: 1.46139473e-06
Iter: 1935 loss: 1.46091384e-06
Iter: 1936 loss: 1.4595588e-06
Iter: 1937 loss: 1.45958052e-06
Iter: 1938 loss: 1.45830791e-06
Iter: 1939 loss: 1.45788738e-06
Iter: 1940 loss: 1.4572006e-06
Iter: 1941 loss: 1.45575439e-06
Iter: 1942 loss: 1.45514389e-06
Iter: 1943 loss: 1.45440242e-06
Iter: 1944 loss: 1.45330023e-06
Iter: 1945 loss: 1.45313425e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi1.2
+ date
Wed Nov  4 13:28:52 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi0.8/300_300_300_1 --function f2 --psi 3 --alpha 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec9a672f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec9954620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec99b4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2f86378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2f86950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2f7e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2f39a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2f19400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2f19510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2ef16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2ef1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2ef1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2d9a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2d9a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2e7e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2e05d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2e05b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2ce78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2cb4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2ce7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2ce7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2c79bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2c79158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2d928c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2d92c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2d548c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2ba40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec9a67d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2ba4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2b8e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2c0f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2c0f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2b1eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2bfabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2aea9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ec2afe048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.013815723
test_loss: 0.01491253
train_loss: 0.00984725
test_loss: 0.01150631
train_loss: 0.00923358
test_loss: 0.010591181
train_loss: 0.007847695
test_loss: 0.009941868
train_loss: 0.007879742
test_loss: 0.009873338
train_loss: 0.007235298
test_loss: 0.009280217
train_loss: 0.0067365924
test_loss: 0.00898188
train_loss: 0.006442372
test_loss: 0.009051473
train_loss: 0.0074100783
test_loss: 0.009336254
train_loss: 0.0069748126
test_loss: 0.008851035
train_loss: 0.0070353923
test_loss: 0.00871047
train_loss: 0.006763545
test_loss: 0.008802588
train_loss: 0.00628087
test_loss: 0.0087936595
train_loss: 0.006351481
test_loss: 0.008379351
train_loss: 0.006365211
test_loss: 0.0084219575
train_loss: 0.006141246
test_loss: 0.008476148
train_loss: 0.006083629
test_loss: 0.008361471
train_loss: 0.006438194
test_loss: 0.0082779545
train_loss: 0.006141862
test_loss: 0.008221794
train_loss: 0.0061298693
test_loss: 0.008211876
train_loss: 0.005971778
test_loss: 0.008202636
train_loss: 0.006251893
test_loss: 0.008191799
train_loss: 0.006085142
test_loss: 0.00814431
train_loss: 0.0057148915
test_loss: 0.008342349
train_loss: 0.0060223234
test_loss: 0.008050279
train_loss: 0.005503491
test_loss: 0.008067234
train_loss: 0.0057717436
test_loss: 0.0078729335
train_loss: 0.0057350136
test_loss: 0.007840964
train_loss: 0.0056146197
test_loss: 0.0077357413
train_loss: 0.0058384207
test_loss: 0.008132091
train_loss: 0.005819233
test_loss: 0.0077302316
train_loss: 0.0060943775
test_loss: 0.0078995805
train_loss: 0.0058807964
test_loss: 0.007937526
train_loss: 0.0061141932
test_loss: 0.008049113
train_loss: 0.005249797
test_loss: 0.007592205
train_loss: 0.005529822
test_loss: 0.007631733
train_loss: 0.0055440865
test_loss: 0.007907353
train_loss: 0.005265454
test_loss: 0.007704525
train_loss: 0.0053605624
test_loss: 0.0076605035
train_loss: 0.005339086
test_loss: 0.0077722888
train_loss: 0.0055864556
test_loss: 0.007786543
train_loss: 0.0056499024
test_loss: 0.007791215
train_loss: 0.005307954
test_loss: 0.0075702113
train_loss: 0.005167991
test_loss: 0.0075793727
train_loss: 0.0054153823
test_loss: 0.007861357
train_loss: 0.0050447076
test_loss: 0.007671745
train_loss: 0.0049614427
test_loss: 0.0076605184
train_loss: 0.0055452664
test_loss: 0.0078549255
train_loss: 0.0050741825
test_loss: 0.0076414766
train_loss: 0.0050152335
test_loss: 0.007539129
train_loss: 0.00533117
test_loss: 0.007589284
train_loss: 0.0053347605
test_loss: 0.007528063
train_loss: 0.005129825
test_loss: 0.007712998
train_loss: 0.0052120783
test_loss: 0.007503924
train_loss: 0.005085388
test_loss: 0.007531097
train_loss: 0.0048935497
test_loss: 0.0073092715
train_loss: 0.005286094
test_loss: 0.0075272005
train_loss: 0.005149961
test_loss: 0.0075538456
train_loss: 0.005394924
test_loss: 0.0075742537
train_loss: 0.00549074
test_loss: 0.0075572995
train_loss: 0.004988912
test_loss: 0.007457156
train_loss: 0.0052553723
test_loss: 0.0076128826
train_loss: 0.0051241918
test_loss: 0.007480437
train_loss: 0.0051681777
test_loss: 0.0074780416
train_loss: 0.0049660676
test_loss: 0.0076356716
train_loss: 0.005145789
test_loss: 0.007618125
train_loss: 0.0051881857
test_loss: 0.007504233
train_loss: 0.004870259
test_loss: 0.0073619043
train_loss: 0.0052098962
test_loss: 0.0075136484
train_loss: 0.0049802107
test_loss: 0.0075851292
train_loss: 0.0051164827
test_loss: 0.007376783
train_loss: 0.0052231923
test_loss: 0.007332924
train_loss: 0.0050518154
test_loss: 0.007285728
train_loss: 0.0050069466
test_loss: 0.007697845
train_loss: 0.005088983
test_loss: 0.007489571
train_loss: 0.0051594614
test_loss: 0.007859179
train_loss: 0.0051301597
test_loss: 0.007505756
train_loss: 0.00480277
test_loss: 0.0072379634
train_loss: 0.005064872
test_loss: 0.0074880864
train_loss: 0.005220919
test_loss: 0.0076117357
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.2/300_300_300_1 --optimizer lbfgs --function f2 --psi 3 --alpha 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df6290d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df6bfea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df59c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df543400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df5438c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df552730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df552950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df512840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d8318400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df552840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d82d67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d8296268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d82c49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d82c4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d826f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d826f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d82466a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d81d3b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d8198840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d8198598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d8170048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d8170840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d81346a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d814c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d81337b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d8098a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df52b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df52b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2df52b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d806aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d806ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d806a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d7fe7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d8009840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d7fe7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d7fbfe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.47242157e-05
Iter: 2 loss: 4.85341261e-05
Iter: 3 loss: 0.000384606596
Iter: 4 loss: 4.85281635e-05
Iter: 5 loss: 4.27567429e-05
Iter: 6 loss: 4.62053686e-05
Iter: 7 loss: 3.90725218e-05
Iter: 8 loss: 3.33524804e-05
Iter: 9 loss: 5.90668642e-05
Iter: 10 loss: 3.22190826e-05
Iter: 11 loss: 2.8917646e-05
Iter: 12 loss: 3.69543486e-05
Iter: 13 loss: 2.77382351e-05
Iter: 14 loss: 2.57489774e-05
Iter: 15 loss: 4.81200041e-05
Iter: 16 loss: 2.57125321e-05
Iter: 17 loss: 2.41951348e-05
Iter: 18 loss: 2.37852983e-05
Iter: 19 loss: 2.28458048e-05
Iter: 20 loss: 2.12728355e-05
Iter: 21 loss: 3.10461874e-05
Iter: 22 loss: 2.10891958e-05
Iter: 23 loss: 1.95622451e-05
Iter: 24 loss: 2.36228916e-05
Iter: 25 loss: 1.90546125e-05
Iter: 26 loss: 1.82493495e-05
Iter: 27 loss: 1.78068731e-05
Iter: 28 loss: 1.74500165e-05
Iter: 29 loss: 1.66767604e-05
Iter: 30 loss: 2.03711479e-05
Iter: 31 loss: 1.65383026e-05
Iter: 32 loss: 1.5817277e-05
Iter: 33 loss: 2.05135293e-05
Iter: 34 loss: 1.57406939e-05
Iter: 35 loss: 1.52925932e-05
Iter: 36 loss: 1.46319298e-05
Iter: 37 loss: 1.4615156e-05
Iter: 38 loss: 1.405285e-05
Iter: 39 loss: 2.16273384e-05
Iter: 40 loss: 1.4050438e-05
Iter: 41 loss: 1.36567751e-05
Iter: 42 loss: 1.60994514e-05
Iter: 43 loss: 1.36103008e-05
Iter: 44 loss: 1.33168114e-05
Iter: 45 loss: 1.40403636e-05
Iter: 46 loss: 1.32126861e-05
Iter: 47 loss: 1.29003656e-05
Iter: 48 loss: 1.37565657e-05
Iter: 49 loss: 1.27983567e-05
Iter: 50 loss: 1.2575365e-05
Iter: 51 loss: 1.40619859e-05
Iter: 52 loss: 1.25526331e-05
Iter: 53 loss: 1.23324298e-05
Iter: 54 loss: 1.22042011e-05
Iter: 55 loss: 1.21109269e-05
Iter: 56 loss: 1.19027191e-05
Iter: 57 loss: 1.47601031e-05
Iter: 58 loss: 1.19020606e-05
Iter: 59 loss: 1.17237769e-05
Iter: 60 loss: 1.16812116e-05
Iter: 61 loss: 1.15678586e-05
Iter: 62 loss: 1.13602318e-05
Iter: 63 loss: 1.15564881e-05
Iter: 64 loss: 1.12413936e-05
Iter: 65 loss: 1.10328901e-05
Iter: 66 loss: 1.32581972e-05
Iter: 67 loss: 1.10277952e-05
Iter: 68 loss: 1.0851888e-05
Iter: 69 loss: 1.1148195e-05
Iter: 70 loss: 1.07728611e-05
Iter: 71 loss: 1.06148236e-05
Iter: 72 loss: 1.13093211e-05
Iter: 73 loss: 1.05829495e-05
Iter: 74 loss: 1.04591754e-05
Iter: 75 loss: 1.08774e-05
Iter: 76 loss: 1.04256023e-05
Iter: 77 loss: 1.03052462e-05
Iter: 78 loss: 1.05779445e-05
Iter: 79 loss: 1.02599843e-05
Iter: 80 loss: 1.01416354e-05
Iter: 81 loss: 1.06131829e-05
Iter: 82 loss: 1.01148526e-05
Iter: 83 loss: 1.00117377e-05
Iter: 84 loss: 1.02238273e-05
Iter: 85 loss: 9.97029e-06
Iter: 86 loss: 9.86514169e-06
Iter: 87 loss: 1.03680377e-05
Iter: 88 loss: 9.84637609e-06
Iter: 89 loss: 9.7550228e-06
Iter: 90 loss: 9.76760111e-06
Iter: 91 loss: 9.68564382e-06
Iter: 92 loss: 9.58310102e-06
Iter: 93 loss: 1.06329417e-05
Iter: 94 loss: 9.57995e-06
Iter: 95 loss: 9.50267713e-06
Iter: 96 loss: 9.44186831e-06
Iter: 97 loss: 9.41775943e-06
Iter: 98 loss: 9.31922386e-06
Iter: 99 loss: 9.77603486e-06
Iter: 100 loss: 9.30076749e-06
Iter: 101 loss: 9.2189066e-06
Iter: 102 loss: 9.8479486e-06
Iter: 103 loss: 9.21296851e-06
Iter: 104 loss: 9.14321754e-06
Iter: 105 loss: 9.15755663e-06
Iter: 106 loss: 9.0916883e-06
Iter: 107 loss: 9.01162184e-06
Iter: 108 loss: 9.57496195e-06
Iter: 109 loss: 9.00419855e-06
Iter: 110 loss: 8.93398e-06
Iter: 111 loss: 9.0298654e-06
Iter: 112 loss: 8.89882631e-06
Iter: 113 loss: 8.83422217e-06
Iter: 114 loss: 9.15401142e-06
Iter: 115 loss: 8.82319273e-06
Iter: 116 loss: 8.76453214e-06
Iter: 117 loss: 8.84998462e-06
Iter: 118 loss: 8.73596855e-06
Iter: 119 loss: 8.6730206e-06
Iter: 120 loss: 8.94123787e-06
Iter: 121 loss: 8.65977472e-06
Iter: 122 loss: 8.6023847e-06
Iter: 123 loss: 8.71730117e-06
Iter: 124 loss: 8.57895247e-06
Iter: 125 loss: 8.52420817e-06
Iter: 126 loss: 8.65667153e-06
Iter: 127 loss: 8.50462311e-06
Iter: 128 loss: 8.439456e-06
Iter: 129 loss: 8.63263631e-06
Iter: 130 loss: 8.41941528e-06
Iter: 131 loss: 8.36813797e-06
Iter: 132 loss: 8.35876e-06
Iter: 133 loss: 8.32390651e-06
Iter: 134 loss: 8.26303949e-06
Iter: 135 loss: 8.84742622e-06
Iter: 136 loss: 8.26067662e-06
Iter: 137 loss: 8.20981e-06
Iter: 138 loss: 8.35718856e-06
Iter: 139 loss: 8.19404158e-06
Iter: 140 loss: 8.14923624e-06
Iter: 141 loss: 8.22350194e-06
Iter: 142 loss: 8.12879625e-06
Iter: 143 loss: 8.0771988e-06
Iter: 144 loss: 8.37220614e-06
Iter: 145 loss: 8.07028391e-06
Iter: 146 loss: 8.03279272e-06
Iter: 147 loss: 8.05672607e-06
Iter: 148 loss: 8.00898306e-06
Iter: 149 loss: 7.96037057e-06
Iter: 150 loss: 8.1550188e-06
Iter: 151 loss: 7.94955304e-06
Iter: 152 loss: 7.90378908e-06
Iter: 153 loss: 7.98388555e-06
Iter: 154 loss: 7.88375382e-06
Iter: 155 loss: 7.83847645e-06
Iter: 156 loss: 8.11089285e-06
Iter: 157 loss: 7.83277937e-06
Iter: 158 loss: 7.80140545e-06
Iter: 159 loss: 7.82307143e-06
Iter: 160 loss: 7.78195408e-06
Iter: 161 loss: 7.74065666e-06
Iter: 162 loss: 7.93777508e-06
Iter: 163 loss: 7.7332279e-06
Iter: 164 loss: 7.69695453e-06
Iter: 165 loss: 7.71626219e-06
Iter: 166 loss: 7.67304482e-06
Iter: 167 loss: 7.63690514e-06
Iter: 168 loss: 7.72128533e-06
Iter: 169 loss: 7.62371792e-06
Iter: 170 loss: 7.58321085e-06
Iter: 171 loss: 7.85474549e-06
Iter: 172 loss: 7.57908947e-06
Iter: 173 loss: 7.5459875e-06
Iter: 174 loss: 7.56575628e-06
Iter: 175 loss: 7.52429514e-06
Iter: 176 loss: 7.48922957e-06
Iter: 177 loss: 7.80430946e-06
Iter: 178 loss: 7.48749062e-06
Iter: 179 loss: 7.45920534e-06
Iter: 180 loss: 7.46310207e-06
Iter: 181 loss: 7.43775854e-06
Iter: 182 loss: 7.4028917e-06
Iter: 183 loss: 7.54830944e-06
Iter: 184 loss: 7.39540837e-06
Iter: 185 loss: 7.36197126e-06
Iter: 186 loss: 7.45348916e-06
Iter: 187 loss: 7.35098865e-06
Iter: 188 loss: 7.32362e-06
Iter: 189 loss: 7.44753288e-06
Iter: 190 loss: 7.31828186e-06
Iter: 191 loss: 7.28948089e-06
Iter: 192 loss: 7.27874885e-06
Iter: 193 loss: 7.26286908e-06
Iter: 194 loss: 7.22709228e-06
Iter: 195 loss: 7.54798248e-06
Iter: 196 loss: 7.2252974e-06
Iter: 197 loss: 7.19753e-06
Iter: 198 loss: 7.23446965e-06
Iter: 199 loss: 7.1835957e-06
Iter: 200 loss: 7.15519718e-06
Iter: 201 loss: 7.16043814e-06
Iter: 202 loss: 7.1338136e-06
Iter: 203 loss: 7.100939e-06
Iter: 204 loss: 7.43559485e-06
Iter: 205 loss: 7.09983578e-06
Iter: 206 loss: 7.07206254e-06
Iter: 207 loss: 7.11314215e-06
Iter: 208 loss: 7.05857065e-06
Iter: 209 loss: 7.03483511e-06
Iter: 210 loss: 7.17692774e-06
Iter: 211 loss: 7.03192291e-06
Iter: 212 loss: 7.00830878e-06
Iter: 213 loss: 7.0306296e-06
Iter: 214 loss: 6.99483417e-06
Iter: 215 loss: 6.96861434e-06
Iter: 216 loss: 7.02233774e-06
Iter: 217 loss: 6.95809285e-06
Iter: 218 loss: 6.93165111e-06
Iter: 219 loss: 7.08532116e-06
Iter: 220 loss: 6.92805588e-06
Iter: 221 loss: 6.90688285e-06
Iter: 222 loss: 6.92537697e-06
Iter: 223 loss: 6.89440958e-06
Iter: 224 loss: 6.86394242e-06
Iter: 225 loss: 6.93645234e-06
Iter: 226 loss: 6.85279338e-06
Iter: 227 loss: 6.82758127e-06
Iter: 228 loss: 6.91544528e-06
Iter: 229 loss: 6.82094196e-06
Iter: 230 loss: 6.79548339e-06
Iter: 231 loss: 6.87659212e-06
Iter: 232 loss: 6.78813103e-06
Iter: 233 loss: 6.76397212e-06
Iter: 234 loss: 6.75353203e-06
Iter: 235 loss: 6.74126e-06
Iter: 236 loss: 6.71156295e-06
Iter: 237 loss: 6.95144945e-06
Iter: 238 loss: 6.70969303e-06
Iter: 239 loss: 6.68590519e-06
Iter: 240 loss: 6.79441882e-06
Iter: 241 loss: 6.68133816e-06
Iter: 242 loss: 6.66176265e-06
Iter: 243 loss: 6.68175153e-06
Iter: 244 loss: 6.65074549e-06
Iter: 245 loss: 6.62616458e-06
Iter: 246 loss: 6.76950094e-06
Iter: 247 loss: 6.62298771e-06
Iter: 248 loss: 6.60412479e-06
Iter: 249 loss: 6.60181104e-06
Iter: 250 loss: 6.58834506e-06
Iter: 251 loss: 6.56309248e-06
Iter: 252 loss: 6.72513306e-06
Iter: 253 loss: 6.56028533e-06
Iter: 254 loss: 6.53885536e-06
Iter: 255 loss: 6.5676445e-06
Iter: 256 loss: 6.52821382e-06
Iter: 257 loss: 6.50485117e-06
Iter: 258 loss: 6.61055765e-06
Iter: 259 loss: 6.50034235e-06
Iter: 260 loss: 6.48065588e-06
Iter: 261 loss: 6.49895583e-06
Iter: 262 loss: 6.46914123e-06
Iter: 263 loss: 6.4488604e-06
Iter: 264 loss: 6.60453043e-06
Iter: 265 loss: 6.44711554e-06
Iter: 266 loss: 6.43008161e-06
Iter: 267 loss: 6.41980068e-06
Iter: 268 loss: 6.41280258e-06
Iter: 269 loss: 6.39028804e-06
Iter: 270 loss: 6.4852e-06
Iter: 271 loss: 6.38563506e-06
Iter: 272 loss: 6.36395907e-06
Iter: 273 loss: 6.47301931e-06
Iter: 274 loss: 6.36055847e-06
Iter: 275 loss: 6.33980608e-06
Iter: 276 loss: 6.35955303e-06
Iter: 277 loss: 6.32804085e-06
Iter: 278 loss: 6.30807108e-06
Iter: 279 loss: 6.4902124e-06
Iter: 280 loss: 6.30710656e-06
Iter: 281 loss: 6.29072292e-06
Iter: 282 loss: 6.28397083e-06
Iter: 283 loss: 6.27525424e-06
Iter: 284 loss: 6.25445273e-06
Iter: 285 loss: 6.37422545e-06
Iter: 286 loss: 6.25161374e-06
Iter: 287 loss: 6.23225333e-06
Iter: 288 loss: 6.27430381e-06
Iter: 289 loss: 6.22498828e-06
Iter: 290 loss: 6.20683113e-06
Iter: 291 loss: 6.2718259e-06
Iter: 292 loss: 6.20243782e-06
Iter: 293 loss: 6.18430431e-06
Iter: 294 loss: 6.20646415e-06
Iter: 295 loss: 6.17489695e-06
Iter: 296 loss: 6.15646968e-06
Iter: 297 loss: 6.23710775e-06
Iter: 298 loss: 6.15271665e-06
Iter: 299 loss: 6.13391967e-06
Iter: 300 loss: 6.16493e-06
Iter: 301 loss: 6.12550684e-06
Iter: 302 loss: 6.10890675e-06
Iter: 303 loss: 6.11823634e-06
Iter: 304 loss: 6.09817062e-06
Iter: 305 loss: 6.07659513e-06
Iter: 306 loss: 6.218821e-06
Iter: 307 loss: 6.07431684e-06
Iter: 308 loss: 6.05514e-06
Iter: 309 loss: 6.09763447e-06
Iter: 310 loss: 6.04762317e-06
Iter: 311 loss: 6.0313323e-06
Iter: 312 loss: 6.12234453e-06
Iter: 313 loss: 6.0289558e-06
Iter: 314 loss: 6.01300962e-06
Iter: 315 loss: 6.02924865e-06
Iter: 316 loss: 6.00418116e-06
Iter: 317 loss: 5.98823499e-06
Iter: 318 loss: 6.0241e-06
Iter: 319 loss: 5.98225415e-06
Iter: 320 loss: 5.9652607e-06
Iter: 321 loss: 6.04808429e-06
Iter: 322 loss: 5.96233713e-06
Iter: 323 loss: 5.94775338e-06
Iter: 324 loss: 5.96366317e-06
Iter: 325 loss: 5.9400254e-06
Iter: 326 loss: 5.92136e-06
Iter: 327 loss: 5.9742415e-06
Iter: 328 loss: 5.91547268e-06
Iter: 329 loss: 5.89842784e-06
Iter: 330 loss: 5.93030336e-06
Iter: 331 loss: 5.89141109e-06
Iter: 332 loss: 5.87125942e-06
Iter: 333 loss: 5.94853782e-06
Iter: 334 loss: 5.8665687e-06
Iter: 335 loss: 5.84984809e-06
Iter: 336 loss: 5.85225098e-06
Iter: 337 loss: 5.83735027e-06
Iter: 338 loss: 5.81986933e-06
Iter: 339 loss: 5.95128768e-06
Iter: 340 loss: 5.81864606e-06
Iter: 341 loss: 5.80232518e-06
Iter: 342 loss: 5.84171312e-06
Iter: 343 loss: 5.79648531e-06
Iter: 344 loss: 5.78154413e-06
Iter: 345 loss: 5.83469318e-06
Iter: 346 loss: 5.77779247e-06
Iter: 347 loss: 5.76306411e-06
Iter: 348 loss: 5.81682798e-06
Iter: 349 loss: 5.75942977e-06
Iter: 350 loss: 5.74665228e-06
Iter: 351 loss: 5.74567139e-06
Iter: 352 loss: 5.73616035e-06
Iter: 353 loss: 5.71966575e-06
Iter: 354 loss: 5.84624e-06
Iter: 355 loss: 5.71853798e-06
Iter: 356 loss: 5.70488191e-06
Iter: 357 loss: 5.71635246e-06
Iter: 358 loss: 5.69694475e-06
Iter: 359 loss: 5.68079531e-06
Iter: 360 loss: 5.74742035e-06
Iter: 361 loss: 5.67721236e-06
Iter: 362 loss: 5.66240396e-06
Iter: 363 loss: 5.67489315e-06
Iter: 364 loss: 5.65372e-06
Iter: 365 loss: 5.6374497e-06
Iter: 366 loss: 5.74887963e-06
Iter: 367 loss: 5.63572939e-06
Iter: 368 loss: 5.62214245e-06
Iter: 369 loss: 5.61692286e-06
Iter: 370 loss: 5.60953094e-06
Iter: 371 loss: 5.59218415e-06
Iter: 372 loss: 5.66767449e-06
Iter: 373 loss: 5.58866532e-06
Iter: 374 loss: 5.57383828e-06
Iter: 375 loss: 5.68485848e-06
Iter: 376 loss: 5.57275735e-06
Iter: 377 loss: 5.56127361e-06
Iter: 378 loss: 5.56536361e-06
Iter: 379 loss: 5.55302813e-06
Iter: 380 loss: 5.5369137e-06
Iter: 381 loss: 5.63404774e-06
Iter: 382 loss: 5.53473046e-06
Iter: 383 loss: 5.52220445e-06
Iter: 384 loss: 5.52724123e-06
Iter: 385 loss: 5.51340418e-06
Iter: 386 loss: 5.49919332e-06
Iter: 387 loss: 5.562767e-06
Iter: 388 loss: 5.49630977e-06
Iter: 389 loss: 5.48110256e-06
Iter: 390 loss: 5.50546338e-06
Iter: 391 loss: 5.47398804e-06
Iter: 392 loss: 5.45978764e-06
Iter: 393 loss: 5.53005702e-06
Iter: 394 loss: 5.45733383e-06
Iter: 395 loss: 5.44514023e-06
Iter: 396 loss: 5.46215597e-06
Iter: 397 loss: 5.43913666e-06
Iter: 398 loss: 5.426391e-06
Iter: 399 loss: 5.47363243e-06
Iter: 400 loss: 5.42311454e-06
Iter: 401 loss: 5.4090915e-06
Iter: 402 loss: 5.4238044e-06
Iter: 403 loss: 5.40130077e-06
Iter: 404 loss: 5.38777567e-06
Iter: 405 loss: 5.40962719e-06
Iter: 406 loss: 5.38157474e-06
Iter: 407 loss: 5.36748485e-06
Iter: 408 loss: 5.47747914e-06
Iter: 409 loss: 5.36643802e-06
Iter: 410 loss: 5.35357685e-06
Iter: 411 loss: 5.36032167e-06
Iter: 412 loss: 5.34507762e-06
Iter: 413 loss: 5.33190268e-06
Iter: 414 loss: 5.46608771e-06
Iter: 415 loss: 5.33166e-06
Iter: 416 loss: 5.32156901e-06
Iter: 417 loss: 5.32058584e-06
Iter: 418 loss: 5.31320484e-06
Iter: 419 loss: 5.29975205e-06
Iter: 420 loss: 5.33022376e-06
Iter: 421 loss: 5.29461204e-06
Iter: 422 loss: 5.27908696e-06
Iter: 423 loss: 5.35242771e-06
Iter: 424 loss: 5.27631346e-06
Iter: 425 loss: 5.26447548e-06
Iter: 426 loss: 5.28853343e-06
Iter: 427 loss: 5.25989708e-06
Iter: 428 loss: 5.24717052e-06
Iter: 429 loss: 5.28511373e-06
Iter: 430 loss: 5.24344796e-06
Iter: 431 loss: 5.23188419e-06
Iter: 432 loss: 5.25021642e-06
Iter: 433 loss: 5.22635582e-06
Iter: 434 loss: 5.21197944e-06
Iter: 435 loss: 5.25977794e-06
Iter: 436 loss: 5.20793446e-06
Iter: 437 loss: 5.19564583e-06
Iter: 438 loss: 5.19856576e-06
Iter: 439 loss: 5.18654178e-06
Iter: 440 loss: 5.17203671e-06
Iter: 441 loss: 5.26728854e-06
Iter: 442 loss: 5.17060653e-06
Iter: 443 loss: 5.15581e-06
Iter: 444 loss: 5.17983563e-06
Iter: 445 loss: 5.14901694e-06
Iter: 446 loss: 5.13634495e-06
Iter: 447 loss: 5.22916707e-06
Iter: 448 loss: 5.13530358e-06
Iter: 449 loss: 5.12473798e-06
Iter: 450 loss: 5.14267413e-06
Iter: 451 loss: 5.11992357e-06
Iter: 452 loss: 5.10954942e-06
Iter: 453 loss: 5.11424787e-06
Iter: 454 loss: 5.10250538e-06
Iter: 455 loss: 5.08837456e-06
Iter: 456 loss: 5.17131866e-06
Iter: 457 loss: 5.08656922e-06
Iter: 458 loss: 5.07478035e-06
Iter: 459 loss: 5.09242864e-06
Iter: 460 loss: 5.06930337e-06
Iter: 461 loss: 5.05740718e-06
Iter: 462 loss: 5.1058696e-06
Iter: 463 loss: 5.05476146e-06
Iter: 464 loss: 5.04339e-06
Iter: 465 loss: 5.05259959e-06
Iter: 466 loss: 5.03668161e-06
Iter: 467 loss: 5.02428111e-06
Iter: 468 loss: 5.11505914e-06
Iter: 469 loss: 5.02335479e-06
Iter: 470 loss: 5.01429577e-06
Iter: 471 loss: 5.01314071e-06
Iter: 472 loss: 5.00666602e-06
Iter: 473 loss: 4.99477846e-06
Iter: 474 loss: 5.03143747e-06
Iter: 475 loss: 4.99130238e-06
Iter: 476 loss: 4.97744759e-06
Iter: 477 loss: 5.03684805e-06
Iter: 478 loss: 4.9746468e-06
Iter: 479 loss: 4.96346e-06
Iter: 480 loss: 4.99306771e-06
Iter: 481 loss: 4.95990753e-06
Iter: 482 loss: 4.94791811e-06
Iter: 483 loss: 5.00094529e-06
Iter: 484 loss: 4.94572578e-06
Iter: 485 loss: 4.93635343e-06
Iter: 486 loss: 4.93393782e-06
Iter: 487 loss: 4.92811e-06
Iter: 488 loss: 4.91597939e-06
Iter: 489 loss: 5.01338809e-06
Iter: 490 loss: 4.9152768e-06
Iter: 491 loss: 4.90542971e-06
Iter: 492 loss: 4.92054096e-06
Iter: 493 loss: 4.90070124e-06
Iter: 494 loss: 4.89085505e-06
Iter: 495 loss: 4.92472964e-06
Iter: 496 loss: 4.88832302e-06
Iter: 497 loss: 4.87827e-06
Iter: 498 loss: 4.89346758e-06
Iter: 499 loss: 4.87360376e-06
Iter: 500 loss: 4.8631664e-06
Iter: 501 loss: 4.90230832e-06
Iter: 502 loss: 4.86070485e-06
Iter: 503 loss: 4.84972952e-06
Iter: 504 loss: 4.85809778e-06
Iter: 505 loss: 4.84293741e-06
Iter: 506 loss: 4.83172562e-06
Iter: 507 loss: 4.85531109e-06
Iter: 508 loss: 4.82712585e-06
Iter: 509 loss: 4.81508368e-06
Iter: 510 loss: 4.89952527e-06
Iter: 511 loss: 4.81394636e-06
Iter: 512 loss: 4.80392464e-06
Iter: 513 loss: 4.81608e-06
Iter: 514 loss: 4.79853679e-06
Iter: 515 loss: 4.78868606e-06
Iter: 516 loss: 4.88054184e-06
Iter: 517 loss: 4.78831134e-06
Iter: 518 loss: 4.7809e-06
Iter: 519 loss: 4.7745807e-06
Iter: 520 loss: 4.77260392e-06
Iter: 521 loss: 4.76130663e-06
Iter: 522 loss: 4.82087216e-06
Iter: 523 loss: 4.75957449e-06
Iter: 524 loss: 4.74886656e-06
Iter: 525 loss: 4.78284119e-06
Iter: 526 loss: 4.74582794e-06
Iter: 527 loss: 4.73691944e-06
Iter: 528 loss: 4.75714796e-06
Iter: 529 loss: 4.73352702e-06
Iter: 530 loss: 4.72371084e-06
Iter: 531 loss: 4.74267654e-06
Iter: 532 loss: 4.71958174e-06
Iter: 533 loss: 4.70910891e-06
Iter: 534 loss: 4.73516138e-06
Iter: 535 loss: 4.7054491e-06
Iter: 536 loss: 4.6946343e-06
Iter: 537 loss: 4.73588261e-06
Iter: 538 loss: 4.69216775e-06
Iter: 539 loss: 4.68303369e-06
Iter: 540 loss: 4.67768496e-06
Iter: 541 loss: 4.67391055e-06
Iter: 542 loss: 4.66047823e-06
Iter: 543 loss: 4.7793219e-06
Iter: 544 loss: 4.65978792e-06
Iter: 545 loss: 4.64870027e-06
Iter: 546 loss: 4.67386235e-06
Iter: 547 loss: 4.64455752e-06
Iter: 548 loss: 4.63566357e-06
Iter: 549 loss: 4.69836777e-06
Iter: 550 loss: 4.63480455e-06
Iter: 551 loss: 4.62636672e-06
Iter: 552 loss: 4.62495063e-06
Iter: 553 loss: 4.61917716e-06
Iter: 554 loss: 4.60942101e-06
Iter: 555 loss: 4.64907953e-06
Iter: 556 loss: 4.60719639e-06
Iter: 557 loss: 4.59821695e-06
Iter: 558 loss: 4.64645746e-06
Iter: 559 loss: 4.59701823e-06
Iter: 560 loss: 4.58953809e-06
Iter: 561 loss: 4.59270086e-06
Iter: 562 loss: 4.58438853e-06
Iter: 563 loss: 4.57379838e-06
Iter: 564 loss: 4.60419142e-06
Iter: 565 loss: 4.57054466e-06
Iter: 566 loss: 4.56004682e-06
Iter: 567 loss: 4.58311524e-06
Iter: 568 loss: 4.55608051e-06
Iter: 569 loss: 4.54640076e-06
Iter: 570 loss: 4.59789e-06
Iter: 571 loss: 4.54502515e-06
Iter: 572 loss: 4.5367633e-06
Iter: 573 loss: 4.53413304e-06
Iter: 574 loss: 4.52940776e-06
Iter: 575 loss: 4.51922915e-06
Iter: 576 loss: 4.58996328e-06
Iter: 577 loss: 4.51822689e-06
Iter: 578 loss: 4.5092479e-06
Iter: 579 loss: 4.53721577e-06
Iter: 580 loss: 4.50643802e-06
Iter: 581 loss: 4.49878917e-06
Iter: 582 loss: 4.52717222e-06
Iter: 583 loss: 4.49696381e-06
Iter: 584 loss: 4.48824812e-06
Iter: 585 loss: 4.50392645e-06
Iter: 586 loss: 4.48439778e-06
Iter: 587 loss: 4.47642242e-06
Iter: 588 loss: 4.48139735e-06
Iter: 589 loss: 4.47120328e-06
Iter: 590 loss: 4.4618555e-06
Iter: 591 loss: 4.5403458e-06
Iter: 592 loss: 4.46144122e-06
Iter: 593 loss: 4.45389287e-06
Iter: 594 loss: 4.45777732e-06
Iter: 595 loss: 4.44884e-06
Iter: 596 loss: 4.43961426e-06
Iter: 597 loss: 4.47332241e-06
Iter: 598 loss: 4.43731187e-06
Iter: 599 loss: 4.42831697e-06
Iter: 600 loss: 4.44392299e-06
Iter: 601 loss: 4.42430519e-06
Iter: 602 loss: 4.41548809e-06
Iter: 603 loss: 4.45494697e-06
Iter: 604 loss: 4.41375e-06
Iter: 605 loss: 4.40515396e-06
Iter: 606 loss: 4.40690292e-06
Iter: 607 loss: 4.39873247e-06
Iter: 608 loss: 4.3881787e-06
Iter: 609 loss: 4.42290047e-06
Iter: 610 loss: 4.38536517e-06
Iter: 611 loss: 4.37576819e-06
Iter: 612 loss: 4.4497333e-06
Iter: 613 loss: 4.37510698e-06
Iter: 614 loss: 4.36821301e-06
Iter: 615 loss: 4.37574818e-06
Iter: 616 loss: 4.36444861e-06
Iter: 617 loss: 4.35535594e-06
Iter: 618 loss: 4.3982418e-06
Iter: 619 loss: 4.35378843e-06
Iter: 620 loss: 4.34652702e-06
Iter: 621 loss: 4.34545063e-06
Iter: 622 loss: 4.3404616e-06
Iter: 623 loss: 4.33194964e-06
Iter: 624 loss: 4.39988253e-06
Iter: 625 loss: 4.33133027e-06
Iter: 626 loss: 4.32362867e-06
Iter: 627 loss: 4.3303321e-06
Iter: 628 loss: 4.31912486e-06
Iter: 629 loss: 4.31076114e-06
Iter: 630 loss: 4.34242884e-06
Iter: 631 loss: 4.30862e-06
Iter: 632 loss: 4.30047794e-06
Iter: 633 loss: 4.31543594e-06
Iter: 634 loss: 4.29691227e-06
Iter: 635 loss: 4.28833391e-06
Iter: 636 loss: 4.31309536e-06
Iter: 637 loss: 4.2857655e-06
Iter: 638 loss: 4.27673922e-06
Iter: 639 loss: 4.29583224e-06
Iter: 640 loss: 4.27319401e-06
Iter: 641 loss: 4.26476072e-06
Iter: 642 loss: 4.2751567e-06
Iter: 643 loss: 4.26045335e-06
Iter: 644 loss: 4.25186863e-06
Iter: 645 loss: 4.32533352e-06
Iter: 646 loss: 4.25119e-06
Iter: 647 loss: 4.24403788e-06
Iter: 648 loss: 4.24988548e-06
Iter: 649 loss: 4.23973506e-06
Iter: 650 loss: 4.23191887e-06
Iter: 651 loss: 4.29875718e-06
Iter: 652 loss: 4.23145457e-06
Iter: 653 loss: 4.22515768e-06
Iter: 654 loss: 4.2191482e-06
Iter: 655 loss: 4.21771574e-06
Iter: 656 loss: 4.20858441e-06
Iter: 657 loss: 4.2669526e-06
Iter: 658 loss: 4.20762035e-06
Iter: 659 loss: 4.19948901e-06
Iter: 660 loss: 4.22638732e-06
Iter: 661 loss: 4.19728894e-06
Iter: 662 loss: 4.19017579e-06
Iter: 663 loss: 4.20018796e-06
Iter: 664 loss: 4.18688705e-06
Iter: 665 loss: 4.17862157e-06
Iter: 666 loss: 4.20643528e-06
Iter: 667 loss: 4.17650153e-06
Iter: 668 loss: 4.16954117e-06
Iter: 669 loss: 4.1872654e-06
Iter: 670 loss: 4.16721105e-06
Iter: 671 loss: 4.16002194e-06
Iter: 672 loss: 4.17940373e-06
Iter: 673 loss: 4.15763088e-06
Iter: 674 loss: 4.15035083e-06
Iter: 675 loss: 4.15436e-06
Iter: 676 loss: 4.14567421e-06
Iter: 677 loss: 4.13752423e-06
Iter: 678 loss: 4.18709351e-06
Iter: 679 loss: 4.13644648e-06
Iter: 680 loss: 4.12787449e-06
Iter: 681 loss: 4.1354383e-06
Iter: 682 loss: 4.12288728e-06
Iter: 683 loss: 4.11431665e-06
Iter: 684 loss: 4.18642503e-06
Iter: 685 loss: 4.11377914e-06
Iter: 686 loss: 4.1063231e-06
Iter: 687 loss: 4.11063957e-06
Iter: 688 loss: 4.10149596e-06
Iter: 689 loss: 4.09383119e-06
Iter: 690 loss: 4.10641542e-06
Iter: 691 loss: 4.09034601e-06
Iter: 692 loss: 4.08125879e-06
Iter: 693 loss: 4.13805719e-06
Iter: 694 loss: 4.08018741e-06
Iter: 695 loss: 4.07385596e-06
Iter: 696 loss: 4.08516917e-06
Iter: 697 loss: 4.07098605e-06
Iter: 698 loss: 4.06473418e-06
Iter: 699 loss: 4.08825599e-06
Iter: 700 loss: 4.0631503e-06
Iter: 701 loss: 4.05665105e-06
Iter: 702 loss: 4.05886e-06
Iter: 703 loss: 4.05215815e-06
Iter: 704 loss: 4.04403909e-06
Iter: 705 loss: 4.09322138e-06
Iter: 706 loss: 4.04298771e-06
Iter: 707 loss: 4.03670037e-06
Iter: 708 loss: 4.04060665e-06
Iter: 709 loss: 4.03262311e-06
Iter: 710 loss: 4.02451315e-06
Iter: 711 loss: 4.04288403e-06
Iter: 712 loss: 4.02145e-06
Iter: 713 loss: 4.01207853e-06
Iter: 714 loss: 4.06225263e-06
Iter: 715 loss: 4.01073839e-06
Iter: 716 loss: 4.00449971e-06
Iter: 717 loss: 4.02676733e-06
Iter: 718 loss: 4.00282e-06
Iter: 719 loss: 3.99551664e-06
Iter: 720 loss: 4.00483e-06
Iter: 721 loss: 3.99172814e-06
Iter: 722 loss: 3.98405291e-06
Iter: 723 loss: 3.98897191e-06
Iter: 724 loss: 3.97922304e-06
Iter: 725 loss: 3.97186614e-06
Iter: 726 loss: 4.06753e-06
Iter: 727 loss: 3.97182066e-06
Iter: 728 loss: 3.96651149e-06
Iter: 729 loss: 3.9668148e-06
Iter: 730 loss: 3.96236783e-06
Iter: 731 loss: 3.95526604e-06
Iter: 732 loss: 3.98185102e-06
Iter: 733 loss: 3.95348752e-06
Iter: 734 loss: 3.94640665e-06
Iter: 735 loss: 3.95909274e-06
Iter: 736 loss: 3.94327708e-06
Iter: 737 loss: 3.9364586e-06
Iter: 738 loss: 3.96354062e-06
Iter: 739 loss: 3.93497248e-06
Iter: 740 loss: 3.92798393e-06
Iter: 741 loss: 3.93350092e-06
Iter: 742 loss: 3.92377524e-06
Iter: 743 loss: 3.91582535e-06
Iter: 744 loss: 3.93814253e-06
Iter: 745 loss: 3.91339427e-06
Iter: 746 loss: 3.90601372e-06
Iter: 747 loss: 3.95514e-06
Iter: 748 loss: 3.90520427e-06
Iter: 749 loss: 3.89898059e-06
Iter: 750 loss: 3.90324567e-06
Iter: 751 loss: 3.89502929e-06
Iter: 752 loss: 3.88636636e-06
Iter: 753 loss: 3.93200526e-06
Iter: 754 loss: 3.88515582e-06
Iter: 755 loss: 3.87854288e-06
Iter: 756 loss: 3.87795353e-06
Iter: 757 loss: 3.87310683e-06
Iter: 758 loss: 3.86641e-06
Iter: 759 loss: 3.93535583e-06
Iter: 760 loss: 3.86611737e-06
Iter: 761 loss: 3.86016336e-06
Iter: 762 loss: 3.86237843e-06
Iter: 763 loss: 3.85595649e-06
Iter: 764 loss: 3.84872192e-06
Iter: 765 loss: 3.87638102e-06
Iter: 766 loss: 3.8471112e-06
Iter: 767 loss: 3.84071791e-06
Iter: 768 loss: 3.86090369e-06
Iter: 769 loss: 3.83880297e-06
Iter: 770 loss: 3.8328335e-06
Iter: 771 loss: 3.8391081e-06
Iter: 772 loss: 3.82952749e-06
Iter: 773 loss: 3.82107373e-06
Iter: 774 loss: 3.84187751e-06
Iter: 775 loss: 3.81807604e-06
Iter: 776 loss: 3.81104974e-06
Iter: 777 loss: 3.83004271e-06
Iter: 778 loss: 3.80877054e-06
Iter: 779 loss: 3.80204347e-06
Iter: 780 loss: 3.8340122e-06
Iter: 781 loss: 3.80086158e-06
Iter: 782 loss: 3.79384687e-06
Iter: 783 loss: 3.79885546e-06
Iter: 784 loss: 3.78949289e-06
Iter: 785 loss: 3.7823088e-06
Iter: 786 loss: 3.86128886e-06
Iter: 787 loss: 3.78227378e-06
Iter: 788 loss: 3.77734023e-06
Iter: 789 loss: 3.77529341e-06
Iter: 790 loss: 3.77283868e-06
Iter: 791 loss: 3.76636808e-06
Iter: 792 loss: 3.79003745e-06
Iter: 793 loss: 3.76474554e-06
Iter: 794 loss: 3.75725813e-06
Iter: 795 loss: 3.77607898e-06
Iter: 796 loss: 3.75449122e-06
Iter: 797 loss: 3.74837828e-06
Iter: 798 loss: 3.76745629e-06
Iter: 799 loss: 3.74660385e-06
Iter: 800 loss: 3.74054775e-06
Iter: 801 loss: 3.75602099e-06
Iter: 802 loss: 3.7384541e-06
Iter: 803 loss: 3.73179637e-06
Iter: 804 loss: 3.7360378e-06
Iter: 805 loss: 3.72745308e-06
Iter: 806 loss: 3.71923034e-06
Iter: 807 loss: 3.76401749e-06
Iter: 808 loss: 3.71793612e-06
Iter: 809 loss: 3.71141914e-06
Iter: 810 loss: 3.71545229e-06
Iter: 811 loss: 3.70730436e-06
Iter: 812 loss: 3.69953841e-06
Iter: 813 loss: 3.73970579e-06
Iter: 814 loss: 3.69839881e-06
Iter: 815 loss: 3.69113195e-06
Iter: 816 loss: 3.71146916e-06
Iter: 817 loss: 3.68869269e-06
Iter: 818 loss: 3.68268593e-06
Iter: 819 loss: 3.71863462e-06
Iter: 820 loss: 3.68195151e-06
Iter: 821 loss: 3.67609846e-06
Iter: 822 loss: 3.67680627e-06
Iter: 823 loss: 3.6718061e-06
Iter: 824 loss: 3.66544373e-06
Iter: 825 loss: 3.68276164e-06
Iter: 826 loss: 3.66345876e-06
Iter: 827 loss: 3.65629239e-06
Iter: 828 loss: 3.69079567e-06
Iter: 829 loss: 3.65515984e-06
Iter: 830 loss: 3.6493343e-06
Iter: 831 loss: 3.65253413e-06
Iter: 832 loss: 3.64546213e-06
Iter: 833 loss: 3.6384663e-06
Iter: 834 loss: 3.6773406e-06
Iter: 835 loss: 3.63753338e-06
Iter: 836 loss: 3.63177014e-06
Iter: 837 loss: 3.63715026e-06
Iter: 838 loss: 3.62836954e-06
Iter: 839 loss: 3.62148489e-06
Iter: 840 loss: 3.64750031e-06
Iter: 841 loss: 3.6197448e-06
Iter: 842 loss: 3.61322873e-06
Iter: 843 loss: 3.62343189e-06
Iter: 844 loss: 3.61011348e-06
Iter: 845 loss: 3.60397962e-06
Iter: 846 loss: 3.62683841e-06
Iter: 847 loss: 3.60239301e-06
Iter: 848 loss: 3.5957346e-06
Iter: 849 loss: 3.6146655e-06
Iter: 850 loss: 3.59360365e-06
Iter: 851 loss: 3.58748548e-06
Iter: 852 loss: 3.61346179e-06
Iter: 853 loss: 3.58619286e-06
Iter: 854 loss: 3.58014154e-06
Iter: 855 loss: 3.5946241e-06
Iter: 856 loss: 3.57801537e-06
Iter: 857 loss: 3.57270346e-06
Iter: 858 loss: 3.57134695e-06
Iter: 859 loss: 3.56802411e-06
Iter: 860 loss: 3.56097235e-06
Iter: 861 loss: 3.63788718e-06
Iter: 862 loss: 3.56081046e-06
Iter: 863 loss: 3.55564248e-06
Iter: 864 loss: 3.55683619e-06
Iter: 865 loss: 3.55187967e-06
Iter: 866 loss: 3.54573808e-06
Iter: 867 loss: 3.57673139e-06
Iter: 868 loss: 3.54475696e-06
Iter: 869 loss: 3.53910036e-06
Iter: 870 loss: 3.54744202e-06
Iter: 871 loss: 3.53642281e-06
Iter: 872 loss: 3.53064138e-06
Iter: 873 loss: 3.55004613e-06
Iter: 874 loss: 3.52912798e-06
Iter: 875 loss: 3.52322832e-06
Iter: 876 loss: 3.53549331e-06
Iter: 877 loss: 3.52101756e-06
Iter: 878 loss: 3.51537301e-06
Iter: 879 loss: 3.52552524e-06
Iter: 880 loss: 3.51304288e-06
Iter: 881 loss: 3.50659047e-06
Iter: 882 loss: 3.53885207e-06
Iter: 883 loss: 3.50556638e-06
Iter: 884 loss: 3.50007303e-06
Iter: 885 loss: 3.51210247e-06
Iter: 886 loss: 3.49787615e-06
Iter: 887 loss: 3.49204424e-06
Iter: 888 loss: 3.52115285e-06
Iter: 889 loss: 3.49109064e-06
Iter: 890 loss: 3.48642197e-06
Iter: 891 loss: 3.48285494e-06
Iter: 892 loss: 3.48126878e-06
Iter: 893 loss: 3.47517e-06
Iter: 894 loss: 3.54327517e-06
Iter: 895 loss: 3.47505033e-06
Iter: 896 loss: 3.46984825e-06
Iter: 897 loss: 3.47365426e-06
Iter: 898 loss: 3.46666661e-06
Iter: 899 loss: 3.46144247e-06
Iter: 900 loss: 3.48259141e-06
Iter: 901 loss: 3.46026627e-06
Iter: 902 loss: 3.4549571e-06
Iter: 903 loss: 3.46358911e-06
Iter: 904 loss: 3.45251919e-06
Iter: 905 loss: 3.44697605e-06
Iter: 906 loss: 3.46167462e-06
Iter: 907 loss: 3.44506202e-06
Iter: 908 loss: 3.43961278e-06
Iter: 909 loss: 3.45910416e-06
Iter: 910 loss: 3.43807187e-06
Iter: 911 loss: 3.43304077e-06
Iter: 912 loss: 3.4370878e-06
Iter: 913 loss: 3.4300474e-06
Iter: 914 loss: 3.42406884e-06
Iter: 915 loss: 3.46136085e-06
Iter: 916 loss: 3.42342196e-06
Iter: 917 loss: 3.4181337e-06
Iter: 918 loss: 3.42800672e-06
Iter: 919 loss: 3.41602686e-06
Iter: 920 loss: 3.41064697e-06
Iter: 921 loss: 3.44244745e-06
Iter: 922 loss: 3.41002692e-06
Iter: 923 loss: 3.40544193e-06
Iter: 924 loss: 3.40175143e-06
Iter: 925 loss: 3.40033353e-06
Iter: 926 loss: 3.39430153e-06
Iter: 927 loss: 3.43938268e-06
Iter: 928 loss: 3.39376084e-06
Iter: 929 loss: 3.38822247e-06
Iter: 930 loss: 3.40351494e-06
Iter: 931 loss: 3.38640962e-06
Iter: 932 loss: 3.38181826e-06
Iter: 933 loss: 3.38814766e-06
Iter: 934 loss: 3.37949496e-06
Iter: 935 loss: 3.37394044e-06
Iter: 936 loss: 3.39538701e-06
Iter: 937 loss: 3.37262054e-06
Iter: 938 loss: 3.36774428e-06
Iter: 939 loss: 3.37545907e-06
Iter: 940 loss: 3.36547055e-06
Iter: 941 loss: 3.36018434e-06
Iter: 942 loss: 3.37972187e-06
Iter: 943 loss: 3.35886421e-06
Iter: 944 loss: 3.35366576e-06
Iter: 945 loss: 3.35891627e-06
Iter: 946 loss: 3.35074446e-06
Iter: 947 loss: 3.34525862e-06
Iter: 948 loss: 3.37413553e-06
Iter: 949 loss: 3.34435208e-06
Iter: 950 loss: 3.33906223e-06
Iter: 951 loss: 3.35149025e-06
Iter: 952 loss: 3.33706248e-06
Iter: 953 loss: 3.33234289e-06
Iter: 954 loss: 3.36106905e-06
Iter: 955 loss: 3.33160779e-06
Iter: 956 loss: 3.32715172e-06
Iter: 957 loss: 3.32697937e-06
Iter: 958 loss: 3.32356262e-06
Iter: 959 loss: 3.31827414e-06
Iter: 960 loss: 3.3323347e-06
Iter: 961 loss: 3.31653109e-06
Iter: 962 loss: 3.31099045e-06
Iter: 963 loss: 3.34884157e-06
Iter: 964 loss: 3.3104443e-06
Iter: 965 loss: 3.30652733e-06
Iter: 966 loss: 3.30609919e-06
Iter: 967 loss: 3.3032577e-06
Iter: 968 loss: 3.29793556e-06
Iter: 969 loss: 3.33003936e-06
Iter: 970 loss: 3.29720842e-06
Iter: 971 loss: 3.29265163e-06
Iter: 972 loss: 3.29565e-06
Iter: 973 loss: 3.28984083e-06
Iter: 974 loss: 3.28434226e-06
Iter: 975 loss: 3.30847274e-06
Iter: 976 loss: 3.28337e-06
Iter: 977 loss: 3.27847192e-06
Iter: 978 loss: 3.28589113e-06
Iter: 979 loss: 3.27623047e-06
Iter: 980 loss: 3.27100179e-06
Iter: 981 loss: 3.28554142e-06
Iter: 982 loss: 3.2692667e-06
Iter: 983 loss: 3.263606e-06
Iter: 984 loss: 3.2864973e-06
Iter: 985 loss: 3.26238251e-06
Iter: 986 loss: 3.25778342e-06
Iter: 987 loss: 3.2749e-06
Iter: 988 loss: 3.25658e-06
Iter: 989 loss: 3.25155088e-06
Iter: 990 loss: 3.25814744e-06
Iter: 991 loss: 3.24903249e-06
Iter: 992 loss: 3.24407711e-06
Iter: 993 loss: 3.24836606e-06
Iter: 994 loss: 3.24119901e-06
Iter: 995 loss: 3.23631184e-06
Iter: 996 loss: 3.29487261e-06
Iter: 997 loss: 3.23623544e-06
Iter: 998 loss: 3.23244876e-06
Iter: 999 loss: 3.23001268e-06
Iter: 1000 loss: 3.22857841e-06
Iter: 1001 loss: 3.22322194e-06
Iter: 1002 loss: 3.2582841e-06
Iter: 1003 loss: 3.22271308e-06
Iter: 1004 loss: 3.21802645e-06
Iter: 1005 loss: 3.22371079e-06
Iter: 1006 loss: 3.21559196e-06
Iter: 1007 loss: 3.21067182e-06
Iter: 1008 loss: 3.22559913e-06
Iter: 1009 loss: 3.20922936e-06
Iter: 1010 loss: 3.20393156e-06
Iter: 1011 loss: 3.21543121e-06
Iter: 1012 loss: 3.20189429e-06
Iter: 1013 loss: 3.1969721e-06
Iter: 1014 loss: 3.21242646e-06
Iter: 1015 loss: 3.19561013e-06
Iter: 1016 loss: 3.19103856e-06
Iter: 1017 loss: 3.21189032e-06
Iter: 1018 loss: 3.19022502e-06
Iter: 1019 loss: 3.18593902e-06
Iter: 1020 loss: 3.1934087e-06
Iter: 1021 loss: 3.18402499e-06
Iter: 1022 loss: 3.17899617e-06
Iter: 1023 loss: 3.20027084e-06
Iter: 1024 loss: 3.17791137e-06
Iter: 1025 loss: 3.17400873e-06
Iter: 1026 loss: 3.17268564e-06
Iter: 1027 loss: 3.17037666e-06
Iter: 1028 loss: 3.16550313e-06
Iter: 1029 loss: 3.21176844e-06
Iter: 1030 loss: 3.16536898e-06
Iter: 1031 loss: 3.16083924e-06
Iter: 1032 loss: 3.16289697e-06
Iter: 1033 loss: 3.15776333e-06
Iter: 1034 loss: 3.15333182e-06
Iter: 1035 loss: 3.17331751e-06
Iter: 1036 loss: 3.15246234e-06
Iter: 1037 loss: 3.14806744e-06
Iter: 1038 loss: 3.1565819e-06
Iter: 1039 loss: 3.14623298e-06
Iter: 1040 loss: 3.14167028e-06
Iter: 1041 loss: 3.14778208e-06
Iter: 1042 loss: 3.13925761e-06
Iter: 1043 loss: 3.13361352e-06
Iter: 1044 loss: 3.15596185e-06
Iter: 1045 loss: 3.13232977e-06
Iter: 1046 loss: 3.12773659e-06
Iter: 1047 loss: 3.13792589e-06
Iter: 1048 loss: 3.12604698e-06
Iter: 1049 loss: 3.12145085e-06
Iter: 1050 loss: 3.14232193e-06
Iter: 1051 loss: 3.12065731e-06
Iter: 1052 loss: 3.11617464e-06
Iter: 1053 loss: 3.1233044e-06
Iter: 1054 loss: 3.11408894e-06
Iter: 1055 loss: 3.10936321e-06
Iter: 1056 loss: 3.13819464e-06
Iter: 1057 loss: 3.10881728e-06
Iter: 1058 loss: 3.10498194e-06
Iter: 1059 loss: 3.10330825e-06
Iter: 1060 loss: 3.10141013e-06
Iter: 1061 loss: 3.09660618e-06
Iter: 1062 loss: 3.12517568e-06
Iter: 1063 loss: 3.09612506e-06
Iter: 1064 loss: 3.09129246e-06
Iter: 1065 loss: 3.10267387e-06
Iter: 1066 loss: 3.08951621e-06
Iter: 1067 loss: 3.08558492e-06
Iter: 1068 loss: 3.09307597e-06
Iter: 1069 loss: 3.08393919e-06
Iter: 1070 loss: 3.07973642e-06
Iter: 1071 loss: 3.09819666e-06
Iter: 1072 loss: 3.07879327e-06
Iter: 1073 loss: 3.07491564e-06
Iter: 1074 loss: 3.07471328e-06
Iter: 1075 loss: 3.07167761e-06
Iter: 1076 loss: 3.06620655e-06
Iter: 1077 loss: 3.09803772e-06
Iter: 1078 loss: 3.06547327e-06
Iter: 1079 loss: 3.06104948e-06
Iter: 1080 loss: 3.06767697e-06
Iter: 1081 loss: 3.05892945e-06
Iter: 1082 loss: 3.05426556e-06
Iter: 1083 loss: 3.07217306e-06
Iter: 1084 loss: 3.05310687e-06
Iter: 1085 loss: 3.04843661e-06
Iter: 1086 loss: 3.06307129e-06
Iter: 1087 loss: 3.04712034e-06
Iter: 1088 loss: 3.04318974e-06
Iter: 1089 loss: 3.065398e-06
Iter: 1090 loss: 3.0426545e-06
Iter: 1091 loss: 3.03905131e-06
Iter: 1092 loss: 3.03779211e-06
Iter: 1093 loss: 3.03567481e-06
Iter: 1094 loss: 3.03064689e-06
Iter: 1095 loss: 3.04301307e-06
Iter: 1096 loss: 3.0288893e-06
Iter: 1097 loss: 3.02338526e-06
Iter: 1098 loss: 3.06256379e-06
Iter: 1099 loss: 3.02289754e-06
Iter: 1100 loss: 3.01912132e-06
Iter: 1101 loss: 3.01825708e-06
Iter: 1102 loss: 3.01590558e-06
Iter: 1103 loss: 3.0108281e-06
Iter: 1104 loss: 3.04694777e-06
Iter: 1105 loss: 3.01037585e-06
Iter: 1106 loss: 3.00627516e-06
Iter: 1107 loss: 3.00953616e-06
Iter: 1108 loss: 3.00393958e-06
Iter: 1109 loss: 2.99955377e-06
Iter: 1110 loss: 3.01730529e-06
Iter: 1111 loss: 2.99862381e-06
Iter: 1112 loss: 2.99394969e-06
Iter: 1113 loss: 2.99712474e-06
Iter: 1114 loss: 2.99103385e-06
Iter: 1115 loss: 2.98563145e-06
Iter: 1116 loss: 3.01197679e-06
Iter: 1117 loss: 2.98466239e-06
Iter: 1118 loss: 2.97993233e-06
Iter: 1119 loss: 2.99739554e-06
Iter: 1120 loss: 2.97875567e-06
Iter: 1121 loss: 2.97450242e-06
Iter: 1122 loss: 2.98889609e-06
Iter: 1123 loss: 2.97337465e-06
Iter: 1124 loss: 2.96898133e-06
Iter: 1125 loss: 2.97711722e-06
Iter: 1126 loss: 2.96706548e-06
Iter: 1127 loss: 2.9632788e-06
Iter: 1128 loss: 2.96661278e-06
Iter: 1129 loss: 2.96104736e-06
Iter: 1130 loss: 2.95679183e-06
Iter: 1131 loss: 2.99648559e-06
Iter: 1132 loss: 2.95665541e-06
Iter: 1133 loss: 2.95313248e-06
Iter: 1134 loss: 2.95139898e-06
Iter: 1135 loss: 2.94986239e-06
Iter: 1136 loss: 2.94545544e-06
Iter: 1137 loss: 2.97724137e-06
Iter: 1138 loss: 2.94509232e-06
Iter: 1139 loss: 2.94115898e-06
Iter: 1140 loss: 2.94273082e-06
Iter: 1141 loss: 2.93834069e-06
Iter: 1142 loss: 2.93338735e-06
Iter: 1143 loss: 2.94844176e-06
Iter: 1144 loss: 2.93201424e-06
Iter: 1145 loss: 2.92692926e-06
Iter: 1146 loss: 2.94238544e-06
Iter: 1147 loss: 2.9253556e-06
Iter: 1148 loss: 2.92097866e-06
Iter: 1149 loss: 2.93241123e-06
Iter: 1150 loss: 2.91940978e-06
Iter: 1151 loss: 2.91503602e-06
Iter: 1152 loss: 2.93675839e-06
Iter: 1153 loss: 2.91421429e-06
Iter: 1154 loss: 2.9105604e-06
Iter: 1155 loss: 2.92159802e-06
Iter: 1156 loss: 2.90947924e-06
Iter: 1157 loss: 2.90575781e-06
Iter: 1158 loss: 2.91581046e-06
Iter: 1159 loss: 2.9044902e-06
Iter: 1160 loss: 2.90086427e-06
Iter: 1161 loss: 2.89994023e-06
Iter: 1162 loss: 2.89769309e-06
Iter: 1163 loss: 2.89362242e-06
Iter: 1164 loss: 2.94409301e-06
Iter: 1165 loss: 2.89366926e-06
Iter: 1166 loss: 2.89014361e-06
Iter: 1167 loss: 2.88918636e-06
Iter: 1168 loss: 2.88703859e-06
Iter: 1169 loss: 2.88265915e-06
Iter: 1170 loss: 2.90032767e-06
Iter: 1171 loss: 2.88164438e-06
Iter: 1172 loss: 2.87725743e-06
Iter: 1173 loss: 2.89195327e-06
Iter: 1174 loss: 2.87607054e-06
Iter: 1175 loss: 2.87246803e-06
Iter: 1176 loss: 2.8753243e-06
Iter: 1177 loss: 2.87027865e-06
Iter: 1178 loss: 2.86523982e-06
Iter: 1179 loss: 2.88295541e-06
Iter: 1180 loss: 2.86404793e-06
Iter: 1181 loss: 2.85952547e-06
Iter: 1182 loss: 2.87091825e-06
Iter: 1183 loss: 2.85801434e-06
Iter: 1184 loss: 2.85402484e-06
Iter: 1185 loss: 2.87502849e-06
Iter: 1186 loss: 2.85342458e-06
Iter: 1187 loss: 2.8498514e-06
Iter: 1188 loss: 2.85559645e-06
Iter: 1189 loss: 2.84811836e-06
Iter: 1190 loss: 2.84392308e-06
Iter: 1191 loss: 2.86278964e-06
Iter: 1192 loss: 2.84312159e-06
Iter: 1193 loss: 2.83959071e-06
Iter: 1194 loss: 2.83996383e-06
Iter: 1195 loss: 2.83687632e-06
Iter: 1196 loss: 2.83292047e-06
Iter: 1197 loss: 2.85357191e-06
Iter: 1198 loss: 2.83227905e-06
Iter: 1199 loss: 2.82784504e-06
Iter: 1200 loss: 2.83584632e-06
Iter: 1201 loss: 2.8259285e-06
Iter: 1202 loss: 2.82216524e-06
Iter: 1203 loss: 2.82950714e-06
Iter: 1204 loss: 2.82061183e-06
Iter: 1205 loss: 2.81646362e-06
Iter: 1206 loss: 2.83790905e-06
Iter: 1207 loss: 2.81578332e-06
Iter: 1208 loss: 2.81229518e-06
Iter: 1209 loss: 2.81230518e-06
Iter: 1210 loss: 2.8095335e-06
Iter: 1211 loss: 2.8049767e-06
Iter: 1212 loss: 2.83241957e-06
Iter: 1213 loss: 2.80439235e-06
Iter: 1214 loss: 2.80062864e-06
Iter: 1215 loss: 2.8053164e-06
Iter: 1216 loss: 2.79865026e-06
Iter: 1217 loss: 2.79443339e-06
Iter: 1218 loss: 2.81341363e-06
Iter: 1219 loss: 2.79361e-06
Iter: 1220 loss: 2.78959624e-06
Iter: 1221 loss: 2.79930237e-06
Iter: 1222 loss: 2.7881847e-06
Iter: 1223 loss: 2.78442803e-06
Iter: 1224 loss: 2.80360064e-06
Iter: 1225 loss: 2.78375092e-06
Iter: 1226 loss: 2.78016932e-06
Iter: 1227 loss: 2.77977642e-06
Iter: 1228 loss: 2.77728941e-06
Iter: 1229 loss: 2.77279605e-06
Iter: 1230 loss: 2.78552034e-06
Iter: 1231 loss: 2.7713611e-06
Iter: 1232 loss: 2.76690753e-06
Iter: 1233 loss: 2.80132735e-06
Iter: 1234 loss: 2.76656169e-06
Iter: 1235 loss: 2.76357059e-06
Iter: 1236 loss: 2.76261699e-06
Iter: 1237 loss: 2.76086416e-06
Iter: 1238 loss: 2.75696289e-06
Iter: 1239 loss: 2.78643051e-06
Iter: 1240 loss: 2.75661068e-06
Iter: 1241 loss: 2.75320735e-06
Iter: 1242 loss: 2.75360685e-06
Iter: 1243 loss: 2.75073467e-06
Iter: 1244 loss: 2.74656736e-06
Iter: 1245 loss: 2.76606647e-06
Iter: 1246 loss: 2.74580589e-06
Iter: 1247 loss: 2.74182162e-06
Iter: 1248 loss: 2.74924878e-06
Iter: 1249 loss: 2.74011882e-06
Iter: 1250 loss: 2.73602791e-06
Iter: 1251 loss: 2.74957188e-06
Iter: 1252 loss: 2.73488467e-06
Iter: 1253 loss: 2.7308015e-06
Iter: 1254 loss: 2.7455676e-06
Iter: 1255 loss: 2.72969828e-06
Iter: 1256 loss: 2.72632769e-06
Iter: 1257 loss: 2.7427277e-06
Iter: 1258 loss: 2.72580382e-06
Iter: 1259 loss: 2.72251441e-06
Iter: 1260 loss: 2.7242229e-06
Iter: 1261 loss: 2.72031457e-06
Iter: 1262 loss: 2.7163303e-06
Iter: 1263 loss: 2.71865588e-06
Iter: 1264 loss: 2.7136773e-06
Iter: 1265 loss: 2.70930832e-06
Iter: 1266 loss: 2.76432365e-06
Iter: 1267 loss: 2.70929286e-06
Iter: 1268 loss: 2.70612782e-06
Iter: 1269 loss: 2.70475653e-06
Iter: 1270 loss: 2.70318151e-06
Iter: 1271 loss: 2.69913539e-06
Iter: 1272 loss: 2.72402258e-06
Iter: 1273 loss: 2.69874909e-06
Iter: 1274 loss: 2.69484917e-06
Iter: 1275 loss: 2.69956217e-06
Iter: 1276 loss: 2.69280508e-06
Iter: 1277 loss: 2.68922076e-06
Iter: 1278 loss: 2.70127384e-06
Iter: 1279 loss: 2.68828398e-06
Iter: 1280 loss: 2.68461918e-06
Iter: 1281 loss: 2.6951368e-06
Iter: 1282 loss: 2.6834723e-06
Iter: 1283 loss: 2.67974133e-06
Iter: 1284 loss: 2.68589974e-06
Iter: 1285 loss: 2.67809264e-06
Iter: 1286 loss: 2.67394967e-06
Iter: 1287 loss: 2.6949192e-06
Iter: 1288 loss: 2.67326277e-06
Iter: 1289 loss: 2.66987172e-06
Iter: 1290 loss: 2.67914174e-06
Iter: 1291 loss: 2.66880852e-06
Iter: 1292 loss: 2.66492179e-06
Iter: 1293 loss: 2.67206406e-06
Iter: 1294 loss: 2.6632074e-06
Iter: 1295 loss: 2.65931476e-06
Iter: 1296 loss: 2.66283132e-06
Iter: 1297 loss: 2.65709195e-06
Iter: 1298 loss: 2.65391145e-06
Iter: 1299 loss: 2.69334396e-06
Iter: 1300 loss: 2.65387962e-06
Iter: 1301 loss: 2.65111521e-06
Iter: 1302 loss: 2.65054632e-06
Iter: 1303 loss: 2.64874734e-06
Iter: 1304 loss: 2.64524579e-06
Iter: 1305 loss: 2.65619792e-06
Iter: 1306 loss: 2.64426171e-06
Iter: 1307 loss: 2.64053733e-06
Iter: 1308 loss: 2.65560607e-06
Iter: 1309 loss: 2.63966058e-06
Iter: 1310 loss: 2.63667471e-06
Iter: 1311 loss: 2.63744619e-06
Iter: 1312 loss: 2.63443985e-06
Iter: 1313 loss: 2.6301359e-06
Iter: 1314 loss: 2.64890014e-06
Iter: 1315 loss: 2.62927369e-06
Iter: 1316 loss: 2.62552498e-06
Iter: 1317 loss: 2.63556853e-06
Iter: 1318 loss: 2.62433446e-06
Iter: 1319 loss: 2.6209218e-06
Iter: 1320 loss: 2.63768652e-06
Iter: 1321 loss: 2.62033245e-06
Iter: 1322 loss: 2.61718196e-06
Iter: 1323 loss: 2.62222306e-06
Iter: 1324 loss: 2.61578475e-06
Iter: 1325 loss: 2.61231025e-06
Iter: 1326 loss: 2.63006064e-06
Iter: 1327 loss: 2.61181299e-06
Iter: 1328 loss: 2.60905404e-06
Iter: 1329 loss: 2.60885622e-06
Iter: 1330 loss: 2.60684692e-06
Iter: 1331 loss: 2.6032576e-06
Iter: 1332 loss: 2.61911487e-06
Iter: 1333 loss: 2.6026355e-06
Iter: 1334 loss: 2.59871331e-06
Iter: 1335 loss: 2.6087846e-06
Iter: 1336 loss: 2.59740727e-06
Iter: 1337 loss: 2.59432431e-06
Iter: 1338 loss: 2.59816125e-06
Iter: 1339 loss: 2.59277795e-06
Iter: 1340 loss: 2.58925343e-06
Iter: 1341 loss: 2.60998195e-06
Iter: 1342 loss: 2.58880868e-06
Iter: 1343 loss: 2.58574073e-06
Iter: 1344 loss: 2.58569753e-06
Iter: 1345 loss: 2.58337468e-06
Iter: 1346 loss: 2.57964075e-06
Iter: 1347 loss: 2.60187971e-06
Iter: 1348 loss: 2.57916963e-06
Iter: 1349 loss: 2.5757804e-06
Iter: 1350 loss: 2.57849524e-06
Iter: 1351 loss: 2.57368765e-06
Iter: 1352 loss: 2.56943281e-06
Iter: 1353 loss: 2.58942896e-06
Iter: 1354 loss: 2.56868839e-06
Iter: 1355 loss: 2.56499561e-06
Iter: 1356 loss: 2.5783238e-06
Iter: 1357 loss: 2.56410249e-06
Iter: 1358 loss: 2.56111844e-06
Iter: 1359 loss: 2.57435022e-06
Iter: 1360 loss: 2.5605359e-06
Iter: 1361 loss: 2.5575514e-06
Iter: 1362 loss: 2.55628811e-06
Iter: 1363 loss: 2.55469877e-06
Iter: 1364 loss: 2.55107034e-06
Iter: 1365 loss: 2.57111151e-06
Iter: 1366 loss: 2.55041596e-06
Iter: 1367 loss: 2.54724091e-06
Iter: 1368 loss: 2.56477097e-06
Iter: 1369 loss: 2.54672022e-06
Iter: 1370 loss: 2.54397492e-06
Iter: 1371 loss: 2.54152883e-06
Iter: 1372 loss: 2.54087604e-06
Iter: 1373 loss: 2.53710527e-06
Iter: 1374 loss: 2.58087448e-06
Iter: 1375 loss: 2.53706548e-06
Iter: 1376 loss: 2.53417147e-06
Iter: 1377 loss: 2.53435019e-06
Iter: 1378 loss: 2.53194571e-06
Iter: 1379 loss: 2.52825976e-06
Iter: 1380 loss: 2.54055908e-06
Iter: 1381 loss: 2.52726932e-06
Iter: 1382 loss: 2.52348764e-06
Iter: 1383 loss: 2.53645794e-06
Iter: 1384 loss: 2.5225022e-06
Iter: 1385 loss: 2.51946335e-06
Iter: 1386 loss: 2.53015082e-06
Iter: 1387 loss: 2.51858546e-06
Iter: 1388 loss: 2.51542178e-06
Iter: 1389 loss: 2.52309451e-06
Iter: 1390 loss: 2.5142258e-06
Iter: 1391 loss: 2.5109407e-06
Iter: 1392 loss: 2.52410109e-06
Iter: 1393 loss: 2.51020583e-06
Iter: 1394 loss: 2.5070849e-06
Iter: 1395 loss: 2.5117838e-06
Iter: 1396 loss: 2.50563198e-06
Iter: 1397 loss: 2.50271205e-06
Iter: 1398 loss: 2.50654375e-06
Iter: 1399 loss: 2.50125572e-06
Iter: 1400 loss: 2.4978458e-06
Iter: 1401 loss: 2.52159953e-06
Iter: 1402 loss: 2.49762797e-06
Iter: 1403 loss: 2.49462846e-06
Iter: 1404 loss: 2.49367054e-06
Iter: 1405 loss: 2.49204163e-06
Iter: 1406 loss: 2.48861215e-06
Iter: 1407 loss: 2.51024449e-06
Iter: 1408 loss: 2.4881731e-06
Iter: 1409 loss: 2.4847659e-06
Iter: 1410 loss: 2.48895321e-06
Iter: 1411 loss: 2.48292736e-06
Iter: 1412 loss: 2.47969501e-06
Iter: 1413 loss: 2.4883841e-06
Iter: 1414 loss: 2.47868502e-06
Iter: 1415 loss: 2.47546859e-06
Iter: 1416 loss: 2.48962806e-06
Iter: 1417 loss: 2.47479124e-06
Iter: 1418 loss: 2.47185085e-06
Iter: 1419 loss: 2.47569324e-06
Iter: 1420 loss: 2.470342e-06
Iter: 1421 loss: 2.46685045e-06
Iter: 1422 loss: 2.48364449e-06
Iter: 1423 loss: 2.46619288e-06
Iter: 1424 loss: 2.46323361e-06
Iter: 1425 loss: 2.47177809e-06
Iter: 1426 loss: 2.46229365e-06
Iter: 1427 loss: 2.45922433e-06
Iter: 1428 loss: 2.46675268e-06
Iter: 1429 loss: 2.45816159e-06
Iter: 1430 loss: 2.45523643e-06
Iter: 1431 loss: 2.45563206e-06
Iter: 1432 loss: 2.45297815e-06
Iter: 1433 loss: 2.44966168e-06
Iter: 1434 loss: 2.48479228e-06
Iter: 1435 loss: 2.44954595e-06
Iter: 1436 loss: 2.44688454e-06
Iter: 1437 loss: 2.44744524e-06
Iter: 1438 loss: 2.44483545e-06
Iter: 1439 loss: 2.44164426e-06
Iter: 1440 loss: 2.44827515e-06
Iter: 1441 loss: 2.44039097e-06
Iter: 1442 loss: 2.43640397e-06
Iter: 1443 loss: 2.45288447e-06
Iter: 1444 loss: 2.4355088e-06
Iter: 1445 loss: 2.43239083e-06
Iter: 1446 loss: 2.43382192e-06
Iter: 1447 loss: 2.43029308e-06
Iter: 1448 loss: 2.42652982e-06
Iter: 1449 loss: 2.44895728e-06
Iter: 1450 loss: 2.42605279e-06
Iter: 1451 loss: 2.42267356e-06
Iter: 1452 loss: 2.42784836e-06
Iter: 1453 loss: 2.42113629e-06
Iter: 1454 loss: 2.41804946e-06
Iter: 1455 loss: 2.44187413e-06
Iter: 1456 loss: 2.41784164e-06
Iter: 1457 loss: 2.4156443e-06
Iter: 1458 loss: 2.42099804e-06
Iter: 1459 loss: 2.41490397e-06
Iter: 1460 loss: 2.4125095e-06
Iter: 1461 loss: 2.41794032e-06
Iter: 1462 loss: 2.41159046e-06
Iter: 1463 loss: 2.40887084e-06
Iter: 1464 loss: 2.40792019e-06
Iter: 1465 loss: 2.40635973e-06
Iter: 1466 loss: 2.40321924e-06
Iter: 1467 loss: 2.43568161e-06
Iter: 1468 loss: 2.40312738e-06
Iter: 1469 loss: 2.40040799e-06
Iter: 1470 loss: 2.4031242e-06
Iter: 1471 loss: 2.39880342e-06
Iter: 1472 loss: 2.39570318e-06
Iter: 1473 loss: 2.39717042e-06
Iter: 1474 loss: 2.39361748e-06
Iter: 1475 loss: 2.39005612e-06
Iter: 1476 loss: 2.42625129e-06
Iter: 1477 loss: 2.38991652e-06
Iter: 1478 loss: 2.38737221e-06
Iter: 1479 loss: 2.38663e-06
Iter: 1480 loss: 2.38511757e-06
Iter: 1481 loss: 2.38173106e-06
Iter: 1482 loss: 2.39880956e-06
Iter: 1483 loss: 2.38122084e-06
Iter: 1484 loss: 2.37788481e-06
Iter: 1485 loss: 2.38378243e-06
Iter: 1486 loss: 2.37646668e-06
Iter: 1487 loss: 2.37319546e-06
Iter: 1488 loss: 2.38953453e-06
Iter: 1489 loss: 2.37269296e-06
Iter: 1490 loss: 2.36980895e-06
Iter: 1491 loss: 2.37710515e-06
Iter: 1492 loss: 2.36878395e-06
Iter: 1493 loss: 2.3659918e-06
Iter: 1494 loss: 2.37807762e-06
Iter: 1495 loss: 2.36549818e-06
Iter: 1496 loss: 2.36294045e-06
Iter: 1497 loss: 2.36376786e-06
Iter: 1498 loss: 2.36109349e-06
Iter: 1499 loss: 2.35840889e-06
Iter: 1500 loss: 2.37084441e-06
Iter: 1501 loss: 2.3578973e-06
Iter: 1502 loss: 2.35511084e-06
Iter: 1503 loss: 2.36359938e-06
Iter: 1504 loss: 2.35430389e-06
Iter: 1505 loss: 2.35170933e-06
Iter: 1506 loss: 2.35055768e-06
Iter: 1507 loss: 2.34920572e-06
Iter: 1508 loss: 2.34568915e-06
Iter: 1509 loss: 2.37715722e-06
Iter: 1510 loss: 2.34552499e-06
Iter: 1511 loss: 2.34250365e-06
Iter: 1512 loss: 2.34364234e-06
Iter: 1513 loss: 2.34036179e-06
Iter: 1514 loss: 2.33707328e-06
Iter: 1515 loss: 2.34662548e-06
Iter: 1516 loss: 2.33600167e-06
Iter: 1517 loss: 2.33233141e-06
Iter: 1518 loss: 2.34473123e-06
Iter: 1519 loss: 2.33127912e-06
Iter: 1520 loss: 2.32803404e-06
Iter: 1521 loss: 2.33989499e-06
Iter: 1522 loss: 2.32727325e-06
Iter: 1523 loss: 2.32423031e-06
Iter: 1524 loss: 2.3354105e-06
Iter: 1525 loss: 2.32355524e-06
Iter: 1526 loss: 2.32103571e-06
Iter: 1527 loss: 2.33020728e-06
Iter: 1528 loss: 2.32049024e-06
Iter: 1529 loss: 2.31780928e-06
Iter: 1530 loss: 2.31944068e-06
Iter: 1531 loss: 2.31617469e-06
Iter: 1532 loss: 2.31315335e-06
Iter: 1533 loss: 2.3197772e-06
Iter: 1534 loss: 2.31210174e-06
Iter: 1535 loss: 2.30910791e-06
Iter: 1536 loss: 2.33286846e-06
Iter: 1537 loss: 2.30894034e-06
Iter: 1538 loss: 2.3067123e-06
Iter: 1539 loss: 2.30484034e-06
Iter: 1540 loss: 2.30415662e-06
Iter: 1541 loss: 2.30094838e-06
Iter: 1542 loss: 2.32354068e-06
Iter: 1543 loss: 2.30068645e-06
Iter: 1544 loss: 2.29780835e-06
Iter: 1545 loss: 2.30477644e-06
Iter: 1546 loss: 2.29677494e-06
Iter: 1547 loss: 2.29416582e-06
Iter: 1548 loss: 2.29419538e-06
Iter: 1549 loss: 2.2919653e-06
Iter: 1550 loss: 2.28841191e-06
Iter: 1551 loss: 2.31629451e-06
Iter: 1552 loss: 2.28811086e-06
Iter: 1553 loss: 2.2855545e-06
Iter: 1554 loss: 2.29233865e-06
Iter: 1555 loss: 2.28468662e-06
Iter: 1556 loss: 2.28207068e-06
Iter: 1557 loss: 2.29058742e-06
Iter: 1558 loss: 2.28138151e-06
Iter: 1559 loss: 2.27869623e-06
Iter: 1560 loss: 2.28375825e-06
Iter: 1561 loss: 2.27762303e-06
Iter: 1562 loss: 2.27454166e-06
Iter: 1563 loss: 2.28403178e-06
Iter: 1564 loss: 2.27363716e-06
Iter: 1565 loss: 2.27111718e-06
Iter: 1566 loss: 2.27269038e-06
Iter: 1567 loss: 2.2695383e-06
Iter: 1568 loss: 2.26649217e-06
Iter: 1569 loss: 2.28856265e-06
Iter: 1570 loss: 2.26631141e-06
Iter: 1571 loss: 2.26344628e-06
Iter: 1572 loss: 2.26362499e-06
Iter: 1573 loss: 2.26125621e-06
Iter: 1574 loss: 2.25855206e-06
Iter: 1575 loss: 2.27169016e-06
Iter: 1576 loss: 2.25815938e-06
Iter: 1577 loss: 2.25547501e-06
Iter: 1578 loss: 2.26467682e-06
Iter: 1579 loss: 2.25481108e-06
Iter: 1580 loss: 2.25221493e-06
Iter: 1581 loss: 2.25083636e-06
Iter: 1582 loss: 2.24970927e-06
Iter: 1583 loss: 2.24660175e-06
Iter: 1584 loss: 2.28040653e-06
Iter: 1585 loss: 2.24654696e-06
Iter: 1586 loss: 2.24394626e-06
Iter: 1587 loss: 2.24591804e-06
Iter: 1588 loss: 2.24239443e-06
Iter: 1589 loss: 2.23914913e-06
Iter: 1590 loss: 2.25332133e-06
Iter: 1591 loss: 2.23848656e-06
Iter: 1592 loss: 2.23553161e-06
Iter: 1593 loss: 2.24330347e-06
Iter: 1594 loss: 2.2345539e-06
Iter: 1595 loss: 2.23185839e-06
Iter: 1596 loss: 2.24440259e-06
Iter: 1597 loss: 2.23130883e-06
Iter: 1598 loss: 2.22909216e-06
Iter: 1599 loss: 2.22844915e-06
Iter: 1600 loss: 2.22696053e-06
Iter: 1601 loss: 2.2240597e-06
Iter: 1602 loss: 2.24496489e-06
Iter: 1603 loss: 2.22376298e-06
Iter: 1604 loss: 2.22101971e-06
Iter: 1605 loss: 2.22500717e-06
Iter: 1606 loss: 2.21965774e-06
Iter: 1607 loss: 2.21702908e-06
Iter: 1608 loss: 2.21920027e-06
Iter: 1609 loss: 2.2154818e-06
Iter: 1610 loss: 2.21235223e-06
Iter: 1611 loss: 2.2369295e-06
Iter: 1612 loss: 2.2121335e-06
Iter: 1613 loss: 2.20961374e-06
Iter: 1614 loss: 2.20919e-06
Iter: 1615 loss: 2.20744255e-06
Iter: 1616 loss: 2.20458355e-06
Iter: 1617 loss: 2.21947357e-06
Iter: 1618 loss: 2.20417178e-06
Iter: 1619 loss: 2.20122774e-06
Iter: 1620 loss: 2.20913398e-06
Iter: 1621 loss: 2.20038078e-06
Iter: 1622 loss: 2.19769026e-06
Iter: 1623 loss: 2.20658035e-06
Iter: 1624 loss: 2.19690196e-06
Iter: 1625 loss: 2.19411641e-06
Iter: 1626 loss: 2.20165884e-06
Iter: 1627 loss: 2.1931628e-06
Iter: 1628 loss: 2.19078947e-06
Iter: 1629 loss: 2.20359357e-06
Iter: 1630 loss: 2.19044159e-06
Iter: 1631 loss: 2.18834157e-06
Iter: 1632 loss: 2.18807281e-06
Iter: 1633 loss: 2.1864862e-06
Iter: 1634 loss: 2.18381524e-06
Iter: 1635 loss: 2.19530375e-06
Iter: 1636 loss: 2.18330501e-06
Iter: 1637 loss: 2.18056539e-06
Iter: 1638 loss: 2.1912258e-06
Iter: 1639 loss: 2.17989873e-06
Iter: 1640 loss: 2.17760612e-06
Iter: 1641 loss: 2.17619504e-06
Iter: 1642 loss: 2.17524735e-06
Iter: 1643 loss: 2.17203296e-06
Iter: 1644 loss: 2.20219408e-06
Iter: 1645 loss: 2.17193747e-06
Iter: 1646 loss: 2.16918579e-06
Iter: 1647 loss: 2.17093861e-06
Iter: 1648 loss: 2.16738636e-06
Iter: 1649 loss: 2.16455646e-06
Iter: 1650 loss: 2.16971e-06
Iter: 1651 loss: 2.16332251e-06
Iter: 1652 loss: 2.16034e-06
Iter: 1653 loss: 2.18362675e-06
Iter: 1654 loss: 2.16013041e-06
Iter: 1655 loss: 2.15801288e-06
Iter: 1656 loss: 2.16217109e-06
Iter: 1657 loss: 2.15715113e-06
Iter: 1658 loss: 2.15485534e-06
Iter: 1659 loss: 2.16409308e-06
Iter: 1660 loss: 2.15430691e-06
Iter: 1661 loss: 2.15225464e-06
Iter: 1662 loss: 2.15685986e-06
Iter: 1663 loss: 2.15147611e-06
Iter: 1664 loss: 2.14893407e-06
Iter: 1665 loss: 2.15144019e-06
Iter: 1666 loss: 2.14750526e-06
Iter: 1667 loss: 2.14477268e-06
Iter: 1668 loss: 2.15034152e-06
Iter: 1669 loss: 2.14371175e-06
Iter: 1670 loss: 2.14081228e-06
Iter: 1671 loss: 2.15994282e-06
Iter: 1672 loss: 2.14046895e-06
Iter: 1673 loss: 2.13815747e-06
Iter: 1674 loss: 2.13691692e-06
Iter: 1675 loss: 2.1357946e-06
Iter: 1676 loss: 2.13301701e-06
Iter: 1677 loss: 2.15262776e-06
Iter: 1678 loss: 2.13274393e-06
Iter: 1679 loss: 2.13015733e-06
Iter: 1680 loss: 2.13703538e-06
Iter: 1681 loss: 2.12926352e-06
Iter: 1682 loss: 2.12687019e-06
Iter: 1683 loss: 2.12640907e-06
Iter: 1684 loss: 2.12476425e-06
Iter: 1685 loss: 2.12174177e-06
Iter: 1686 loss: 2.14899796e-06
Iter: 1687 loss: 2.12158488e-06
Iter: 1688 loss: 2.11907809e-06
Iter: 1689 loss: 2.12315399e-06
Iter: 1690 loss: 2.11793053e-06
Iter: 1691 loss: 2.11538782e-06
Iter: 1692 loss: 2.12963073e-06
Iter: 1693 loss: 2.11500628e-06
Iter: 1694 loss: 2.11287966e-06
Iter: 1695 loss: 2.11587212e-06
Iter: 1696 loss: 2.11179486e-06
Iter: 1697 loss: 2.10922735e-06
Iter: 1698 loss: 2.11935139e-06
Iter: 1699 loss: 2.10868711e-06
Iter: 1700 loss: 2.10661051e-06
Iter: 1701 loss: 2.10678036e-06
Iter: 1702 loss: 2.10502e-06
Iter: 1703 loss: 2.10239023e-06
Iter: 1704 loss: 2.1243834e-06
Iter: 1705 loss: 2.1022629e-06
Iter: 1706 loss: 2.09997484e-06
Iter: 1707 loss: 2.10005828e-06
Iter: 1708 loss: 2.0981588e-06
Iter: 1709 loss: 2.0954642e-06
Iter: 1710 loss: 2.10069857e-06
Iter: 1711 loss: 2.09435075e-06
Iter: 1712 loss: 2.0914249e-06
Iter: 1713 loss: 2.11422821e-06
Iter: 1714 loss: 2.09123618e-06
Iter: 1715 loss: 2.08906476e-06
Iter: 1716 loss: 2.08758797e-06
Iter: 1717 loss: 2.08675e-06
Iter: 1718 loss: 2.08364122e-06
Iter: 1719 loss: 2.09971677e-06
Iter: 1720 loss: 2.08313122e-06
Iter: 1721 loss: 2.08019082e-06
Iter: 1722 loss: 2.09136806e-06
Iter: 1723 loss: 2.07945595e-06
Iter: 1724 loss: 2.07698827e-06
Iter: 1725 loss: 2.08566053e-06
Iter: 1726 loss: 2.07633207e-06
Iter: 1727 loss: 2.07387302e-06
Iter: 1728 loss: 2.07976746e-06
Iter: 1729 loss: 2.07294784e-06
Iter: 1730 loss: 2.07076823e-06
Iter: 1731 loss: 2.08184065e-06
Iter: 1732 loss: 2.07042785e-06
Iter: 1733 loss: 2.06845721e-06
Iter: 1734 loss: 2.0682628e-06
Iter: 1735 loss: 2.06679806e-06
Iter: 1736 loss: 2.06446703e-06
Iter: 1737 loss: 2.07923722e-06
Iter: 1738 loss: 2.06427831e-06
Iter: 1739 loss: 2.06188633e-06
Iter: 1740 loss: 2.06538334e-06
Iter: 1741 loss: 2.0607813e-06
Iter: 1742 loss: 2.05847255e-06
Iter: 1743 loss: 2.06010941e-06
Iter: 1744 loss: 2.05705874e-06
Iter: 1745 loss: 2.05479364e-06
Iter: 1746 loss: 2.07650351e-06
Iter: 1747 loss: 2.05467495e-06
Iter: 1748 loss: 2.05254923e-06
Iter: 1749 loss: 2.05187825e-06
Iter: 1750 loss: 2.05057427e-06
Iter: 1751 loss: 2.04795924e-06
Iter: 1752 loss: 2.05612241e-06
Iter: 1753 loss: 2.04714979e-06
Iter: 1754 loss: 2.04467597e-06
Iter: 1755 loss: 2.06160553e-06
Iter: 1756 loss: 2.04435514e-06
Iter: 1757 loss: 2.04224762e-06
Iter: 1758 loss: 2.04489561e-06
Iter: 1759 loss: 2.04111916e-06
Iter: 1760 loss: 2.03864352e-06
Iter: 1761 loss: 2.05169317e-06
Iter: 1762 loss: 2.03827e-06
Iter: 1763 loss: 2.03631771e-06
Iter: 1764 loss: 2.04040248e-06
Iter: 1765 loss: 2.03557829e-06
Iter: 1766 loss: 2.03324225e-06
Iter: 1767 loss: 2.03578747e-06
Iter: 1768 loss: 2.03198078e-06
Iter: 1769 loss: 2.0295488e-06
Iter: 1770 loss: 2.0362022e-06
Iter: 1771 loss: 2.02886986e-06
Iter: 1772 loss: 2.02641831e-06
Iter: 1773 loss: 2.04073763e-06
Iter: 1774 loss: 2.02607725e-06
Iter: 1775 loss: 2.02424894e-06
Iter: 1776 loss: 2.02329784e-06
Iter: 1777 loss: 2.02245769e-06
Iter: 1778 loss: 2.01991702e-06
Iter: 1779 loss: 2.03398645e-06
Iter: 1780 loss: 2.01958e-06
Iter: 1781 loss: 2.01712942e-06
Iter: 1782 loss: 2.02551519e-06
Iter: 1783 loss: 2.01649527e-06
Iter: 1784 loss: 2.01462717e-06
Iter: 1785 loss: 2.01457055e-06
Iter: 1786 loss: 2.01311127e-06
Iter: 1787 loss: 2.01074226e-06
Iter: 1788 loss: 2.02881847e-06
Iter: 1789 loss: 2.01057765e-06
Iter: 1790 loss: 2.00841077e-06
Iter: 1791 loss: 2.01225293e-06
Iter: 1792 loss: 2.00749355e-06
Iter: 1793 loss: 2.00540489e-06
Iter: 1794 loss: 2.01522653e-06
Iter: 1795 loss: 2.00496243e-06
Iter: 1796 loss: 2.00289287e-06
Iter: 1797 loss: 2.00584964e-06
Iter: 1798 loss: 2.00189425e-06
Iter: 1799 loss: 1.999613e-06
Iter: 1800 loss: 2.00928093e-06
Iter: 1801 loss: 1.99919123e-06
Iter: 1802 loss: 1.997254e-06
Iter: 1803 loss: 1.99735132e-06
Iter: 1804 loss: 1.99574856e-06
Iter: 1805 loss: 1.99340093e-06
Iter: 1806 loss: 2.01645389e-06
Iter: 1807 loss: 1.9933359e-06
Iter: 1808 loss: 1.99143733e-06
Iter: 1809 loss: 1.99073861e-06
Iter: 1810 loss: 1.98974521e-06
Iter: 1811 loss: 1.98729117e-06
Iter: 1812 loss: 1.9944373e-06
Iter: 1813 loss: 1.9865563e-06
Iter: 1814 loss: 1.98422867e-06
Iter: 1815 loss: 2.00142767e-06
Iter: 1816 loss: 1.98399312e-06
Iter: 1817 loss: 1.982208e-06
Iter: 1818 loss: 1.98148746e-06
Iter: 1819 loss: 1.9805284e-06
Iter: 1820 loss: 1.97827239e-06
Iter: 1821 loss: 1.98881253e-06
Iter: 1822 loss: 1.97784311e-06
Iter: 1823 loss: 1.97544159e-06
Iter: 1824 loss: 1.98456e-06
Iter: 1825 loss: 1.97493364e-06
Iter: 1826 loss: 1.97289978e-06
Iter: 1827 loss: 1.9798822e-06
Iter: 1828 loss: 1.97242571e-06
Iter: 1829 loss: 1.97044369e-06
Iter: 1830 loss: 1.97591453e-06
Iter: 1831 loss: 1.96981091e-06
Iter: 1832 loss: 1.96796123e-06
Iter: 1833 loss: 1.97290206e-06
Iter: 1834 loss: 1.96734641e-06
Iter: 1835 loss: 1.96531914e-06
Iter: 1836 loss: 1.96603378e-06
Iter: 1837 loss: 1.96376436e-06
Iter: 1838 loss: 1.96171982e-06
Iter: 1839 loss: 1.97811778e-06
Iter: 1840 loss: 1.96154542e-06
Iter: 1841 loss: 1.95953294e-06
Iter: 1842 loss: 1.9614431e-06
Iter: 1843 loss: 1.95841449e-06
Iter: 1844 loss: 1.95632401e-06
Iter: 1845 loss: 1.95734674e-06
Iter: 1846 loss: 1.95496932e-06
Iter: 1847 loss: 1.95254552e-06
Iter: 1848 loss: 1.97547206e-06
Iter: 1849 loss: 1.95245048e-06
Iter: 1850 loss: 1.95045277e-06
Iter: 1851 loss: 1.95077473e-06
Iter: 1852 loss: 1.94892414e-06
Iter: 1853 loss: 1.94656468e-06
Iter: 1854 loss: 1.95053508e-06
Iter: 1855 loss: 1.94552649e-06
Iter: 1856 loss: 1.94320728e-06
Iter: 1857 loss: 1.96450378e-06
Iter: 1858 loss: 1.94308473e-06
Iter: 1859 loss: 1.94128961e-06
Iter: 1860 loss: 1.94401855e-06
Iter: 1861 loss: 1.94036284e-06
Iter: 1862 loss: 1.93843198e-06
Iter: 1863 loss: 1.94920722e-06
Iter: 1864 loss: 1.93814549e-06
Iter: 1865 loss: 1.93654319e-06
Iter: 1866 loss: 1.93886967e-06
Iter: 1867 loss: 1.93586266e-06
Iter: 1868 loss: 1.93390497e-06
Iter: 1869 loss: 1.93795859e-06
Iter: 1870 loss: 1.93321284e-06
Iter: 1871 loss: 1.93143023e-06
Iter: 1872 loss: 1.93447954e-06
Iter: 1873 loss: 1.93057235e-06
Iter: 1874 loss: 1.92854259e-06
Iter: 1875 loss: 1.94103e-06
Iter: 1876 loss: 1.92828929e-06
Iter: 1877 loss: 1.92664766e-06
Iter: 1878 loss: 1.9247077e-06
Iter: 1879 loss: 1.92442144e-06
Iter: 1880 loss: 1.92200196e-06
Iter: 1881 loss: 1.94457675e-06
Iter: 1882 loss: 1.92193716e-06
Iter: 1883 loss: 1.92001335e-06
Iter: 1884 loss: 1.92762673e-06
Iter: 1885 loss: 1.91953268e-06
Iter: 1886 loss: 1.91791537e-06
Iter: 1887 loss: 1.91640856e-06
Iter: 1888 loss: 1.91604136e-06
Iter: 1889 loss: 1.91377012e-06
Iter: 1890 loss: 1.93553365e-06
Iter: 1891 loss: 1.91375807e-06
Iter: 1892 loss: 1.91171353e-06
Iter: 1893 loss: 1.91634763e-06
Iter: 1894 loss: 1.91098525e-06
Iter: 1895 loss: 1.90897686e-06
Iter: 1896 loss: 1.91568438e-06
Iter: 1897 loss: 1.90843502e-06
Iter: 1898 loss: 1.90645937e-06
Iter: 1899 loss: 1.91121239e-06
Iter: 1900 loss: 1.90574042e-06
Iter: 1901 loss: 1.90395349e-06
Iter: 1902 loss: 1.9115696e-06
Iter: 1903 loss: 1.90363e-06
Iter: 1904 loss: 1.90200421e-06
Iter: 1905 loss: 1.90150945e-06
Iter: 1906 loss: 1.90049468e-06
Iter: 1907 loss: 1.89845673e-06
Iter: 1908 loss: 1.91932736e-06
Iter: 1909 loss: 1.89834282e-06
Iter: 1910 loss: 1.89667355e-06
Iter: 1911 loss: 1.89709556e-06
Iter: 1912 loss: 1.8954313e-06
Iter: 1913 loss: 1.89361322e-06
Iter: 1914 loss: 1.89537934e-06
Iter: 1915 loss: 1.89251341e-06
Iter: 1916 loss: 1.89023694e-06
Iter: 1917 loss: 1.90957303e-06
Iter: 1918 loss: 1.89014065e-06
Iter: 1919 loss: 1.88834247e-06
Iter: 1920 loss: 1.88862657e-06
Iter: 1921 loss: 1.88700699e-06
Iter: 1922 loss: 1.88506533e-06
Iter: 1923 loss: 1.88869092e-06
Iter: 1924 loss: 1.88422359e-06
Iter: 1925 loss: 1.88188358e-06
Iter: 1926 loss: 1.89852483e-06
Iter: 1927 loss: 1.88171316e-06
Iter: 1928 loss: 1.87984278e-06
Iter: 1929 loss: 1.88363992e-06
Iter: 1930 loss: 1.87912235e-06
Iter: 1931 loss: 1.87737828e-06
Iter: 1932 loss: 1.88619515e-06
Iter: 1933 loss: 1.87707064e-06
Iter: 1934 loss: 1.87556202e-06
Iter: 1935 loss: 1.87778414e-06
Iter: 1936 loss: 1.87487933e-06
Iter: 1937 loss: 1.87313196e-06
Iter: 1938 loss: 1.87623925e-06
Iter: 1939 loss: 1.87232433e-06
Iter: 1940 loss: 1.87065154e-06
Iter: 1941 loss: 1.87677949e-06
Iter: 1942 loss: 1.87023329e-06
Iter: 1943 loss: 1.86837519e-06
Iter: 1944 loss: 1.87295961e-06
Iter: 1945 loss: 1.86777925e-06
Iter: 1946 loss: 1.86598777e-06
Iter: 1947 loss: 1.86467844e-06
Iter: 1948 loss: 1.86410057e-06
Iter: 1949 loss: 1.86198258e-06
Iter: 1950 loss: 1.89042225e-06
Iter: 1951 loss: 1.86199895e-06
Iter: 1952 loss: 1.86026091e-06
Iter: 1953 loss: 1.86256364e-06
Iter: 1954 loss: 1.85936983e-06
Iter: 1955 loss: 1.85757312e-06
Iter: 1956 loss: 1.85711622e-06
Iter: 1957 loss: 1.85606e-06
Iter: 1958 loss: 1.85400404e-06
Iter: 1959 loss: 1.87972796e-06
Iter: 1960 loss: 1.85397039e-06
Iter: 1961 loss: 1.85232238e-06
Iter: 1962 loss: 1.85607053e-06
Iter: 1963 loss: 1.85165254e-06
Iter: 1964 loss: 1.85012038e-06
Iter: 1965 loss: 1.8557605e-06
Iter: 1966 loss: 1.84974215e-06
Iter: 1967 loss: 1.84811915e-06
Iter: 1968 loss: 1.85025442e-06
Iter: 1969 loss: 1.84725047e-06
Iter: 1970 loss: 1.84551368e-06
Iter: 1971 loss: 1.85269175e-06
Iter: 1972 loss: 1.8451907e-06
Iter: 1973 loss: 1.84364512e-06
Iter: 1974 loss: 1.84464579e-06
Iter: 1975 loss: 1.84273142e-06
Iter: 1976 loss: 1.84100338e-06
Iter: 1977 loss: 1.85634951e-06
Iter: 1978 loss: 1.84096484e-06
Iter: 1979 loss: 1.83959423e-06
Iter: 1980 loss: 1.83886596e-06
Iter: 1981 loss: 1.83836107e-06
Iter: 1982 loss: 1.83663701e-06
Iter: 1983 loss: 1.8405874e-06
Iter: 1984 loss: 1.83599172e-06
Iter: 1985 loss: 1.83412953e-06
Iter: 1986 loss: 1.84889564e-06
Iter: 1987 loss: 1.83399948e-06
Iter: 1988 loss: 1.8325145e-06
Iter: 1989 loss: 1.8315402e-06
Iter: 1990 loss: 1.83102145e-06
Iter: 1991 loss: 1.82887516e-06
Iter: 1992 loss: 1.83509951e-06
Iter: 1993 loss: 1.8282559e-06
Iter: 1994 loss: 1.82640019e-06
Iter: 1995 loss: 1.84688088e-06
Iter: 1996 loss: 1.82637791e-06
Iter: 1997 loss: 1.82492147e-06
Iter: 1998 loss: 1.8263878e-06
Iter: 1999 loss: 1.82417352e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi1.6
+ date
Wed Nov  4 14:14:42 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.2/300_300_300_1 --function f2 --psi 3 --alpha 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f8bb67950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f8bacc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f8309d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f8308b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f8308bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f83075400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f83048510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82fa8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82fa8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f830169d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f83016ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f83016400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82ff9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82ff6620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82f40840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82ed19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82ed1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82e58840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82dc8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82dc8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82e58730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82e087b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82da9ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82da9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82da5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82cbba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f8baccd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f8308b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82f14158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82f14f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82d67488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82d32048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82d579d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82d32a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82c179d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f82c17268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.011890464
test_loss: 0.013773781
train_loss: 0.0085040545
test_loss: 0.011395633
train_loss: 0.009075026
test_loss: 0.010728105
train_loss: 0.0076034875
test_loss: 0.010155532
train_loss: 0.0075080954
test_loss: 0.009642793
train_loss: 0.008015901
test_loss: 0.00987282
train_loss: 0.0072086905
test_loss: 0.009645007
train_loss: 0.0071780095
test_loss: 0.009385474
train_loss: 0.0071657803
test_loss: 0.009368423
train_loss: 0.0068597617
test_loss: 0.009493013
train_loss: 0.006394118
test_loss: 0.008883567
train_loss: 0.0071229665
test_loss: 0.009347416
train_loss: 0.007032132
test_loss: 0.008942459
train_loss: 0.006343979
test_loss: 0.0091093285
train_loss: 0.0066469205
test_loss: 0.008989918
train_loss: 0.0065450273
test_loss: 0.009212447
train_loss: 0.006087018
test_loss: 0.0090882545
train_loss: 0.0068254233
test_loss: 0.008845431
train_loss: 0.006246186
test_loss: 0.008771153
train_loss: 0.006289587
test_loss: 0.008891387
train_loss: 0.0071817795
test_loss: 0.008815298
train_loss: 0.006010056
test_loss: 0.008659493
train_loss: 0.0060364353
test_loss: 0.00900797
train_loss: 0.006034489
test_loss: 0.00873018
train_loss: 0.005786074
test_loss: 0.008644846
train_loss: 0.006098264
test_loss: 0.008766521
train_loss: 0.0057644597
test_loss: 0.008422727
train_loss: 0.005564736
test_loss: 0.0085749
train_loss: 0.0061074398
test_loss: 0.008653783
train_loss: 0.006033552
test_loss: 0.008864662
train_loss: 0.005758006
test_loss: 0.008522154
train_loss: 0.005976465
test_loss: 0.008522257
train_loss: 0.0055682384
test_loss: 0.008560972
train_loss: 0.0057926215
test_loss: 0.0087291505
train_loss: 0.0058231307
test_loss: 0.008499764
train_loss: 0.006015635
test_loss: 0.008797155
train_loss: 0.0059618205
test_loss: 0.008359706
train_loss: 0.0061404337
test_loss: 0.008554486
train_loss: 0.0057228925
test_loss: 0.008528616
train_loss: 0.005503098
test_loss: 0.00830928
train_loss: 0.0058518266
test_loss: 0.008332767
train_loss: 0.005765951
test_loss: 0.008663031
train_loss: 0.0057013663
test_loss: 0.008399139
train_loss: 0.005519989
test_loss: 0.008423523
train_loss: 0.0058573103
test_loss: 0.008478854
train_loss: 0.0054927943
test_loss: 0.00829504
train_loss: 0.0059043244
test_loss: 0.008540197
train_loss: 0.00553484
test_loss: 0.008262391
train_loss: 0.0054089604
test_loss: 0.008074726
train_loss: 0.0053514997
test_loss: 0.008389937
train_loss: 0.0055225105
test_loss: 0.008298019
train_loss: 0.0055132303
test_loss: 0.008489589
train_loss: 0.0053781923
test_loss: 0.008223744
train_loss: 0.0059670834
test_loss: 0.008323847
train_loss: 0.0055149524
test_loss: 0.008402206
train_loss: 0.0057016127
test_loss: 0.008226414
train_loss: 0.0059066406
test_loss: 0.008364285
train_loss: 0.005369176
test_loss: 0.0083344225
train_loss: 0.005505225
test_loss: 0.0082226405
train_loss: 0.0051793773
test_loss: 0.008275972
train_loss: 0.005712276
test_loss: 0.008185764
train_loss: 0.0050987974
test_loss: 0.00814653
train_loss: 0.0054800357
test_loss: 0.008224051
train_loss: 0.0056799534
test_loss: 0.008364032
train_loss: 0.0054556476
test_loss: 0.008320992
train_loss: 0.0055690752
test_loss: 0.008236182
train_loss: 0.005534861
test_loss: 0.008106124
train_loss: 0.0051541114
test_loss: 0.008032222
train_loss: 0.005353205
test_loss: 0.008134925
train_loss: 0.0058216183
test_loss: 0.008249621
train_loss: 0.0058078845
test_loss: 0.00852119
train_loss: 0.005584616
test_loss: 0.0086200535
train_loss: 0.0053735333
test_loss: 0.008247321
train_loss: 0.00519816
test_loss: 0.00813178
train_loss: 0.0053050737
test_loss: 0.008140065
train_loss: 0.0050178394
test_loss: 0.008129709
train_loss: 0.005361814
test_loss: 0.008030595
train_loss: 0.005042081
test_loss: 0.008141448
train_loss: 0.0053299624
test_loss: 0.008122797
train_loss: 0.0052809
test_loss: 0.008172415
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.6/300_300_300_1 --optimizer lbfgs --function f2 --psi 3 --alpha 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f090fc85598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f090fc85950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc631b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc59a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc59ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc5b1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc580ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc531840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc525620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc4f6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc4f6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc49a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc4cea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc476620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc476598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc476c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08d1fad730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08d1f86840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08d1f356a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08d1f86a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08d1ee8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08d1f09950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08d1eb3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08ac75f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08ac75fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08ac725a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc4d62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08fc4d6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08ac6ea400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08ac6b0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08ac7136a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08ac6b0a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08ac6ca510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08ac6b99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08ac6b9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08ac5df378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.78231266e-05
Iter: 2 loss: 4.75583583e-05
Iter: 3 loss: 8.71315424e-05
Iter: 4 loss: 4.52005188e-05
Iter: 5 loss: 3.86328538e-05
Iter: 6 loss: 5.08812227e-05
Iter: 7 loss: 3.58000616e-05
Iter: 8 loss: 3.12957272e-05
Iter: 9 loss: 6.25920293e-05
Iter: 10 loss: 3.0846888e-05
Iter: 11 loss: 2.79844207e-05
Iter: 12 loss: 3.35989134e-05
Iter: 13 loss: 2.68083986e-05
Iter: 14 loss: 2.48658853e-05
Iter: 15 loss: 3.05934846e-05
Iter: 16 loss: 2.4271525e-05
Iter: 17 loss: 2.24024898e-05
Iter: 18 loss: 3.47648602e-05
Iter: 19 loss: 2.22040908e-05
Iter: 20 loss: 2.10224152e-05
Iter: 21 loss: 2.12354826e-05
Iter: 22 loss: 2.01392686e-05
Iter: 23 loss: 1.89901075e-05
Iter: 24 loss: 2.99089315e-05
Iter: 25 loss: 1.89455059e-05
Iter: 26 loss: 1.79735907e-05
Iter: 27 loss: 2.05456363e-05
Iter: 28 loss: 1.76471876e-05
Iter: 29 loss: 1.69449631e-05
Iter: 30 loss: 1.81484284e-05
Iter: 31 loss: 1.66314949e-05
Iter: 32 loss: 1.59153115e-05
Iter: 33 loss: 2.26323282e-05
Iter: 34 loss: 1.5886475e-05
Iter: 35 loss: 1.54337467e-05
Iter: 36 loss: 1.54135632e-05
Iter: 37 loss: 1.50655806e-05
Iter: 38 loss: 1.45106642e-05
Iter: 39 loss: 1.52494213e-05
Iter: 40 loss: 1.42307745e-05
Iter: 41 loss: 1.38800679e-05
Iter: 42 loss: 1.3880157e-05
Iter: 43 loss: 1.35361179e-05
Iter: 44 loss: 1.46278135e-05
Iter: 45 loss: 1.34377442e-05
Iter: 46 loss: 1.31999141e-05
Iter: 47 loss: 1.35322944e-05
Iter: 48 loss: 1.30821463e-05
Iter: 49 loss: 1.27971489e-05
Iter: 50 loss: 1.38396263e-05
Iter: 51 loss: 1.27262338e-05
Iter: 52 loss: 1.25019542e-05
Iter: 53 loss: 1.25724418e-05
Iter: 54 loss: 1.23414902e-05
Iter: 55 loss: 1.21044613e-05
Iter: 56 loss: 1.47434575e-05
Iter: 57 loss: 1.20997393e-05
Iter: 58 loss: 1.19083052e-05
Iter: 59 loss: 1.18913376e-05
Iter: 60 loss: 1.17500649e-05
Iter: 61 loss: 1.15478806e-05
Iter: 62 loss: 1.22966576e-05
Iter: 63 loss: 1.14982904e-05
Iter: 64 loss: 1.13066244e-05
Iter: 65 loss: 1.28868869e-05
Iter: 66 loss: 1.12947455e-05
Iter: 67 loss: 1.11673053e-05
Iter: 68 loss: 1.10940373e-05
Iter: 69 loss: 1.10393212e-05
Iter: 70 loss: 1.08834092e-05
Iter: 71 loss: 1.2939915e-05
Iter: 72 loss: 1.08826689e-05
Iter: 73 loss: 1.07587693e-05
Iter: 74 loss: 1.06623293e-05
Iter: 75 loss: 1.06234793e-05
Iter: 76 loss: 1.0455653e-05
Iter: 77 loss: 1.05375293e-05
Iter: 78 loss: 1.0343354e-05
Iter: 79 loss: 1.02186486e-05
Iter: 80 loss: 1.0217138e-05
Iter: 81 loss: 1.01006644e-05
Iter: 82 loss: 1.05472045e-05
Iter: 83 loss: 1.00731577e-05
Iter: 84 loss: 9.98652195e-06
Iter: 85 loss: 9.931764e-06
Iter: 86 loss: 9.89749878e-06
Iter: 87 loss: 9.8003984e-06
Iter: 88 loss: 1.12304924e-05
Iter: 89 loss: 9.80028562e-06
Iter: 90 loss: 9.72762518e-06
Iter: 91 loss: 9.62757531e-06
Iter: 92 loss: 9.62304057e-06
Iter: 93 loss: 9.54705865e-06
Iter: 94 loss: 9.54501775e-06
Iter: 95 loss: 9.47964509e-06
Iter: 96 loss: 9.39480924e-06
Iter: 97 loss: 9.38924495e-06
Iter: 98 loss: 9.3041017e-06
Iter: 99 loss: 1.00876614e-05
Iter: 100 loss: 9.3003855e-06
Iter: 101 loss: 9.2218088e-06
Iter: 102 loss: 9.46036926e-06
Iter: 103 loss: 9.19844479e-06
Iter: 104 loss: 9.1291331e-06
Iter: 105 loss: 9.15589771e-06
Iter: 106 loss: 9.08130096e-06
Iter: 107 loss: 9.00090254e-06
Iter: 108 loss: 9.75113289e-06
Iter: 109 loss: 8.9975365e-06
Iter: 110 loss: 8.93833294e-06
Iter: 111 loss: 8.96600432e-06
Iter: 112 loss: 8.89840339e-06
Iter: 113 loss: 8.83520624e-06
Iter: 114 loss: 8.87979877e-06
Iter: 115 loss: 8.79580057e-06
Iter: 116 loss: 8.72883629e-06
Iter: 117 loss: 9.12636824e-06
Iter: 118 loss: 8.72036071e-06
Iter: 119 loss: 8.65774382e-06
Iter: 120 loss: 9.25059794e-06
Iter: 121 loss: 8.65519632e-06
Iter: 122 loss: 8.61740773e-06
Iter: 123 loss: 8.54430164e-06
Iter: 124 loss: 1.00551806e-05
Iter: 125 loss: 8.54367863e-06
Iter: 126 loss: 8.48338823e-06
Iter: 127 loss: 8.48341733e-06
Iter: 128 loss: 8.43314047e-06
Iter: 129 loss: 8.44386705e-06
Iter: 130 loss: 8.39572e-06
Iter: 131 loss: 8.33695867e-06
Iter: 132 loss: 8.51250479e-06
Iter: 133 loss: 8.31914349e-06
Iter: 134 loss: 8.26311953e-06
Iter: 135 loss: 8.62588786e-06
Iter: 136 loss: 8.25716234e-06
Iter: 137 loss: 8.2149154e-06
Iter: 138 loss: 8.20621881e-06
Iter: 139 loss: 8.17850923e-06
Iter: 140 loss: 8.12942926e-06
Iter: 141 loss: 8.62213074e-06
Iter: 142 loss: 8.12790495e-06
Iter: 143 loss: 8.08273398e-06
Iter: 144 loss: 8.10209713e-06
Iter: 145 loss: 8.05205309e-06
Iter: 146 loss: 8.01327769e-06
Iter: 147 loss: 8.1678e-06
Iter: 148 loss: 8.00461385e-06
Iter: 149 loss: 7.95454253e-06
Iter: 150 loss: 8.00468479e-06
Iter: 151 loss: 7.92626179e-06
Iter: 152 loss: 7.88171565e-06
Iter: 153 loss: 7.88016405e-06
Iter: 154 loss: 7.84563417e-06
Iter: 155 loss: 7.7941e-06
Iter: 156 loss: 8.12201324e-06
Iter: 157 loss: 7.78822323e-06
Iter: 158 loss: 7.73552074e-06
Iter: 159 loss: 8.14991108e-06
Iter: 160 loss: 7.7319819e-06
Iter: 161 loss: 7.70347651e-06
Iter: 162 loss: 7.66914854e-06
Iter: 163 loss: 7.66563335e-06
Iter: 164 loss: 7.62289028e-06
Iter: 165 loss: 7.91336061e-06
Iter: 166 loss: 7.61885349e-06
Iter: 167 loss: 7.57980069e-06
Iter: 168 loss: 7.74581258e-06
Iter: 169 loss: 7.57160842e-06
Iter: 170 loss: 7.54128723e-06
Iter: 171 loss: 7.52464712e-06
Iter: 172 loss: 7.51127e-06
Iter: 173 loss: 7.47145032e-06
Iter: 174 loss: 7.97430675e-06
Iter: 175 loss: 7.47095328e-06
Iter: 176 loss: 7.44026602e-06
Iter: 177 loss: 7.40996165e-06
Iter: 178 loss: 7.40348969e-06
Iter: 179 loss: 7.36920174e-06
Iter: 180 loss: 7.36923312e-06
Iter: 181 loss: 7.34046898e-06
Iter: 182 loss: 7.33643219e-06
Iter: 183 loss: 7.3164083e-06
Iter: 184 loss: 7.28233044e-06
Iter: 185 loss: 7.41609756e-06
Iter: 186 loss: 7.27466067e-06
Iter: 187 loss: 7.24414485e-06
Iter: 188 loss: 7.41058284e-06
Iter: 189 loss: 7.23967787e-06
Iter: 190 loss: 7.2141811e-06
Iter: 191 loss: 7.19751915e-06
Iter: 192 loss: 7.18776937e-06
Iter: 193 loss: 7.15193e-06
Iter: 194 loss: 7.27194947e-06
Iter: 195 loss: 7.14213093e-06
Iter: 196 loss: 7.11745179e-06
Iter: 197 loss: 7.11712028e-06
Iter: 198 loss: 7.09561664e-06
Iter: 199 loss: 7.05892035e-06
Iter: 200 loss: 7.05875254e-06
Iter: 201 loss: 7.02370153e-06
Iter: 202 loss: 7.08717107e-06
Iter: 203 loss: 7.0084493e-06
Iter: 204 loss: 6.97799214e-06
Iter: 205 loss: 7.45286525e-06
Iter: 206 loss: 6.97795713e-06
Iter: 207 loss: 6.95158678e-06
Iter: 208 loss: 6.94246773e-06
Iter: 209 loss: 6.92751337e-06
Iter: 210 loss: 6.90215029e-06
Iter: 211 loss: 7.03739215e-06
Iter: 212 loss: 6.89838635e-06
Iter: 213 loss: 6.87019065e-06
Iter: 214 loss: 6.93701168e-06
Iter: 215 loss: 6.86004114e-06
Iter: 216 loss: 6.83642884e-06
Iter: 217 loss: 6.8733716e-06
Iter: 218 loss: 6.82535438e-06
Iter: 219 loss: 6.79449295e-06
Iter: 220 loss: 6.92276262e-06
Iter: 221 loss: 6.78794459e-06
Iter: 222 loss: 6.76098716e-06
Iter: 223 loss: 6.78698871e-06
Iter: 224 loss: 6.74566036e-06
Iter: 225 loss: 6.72233546e-06
Iter: 226 loss: 6.85838268e-06
Iter: 227 loss: 6.71920679e-06
Iter: 228 loss: 6.69248493e-06
Iter: 229 loss: 6.69002156e-06
Iter: 230 loss: 6.67019822e-06
Iter: 231 loss: 6.64213439e-06
Iter: 232 loss: 6.70368536e-06
Iter: 233 loss: 6.63121318e-06
Iter: 234 loss: 6.61638023e-06
Iter: 235 loss: 6.61440527e-06
Iter: 236 loss: 6.60014075e-06
Iter: 237 loss: 6.568152e-06
Iter: 238 loss: 7.00584587e-06
Iter: 239 loss: 6.56618658e-06
Iter: 240 loss: 6.53732104e-06
Iter: 241 loss: 6.61415334e-06
Iter: 242 loss: 6.52756626e-06
Iter: 243 loss: 6.50385891e-06
Iter: 244 loss: 6.50391485e-06
Iter: 245 loss: 6.48494643e-06
Iter: 246 loss: 6.4848723e-06
Iter: 247 loss: 6.46961144e-06
Iter: 248 loss: 6.44703414e-06
Iter: 249 loss: 6.50613129e-06
Iter: 250 loss: 6.43944622e-06
Iter: 251 loss: 6.41707447e-06
Iter: 252 loss: 6.57698729e-06
Iter: 253 loss: 6.41519819e-06
Iter: 254 loss: 6.39798691e-06
Iter: 255 loss: 6.40884446e-06
Iter: 256 loss: 6.38705205e-06
Iter: 257 loss: 6.36748427e-06
Iter: 258 loss: 6.47372144e-06
Iter: 259 loss: 6.36456025e-06
Iter: 260 loss: 6.34417484e-06
Iter: 261 loss: 6.35115157e-06
Iter: 262 loss: 6.32986666e-06
Iter: 263 loss: 6.30742488e-06
Iter: 264 loss: 6.36542882e-06
Iter: 265 loss: 6.29992064e-06
Iter: 266 loss: 6.27790359e-06
Iter: 267 loss: 6.4401811e-06
Iter: 268 loss: 6.27610098e-06
Iter: 269 loss: 6.25984103e-06
Iter: 270 loss: 6.24232052e-06
Iter: 271 loss: 6.23980395e-06
Iter: 272 loss: 6.22367406e-06
Iter: 273 loss: 6.22295647e-06
Iter: 274 loss: 6.20660649e-06
Iter: 275 loss: 6.19765342e-06
Iter: 276 loss: 6.19052753e-06
Iter: 277 loss: 6.17044043e-06
Iter: 278 loss: 6.18198919e-06
Iter: 279 loss: 6.15748922e-06
Iter: 280 loss: 6.13718976e-06
Iter: 281 loss: 6.21915e-06
Iter: 282 loss: 6.13255179e-06
Iter: 283 loss: 6.11016094e-06
Iter: 284 loss: 6.26525889e-06
Iter: 285 loss: 6.10788175e-06
Iter: 286 loss: 6.09324798e-06
Iter: 287 loss: 6.07473703e-06
Iter: 288 loss: 6.0733355e-06
Iter: 289 loss: 6.05796731e-06
Iter: 290 loss: 6.0572138e-06
Iter: 291 loss: 6.04405068e-06
Iter: 292 loss: 6.03315038e-06
Iter: 293 loss: 6.02935097e-06
Iter: 294 loss: 6.00958356e-06
Iter: 295 loss: 6.13653e-06
Iter: 296 loss: 6.0075231e-06
Iter: 297 loss: 5.99067334e-06
Iter: 298 loss: 6.04101751e-06
Iter: 299 loss: 5.9857648e-06
Iter: 300 loss: 5.97100734e-06
Iter: 301 loss: 5.95725487e-06
Iter: 302 loss: 5.95395613e-06
Iter: 303 loss: 5.93248797e-06
Iter: 304 loss: 6.26006795e-06
Iter: 305 loss: 5.93256209e-06
Iter: 306 loss: 5.91980142e-06
Iter: 307 loss: 5.91205298e-06
Iter: 308 loss: 5.90702712e-06
Iter: 309 loss: 5.89184128e-06
Iter: 310 loss: 6.11471296e-06
Iter: 311 loss: 5.89191859e-06
Iter: 312 loss: 5.87806744e-06
Iter: 313 loss: 5.87855e-06
Iter: 314 loss: 5.86723e-06
Iter: 315 loss: 5.85372436e-06
Iter: 316 loss: 5.83713518e-06
Iter: 317 loss: 5.83554311e-06
Iter: 318 loss: 5.8108476e-06
Iter: 319 loss: 5.96466043e-06
Iter: 320 loss: 5.80799133e-06
Iter: 321 loss: 5.79170137e-06
Iter: 322 loss: 5.97282724e-06
Iter: 323 loss: 5.79143489e-06
Iter: 324 loss: 5.77637957e-06
Iter: 325 loss: 5.78071376e-06
Iter: 326 loss: 5.7655343e-06
Iter: 327 loss: 5.74839532e-06
Iter: 328 loss: 5.76117418e-06
Iter: 329 loss: 5.73797797e-06
Iter: 330 loss: 5.71831333e-06
Iter: 331 loss: 5.9447575e-06
Iter: 332 loss: 5.71801183e-06
Iter: 333 loss: 5.70463271e-06
Iter: 334 loss: 5.7176303e-06
Iter: 335 loss: 5.697083e-06
Iter: 336 loss: 5.68321821e-06
Iter: 337 loss: 5.72722547e-06
Iter: 338 loss: 5.67926872e-06
Iter: 339 loss: 5.66307835e-06
Iter: 340 loss: 5.69799249e-06
Iter: 341 loss: 5.65692699e-06
Iter: 342 loss: 5.64291531e-06
Iter: 343 loss: 5.64854599e-06
Iter: 344 loss: 5.63328922e-06
Iter: 345 loss: 5.61909383e-06
Iter: 346 loss: 5.82467737e-06
Iter: 347 loss: 5.61910474e-06
Iter: 348 loss: 5.60868e-06
Iter: 349 loss: 5.60292938e-06
Iter: 350 loss: 5.59836053e-06
Iter: 351 loss: 5.58552665e-06
Iter: 352 loss: 5.79074094e-06
Iter: 353 loss: 5.58550437e-06
Iter: 354 loss: 5.57632575e-06
Iter: 355 loss: 5.55769202e-06
Iter: 356 loss: 5.90259606e-06
Iter: 357 loss: 5.55735687e-06
Iter: 358 loss: 5.53713471e-06
Iter: 359 loss: 5.5431783e-06
Iter: 360 loss: 5.52261736e-06
Iter: 361 loss: 5.51194807e-06
Iter: 362 loss: 5.50943651e-06
Iter: 363 loss: 5.49597e-06
Iter: 364 loss: 5.48840399e-06
Iter: 365 loss: 5.48248909e-06
Iter: 366 loss: 5.46731189e-06
Iter: 367 loss: 5.51534094e-06
Iter: 368 loss: 5.46301362e-06
Iter: 369 loss: 5.44835348e-06
Iter: 370 loss: 5.56862506e-06
Iter: 371 loss: 5.44749491e-06
Iter: 372 loss: 5.43603119e-06
Iter: 373 loss: 5.43808892e-06
Iter: 374 loss: 5.42752423e-06
Iter: 375 loss: 5.41377631e-06
Iter: 376 loss: 5.46675e-06
Iter: 377 loss: 5.41050531e-06
Iter: 378 loss: 5.3946892e-06
Iter: 379 loss: 5.42344e-06
Iter: 380 loss: 5.38788299e-06
Iter: 381 loss: 5.37518372e-06
Iter: 382 loss: 5.38304266e-06
Iter: 383 loss: 5.36688458e-06
Iter: 384 loss: 5.35217805e-06
Iter: 385 loss: 5.52863094e-06
Iter: 386 loss: 5.35197159e-06
Iter: 387 loss: 5.34182163e-06
Iter: 388 loss: 5.34499122e-06
Iter: 389 loss: 5.33454522e-06
Iter: 390 loss: 5.31919432e-06
Iter: 391 loss: 5.40092333e-06
Iter: 392 loss: 5.31672504e-06
Iter: 393 loss: 5.30597208e-06
Iter: 394 loss: 5.29582212e-06
Iter: 395 loss: 5.29329645e-06
Iter: 396 loss: 5.27644261e-06
Iter: 397 loss: 5.31757541e-06
Iter: 398 loss: 5.27051543e-06
Iter: 399 loss: 5.25640644e-06
Iter: 400 loss: 5.31607475e-06
Iter: 401 loss: 5.25336873e-06
Iter: 402 loss: 5.24101506e-06
Iter: 403 loss: 5.3650856e-06
Iter: 404 loss: 5.24066127e-06
Iter: 405 loss: 5.230032e-06
Iter: 406 loss: 5.22151e-06
Iter: 407 loss: 5.21812763e-06
Iter: 408 loss: 5.20474e-06
Iter: 409 loss: 5.25956875e-06
Iter: 410 loss: 5.20179037e-06
Iter: 411 loss: 5.18689558e-06
Iter: 412 loss: 5.26615486e-06
Iter: 413 loss: 5.18465913e-06
Iter: 414 loss: 5.17385206e-06
Iter: 415 loss: 5.17367698e-06
Iter: 416 loss: 5.1652205e-06
Iter: 417 loss: 5.15407919e-06
Iter: 418 loss: 5.26885106e-06
Iter: 419 loss: 5.15372949e-06
Iter: 420 loss: 5.14364638e-06
Iter: 421 loss: 5.13868235e-06
Iter: 422 loss: 5.1337347e-06
Iter: 423 loss: 5.11919734e-06
Iter: 424 loss: 5.16356704e-06
Iter: 425 loss: 5.11499366e-06
Iter: 426 loss: 5.10177961e-06
Iter: 427 loss: 5.25893711e-06
Iter: 428 loss: 5.10166274e-06
Iter: 429 loss: 5.09374058e-06
Iter: 430 loss: 5.09289157e-06
Iter: 431 loss: 5.08725134e-06
Iter: 432 loss: 5.07405912e-06
Iter: 433 loss: 5.10519658e-06
Iter: 434 loss: 5.06923152e-06
Iter: 435 loss: 5.05862499e-06
Iter: 436 loss: 5.05239404e-06
Iter: 437 loss: 5.047993e-06
Iter: 438 loss: 5.03211049e-06
Iter: 439 loss: 5.04524e-06
Iter: 440 loss: 5.02272769e-06
Iter: 441 loss: 5.00607212e-06
Iter: 442 loss: 5.1441848e-06
Iter: 443 loss: 5.00513397e-06
Iter: 444 loss: 4.99363e-06
Iter: 445 loss: 5.15439569e-06
Iter: 446 loss: 4.99361249e-06
Iter: 447 loss: 4.98481222e-06
Iter: 448 loss: 4.97696601e-06
Iter: 449 loss: 4.97467863e-06
Iter: 450 loss: 4.96165285e-06
Iter: 451 loss: 5.02116063e-06
Iter: 452 loss: 4.95926497e-06
Iter: 453 loss: 4.94928281e-06
Iter: 454 loss: 5.04193713e-06
Iter: 455 loss: 4.94877622e-06
Iter: 456 loss: 4.94037613e-06
Iter: 457 loss: 4.92688e-06
Iter: 458 loss: 4.92678373e-06
Iter: 459 loss: 4.91458923e-06
Iter: 460 loss: 5.0438548e-06
Iter: 461 loss: 4.91422361e-06
Iter: 462 loss: 4.90349703e-06
Iter: 463 loss: 4.93213065e-06
Iter: 464 loss: 4.89993135e-06
Iter: 465 loss: 4.88943624e-06
Iter: 466 loss: 4.8990496e-06
Iter: 467 loss: 4.88354908e-06
Iter: 468 loss: 4.87185025e-06
Iter: 469 loss: 5.00152692e-06
Iter: 470 loss: 4.8715292e-06
Iter: 471 loss: 4.8635311e-06
Iter: 472 loss: 4.85083319e-06
Iter: 473 loss: 4.85063083e-06
Iter: 474 loss: 4.83719941e-06
Iter: 475 loss: 4.97887322e-06
Iter: 476 loss: 4.83696658e-06
Iter: 477 loss: 4.8264983e-06
Iter: 478 loss: 4.82798714e-06
Iter: 479 loss: 4.81858842e-06
Iter: 480 loss: 4.8063157e-06
Iter: 481 loss: 4.82421683e-06
Iter: 482 loss: 4.80033259e-06
Iter: 483 loss: 4.78760603e-06
Iter: 484 loss: 4.8102238e-06
Iter: 485 loss: 4.78180118e-06
Iter: 486 loss: 4.77216463e-06
Iter: 487 loss: 4.77202138e-06
Iter: 488 loss: 4.762971e-06
Iter: 489 loss: 4.75867091e-06
Iter: 490 loss: 4.75428214e-06
Iter: 491 loss: 4.74400485e-06
Iter: 492 loss: 4.75374145e-06
Iter: 493 loss: 4.73819046e-06
Iter: 494 loss: 4.72443662e-06
Iter: 495 loss: 4.86709177e-06
Iter: 496 loss: 4.72415149e-06
Iter: 497 loss: 4.71627163e-06
Iter: 498 loss: 4.70182113e-06
Iter: 499 loss: 5.02875218e-06
Iter: 500 loss: 4.70186478e-06
Iter: 501 loss: 4.69008228e-06
Iter: 502 loss: 4.69006363e-06
Iter: 503 loss: 4.68034887e-06
Iter: 504 loss: 4.70364375e-06
Iter: 505 loss: 4.67691e-06
Iter: 506 loss: 4.66897836e-06
Iter: 507 loss: 4.70152781e-06
Iter: 508 loss: 4.66725396e-06
Iter: 509 loss: 4.65760195e-06
Iter: 510 loss: 4.65862331e-06
Iter: 511 loss: 4.65026e-06
Iter: 512 loss: 4.63995912e-06
Iter: 513 loss: 4.65239555e-06
Iter: 514 loss: 4.63455035e-06
Iter: 515 loss: 4.62355911e-06
Iter: 516 loss: 4.68687813e-06
Iter: 517 loss: 4.62218077e-06
Iter: 518 loss: 4.61145919e-06
Iter: 519 loss: 4.62427852e-06
Iter: 520 loss: 4.60591946e-06
Iter: 521 loss: 4.5946781e-06
Iter: 522 loss: 4.60806041e-06
Iter: 523 loss: 4.58891554e-06
Iter: 524 loss: 4.57649321e-06
Iter: 525 loss: 4.59670673e-06
Iter: 526 loss: 4.57091119e-06
Iter: 527 loss: 4.56111911e-06
Iter: 528 loss: 4.56096723e-06
Iter: 529 loss: 4.553297e-06
Iter: 530 loss: 4.54720976e-06
Iter: 531 loss: 4.54498377e-06
Iter: 532 loss: 4.53443636e-06
Iter: 533 loss: 4.56275075e-06
Iter: 534 loss: 4.53096709e-06
Iter: 535 loss: 4.52080803e-06
Iter: 536 loss: 4.63584729e-06
Iter: 537 loss: 4.52060613e-06
Iter: 538 loss: 4.51437199e-06
Iter: 539 loss: 4.5014358e-06
Iter: 540 loss: 4.72559e-06
Iter: 541 loss: 4.50114612e-06
Iter: 542 loss: 4.49053459e-06
Iter: 543 loss: 4.49055096e-06
Iter: 544 loss: 4.48054834e-06
Iter: 545 loss: 4.49992876e-06
Iter: 546 loss: 4.47645334e-06
Iter: 547 loss: 4.46764807e-06
Iter: 548 loss: 4.48267656e-06
Iter: 549 loss: 4.46371087e-06
Iter: 550 loss: 4.45246542e-06
Iter: 551 loss: 4.47750972e-06
Iter: 552 loss: 4.44815851e-06
Iter: 553 loss: 4.43758836e-06
Iter: 554 loss: 4.43679e-06
Iter: 555 loss: 4.42888495e-06
Iter: 556 loss: 4.41839165e-06
Iter: 557 loss: 4.46860577e-06
Iter: 558 loss: 4.41652037e-06
Iter: 559 loss: 4.40621216e-06
Iter: 560 loss: 4.47325192e-06
Iter: 561 loss: 4.40504118e-06
Iter: 562 loss: 4.39705673e-06
Iter: 563 loss: 4.39065434e-06
Iter: 564 loss: 4.38821962e-06
Iter: 565 loss: 4.37731296e-06
Iter: 566 loss: 4.41788552e-06
Iter: 567 loss: 4.37459448e-06
Iter: 568 loss: 4.36376e-06
Iter: 569 loss: 4.49266145e-06
Iter: 570 loss: 4.36359278e-06
Iter: 571 loss: 4.3574646e-06
Iter: 572 loss: 4.34782396e-06
Iter: 573 loss: 4.34772346e-06
Iter: 574 loss: 4.33600599e-06
Iter: 575 loss: 4.44937e-06
Iter: 576 loss: 4.33565765e-06
Iter: 577 loss: 4.32747174e-06
Iter: 578 loss: 4.37023209e-06
Iter: 579 loss: 4.32626348e-06
Iter: 580 loss: 4.31953595e-06
Iter: 581 loss: 4.30793443e-06
Iter: 582 loss: 4.30787532e-06
Iter: 583 loss: 4.30249474e-06
Iter: 584 loss: 4.30068394e-06
Iter: 585 loss: 4.29439206e-06
Iter: 586 loss: 4.2880506e-06
Iter: 587 loss: 4.28678413e-06
Iter: 588 loss: 4.27774e-06
Iter: 589 loss: 4.30409818e-06
Iter: 590 loss: 4.27493524e-06
Iter: 591 loss: 4.26524184e-06
Iter: 592 loss: 4.31373883e-06
Iter: 593 loss: 4.2634947e-06
Iter: 594 loss: 4.25693406e-06
Iter: 595 loss: 4.24912196e-06
Iter: 596 loss: 4.24844484e-06
Iter: 597 loss: 4.23713163e-06
Iter: 598 loss: 4.27768055e-06
Iter: 599 loss: 4.23430447e-06
Iter: 600 loss: 4.22322591e-06
Iter: 601 loss: 4.30591808e-06
Iter: 602 loss: 4.22244557e-06
Iter: 603 loss: 4.2131619e-06
Iter: 604 loss: 4.22809626e-06
Iter: 605 loss: 4.20904053e-06
Iter: 606 loss: 4.20083779e-06
Iter: 607 loss: 4.20754668e-06
Iter: 608 loss: 4.19590106e-06
Iter: 609 loss: 4.18672107e-06
Iter: 610 loss: 4.31654917e-06
Iter: 611 loss: 4.18669606e-06
Iter: 612 loss: 4.18074251e-06
Iter: 613 loss: 4.17233696e-06
Iter: 614 loss: 4.17207048e-06
Iter: 615 loss: 4.1635044e-06
Iter: 616 loss: 4.24728023e-06
Iter: 617 loss: 4.16319835e-06
Iter: 618 loss: 4.15453815e-06
Iter: 619 loss: 4.16863804e-06
Iter: 620 loss: 4.15050908e-06
Iter: 621 loss: 4.14280385e-06
Iter: 622 loss: 4.14045235e-06
Iter: 623 loss: 4.1358644e-06
Iter: 624 loss: 4.12967074e-06
Iter: 625 loss: 4.12899408e-06
Iter: 626 loss: 4.12357213e-06
Iter: 627 loss: 4.11565543e-06
Iter: 628 loss: 4.11558722e-06
Iter: 629 loss: 4.1068588e-06
Iter: 630 loss: 4.16157081e-06
Iter: 631 loss: 4.10585108e-06
Iter: 632 loss: 4.09876202e-06
Iter: 633 loss: 4.12845839e-06
Iter: 634 loss: 4.09722861e-06
Iter: 635 loss: 4.09112181e-06
Iter: 636 loss: 4.07913149e-06
Iter: 637 loss: 4.31854642e-06
Iter: 638 loss: 4.07900916e-06
Iter: 639 loss: 4.06757044e-06
Iter: 640 loss: 4.18075342e-06
Iter: 641 loss: 4.0670684e-06
Iter: 642 loss: 4.05881201e-06
Iter: 643 loss: 4.13181169e-06
Iter: 644 loss: 4.05827905e-06
Iter: 645 loss: 4.05178343e-06
Iter: 646 loss: 4.04876937e-06
Iter: 647 loss: 4.0455011e-06
Iter: 648 loss: 4.03790636e-06
Iter: 649 loss: 4.12022109e-06
Iter: 650 loss: 4.03768627e-06
Iter: 651 loss: 4.03168087e-06
Iter: 652 loss: 4.05015589e-06
Iter: 653 loss: 4.03001286e-06
Iter: 654 loss: 4.02361138e-06
Iter: 655 loss: 4.01444549e-06
Iter: 656 loss: 4.0141631e-06
Iter: 657 loss: 4.00739054e-06
Iter: 658 loss: 4.00725094e-06
Iter: 659 loss: 4.00100134e-06
Iter: 660 loss: 4.00290446e-06
Iter: 661 loss: 3.99641704e-06
Iter: 662 loss: 3.98918337e-06
Iter: 663 loss: 3.99380087e-06
Iter: 664 loss: 3.98454631e-06
Iter: 665 loss: 3.97692656e-06
Iter: 666 loss: 3.97690155e-06
Iter: 667 loss: 3.97190934e-06
Iter: 668 loss: 3.96325504e-06
Iter: 669 loss: 3.9632414e-06
Iter: 670 loss: 3.95533198e-06
Iter: 671 loss: 4.00886e-06
Iter: 672 loss: 3.95456436e-06
Iter: 673 loss: 3.94633298e-06
Iter: 674 loss: 3.96802852e-06
Iter: 675 loss: 3.94348808e-06
Iter: 676 loss: 3.9361671e-06
Iter: 677 loss: 3.92839456e-06
Iter: 678 loss: 3.92717175e-06
Iter: 679 loss: 3.91620506e-06
Iter: 680 loss: 3.96163614e-06
Iter: 681 loss: 3.91392223e-06
Iter: 682 loss: 3.90438709e-06
Iter: 683 loss: 3.97607164e-06
Iter: 684 loss: 3.90364585e-06
Iter: 685 loss: 3.89545812e-06
Iter: 686 loss: 3.93640312e-06
Iter: 687 loss: 3.89406796e-06
Iter: 688 loss: 3.887807e-06
Iter: 689 loss: 3.89491197e-06
Iter: 690 loss: 3.8845933e-06
Iter: 691 loss: 3.87736509e-06
Iter: 692 loss: 3.92368202e-06
Iter: 693 loss: 3.8765329e-06
Iter: 694 loss: 3.87085447e-06
Iter: 695 loss: 3.86502916e-06
Iter: 696 loss: 3.86395232e-06
Iter: 697 loss: 3.85697558e-06
Iter: 698 loss: 3.95294273e-06
Iter: 699 loss: 3.8569633e-06
Iter: 700 loss: 3.85116e-06
Iter: 701 loss: 3.86433976e-06
Iter: 702 loss: 3.84912073e-06
Iter: 703 loss: 3.84344594e-06
Iter: 704 loss: 3.84602163e-06
Iter: 705 loss: 3.83967972e-06
Iter: 706 loss: 3.83115e-06
Iter: 707 loss: 3.88566514e-06
Iter: 708 loss: 3.83028146e-06
Iter: 709 loss: 3.82529197e-06
Iter: 710 loss: 3.81915061e-06
Iter: 711 loss: 3.8185608e-06
Iter: 712 loss: 3.80986489e-06
Iter: 713 loss: 3.83820407e-06
Iter: 714 loss: 3.80733854e-06
Iter: 715 loss: 3.80033589e-06
Iter: 716 loss: 3.86453212e-06
Iter: 717 loss: 3.80001848e-06
Iter: 718 loss: 3.79292214e-06
Iter: 719 loss: 3.78951563e-06
Iter: 720 loss: 3.78585673e-06
Iter: 721 loss: 3.77729293e-06
Iter: 722 loss: 3.79345852e-06
Iter: 723 loss: 3.77356105e-06
Iter: 724 loss: 3.76561638e-06
Iter: 725 loss: 3.78806749e-06
Iter: 726 loss: 3.7630748e-06
Iter: 727 loss: 3.75493119e-06
Iter: 728 loss: 3.85998374e-06
Iter: 729 loss: 3.75490345e-06
Iter: 730 loss: 3.74973092e-06
Iter: 731 loss: 3.74460546e-06
Iter: 732 loss: 3.7434404e-06
Iter: 733 loss: 3.73671855e-06
Iter: 734 loss: 3.81253653e-06
Iter: 735 loss: 3.73654575e-06
Iter: 736 loss: 3.73030639e-06
Iter: 737 loss: 3.72621298e-06
Iter: 738 loss: 3.72380214e-06
Iter: 739 loss: 3.71769147e-06
Iter: 740 loss: 3.71771262e-06
Iter: 741 loss: 3.71278975e-06
Iter: 742 loss: 3.71449914e-06
Iter: 743 loss: 3.70933049e-06
Iter: 744 loss: 3.70354132e-06
Iter: 745 loss: 3.73129433e-06
Iter: 746 loss: 3.70261523e-06
Iter: 747 loss: 3.6966278e-06
Iter: 748 loss: 3.69684813e-06
Iter: 749 loss: 3.69180611e-06
Iter: 750 loss: 3.68526526e-06
Iter: 751 loss: 3.68769179e-06
Iter: 752 loss: 3.68080759e-06
Iter: 753 loss: 3.671174e-06
Iter: 754 loss: 3.68416454e-06
Iter: 755 loss: 3.66624181e-06
Iter: 756 loss: 3.65824326e-06
Iter: 757 loss: 3.74319734e-06
Iter: 758 loss: 3.65796063e-06
Iter: 759 loss: 3.65113237e-06
Iter: 760 loss: 3.680183e-06
Iter: 761 loss: 3.64960306e-06
Iter: 762 loss: 3.64369112e-06
Iter: 763 loss: 3.64120069e-06
Iter: 764 loss: 3.63819618e-06
Iter: 765 loss: 3.63000322e-06
Iter: 766 loss: 3.65674077e-06
Iter: 767 loss: 3.62781066e-06
Iter: 768 loss: 3.62202945e-06
Iter: 769 loss: 3.62205014e-06
Iter: 770 loss: 3.61709272e-06
Iter: 771 loss: 3.61383854e-06
Iter: 772 loss: 3.61193e-06
Iter: 773 loss: 3.60586864e-06
Iter: 774 loss: 3.61077e-06
Iter: 775 loss: 3.60227546e-06
Iter: 776 loss: 3.59664773e-06
Iter: 777 loss: 3.59641467e-06
Iter: 778 loss: 3.5930841e-06
Iter: 779 loss: 3.58825287e-06
Iter: 780 loss: 3.58821535e-06
Iter: 781 loss: 3.58102807e-06
Iter: 782 loss: 3.63711047e-06
Iter: 783 loss: 3.58056377e-06
Iter: 784 loss: 3.57575982e-06
Iter: 785 loss: 3.57227782e-06
Iter: 786 loss: 3.5706189e-06
Iter: 787 loss: 3.56427654e-06
Iter: 788 loss: 3.58699776e-06
Iter: 789 loss: 3.56269493e-06
Iter: 790 loss: 3.55589373e-06
Iter: 791 loss: 3.59134287e-06
Iter: 792 loss: 3.55483644e-06
Iter: 793 loss: 3.54963299e-06
Iter: 794 loss: 3.54505596e-06
Iter: 795 loss: 3.54371832e-06
Iter: 796 loss: 3.53477503e-06
Iter: 797 loss: 3.56449345e-06
Iter: 798 loss: 3.53239716e-06
Iter: 799 loss: 3.5258272e-06
Iter: 800 loss: 3.60146055e-06
Iter: 801 loss: 3.52569964e-06
Iter: 802 loss: 3.51956214e-06
Iter: 803 loss: 3.52222582e-06
Iter: 804 loss: 3.51542712e-06
Iter: 805 loss: 3.50962023e-06
Iter: 806 loss: 3.52496272e-06
Iter: 807 loss: 3.5076414e-06
Iter: 808 loss: 3.50228493e-06
Iter: 809 loss: 3.54601866e-06
Iter: 810 loss: 3.50195955e-06
Iter: 811 loss: 3.49665925e-06
Iter: 812 loss: 3.50060191e-06
Iter: 813 loss: 3.49357606e-06
Iter: 814 loss: 3.48824233e-06
Iter: 815 loss: 3.48762387e-06
Iter: 816 loss: 3.48366302e-06
Iter: 817 loss: 3.47803643e-06
Iter: 818 loss: 3.47787886e-06
Iter: 819 loss: 3.47393689e-06
Iter: 820 loss: 3.46624961e-06
Iter: 821 loss: 3.61363323e-06
Iter: 822 loss: 3.46607294e-06
Iter: 823 loss: 3.45933063e-06
Iter: 824 loss: 3.55593079e-06
Iter: 825 loss: 3.45933222e-06
Iter: 826 loss: 3.45365652e-06
Iter: 827 loss: 3.46004413e-06
Iter: 828 loss: 3.45058265e-06
Iter: 829 loss: 3.44566206e-06
Iter: 830 loss: 3.44161504e-06
Iter: 831 loss: 3.44006912e-06
Iter: 832 loss: 3.43166312e-06
Iter: 833 loss: 3.45794979e-06
Iter: 834 loss: 3.4291802e-06
Iter: 835 loss: 3.42275757e-06
Iter: 836 loss: 3.49918719e-06
Iter: 837 loss: 3.42267867e-06
Iter: 838 loss: 3.41654913e-06
Iter: 839 loss: 3.41866757e-06
Iter: 840 loss: 3.41219743e-06
Iter: 841 loss: 3.40557631e-06
Iter: 842 loss: 3.41923192e-06
Iter: 843 loss: 3.40291581e-06
Iter: 844 loss: 3.39674625e-06
Iter: 845 loss: 3.45860144e-06
Iter: 846 loss: 3.39663029e-06
Iter: 847 loss: 3.3915951e-06
Iter: 848 loss: 3.39366466e-06
Iter: 849 loss: 3.38810355e-06
Iter: 850 loss: 3.38263567e-06
Iter: 851 loss: 3.40491397e-06
Iter: 852 loss: 3.38126938e-06
Iter: 853 loss: 3.37524943e-06
Iter: 854 loss: 3.40022211e-06
Iter: 855 loss: 3.37396568e-06
Iter: 856 loss: 3.36941275e-06
Iter: 857 loss: 3.36860148e-06
Iter: 858 loss: 3.36552193e-06
Iter: 859 loss: 3.36013227e-06
Iter: 860 loss: 3.44535829e-06
Iter: 861 loss: 3.36014182e-06
Iter: 862 loss: 3.3564254e-06
Iter: 863 loss: 3.35057257e-06
Iter: 864 loss: 3.35055597e-06
Iter: 865 loss: 3.3437309e-06
Iter: 866 loss: 3.37313281e-06
Iter: 867 loss: 3.34237393e-06
Iter: 868 loss: 3.33693265e-06
Iter: 869 loss: 3.38659311e-06
Iter: 870 loss: 3.33665889e-06
Iter: 871 loss: 3.33197659e-06
Iter: 872 loss: 3.32561581e-06
Iter: 873 loss: 3.32535797e-06
Iter: 874 loss: 3.31816887e-06
Iter: 875 loss: 3.33790967e-06
Iter: 876 loss: 3.31581487e-06
Iter: 877 loss: 3.30780767e-06
Iter: 878 loss: 3.30969601e-06
Iter: 879 loss: 3.30188072e-06
Iter: 880 loss: 3.29727095e-06
Iter: 881 loss: 3.29634577e-06
Iter: 882 loss: 3.29151362e-06
Iter: 883 loss: 3.28922056e-06
Iter: 884 loss: 3.28683245e-06
Iter: 885 loss: 3.28066835e-06
Iter: 886 loss: 3.30181342e-06
Iter: 887 loss: 3.27907765e-06
Iter: 888 loss: 3.27366274e-06
Iter: 889 loss: 3.32725153e-06
Iter: 890 loss: 3.27349449e-06
Iter: 891 loss: 3.26997656e-06
Iter: 892 loss: 3.26785948e-06
Iter: 893 loss: 3.26642248e-06
Iter: 894 loss: 3.26016834e-06
Iter: 895 loss: 3.29922113e-06
Iter: 896 loss: 3.25943847e-06
Iter: 897 loss: 3.25471251e-06
Iter: 898 loss: 3.25786095e-06
Iter: 899 loss: 3.2518119e-06
Iter: 900 loss: 3.24612665e-06
Iter: 901 loss: 3.2914113e-06
Iter: 902 loss: 3.24575603e-06
Iter: 903 loss: 3.241488e-06
Iter: 904 loss: 3.23773656e-06
Iter: 905 loss: 3.2366504e-06
Iter: 906 loss: 3.23027302e-06
Iter: 907 loss: 3.23567156e-06
Iter: 908 loss: 3.22648202e-06
Iter: 909 loss: 3.22143478e-06
Iter: 910 loss: 3.22130427e-06
Iter: 911 loss: 3.21712105e-06
Iter: 912 loss: 3.21377479e-06
Iter: 913 loss: 3.21253651e-06
Iter: 914 loss: 3.20650065e-06
Iter: 915 loss: 3.21882771e-06
Iter: 916 loss: 3.20401659e-06
Iter: 917 loss: 3.1977354e-06
Iter: 918 loss: 3.20127037e-06
Iter: 919 loss: 3.19370497e-06
Iter: 920 loss: 3.1857694e-06
Iter: 921 loss: 3.21850712e-06
Iter: 922 loss: 3.18403363e-06
Iter: 923 loss: 3.1790596e-06
Iter: 924 loss: 3.17905096e-06
Iter: 925 loss: 3.17462127e-06
Iter: 926 loss: 3.17323452e-06
Iter: 927 loss: 3.1706254e-06
Iter: 928 loss: 3.16535352e-06
Iter: 929 loss: 3.19394485e-06
Iter: 930 loss: 3.16451042e-06
Iter: 931 loss: 3.15884154e-06
Iter: 932 loss: 3.17296599e-06
Iter: 933 loss: 3.15680245e-06
Iter: 934 loss: 3.15259445e-06
Iter: 935 loss: 3.16476417e-06
Iter: 936 loss: 3.15128545e-06
Iter: 937 loss: 3.14606e-06
Iter: 938 loss: 3.15933175e-06
Iter: 939 loss: 3.14426779e-06
Iter: 940 loss: 3.1401969e-06
Iter: 941 loss: 3.14240651e-06
Iter: 942 loss: 3.13743431e-06
Iter: 943 loss: 3.13184455e-06
Iter: 944 loss: 3.15895568e-06
Iter: 945 loss: 3.13089481e-06
Iter: 946 loss: 3.12618135e-06
Iter: 947 loss: 3.12247766e-06
Iter: 948 loss: 3.12107431e-06
Iter: 949 loss: 3.11504436e-06
Iter: 950 loss: 3.17521904e-06
Iter: 951 loss: 3.11494546e-06
Iter: 952 loss: 3.10970245e-06
Iter: 953 loss: 3.12557472e-06
Iter: 954 loss: 3.1081222e-06
Iter: 955 loss: 3.10309019e-06
Iter: 956 loss: 3.10008363e-06
Iter: 957 loss: 3.09804159e-06
Iter: 958 loss: 3.09075767e-06
Iter: 959 loss: 3.10184623e-06
Iter: 960 loss: 3.08724975e-06
Iter: 961 loss: 3.07969776e-06
Iter: 962 loss: 3.12129532e-06
Iter: 963 loss: 3.07857545e-06
Iter: 964 loss: 3.07259847e-06
Iter: 965 loss: 3.12124939e-06
Iter: 966 loss: 3.07222285e-06
Iter: 967 loss: 3.06672109e-06
Iter: 968 loss: 3.07773462e-06
Iter: 969 loss: 3.06432912e-06
Iter: 970 loss: 3.05994672e-06
Iter: 971 loss: 3.0725862e-06
Iter: 972 loss: 3.05855747e-06
Iter: 973 loss: 3.05271851e-06
Iter: 974 loss: 3.06518768e-06
Iter: 975 loss: 3.05048889e-06
Iter: 976 loss: 3.04663058e-06
Iter: 977 loss: 3.06619313e-06
Iter: 978 loss: 3.04603577e-06
Iter: 979 loss: 3.04183141e-06
Iter: 980 loss: 3.04269952e-06
Iter: 981 loss: 3.03873549e-06
Iter: 982 loss: 3.03373849e-06
Iter: 983 loss: 3.03464026e-06
Iter: 984 loss: 3.0300198e-06
Iter: 985 loss: 3.02422018e-06
Iter: 986 loss: 3.07082064e-06
Iter: 987 loss: 3.02393232e-06
Iter: 988 loss: 3.01842329e-06
Iter: 989 loss: 3.02585136e-06
Iter: 990 loss: 3.01569253e-06
Iter: 991 loss: 3.01113732e-06
Iter: 992 loss: 3.02032e-06
Iter: 993 loss: 3.00939428e-06
Iter: 994 loss: 3.00369902e-06
Iter: 995 loss: 3.03651359e-06
Iter: 996 loss: 3.00291913e-06
Iter: 997 loss: 2.99891985e-06
Iter: 998 loss: 2.99510589e-06
Iter: 999 loss: 2.99420299e-06
Iter: 1000 loss: 2.98768327e-06
Iter: 1001 loss: 3.0019205e-06
Iter: 1002 loss: 2.98506552e-06
Iter: 1003 loss: 2.97817633e-06
Iter: 1004 loss: 2.98526288e-06
Iter: 1005 loss: 2.97439738e-06
Iter: 1006 loss: 2.96914595e-06
Iter: 1007 loss: 2.96892722e-06
Iter: 1008 loss: 2.96459643e-06
Iter: 1009 loss: 2.98119585e-06
Iter: 1010 loss: 2.96353574e-06
Iter: 1011 loss: 2.95981636e-06
Iter: 1012 loss: 2.95769314e-06
Iter: 1013 loss: 2.95607902e-06
Iter: 1014 loss: 2.95047812e-06
Iter: 1015 loss: 3.00946795e-06
Iter: 1016 loss: 2.95028576e-06
Iter: 1017 loss: 2.94683832e-06
Iter: 1018 loss: 2.94427241e-06
Iter: 1019 loss: 2.94308256e-06
Iter: 1020 loss: 2.93820358e-06
Iter: 1021 loss: 2.97238262e-06
Iter: 1022 loss: 2.93774519e-06
Iter: 1023 loss: 2.93338712e-06
Iter: 1024 loss: 2.93992639e-06
Iter: 1025 loss: 2.93124549e-06
Iter: 1026 loss: 2.92692334e-06
Iter: 1027 loss: 2.92511322e-06
Iter: 1028 loss: 2.92283175e-06
Iter: 1029 loss: 2.91689707e-06
Iter: 1030 loss: 2.96082226e-06
Iter: 1031 loss: 2.91646757e-06
Iter: 1032 loss: 2.91069318e-06
Iter: 1033 loss: 2.93039898e-06
Iter: 1034 loss: 2.90910589e-06
Iter: 1035 loss: 2.90424782e-06
Iter: 1036 loss: 2.90381467e-06
Iter: 1037 loss: 2.90019079e-06
Iter: 1038 loss: 2.89487753e-06
Iter: 1039 loss: 2.97617567e-06
Iter: 1040 loss: 2.89487139e-06
Iter: 1041 loss: 2.89136551e-06
Iter: 1042 loss: 2.88446086e-06
Iter: 1043 loss: 3.01673981e-06
Iter: 1044 loss: 2.8844056e-06
Iter: 1045 loss: 2.87720081e-06
Iter: 1046 loss: 2.91492552e-06
Iter: 1047 loss: 2.8762031e-06
Iter: 1048 loss: 2.87140392e-06
Iter: 1049 loss: 2.94781626e-06
Iter: 1050 loss: 2.87139596e-06
Iter: 1051 loss: 2.86678642e-06
Iter: 1052 loss: 2.87080798e-06
Iter: 1053 loss: 2.86410022e-06
Iter: 1054 loss: 2.86015779e-06
Iter: 1055 loss: 2.8666559e-06
Iter: 1056 loss: 2.8583745e-06
Iter: 1057 loss: 2.85353099e-06
Iter: 1058 loss: 2.88107185e-06
Iter: 1059 loss: 2.8527802e-06
Iter: 1060 loss: 2.84915882e-06
Iter: 1061 loss: 2.84559701e-06
Iter: 1062 loss: 2.84483303e-06
Iter: 1063 loss: 2.8401214e-06
Iter: 1064 loss: 2.87982607e-06
Iter: 1065 loss: 2.8399113e-06
Iter: 1066 loss: 2.83497889e-06
Iter: 1067 loss: 2.83813688e-06
Iter: 1068 loss: 2.83190388e-06
Iter: 1069 loss: 2.82653377e-06
Iter: 1070 loss: 2.83177269e-06
Iter: 1071 loss: 2.82342512e-06
Iter: 1072 loss: 2.81818961e-06
Iter: 1073 loss: 2.84914813e-06
Iter: 1074 loss: 2.81744246e-06
Iter: 1075 loss: 2.81330631e-06
Iter: 1076 loss: 2.84432554e-06
Iter: 1077 loss: 2.8129366e-06
Iter: 1078 loss: 2.80886752e-06
Iter: 1079 loss: 2.80512768e-06
Iter: 1080 loss: 2.80415634e-06
Iter: 1081 loss: 2.79885671e-06
Iter: 1082 loss: 2.82430869e-06
Iter: 1083 loss: 2.79792653e-06
Iter: 1084 loss: 2.79322057e-06
Iter: 1085 loss: 2.82562951e-06
Iter: 1086 loss: 2.79280539e-06
Iter: 1087 loss: 2.78904463e-06
Iter: 1088 loss: 2.78595e-06
Iter: 1089 loss: 2.78496691e-06
Iter: 1090 loss: 2.78244e-06
Iter: 1091 loss: 2.78182392e-06
Iter: 1092 loss: 2.77913932e-06
Iter: 1093 loss: 2.7735714e-06
Iter: 1094 loss: 2.87006151e-06
Iter: 1095 loss: 2.77347658e-06
Iter: 1096 loss: 2.76865853e-06
Iter: 1097 loss: 2.80540439e-06
Iter: 1098 loss: 2.76838091e-06
Iter: 1099 loss: 2.76365836e-06
Iter: 1100 loss: 2.78293192e-06
Iter: 1101 loss: 2.76257742e-06
Iter: 1102 loss: 2.75873936e-06
Iter: 1103 loss: 2.75408729e-06
Iter: 1104 loss: 2.75373554e-06
Iter: 1105 loss: 2.74857666e-06
Iter: 1106 loss: 2.79710503e-06
Iter: 1107 loss: 2.74838817e-06
Iter: 1108 loss: 2.74396962e-06
Iter: 1109 loss: 2.76320588e-06
Iter: 1110 loss: 2.74300055e-06
Iter: 1111 loss: 2.73921682e-06
Iter: 1112 loss: 2.73655769e-06
Iter: 1113 loss: 2.73517708e-06
Iter: 1114 loss: 2.73084174e-06
Iter: 1115 loss: 2.77400159e-06
Iter: 1116 loss: 2.73067235e-06
Iter: 1117 loss: 2.72686634e-06
Iter: 1118 loss: 2.7400049e-06
Iter: 1119 loss: 2.72581019e-06
Iter: 1120 loss: 2.72216039e-06
Iter: 1121 loss: 2.72004354e-06
Iter: 1122 loss: 2.71842828e-06
Iter: 1123 loss: 2.71393742e-06
Iter: 1124 loss: 2.74118793e-06
Iter: 1125 loss: 2.71340468e-06
Iter: 1126 loss: 2.70919463e-06
Iter: 1127 loss: 2.72759962e-06
Iter: 1128 loss: 2.70835449e-06
Iter: 1129 loss: 2.7044432e-06
Iter: 1130 loss: 2.71285217e-06
Iter: 1131 loss: 2.7028932e-06
Iter: 1132 loss: 2.69825887e-06
Iter: 1133 loss: 2.71786985e-06
Iter: 1134 loss: 2.697318e-06
Iter: 1135 loss: 2.69397924e-06
Iter: 1136 loss: 2.6925718e-06
Iter: 1137 loss: 2.69085922e-06
Iter: 1138 loss: 2.68682197e-06
Iter: 1139 loss: 2.7093065e-06
Iter: 1140 loss: 2.68622625e-06
Iter: 1141 loss: 2.68152462e-06
Iter: 1142 loss: 2.69239717e-06
Iter: 1143 loss: 2.6798225e-06
Iter: 1144 loss: 2.67628457e-06
Iter: 1145 loss: 2.67363021e-06
Iter: 1146 loss: 2.6724806e-06
Iter: 1147 loss: 2.66714392e-06
Iter: 1148 loss: 2.68848316e-06
Iter: 1149 loss: 2.6659086e-06
Iter: 1150 loss: 2.66097777e-06
Iter: 1151 loss: 2.71119e-06
Iter: 1152 loss: 2.6607845e-06
Iter: 1153 loss: 2.65730387e-06
Iter: 1154 loss: 2.65335166e-06
Iter: 1155 loss: 2.65283188e-06
Iter: 1156 loss: 2.64751e-06
Iter: 1157 loss: 2.67982387e-06
Iter: 1158 loss: 2.64686855e-06
Iter: 1159 loss: 2.64221376e-06
Iter: 1160 loss: 2.67513724e-06
Iter: 1161 loss: 2.64173832e-06
Iter: 1162 loss: 2.63843367e-06
Iter: 1163 loss: 2.63447987e-06
Iter: 1164 loss: 2.6341454e-06
Iter: 1165 loss: 2.62959611e-06
Iter: 1166 loss: 2.66876577e-06
Iter: 1167 loss: 2.62925846e-06
Iter: 1168 loss: 2.6251264e-06
Iter: 1169 loss: 2.65026165e-06
Iter: 1170 loss: 2.62461867e-06
Iter: 1171 loss: 2.62127264e-06
Iter: 1172 loss: 2.62107983e-06
Iter: 1173 loss: 2.61861601e-06
Iter: 1174 loss: 2.61459763e-06
Iter: 1175 loss: 2.63991433e-06
Iter: 1176 loss: 2.61411788e-06
Iter: 1177 loss: 2.61095101e-06
Iter: 1178 loss: 2.60984143e-06
Iter: 1179 loss: 2.60792876e-06
Iter: 1180 loss: 2.60399065e-06
Iter: 1181 loss: 2.64193181e-06
Iter: 1182 loss: 2.60380716e-06
Iter: 1183 loss: 2.60005891e-06
Iter: 1184 loss: 2.60287652e-06
Iter: 1185 loss: 2.59774e-06
Iter: 1186 loss: 2.59361605e-06
Iter: 1187 loss: 2.59319336e-06
Iter: 1188 loss: 2.59022545e-06
Iter: 1189 loss: 2.58501632e-06
Iter: 1190 loss: 2.60467e-06
Iter: 1191 loss: 2.58377827e-06
Iter: 1192 loss: 2.58002069e-06
Iter: 1193 loss: 2.63366337e-06
Iter: 1194 loss: 2.57997976e-06
Iter: 1195 loss: 2.57676129e-06
Iter: 1196 loss: 2.57309785e-06
Iter: 1197 loss: 2.57264287e-06
Iter: 1198 loss: 2.56800399e-06
Iter: 1199 loss: 2.58743148e-06
Iter: 1200 loss: 2.56691601e-06
Iter: 1201 loss: 2.56265525e-06
Iter: 1202 loss: 2.60054685e-06
Iter: 1203 loss: 2.56241674e-06
Iter: 1204 loss: 2.55929785e-06
Iter: 1205 loss: 2.55759687e-06
Iter: 1206 loss: 2.55613986e-06
Iter: 1207 loss: 2.5529871e-06
Iter: 1208 loss: 2.59455101e-06
Iter: 1209 loss: 2.55294935e-06
Iter: 1210 loss: 2.54947327e-06
Iter: 1211 loss: 2.54825363e-06
Iter: 1212 loss: 2.54623956e-06
Iter: 1213 loss: 2.54262341e-06
Iter: 1214 loss: 2.54527436e-06
Iter: 1215 loss: 2.5404006e-06
Iter: 1216 loss: 2.53590633e-06
Iter: 1217 loss: 2.57285456e-06
Iter: 1218 loss: 2.5355771e-06
Iter: 1219 loss: 2.53222788e-06
Iter: 1220 loss: 2.53680309e-06
Iter: 1221 loss: 2.5305319e-06
Iter: 1222 loss: 2.52704785e-06
Iter: 1223 loss: 2.54441238e-06
Iter: 1224 loss: 2.52644531e-06
Iter: 1225 loss: 2.52284349e-06
Iter: 1226 loss: 2.52510131e-06
Iter: 1227 loss: 2.5206482e-06
Iter: 1228 loss: 2.51710617e-06
Iter: 1229 loss: 2.51512847e-06
Iter: 1230 loss: 2.51363326e-06
Iter: 1231 loss: 2.50913445e-06
Iter: 1232 loss: 2.55689474e-06
Iter: 1233 loss: 2.50903804e-06
Iter: 1234 loss: 2.50514904e-06
Iter: 1235 loss: 2.5203949e-06
Iter: 1236 loss: 2.5041661e-06
Iter: 1237 loss: 2.50046514e-06
Iter: 1238 loss: 2.50314406e-06
Iter: 1239 loss: 2.49824188e-06
Iter: 1240 loss: 2.49450795e-06
Iter: 1241 loss: 2.50460948e-06
Iter: 1242 loss: 2.49328355e-06
Iter: 1243 loss: 2.48886863e-06
Iter: 1244 loss: 2.51336087e-06
Iter: 1245 loss: 2.48816741e-06
Iter: 1246 loss: 2.48508331e-06
Iter: 1247 loss: 2.48861625e-06
Iter: 1248 loss: 2.48337165e-06
Iter: 1249 loss: 2.48035849e-06
Iter: 1250 loss: 2.52045265e-06
Iter: 1251 loss: 2.48040078e-06
Iter: 1252 loss: 2.47839716e-06
Iter: 1253 loss: 2.47287835e-06
Iter: 1254 loss: 2.50763151e-06
Iter: 1255 loss: 2.4714659e-06
Iter: 1256 loss: 2.46644868e-06
Iter: 1257 loss: 2.51958681e-06
Iter: 1258 loss: 2.46630043e-06
Iter: 1259 loss: 2.46241189e-06
Iter: 1260 loss: 2.50487574e-06
Iter: 1261 loss: 2.46237369e-06
Iter: 1262 loss: 2.45948149e-06
Iter: 1263 loss: 2.45812907e-06
Iter: 1264 loss: 2.45673391e-06
Iter: 1265 loss: 2.45337128e-06
Iter: 1266 loss: 2.47961407e-06
Iter: 1267 loss: 2.45319325e-06
Iter: 1268 loss: 2.45000138e-06
Iter: 1269 loss: 2.44918419e-06
Iter: 1270 loss: 2.44718194e-06
Iter: 1271 loss: 2.44272e-06
Iter: 1272 loss: 2.44598141e-06
Iter: 1273 loss: 2.43993395e-06
Iter: 1274 loss: 2.43528166e-06
Iter: 1275 loss: 2.46637774e-06
Iter: 1276 loss: 2.43479667e-06
Iter: 1277 loss: 2.43120303e-06
Iter: 1278 loss: 2.46410104e-06
Iter: 1279 loss: 2.43098725e-06
Iter: 1280 loss: 2.42832903e-06
Iter: 1281 loss: 2.42663964e-06
Iter: 1282 loss: 2.42560554e-06
Iter: 1283 loss: 2.42242777e-06
Iter: 1284 loss: 2.44857e-06
Iter: 1285 loss: 2.42219767e-06
Iter: 1286 loss: 2.41872863e-06
Iter: 1287 loss: 2.42201713e-06
Iter: 1288 loss: 2.41673615e-06
Iter: 1289 loss: 2.41297312e-06
Iter: 1290 loss: 2.42928627e-06
Iter: 1291 loss: 2.41223597e-06
Iter: 1292 loss: 2.40887493e-06
Iter: 1293 loss: 2.41950738e-06
Iter: 1294 loss: 2.40796862e-06
Iter: 1295 loss: 2.40515601e-06
Iter: 1296 loss: 2.40186159e-06
Iter: 1297 loss: 2.40148779e-06
Iter: 1298 loss: 2.39667202e-06
Iter: 1299 loss: 2.41198154e-06
Iter: 1300 loss: 2.395298e-06
Iter: 1301 loss: 2.39253791e-06
Iter: 1302 loss: 2.39247311e-06
Iter: 1303 loss: 2.38983625e-06
Iter: 1304 loss: 2.38620351e-06
Iter: 1305 loss: 2.38605821e-06
Iter: 1306 loss: 2.38228859e-06
Iter: 1307 loss: 2.3953844e-06
Iter: 1308 loss: 2.38128268e-06
Iter: 1309 loss: 2.37746235e-06
Iter: 1310 loss: 2.40195504e-06
Iter: 1311 loss: 2.37705262e-06
Iter: 1312 loss: 2.37403674e-06
Iter: 1313 loss: 2.37363815e-06
Iter: 1314 loss: 2.37143377e-06
Iter: 1315 loss: 2.36733626e-06
Iter: 1316 loss: 2.37073846e-06
Iter: 1317 loss: 2.36497544e-06
Iter: 1318 loss: 2.36180949e-06
Iter: 1319 loss: 2.3616285e-06
Iter: 1320 loss: 2.3589073e-06
Iter: 1321 loss: 2.35763696e-06
Iter: 1322 loss: 2.35630068e-06
Iter: 1323 loss: 2.35295079e-06
Iter: 1324 loss: 2.37266067e-06
Iter: 1325 loss: 2.35252924e-06
Iter: 1326 loss: 2.34936306e-06
Iter: 1327 loss: 2.36698679e-06
Iter: 1328 loss: 2.34887648e-06
Iter: 1329 loss: 2.34675645e-06
Iter: 1330 loss: 2.34582831e-06
Iter: 1331 loss: 2.34469917e-06
Iter: 1332 loss: 2.34143272e-06
Iter: 1333 loss: 2.36201709e-06
Iter: 1334 loss: 2.34101867e-06
Iter: 1335 loss: 2.33828541e-06
Iter: 1336 loss: 2.33675064e-06
Iter: 1337 loss: 2.33552169e-06
Iter: 1338 loss: 2.3320863e-06
Iter: 1339 loss: 2.34252411e-06
Iter: 1340 loss: 2.33106221e-06
Iter: 1341 loss: 2.3279481e-06
Iter: 1342 loss: 2.35306607e-06
Iter: 1343 loss: 2.32780712e-06
Iter: 1344 loss: 2.32460161e-06
Iter: 1345 loss: 2.32882758e-06
Iter: 1346 loss: 2.32294747e-06
Iter: 1347 loss: 2.32015191e-06
Iter: 1348 loss: 2.31893591e-06
Iter: 1349 loss: 2.31748413e-06
Iter: 1350 loss: 2.31368176e-06
Iter: 1351 loss: 2.33609944e-06
Iter: 1352 loss: 2.31319359e-06
Iter: 1353 loss: 2.30993214e-06
Iter: 1354 loss: 2.32806e-06
Iter: 1355 loss: 2.3093603e-06
Iter: 1356 loss: 2.30639648e-06
Iter: 1357 loss: 2.30468572e-06
Iter: 1358 loss: 2.30332785e-06
Iter: 1359 loss: 2.29982697e-06
Iter: 1360 loss: 2.32494835e-06
Iter: 1361 loss: 2.29952957e-06
Iter: 1362 loss: 2.29594639e-06
Iter: 1363 loss: 2.31247191e-06
Iter: 1364 loss: 2.29533885e-06
Iter: 1365 loss: 2.29293664e-06
Iter: 1366 loss: 2.29662919e-06
Iter: 1367 loss: 2.29182547e-06
Iter: 1368 loss: 2.28863428e-06
Iter: 1369 loss: 2.30148203e-06
Iter: 1370 loss: 2.28794761e-06
Iter: 1371 loss: 2.2853651e-06
Iter: 1372 loss: 2.28264093e-06
Iter: 1373 loss: 2.28222416e-06
Iter: 1374 loss: 2.27872556e-06
Iter: 1375 loss: 2.32079515e-06
Iter: 1376 loss: 2.27868622e-06
Iter: 1377 loss: 2.27562123e-06
Iter: 1378 loss: 2.2743543e-06
Iter: 1379 loss: 2.27267856e-06
Iter: 1380 loss: 2.26884231e-06
Iter: 1381 loss: 2.27609053e-06
Iter: 1382 loss: 2.26725297e-06
Iter: 1383 loss: 2.26376187e-06
Iter: 1384 loss: 2.29034231e-06
Iter: 1385 loss: 2.26351e-06
Iter: 1386 loss: 2.25994881e-06
Iter: 1387 loss: 2.27081e-06
Iter: 1388 loss: 2.25891222e-06
Iter: 1389 loss: 2.25608983e-06
Iter: 1390 loss: 2.25518511e-06
Iter: 1391 loss: 2.25361919e-06
Iter: 1392 loss: 2.24969472e-06
Iter: 1393 loss: 2.25687154e-06
Iter: 1394 loss: 2.24790892e-06
Iter: 1395 loss: 2.24432188e-06
Iter: 1396 loss: 2.28205795e-06
Iter: 1397 loss: 2.24418022e-06
Iter: 1398 loss: 2.24079395e-06
Iter: 1399 loss: 2.24238465e-06
Iter: 1400 loss: 2.23859388e-06
Iter: 1401 loss: 2.23497773e-06
Iter: 1402 loss: 2.24623113e-06
Iter: 1403 loss: 2.23396364e-06
Iter: 1404 loss: 2.23065354e-06
Iter: 1405 loss: 2.26490511e-06
Iter: 1406 loss: 2.23056895e-06
Iter: 1407 loss: 2.22791459e-06
Iter: 1408 loss: 2.22607628e-06
Iter: 1409 loss: 2.22510312e-06
Iter: 1410 loss: 2.22208359e-06
Iter: 1411 loss: 2.26890052e-06
Iter: 1412 loss: 2.22208701e-06
Iter: 1413 loss: 2.22000199e-06
Iter: 1414 loss: 2.21572191e-06
Iter: 1415 loss: 2.28808904e-06
Iter: 1416 loss: 2.21558457e-06
Iter: 1417 loss: 2.21221921e-06
Iter: 1418 loss: 2.25990925e-06
Iter: 1419 loss: 2.21224832e-06
Iter: 1420 loss: 2.20905758e-06
Iter: 1421 loss: 2.212651e-06
Iter: 1422 loss: 2.20740435e-06
Iter: 1423 loss: 2.20444213e-06
Iter: 1424 loss: 2.20597599e-06
Iter: 1425 loss: 2.20244056e-06
Iter: 1426 loss: 2.19919161e-06
Iter: 1427 loss: 2.23362554e-06
Iter: 1428 loss: 2.19913704e-06
Iter: 1429 loss: 2.19621188e-06
Iter: 1430 loss: 2.20056654e-06
Iter: 1431 loss: 2.19477079e-06
Iter: 1432 loss: 2.19180629e-06
Iter: 1433 loss: 2.18862806e-06
Iter: 1434 loss: 2.18805553e-06
Iter: 1435 loss: 2.18400578e-06
Iter: 1436 loss: 2.21357664e-06
Iter: 1437 loss: 2.18371542e-06
Iter: 1438 loss: 2.18040509e-06
Iter: 1439 loss: 2.19759681e-06
Iter: 1440 loss: 2.17983143e-06
Iter: 1441 loss: 2.17661386e-06
Iter: 1442 loss: 2.18166201e-06
Iter: 1443 loss: 2.17502497e-06
Iter: 1444 loss: 2.17190154e-06
Iter: 1445 loss: 2.18139303e-06
Iter: 1446 loss: 2.17101547e-06
Iter: 1447 loss: 2.16813942e-06
Iter: 1448 loss: 2.1972985e-06
Iter: 1449 loss: 2.16804256e-06
Iter: 1450 loss: 2.16610852e-06
Iter: 1451 loss: 2.16535113e-06
Iter: 1452 loss: 2.16434273e-06
Iter: 1453 loss: 2.1615474e-06
Iter: 1454 loss: 2.18141031e-06
Iter: 1455 loss: 2.16126182e-06
Iter: 1456 loss: 2.15894033e-06
Iter: 1457 loss: 2.15519708e-06
Iter: 1458 loss: 2.15518457e-06
Iter: 1459 loss: 2.151628e-06
Iter: 1460 loss: 2.17625461e-06
Iter: 1461 loss: 2.15130535e-06
Iter: 1462 loss: 2.14767169e-06
Iter: 1463 loss: 2.16239e-06
Iter: 1464 loss: 2.1469009e-06
Iter: 1465 loss: 2.14423926e-06
Iter: 1466 loss: 2.14172405e-06
Iter: 1467 loss: 2.14102602e-06
Iter: 1468 loss: 2.13713065e-06
Iter: 1469 loss: 2.16034914e-06
Iter: 1470 loss: 2.13661406e-06
Iter: 1471 loss: 2.13325188e-06
Iter: 1472 loss: 2.16689773e-06
Iter: 1473 loss: 2.13313024e-06
Iter: 1474 loss: 2.1309379e-06
Iter: 1475 loss: 2.12848204e-06
Iter: 1476 loss: 2.12808641e-06
Iter: 1477 loss: 2.12455689e-06
Iter: 1478 loss: 2.13338194e-06
Iter: 1479 loss: 2.12328814e-06
Iter: 1480 loss: 2.11988663e-06
Iter: 1481 loss: 2.14238412e-06
Iter: 1482 loss: 2.11958786e-06
Iter: 1483 loss: 2.11618681e-06
Iter: 1484 loss: 2.12201303e-06
Iter: 1485 loss: 2.11474025e-06
Iter: 1486 loss: 2.11151246e-06
Iter: 1487 loss: 2.11760744e-06
Iter: 1488 loss: 2.11021711e-06
Iter: 1489 loss: 2.10699591e-06
Iter: 1490 loss: 2.14376132e-06
Iter: 1491 loss: 2.10696476e-06
Iter: 1492 loss: 2.1046726e-06
Iter: 1493 loss: 2.10396615e-06
Iter: 1494 loss: 2.10265671e-06
Iter: 1495 loss: 2.09944437e-06
Iter: 1496 loss: 2.11926931e-06
Iter: 1497 loss: 2.09917971e-06
Iter: 1498 loss: 2.09645737e-06
Iter: 1499 loss: 2.0956104e-06
Iter: 1500 loss: 2.09400332e-06
Iter: 1501 loss: 2.09081395e-06
Iter: 1502 loss: 2.09766176e-06
Iter: 1503 loss: 2.0896307e-06
Iter: 1504 loss: 2.08686606e-06
Iter: 1505 loss: 2.12312557e-06
Iter: 1506 loss: 2.08686311e-06
Iter: 1507 loss: 2.08470806e-06
Iter: 1508 loss: 2.08170832e-06
Iter: 1509 loss: 2.08156098e-06
Iter: 1510 loss: 2.0778466e-06
Iter: 1511 loss: 2.08741585e-06
Iter: 1512 loss: 2.07651874e-06
Iter: 1513 loss: 2.07354901e-06
Iter: 1514 loss: 2.07353833e-06
Iter: 1515 loss: 2.07092694e-06
Iter: 1516 loss: 2.07124594e-06
Iter: 1517 loss: 2.06891764e-06
Iter: 1518 loss: 2.06603158e-06
Iter: 1519 loss: 2.06487107e-06
Iter: 1520 loss: 2.06341974e-06
Iter: 1521 loss: 2.06059713e-06
Iter: 1522 loss: 2.06057302e-06
Iter: 1523 loss: 2.05816195e-06
Iter: 1524 loss: 2.05928791e-06
Iter: 1525 loss: 2.0565044e-06
Iter: 1526 loss: 2.05347669e-06
Iter: 1527 loss: 2.05929769e-06
Iter: 1528 loss: 2.05213883e-06
Iter: 1529 loss: 2.04944081e-06
Iter: 1530 loss: 2.08088795e-06
Iter: 1531 loss: 2.04942808e-06
Iter: 1532 loss: 2.04722573e-06
Iter: 1533 loss: 2.04822209e-06
Iter: 1534 loss: 2.04566118e-06
Iter: 1535 loss: 2.04318576e-06
Iter: 1536 loss: 2.05634024e-06
Iter: 1537 loss: 2.04274556e-06
Iter: 1538 loss: 2.0400762e-06
Iter: 1539 loss: 2.03887203e-06
Iter: 1540 loss: 2.03750483e-06
Iter: 1541 loss: 2.0342577e-06
Iter: 1542 loss: 2.038714e-06
Iter: 1543 loss: 2.03269292e-06
Iter: 1544 loss: 2.02972046e-06
Iter: 1545 loss: 2.06776713e-06
Iter: 1546 loss: 2.02962951e-06
Iter: 1547 loss: 2.0272289e-06
Iter: 1548 loss: 2.02632259e-06
Iter: 1549 loss: 2.02501633e-06
Iter: 1550 loss: 2.0218954e-06
Iter: 1551 loss: 2.02438537e-06
Iter: 1552 loss: 2.02003139e-06
Iter: 1553 loss: 2.01633566e-06
Iter: 1554 loss: 2.04199296e-06
Iter: 1555 loss: 2.01604985e-06
Iter: 1556 loss: 2.01260423e-06
Iter: 1557 loss: 2.03310719e-06
Iter: 1558 loss: 2.01219927e-06
Iter: 1559 loss: 2.01012335e-06
Iter: 1560 loss: 2.00708973e-06
Iter: 1561 loss: 2.0070197e-06
Iter: 1562 loss: 2.00305089e-06
Iter: 1563 loss: 2.02029946e-06
Iter: 1564 loss: 2.00224645e-06
Iter: 1565 loss: 1.99915144e-06
Iter: 1566 loss: 2.03764739e-06
Iter: 1567 loss: 1.99910187e-06
Iter: 1568 loss: 1.99723854e-06
Iter: 1569 loss: 1.99401052e-06
Iter: 1570 loss: 1.99404349e-06
Iter: 1571 loss: 1.99069291e-06
Iter: 1572 loss: 1.9906638e-06
Iter: 1573 loss: 1.98826729e-06
Iter: 1574 loss: 1.99346323e-06
Iter: 1575 loss: 1.98728117e-06
Iter: 1576 loss: 1.98488669e-06
Iter: 1577 loss: 1.99053034e-06
Iter: 1578 loss: 1.98394537e-06
Iter: 1579 loss: 1.98105135e-06
Iter: 1580 loss: 1.99063106e-06
Iter: 1581 loss: 1.98020916e-06
Iter: 1582 loss: 1.97834311e-06
Iter: 1583 loss: 1.97715735e-06
Iter: 1584 loss: 1.97634085e-06
Iter: 1585 loss: 1.97375721e-06
Iter: 1586 loss: 1.99804253e-06
Iter: 1587 loss: 1.97361692e-06
Iter: 1588 loss: 1.97125883e-06
Iter: 1589 loss: 1.97255372e-06
Iter: 1590 loss: 1.9696663e-06
Iter: 1591 loss: 1.96691667e-06
Iter: 1592 loss: 1.96729866e-06
Iter: 1593 loss: 1.96482415e-06
Iter: 1594 loss: 1.96116889e-06
Iter: 1595 loss: 1.96792712e-06
Iter: 1596 loss: 1.95972734e-06
Iter: 1597 loss: 1.95589837e-06
Iter: 1598 loss: 1.98890143e-06
Iter: 1599 loss: 1.95571624e-06
Iter: 1600 loss: 1.95247367e-06
Iter: 1601 loss: 1.97357986e-06
Iter: 1602 loss: 1.9521492e-06
Iter: 1603 loss: 1.9503259e-06
Iter: 1604 loss: 1.94721679e-06
Iter: 1605 loss: 1.94721633e-06
Iter: 1606 loss: 1.94408767e-06
Iter: 1607 loss: 1.9926058e-06
Iter: 1608 loss: 1.94410586e-06
Iter: 1609 loss: 1.9417198e-06
Iter: 1610 loss: 1.94226595e-06
Iter: 1611 loss: 1.94003928e-06
Iter: 1612 loss: 1.93731103e-06
Iter: 1613 loss: 1.94397035e-06
Iter: 1614 loss: 1.93642518e-06
Iter: 1615 loss: 1.93374535e-06
Iter: 1616 loss: 1.95896337e-06
Iter: 1617 loss: 1.93366259e-06
Iter: 1618 loss: 1.93180722e-06
Iter: 1619 loss: 1.93080587e-06
Iter: 1620 loss: 1.92995867e-06
Iter: 1621 loss: 1.9273582e-06
Iter: 1622 loss: 1.95403027e-06
Iter: 1623 loss: 1.92727111e-06
Iter: 1624 loss: 1.92498601e-06
Iter: 1625 loss: 1.92468e-06
Iter: 1626 loss: 1.92296943e-06
Iter: 1627 loss: 1.92025232e-06
Iter: 1628 loss: 1.92563357e-06
Iter: 1629 loss: 1.91915387e-06
Iter: 1630 loss: 1.91660934e-06
Iter: 1631 loss: 1.94094355e-06
Iter: 1632 loss: 1.91649133e-06
Iter: 1633 loss: 1.91470372e-06
Iter: 1634 loss: 1.91153731e-06
Iter: 1635 loss: 1.98952443e-06
Iter: 1636 loss: 1.91158097e-06
Iter: 1637 loss: 1.9080378e-06
Iter: 1638 loss: 1.92466769e-06
Iter: 1639 loss: 1.90740411e-06
Iter: 1640 loss: 1.90444166e-06
Iter: 1641 loss: 1.91816594e-06
Iter: 1642 loss: 1.90384458e-06
Iter: 1643 loss: 1.90074707e-06
Iter: 1644 loss: 1.92244e-06
Iter: 1645 loss: 1.90048786e-06
Iter: 1646 loss: 1.89838727e-06
Iter: 1647 loss: 1.89629509e-06
Iter: 1648 loss: 1.89581419e-06
Iter: 1649 loss: 1.89312527e-06
Iter: 1650 loss: 1.91432036e-06
Iter: 1651 loss: 1.8929702e-06
Iter: 1652 loss: 1.89007437e-06
Iter: 1653 loss: 1.89248613e-06
Iter: 1654 loss: 1.88837726e-06
Iter: 1655 loss: 1.88567674e-06
Iter: 1656 loss: 1.89184971e-06
Iter: 1657 loss: 1.88468414e-06
Iter: 1658 loss: 1.88204547e-06
Iter: 1659 loss: 1.90451385e-06
Iter: 1660 loss: 1.88190143e-06
Iter: 1661 loss: 1.87958904e-06
Iter: 1662 loss: 1.87876776e-06
Iter: 1663 loss: 1.87742967e-06
Iter: 1664 loss: 1.87538194e-06
Iter: 1665 loss: 1.90614264e-06
Iter: 1666 loss: 1.87536466e-06
Iter: 1667 loss: 1.87331659e-06
Iter: 1668 loss: 1.87261287e-06
Iter: 1669 loss: 1.87153864e-06
Iter: 1670 loss: 1.8689534e-06
Iter: 1671 loss: 1.87702665e-06
Iter: 1672 loss: 1.86825423e-06
Iter: 1673 loss: 1.86588761e-06
Iter: 1674 loss: 1.87715102e-06
Iter: 1675 loss: 1.86543639e-06
Iter: 1676 loss: 1.86337684e-06
Iter: 1677 loss: 1.86166608e-06
Iter: 1678 loss: 1.8610757e-06
Iter: 1679 loss: 1.85790543e-06
Iter: 1680 loss: 1.86535112e-06
Iter: 1681 loss: 1.85674662e-06
Iter: 1682 loss: 1.85354668e-06
Iter: 1683 loss: 1.85941281e-06
Iter: 1684 loss: 1.85217323e-06
Iter: 1685 loss: 1.85019826e-06
Iter: 1686 loss: 1.84994803e-06
Iter: 1687 loss: 1.8484609e-06
Iter: 1688 loss: 1.84683745e-06
Iter: 1689 loss: 1.84671819e-06
Iter: 1690 loss: 1.84437113e-06
Iter: 1691 loss: 1.85020247e-06
Iter: 1692 loss: 1.8435702e-06
Iter: 1693 loss: 1.84081136e-06
Iter: 1694 loss: 1.85450244e-06
Iter: 1695 loss: 1.84032876e-06
Iter: 1696 loss: 1.83798352e-06
Iter: 1697 loss: 1.8387932e-06
Iter: 1698 loss: 1.83633347e-06
Iter: 1699 loss: 1.83400664e-06
Iter: 1700 loss: 1.85428826e-06
Iter: 1701 loss: 1.83391535e-06
Iter: 1702 loss: 1.83136353e-06
Iter: 1703 loss: 1.83104612e-06
Iter: 1704 loss: 1.82925396e-06
Iter: 1705 loss: 1.82715689e-06
Iter: 1706 loss: 1.85579461e-06
Iter: 1707 loss: 1.82719236e-06
Iter: 1708 loss: 1.82523638e-06
Iter: 1709 loss: 1.82607505e-06
Iter: 1710 loss: 1.8239823e-06
Iter: 1711 loss: 1.82178337e-06
Iter: 1712 loss: 1.82213739e-06
Iter: 1713 loss: 1.82015071e-06
Iter: 1714 loss: 1.81752955e-06
Iter: 1715 loss: 1.84508235e-06
Iter: 1716 loss: 1.81749692e-06
Iter: 1717 loss: 1.81555129e-06
Iter: 1718 loss: 1.81390351e-06
Iter: 1719 loss: 1.81335e-06
Iter: 1720 loss: 1.81102985e-06
Iter: 1721 loss: 1.82381405e-06
Iter: 1722 loss: 1.81068776e-06
Iter: 1723 loss: 1.80883967e-06
Iter: 1724 loss: 1.81838459e-06
Iter: 1725 loss: 1.80852089e-06
Iter: 1726 loss: 1.80615757e-06
Iter: 1727 loss: 1.80726579e-06
Iter: 1728 loss: 1.80454288e-06
Iter: 1729 loss: 1.80196844e-06
Iter: 1730 loss: 1.80234497e-06
Iter: 1731 loss: 1.80007703e-06
Iter: 1732 loss: 1.79703329e-06
Iter: 1733 loss: 1.81193991e-06
Iter: 1734 loss: 1.79648646e-06
Iter: 1735 loss: 1.79325502e-06
Iter: 1736 loss: 1.80927566e-06
Iter: 1737 loss: 1.79275435e-06
Iter: 1738 loss: 1.79068252e-06
Iter: 1739 loss: 1.79318215e-06
Iter: 1740 loss: 1.78947914e-06
Iter: 1741 loss: 1.78740027e-06
Iter: 1742 loss: 1.81141877e-06
Iter: 1743 loss: 1.787364e-06
Iter: 1744 loss: 1.78586129e-06
Iter: 1745 loss: 1.7853672e-06
Iter: 1746 loss: 1.78445043e-06
Iter: 1747 loss: 1.78225173e-06
Iter: 1748 loss: 1.80016264e-06
Iter: 1749 loss: 1.78212963e-06
Iter: 1750 loss: 1.78062271e-06
Iter: 1751 loss: 1.77821039e-06
Iter: 1752 loss: 1.7782213e-06
Iter: 1753 loss: 1.77588447e-06
Iter: 1754 loss: 1.79975427e-06
Iter: 1755 loss: 1.77575055e-06
Iter: 1756 loss: 1.77346033e-06
Iter: 1757 loss: 1.77722143e-06
Iter: 1758 loss: 1.77247603e-06
Iter: 1759 loss: 1.77026664e-06
Iter: 1760 loss: 1.7705031e-06
Iter: 1761 loss: 1.76862761e-06
Iter: 1762 loss: 1.76574713e-06
Iter: 1763 loss: 1.77626328e-06
Iter: 1764 loss: 1.76502203e-06
Iter: 1765 loss: 1.76273852e-06
Iter: 1766 loss: 1.79714664e-06
Iter: 1767 loss: 1.76271897e-06
Iter: 1768 loss: 1.76113213e-06
Iter: 1769 loss: 1.75825892e-06
Iter: 1770 loss: 1.82690076e-06
Iter: 1771 loss: 1.75820787e-06
Iter: 1772 loss: 1.75531011e-06
Iter: 1773 loss: 1.7722366e-06
Iter: 1774 loss: 1.75495927e-06
Iter: 1775 loss: 1.75282253e-06
Iter: 1776 loss: 1.76676474e-06
Iter: 1777 loss: 1.75260345e-06
Iter: 1778 loss: 1.75055936e-06
Iter: 1779 loss: 1.75602577e-06
Iter: 1780 loss: 1.74989668e-06
Iter: 1781 loss: 1.74797378e-06
Iter: 1782 loss: 1.75207776e-06
Iter: 1783 loss: 1.74725324e-06
Iter: 1784 loss: 1.74480874e-06
Iter: 1785 loss: 1.75247351e-06
Iter: 1786 loss: 1.74408899e-06
Iter: 1787 loss: 1.74219372e-06
Iter: 1788 loss: 1.7442735e-06
Iter: 1789 loss: 1.74110028e-06
Iter: 1790 loss: 1.73883075e-06
Iter: 1791 loss: 1.75042942e-06
Iter: 1792 loss: 1.73851402e-06
Iter: 1793 loss: 1.73663523e-06
Iter: 1794 loss: 1.73420699e-06
Iter: 1795 loss: 1.73409876e-06
Iter: 1796 loss: 1.73158048e-06
Iter: 1797 loss: 1.76329388e-06
Iter: 1798 loss: 1.7315582e-06
Iter: 1799 loss: 1.72931436e-06
Iter: 1800 loss: 1.73455021e-06
Iter: 1801 loss: 1.72851139e-06
Iter: 1802 loss: 1.72649675e-06
Iter: 1803 loss: 1.72480429e-06
Iter: 1804 loss: 1.72431442e-06
Iter: 1805 loss: 1.72204864e-06
Iter: 1806 loss: 1.75318416e-06
Iter: 1807 loss: 1.72198611e-06
Iter: 1808 loss: 1.71966462e-06
Iter: 1809 loss: 1.72510499e-06
Iter: 1810 loss: 1.71887166e-06
Iter: 1811 loss: 1.71686679e-06
Iter: 1812 loss: 1.71618512e-06
Iter: 1813 loss: 1.71504507e-06
Iter: 1814 loss: 1.71234387e-06
Iter: 1815 loss: 1.71264037e-06
Iter: 1816 loss: 1.71026181e-06
Iter: 1817 loss: 1.70774376e-06
Iter: 1818 loss: 1.70776286e-06
Iter: 1819 loss: 1.70567773e-06
Iter: 1820 loss: 1.72238015e-06
Iter: 1821 loss: 1.7055761e-06
Iter: 1822 loss: 1.70418775e-06
Iter: 1823 loss: 1.70254691e-06
Iter: 1824 loss: 1.70239434e-06
Iter: 1825 loss: 1.70006774e-06
Iter: 1826 loss: 1.72144541e-06
Iter: 1827 loss: 1.69998214e-06
Iter: 1828 loss: 1.69829264e-06
Iter: 1829 loss: 1.6990565e-06
Iter: 1830 loss: 1.69711257e-06
Iter: 1831 loss: 1.69508019e-06
Iter: 1832 loss: 1.69533132e-06
Iter: 1833 loss: 1.69351017e-06
Iter: 1834 loss: 1.69084819e-06
Iter: 1835 loss: 1.71654824e-06
Iter: 1836 loss: 1.69075975e-06
Iter: 1837 loss: 1.68848032e-06
Iter: 1838 loss: 1.68837641e-06
Iter: 1839 loss: 1.68659176e-06
Iter: 1840 loss: 1.68416398e-06
Iter: 1841 loss: 1.69566897e-06
Iter: 1842 loss: 1.68369957e-06
Iter: 1843 loss: 1.68127553e-06
Iter: 1844 loss: 1.69491398e-06
Iter: 1845 loss: 1.68100848e-06
Iter: 1846 loss: 1.67915096e-06
Iter: 1847 loss: 1.67937765e-06
Iter: 1848 loss: 1.67774624e-06
Iter: 1849 loss: 1.67568214e-06
Iter: 1850 loss: 1.69924272e-06
Iter: 1851 loss: 1.67569146e-06
Iter: 1852 loss: 1.67414487e-06
Iter: 1853 loss: 1.67208213e-06
Iter: 1854 loss: 1.67188955e-06
Iter: 1855 loss: 1.6691605e-06
Iter: 1856 loss: 1.67242843e-06
Iter: 1857 loss: 1.66778443e-06
Iter: 1858 loss: 1.66548023e-06
Iter: 1859 loss: 1.69215593e-06
Iter: 1860 loss: 1.66539303e-06
Iter: 1861 loss: 1.66355335e-06
Iter: 1862 loss: 1.67868393e-06
Iter: 1863 loss: 1.66342795e-06
Iter: 1864 loss: 1.66199857e-06
Iter: 1865 loss: 1.65910387e-06
Iter: 1866 loss: 1.71301076e-06
Iter: 1867 loss: 1.65910023e-06
Iter: 1868 loss: 1.65634458e-06
Iter: 1869 loss: 1.66513519e-06
Iter: 1870 loss: 1.65566416e-06
Iter: 1871 loss: 1.65379333e-06
Iter: 1872 loss: 1.65381766e-06
Iter: 1873 loss: 1.6520371e-06
Iter: 1874 loss: 1.65054041e-06
Iter: 1875 loss: 1.65006691e-06
Iter: 1876 loss: 1.64758183e-06
Iter: 1877 loss: 1.65330471e-06
Iter: 1878 loss: 1.64667654e-06
Iter: 1879 loss: 1.64433914e-06
Iter: 1880 loss: 1.6580625e-06
Iter: 1881 loss: 1.6440614e-06
Iter: 1882 loss: 1.64160906e-06
Iter: 1883 loss: 1.64607059e-06
Iter: 1884 loss: 1.64060202e-06
Iter: 1885 loss: 1.63854895e-06
Iter: 1886 loss: 1.64867765e-06
Iter: 1887 loss: 1.63817583e-06
Iter: 1888 loss: 1.63633831e-06
Iter: 1889 loss: 1.64243306e-06
Iter: 1890 loss: 1.63581376e-06
Iter: 1891 loss: 1.63405502e-06
Iter: 1892 loss: 1.63364439e-06
Iter: 1893 loss: 1.63250547e-06
Iter: 1894 loss: 1.63019786e-06
Iter: 1895 loss: 1.6366115e-06
Iter: 1896 loss: 1.62945344e-06
Iter: 1897 loss: 1.62735137e-06
Iter: 1898 loss: 1.65313804e-06
Iter: 1899 loss: 1.62732385e-06
Iter: 1900 loss: 1.62606307e-06
Iter: 1901 loss: 1.62573701e-06
Iter: 1902 loss: 1.62493563e-06
Iter: 1903 loss: 1.62265587e-06
Iter: 1904 loss: 1.63418918e-06
Iter: 1905 loss: 1.62230617e-06
Iter: 1906 loss: 1.62062133e-06
Iter: 1907 loss: 1.6186749e-06
Iter: 1908 loss: 1.61846469e-06
Iter: 1909 loss: 1.61591561e-06
Iter: 1910 loss: 1.61778075e-06
Iter: 1911 loss: 1.61438913e-06
Iter: 1912 loss: 1.61134381e-06
Iter: 1913 loss: 1.62794595e-06
Iter: 1914 loss: 1.61089258e-06
Iter: 1915 loss: 1.60832315e-06
Iter: 1916 loss: 1.61377852e-06
Iter: 1917 loss: 1.60733111e-06
Iter: 1918 loss: 1.60551792e-06
Iter: 1919 loss: 1.60544323e-06
Iter: 1920 loss: 1.604099e-06
Iter: 1921 loss: 1.60340255e-06
Iter: 1922 loss: 1.6027717e-06
Iter: 1923 loss: 1.60095772e-06
Iter: 1924 loss: 1.61089179e-06
Iter: 1925 loss: 1.60078e-06
Iter: 1926 loss: 1.5988868e-06
Iter: 1927 loss: 1.6017791e-06
Iter: 1928 loss: 1.59800061e-06
Iter: 1929 loss: 1.59621163e-06
Iter: 1930 loss: 1.59869774e-06
Iter: 1931 loss: 1.59534466e-06
Iter: 1932 loss: 1.59315277e-06
Iter: 1933 loss: 1.61037417e-06
Iter: 1934 loss: 1.59297565e-06
Iter: 1935 loss: 1.59164267e-06
Iter: 1936 loss: 1.58960529e-06
Iter: 1937 loss: 1.589563e-06
Iter: 1938 loss: 1.58789317e-06
Iter: 1939 loss: 1.58773423e-06
Iter: 1940 loss: 1.58638352e-06
Iter: 1941 loss: 1.58523244e-06
Iter: 1942 loss: 1.58482635e-06
Iter: 1943 loss: 1.5827635e-06
Iter: 1944 loss: 1.5920881e-06
Iter: 1945 loss: 1.58236548e-06
Iter: 1946 loss: 1.58056719e-06
Iter: 1947 loss: 1.58653518e-06
Iter: 1948 loss: 1.58002445e-06
Iter: 1949 loss: 1.57845989e-06
Iter: 1950 loss: 1.57702834e-06
Iter: 1951 loss: 1.57654347e-06
Iter: 1952 loss: 1.57425109e-06
Iter: 1953 loss: 1.58005741e-06
Iter: 1954 loss: 1.57339582e-06
Iter: 1955 loss: 1.5715982e-06
Iter: 1956 loss: 1.57159889e-06
Iter: 1957 loss: 1.57007014e-06
Iter: 1958 loss: 1.56998e-06
Iter: 1959 loss: 1.56873989e-06
Iter: 1960 loss: 1.5669857e-06
Iter: 1961 loss: 1.571437e-06
Iter: 1962 loss: 1.56636406e-06
Iter: 1963 loss: 1.56442434e-06
Iter: 1964 loss: 1.57785598e-06
Iter: 1965 loss: 1.56422607e-06
Iter: 1966 loss: 1.56294163e-06
Iter: 1967 loss: 1.5631947e-06
Iter: 1968 loss: 1.56196029e-06
Iter: 1969 loss: 1.56029034e-06
Iter: 1970 loss: 1.5727685e-06
Iter: 1971 loss: 1.56017427e-06
Iter: 1972 loss: 1.55875455e-06
Iter: 1973 loss: 1.55813541e-06
Iter: 1974 loss: 1.55740418e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2
+ date
Wed Nov  4 14:55:42 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi1.6/300_300_300_1 --function f2 --psi 3 --alpha 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1345f3b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1345eb6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1345e4a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1345dfd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1345dfda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1345dd1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1345e11488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f3cc158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f3a5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f33f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f33fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f362488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f3629d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f380268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f407bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f2b5510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f2b5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f2377b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f21d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f2379d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f265400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f1b7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f1ab510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f1a2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f20f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f157a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f11c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f11c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f11c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f13e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f10eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f0488c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f01eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f01e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f0a9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f133f00e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.011249629
test_loss: 0.012962125
train_loss: 0.009239723
test_loss: 0.0120367175
train_loss: 0.0087181525
test_loss: 0.011454486
train_loss: 0.0077315895
test_loss: 0.01097806
train_loss: 0.00755749
test_loss: 0.0111878095
train_loss: 0.008334445
test_loss: 0.0106511675
train_loss: 0.007359395
test_loss: 0.010841427
train_loss: 0.006950736
test_loss: 0.010226366
train_loss: 0.0068408633
test_loss: 0.010215311
train_loss: 0.0073496886
test_loss: 0.0102106435
train_loss: 0.0069602234
test_loss: 0.010349933
train_loss: 0.006780089
test_loss: 0.010054712
train_loss: 0.0072483206
test_loss: 0.0100191785
train_loss: 0.007168646
test_loss: 0.010368596
train_loss: 0.0082417
test_loss: 0.010152738
train_loss: 0.007134841
test_loss: 0.010591288
train_loss: 0.00709643
test_loss: 0.010401184
train_loss: 0.0068424027
test_loss: 0.009699294
train_loss: 0.006755364
test_loss: 0.010152207
train_loss: 0.006920893
test_loss: 0.009972595
train_loss: 0.007067607
test_loss: 0.010416585
train_loss: 0.0066187102
test_loss: 0.009872733
train_loss: 0.0064874412
test_loss: 0.009926899
train_loss: 0.00683823
test_loss: 0.009517823
train_loss: 0.006818955
test_loss: 0.009864631
train_loss: 0.00659563
test_loss: 0.009706192
train_loss: 0.006149526
test_loss: 0.0097772125
train_loss: 0.006413097
test_loss: 0.009678527
train_loss: 0.0061358996
test_loss: 0.009700307
train_loss: 0.006364886
test_loss: 0.009585539
train_loss: 0.0059007443
test_loss: 0.00949885
train_loss: 0.0062063183
test_loss: 0.009836892
train_loss: 0.006192068
test_loss: 0.009675572
train_loss: 0.006581957
test_loss: 0.009718222
train_loss: 0.006081598
test_loss: 0.009676709
train_loss: 0.005999144
test_loss: 0.009957217
train_loss: 0.0060731596
test_loss: 0.010234555
train_loss: 0.006491597
test_loss: 0.009615117
train_loss: 0.0064974935
test_loss: 0.009467417
train_loss: 0.0056808097
test_loss: 0.009240135
train_loss: 0.005645137
test_loss: 0.009369027
train_loss: 0.0059254495
test_loss: 0.009710447
train_loss: 0.007066665
test_loss: 0.0095861005
train_loss: 0.0057412675
test_loss: 0.009213859
train_loss: 0.005829878
test_loss: 0.009283419
train_loss: 0.0056609563
test_loss: 0.009432762
train_loss: 0.005909369
test_loss: 0.009361724
train_loss: 0.005683993
test_loss: 0.009307522
train_loss: 0.005892736
test_loss: 0.0091465255
train_loss: 0.005918142
test_loss: 0.009508798
train_loss: 0.006214378
test_loss: 0.00943839
train_loss: 0.0067112027
test_loss: 0.009477878
train_loss: 0.005425165
test_loss: 0.009377555
train_loss: 0.0060362127
test_loss: 0.009687651
train_loss: 0.0056094555
test_loss: 0.009380573
train_loss: 0.0056573357
test_loss: 0.009406227
train_loss: 0.005862212
test_loss: 0.009388795
train_loss: 0.0056580314
test_loss: 0.009546511
train_loss: 0.0056327973
test_loss: 0.0094057685
train_loss: 0.0061885947
test_loss: 0.009330644
train_loss: 0.0058251596
test_loss: 0.009609535
train_loss: 0.0052627753
test_loss: 0.009490455
train_loss: 0.0058968146
test_loss: 0.009513593
train_loss: 0.005482479
test_loss: 0.009338298
train_loss: 0.005765664
test_loss: 0.009472039
train_loss: 0.006138441
test_loss: 0.009423078
train_loss: 0.0058325888
test_loss: 0.009640155
train_loss: 0.006134348
test_loss: 0.009391918
train_loss: 0.005253043
test_loss: 0.00924201
train_loss: 0.005976768
test_loss: 0.009594861
train_loss: 0.005438451
test_loss: 0.009341624
train_loss: 0.0056495424
test_loss: 0.009277419
train_loss: 0.005641605
test_loss: 0.009379404
train_loss: 0.0053431783
test_loss: 0.009180859
train_loss: 0.0054679913
test_loss: 0.009137399
train_loss: 0.005824914
test_loss: 0.009355019
train_loss: 0.0050548445
test_loss: 0.009148127
train_loss: 0.0056749396
test_loss: 0.009199231
train_loss: 0.0053991335
test_loss: 0.009398024
train_loss: 0.005393092
test_loss: 0.009262261
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2/300_300_300_1 --optimizer lbfgs --function f2 --psi 3 --alpha 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f498a4df048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f498a4b4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f498a41c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f498a44b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f498a44b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4969224400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49691f4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49692101e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49691a9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49691a9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49691dd950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4969124268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4969156c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4969156ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49690f8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49690aa6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49690b0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4969066730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49690812f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f496909d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4969081268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968fe4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968fa56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968fa51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968fc58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968f68bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968f31ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968f68048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968f687b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968f51d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968f0abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968f0aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968ec2b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4969081510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968ee1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4968e51620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.68013456e-05
Iter: 2 loss: 4.76934547e-05
Iter: 3 loss: 6.78968208e-05
Iter: 4 loss: 4.44741963e-05
Iter: 5 loss: 3.96811083e-05
Iter: 6 loss: 4.22162557e-05
Iter: 7 loss: 3.64898915e-05
Iter: 8 loss: 3.22944834e-05
Iter: 9 loss: 4.44677426e-05
Iter: 10 loss: 3.09889947e-05
Iter: 11 loss: 2.78549924e-05
Iter: 12 loss: 5.29553e-05
Iter: 13 loss: 2.76519149e-05
Iter: 14 loss: 2.56349795e-05
Iter: 15 loss: 2.68524091e-05
Iter: 16 loss: 2.4336181e-05
Iter: 17 loss: 2.28000827e-05
Iter: 18 loss: 2.27856581e-05
Iter: 19 loss: 2.16480803e-05
Iter: 20 loss: 2.18323767e-05
Iter: 21 loss: 2.07929425e-05
Iter: 22 loss: 1.9650739e-05
Iter: 23 loss: 2.82205292e-05
Iter: 24 loss: 1.95603898e-05
Iter: 25 loss: 1.87570404e-05
Iter: 26 loss: 1.94073764e-05
Iter: 27 loss: 1.82744589e-05
Iter: 28 loss: 1.74657e-05
Iter: 29 loss: 1.81731775e-05
Iter: 30 loss: 1.69913528e-05
Iter: 31 loss: 1.6430582e-05
Iter: 32 loss: 1.64279045e-05
Iter: 33 loss: 1.59431402e-05
Iter: 34 loss: 1.60631789e-05
Iter: 35 loss: 1.55892958e-05
Iter: 36 loss: 1.50366586e-05
Iter: 37 loss: 1.59135307e-05
Iter: 38 loss: 1.47798e-05
Iter: 39 loss: 1.432715e-05
Iter: 40 loss: 1.55115558e-05
Iter: 41 loss: 1.41740111e-05
Iter: 42 loss: 1.3800749e-05
Iter: 43 loss: 1.93253454e-05
Iter: 44 loss: 1.38005307e-05
Iter: 45 loss: 1.35435175e-05
Iter: 46 loss: 1.35818755e-05
Iter: 47 loss: 1.33489866e-05
Iter: 48 loss: 1.30714798e-05
Iter: 49 loss: 1.49964e-05
Iter: 50 loss: 1.30455228e-05
Iter: 51 loss: 1.28205356e-05
Iter: 52 loss: 1.3310555e-05
Iter: 53 loss: 1.27333569e-05
Iter: 54 loss: 1.2533922e-05
Iter: 55 loss: 1.36694825e-05
Iter: 56 loss: 1.25066499e-05
Iter: 57 loss: 1.23221853e-05
Iter: 58 loss: 1.30045537e-05
Iter: 59 loss: 1.22768779e-05
Iter: 60 loss: 1.21340809e-05
Iter: 61 loss: 1.21910871e-05
Iter: 62 loss: 1.20354234e-05
Iter: 63 loss: 1.18293756e-05
Iter: 64 loss: 1.25985243e-05
Iter: 65 loss: 1.17791842e-05
Iter: 66 loss: 1.1627706e-05
Iter: 67 loss: 1.1698583e-05
Iter: 68 loss: 1.15257681e-05
Iter: 69 loss: 1.13542574e-05
Iter: 70 loss: 1.19191573e-05
Iter: 71 loss: 1.13065944e-05
Iter: 72 loss: 1.11464e-05
Iter: 73 loss: 1.25964807e-05
Iter: 74 loss: 1.11390409e-05
Iter: 75 loss: 1.1017798e-05
Iter: 76 loss: 1.10267256e-05
Iter: 77 loss: 1.09234916e-05
Iter: 78 loss: 1.0781795e-05
Iter: 79 loss: 1.07162268e-05
Iter: 80 loss: 1.06462703e-05
Iter: 81 loss: 1.05071776e-05
Iter: 82 loss: 1.05070212e-05
Iter: 83 loss: 1.03973243e-05
Iter: 84 loss: 1.09398343e-05
Iter: 85 loss: 1.03787133e-05
Iter: 86 loss: 1.02889799e-05
Iter: 87 loss: 1.03114926e-05
Iter: 88 loss: 1.02236063e-05
Iter: 89 loss: 1.01338273e-05
Iter: 90 loss: 1.08285203e-05
Iter: 91 loss: 1.01273235e-05
Iter: 92 loss: 1.00367834e-05
Iter: 93 loss: 1.03236562e-05
Iter: 94 loss: 1.00109837e-05
Iter: 95 loss: 9.94039692e-06
Iter: 96 loss: 1.0028396e-05
Iter: 97 loss: 9.9039e-06
Iter: 98 loss: 9.81907124e-06
Iter: 99 loss: 1.01051646e-05
Iter: 100 loss: 9.79601555e-06
Iter: 101 loss: 9.71761528e-06
Iter: 102 loss: 9.68501e-06
Iter: 103 loss: 9.64374703e-06
Iter: 104 loss: 9.55698488e-06
Iter: 105 loss: 1.06139069e-05
Iter: 106 loss: 9.55601899e-06
Iter: 107 loss: 9.48554498e-06
Iter: 108 loss: 9.47753506e-06
Iter: 109 loss: 9.4268089e-06
Iter: 110 loss: 9.35026401e-06
Iter: 111 loss: 9.96134713e-06
Iter: 112 loss: 9.345e-06
Iter: 113 loss: 9.27978544e-06
Iter: 114 loss: 9.49824698e-06
Iter: 115 loss: 9.26198118e-06
Iter: 116 loss: 9.2096343e-06
Iter: 117 loss: 9.17121179e-06
Iter: 118 loss: 9.15340843e-06
Iter: 119 loss: 9.0751937e-06
Iter: 120 loss: 9.16806857e-06
Iter: 121 loss: 9.03391356e-06
Iter: 122 loss: 8.95272933e-06
Iter: 123 loss: 9.9584795e-06
Iter: 124 loss: 8.95193e-06
Iter: 125 loss: 8.89394232e-06
Iter: 126 loss: 9.24893e-06
Iter: 127 loss: 8.88689e-06
Iter: 128 loss: 8.84664e-06
Iter: 129 loss: 8.84671499e-06
Iter: 130 loss: 8.81428605e-06
Iter: 131 loss: 8.75439582e-06
Iter: 132 loss: 9.31164323e-06
Iter: 133 loss: 8.75185378e-06
Iter: 134 loss: 8.71647626e-06
Iter: 135 loss: 8.67305698e-06
Iter: 136 loss: 8.66890787e-06
Iter: 137 loss: 8.61770241e-06
Iter: 138 loss: 9.01047861e-06
Iter: 139 loss: 8.61400258e-06
Iter: 140 loss: 8.56397128e-06
Iter: 141 loss: 8.63913192e-06
Iter: 142 loss: 8.53991787e-06
Iter: 143 loss: 8.4938e-06
Iter: 144 loss: 8.49897879e-06
Iter: 145 loss: 8.45837621e-06
Iter: 146 loss: 8.41760175e-06
Iter: 147 loss: 8.41774181e-06
Iter: 148 loss: 8.37996e-06
Iter: 149 loss: 8.36656e-06
Iter: 150 loss: 8.34535e-06
Iter: 151 loss: 8.29611417e-06
Iter: 152 loss: 8.57636769e-06
Iter: 153 loss: 8.2893439e-06
Iter: 154 loss: 8.24958715e-06
Iter: 155 loss: 8.35202263e-06
Iter: 156 loss: 8.23610753e-06
Iter: 157 loss: 8.1946946e-06
Iter: 158 loss: 8.19032e-06
Iter: 159 loss: 8.16017564e-06
Iter: 160 loss: 8.10473284e-06
Iter: 161 loss: 8.22905349e-06
Iter: 162 loss: 8.08388904e-06
Iter: 163 loss: 8.05093077e-06
Iter: 164 loss: 8.04985939e-06
Iter: 165 loss: 8.01913e-06
Iter: 166 loss: 8.04046613e-06
Iter: 167 loss: 7.99980717e-06
Iter: 168 loss: 7.9708534e-06
Iter: 169 loss: 8.17440377e-06
Iter: 170 loss: 7.96843597e-06
Iter: 171 loss: 7.94027255e-06
Iter: 172 loss: 7.90557897e-06
Iter: 173 loss: 7.90259946e-06
Iter: 174 loss: 7.86195869e-06
Iter: 175 loss: 8.02744e-06
Iter: 176 loss: 7.85296288e-06
Iter: 177 loss: 7.81557719e-06
Iter: 178 loss: 7.96693894e-06
Iter: 179 loss: 7.80737173e-06
Iter: 180 loss: 7.7680761e-06
Iter: 181 loss: 7.8804369e-06
Iter: 182 loss: 7.75575791e-06
Iter: 183 loss: 7.72320072e-06
Iter: 184 loss: 7.71463783e-06
Iter: 185 loss: 7.69451071e-06
Iter: 186 loss: 7.65928598e-06
Iter: 187 loss: 8.13259e-06
Iter: 188 loss: 7.65906225e-06
Iter: 189 loss: 7.62595528e-06
Iter: 190 loss: 7.64183278e-06
Iter: 191 loss: 7.6034903e-06
Iter: 192 loss: 7.5723633e-06
Iter: 193 loss: 7.61688943e-06
Iter: 194 loss: 7.55725705e-06
Iter: 195 loss: 7.51982498e-06
Iter: 196 loss: 7.60583953e-06
Iter: 197 loss: 7.50587333e-06
Iter: 198 loss: 7.4660029e-06
Iter: 199 loss: 7.74692489e-06
Iter: 200 loss: 7.46220849e-06
Iter: 201 loss: 7.43577766e-06
Iter: 202 loss: 7.46041405e-06
Iter: 203 loss: 7.4205e-06
Iter: 204 loss: 7.38657081e-06
Iter: 205 loss: 7.70893166e-06
Iter: 206 loss: 7.38527e-06
Iter: 207 loss: 7.36316269e-06
Iter: 208 loss: 7.34404875e-06
Iter: 209 loss: 7.33811339e-06
Iter: 210 loss: 7.31111868e-06
Iter: 211 loss: 7.57042199e-06
Iter: 212 loss: 7.310211e-06
Iter: 213 loss: 7.28502164e-06
Iter: 214 loss: 7.27049928e-06
Iter: 215 loss: 7.25972131e-06
Iter: 216 loss: 7.23097082e-06
Iter: 217 loss: 7.29606518e-06
Iter: 218 loss: 7.22003824e-06
Iter: 219 loss: 7.19349e-06
Iter: 220 loss: 7.4724785e-06
Iter: 221 loss: 7.19286436e-06
Iter: 222 loss: 7.16913382e-06
Iter: 223 loss: 7.17423427e-06
Iter: 224 loss: 7.15167198e-06
Iter: 225 loss: 7.12343171e-06
Iter: 226 loss: 7.1495906e-06
Iter: 227 loss: 7.10734366e-06
Iter: 228 loss: 7.0747883e-06
Iter: 229 loss: 7.41158965e-06
Iter: 230 loss: 7.07389972e-06
Iter: 231 loss: 7.0478186e-06
Iter: 232 loss: 7.04900776e-06
Iter: 233 loss: 7.02720263e-06
Iter: 234 loss: 7.00085911e-06
Iter: 235 loss: 7.01824774e-06
Iter: 236 loss: 6.98435542e-06
Iter: 237 loss: 6.95274412e-06
Iter: 238 loss: 7.15304031e-06
Iter: 239 loss: 6.9491e-06
Iter: 240 loss: 6.92964204e-06
Iter: 241 loss: 6.92958656e-06
Iter: 242 loss: 6.91339255e-06
Iter: 243 loss: 6.88904811e-06
Iter: 244 loss: 6.88841601e-06
Iter: 245 loss: 6.86247176e-06
Iter: 246 loss: 7.11825032e-06
Iter: 247 loss: 6.86177191e-06
Iter: 248 loss: 6.84221823e-06
Iter: 249 loss: 6.83765666e-06
Iter: 250 loss: 6.82522113e-06
Iter: 251 loss: 6.80380663e-06
Iter: 252 loss: 6.99620068e-06
Iter: 253 loss: 6.8025729e-06
Iter: 254 loss: 6.78276274e-06
Iter: 255 loss: 6.77709886e-06
Iter: 256 loss: 6.76516402e-06
Iter: 257 loss: 6.73925342e-06
Iter: 258 loss: 6.80313042e-06
Iter: 259 loss: 6.73016621e-06
Iter: 260 loss: 6.70651025e-06
Iter: 261 loss: 6.89689068e-06
Iter: 262 loss: 6.70505415e-06
Iter: 263 loss: 6.68397888e-06
Iter: 264 loss: 6.70694408e-06
Iter: 265 loss: 6.67255654e-06
Iter: 266 loss: 6.64813115e-06
Iter: 267 loss: 6.67003e-06
Iter: 268 loss: 6.63405262e-06
Iter: 269 loss: 6.61209424e-06
Iter: 270 loss: 6.85952182e-06
Iter: 271 loss: 6.61156309e-06
Iter: 272 loss: 6.59137322e-06
Iter: 273 loss: 6.58262115e-06
Iter: 274 loss: 6.57198143e-06
Iter: 275 loss: 6.54950782e-06
Iter: 276 loss: 6.58482e-06
Iter: 277 loss: 6.53889765e-06
Iter: 278 loss: 6.52065455e-06
Iter: 279 loss: 6.52067138e-06
Iter: 280 loss: 6.50234915e-06
Iter: 281 loss: 6.51031587e-06
Iter: 282 loss: 6.48990817e-06
Iter: 283 loss: 6.47245633e-06
Iter: 284 loss: 6.4625915e-06
Iter: 285 loss: 6.45507316e-06
Iter: 286 loss: 6.43181875e-06
Iter: 287 loss: 6.61128252e-06
Iter: 288 loss: 6.43008298e-06
Iter: 289 loss: 6.40919e-06
Iter: 290 loss: 6.50313177e-06
Iter: 291 loss: 6.4050264e-06
Iter: 292 loss: 6.38874826e-06
Iter: 293 loss: 6.37612902e-06
Iter: 294 loss: 6.37104631e-06
Iter: 295 loss: 6.34877279e-06
Iter: 296 loss: 6.4672895e-06
Iter: 297 loss: 6.34515527e-06
Iter: 298 loss: 6.32077717e-06
Iter: 299 loss: 6.42693294e-06
Iter: 300 loss: 6.31578e-06
Iter: 301 loss: 6.30157774e-06
Iter: 302 loss: 6.31604416e-06
Iter: 303 loss: 6.29356327e-06
Iter: 304 loss: 6.27288136e-06
Iter: 305 loss: 6.34632306e-06
Iter: 306 loss: 6.26751626e-06
Iter: 307 loss: 6.25172697e-06
Iter: 308 loss: 6.28166345e-06
Iter: 309 loss: 6.24507811e-06
Iter: 310 loss: 6.22629932e-06
Iter: 311 loss: 6.27045483e-06
Iter: 312 loss: 6.21928712e-06
Iter: 313 loss: 6.19984e-06
Iter: 314 loss: 6.29747547e-06
Iter: 315 loss: 6.19655202e-06
Iter: 316 loss: 6.18320519e-06
Iter: 317 loss: 6.20180163e-06
Iter: 318 loss: 6.17656769e-06
Iter: 319 loss: 6.15344561e-06
Iter: 320 loss: 6.1906635e-06
Iter: 321 loss: 6.1427786e-06
Iter: 322 loss: 6.1285682e-06
Iter: 323 loss: 6.13496104e-06
Iter: 324 loss: 6.11895894e-06
Iter: 325 loss: 6.09880499e-06
Iter: 326 loss: 6.09337803e-06
Iter: 327 loss: 6.08106711e-06
Iter: 328 loss: 6.0665061e-06
Iter: 329 loss: 6.06499043e-06
Iter: 330 loss: 6.05236482e-06
Iter: 331 loss: 6.06470439e-06
Iter: 332 loss: 6.04509205e-06
Iter: 333 loss: 6.02882301e-06
Iter: 334 loss: 6.02102864e-06
Iter: 335 loss: 6.01299507e-06
Iter: 336 loss: 5.9928916e-06
Iter: 337 loss: 6.1065e-06
Iter: 338 loss: 5.99008126e-06
Iter: 339 loss: 5.97133476e-06
Iter: 340 loss: 6.09903918e-06
Iter: 341 loss: 5.969423e-06
Iter: 342 loss: 5.95488382e-06
Iter: 343 loss: 5.95006577e-06
Iter: 344 loss: 5.94168205e-06
Iter: 345 loss: 5.92318656e-06
Iter: 346 loss: 5.99986242e-06
Iter: 347 loss: 5.91915614e-06
Iter: 348 loss: 5.90218588e-06
Iter: 349 loss: 6.03287936e-06
Iter: 350 loss: 5.90105901e-06
Iter: 351 loss: 5.88882494e-06
Iter: 352 loss: 5.88044259e-06
Iter: 353 loss: 5.87626528e-06
Iter: 354 loss: 5.85741509e-06
Iter: 355 loss: 6.1132464e-06
Iter: 356 loss: 5.85736097e-06
Iter: 357 loss: 5.84746886e-06
Iter: 358 loss: 5.83176507e-06
Iter: 359 loss: 5.83153451e-06
Iter: 360 loss: 5.81228051e-06
Iter: 361 loss: 5.86985152e-06
Iter: 362 loss: 5.80652068e-06
Iter: 363 loss: 5.78836352e-06
Iter: 364 loss: 5.92752167e-06
Iter: 365 loss: 5.78682148e-06
Iter: 366 loss: 5.77297351e-06
Iter: 367 loss: 5.76760704e-06
Iter: 368 loss: 5.76008733e-06
Iter: 369 loss: 5.73940815e-06
Iter: 370 loss: 5.85081489e-06
Iter: 371 loss: 5.73643456e-06
Iter: 372 loss: 5.71825058e-06
Iter: 373 loss: 5.82790472e-06
Iter: 374 loss: 5.71595547e-06
Iter: 375 loss: 5.70429711e-06
Iter: 376 loss: 5.68463111e-06
Iter: 377 loss: 5.6846e-06
Iter: 378 loss: 5.6675367e-06
Iter: 379 loss: 5.66745848e-06
Iter: 380 loss: 5.65300934e-06
Iter: 381 loss: 5.68772657e-06
Iter: 382 loss: 5.64764377e-06
Iter: 383 loss: 5.63471076e-06
Iter: 384 loss: 5.64068932e-06
Iter: 385 loss: 5.62589912e-06
Iter: 386 loss: 5.61019533e-06
Iter: 387 loss: 5.68773521e-06
Iter: 388 loss: 5.60759645e-06
Iter: 389 loss: 5.59128785e-06
Iter: 390 loss: 5.67890083e-06
Iter: 391 loss: 5.58874535e-06
Iter: 392 loss: 5.57799876e-06
Iter: 393 loss: 5.58393276e-06
Iter: 394 loss: 5.57084331e-06
Iter: 395 loss: 5.55454471e-06
Iter: 396 loss: 5.59583805e-06
Iter: 397 loss: 5.54892586e-06
Iter: 398 loss: 5.53535938e-06
Iter: 399 loss: 5.53552854e-06
Iter: 400 loss: 5.52449364e-06
Iter: 401 loss: 5.5061837e-06
Iter: 402 loss: 5.5197097e-06
Iter: 403 loss: 5.49497508e-06
Iter: 404 loss: 5.4766997e-06
Iter: 405 loss: 5.62481637e-06
Iter: 406 loss: 5.47548188e-06
Iter: 407 loss: 5.45999683e-06
Iter: 408 loss: 5.58000193e-06
Iter: 409 loss: 5.45886451e-06
Iter: 410 loss: 5.447e-06
Iter: 411 loss: 5.43964234e-06
Iter: 412 loss: 5.43479382e-06
Iter: 413 loss: 5.41749705e-06
Iter: 414 loss: 5.46256433e-06
Iter: 415 loss: 5.41164945e-06
Iter: 416 loss: 5.39766552e-06
Iter: 417 loss: 5.5919445e-06
Iter: 418 loss: 5.39764687e-06
Iter: 419 loss: 5.38707809e-06
Iter: 420 loss: 5.38688346e-06
Iter: 421 loss: 5.3787744e-06
Iter: 422 loss: 5.36594052e-06
Iter: 423 loss: 5.39915527e-06
Iter: 424 loss: 5.36158132e-06
Iter: 425 loss: 5.34608535e-06
Iter: 426 loss: 5.4374882e-06
Iter: 427 loss: 5.34403534e-06
Iter: 428 loss: 5.33260027e-06
Iter: 429 loss: 5.35422396e-06
Iter: 430 loss: 5.32788545e-06
Iter: 431 loss: 5.31485421e-06
Iter: 432 loss: 5.33168304e-06
Iter: 433 loss: 5.30830039e-06
Iter: 434 loss: 5.29590216e-06
Iter: 435 loss: 5.30231546e-06
Iter: 436 loss: 5.28774672e-06
Iter: 437 loss: 5.27205e-06
Iter: 438 loss: 5.40373549e-06
Iter: 439 loss: 5.27099473e-06
Iter: 440 loss: 5.25948053e-06
Iter: 441 loss: 5.25802079e-06
Iter: 442 loss: 5.24981897e-06
Iter: 443 loss: 5.23496419e-06
Iter: 444 loss: 5.2661444e-06
Iter: 445 loss: 5.22896153e-06
Iter: 446 loss: 5.21354468e-06
Iter: 447 loss: 5.23424433e-06
Iter: 448 loss: 5.2056248e-06
Iter: 449 loss: 5.19170271e-06
Iter: 450 loss: 5.19171181e-06
Iter: 451 loss: 5.17985609e-06
Iter: 452 loss: 5.18995876e-06
Iter: 453 loss: 5.17266699e-06
Iter: 454 loss: 5.16024e-06
Iter: 455 loss: 5.16717228e-06
Iter: 456 loss: 5.15222428e-06
Iter: 457 loss: 5.13849682e-06
Iter: 458 loss: 5.2528e-06
Iter: 459 loss: 5.137676e-06
Iter: 460 loss: 5.12492625e-06
Iter: 461 loss: 5.16493083e-06
Iter: 462 loss: 5.12118e-06
Iter: 463 loss: 5.10995915e-06
Iter: 464 loss: 5.13386567e-06
Iter: 465 loss: 5.10571317e-06
Iter: 466 loss: 5.09249548e-06
Iter: 467 loss: 5.13591249e-06
Iter: 468 loss: 5.08880748e-06
Iter: 469 loss: 5.07964796e-06
Iter: 470 loss: 5.06816605e-06
Iter: 471 loss: 5.06720789e-06
Iter: 472 loss: 5.05400931e-06
Iter: 473 loss: 5.19387777e-06
Iter: 474 loss: 5.05369871e-06
Iter: 475 loss: 5.0418771e-06
Iter: 476 loss: 5.06175047e-06
Iter: 477 loss: 5.03656156e-06
Iter: 478 loss: 5.02534385e-06
Iter: 479 loss: 5.02926741e-06
Iter: 480 loss: 5.01738032e-06
Iter: 481 loss: 5.00310671e-06
Iter: 482 loss: 5.14933299e-06
Iter: 483 loss: 5.00265833e-06
Iter: 484 loss: 4.99274029e-06
Iter: 485 loss: 5.00111219e-06
Iter: 486 loss: 4.98690906e-06
Iter: 487 loss: 4.97524434e-06
Iter: 488 loss: 4.9686455e-06
Iter: 489 loss: 4.96354824e-06
Iter: 490 loss: 4.95082622e-06
Iter: 491 loss: 5.12611223e-06
Iter: 492 loss: 4.95069e-06
Iter: 493 loss: 4.93750167e-06
Iter: 494 loss: 4.96169832e-06
Iter: 495 loss: 4.93181733e-06
Iter: 496 loss: 4.91960873e-06
Iter: 497 loss: 4.94667529e-06
Iter: 498 loss: 4.91504579e-06
Iter: 499 loss: 4.90501134e-06
Iter: 500 loss: 5.0224844e-06
Iter: 501 loss: 4.9048881e-06
Iter: 502 loss: 4.89633976e-06
Iter: 503 loss: 4.88732076e-06
Iter: 504 loss: 4.88560545e-06
Iter: 505 loss: 4.8730335e-06
Iter: 506 loss: 4.89836566e-06
Iter: 507 loss: 4.86795e-06
Iter: 508 loss: 4.85335659e-06
Iter: 509 loss: 4.96184384e-06
Iter: 510 loss: 4.85204055e-06
Iter: 511 loss: 4.84338307e-06
Iter: 512 loss: 4.83768326e-06
Iter: 513 loss: 4.83450231e-06
Iter: 514 loss: 4.81976167e-06
Iter: 515 loss: 4.83240456e-06
Iter: 516 loss: 4.81115649e-06
Iter: 517 loss: 4.7999747e-06
Iter: 518 loss: 4.79993e-06
Iter: 519 loss: 4.79093524e-06
Iter: 520 loss: 4.79247956e-06
Iter: 521 loss: 4.78401716e-06
Iter: 522 loss: 4.7701069e-06
Iter: 523 loss: 4.79177652e-06
Iter: 524 loss: 4.76366949e-06
Iter: 525 loss: 4.75387151e-06
Iter: 526 loss: 4.88878504e-06
Iter: 527 loss: 4.75385377e-06
Iter: 528 loss: 4.74511035e-06
Iter: 529 loss: 4.73842783e-06
Iter: 530 loss: 4.73562e-06
Iter: 531 loss: 4.72485499e-06
Iter: 532 loss: 4.78020047e-06
Iter: 533 loss: 4.72318288e-06
Iter: 534 loss: 4.71192106e-06
Iter: 535 loss: 4.78215043e-06
Iter: 536 loss: 4.71071053e-06
Iter: 537 loss: 4.70325722e-06
Iter: 538 loss: 4.70747091e-06
Iter: 539 loss: 4.69834958e-06
Iter: 540 loss: 4.68736789e-06
Iter: 541 loss: 4.7249132e-06
Iter: 542 loss: 4.68453891e-06
Iter: 543 loss: 4.67481641e-06
Iter: 544 loss: 4.67284463e-06
Iter: 545 loss: 4.66658821e-06
Iter: 546 loss: 4.65492485e-06
Iter: 547 loss: 4.68098597e-06
Iter: 548 loss: 4.65055655e-06
Iter: 549 loss: 4.64045024e-06
Iter: 550 loss: 4.75999605e-06
Iter: 551 loss: 4.64023469e-06
Iter: 552 loss: 4.63105789e-06
Iter: 553 loss: 4.62824755e-06
Iter: 554 loss: 4.62286471e-06
Iter: 555 loss: 4.61310174e-06
Iter: 556 loss: 4.61102627e-06
Iter: 557 loss: 4.60477668e-06
Iter: 558 loss: 4.5898696e-06
Iter: 559 loss: 4.68490134e-06
Iter: 560 loss: 4.58830527e-06
Iter: 561 loss: 4.57685064e-06
Iter: 562 loss: 4.63474362e-06
Iter: 563 loss: 4.57492479e-06
Iter: 564 loss: 4.56445241e-06
Iter: 565 loss: 4.5897873e-06
Iter: 566 loss: 4.56057387e-06
Iter: 567 loss: 4.55020381e-06
Iter: 568 loss: 4.56292764e-06
Iter: 569 loss: 4.5447523e-06
Iter: 570 loss: 4.53374059e-06
Iter: 571 loss: 4.68270264e-06
Iter: 572 loss: 4.53376106e-06
Iter: 573 loss: 4.52658696e-06
Iter: 574 loss: 4.52512768e-06
Iter: 575 loss: 4.52049699e-06
Iter: 576 loss: 4.51096548e-06
Iter: 577 loss: 4.57758688e-06
Iter: 578 loss: 4.51004462e-06
Iter: 579 loss: 4.50272546e-06
Iter: 580 loss: 4.49545814e-06
Iter: 581 loss: 4.49381696e-06
Iter: 582 loss: 4.48287483e-06
Iter: 583 loss: 4.52916584e-06
Iter: 584 loss: 4.48063201e-06
Iter: 585 loss: 4.4693943e-06
Iter: 586 loss: 4.52781887e-06
Iter: 587 loss: 4.46771264e-06
Iter: 588 loss: 4.45874912e-06
Iter: 589 loss: 4.45219302e-06
Iter: 590 loss: 4.44931266e-06
Iter: 591 loss: 4.43722138e-06
Iter: 592 loss: 4.48519131e-06
Iter: 593 loss: 4.43448153e-06
Iter: 594 loss: 4.42426972e-06
Iter: 595 loss: 4.54819e-06
Iter: 596 loss: 4.42411283e-06
Iter: 597 loss: 4.41723887e-06
Iter: 598 loss: 4.41223801e-06
Iter: 599 loss: 4.41000066e-06
Iter: 600 loss: 4.39816085e-06
Iter: 601 loss: 4.39520682e-06
Iter: 602 loss: 4.38780353e-06
Iter: 603 loss: 4.37396466e-06
Iter: 604 loss: 4.51744563e-06
Iter: 605 loss: 4.37366543e-06
Iter: 606 loss: 4.36375149e-06
Iter: 607 loss: 4.43821136e-06
Iter: 608 loss: 4.36305e-06
Iter: 609 loss: 4.3542268e-06
Iter: 610 loss: 4.37482686e-06
Iter: 611 loss: 4.35104084e-06
Iter: 612 loss: 4.34169215e-06
Iter: 613 loss: 4.36888831e-06
Iter: 614 loss: 4.33894638e-06
Iter: 615 loss: 4.33160312e-06
Iter: 616 loss: 4.33114974e-06
Iter: 617 loss: 4.32563502e-06
Iter: 618 loss: 4.3172713e-06
Iter: 619 loss: 4.3839982e-06
Iter: 620 loss: 4.3167347e-06
Iter: 621 loss: 4.30808632e-06
Iter: 622 loss: 4.30968066e-06
Iter: 623 loss: 4.30148611e-06
Iter: 624 loss: 4.29150896e-06
Iter: 625 loss: 4.31469562e-06
Iter: 626 loss: 4.28776184e-06
Iter: 627 loss: 4.27763916e-06
Iter: 628 loss: 4.26983252e-06
Iter: 629 loss: 4.26659e-06
Iter: 630 loss: 4.25977851e-06
Iter: 631 loss: 4.25788221e-06
Iter: 632 loss: 4.25012604e-06
Iter: 633 loss: 4.24101972e-06
Iter: 634 loss: 4.2400643e-06
Iter: 635 loss: 4.22761741e-06
Iter: 636 loss: 4.25942e-06
Iter: 637 loss: 4.22328503e-06
Iter: 638 loss: 4.21290679e-06
Iter: 639 loss: 4.2394613e-06
Iter: 640 loss: 4.20925517e-06
Iter: 641 loss: 4.19952721e-06
Iter: 642 loss: 4.33742071e-06
Iter: 643 loss: 4.19953676e-06
Iter: 644 loss: 4.1935391e-06
Iter: 645 loss: 4.2049337e-06
Iter: 646 loss: 4.19082107e-06
Iter: 647 loss: 4.18401942e-06
Iter: 648 loss: 4.21542245e-06
Iter: 649 loss: 4.18275613e-06
Iter: 650 loss: 4.17637784e-06
Iter: 651 loss: 4.16965895e-06
Iter: 652 loss: 4.16866715e-06
Iter: 653 loss: 4.15883687e-06
Iter: 654 loss: 4.21027971e-06
Iter: 655 loss: 4.15737804e-06
Iter: 656 loss: 4.14833312e-06
Iter: 657 loss: 4.17277488e-06
Iter: 658 loss: 4.14536134e-06
Iter: 659 loss: 4.13693397e-06
Iter: 660 loss: 4.1440062e-06
Iter: 661 loss: 4.1319654e-06
Iter: 662 loss: 4.12204554e-06
Iter: 663 loss: 4.21296136e-06
Iter: 664 loss: 4.12157169e-06
Iter: 665 loss: 4.11400424e-06
Iter: 666 loss: 4.11157635e-06
Iter: 667 loss: 4.10717621e-06
Iter: 668 loss: 4.09841869e-06
Iter: 669 loss: 4.10415578e-06
Iter: 670 loss: 4.09269069e-06
Iter: 671 loss: 4.081935e-06
Iter: 672 loss: 4.14940587e-06
Iter: 673 loss: 4.08080541e-06
Iter: 674 loss: 4.07158586e-06
Iter: 675 loss: 4.13503449e-06
Iter: 676 loss: 4.0705936e-06
Iter: 677 loss: 4.06239269e-06
Iter: 678 loss: 4.05868605e-06
Iter: 679 loss: 4.0545192e-06
Iter: 680 loss: 4.04406046e-06
Iter: 681 loss: 4.07720654e-06
Iter: 682 loss: 4.04117327e-06
Iter: 683 loss: 4.03353e-06
Iter: 684 loss: 4.03346075e-06
Iter: 685 loss: 4.02744763e-06
Iter: 686 loss: 4.01875786e-06
Iter: 687 loss: 4.01855823e-06
Iter: 688 loss: 4.00987e-06
Iter: 689 loss: 4.0283112e-06
Iter: 690 loss: 4.0067e-06
Iter: 691 loss: 3.99677629e-06
Iter: 692 loss: 4.0624509e-06
Iter: 693 loss: 3.99570126e-06
Iter: 694 loss: 3.98757584e-06
Iter: 695 loss: 3.99843202e-06
Iter: 696 loss: 3.98358679e-06
Iter: 697 loss: 3.97486383e-06
Iter: 698 loss: 4.00053113e-06
Iter: 699 loss: 3.97217309e-06
Iter: 700 loss: 3.96257383e-06
Iter: 701 loss: 3.99272358e-06
Iter: 702 loss: 3.9598508e-06
Iter: 703 loss: 3.95196821e-06
Iter: 704 loss: 3.95924508e-06
Iter: 705 loss: 3.94735207e-06
Iter: 706 loss: 3.93935261e-06
Iter: 707 loss: 4.02560181e-06
Iter: 708 loss: 3.93919572e-06
Iter: 709 loss: 3.93286336e-06
Iter: 710 loss: 3.92100674e-06
Iter: 711 loss: 4.17547108e-06
Iter: 712 loss: 3.92092e-06
Iter: 713 loss: 3.91020239e-06
Iter: 714 loss: 3.96200357e-06
Iter: 715 loss: 3.90827154e-06
Iter: 716 loss: 3.89881552e-06
Iter: 717 loss: 3.97979693e-06
Iter: 718 loss: 3.89825482e-06
Iter: 719 loss: 3.8898188e-06
Iter: 720 loss: 3.92946276e-06
Iter: 721 loss: 3.88829812e-06
Iter: 722 loss: 3.88198805e-06
Iter: 723 loss: 3.8954604e-06
Iter: 724 loss: 3.87959517e-06
Iter: 725 loss: 3.87157252e-06
Iter: 726 loss: 3.88300577e-06
Iter: 727 loss: 3.86784768e-06
Iter: 728 loss: 3.8601429e-06
Iter: 729 loss: 3.86084594e-06
Iter: 730 loss: 3.85428029e-06
Iter: 731 loss: 3.84456689e-06
Iter: 732 loss: 3.85581e-06
Iter: 733 loss: 3.83941369e-06
Iter: 734 loss: 3.82930239e-06
Iter: 735 loss: 3.91307094e-06
Iter: 736 loss: 3.82861344e-06
Iter: 737 loss: 3.8205535e-06
Iter: 738 loss: 3.86577403e-06
Iter: 739 loss: 3.81938389e-06
Iter: 740 loss: 3.81260952e-06
Iter: 741 loss: 3.81433074e-06
Iter: 742 loss: 3.80756592e-06
Iter: 743 loss: 3.8000635e-06
Iter: 744 loss: 3.87028285e-06
Iter: 745 loss: 3.79975927e-06
Iter: 746 loss: 3.79428593e-06
Iter: 747 loss: 3.78636287e-06
Iter: 748 loss: 3.7861264e-06
Iter: 749 loss: 3.77462538e-06
Iter: 750 loss: 3.7946329e-06
Iter: 751 loss: 3.7695313e-06
Iter: 752 loss: 3.76186e-06
Iter: 753 loss: 3.87232285e-06
Iter: 754 loss: 3.76190474e-06
Iter: 755 loss: 3.75468449e-06
Iter: 756 loss: 3.7830514e-06
Iter: 757 loss: 3.75311674e-06
Iter: 758 loss: 3.74639558e-06
Iter: 759 loss: 3.75172704e-06
Iter: 760 loss: 3.74244951e-06
Iter: 761 loss: 3.73509101e-06
Iter: 762 loss: 3.78853974e-06
Iter: 763 loss: 3.73449393e-06
Iter: 764 loss: 3.72967133e-06
Iter: 765 loss: 3.72280806e-06
Iter: 766 loss: 3.72257887e-06
Iter: 767 loss: 3.71351416e-06
Iter: 768 loss: 3.73852549e-06
Iter: 769 loss: 3.71060491e-06
Iter: 770 loss: 3.70245152e-06
Iter: 771 loss: 3.79529479e-06
Iter: 772 loss: 3.70235648e-06
Iter: 773 loss: 3.6958595e-06
Iter: 774 loss: 3.69595364e-06
Iter: 775 loss: 3.69071836e-06
Iter: 776 loss: 3.68251631e-06
Iter: 777 loss: 3.69669169e-06
Iter: 778 loss: 3.67884286e-06
Iter: 779 loss: 3.67044e-06
Iter: 780 loss: 3.74932711e-06
Iter: 781 loss: 3.6701208e-06
Iter: 782 loss: 3.66363975e-06
Iter: 783 loss: 3.67042549e-06
Iter: 784 loss: 3.66006475e-06
Iter: 785 loss: 3.65271217e-06
Iter: 786 loss: 3.66652375e-06
Iter: 787 loss: 3.64970583e-06
Iter: 788 loss: 3.64188486e-06
Iter: 789 loss: 3.68340943e-06
Iter: 790 loss: 3.64062157e-06
Iter: 791 loss: 3.63464483e-06
Iter: 792 loss: 3.62757464e-06
Iter: 793 loss: 3.62679953e-06
Iter: 794 loss: 3.62390165e-06
Iter: 795 loss: 3.62164747e-06
Iter: 796 loss: 3.61728098e-06
Iter: 797 loss: 3.61019033e-06
Iter: 798 loss: 3.61023012e-06
Iter: 799 loss: 3.60295439e-06
Iter: 800 loss: 3.62193396e-06
Iter: 801 loss: 3.60050808e-06
Iter: 802 loss: 3.59291e-06
Iter: 803 loss: 3.61604589e-06
Iter: 804 loss: 3.59078513e-06
Iter: 805 loss: 3.58182865e-06
Iter: 806 loss: 3.62005449e-06
Iter: 807 loss: 3.58002421e-06
Iter: 808 loss: 3.57412182e-06
Iter: 809 loss: 3.56888745e-06
Iter: 810 loss: 3.56736541e-06
Iter: 811 loss: 3.55777888e-06
Iter: 812 loss: 3.5837852e-06
Iter: 813 loss: 3.55463408e-06
Iter: 814 loss: 3.54686745e-06
Iter: 815 loss: 3.65060191e-06
Iter: 816 loss: 3.5468604e-06
Iter: 817 loss: 3.53982523e-06
Iter: 818 loss: 3.54714984e-06
Iter: 819 loss: 3.53591213e-06
Iter: 820 loss: 3.52928669e-06
Iter: 821 loss: 3.53579344e-06
Iter: 822 loss: 3.52566576e-06
Iter: 823 loss: 3.51712811e-06
Iter: 824 loss: 3.56906912e-06
Iter: 825 loss: 3.51615245e-06
Iter: 826 loss: 3.50999767e-06
Iter: 827 loss: 3.51696849e-06
Iter: 828 loss: 3.50660935e-06
Iter: 829 loss: 3.50074788e-06
Iter: 830 loss: 3.55578572e-06
Iter: 831 loss: 3.50049777e-06
Iter: 832 loss: 3.49466063e-06
Iter: 833 loss: 3.49452239e-06
Iter: 834 loss: 3.48994581e-06
Iter: 835 loss: 3.48313188e-06
Iter: 836 loss: 3.53472751e-06
Iter: 837 loss: 3.48251e-06
Iter: 838 loss: 3.4776981e-06
Iter: 839 loss: 3.47604168e-06
Iter: 840 loss: 3.47334208e-06
Iter: 841 loss: 3.46614092e-06
Iter: 842 loss: 3.46759e-06
Iter: 843 loss: 3.4607931e-06
Iter: 844 loss: 3.45244393e-06
Iter: 845 loss: 3.50393179e-06
Iter: 846 loss: 3.45149374e-06
Iter: 847 loss: 3.44454475e-06
Iter: 848 loss: 3.48562321e-06
Iter: 849 loss: 3.44363616e-06
Iter: 850 loss: 3.43634042e-06
Iter: 851 loss: 3.44162254e-06
Iter: 852 loss: 3.43174975e-06
Iter: 853 loss: 3.42496605e-06
Iter: 854 loss: 3.42604062e-06
Iter: 855 loss: 3.41981695e-06
Iter: 856 loss: 3.41198711e-06
Iter: 857 loss: 3.49277343e-06
Iter: 858 loss: 3.41175019e-06
Iter: 859 loss: 3.40578617e-06
Iter: 860 loss: 3.44334944e-06
Iter: 861 loss: 3.40521865e-06
Iter: 862 loss: 3.40077304e-06
Iter: 863 loss: 3.39391158e-06
Iter: 864 loss: 3.39372718e-06
Iter: 865 loss: 3.38647874e-06
Iter: 866 loss: 3.45209196e-06
Iter: 867 loss: 3.38622181e-06
Iter: 868 loss: 3.3788192e-06
Iter: 869 loss: 3.41335226e-06
Iter: 870 loss: 3.37757547e-06
Iter: 871 loss: 3.37288679e-06
Iter: 872 loss: 3.37701385e-06
Iter: 873 loss: 3.37021197e-06
Iter: 874 loss: 3.36374387e-06
Iter: 875 loss: 3.37615847e-06
Iter: 876 loss: 3.36116796e-06
Iter: 877 loss: 3.35466621e-06
Iter: 878 loss: 3.35808022e-06
Iter: 879 loss: 3.35046934e-06
Iter: 880 loss: 3.34403512e-06
Iter: 881 loss: 3.41898635e-06
Iter: 882 loss: 3.34396918e-06
Iter: 883 loss: 3.33907724e-06
Iter: 884 loss: 3.33656362e-06
Iter: 885 loss: 3.33434173e-06
Iter: 886 loss: 3.32713807e-06
Iter: 887 loss: 3.3413321e-06
Iter: 888 loss: 3.3241256e-06
Iter: 889 loss: 3.31736e-06
Iter: 890 loss: 3.37093297e-06
Iter: 891 loss: 3.31695674e-06
Iter: 892 loss: 3.3099659e-06
Iter: 893 loss: 3.31914089e-06
Iter: 894 loss: 3.30641888e-06
Iter: 895 loss: 3.30043e-06
Iter: 896 loss: 3.31016327e-06
Iter: 897 loss: 3.2977473e-06
Iter: 898 loss: 3.29128807e-06
Iter: 899 loss: 3.30683861e-06
Iter: 900 loss: 3.28884653e-06
Iter: 901 loss: 3.28297392e-06
Iter: 902 loss: 3.36011431e-06
Iter: 903 loss: 3.28295414e-06
Iter: 904 loss: 3.27872817e-06
Iter: 905 loss: 3.27085331e-06
Iter: 906 loss: 3.44729733e-06
Iter: 907 loss: 3.27079306e-06
Iter: 908 loss: 3.2671503e-06
Iter: 909 loss: 3.26559802e-06
Iter: 910 loss: 3.26264262e-06
Iter: 911 loss: 3.25620658e-06
Iter: 912 loss: 3.36136804e-06
Iter: 913 loss: 3.25612837e-06
Iter: 914 loss: 3.24815301e-06
Iter: 915 loss: 3.26838199e-06
Iter: 916 loss: 3.2454484e-06
Iter: 917 loss: 3.23905215e-06
Iter: 918 loss: 3.27933913e-06
Iter: 919 loss: 3.23824861e-06
Iter: 920 loss: 3.23165409e-06
Iter: 921 loss: 3.24969574e-06
Iter: 922 loss: 3.22953656e-06
Iter: 923 loss: 3.22407641e-06
Iter: 924 loss: 3.22459118e-06
Iter: 925 loss: 3.2198418e-06
Iter: 926 loss: 3.21307471e-06
Iter: 927 loss: 3.26116083e-06
Iter: 928 loss: 3.21247171e-06
Iter: 929 loss: 3.20646609e-06
Iter: 930 loss: 3.22252936e-06
Iter: 931 loss: 3.20437766e-06
Iter: 932 loss: 3.1993859e-06
Iter: 933 loss: 3.204314e-06
Iter: 934 loss: 3.19658329e-06
Iter: 935 loss: 3.18933485e-06
Iter: 936 loss: 3.22628512e-06
Iter: 937 loss: 3.18806451e-06
Iter: 938 loss: 3.18334014e-06
Iter: 939 loss: 3.18347566e-06
Iter: 940 loss: 3.17968215e-06
Iter: 941 loss: 3.17296417e-06
Iter: 942 loss: 3.20495633e-06
Iter: 943 loss: 3.17180957e-06
Iter: 944 loss: 3.16506839e-06
Iter: 945 loss: 3.20508957e-06
Iter: 946 loss: 3.16424985e-06
Iter: 947 loss: 3.16016849e-06
Iter: 948 loss: 3.16577143e-06
Iter: 949 loss: 3.15807802e-06
Iter: 950 loss: 3.15280613e-06
Iter: 951 loss: 3.17072499e-06
Iter: 952 loss: 3.15144439e-06
Iter: 953 loss: 3.14726276e-06
Iter: 954 loss: 3.13986538e-06
Iter: 955 loss: 3.13986948e-06
Iter: 956 loss: 3.13191731e-06
Iter: 957 loss: 3.18697039e-06
Iter: 958 loss: 3.13121927e-06
Iter: 959 loss: 3.12500379e-06
Iter: 960 loss: 3.13684541e-06
Iter: 961 loss: 3.12228758e-06
Iter: 962 loss: 3.11595818e-06
Iter: 963 loss: 3.16665819e-06
Iter: 964 loss: 3.11556596e-06
Iter: 965 loss: 3.10975247e-06
Iter: 966 loss: 3.12265e-06
Iter: 967 loss: 3.1074087e-06
Iter: 968 loss: 3.10243331e-06
Iter: 969 loss: 3.09930533e-06
Iter: 970 loss: 3.09723509e-06
Iter: 971 loss: 3.09056441e-06
Iter: 972 loss: 3.18263119e-06
Iter: 973 loss: 3.09055304e-06
Iter: 974 loss: 3.08518e-06
Iter: 975 loss: 3.09096367e-06
Iter: 976 loss: 3.08240624e-06
Iter: 977 loss: 3.07727919e-06
Iter: 978 loss: 3.09443908e-06
Iter: 979 loss: 3.07590926e-06
Iter: 980 loss: 3.07045366e-06
Iter: 981 loss: 3.09365032e-06
Iter: 982 loss: 3.06925585e-06
Iter: 983 loss: 3.06508514e-06
Iter: 984 loss: 3.08433778e-06
Iter: 985 loss: 3.06433958e-06
Iter: 986 loss: 3.05997719e-06
Iter: 987 loss: 3.05930052e-06
Iter: 988 loss: 3.05626236e-06
Iter: 989 loss: 3.0510605e-06
Iter: 990 loss: 3.05945787e-06
Iter: 991 loss: 3.04867399e-06
Iter: 992 loss: 3.04261084e-06
Iter: 993 loss: 3.0873216e-06
Iter: 994 loss: 3.042137e-06
Iter: 995 loss: 3.03776415e-06
Iter: 996 loss: 3.03197498e-06
Iter: 997 loss: 3.03164052e-06
Iter: 998 loss: 3.02422677e-06
Iter: 999 loss: 3.04914965e-06
Iter: 1000 loss: 3.02236731e-06
Iter: 1001 loss: 3.01528416e-06
Iter: 1002 loss: 3.03630191e-06
Iter: 1003 loss: 3.01315094e-06
Iter: 1004 loss: 3.006101e-06
Iter: 1005 loss: 3.01361752e-06
Iter: 1006 loss: 3.00227794e-06
Iter: 1007 loss: 2.99602152e-06
Iter: 1008 loss: 3.09096117e-06
Iter: 1009 loss: 2.99601152e-06
Iter: 1010 loss: 2.99078602e-06
Iter: 1011 loss: 2.99942053e-06
Iter: 1012 loss: 2.98839518e-06
Iter: 1013 loss: 2.98247528e-06
Iter: 1014 loss: 2.98779833e-06
Iter: 1015 loss: 2.97907764e-06
Iter: 1016 loss: 2.97436054e-06
Iter: 1017 loss: 2.97432416e-06
Iter: 1018 loss: 2.97119209e-06
Iter: 1019 loss: 2.97059296e-06
Iter: 1020 loss: 2.96844746e-06
Iter: 1021 loss: 2.96279836e-06
Iter: 1022 loss: 2.97208817e-06
Iter: 1023 loss: 2.96016651e-06
Iter: 1024 loss: 2.95459358e-06
Iter: 1025 loss: 2.96588064e-06
Iter: 1026 loss: 2.95241261e-06
Iter: 1027 loss: 2.94760639e-06
Iter: 1028 loss: 2.98153145e-06
Iter: 1029 loss: 2.94727124e-06
Iter: 1030 loss: 2.94298457e-06
Iter: 1031 loss: 2.93883204e-06
Iter: 1032 loss: 2.93785934e-06
Iter: 1033 loss: 2.93175253e-06
Iter: 1034 loss: 2.94566826e-06
Iter: 1035 loss: 2.9294597e-06
Iter: 1036 loss: 2.9243929e-06
Iter: 1037 loss: 2.99264548e-06
Iter: 1038 loss: 2.92437107e-06
Iter: 1039 loss: 2.91986e-06
Iter: 1040 loss: 2.9222756e-06
Iter: 1041 loss: 2.91687161e-06
Iter: 1042 loss: 2.91221e-06
Iter: 1043 loss: 2.91278047e-06
Iter: 1044 loss: 2.90851358e-06
Iter: 1045 loss: 2.90158596e-06
Iter: 1046 loss: 2.91856895e-06
Iter: 1047 loss: 2.89920877e-06
Iter: 1048 loss: 2.89315176e-06
Iter: 1049 loss: 2.92309232e-06
Iter: 1050 loss: 2.89211266e-06
Iter: 1051 loss: 2.8876168e-06
Iter: 1052 loss: 2.94636288e-06
Iter: 1053 loss: 2.88759452e-06
Iter: 1054 loss: 2.8836e-06
Iter: 1055 loss: 2.88598699e-06
Iter: 1056 loss: 2.8810241e-06
Iter: 1057 loss: 2.87654257e-06
Iter: 1058 loss: 2.88956494e-06
Iter: 1059 loss: 2.87522312e-06
Iter: 1060 loss: 2.8701329e-06
Iter: 1061 loss: 2.87263538e-06
Iter: 1062 loss: 2.86676323e-06
Iter: 1063 loss: 2.86246018e-06
Iter: 1064 loss: 2.90450089e-06
Iter: 1065 loss: 2.8622203e-06
Iter: 1066 loss: 2.85816441e-06
Iter: 1067 loss: 2.85392275e-06
Iter: 1068 loss: 2.85317401e-06
Iter: 1069 loss: 2.84783664e-06
Iter: 1070 loss: 2.87522835e-06
Iter: 1071 loss: 2.84692987e-06
Iter: 1072 loss: 2.84245243e-06
Iter: 1073 loss: 2.86459181e-06
Iter: 1074 loss: 2.84161183e-06
Iter: 1075 loss: 2.83702684e-06
Iter: 1076 loss: 2.84392104e-06
Iter: 1077 loss: 2.83473355e-06
Iter: 1078 loss: 2.82987276e-06
Iter: 1079 loss: 2.83111285e-06
Iter: 1080 loss: 2.82637848e-06
Iter: 1081 loss: 2.82123347e-06
Iter: 1082 loss: 2.85713577e-06
Iter: 1083 loss: 2.82076076e-06
Iter: 1084 loss: 2.81474559e-06
Iter: 1085 loss: 2.82275e-06
Iter: 1086 loss: 2.81174925e-06
Iter: 1087 loss: 2.80676227e-06
Iter: 1088 loss: 2.81388589e-06
Iter: 1089 loss: 2.80428594e-06
Iter: 1090 loss: 2.79973347e-06
Iter: 1091 loss: 2.84894236e-06
Iter: 1092 loss: 2.79956976e-06
Iter: 1093 loss: 2.79511e-06
Iter: 1094 loss: 2.8014947e-06
Iter: 1095 loss: 2.7929e-06
Iter: 1096 loss: 2.78917878e-06
Iter: 1097 loss: 2.78783727e-06
Iter: 1098 loss: 2.78578955e-06
Iter: 1099 loss: 2.78089556e-06
Iter: 1100 loss: 2.81127359e-06
Iter: 1101 loss: 2.78034804e-06
Iter: 1102 loss: 2.77553818e-06
Iter: 1103 loss: 2.78757807e-06
Iter: 1104 loss: 2.77383492e-06
Iter: 1105 loss: 2.76984474e-06
Iter: 1106 loss: 2.78182551e-06
Iter: 1107 loss: 2.76864e-06
Iter: 1108 loss: 2.7639112e-06
Iter: 1109 loss: 2.77178924e-06
Iter: 1110 loss: 2.76181868e-06
Iter: 1111 loss: 2.75743105e-06
Iter: 1112 loss: 2.7611693e-06
Iter: 1113 loss: 2.75481807e-06
Iter: 1114 loss: 2.74928607e-06
Iter: 1115 loss: 2.7564563e-06
Iter: 1116 loss: 2.74651575e-06
Iter: 1117 loss: 2.74130753e-06
Iter: 1118 loss: 2.78540779e-06
Iter: 1119 loss: 2.74106833e-06
Iter: 1120 loss: 2.73622322e-06
Iter: 1121 loss: 2.75322373e-06
Iter: 1122 loss: 2.7349281e-06
Iter: 1123 loss: 2.73060459e-06
Iter: 1124 loss: 2.72809939e-06
Iter: 1125 loss: 2.72631451e-06
Iter: 1126 loss: 2.72182615e-06
Iter: 1127 loss: 2.72186026e-06
Iter: 1128 loss: 2.7175e-06
Iter: 1129 loss: 2.72485977e-06
Iter: 1130 loss: 2.71565727e-06
Iter: 1131 loss: 2.71253043e-06
Iter: 1132 loss: 2.70850524e-06
Iter: 1133 loss: 2.70821693e-06
Iter: 1134 loss: 2.70225496e-06
Iter: 1135 loss: 2.73717706e-06
Iter: 1136 loss: 2.70142732e-06
Iter: 1137 loss: 2.69628e-06
Iter: 1138 loss: 2.73119963e-06
Iter: 1139 loss: 2.6958196e-06
Iter: 1140 loss: 2.69242037e-06
Iter: 1141 loss: 2.69205384e-06
Iter: 1142 loss: 2.6896214e-06
Iter: 1143 loss: 2.68381655e-06
Iter: 1144 loss: 2.70739201e-06
Iter: 1145 loss: 2.68257054e-06
Iter: 1146 loss: 2.67826499e-06
Iter: 1147 loss: 2.67922223e-06
Iter: 1148 loss: 2.6749874e-06
Iter: 1149 loss: 2.66942266e-06
Iter: 1150 loss: 2.69971679e-06
Iter: 1151 loss: 2.6685384e-06
Iter: 1152 loss: 2.66358393e-06
Iter: 1153 loss: 2.68545273e-06
Iter: 1154 loss: 2.66268694e-06
Iter: 1155 loss: 2.65825929e-06
Iter: 1156 loss: 2.65856966e-06
Iter: 1157 loss: 2.65481776e-06
Iter: 1158 loss: 2.65038125e-06
Iter: 1159 loss: 2.65836252e-06
Iter: 1160 loss: 2.64836376e-06
Iter: 1161 loss: 2.6441262e-06
Iter: 1162 loss: 2.64412279e-06
Iter: 1163 loss: 2.64060145e-06
Iter: 1164 loss: 2.63862694e-06
Iter: 1165 loss: 2.63712445e-06
Iter: 1166 loss: 2.63240281e-06
Iter: 1167 loss: 2.67031851e-06
Iter: 1168 loss: 2.6320181e-06
Iter: 1169 loss: 2.62852564e-06
Iter: 1170 loss: 2.62973163e-06
Iter: 1171 loss: 2.62614822e-06
Iter: 1172 loss: 2.62232197e-06
Iter: 1173 loss: 2.61990363e-06
Iter: 1174 loss: 2.61841478e-06
Iter: 1175 loss: 2.6122259e-06
Iter: 1176 loss: 2.64715e-06
Iter: 1177 loss: 2.61137484e-06
Iter: 1178 loss: 2.60695379e-06
Iter: 1179 loss: 2.66422785e-06
Iter: 1180 loss: 2.6069165e-06
Iter: 1181 loss: 2.6039088e-06
Iter: 1182 loss: 2.60039224e-06
Iter: 1183 loss: 2.59998205e-06
Iter: 1184 loss: 2.59497529e-06
Iter: 1185 loss: 2.63951301e-06
Iter: 1186 loss: 2.59466469e-06
Iter: 1187 loss: 2.59088574e-06
Iter: 1188 loss: 2.5915831e-06
Iter: 1189 loss: 2.58803e-06
Iter: 1190 loss: 2.58286e-06
Iter: 1191 loss: 2.58683167e-06
Iter: 1192 loss: 2.57968031e-06
Iter: 1193 loss: 2.57439933e-06
Iter: 1194 loss: 2.60573461e-06
Iter: 1195 loss: 2.57387887e-06
Iter: 1196 loss: 2.56983913e-06
Iter: 1197 loss: 2.61954824e-06
Iter: 1198 loss: 2.56979774e-06
Iter: 1199 loss: 2.56649e-06
Iter: 1200 loss: 2.56679846e-06
Iter: 1201 loss: 2.56397811e-06
Iter: 1202 loss: 2.56053158e-06
Iter: 1203 loss: 2.58150794e-06
Iter: 1204 loss: 2.56007093e-06
Iter: 1205 loss: 2.5565314e-06
Iter: 1206 loss: 2.55282112e-06
Iter: 1207 loss: 2.5522022e-06
Iter: 1208 loss: 2.54722045e-06
Iter: 1209 loss: 2.55790837e-06
Iter: 1210 loss: 2.54525412e-06
Iter: 1211 loss: 2.54028e-06
Iter: 1212 loss: 2.55645091e-06
Iter: 1213 loss: 2.53883354e-06
Iter: 1214 loss: 2.53372968e-06
Iter: 1215 loss: 2.57844431e-06
Iter: 1216 loss: 2.53346525e-06
Iter: 1217 loss: 2.52976565e-06
Iter: 1218 loss: 2.53038024e-06
Iter: 1219 loss: 2.52701057e-06
Iter: 1220 loss: 2.52262862e-06
Iter: 1221 loss: 2.54115798e-06
Iter: 1222 loss: 2.52169093e-06
Iter: 1223 loss: 2.51721508e-06
Iter: 1224 loss: 2.52675227e-06
Iter: 1225 loss: 2.5154759e-06
Iter: 1226 loss: 2.51147731e-06
Iter: 1227 loss: 2.51867277e-06
Iter: 1228 loss: 2.50968515e-06
Iter: 1229 loss: 2.50567473e-06
Iter: 1230 loss: 2.52944051e-06
Iter: 1231 loss: 2.50513972e-06
Iter: 1232 loss: 2.50094763e-06
Iter: 1233 loss: 2.50528046e-06
Iter: 1234 loss: 2.49867026e-06
Iter: 1235 loss: 2.49506456e-06
Iter: 1236 loss: 2.52538939e-06
Iter: 1237 loss: 2.49494224e-06
Iter: 1238 loss: 2.49106461e-06
Iter: 1239 loss: 2.49302684e-06
Iter: 1240 loss: 2.48858623e-06
Iter: 1241 loss: 2.48476044e-06
Iter: 1242 loss: 2.48057904e-06
Iter: 1243 loss: 2.47995695e-06
Iter: 1244 loss: 2.47471621e-06
Iter: 1245 loss: 2.53923872e-06
Iter: 1246 loss: 2.47460503e-06
Iter: 1247 loss: 2.47103117e-06
Iter: 1248 loss: 2.49328696e-06
Iter: 1249 loss: 2.47057801e-06
Iter: 1250 loss: 2.4673077e-06
Iter: 1251 loss: 2.46289687e-06
Iter: 1252 loss: 2.46267109e-06
Iter: 1253 loss: 2.45810043e-06
Iter: 1254 loss: 2.48012134e-06
Iter: 1255 loss: 2.45721185e-06
Iter: 1256 loss: 2.45292631e-06
Iter: 1257 loss: 2.48505853e-06
Iter: 1258 loss: 2.45258047e-06
Iter: 1259 loss: 2.44873263e-06
Iter: 1260 loss: 2.45398701e-06
Iter: 1261 loss: 2.44667922e-06
Iter: 1262 loss: 2.44303578e-06
Iter: 1263 loss: 2.44507555e-06
Iter: 1264 loss: 2.4405922e-06
Iter: 1265 loss: 2.43587374e-06
Iter: 1266 loss: 2.46787249e-06
Iter: 1267 loss: 2.43548789e-06
Iter: 1268 loss: 2.43174281e-06
Iter: 1269 loss: 2.43605473e-06
Iter: 1270 loss: 2.42976967e-06
Iter: 1271 loss: 2.42628221e-06
Iter: 1272 loss: 2.46374043e-06
Iter: 1273 loss: 2.42622446e-06
Iter: 1274 loss: 2.42283863e-06
Iter: 1275 loss: 2.42345459e-06
Iter: 1276 loss: 2.42028909e-06
Iter: 1277 loss: 2.41733983e-06
Iter: 1278 loss: 2.41674161e-06
Iter: 1279 loss: 2.41467797e-06
Iter: 1280 loss: 2.41017074e-06
Iter: 1281 loss: 2.43104296e-06
Iter: 1282 loss: 2.40929262e-06
Iter: 1283 loss: 2.40516533e-06
Iter: 1284 loss: 2.43541035e-06
Iter: 1285 loss: 2.40489521e-06
Iter: 1286 loss: 2.4018625e-06
Iter: 1287 loss: 2.39716428e-06
Iter: 1288 loss: 2.39707902e-06
Iter: 1289 loss: 2.392223e-06
Iter: 1290 loss: 2.4410615e-06
Iter: 1291 loss: 2.39204019e-06
Iter: 1292 loss: 2.38756093e-06
Iter: 1293 loss: 2.40446639e-06
Iter: 1294 loss: 2.3865075e-06
Iter: 1295 loss: 2.38309599e-06
Iter: 1296 loss: 2.38291136e-06
Iter: 1297 loss: 2.38022176e-06
Iter: 1298 loss: 2.37582344e-06
Iter: 1299 loss: 2.4050396e-06
Iter: 1300 loss: 2.37533595e-06
Iter: 1301 loss: 2.37133509e-06
Iter: 1302 loss: 2.38642588e-06
Iter: 1303 loss: 2.37041786e-06
Iter: 1304 loss: 2.36714368e-06
Iter: 1305 loss: 2.36807159e-06
Iter: 1306 loss: 2.36480469e-06
Iter: 1307 loss: 2.36168125e-06
Iter: 1308 loss: 2.40118652e-06
Iter: 1309 loss: 2.36164942e-06
Iter: 1310 loss: 2.35872949e-06
Iter: 1311 loss: 2.36032861e-06
Iter: 1312 loss: 2.35671246e-06
Iter: 1313 loss: 2.35311609e-06
Iter: 1314 loss: 2.36393907e-06
Iter: 1315 loss: 2.35207153e-06
Iter: 1316 loss: 2.34836261e-06
Iter: 1317 loss: 2.34615436e-06
Iter: 1318 loss: 2.3445607e-06
Iter: 1319 loss: 2.34010486e-06
Iter: 1320 loss: 2.35653988e-06
Iter: 1321 loss: 2.33905712e-06
Iter: 1322 loss: 2.33466653e-06
Iter: 1323 loss: 2.33554078e-06
Iter: 1324 loss: 2.33138235e-06
Iter: 1325 loss: 2.32673437e-06
Iter: 1326 loss: 2.39544806e-06
Iter: 1327 loss: 2.32670936e-06
Iter: 1328 loss: 2.32326465e-06
Iter: 1329 loss: 2.34081017e-06
Iter: 1330 loss: 2.32271327e-06
Iter: 1331 loss: 2.31979288e-06
Iter: 1332 loss: 2.31479589e-06
Iter: 1333 loss: 2.31470949e-06
Iter: 1334 loss: 2.31101399e-06
Iter: 1335 loss: 2.31090849e-06
Iter: 1336 loss: 2.30762862e-06
Iter: 1337 loss: 2.30812111e-06
Iter: 1338 loss: 2.30511569e-06
Iter: 1339 loss: 2.30116166e-06
Iter: 1340 loss: 2.31152944e-06
Iter: 1341 loss: 2.29976695e-06
Iter: 1342 loss: 2.29615853e-06
Iter: 1343 loss: 2.33156152e-06
Iter: 1344 loss: 2.29603347e-06
Iter: 1345 loss: 2.29306647e-06
Iter: 1346 loss: 2.29577063e-06
Iter: 1347 loss: 2.29134639e-06
Iter: 1348 loss: 2.28837e-06
Iter: 1349 loss: 2.29997659e-06
Iter: 1350 loss: 2.28762701e-06
Iter: 1351 loss: 2.28436784e-06
Iter: 1352 loss: 2.2813515e-06
Iter: 1353 loss: 2.28055319e-06
Iter: 1354 loss: 2.27654459e-06
Iter: 1355 loss: 2.32575417e-06
Iter: 1356 loss: 2.27649707e-06
Iter: 1357 loss: 2.27352643e-06
Iter: 1358 loss: 2.27597866e-06
Iter: 1359 loss: 2.27185888e-06
Iter: 1360 loss: 2.26824977e-06
Iter: 1361 loss: 2.26976067e-06
Iter: 1362 loss: 2.26583961e-06
Iter: 1363 loss: 2.26134421e-06
Iter: 1364 loss: 2.2636259e-06
Iter: 1365 loss: 2.25836447e-06
Iter: 1366 loss: 2.25421263e-06
Iter: 1367 loss: 2.31123977e-06
Iter: 1368 loss: 2.25418762e-06
Iter: 1369 loss: 2.25035683e-06
Iter: 1370 loss: 2.26528618e-06
Iter: 1371 loss: 2.24953146e-06
Iter: 1372 loss: 2.24598307e-06
Iter: 1373 loss: 2.24465884e-06
Iter: 1374 loss: 2.24275027e-06
Iter: 1375 loss: 2.23894813e-06
Iter: 1376 loss: 2.26176053e-06
Iter: 1377 loss: 2.23848451e-06
Iter: 1378 loss: 2.23504185e-06
Iter: 1379 loss: 2.26327415e-06
Iter: 1380 loss: 2.23483721e-06
Iter: 1381 loss: 2.23238817e-06
Iter: 1382 loss: 2.23322263e-06
Iter: 1383 loss: 2.23069901e-06
Iter: 1384 loss: 2.22729523e-06
Iter: 1385 loss: 2.2400186e-06
Iter: 1386 loss: 2.22657354e-06
Iter: 1387 loss: 2.22345193e-06
Iter: 1388 loss: 2.22280551e-06
Iter: 1389 loss: 2.2207405e-06
Iter: 1390 loss: 2.21721757e-06
Iter: 1391 loss: 2.23703228e-06
Iter: 1392 loss: 2.2166987e-06
Iter: 1393 loss: 2.21315349e-06
Iter: 1394 loss: 2.22083168e-06
Iter: 1395 loss: 2.21178971e-06
Iter: 1396 loss: 2.20858647e-06
Iter: 1397 loss: 2.21439677e-06
Iter: 1398 loss: 2.20728543e-06
Iter: 1399 loss: 2.2040465e-06
Iter: 1400 loss: 2.22352355e-06
Iter: 1401 loss: 2.20361835e-06
Iter: 1402 loss: 2.20049219e-06
Iter: 1403 loss: 2.20058701e-06
Iter: 1404 loss: 2.19798062e-06
Iter: 1405 loss: 2.19409867e-06
Iter: 1406 loss: 2.19874664e-06
Iter: 1407 loss: 2.19200138e-06
Iter: 1408 loss: 2.18766809e-06
Iter: 1409 loss: 2.20109541e-06
Iter: 1410 loss: 2.18633681e-06
Iter: 1411 loss: 2.18341165e-06
Iter: 1412 loss: 2.18339096e-06
Iter: 1413 loss: 2.18058494e-06
Iter: 1414 loss: 2.18050127e-06
Iter: 1415 loss: 2.1783635e-06
Iter: 1416 loss: 2.17544948e-06
Iter: 1417 loss: 2.20002494e-06
Iter: 1418 loss: 2.17528918e-06
Iter: 1419 loss: 2.17247452e-06
Iter: 1420 loss: 2.17538332e-06
Iter: 1421 loss: 2.17090837e-06
Iter: 1422 loss: 2.16832564e-06
Iter: 1423 loss: 2.16619128e-06
Iter: 1424 loss: 2.16536387e-06
Iter: 1425 loss: 2.16164926e-06
Iter: 1426 loss: 2.18383911e-06
Iter: 1427 loss: 2.16110266e-06
Iter: 1428 loss: 2.15701402e-06
Iter: 1429 loss: 2.17136812e-06
Iter: 1430 loss: 2.15600471e-06
Iter: 1431 loss: 2.15276486e-06
Iter: 1432 loss: 2.15866908e-06
Iter: 1433 loss: 2.15138743e-06
Iter: 1434 loss: 2.14817942e-06
Iter: 1435 loss: 2.15963018e-06
Iter: 1436 loss: 2.14739907e-06
Iter: 1437 loss: 2.14398028e-06
Iter: 1438 loss: 2.15030718e-06
Iter: 1439 loss: 2.14261331e-06
Iter: 1440 loss: 2.13938188e-06
Iter: 1441 loss: 2.13984458e-06
Iter: 1442 loss: 2.13691419e-06
Iter: 1443 loss: 2.1329688e-06
Iter: 1444 loss: 2.14268812e-06
Iter: 1445 loss: 2.13155863e-06
Iter: 1446 loss: 2.12815689e-06
Iter: 1447 loss: 2.12814462e-06
Iter: 1448 loss: 2.12556324e-06
Iter: 1449 loss: 2.12470786e-06
Iter: 1450 loss: 2.12317127e-06
Iter: 1451 loss: 2.11966653e-06
Iter: 1452 loss: 2.15256932e-06
Iter: 1453 loss: 2.11951487e-06
Iter: 1454 loss: 2.11669271e-06
Iter: 1455 loss: 2.11966722e-06
Iter: 1456 loss: 2.11510951e-06
Iter: 1457 loss: 2.11251472e-06
Iter: 1458 loss: 2.11498923e-06
Iter: 1459 loss: 2.1111382e-06
Iter: 1460 loss: 2.10766666e-06
Iter: 1461 loss: 2.1249964e-06
Iter: 1462 loss: 2.10709368e-06
Iter: 1463 loss: 2.10413691e-06
Iter: 1464 loss: 2.10224221e-06
Iter: 1465 loss: 2.10109965e-06
Iter: 1466 loss: 2.09718519e-06
Iter: 1467 loss: 2.10286862e-06
Iter: 1468 loss: 2.09540531e-06
Iter: 1469 loss: 2.09172595e-06
Iter: 1470 loss: 2.14002694e-06
Iter: 1471 loss: 2.09170412e-06
Iter: 1472 loss: 2.08883853e-06
Iter: 1473 loss: 2.0951652e-06
Iter: 1474 loss: 2.08774463e-06
Iter: 1475 loss: 2.08500865e-06
Iter: 1476 loss: 2.08659276e-06
Iter: 1477 loss: 2.08312258e-06
Iter: 1478 loss: 2.07974495e-06
Iter: 1479 loss: 2.1047581e-06
Iter: 1480 loss: 2.07940661e-06
Iter: 1481 loss: 2.0767518e-06
Iter: 1482 loss: 2.07444327e-06
Iter: 1483 loss: 2.07371295e-06
Iter: 1484 loss: 2.06920527e-06
Iter: 1485 loss: 2.07917674e-06
Iter: 1486 loss: 2.06745358e-06
Iter: 1487 loss: 2.06608593e-06
Iter: 1488 loss: 2.06525692e-06
Iter: 1489 loss: 2.06353593e-06
Iter: 1490 loss: 2.06111372e-06
Iter: 1491 loss: 2.06098321e-06
Iter: 1492 loss: 2.05784e-06
Iter: 1493 loss: 2.06244931e-06
Iter: 1494 loss: 2.05634024e-06
Iter: 1495 loss: 2.0528455e-06
Iter: 1496 loss: 2.08733354e-06
Iter: 1497 loss: 2.05278025e-06
Iter: 1498 loss: 2.05043352e-06
Iter: 1499 loss: 2.04759385e-06
Iter: 1500 loss: 2.04720527e-06
Iter: 1501 loss: 2.04334901e-06
Iter: 1502 loss: 2.05434367e-06
Iter: 1503 loss: 2.04200683e-06
Iter: 1504 loss: 2.03851891e-06
Iter: 1505 loss: 2.05761671e-06
Iter: 1506 loss: 2.0379166e-06
Iter: 1507 loss: 2.0345683e-06
Iter: 1508 loss: 2.05192964e-06
Iter: 1509 loss: 2.03398895e-06
Iter: 1510 loss: 2.03132322e-06
Iter: 1511 loss: 2.03092054e-06
Iter: 1512 loss: 2.02907563e-06
Iter: 1513 loss: 2.02549654e-06
Iter: 1514 loss: 2.04720345e-06
Iter: 1515 loss: 2.02506635e-06
Iter: 1516 loss: 2.02189449e-06
Iter: 1517 loss: 2.02963543e-06
Iter: 1518 loss: 2.02074034e-06
Iter: 1519 loss: 2.01776083e-06
Iter: 1520 loss: 2.01791954e-06
Iter: 1521 loss: 2.01532521e-06
Iter: 1522 loss: 2.01226476e-06
Iter: 1523 loss: 2.01226794e-06
Iter: 1524 loss: 2.01045532e-06
Iter: 1525 loss: 2.01049806e-06
Iter: 1526 loss: 2.00908084e-06
Iter: 1527 loss: 2.00573777e-06
Iter: 1528 loss: 2.01250532e-06
Iter: 1529 loss: 2.0043592e-06
Iter: 1530 loss: 2.00174736e-06
Iter: 1531 loss: 2.00073509e-06
Iter: 1532 loss: 1.99933106e-06
Iter: 1533 loss: 1.99602164e-06
Iter: 1534 loss: 2.01036437e-06
Iter: 1535 loss: 1.99536635e-06
Iter: 1536 loss: 1.99230476e-06
Iter: 1537 loss: 2.01535249e-06
Iter: 1538 loss: 1.99204669e-06
Iter: 1539 loss: 1.98919406e-06
Iter: 1540 loss: 1.98832049e-06
Iter: 1541 loss: 1.98661701e-06
Iter: 1542 loss: 1.9833451e-06
Iter: 1543 loss: 1.98912221e-06
Iter: 1544 loss: 1.98191265e-06
Iter: 1545 loss: 1.97811187e-06
Iter: 1546 loss: 1.98342468e-06
Iter: 1547 loss: 1.97620739e-06
Iter: 1548 loss: 1.97270356e-06
Iter: 1549 loss: 2.00969043e-06
Iter: 1550 loss: 1.97261443e-06
Iter: 1551 loss: 1.96964584e-06
Iter: 1552 loss: 1.97620238e-06
Iter: 1553 loss: 1.96849214e-06
Iter: 1554 loss: 1.965602e-06
Iter: 1555 loss: 1.97145141e-06
Iter: 1556 loss: 1.96453448e-06
Iter: 1557 loss: 1.96173232e-06
Iter: 1558 loss: 1.98837188e-06
Iter: 1559 loss: 1.96168435e-06
Iter: 1560 loss: 1.95984512e-06
Iter: 1561 loss: 1.961816e-06
Iter: 1562 loss: 1.95883149e-06
Iter: 1563 loss: 1.95615712e-06
Iter: 1564 loss: 1.96180326e-06
Iter: 1565 loss: 1.95508892e-06
Iter: 1566 loss: 1.95225152e-06
Iter: 1567 loss: 1.95079565e-06
Iter: 1568 loss: 1.94948598e-06
Iter: 1569 loss: 1.94619679e-06
Iter: 1570 loss: 1.96096403e-06
Iter: 1571 loss: 1.94554923e-06
Iter: 1572 loss: 1.94296081e-06
Iter: 1573 loss: 1.97055647e-06
Iter: 1574 loss: 1.94287691e-06
Iter: 1575 loss: 1.94094173e-06
Iter: 1576 loss: 1.93865526e-06
Iter: 1577 loss: 1.93834194e-06
Iter: 1578 loss: 1.93522351e-06
Iter: 1579 loss: 1.9538677e-06
Iter: 1580 loss: 1.93489063e-06
Iter: 1581 loss: 1.93217329e-06
Iter: 1582 loss: 1.94965014e-06
Iter: 1583 loss: 1.93192659e-06
Iter: 1584 loss: 1.92969128e-06
Iter: 1585 loss: 1.92642187e-06
Iter: 1586 loss: 1.92633593e-06
Iter: 1587 loss: 1.92244624e-06
Iter: 1588 loss: 1.93850474e-06
Iter: 1589 loss: 1.92163816e-06
Iter: 1590 loss: 1.91804247e-06
Iter: 1591 loss: 1.92555e-06
Iter: 1592 loss: 1.91665049e-06
Iter: 1593 loss: 1.91407e-06
Iter: 1594 loss: 1.9140623e-06
Iter: 1595 loss: 1.91200888e-06
Iter: 1596 loss: 1.91404229e-06
Iter: 1597 loss: 1.91085428e-06
Iter: 1598 loss: 1.90801074e-06
Iter: 1599 loss: 1.91886465e-06
Iter: 1600 loss: 1.90742048e-06
Iter: 1601 loss: 1.90553146e-06
Iter: 1602 loss: 1.90412652e-06
Iter: 1603 loss: 1.90350397e-06
Iter: 1604 loss: 1.90033575e-06
Iter: 1605 loss: 1.90611263e-06
Iter: 1606 loss: 1.89886543e-06
Iter: 1607 loss: 1.89572575e-06
Iter: 1608 loss: 1.9343754e-06
Iter: 1609 loss: 1.89569118e-06
Iter: 1610 loss: 1.89338061e-06
Iter: 1611 loss: 1.89263687e-06
Iter: 1612 loss: 1.89136506e-06
Iter: 1613 loss: 1.88837691e-06
Iter: 1614 loss: 1.897326e-06
Iter: 1615 loss: 1.88748311e-06
Iter: 1616 loss: 1.88501849e-06
Iter: 1617 loss: 1.91214713e-06
Iter: 1618 loss: 1.88497916e-06
Iter: 1619 loss: 1.88291119e-06
Iter: 1620 loss: 1.8804343e-06
Iter: 1621 loss: 1.88022409e-06
Iter: 1622 loss: 1.87727665e-06
Iter: 1623 loss: 1.8952187e-06
Iter: 1624 loss: 1.87694445e-06
Iter: 1625 loss: 1.87437593e-06
Iter: 1626 loss: 1.88713864e-06
Iter: 1627 loss: 1.87396154e-06
Iter: 1628 loss: 1.87110231e-06
Iter: 1629 loss: 1.8685065e-06
Iter: 1630 loss: 1.86783927e-06
Iter: 1631 loss: 1.86486068e-06
Iter: 1632 loss: 1.88262948e-06
Iter: 1633 loss: 1.86445345e-06
Iter: 1634 loss: 1.86173202e-06
Iter: 1635 loss: 1.88882382e-06
Iter: 1636 loss: 1.86159741e-06
Iter: 1637 loss: 1.8598065e-06
Iter: 1638 loss: 1.85855174e-06
Iter: 1639 loss: 1.85794534e-06
Iter: 1640 loss: 1.85482099e-06
Iter: 1641 loss: 1.86437956e-06
Iter: 1642 loss: 1.85387444e-06
Iter: 1643 loss: 1.85130693e-06
Iter: 1644 loss: 1.85481326e-06
Iter: 1645 loss: 1.85005342e-06
Iter: 1646 loss: 1.84697933e-06
Iter: 1647 loss: 1.84854093e-06
Iter: 1648 loss: 1.84487806e-06
Iter: 1649 loss: 1.84162434e-06
Iter: 1650 loss: 1.85932186e-06
Iter: 1651 loss: 1.84111786e-06
Iter: 1652 loss: 1.83844452e-06
Iter: 1653 loss: 1.86709826e-06
Iter: 1654 loss: 1.83839279e-06
Iter: 1655 loss: 1.83635302e-06
Iter: 1656 loss: 1.83595876e-06
Iter: 1657 loss: 1.83463044e-06
Iter: 1658 loss: 1.83207771e-06
Iter: 1659 loss: 1.83897635e-06
Iter: 1660 loss: 1.831183e-06
Iter: 1661 loss: 1.82828353e-06
Iter: 1662 loss: 1.8459067e-06
Iter: 1663 loss: 1.82797544e-06
Iter: 1664 loss: 1.82584336e-06
Iter: 1665 loss: 1.82601548e-06
Iter: 1666 loss: 1.82423332e-06
Iter: 1667 loss: 1.82155281e-06
Iter: 1668 loss: 1.82525764e-06
Iter: 1669 loss: 1.82025656e-06
Iter: 1670 loss: 1.81769371e-06
Iter: 1671 loss: 1.81771065e-06
Iter: 1672 loss: 1.8161071e-06
Iter: 1673 loss: 1.81377277e-06
Iter: 1674 loss: 1.81372729e-06
Iter: 1675 loss: 1.81114751e-06
Iter: 1676 loss: 1.83099746e-06
Iter: 1677 loss: 1.81098881e-06
Iter: 1678 loss: 1.8087602e-06
Iter: 1679 loss: 1.80854977e-06
Iter: 1680 loss: 1.80685049e-06
Iter: 1681 loss: 1.80403958e-06
Iter: 1682 loss: 1.81075654e-06
Iter: 1683 loss: 1.80307177e-06
Iter: 1684 loss: 1.80053132e-06
Iter: 1685 loss: 1.82081703e-06
Iter: 1686 loss: 1.80036807e-06
Iter: 1687 loss: 1.79781159e-06
Iter: 1688 loss: 1.80042298e-06
Iter: 1689 loss: 1.79640847e-06
Iter: 1690 loss: 1.79395261e-06
Iter: 1691 loss: 1.79646202e-06
Iter: 1692 loss: 1.7924782e-06
Iter: 1693 loss: 1.78953303e-06
Iter: 1694 loss: 1.79633798e-06
Iter: 1695 loss: 1.78840185e-06
Iter: 1696 loss: 1.78579637e-06
Iter: 1697 loss: 1.82003373e-06
Iter: 1698 loss: 1.78570986e-06
Iter: 1699 loss: 1.78376604e-06
Iter: 1700 loss: 1.78259438e-06
Iter: 1701 loss: 1.78178823e-06
Iter: 1702 loss: 1.77921788e-06
Iter: 1703 loss: 1.79217409e-06
Iter: 1704 loss: 1.7788285e-06
Iter: 1705 loss: 1.77636457e-06
Iter: 1706 loss: 1.79456e-06
Iter: 1707 loss: 1.77613947e-06
Iter: 1708 loss: 1.77455377e-06
Iter: 1709 loss: 1.77553602e-06
Iter: 1710 loss: 1.77347943e-06
Iter: 1711 loss: 1.771249e-06
Iter: 1712 loss: 1.7792587e-06
Iter: 1713 loss: 1.77062338e-06
Iter: 1714 loss: 1.76864751e-06
Iter: 1715 loss: 1.76686399e-06
Iter: 1716 loss: 1.76641311e-06
Iter: 1717 loss: 1.7638281e-06
Iter: 1718 loss: 1.78529035e-06
Iter: 1719 loss: 1.76362516e-06
Iter: 1720 loss: 1.76115623e-06
Iter: 1721 loss: 1.76432343e-06
Iter: 1722 loss: 1.75990385e-06
Iter: 1723 loss: 1.75738774e-06
Iter: 1724 loss: 1.76158801e-06
Iter: 1725 loss: 1.75622688e-06
Iter: 1726 loss: 1.753383e-06
Iter: 1727 loss: 1.7548341e-06
Iter: 1728 loss: 1.75150626e-06
Iter: 1729 loss: 1.74876413e-06
Iter: 1730 loss: 1.74877869e-06
Iter: 1731 loss: 1.74631361e-06
Iter: 1732 loss: 1.74811294e-06
Iter: 1733 loss: 1.74479817e-06
Iter: 1734 loss: 1.74229467e-06
Iter: 1735 loss: 1.74521415e-06
Iter: 1736 loss: 1.74093088e-06
Iter: 1737 loss: 1.73836429e-06
Iter: 1738 loss: 1.75156538e-06
Iter: 1739 loss: 1.73801789e-06
Iter: 1740 loss: 1.73558794e-06
Iter: 1741 loss: 1.74952834e-06
Iter: 1742 loss: 1.73523063e-06
Iter: 1743 loss: 1.73357125e-06
Iter: 1744 loss: 1.7356748e-06
Iter: 1745 loss: 1.73264266e-06
Iter: 1746 loss: 1.73027433e-06
Iter: 1747 loss: 1.74149056e-06
Iter: 1748 loss: 1.72982527e-06
Iter: 1749 loss: 1.7282448e-06
Iter: 1750 loss: 1.72671105e-06
Iter: 1751 loss: 1.72644059e-06
Iter: 1752 loss: 1.72365276e-06
Iter: 1753 loss: 1.73445108e-06
Iter: 1754 loss: 1.72305806e-06
Iter: 1755 loss: 1.72066018e-06
Iter: 1756 loss: 1.74048353e-06
Iter: 1757 loss: 1.72055184e-06
Iter: 1758 loss: 1.71891941e-06
Iter: 1759 loss: 1.71762008e-06
Iter: 1760 loss: 1.71709951e-06
Iter: 1761 loss: 1.7146549e-06
Iter: 1762 loss: 1.73572221e-06
Iter: 1763 loss: 1.71450688e-06
Iter: 1764 loss: 1.71250349e-06
Iter: 1765 loss: 1.71131762e-06
Iter: 1766 loss: 1.71049237e-06
Iter: 1767 loss: 1.70761768e-06
Iter: 1768 loss: 1.71404304e-06
Iter: 1769 loss: 1.70649241e-06
Iter: 1770 loss: 1.7035668e-06
Iter: 1771 loss: 1.71207398e-06
Iter: 1772 loss: 1.70266446e-06
Iter: 1773 loss: 1.69978864e-06
Iter: 1774 loss: 1.70923408e-06
Iter: 1775 loss: 1.69896532e-06
Iter: 1776 loss: 1.69654936e-06
Iter: 1777 loss: 1.72663147e-06
Iter: 1778 loss: 1.69652094e-06
Iter: 1779 loss: 1.6947256e-06
Iter: 1780 loss: 1.69510099e-06
Iter: 1781 loss: 1.69333782e-06
Iter: 1782 loss: 1.69146722e-06
Iter: 1783 loss: 1.71869556e-06
Iter: 1784 loss: 1.6914455e-06
Iter: 1785 loss: 1.69012321e-06
Iter: 1786 loss: 1.68714666e-06
Iter: 1787 loss: 1.73592537e-06
Iter: 1788 loss: 1.68706401e-06
Iter: 1789 loss: 1.68467864e-06
Iter: 1790 loss: 1.70553881e-06
Iter: 1791 loss: 1.68455927e-06
Iter: 1792 loss: 1.68239671e-06
Iter: 1793 loss: 1.6909396e-06
Iter: 1794 loss: 1.68189456e-06
Iter: 1795 loss: 1.67986207e-06
Iter: 1796 loss: 1.67867654e-06
Iter: 1797 loss: 1.67788937e-06
Iter: 1798 loss: 1.67544226e-06
Iter: 1799 loss: 1.69784209e-06
Iter: 1800 loss: 1.67534768e-06
Iter: 1801 loss: 1.67311123e-06
Iter: 1802 loss: 1.67532153e-06
Iter: 1803 loss: 1.67194253e-06
Iter: 1804 loss: 1.6692004e-06
Iter: 1805 loss: 1.67813073e-06
Iter: 1806 loss: 1.66853874e-06
Iter: 1807 loss: 1.66651478e-06
Iter: 1808 loss: 1.68015333e-06
Iter: 1809 loss: 1.66628911e-06
Iter: 1810 loss: 1.66434893e-06
Iter: 1811 loss: 1.66153632e-06
Iter: 1812 loss: 1.66144332e-06
Iter: 1813 loss: 1.65853271e-06
Iter: 1814 loss: 1.67349754e-06
Iter: 1815 loss: 1.65796484e-06
Iter: 1816 loss: 1.65529843e-06
Iter: 1817 loss: 1.65898723e-06
Iter: 1818 loss: 1.65389952e-06
Iter: 1819 loss: 1.65233291e-06
Iter: 1820 loss: 1.65216181e-06
Iter: 1821 loss: 1.65050665e-06
Iter: 1822 loss: 1.64899097e-06
Iter: 1823 loss: 1.64862797e-06
Iter: 1824 loss: 1.6465425e-06
Iter: 1825 loss: 1.66088489e-06
Iter: 1826 loss: 1.64632775e-06
Iter: 1827 loss: 1.64437563e-06
Iter: 1828 loss: 1.64427649e-06
Iter: 1829 loss: 1.64275014e-06
Iter: 1830 loss: 1.64038238e-06
Iter: 1831 loss: 1.64202572e-06
Iter: 1832 loss: 1.63885397e-06
Iter: 1833 loss: 1.63610775e-06
Iter: 1834 loss: 1.6518328e-06
Iter: 1835 loss: 1.63569609e-06
Iter: 1836 loss: 1.63330628e-06
Iter: 1837 loss: 1.6476406e-06
Iter: 1838 loss: 1.63295533e-06
Iter: 1839 loss: 1.63101458e-06
Iter: 1840 loss: 1.63108916e-06
Iter: 1841 loss: 1.62946412e-06
Iter: 1842 loss: 1.62700735e-06
Iter: 1843 loss: 1.64457538e-06
Iter: 1844 loss: 1.62679066e-06
Iter: 1845 loss: 1.62463903e-06
Iter: 1846 loss: 1.62504716e-06
Iter: 1847 loss: 1.62292986e-06
Iter: 1848 loss: 1.62066294e-06
Iter: 1849 loss: 1.62568642e-06
Iter: 1850 loss: 1.61972616e-06
Iter: 1851 loss: 1.61685387e-06
Iter: 1852 loss: 1.61752132e-06
Iter: 1853 loss: 1.61473815e-06
Iter: 1854 loss: 1.61288972e-06
Iter: 1855 loss: 1.61260266e-06
Iter: 1856 loss: 1.61069215e-06
Iter: 1857 loss: 1.6108487e-06
Iter: 1858 loss: 1.60914931e-06
Iter: 1859 loss: 1.6070677e-06
Iter: 1860 loss: 1.61386538e-06
Iter: 1861 loss: 1.60645857e-06
Iter: 1862 loss: 1.60428795e-06
Iter: 1863 loss: 1.61146875e-06
Iter: 1864 loss: 1.60364129e-06
Iter: 1865 loss: 1.60189938e-06
Iter: 1866 loss: 1.60075069e-06
Iter: 1867 loss: 1.60012428e-06
Iter: 1868 loss: 1.59798367e-06
Iter: 1869 loss: 1.62933225e-06
Iter: 1870 loss: 1.59800129e-06
Iter: 1871 loss: 1.59629758e-06
Iter: 1872 loss: 1.5951822e-06
Iter: 1873 loss: 1.5945061e-06
Iter: 1874 loss: 1.59186038e-06
Iter: 1875 loss: 1.59633112e-06
Iter: 1876 loss: 1.59068077e-06
Iter: 1877 loss: 1.58820137e-06
Iter: 1878 loss: 1.60500417e-06
Iter: 1879 loss: 1.5879582e-06
Iter: 1880 loss: 1.58546732e-06
Iter: 1881 loss: 1.58848593e-06
Iter: 1882 loss: 1.58422108e-06
Iter: 1883 loss: 1.58175783e-06
Iter: 1884 loss: 1.59098227e-06
Iter: 1885 loss: 1.58127386e-06
Iter: 1886 loss: 1.5793014e-06
Iter: 1887 loss: 1.59149886e-06
Iter: 1888 loss: 1.57907346e-06
Iter: 1889 loss: 1.57710747e-06
Iter: 1890 loss: 1.57711452e-06
Iter: 1891 loss: 1.57548641e-06
Iter: 1892 loss: 1.5735643e-06
Iter: 1893 loss: 1.57903582e-06
Iter: 1894 loss: 1.57290424e-06
Iter: 1895 loss: 1.57067643e-06
Iter: 1896 loss: 1.59388424e-06
Iter: 1897 loss: 1.57062323e-06
Iter: 1898 loss: 1.56917599e-06
Iter: 1899 loss: 1.56666624e-06
Iter: 1900 loss: 1.56663668e-06
Iter: 1901 loss: 1.56364672e-06
Iter: 1902 loss: 1.57039926e-06
Iter: 1903 loss: 1.56252145e-06
Iter: 1904 loss: 1.56004467e-06
Iter: 1905 loss: 1.58511443e-06
Iter: 1906 loss: 1.55996395e-06
Iter: 1907 loss: 1.55778366e-06
Iter: 1908 loss: 1.56366968e-06
Iter: 1909 loss: 1.55704481e-06
Iter: 1910 loss: 1.55514101e-06
Iter: 1911 loss: 1.55659757e-06
Iter: 1912 loss: 1.55394787e-06
Iter: 1913 loss: 1.55211342e-06
Iter: 1914 loss: 1.56747615e-06
Iter: 1915 loss: 1.5519845e-06
Iter: 1916 loss: 1.55018904e-06
Iter: 1917 loss: 1.55127896e-06
Iter: 1918 loss: 1.54902204e-06
Iter: 1919 loss: 1.54719964e-06
Iter: 1920 loss: 1.54791724e-06
Iter: 1921 loss: 1.54581676e-06
Iter: 1922 loss: 1.54278359e-06
Iter: 1923 loss: 1.55781584e-06
Iter: 1924 loss: 1.54235317e-06
Iter: 1925 loss: 1.54030204e-06
Iter: 1926 loss: 1.54529732e-06
Iter: 1927 loss: 1.53952328e-06
Iter: 1928 loss: 1.53737687e-06
Iter: 1929 loss: 1.53780809e-06
Iter: 1930 loss: 1.5356884e-06
Iter: 1931 loss: 1.5342489e-06
Iter: 1932 loss: 1.53392205e-06
Iter: 1933 loss: 1.53277847e-06
Iter: 1934 loss: 1.53038241e-06
Iter: 1935 loss: 1.56754368e-06
Iter: 1936 loss: 1.530296e-06
Iter: 1937 loss: 1.52823441e-06
Iter: 1938 loss: 1.55182943e-06
Iter: 1939 loss: 1.52818052e-06
Iter: 1940 loss: 1.52618509e-06
Iter: 1941 loss: 1.52853193e-06
Iter: 1942 loss: 1.52524331e-06
Iter: 1943 loss: 1.52341249e-06
Iter: 1944 loss: 1.52335394e-06
Iter: 1945 loss: 1.52186885e-06
Iter: 1946 loss: 1.51928498e-06
Iter: 1947 loss: 1.52857433e-06
Iter: 1948 loss: 1.51863651e-06
Iter: 1949 loss: 1.51619884e-06
Iter: 1950 loss: 1.53040628e-06
Iter: 1951 loss: 1.51586653e-06
Iter: 1952 loss: 1.5132764e-06
Iter: 1953 loss: 1.51618053e-06
Iter: 1954 loss: 1.51190807e-06
Iter: 1955 loss: 1.50969379e-06
Iter: 1956 loss: 1.51496749e-06
Iter: 1957 loss: 1.50889434e-06
Iter: 1958 loss: 1.50681626e-06
Iter: 1959 loss: 1.51806285e-06
Iter: 1960 loss: 1.50646611e-06
Iter: 1961 loss: 1.50439723e-06
Iter: 1962 loss: 1.50597498e-06
Iter: 1963 loss: 1.50311189e-06
Iter: 1964 loss: 1.50091944e-06
Iter: 1965 loss: 1.5078183e-06
Iter: 1966 loss: 1.50028643e-06
Iter: 1967 loss: 1.49837206e-06
Iter: 1968 loss: 1.52598113e-06
Iter: 1969 loss: 1.49834432e-06
Iter: 1970 loss: 1.49705659e-06
Iter: 1971 loss: 1.4946952e-06
Iter: 1972 loss: 1.55213434e-06
Iter: 1973 loss: 1.49468337e-06
Iter: 1974 loss: 1.4922764e-06
Iter: 1975 loss: 1.5069179e-06
Iter: 1976 loss: 1.49194034e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2.4
+ date
Wed Nov  4 15:36:17 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2/300_300_300_1 --function f2 --psi 3 --alpha 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b7d4f4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b7d5aa400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b58525510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b58512510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b585128c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b58558400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b5850dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b584f5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b584f5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b583e19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b583e1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b583e1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b583e1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b58400840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b584c1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b58389950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b58389840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b5833e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b5830e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b5833eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b5833e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b583b9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b583b9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b5828a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b5828ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b581fa6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b5850dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b58204e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b58233158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b58233378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b581467b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b58146840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b5814d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b5814d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b581977b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b581c6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.008968239
test_loss: 0.011777565
train_loss: 0.008087378
test_loss: 0.010851562
train_loss: 0.0074725905
test_loss: 0.010445531
train_loss: 0.0066017928
test_loss: 0.0100701125
train_loss: 0.0063760835
test_loss: 0.009920007
train_loss: 0.0065059476
test_loss: 0.009980583
train_loss: 0.006872367
test_loss: 0.009833405
train_loss: 0.0060839253
test_loss: 0.009873839
train_loss: 0.0056872955
test_loss: 0.009805796
train_loss: 0.0058551366
test_loss: 0.009667046
train_loss: 0.0066514714
test_loss: 0.009726554
train_loss: 0.00611487
test_loss: 0.009609123
train_loss: 0.0063369307
test_loss: 0.009466556
train_loss: 0.0057480233
test_loss: 0.009639392
train_loss: 0.0061538853
test_loss: 0.009651531
train_loss: 0.0059611113
test_loss: 0.00950944
train_loss: 0.0058245044
test_loss: 0.009608078
train_loss: 0.005551257
test_loss: 0.009585387
train_loss: 0.005873743
test_loss: 0.009473712
train_loss: 0.0064786305
test_loss: 0.009525517
train_loss: 0.0056006676
test_loss: 0.009419479
train_loss: 0.0063118883
test_loss: 0.009370625
train_loss: 0.0054150103
test_loss: 0.009222521
train_loss: 0.006485025
test_loss: 0.009318808
train_loss: 0.005724586
test_loss: 0.009550967
train_loss: 0.0053020776
test_loss: 0.009340054
train_loss: 0.005063379
test_loss: 0.009196484
train_loss: 0.0056276983
test_loss: 0.00928295
train_loss: 0.00548282
test_loss: 0.009267855
train_loss: 0.0054582264
test_loss: 0.009333168
train_loss: 0.005131909
test_loss: 0.009154613
train_loss: 0.0055532902
test_loss: 0.009274616
train_loss: 0.0051502557
test_loss: 0.009173633
train_loss: 0.005303671
test_loss: 0.009166776
train_loss: 0.005359646
test_loss: 0.00927084
train_loss: 0.005706802
test_loss: 0.009359388
train_loss: 0.005288257
test_loss: 0.0091994805
train_loss: 0.0057271933
test_loss: 0.009265521
train_loss: 0.0055045863
test_loss: 0.009301375
train_loss: 0.0052480805
test_loss: 0.008962686
train_loss: 0.0051612174
test_loss: 0.009129818
train_loss: 0.005177152
test_loss: 0.009204599
train_loss: 0.0051339036
test_loss: 0.009146994
train_loss: 0.0055133654
test_loss: 0.009039989
train_loss: 0.0054303175
test_loss: 0.00907511
train_loss: 0.00540396
test_loss: 0.009071114
train_loss: 0.0058094244
test_loss: 0.009221193
train_loss: 0.0056845974
test_loss: 0.009363091
train_loss: 0.005048481
test_loss: 0.009090259
train_loss: 0.005500181
test_loss: 0.009115422
train_loss: 0.005383614
test_loss: 0.009287331
train_loss: 0.0052878233
test_loss: 0.00902985
train_loss: 0.005501326
test_loss: 0.009102577
train_loss: 0.0051124445
test_loss: 0.00916699
train_loss: 0.0051819542
test_loss: 0.009092756
train_loss: 0.0052712774
test_loss: 0.009099444
train_loss: 0.005493156
test_loss: 0.009042334
train_loss: 0.0047818106
test_loss: 0.008961489
train_loss: 0.005460465
test_loss: 0.0091094775
train_loss: 0.0051736925
test_loss: 0.009050498
train_loss: 0.0055109574
test_loss: 0.009114138
train_loss: 0.005602414
test_loss: 0.009439122
train_loss: 0.0053033982
test_loss: 0.00889546
train_loss: 0.0047493493
test_loss: 0.008994373
train_loss: 0.0049657724
test_loss: 0.008963792
train_loss: 0.0056737415
test_loss: 0.0090661645
train_loss: 0.0047512376
test_loss: 0.008807046
train_loss: 0.0051279436
test_loss: 0.009062367
train_loss: 0.0052394723
test_loss: 0.008970705
train_loss: 0.0051707807
test_loss: 0.00917479
train_loss: 0.005049784
test_loss: 0.009035327
train_loss: 0.004714421
test_loss: 0.008942062
train_loss: 0.005073645
test_loss: 0.008924835
train_loss: 0.0050171623
test_loss: 0.00896562
train_loss: 0.0048310515
test_loss: 0.008852119
train_loss: 0.0048623206
test_loss: 0.00885322
train_loss: 0.0050865076
test_loss: 0.009074514
train_loss: 0.0049516372
test_loss: 0.008938254
train_loss: 0.004775055
test_loss: 0.009042452
train_loss: 0.004774478
test_loss: 0.008829473
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.4/300_300_300_1 --optimizer lbfgs --function f2 --psi 3 --alpha 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ce701fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ce6fb40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ce6f84598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ce6f2c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ce6f2cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ce6f51598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ce6ef96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ce6ebe488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ce6ebf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddce4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddce42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddcbdbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddc8e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddcb8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddc65bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddc656a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddc11620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddbd2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddb7d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddc11c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddc11a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddb5d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddb0c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddb1c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddad97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cdda91a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cdda90510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cdda906a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cdda49620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cdda08598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cdda08c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cdda08488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cdd9f9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cddc2b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cdd992e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cdd93b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.75693414e-05
Iter: 2 loss: 4.05176652e-05
Iter: 3 loss: 4.82634896e-05
Iter: 4 loss: 3.67760367e-05
Iter: 5 loss: 3.25489382e-05
Iter: 6 loss: 3.93352966e-05
Iter: 7 loss: 3.05891663e-05
Iter: 8 loss: 2.6992142e-05
Iter: 9 loss: 4.97218316e-05
Iter: 10 loss: 2.65665385e-05
Iter: 11 loss: 2.43425529e-05
Iter: 12 loss: 2.9659026e-05
Iter: 13 loss: 2.35412572e-05
Iter: 14 loss: 2.20997026e-05
Iter: 15 loss: 3.29556024e-05
Iter: 16 loss: 2.19873218e-05
Iter: 17 loss: 2.08178608e-05
Iter: 18 loss: 2.03142572e-05
Iter: 19 loss: 1.97111367e-05
Iter: 20 loss: 1.83181164e-05
Iter: 21 loss: 2.21755872e-05
Iter: 22 loss: 1.78708287e-05
Iter: 23 loss: 1.67669532e-05
Iter: 24 loss: 3.12950178e-05
Iter: 25 loss: 1.67603685e-05
Iter: 26 loss: 1.60552881e-05
Iter: 27 loss: 1.60010131e-05
Iter: 28 loss: 1.54743884e-05
Iter: 29 loss: 1.4729776e-05
Iter: 30 loss: 1.93851629e-05
Iter: 31 loss: 1.46436669e-05
Iter: 32 loss: 1.41578457e-05
Iter: 33 loss: 1.71264219e-05
Iter: 34 loss: 1.40988213e-05
Iter: 35 loss: 1.36914077e-05
Iter: 36 loss: 1.38962541e-05
Iter: 37 loss: 1.34198872e-05
Iter: 38 loss: 1.29732471e-05
Iter: 39 loss: 1.37708385e-05
Iter: 40 loss: 1.27776593e-05
Iter: 41 loss: 1.24294556e-05
Iter: 42 loss: 1.62594533e-05
Iter: 43 loss: 1.24221824e-05
Iter: 44 loss: 1.21427129e-05
Iter: 45 loss: 1.3763276e-05
Iter: 46 loss: 1.21056537e-05
Iter: 47 loss: 1.19031483e-05
Iter: 48 loss: 1.18617345e-05
Iter: 49 loss: 1.1727906e-05
Iter: 50 loss: 1.14624645e-05
Iter: 51 loss: 1.35316059e-05
Iter: 52 loss: 1.14439263e-05
Iter: 53 loss: 1.12577545e-05
Iter: 54 loss: 1.13642109e-05
Iter: 55 loss: 1.11366535e-05
Iter: 56 loss: 1.09253542e-05
Iter: 57 loss: 1.18501493e-05
Iter: 58 loss: 1.08828353e-05
Iter: 59 loss: 1.06948846e-05
Iter: 60 loss: 1.10274168e-05
Iter: 61 loss: 1.0612458e-05
Iter: 62 loss: 1.04472392e-05
Iter: 63 loss: 1.10978617e-05
Iter: 64 loss: 1.04094197e-05
Iter: 65 loss: 1.02563445e-05
Iter: 66 loss: 1.10291521e-05
Iter: 67 loss: 1.02306185e-05
Iter: 68 loss: 1.00980906e-05
Iter: 69 loss: 1.01805072e-05
Iter: 70 loss: 1.00134439e-05
Iter: 71 loss: 9.88706324e-06
Iter: 72 loss: 1.01979213e-05
Iter: 73 loss: 9.84264261e-06
Iter: 74 loss: 9.70726251e-06
Iter: 75 loss: 1.06175376e-05
Iter: 76 loss: 9.69379926e-06
Iter: 77 loss: 9.58781493e-06
Iter: 78 loss: 9.65226172e-06
Iter: 79 loss: 9.51959555e-06
Iter: 80 loss: 9.39599522e-06
Iter: 81 loss: 9.42455699e-06
Iter: 82 loss: 9.30502756e-06
Iter: 83 loss: 9.17340185e-06
Iter: 84 loss: 9.92226614e-06
Iter: 85 loss: 9.15504461e-06
Iter: 86 loss: 9.05148954e-06
Iter: 87 loss: 9.82871552e-06
Iter: 88 loss: 9.04354783e-06
Iter: 89 loss: 8.94894583e-06
Iter: 90 loss: 9.57076372e-06
Iter: 91 loss: 8.93921606e-06
Iter: 92 loss: 8.88642353e-06
Iter: 93 loss: 8.83233224e-06
Iter: 94 loss: 8.82255e-06
Iter: 95 loss: 8.73918725e-06
Iter: 96 loss: 9.66633343e-06
Iter: 97 loss: 8.73742738e-06
Iter: 98 loss: 8.6768905e-06
Iter: 99 loss: 8.62474735e-06
Iter: 100 loss: 8.60815817e-06
Iter: 101 loss: 8.53024e-06
Iter: 102 loss: 8.9269e-06
Iter: 103 loss: 8.51743062e-06
Iter: 104 loss: 8.44434544e-06
Iter: 105 loss: 8.72860346e-06
Iter: 106 loss: 8.42732061e-06
Iter: 107 loss: 8.3640607e-06
Iter: 108 loss: 8.61612e-06
Iter: 109 loss: 8.3497e-06
Iter: 110 loss: 8.29944383e-06
Iter: 111 loss: 8.37961761e-06
Iter: 112 loss: 8.27603435e-06
Iter: 113 loss: 8.21416052e-06
Iter: 114 loss: 8.37600419e-06
Iter: 115 loss: 8.19301204e-06
Iter: 116 loss: 8.13839051e-06
Iter: 117 loss: 8.23695245e-06
Iter: 118 loss: 8.11463542e-06
Iter: 119 loss: 8.05454147e-06
Iter: 120 loss: 8.17076216e-06
Iter: 121 loss: 8.02948125e-06
Iter: 122 loss: 7.96588938e-06
Iter: 123 loss: 8.47314e-06
Iter: 124 loss: 7.96146742e-06
Iter: 125 loss: 7.91687671e-06
Iter: 126 loss: 7.87701902e-06
Iter: 127 loss: 7.86564669e-06
Iter: 128 loss: 7.80206574e-06
Iter: 129 loss: 8.00915768e-06
Iter: 130 loss: 7.78442427e-06
Iter: 131 loss: 7.74219e-06
Iter: 132 loss: 7.73915599e-06
Iter: 133 loss: 7.71415125e-06
Iter: 134 loss: 7.6656961e-06
Iter: 135 loss: 8.65706e-06
Iter: 136 loss: 7.66529865e-06
Iter: 137 loss: 7.60697503e-06
Iter: 138 loss: 7.78176764e-06
Iter: 139 loss: 7.58925262e-06
Iter: 140 loss: 7.54665916e-06
Iter: 141 loss: 7.83351243e-06
Iter: 142 loss: 7.54230587e-06
Iter: 143 loss: 7.50121399e-06
Iter: 144 loss: 7.67559868e-06
Iter: 145 loss: 7.49254787e-06
Iter: 146 loss: 7.45713896e-06
Iter: 147 loss: 7.44246836e-06
Iter: 148 loss: 7.42388e-06
Iter: 149 loss: 7.37710297e-06
Iter: 150 loss: 7.45563239e-06
Iter: 151 loss: 7.35609592e-06
Iter: 152 loss: 7.31506407e-06
Iter: 153 loss: 7.94527477e-06
Iter: 154 loss: 7.31501586e-06
Iter: 155 loss: 7.28226314e-06
Iter: 156 loss: 7.27786346e-06
Iter: 157 loss: 7.25468453e-06
Iter: 158 loss: 7.21694323e-06
Iter: 159 loss: 7.45569514e-06
Iter: 160 loss: 7.21261131e-06
Iter: 161 loss: 7.17889816e-06
Iter: 162 loss: 7.23385165e-06
Iter: 163 loss: 7.16352133e-06
Iter: 164 loss: 7.1331674e-06
Iter: 165 loss: 7.14608905e-06
Iter: 166 loss: 7.11235452e-06
Iter: 167 loss: 7.07198615e-06
Iter: 168 loss: 7.29520889e-06
Iter: 169 loss: 7.0663832e-06
Iter: 170 loss: 7.03409296e-06
Iter: 171 loss: 7.28417308e-06
Iter: 172 loss: 7.03188e-06
Iter: 173 loss: 7.0077117e-06
Iter: 174 loss: 7.04252079e-06
Iter: 175 loss: 6.99590828e-06
Iter: 176 loss: 6.96305233e-06
Iter: 177 loss: 7.05154844e-06
Iter: 178 loss: 6.95214521e-06
Iter: 179 loss: 6.92685126e-06
Iter: 180 loss: 6.92589401e-06
Iter: 181 loss: 6.90618e-06
Iter: 182 loss: 6.87351348e-06
Iter: 183 loss: 6.94175378e-06
Iter: 184 loss: 6.86057729e-06
Iter: 185 loss: 6.82380596e-06
Iter: 186 loss: 6.89759418e-06
Iter: 187 loss: 6.80859057e-06
Iter: 188 loss: 6.7836886e-06
Iter: 189 loss: 6.78337528e-06
Iter: 190 loss: 6.76241461e-06
Iter: 191 loss: 6.75005231e-06
Iter: 192 loss: 6.7412293e-06
Iter: 193 loss: 6.71066e-06
Iter: 194 loss: 6.73749673e-06
Iter: 195 loss: 6.69260135e-06
Iter: 196 loss: 6.65575772e-06
Iter: 197 loss: 6.72140141e-06
Iter: 198 loss: 6.63950459e-06
Iter: 199 loss: 6.61180138e-06
Iter: 200 loss: 6.61126e-06
Iter: 201 loss: 6.58874706e-06
Iter: 202 loss: 6.56726479e-06
Iter: 203 loss: 6.56213069e-06
Iter: 204 loss: 6.53334e-06
Iter: 205 loss: 6.60433761e-06
Iter: 206 loss: 6.52326253e-06
Iter: 207 loss: 6.49571e-06
Iter: 208 loss: 6.72602528e-06
Iter: 209 loss: 6.49423055e-06
Iter: 210 loss: 6.46644503e-06
Iter: 211 loss: 6.55252552e-06
Iter: 212 loss: 6.45854652e-06
Iter: 213 loss: 6.43927615e-06
Iter: 214 loss: 6.47416891e-06
Iter: 215 loss: 6.43103249e-06
Iter: 216 loss: 6.40908638e-06
Iter: 217 loss: 6.44506508e-06
Iter: 218 loss: 6.39916e-06
Iter: 219 loss: 6.37632866e-06
Iter: 220 loss: 6.40258349e-06
Iter: 221 loss: 6.36442473e-06
Iter: 222 loss: 6.33896116e-06
Iter: 223 loss: 6.47046227e-06
Iter: 224 loss: 6.33500395e-06
Iter: 225 loss: 6.31018611e-06
Iter: 226 loss: 6.35378365e-06
Iter: 227 loss: 6.29932674e-06
Iter: 228 loss: 6.27384816e-06
Iter: 229 loss: 6.30773366e-06
Iter: 230 loss: 6.26110523e-06
Iter: 231 loss: 6.23526284e-06
Iter: 232 loss: 6.46221952e-06
Iter: 233 loss: 6.233975e-06
Iter: 234 loss: 6.21179879e-06
Iter: 235 loss: 6.24254699e-06
Iter: 236 loss: 6.20113678e-06
Iter: 237 loss: 6.17978503e-06
Iter: 238 loss: 6.17381102e-06
Iter: 239 loss: 6.16087846e-06
Iter: 240 loss: 6.13206839e-06
Iter: 241 loss: 6.19628281e-06
Iter: 242 loss: 6.12108488e-06
Iter: 243 loss: 6.10021652e-06
Iter: 244 loss: 6.10000643e-06
Iter: 245 loss: 6.08258233e-06
Iter: 246 loss: 6.0923e-06
Iter: 247 loss: 6.07128595e-06
Iter: 248 loss: 6.05127934e-06
Iter: 249 loss: 6.14556302e-06
Iter: 250 loss: 6.04761317e-06
Iter: 251 loss: 6.03134777e-06
Iter: 252 loss: 6.13553402e-06
Iter: 253 loss: 6.02944056e-06
Iter: 254 loss: 6.01475222e-06
Iter: 255 loss: 5.98883e-06
Iter: 256 loss: 5.98887573e-06
Iter: 257 loss: 5.96225209e-06
Iter: 258 loss: 6.00344356e-06
Iter: 259 loss: 5.94971834e-06
Iter: 260 loss: 5.92092738e-06
Iter: 261 loss: 6.06899e-06
Iter: 262 loss: 5.91637e-06
Iter: 263 loss: 5.89246702e-06
Iter: 264 loss: 6.00836256e-06
Iter: 265 loss: 5.88811054e-06
Iter: 266 loss: 5.86363149e-06
Iter: 267 loss: 5.95861275e-06
Iter: 268 loss: 5.85795806e-06
Iter: 269 loss: 5.84063855e-06
Iter: 270 loss: 5.89244428e-06
Iter: 271 loss: 5.83539941e-06
Iter: 272 loss: 5.8143969e-06
Iter: 273 loss: 5.85043563e-06
Iter: 274 loss: 5.80515871e-06
Iter: 275 loss: 5.78632626e-06
Iter: 276 loss: 5.82736629e-06
Iter: 277 loss: 5.77919945e-06
Iter: 278 loss: 5.76193816e-06
Iter: 279 loss: 5.80054029e-06
Iter: 280 loss: 5.75537524e-06
Iter: 281 loss: 5.73293e-06
Iter: 282 loss: 5.81628956e-06
Iter: 283 loss: 5.72725e-06
Iter: 284 loss: 5.71146893e-06
Iter: 285 loss: 5.71168857e-06
Iter: 286 loss: 5.69894928e-06
Iter: 287 loss: 5.68019368e-06
Iter: 288 loss: 5.95845586e-06
Iter: 289 loss: 5.68017731e-06
Iter: 290 loss: 5.66434892e-06
Iter: 291 loss: 5.67415509e-06
Iter: 292 loss: 5.65431628e-06
Iter: 293 loss: 5.63942467e-06
Iter: 294 loss: 5.6688682e-06
Iter: 295 loss: 5.6336221e-06
Iter: 296 loss: 5.61370689e-06
Iter: 297 loss: 5.66416747e-06
Iter: 298 loss: 5.60653461e-06
Iter: 299 loss: 5.59218915e-06
Iter: 300 loss: 5.59343653e-06
Iter: 301 loss: 5.58090369e-06
Iter: 302 loss: 5.5615883e-06
Iter: 303 loss: 5.62990681e-06
Iter: 304 loss: 5.55657607e-06
Iter: 305 loss: 5.53760856e-06
Iter: 306 loss: 5.55313727e-06
Iter: 307 loss: 5.52620713e-06
Iter: 308 loss: 5.50972845e-06
Iter: 309 loss: 5.76043476e-06
Iter: 310 loss: 5.50972891e-06
Iter: 311 loss: 5.49411243e-06
Iter: 312 loss: 5.50909954e-06
Iter: 313 loss: 5.48530306e-06
Iter: 314 loss: 5.46878618e-06
Iter: 315 loss: 5.47757645e-06
Iter: 316 loss: 5.45790135e-06
Iter: 317 loss: 5.43708802e-06
Iter: 318 loss: 5.57712428e-06
Iter: 319 loss: 5.43512942e-06
Iter: 320 loss: 5.41994768e-06
Iter: 321 loss: 5.45031162e-06
Iter: 322 loss: 5.41391455e-06
Iter: 323 loss: 5.39653411e-06
Iter: 324 loss: 5.41489453e-06
Iter: 325 loss: 5.38702898e-06
Iter: 326 loss: 5.37293909e-06
Iter: 327 loss: 5.37287178e-06
Iter: 328 loss: 5.36334437e-06
Iter: 329 loss: 5.36485459e-06
Iter: 330 loss: 5.35604977e-06
Iter: 331 loss: 5.34037099e-06
Iter: 332 loss: 5.36206971e-06
Iter: 333 loss: 5.33261618e-06
Iter: 334 loss: 5.31705518e-06
Iter: 335 loss: 5.34671426e-06
Iter: 336 loss: 5.31048499e-06
Iter: 337 loss: 5.29617409e-06
Iter: 338 loss: 5.35028357e-06
Iter: 339 loss: 5.29271529e-06
Iter: 340 loss: 5.2761352e-06
Iter: 341 loss: 5.31474871e-06
Iter: 342 loss: 5.27014072e-06
Iter: 343 loss: 5.25579435e-06
Iter: 344 loss: 5.25825862e-06
Iter: 345 loss: 5.245e-06
Iter: 346 loss: 5.22780465e-06
Iter: 347 loss: 5.23426297e-06
Iter: 348 loss: 5.21568109e-06
Iter: 349 loss: 5.19690639e-06
Iter: 350 loss: 5.44890099e-06
Iter: 351 loss: 5.19681498e-06
Iter: 352 loss: 5.18072284e-06
Iter: 353 loss: 5.25662563e-06
Iter: 354 loss: 5.17787248e-06
Iter: 355 loss: 5.16546334e-06
Iter: 356 loss: 5.15103966e-06
Iter: 357 loss: 5.14925387e-06
Iter: 358 loss: 5.12936822e-06
Iter: 359 loss: 5.21986112e-06
Iter: 360 loss: 5.12555562e-06
Iter: 361 loss: 5.10980908e-06
Iter: 362 loss: 5.28888859e-06
Iter: 363 loss: 5.1095144e-06
Iter: 364 loss: 5.0982726e-06
Iter: 365 loss: 5.11237795e-06
Iter: 366 loss: 5.09250276e-06
Iter: 367 loss: 5.07755522e-06
Iter: 368 loss: 5.13435134e-06
Iter: 369 loss: 5.07398181e-06
Iter: 370 loss: 5.06236802e-06
Iter: 371 loss: 5.07543655e-06
Iter: 372 loss: 5.05613116e-06
Iter: 373 loss: 5.04290301e-06
Iter: 374 loss: 5.0551962e-06
Iter: 375 loss: 5.03522551e-06
Iter: 376 loss: 5.01794511e-06
Iter: 377 loss: 5.11046e-06
Iter: 378 loss: 5.01544855e-06
Iter: 379 loss: 5.00334227e-06
Iter: 380 loss: 5.01773957e-06
Iter: 381 loss: 4.9971286e-06
Iter: 382 loss: 4.98174e-06
Iter: 383 loss: 5.04317359e-06
Iter: 384 loss: 4.97823657e-06
Iter: 385 loss: 4.96613666e-06
Iter: 386 loss: 4.99845783e-06
Iter: 387 loss: 4.96222674e-06
Iter: 388 loss: 4.94946107e-06
Iter: 389 loss: 4.9351429e-06
Iter: 390 loss: 4.93339849e-06
Iter: 391 loss: 4.91592073e-06
Iter: 392 loss: 5.09029178e-06
Iter: 393 loss: 4.91539686e-06
Iter: 394 loss: 4.90271623e-06
Iter: 395 loss: 4.99595171e-06
Iter: 396 loss: 4.90176717e-06
Iter: 397 loss: 4.88929709e-06
Iter: 398 loss: 4.89420245e-06
Iter: 399 loss: 4.88072556e-06
Iter: 400 loss: 4.86796671e-06
Iter: 401 loss: 4.8702268e-06
Iter: 402 loss: 4.85844066e-06
Iter: 403 loss: 4.84704788e-06
Iter: 404 loss: 4.8470547e-06
Iter: 405 loss: 4.83500389e-06
Iter: 406 loss: 4.83770918e-06
Iter: 407 loss: 4.82625728e-06
Iter: 408 loss: 4.81420693e-06
Iter: 409 loss: 4.84130078e-06
Iter: 410 loss: 4.80985e-06
Iter: 411 loss: 4.79584423e-06
Iter: 412 loss: 4.83690246e-06
Iter: 413 loss: 4.7915928e-06
Iter: 414 loss: 4.77913727e-06
Iter: 415 loss: 4.79332039e-06
Iter: 416 loss: 4.7724061e-06
Iter: 417 loss: 4.75823526e-06
Iter: 418 loss: 4.81278312e-06
Iter: 419 loss: 4.75488741e-06
Iter: 420 loss: 4.74227545e-06
Iter: 421 loss: 4.81281131e-06
Iter: 422 loss: 4.74040053e-06
Iter: 423 loss: 4.73004229e-06
Iter: 424 loss: 4.72254351e-06
Iter: 425 loss: 4.71895783e-06
Iter: 426 loss: 4.70434361e-06
Iter: 427 loss: 4.82831229e-06
Iter: 428 loss: 4.70358782e-06
Iter: 429 loss: 4.69158e-06
Iter: 430 loss: 4.70828854e-06
Iter: 431 loss: 4.68570443e-06
Iter: 432 loss: 4.67261634e-06
Iter: 433 loss: 4.66711936e-06
Iter: 434 loss: 4.66026313e-06
Iter: 435 loss: 4.64777349e-06
Iter: 436 loss: 4.64761479e-06
Iter: 437 loss: 4.63654442e-06
Iter: 438 loss: 4.6568689e-06
Iter: 439 loss: 4.63175e-06
Iter: 440 loss: 4.62113712e-06
Iter: 441 loss: 4.61659147e-06
Iter: 442 loss: 4.61134778e-06
Iter: 443 loss: 4.60153933e-06
Iter: 444 loss: 4.60107367e-06
Iter: 445 loss: 4.59262446e-06
Iter: 446 loss: 4.59323564e-06
Iter: 447 loss: 4.58609475e-06
Iter: 448 loss: 4.57535953e-06
Iter: 449 loss: 4.57244641e-06
Iter: 450 loss: 4.56588623e-06
Iter: 451 loss: 4.55394456e-06
Iter: 452 loss: 4.65303219e-06
Iter: 453 loss: 4.5530669e-06
Iter: 454 loss: 4.54287465e-06
Iter: 455 loss: 4.57309034e-06
Iter: 456 loss: 4.53980738e-06
Iter: 457 loss: 4.52805398e-06
Iter: 458 loss: 4.52799486e-06
Iter: 459 loss: 4.51865299e-06
Iter: 460 loss: 4.50761536e-06
Iter: 461 loss: 4.60710726e-06
Iter: 462 loss: 4.5070492e-06
Iter: 463 loss: 4.49662548e-06
Iter: 464 loss: 4.51176902e-06
Iter: 465 loss: 4.49169329e-06
Iter: 466 loss: 4.48114952e-06
Iter: 467 loss: 4.48500487e-06
Iter: 468 loss: 4.47374123e-06
Iter: 469 loss: 4.46254035e-06
Iter: 470 loss: 4.53669918e-06
Iter: 471 loss: 4.46137e-06
Iter: 472 loss: 4.45009027e-06
Iter: 473 loss: 4.46845206e-06
Iter: 474 loss: 4.44492616e-06
Iter: 475 loss: 4.43334829e-06
Iter: 476 loss: 4.46285321e-06
Iter: 477 loss: 4.42943747e-06
Iter: 478 loss: 4.41943894e-06
Iter: 479 loss: 4.52048653e-06
Iter: 480 loss: 4.41911379e-06
Iter: 481 loss: 4.4108674e-06
Iter: 482 loss: 4.40237818e-06
Iter: 483 loss: 4.40087797e-06
Iter: 484 loss: 4.39217274e-06
Iter: 485 loss: 4.3920536e-06
Iter: 486 loss: 4.38363622e-06
Iter: 487 loss: 4.37118933e-06
Iter: 488 loss: 4.37097242e-06
Iter: 489 loss: 4.3588775e-06
Iter: 490 loss: 4.39616e-06
Iter: 491 loss: 4.35536185e-06
Iter: 492 loss: 4.34230788e-06
Iter: 493 loss: 4.34459253e-06
Iter: 494 loss: 4.33263085e-06
Iter: 495 loss: 4.327022e-06
Iter: 496 loss: 4.32459183e-06
Iter: 497 loss: 4.3186e-06
Iter: 498 loss: 4.30786258e-06
Iter: 499 loss: 4.30790442e-06
Iter: 500 loss: 4.29510965e-06
Iter: 501 loss: 4.37737481e-06
Iter: 502 loss: 4.29372221e-06
Iter: 503 loss: 4.28441035e-06
Iter: 504 loss: 4.33834293e-06
Iter: 505 loss: 4.28320527e-06
Iter: 506 loss: 4.2749034e-06
Iter: 507 loss: 4.26999668e-06
Iter: 508 loss: 4.26658244e-06
Iter: 509 loss: 4.25338885e-06
Iter: 510 loss: 4.28628209e-06
Iter: 511 loss: 4.24866448e-06
Iter: 512 loss: 4.23746587e-06
Iter: 513 loss: 4.27913255e-06
Iter: 514 loss: 4.2346478e-06
Iter: 515 loss: 4.22433868e-06
Iter: 516 loss: 4.29885222e-06
Iter: 517 loss: 4.22355e-06
Iter: 518 loss: 4.21272125e-06
Iter: 519 loss: 4.21632376e-06
Iter: 520 loss: 4.20505421e-06
Iter: 521 loss: 4.19644039e-06
Iter: 522 loss: 4.32168144e-06
Iter: 523 loss: 4.19637763e-06
Iter: 524 loss: 4.18990476e-06
Iter: 525 loss: 4.19038361e-06
Iter: 526 loss: 4.18499258e-06
Iter: 527 loss: 4.17534739e-06
Iter: 528 loss: 4.19691787e-06
Iter: 529 loss: 4.17167212e-06
Iter: 530 loss: 4.16179682e-06
Iter: 531 loss: 4.17808815e-06
Iter: 532 loss: 4.15744762e-06
Iter: 533 loss: 4.14841e-06
Iter: 534 loss: 4.1451749e-06
Iter: 535 loss: 4.1402227e-06
Iter: 536 loss: 4.12669124e-06
Iter: 537 loss: 4.21124332e-06
Iter: 538 loss: 4.12511054e-06
Iter: 539 loss: 4.11467863e-06
Iter: 540 loss: 4.17421825e-06
Iter: 541 loss: 4.11321116e-06
Iter: 542 loss: 4.10340317e-06
Iter: 543 loss: 4.13103453e-06
Iter: 544 loss: 4.10037592e-06
Iter: 545 loss: 4.09235554e-06
Iter: 546 loss: 4.08528695e-06
Iter: 547 loss: 4.08315691e-06
Iter: 548 loss: 4.0737109e-06
Iter: 549 loss: 4.07355674e-06
Iter: 550 loss: 4.06684876e-06
Iter: 551 loss: 4.05773153e-06
Iter: 552 loss: 4.05733954e-06
Iter: 553 loss: 4.04611774e-06
Iter: 554 loss: 4.06669415e-06
Iter: 555 loss: 4.04111142e-06
Iter: 556 loss: 4.03007471e-06
Iter: 557 loss: 4.1958092e-06
Iter: 558 loss: 4.03009108e-06
Iter: 559 loss: 4.02288788e-06
Iter: 560 loss: 4.04798357e-06
Iter: 561 loss: 4.02088654e-06
Iter: 562 loss: 4.01451189e-06
Iter: 563 loss: 4.0210266e-06
Iter: 564 loss: 4.01089665e-06
Iter: 565 loss: 4.00141926e-06
Iter: 566 loss: 4.02976139e-06
Iter: 567 loss: 3.99859573e-06
Iter: 568 loss: 3.9910592e-06
Iter: 569 loss: 4.00008139e-06
Iter: 570 loss: 3.98705515e-06
Iter: 571 loss: 3.9772508e-06
Iter: 572 loss: 3.99062e-06
Iter: 573 loss: 3.97237409e-06
Iter: 574 loss: 3.96162886e-06
Iter: 575 loss: 3.98802104e-06
Iter: 576 loss: 3.95795678e-06
Iter: 577 loss: 3.94766766e-06
Iter: 578 loss: 3.95621464e-06
Iter: 579 loss: 3.94153e-06
Iter: 580 loss: 3.93157e-06
Iter: 581 loss: 4.02136538e-06
Iter: 582 loss: 3.9309848e-06
Iter: 583 loss: 3.92205584e-06
Iter: 584 loss: 3.95450843e-06
Iter: 585 loss: 3.9198485e-06
Iter: 586 loss: 3.91178673e-06
Iter: 587 loss: 3.91052572e-06
Iter: 588 loss: 3.90500691e-06
Iter: 589 loss: 3.89493562e-06
Iter: 590 loss: 3.94564631e-06
Iter: 591 loss: 3.89337038e-06
Iter: 592 loss: 3.88362878e-06
Iter: 593 loss: 3.92556331e-06
Iter: 594 loss: 3.88153785e-06
Iter: 595 loss: 3.87376667e-06
Iter: 596 loss: 3.867538e-06
Iter: 597 loss: 3.86522879e-06
Iter: 598 loss: 3.85477369e-06
Iter: 599 loss: 3.93176379e-06
Iter: 600 loss: 3.85395197e-06
Iter: 601 loss: 3.84525174e-06
Iter: 602 loss: 3.91690401e-06
Iter: 603 loss: 3.84468331e-06
Iter: 604 loss: 3.83819042e-06
Iter: 605 loss: 3.84073201e-06
Iter: 606 loss: 3.83366068e-06
Iter: 607 loss: 3.82636881e-06
Iter: 608 loss: 3.885968e-06
Iter: 609 loss: 3.82595226e-06
Iter: 610 loss: 3.81912196e-06
Iter: 611 loss: 3.81080145e-06
Iter: 612 loss: 3.81004565e-06
Iter: 613 loss: 3.80095094e-06
Iter: 614 loss: 3.87570799e-06
Iter: 615 loss: 3.80043275e-06
Iter: 616 loss: 3.79236872e-06
Iter: 617 loss: 3.79204812e-06
Iter: 618 loss: 3.78580489e-06
Iter: 619 loss: 3.77480546e-06
Iter: 620 loss: 3.8158646e-06
Iter: 621 loss: 3.77200467e-06
Iter: 622 loss: 3.76336129e-06
Iter: 623 loss: 3.77659535e-06
Iter: 624 loss: 3.75941181e-06
Iter: 625 loss: 3.7502391e-06
Iter: 626 loss: 3.83680253e-06
Iter: 627 loss: 3.74991077e-06
Iter: 628 loss: 3.74193451e-06
Iter: 629 loss: 3.74543106e-06
Iter: 630 loss: 3.7364016e-06
Iter: 631 loss: 3.7276634e-06
Iter: 632 loss: 3.74148112e-06
Iter: 633 loss: 3.7236191e-06
Iter: 634 loss: 3.71438728e-06
Iter: 635 loss: 3.78858977e-06
Iter: 636 loss: 3.71382316e-06
Iter: 637 loss: 3.7065472e-06
Iter: 638 loss: 3.71314172e-06
Iter: 639 loss: 3.7023965e-06
Iter: 640 loss: 3.69421377e-06
Iter: 641 loss: 3.7015534e-06
Iter: 642 loss: 3.68940937e-06
Iter: 643 loss: 3.68062183e-06
Iter: 644 loss: 3.73869693e-06
Iter: 645 loss: 3.67967414e-06
Iter: 646 loss: 3.67139842e-06
Iter: 647 loss: 3.70513021e-06
Iter: 648 loss: 3.66956897e-06
Iter: 649 loss: 3.66228505e-06
Iter: 650 loss: 3.66827317e-06
Iter: 651 loss: 3.65797086e-06
Iter: 652 loss: 3.65184815e-06
Iter: 653 loss: 3.72818772e-06
Iter: 654 loss: 3.65185861e-06
Iter: 655 loss: 3.64598441e-06
Iter: 656 loss: 3.64195012e-06
Iter: 657 loss: 3.63987397e-06
Iter: 658 loss: 3.63341496e-06
Iter: 659 loss: 3.65929372e-06
Iter: 660 loss: 3.63187655e-06
Iter: 661 loss: 3.62443529e-06
Iter: 662 loss: 3.6296783e-06
Iter: 663 loss: 3.61980346e-06
Iter: 664 loss: 3.61179718e-06
Iter: 665 loss: 3.62247533e-06
Iter: 666 loss: 3.60792774e-06
Iter: 667 loss: 3.59874889e-06
Iter: 668 loss: 3.62945821e-06
Iter: 669 loss: 3.5963983e-06
Iter: 670 loss: 3.58715079e-06
Iter: 671 loss: 3.66554195e-06
Iter: 672 loss: 3.58658554e-06
Iter: 673 loss: 3.58067382e-06
Iter: 674 loss: 3.57338877e-06
Iter: 675 loss: 3.57291469e-06
Iter: 676 loss: 3.56443934e-06
Iter: 677 loss: 3.67481243e-06
Iter: 678 loss: 3.56443115e-06
Iter: 679 loss: 3.55856878e-06
Iter: 680 loss: 3.57227782e-06
Iter: 681 loss: 3.55632255e-06
Iter: 682 loss: 3.54957774e-06
Iter: 683 loss: 3.547098e-06
Iter: 684 loss: 3.54329245e-06
Iter: 685 loss: 3.53487985e-06
Iter: 686 loss: 3.57123531e-06
Iter: 687 loss: 3.53316386e-06
Iter: 688 loss: 3.52527672e-06
Iter: 689 loss: 3.58584498e-06
Iter: 690 loss: 3.52466259e-06
Iter: 691 loss: 3.51783456e-06
Iter: 692 loss: 3.52257507e-06
Iter: 693 loss: 3.51359063e-06
Iter: 694 loss: 3.50599657e-06
Iter: 695 loss: 3.51089875e-06
Iter: 696 loss: 3.50120513e-06
Iter: 697 loss: 3.49254424e-06
Iter: 698 loss: 3.54540498e-06
Iter: 699 loss: 3.49160882e-06
Iter: 700 loss: 3.48400113e-06
Iter: 701 loss: 3.52234611e-06
Iter: 702 loss: 3.48273397e-06
Iter: 703 loss: 3.47642117e-06
Iter: 704 loss: 3.5004598e-06
Iter: 705 loss: 3.47488958e-06
Iter: 706 loss: 3.46836259e-06
Iter: 707 loss: 3.46707611e-06
Iter: 708 loss: 3.46267143e-06
Iter: 709 loss: 3.45497506e-06
Iter: 710 loss: 3.46584739e-06
Iter: 711 loss: 3.45110493e-06
Iter: 712 loss: 3.44431646e-06
Iter: 713 loss: 3.50571099e-06
Iter: 714 loss: 3.44392788e-06
Iter: 715 loss: 3.43710121e-06
Iter: 716 loss: 3.44635509e-06
Iter: 717 loss: 3.43361125e-06
Iter: 718 loss: 3.42726298e-06
Iter: 719 loss: 3.42340422e-06
Iter: 720 loss: 3.42068097e-06
Iter: 721 loss: 3.41292366e-06
Iter: 722 loss: 3.41292025e-06
Iter: 723 loss: 3.40743895e-06
Iter: 724 loss: 3.40329598e-06
Iter: 725 loss: 3.40150814e-06
Iter: 726 loss: 3.39261237e-06
Iter: 727 loss: 3.41953137e-06
Iter: 728 loss: 3.38999e-06
Iter: 729 loss: 3.38324116e-06
Iter: 730 loss: 3.41267696e-06
Iter: 731 loss: 3.38189511e-06
Iter: 732 loss: 3.3742308e-06
Iter: 733 loss: 3.40704128e-06
Iter: 734 loss: 3.37268148e-06
Iter: 735 loss: 3.36652852e-06
Iter: 736 loss: 3.37411439e-06
Iter: 737 loss: 3.36323546e-06
Iter: 738 loss: 3.35689788e-06
Iter: 739 loss: 3.36090307e-06
Iter: 740 loss: 3.35293657e-06
Iter: 741 loss: 3.34473089e-06
Iter: 742 loss: 3.4267814e-06
Iter: 743 loss: 3.34454e-06
Iter: 744 loss: 3.33907064e-06
Iter: 745 loss: 3.35996e-06
Iter: 746 loss: 3.33768435e-06
Iter: 747 loss: 3.33245612e-06
Iter: 748 loss: 3.333839e-06
Iter: 749 loss: 3.32864352e-06
Iter: 750 loss: 3.3212e-06
Iter: 751 loss: 3.32148056e-06
Iter: 752 loss: 3.31536603e-06
Iter: 753 loss: 3.30722e-06
Iter: 754 loss: 3.34192646e-06
Iter: 755 loss: 3.3056117e-06
Iter: 756 loss: 3.29788554e-06
Iter: 757 loss: 3.35263508e-06
Iter: 758 loss: 3.29711293e-06
Iter: 759 loss: 3.29116733e-06
Iter: 760 loss: 3.29632621e-06
Iter: 761 loss: 3.28767283e-06
Iter: 762 loss: 3.28062924e-06
Iter: 763 loss: 3.29186514e-06
Iter: 764 loss: 3.27748353e-06
Iter: 765 loss: 3.27049747e-06
Iter: 766 loss: 3.32269246e-06
Iter: 767 loss: 3.26988197e-06
Iter: 768 loss: 3.26364034e-06
Iter: 769 loss: 3.26565032e-06
Iter: 770 loss: 3.25931842e-06
Iter: 771 loss: 3.25245924e-06
Iter: 772 loss: 3.25300107e-06
Iter: 773 loss: 3.24708753e-06
Iter: 774 loss: 3.23941481e-06
Iter: 775 loss: 3.34004358e-06
Iter: 776 loss: 3.23943141e-06
Iter: 777 loss: 3.23394806e-06
Iter: 778 loss: 3.26746022e-06
Iter: 779 loss: 3.23322706e-06
Iter: 780 loss: 3.22844699e-06
Iter: 781 loss: 3.22157962e-06
Iter: 782 loss: 3.22134315e-06
Iter: 783 loss: 3.21599441e-06
Iter: 784 loss: 3.21564221e-06
Iter: 785 loss: 3.21133848e-06
Iter: 786 loss: 3.21368907e-06
Iter: 787 loss: 3.20858339e-06
Iter: 788 loss: 3.20305139e-06
Iter: 789 loss: 3.21165089e-06
Iter: 790 loss: 3.20050776e-06
Iter: 791 loss: 3.19364517e-06
Iter: 792 loss: 3.19687797e-06
Iter: 793 loss: 3.18902767e-06
Iter: 794 loss: 3.1824643e-06
Iter: 795 loss: 3.20294521e-06
Iter: 796 loss: 3.18065486e-06
Iter: 797 loss: 3.17380659e-06
Iter: 798 loss: 3.19552737e-06
Iter: 799 loss: 3.17192166e-06
Iter: 800 loss: 3.16515798e-06
Iter: 801 loss: 3.21027187e-06
Iter: 802 loss: 3.16460864e-06
Iter: 803 loss: 3.15919874e-06
Iter: 804 loss: 3.15411307e-06
Iter: 805 loss: 3.15296211e-06
Iter: 806 loss: 3.14583781e-06
Iter: 807 loss: 3.22978349e-06
Iter: 808 loss: 3.14569616e-06
Iter: 809 loss: 3.14008321e-06
Iter: 810 loss: 3.14504632e-06
Iter: 811 loss: 3.13672e-06
Iter: 812 loss: 3.12951079e-06
Iter: 813 loss: 3.14339741e-06
Iter: 814 loss: 3.12643715e-06
Iter: 815 loss: 3.1201721e-06
Iter: 816 loss: 3.11610279e-06
Iter: 817 loss: 3.11377948e-06
Iter: 818 loss: 3.10986479e-06
Iter: 819 loss: 3.10837936e-06
Iter: 820 loss: 3.10380028e-06
Iter: 821 loss: 3.10445012e-06
Iter: 822 loss: 3.1003317e-06
Iter: 823 loss: 3.09498637e-06
Iter: 824 loss: 3.11425947e-06
Iter: 825 loss: 3.09355778e-06
Iter: 826 loss: 3.08772e-06
Iter: 827 loss: 3.09216875e-06
Iter: 828 loss: 3.08417498e-06
Iter: 829 loss: 3.07863775e-06
Iter: 830 loss: 3.1179502e-06
Iter: 831 loss: 3.07812888e-06
Iter: 832 loss: 3.07308028e-06
Iter: 833 loss: 3.07183677e-06
Iter: 834 loss: 3.06873608e-06
Iter: 835 loss: 3.06139464e-06
Iter: 836 loss: 3.0678184e-06
Iter: 837 loss: 3.05692583e-06
Iter: 838 loss: 3.04973264e-06
Iter: 839 loss: 3.08181461e-06
Iter: 840 loss: 3.04841524e-06
Iter: 841 loss: 3.041426e-06
Iter: 842 loss: 3.07163918e-06
Iter: 843 loss: 3.03991033e-06
Iter: 844 loss: 3.03394722e-06
Iter: 845 loss: 3.06770312e-06
Iter: 846 loss: 3.03317802e-06
Iter: 847 loss: 3.02858052e-06
Iter: 848 loss: 3.02499438e-06
Iter: 849 loss: 3.02354852e-06
Iter: 850 loss: 3.01629188e-06
Iter: 851 loss: 3.04473451e-06
Iter: 852 loss: 3.01457385e-06
Iter: 853 loss: 3.00861211e-06
Iter: 854 loss: 3.06456286e-06
Iter: 855 loss: 3.00836155e-06
Iter: 856 loss: 3.00250758e-06
Iter: 857 loss: 3.01527712e-06
Iter: 858 loss: 3.00039164e-06
Iter: 859 loss: 2.99568387e-06
Iter: 860 loss: 2.9973603e-06
Iter: 861 loss: 2.99236763e-06
Iter: 862 loss: 2.98527925e-06
Iter: 863 loss: 3.01791079e-06
Iter: 864 loss: 2.98401619e-06
Iter: 865 loss: 2.97932934e-06
Iter: 866 loss: 2.9883895e-06
Iter: 867 loss: 2.97745601e-06
Iter: 868 loss: 2.97183283e-06
Iter: 869 loss: 2.98157192e-06
Iter: 870 loss: 2.96936696e-06
Iter: 871 loss: 2.96322605e-06
Iter: 872 loss: 2.96930102e-06
Iter: 873 loss: 2.95965719e-06
Iter: 874 loss: 2.95366272e-06
Iter: 875 loss: 2.98332407e-06
Iter: 876 loss: 2.95258451e-06
Iter: 877 loss: 2.94725191e-06
Iter: 878 loss: 2.98222403e-06
Iter: 879 loss: 2.94669553e-06
Iter: 880 loss: 2.94220035e-06
Iter: 881 loss: 2.93744665e-06
Iter: 882 loss: 2.9367427e-06
Iter: 883 loss: 2.92985646e-06
Iter: 884 loss: 2.95872769e-06
Iter: 885 loss: 2.92844584e-06
Iter: 886 loss: 2.9220032e-06
Iter: 887 loss: 2.96119265e-06
Iter: 888 loss: 2.92117761e-06
Iter: 889 loss: 2.9155151e-06
Iter: 890 loss: 2.92234427e-06
Iter: 891 loss: 2.91253286e-06
Iter: 892 loss: 2.90735943e-06
Iter: 893 loss: 2.93305357e-06
Iter: 894 loss: 2.90649905e-06
Iter: 895 loss: 2.90150979e-06
Iter: 896 loss: 2.93055655e-06
Iter: 897 loss: 2.90078697e-06
Iter: 898 loss: 2.89657237e-06
Iter: 899 loss: 2.89213108e-06
Iter: 900 loss: 2.89133277e-06
Iter: 901 loss: 2.88567117e-06
Iter: 902 loss: 2.90134221e-06
Iter: 903 loss: 2.88379306e-06
Iter: 904 loss: 2.87826856e-06
Iter: 905 loss: 2.91908373e-06
Iter: 906 loss: 2.87783132e-06
Iter: 907 loss: 2.87265698e-06
Iter: 908 loss: 2.87585181e-06
Iter: 909 loss: 2.86927752e-06
Iter: 910 loss: 2.86379418e-06
Iter: 911 loss: 2.89602372e-06
Iter: 912 loss: 2.86308477e-06
Iter: 913 loss: 2.85816486e-06
Iter: 914 loss: 2.85798501e-06
Iter: 915 loss: 2.8540876e-06
Iter: 916 loss: 2.84736e-06
Iter: 917 loss: 2.86219074e-06
Iter: 918 loss: 2.84474527e-06
Iter: 919 loss: 2.83918143e-06
Iter: 920 loss: 2.86335262e-06
Iter: 921 loss: 2.83802206e-06
Iter: 922 loss: 2.83246618e-06
Iter: 923 loss: 2.86135264e-06
Iter: 924 loss: 2.83156214e-06
Iter: 925 loss: 2.82622159e-06
Iter: 926 loss: 2.83082863e-06
Iter: 927 loss: 2.82312021e-06
Iter: 928 loss: 2.8181189e-06
Iter: 929 loss: 2.83220379e-06
Iter: 930 loss: 2.81657412e-06
Iter: 931 loss: 2.81162193e-06
Iter: 932 loss: 2.85573356e-06
Iter: 933 loss: 2.81132611e-06
Iter: 934 loss: 2.80709219e-06
Iter: 935 loss: 2.80614177e-06
Iter: 936 loss: 2.80339827e-06
Iter: 937 loss: 2.79849883e-06
Iter: 938 loss: 2.83797567e-06
Iter: 939 loss: 2.79810683e-06
Iter: 940 loss: 2.79426968e-06
Iter: 941 loss: 2.78836978e-06
Iter: 942 loss: 2.78826587e-06
Iter: 943 loss: 2.78143398e-06
Iter: 944 loss: 2.80971017e-06
Iter: 945 loss: 2.77996719e-06
Iter: 946 loss: 2.77294134e-06
Iter: 947 loss: 2.7991714e-06
Iter: 948 loss: 2.77127219e-06
Iter: 949 loss: 2.7658798e-06
Iter: 950 loss: 2.80547874e-06
Iter: 951 loss: 2.76542778e-06
Iter: 952 loss: 2.7611411e-06
Iter: 953 loss: 2.75806224e-06
Iter: 954 loss: 2.75659886e-06
Iter: 955 loss: 2.75117372e-06
Iter: 956 loss: 2.82810765e-06
Iter: 957 loss: 2.75115463e-06
Iter: 958 loss: 2.74716263e-06
Iter: 959 loss: 2.75560387e-06
Iter: 960 loss: 2.74556578e-06
Iter: 961 loss: 2.74146487e-06
Iter: 962 loss: 2.7373967e-06
Iter: 963 loss: 2.73643127e-06
Iter: 964 loss: 2.7303704e-06
Iter: 965 loss: 2.78209063e-06
Iter: 966 loss: 2.73000433e-06
Iter: 967 loss: 2.72495163e-06
Iter: 968 loss: 2.75173306e-06
Iter: 969 loss: 2.72413899e-06
Iter: 970 loss: 2.71965018e-06
Iter: 971 loss: 2.72562e-06
Iter: 972 loss: 2.71744398e-06
Iter: 973 loss: 2.71300041e-06
Iter: 974 loss: 2.7479598e-06
Iter: 975 loss: 2.71263457e-06
Iter: 976 loss: 2.70951023e-06
Iter: 977 loss: 2.70531586e-06
Iter: 978 loss: 2.70504916e-06
Iter: 979 loss: 2.70017654e-06
Iter: 980 loss: 2.74009608e-06
Iter: 981 loss: 2.69981911e-06
Iter: 982 loss: 2.69506836e-06
Iter: 983 loss: 2.69857173e-06
Iter: 984 loss: 2.69206203e-06
Iter: 985 loss: 2.68726262e-06
Iter: 986 loss: 2.69268821e-06
Iter: 987 loss: 2.68465919e-06
Iter: 988 loss: 2.67884843e-06
Iter: 989 loss: 2.69334259e-06
Iter: 990 loss: 2.67675023e-06
Iter: 991 loss: 2.67037012e-06
Iter: 992 loss: 2.71280487e-06
Iter: 993 loss: 2.66976303e-06
Iter: 994 loss: 2.66545931e-06
Iter: 995 loss: 2.66552252e-06
Iter: 996 loss: 2.66211e-06
Iter: 997 loss: 2.6567127e-06
Iter: 998 loss: 2.69201018e-06
Iter: 999 loss: 2.65607969e-06
Iter: 1000 loss: 2.65149447e-06
Iter: 1001 loss: 2.6745563e-06
Iter: 1002 loss: 2.65073618e-06
Iter: 1003 loss: 2.64654705e-06
Iter: 1004 loss: 2.64717391e-06
Iter: 1005 loss: 2.64337791e-06
Iter: 1006 loss: 2.63865832e-06
Iter: 1007 loss: 2.65062113e-06
Iter: 1008 loss: 2.63704396e-06
Iter: 1009 loss: 2.63235734e-06
Iter: 1010 loss: 2.68054168e-06
Iter: 1011 loss: 2.63216225e-06
Iter: 1012 loss: 2.62862454e-06
Iter: 1013 loss: 2.62976801e-06
Iter: 1014 loss: 2.62599133e-06
Iter: 1015 loss: 2.62169078e-06
Iter: 1016 loss: 2.64835171e-06
Iter: 1017 loss: 2.62118351e-06
Iter: 1018 loss: 2.61784976e-06
Iter: 1019 loss: 2.61787568e-06
Iter: 1020 loss: 2.61524565e-06
Iter: 1021 loss: 2.61072e-06
Iter: 1022 loss: 2.61789341e-06
Iter: 1023 loss: 2.60876368e-06
Iter: 1024 loss: 2.60328579e-06
Iter: 1025 loss: 2.63896254e-06
Iter: 1026 loss: 2.60278034e-06
Iter: 1027 loss: 2.59919329e-06
Iter: 1028 loss: 2.59454487e-06
Iter: 1029 loss: 2.59430408e-06
Iter: 1030 loss: 2.58850241e-06
Iter: 1031 loss: 2.63088668e-06
Iter: 1032 loss: 2.58802e-06
Iter: 1033 loss: 2.58251384e-06
Iter: 1034 loss: 2.59643866e-06
Iter: 1035 loss: 2.58062391e-06
Iter: 1036 loss: 2.57534748e-06
Iter: 1037 loss: 2.59135504e-06
Iter: 1038 loss: 2.57370539e-06
Iter: 1039 loss: 2.56905946e-06
Iter: 1040 loss: 2.57409511e-06
Iter: 1041 loss: 2.56657972e-06
Iter: 1042 loss: 2.56205249e-06
Iter: 1043 loss: 2.59677904e-06
Iter: 1044 loss: 2.56164367e-06
Iter: 1045 loss: 2.55749228e-06
Iter: 1046 loss: 2.57355578e-06
Iter: 1047 loss: 2.55642271e-06
Iter: 1048 loss: 2.55313307e-06
Iter: 1049 loss: 2.55710711e-06
Iter: 1050 loss: 2.55139184e-06
Iter: 1051 loss: 2.54718907e-06
Iter: 1052 loss: 2.57260081e-06
Iter: 1053 loss: 2.54663064e-06
Iter: 1054 loss: 2.5433767e-06
Iter: 1055 loss: 2.54369024e-06
Iter: 1056 loss: 2.54084171e-06
Iter: 1057 loss: 2.53657367e-06
Iter: 1058 loss: 2.54632141e-06
Iter: 1059 loss: 2.53495e-06
Iter: 1060 loss: 2.52954442e-06
Iter: 1061 loss: 2.54519e-06
Iter: 1062 loss: 2.52789414e-06
Iter: 1063 loss: 2.5243005e-06
Iter: 1064 loss: 2.52755194e-06
Iter: 1065 loss: 2.52221184e-06
Iter: 1066 loss: 2.51710208e-06
Iter: 1067 loss: 2.54289193e-06
Iter: 1068 loss: 2.51621623e-06
Iter: 1069 loss: 2.51209349e-06
Iter: 1070 loss: 2.51774782e-06
Iter: 1071 loss: 2.50999756e-06
Iter: 1072 loss: 2.50580865e-06
Iter: 1073 loss: 2.50964331e-06
Iter: 1074 loss: 2.50337098e-06
Iter: 1075 loss: 2.49911182e-06
Iter: 1076 loss: 2.53195e-06
Iter: 1077 loss: 2.49882305e-06
Iter: 1078 loss: 2.49459663e-06
Iter: 1079 loss: 2.49482719e-06
Iter: 1080 loss: 2.49131881e-06
Iter: 1081 loss: 2.48592551e-06
Iter: 1082 loss: 2.50505786e-06
Iter: 1083 loss: 2.48452329e-06
Iter: 1084 loss: 2.47964408e-06
Iter: 1085 loss: 2.48675633e-06
Iter: 1086 loss: 2.47724847e-06
Iter: 1087 loss: 2.47423782e-06
Iter: 1088 loss: 2.47393837e-06
Iter: 1089 loss: 2.47121534e-06
Iter: 1090 loss: 2.46682885e-06
Iter: 1091 loss: 2.46678383e-06
Iter: 1092 loss: 2.46263517e-06
Iter: 1093 loss: 2.52498944e-06
Iter: 1094 loss: 2.46262198e-06
Iter: 1095 loss: 2.45977071e-06
Iter: 1096 loss: 2.4562346e-06
Iter: 1097 loss: 2.45585716e-06
Iter: 1098 loss: 2.45089268e-06
Iter: 1099 loss: 2.47504204e-06
Iter: 1100 loss: 2.45000774e-06
Iter: 1101 loss: 2.4452786e-06
Iter: 1102 loss: 2.46740228e-06
Iter: 1103 loss: 2.44447324e-06
Iter: 1104 loss: 2.44120884e-06
Iter: 1105 loss: 2.43971e-06
Iter: 1106 loss: 2.43804789e-06
Iter: 1107 loss: 2.4328383e-06
Iter: 1108 loss: 2.46703848e-06
Iter: 1109 loss: 2.43231921e-06
Iter: 1110 loss: 2.42791521e-06
Iter: 1111 loss: 2.44140665e-06
Iter: 1112 loss: 2.42662509e-06
Iter: 1113 loss: 2.42287524e-06
Iter: 1114 loss: 2.42318629e-06
Iter: 1115 loss: 2.41994712e-06
Iter: 1116 loss: 2.41498515e-06
Iter: 1117 loss: 2.43628847e-06
Iter: 1118 loss: 2.41394537e-06
Iter: 1119 loss: 2.40938743e-06
Iter: 1120 loss: 2.42869442e-06
Iter: 1121 loss: 2.40841428e-06
Iter: 1122 loss: 2.40443751e-06
Iter: 1123 loss: 2.40339614e-06
Iter: 1124 loss: 2.40085683e-06
Iter: 1125 loss: 2.39517294e-06
Iter: 1126 loss: 2.4197484e-06
Iter: 1127 loss: 2.39392602e-06
Iter: 1128 loss: 2.39039423e-06
Iter: 1129 loss: 2.39038854e-06
Iter: 1130 loss: 2.38735788e-06
Iter: 1131 loss: 2.38493112e-06
Iter: 1132 loss: 2.38401981e-06
Iter: 1133 loss: 2.3806856e-06
Iter: 1134 loss: 2.40116788e-06
Iter: 1135 loss: 2.3802304e-06
Iter: 1136 loss: 2.37659606e-06
Iter: 1137 loss: 2.37609606e-06
Iter: 1138 loss: 2.37344375e-06
Iter: 1139 loss: 2.3689413e-06
Iter: 1140 loss: 2.38623625e-06
Iter: 1141 loss: 2.36790265e-06
Iter: 1142 loss: 2.36453047e-06
Iter: 1143 loss: 2.39249584e-06
Iter: 1144 loss: 2.36429742e-06
Iter: 1145 loss: 2.36123e-06
Iter: 1146 loss: 2.35711309e-06
Iter: 1147 loss: 2.35696e-06
Iter: 1148 loss: 2.3515322e-06
Iter: 1149 loss: 2.38606344e-06
Iter: 1150 loss: 2.35098469e-06
Iter: 1151 loss: 2.34678123e-06
Iter: 1152 loss: 2.37650784e-06
Iter: 1153 loss: 2.34622985e-06
Iter: 1154 loss: 2.34342451e-06
Iter: 1155 loss: 2.34332902e-06
Iter: 1156 loss: 2.34110576e-06
Iter: 1157 loss: 2.33682567e-06
Iter: 1158 loss: 2.34500908e-06
Iter: 1159 loss: 2.33512833e-06
Iter: 1160 loss: 2.33059382e-06
Iter: 1161 loss: 2.35760581e-06
Iter: 1162 loss: 2.33006085e-06
Iter: 1163 loss: 2.32680168e-06
Iter: 1164 loss: 2.32704133e-06
Iter: 1165 loss: 2.32414e-06
Iter: 1166 loss: 2.31957324e-06
Iter: 1167 loss: 2.33910987e-06
Iter: 1168 loss: 2.31849799e-06
Iter: 1169 loss: 2.31525428e-06
Iter: 1170 loss: 2.315237e-06
Iter: 1171 loss: 2.31261265e-06
Iter: 1172 loss: 2.3082539e-06
Iter: 1173 loss: 2.30823775e-06
Iter: 1174 loss: 2.30380351e-06
Iter: 1175 loss: 2.31897957e-06
Iter: 1176 loss: 2.30266278e-06
Iter: 1177 loss: 2.29821944e-06
Iter: 1178 loss: 2.33475475e-06
Iter: 1179 loss: 2.298043e-06
Iter: 1180 loss: 2.29533021e-06
Iter: 1181 loss: 2.29285183e-06
Iter: 1182 loss: 2.29222337e-06
Iter: 1183 loss: 2.28787485e-06
Iter: 1184 loss: 2.32537877e-06
Iter: 1185 loss: 2.2876568e-06
Iter: 1186 loss: 2.28409476e-06
Iter: 1187 loss: 2.29240027e-06
Iter: 1188 loss: 2.28281033e-06
Iter: 1189 loss: 2.27966621e-06
Iter: 1190 loss: 2.27801229e-06
Iter: 1191 loss: 2.27659075e-06
Iter: 1192 loss: 2.27196961e-06
Iter: 1193 loss: 2.32475077e-06
Iter: 1194 loss: 2.2719878e-06
Iter: 1195 loss: 2.26880275e-06
Iter: 1196 loss: 2.27398164e-06
Iter: 1197 loss: 2.26730708e-06
Iter: 1198 loss: 2.26399698e-06
Iter: 1199 loss: 2.26607176e-06
Iter: 1200 loss: 2.26181737e-06
Iter: 1201 loss: 2.25747658e-06
Iter: 1202 loss: 2.28084491e-06
Iter: 1203 loss: 2.25669737e-06
Iter: 1204 loss: 2.25376311e-06
Iter: 1205 loss: 2.25831309e-06
Iter: 1206 loss: 2.25228064e-06
Iter: 1207 loss: 2.24868199e-06
Iter: 1208 loss: 2.2574618e-06
Iter: 1209 loss: 2.24731957e-06
Iter: 1210 loss: 2.24331711e-06
Iter: 1211 loss: 2.280324e-06
Iter: 1212 loss: 2.24310088e-06
Iter: 1213 loss: 2.24084624e-06
Iter: 1214 loss: 2.23674851e-06
Iter: 1215 loss: 2.23674238e-06
Iter: 1216 loss: 2.23191773e-06
Iter: 1217 loss: 2.25304484e-06
Iter: 1218 loss: 2.23079087e-06
Iter: 1219 loss: 2.22748304e-06
Iter: 1220 loss: 2.2641359e-06
Iter: 1221 loss: 2.22738413e-06
Iter: 1222 loss: 2.22456902e-06
Iter: 1223 loss: 2.22275e-06
Iter: 1224 loss: 2.22154176e-06
Iter: 1225 loss: 2.21761593e-06
Iter: 1226 loss: 2.23450456e-06
Iter: 1227 loss: 2.21678329e-06
Iter: 1228 loss: 2.21311711e-06
Iter: 1229 loss: 2.23496272e-06
Iter: 1230 loss: 2.21257e-06
Iter: 1231 loss: 2.20921925e-06
Iter: 1232 loss: 2.20814445e-06
Iter: 1233 loss: 2.2062261e-06
Iter: 1234 loss: 2.20226e-06
Iter: 1235 loss: 2.21264872e-06
Iter: 1236 loss: 2.20090897e-06
Iter: 1237 loss: 2.19605931e-06
Iter: 1238 loss: 2.2235613e-06
Iter: 1239 loss: 2.19536696e-06
Iter: 1240 loss: 2.19236267e-06
Iter: 1241 loss: 2.20168909e-06
Iter: 1242 loss: 2.19135018e-06
Iter: 1243 loss: 2.18813329e-06
Iter: 1244 loss: 2.19004073e-06
Iter: 1245 loss: 2.18602236e-06
Iter: 1246 loss: 2.18173159e-06
Iter: 1247 loss: 2.20036827e-06
Iter: 1248 loss: 2.18085847e-06
Iter: 1249 loss: 2.17794741e-06
Iter: 1250 loss: 2.18995979e-06
Iter: 1251 loss: 2.17728802e-06
Iter: 1252 loss: 2.17357729e-06
Iter: 1253 loss: 2.17938896e-06
Iter: 1254 loss: 2.17177762e-06
Iter: 1255 loss: 2.16849526e-06
Iter: 1256 loss: 2.16628928e-06
Iter: 1257 loss: 2.16505578e-06
Iter: 1258 loss: 2.16088347e-06
Iter: 1259 loss: 2.18258037e-06
Iter: 1260 loss: 2.16024364e-06
Iter: 1261 loss: 2.15639375e-06
Iter: 1262 loss: 2.17234469e-06
Iter: 1263 loss: 2.15550449e-06
Iter: 1264 loss: 2.1513556e-06
Iter: 1265 loss: 2.16019498e-06
Iter: 1266 loss: 2.14977081e-06
Iter: 1267 loss: 2.14646525e-06
Iter: 1268 loss: 2.14953616e-06
Iter: 1269 loss: 2.1446358e-06
Iter: 1270 loss: 2.14044326e-06
Iter: 1271 loss: 2.16694889e-06
Iter: 1272 loss: 2.13995258e-06
Iter: 1273 loss: 2.13642261e-06
Iter: 1274 loss: 2.1461035e-06
Iter: 1275 loss: 2.13536441e-06
Iter: 1276 loss: 2.13217868e-06
Iter: 1277 loss: 2.13168869e-06
Iter: 1278 loss: 2.12946952e-06
Iter: 1279 loss: 2.12617533e-06
Iter: 1280 loss: 2.16399758e-06
Iter: 1281 loss: 2.12607665e-06
Iter: 1282 loss: 2.12298e-06
Iter: 1283 loss: 2.12421492e-06
Iter: 1284 loss: 2.12085956e-06
Iter: 1285 loss: 2.11695442e-06
Iter: 1286 loss: 2.13161047e-06
Iter: 1287 loss: 2.11595079e-06
Iter: 1288 loss: 2.11232191e-06
Iter: 1289 loss: 2.12541681e-06
Iter: 1290 loss: 2.11141469e-06
Iter: 1291 loss: 2.10819e-06
Iter: 1292 loss: 2.11664951e-06
Iter: 1293 loss: 2.10710641e-06
Iter: 1294 loss: 2.10364283e-06
Iter: 1295 loss: 2.12190116e-06
Iter: 1296 loss: 2.10313146e-06
Iter: 1297 loss: 2.10091184e-06
Iter: 1298 loss: 2.09715836e-06
Iter: 1299 loss: 2.09717018e-06
Iter: 1300 loss: 2.09255586e-06
Iter: 1301 loss: 2.09950281e-06
Iter: 1302 loss: 2.09036671e-06
Iter: 1303 loss: 2.08599272e-06
Iter: 1304 loss: 2.14302e-06
Iter: 1305 loss: 2.08592064e-06
Iter: 1306 loss: 2.08261554e-06
Iter: 1307 loss: 2.09232371e-06
Iter: 1308 loss: 2.08155325e-06
Iter: 1309 loss: 2.07818221e-06
Iter: 1310 loss: 2.07955145e-06
Iter: 1311 loss: 2.07581661e-06
Iter: 1312 loss: 2.07176095e-06
Iter: 1313 loss: 2.07900621e-06
Iter: 1314 loss: 2.06991922e-06
Iter: 1315 loss: 2.06648451e-06
Iter: 1316 loss: 2.11606084e-06
Iter: 1317 loss: 2.06647542e-06
Iter: 1318 loss: 2.06392224e-06
Iter: 1319 loss: 2.06596678e-06
Iter: 1320 loss: 2.06236655e-06
Iter: 1321 loss: 2.05912647e-06
Iter: 1322 loss: 2.06319032e-06
Iter: 1323 loss: 2.05736569e-06
Iter: 1324 loss: 2.05358174e-06
Iter: 1325 loss: 2.07862558e-06
Iter: 1326 loss: 2.05318e-06
Iter: 1327 loss: 2.05058586e-06
Iter: 1328 loss: 2.0630855e-06
Iter: 1329 loss: 2.05012384e-06
Iter: 1330 loss: 2.04760545e-06
Iter: 1331 loss: 2.05070182e-06
Iter: 1332 loss: 2.04632784e-06
Iter: 1333 loss: 2.04321714e-06
Iter: 1334 loss: 2.04211301e-06
Iter: 1335 loss: 2.04037678e-06
Iter: 1336 loss: 2.03728973e-06
Iter: 1337 loss: 2.06768073e-06
Iter: 1338 loss: 2.03719469e-06
Iter: 1339 loss: 2.03402465e-06
Iter: 1340 loss: 2.0353641e-06
Iter: 1341 loss: 2.03177069e-06
Iter: 1342 loss: 2.02851152e-06
Iter: 1343 loss: 2.03009199e-06
Iter: 1344 loss: 2.02628871e-06
Iter: 1345 loss: 2.02267552e-06
Iter: 1346 loss: 2.03660147e-06
Iter: 1347 loss: 2.02180649e-06
Iter: 1348 loss: 2.01822013e-06
Iter: 1349 loss: 2.0405289e-06
Iter: 1350 loss: 2.01777834e-06
Iter: 1351 loss: 2.01439025e-06
Iter: 1352 loss: 2.01659896e-06
Iter: 1353 loss: 2.01224043e-06
Iter: 1354 loss: 2.00877685e-06
Iter: 1355 loss: 2.01275316e-06
Iter: 1356 loss: 2.00693512e-06
Iter: 1357 loss: 2.00285535e-06
Iter: 1358 loss: 2.02712317e-06
Iter: 1359 loss: 2.0023042e-06
Iter: 1360 loss: 1.99907413e-06
Iter: 1361 loss: 2.01637658e-06
Iter: 1362 loss: 1.99864121e-06
Iter: 1363 loss: 1.99586702e-06
Iter: 1364 loss: 2.00377735e-06
Iter: 1365 loss: 1.99505598e-06
Iter: 1366 loss: 1.99204305e-06
Iter: 1367 loss: 2.00002864e-06
Iter: 1368 loss: 1.99106739e-06
Iter: 1369 loss: 1.98819384e-06
Iter: 1370 loss: 1.99361443e-06
Iter: 1371 loss: 1.98698649e-06
Iter: 1372 loss: 1.98455473e-06
Iter: 1373 loss: 1.99530041e-06
Iter: 1374 loss: 1.9839772e-06
Iter: 1375 loss: 1.98117141e-06
Iter: 1376 loss: 1.97719669e-06
Iter: 1377 loss: 1.97707482e-06
Iter: 1378 loss: 1.97269628e-06
Iter: 1379 loss: 1.99100077e-06
Iter: 1380 loss: 1.97184272e-06
Iter: 1381 loss: 1.96749033e-06
Iter: 1382 loss: 1.98426483e-06
Iter: 1383 loss: 1.96657584e-06
Iter: 1384 loss: 1.96361134e-06
Iter: 1385 loss: 2.00547106e-06
Iter: 1386 loss: 1.96358633e-06
Iter: 1387 loss: 1.96140854e-06
Iter: 1388 loss: 1.95866687e-06
Iter: 1389 loss: 1.95841812e-06
Iter: 1390 loss: 1.95528901e-06
Iter: 1391 loss: 1.98390808e-06
Iter: 1392 loss: 1.95517e-06
Iter: 1393 loss: 1.95215625e-06
Iter: 1394 loss: 1.95386224e-06
Iter: 1395 loss: 1.95023472e-06
Iter: 1396 loss: 1.94651034e-06
Iter: 1397 loss: 1.95306779e-06
Iter: 1398 loss: 1.94488166e-06
Iter: 1399 loss: 1.94160725e-06
Iter: 1400 loss: 1.95642042e-06
Iter: 1401 loss: 1.94095173e-06
Iter: 1402 loss: 1.93822484e-06
Iter: 1403 loss: 1.97183908e-06
Iter: 1404 loss: 1.93818801e-06
Iter: 1405 loss: 1.93640653e-06
Iter: 1406 loss: 1.93343703e-06
Iter: 1407 loss: 1.93337246e-06
Iter: 1408 loss: 1.9294921e-06
Iter: 1409 loss: 1.95370399e-06
Iter: 1410 loss: 1.92904417e-06
Iter: 1411 loss: 1.92628295e-06
Iter: 1412 loss: 1.92815037e-06
Iter: 1413 loss: 1.92451375e-06
Iter: 1414 loss: 1.92117932e-06
Iter: 1415 loss: 1.93659889e-06
Iter: 1416 loss: 1.92044809e-06
Iter: 1417 loss: 1.91731328e-06
Iter: 1418 loss: 1.93237292e-06
Iter: 1419 loss: 1.91679396e-06
Iter: 1420 loss: 1.91433674e-06
Iter: 1421 loss: 1.91187519e-06
Iter: 1422 loss: 1.91135473e-06
Iter: 1423 loss: 1.90751871e-06
Iter: 1424 loss: 1.92621542e-06
Iter: 1425 loss: 1.90673404e-06
Iter: 1426 loss: 1.90388573e-06
Iter: 1427 loss: 1.92017205e-06
Iter: 1428 loss: 1.90342246e-06
Iter: 1429 loss: 1.90045262e-06
Iter: 1430 loss: 1.91135359e-06
Iter: 1431 loss: 1.89966181e-06
Iter: 1432 loss: 1.89703439e-06
Iter: 1433 loss: 1.89835691e-06
Iter: 1434 loss: 1.8953582e-06
Iter: 1435 loss: 1.89228012e-06
Iter: 1436 loss: 1.90946594e-06
Iter: 1437 loss: 1.89182674e-06
Iter: 1438 loss: 1.88895683e-06
Iter: 1439 loss: 1.8952469e-06
Iter: 1440 loss: 1.88781166e-06
Iter: 1441 loss: 1.88510637e-06
Iter: 1442 loss: 1.89901857e-06
Iter: 1443 loss: 1.88470608e-06
Iter: 1444 loss: 1.88208355e-06
Iter: 1445 loss: 1.88899071e-06
Iter: 1446 loss: 1.8813173e-06
Iter: 1447 loss: 1.87910177e-06
Iter: 1448 loss: 1.87498881e-06
Iter: 1449 loss: 1.96247424e-06
Iter: 1450 loss: 1.87499768e-06
Iter: 1451 loss: 1.87195428e-06
Iter: 1452 loss: 1.87191665e-06
Iter: 1453 loss: 1.8693305e-06
Iter: 1454 loss: 1.86989905e-06
Iter: 1455 loss: 1.86751106e-06
Iter: 1456 loss: 1.8642636e-06
Iter: 1457 loss: 1.86597345e-06
Iter: 1458 loss: 1.86205807e-06
Iter: 1459 loss: 1.85828821e-06
Iter: 1460 loss: 1.87492265e-06
Iter: 1461 loss: 1.85758859e-06
Iter: 1462 loss: 1.85450722e-06
Iter: 1463 loss: 1.8766666e-06
Iter: 1464 loss: 1.85421175e-06
Iter: 1465 loss: 1.85135059e-06
Iter: 1466 loss: 1.86100397e-06
Iter: 1467 loss: 1.85056479e-06
Iter: 1468 loss: 1.84821261e-06
Iter: 1469 loss: 1.85068711e-06
Iter: 1470 loss: 1.84689475e-06
Iter: 1471 loss: 1.8442795e-06
Iter: 1472 loss: 1.85446697e-06
Iter: 1473 loss: 1.84370333e-06
Iter: 1474 loss: 1.8408067e-06
Iter: 1475 loss: 1.84915166e-06
Iter: 1476 loss: 1.83997497e-06
Iter: 1477 loss: 1.83756856e-06
Iter: 1478 loss: 1.85100942e-06
Iter: 1479 loss: 1.83726615e-06
Iter: 1480 loss: 1.8349275e-06
Iter: 1481 loss: 1.83580323e-06
Iter: 1482 loss: 1.83336476e-06
Iter: 1483 loss: 1.83062957e-06
Iter: 1484 loss: 1.83187717e-06
Iter: 1485 loss: 1.82883605e-06
Iter: 1486 loss: 1.82590372e-06
Iter: 1487 loss: 1.84262319e-06
Iter: 1488 loss: 1.82548479e-06
Iter: 1489 loss: 1.82254325e-06
Iter: 1490 loss: 1.83221277e-06
Iter: 1491 loss: 1.82180122e-06
Iter: 1492 loss: 1.81941778e-06
Iter: 1493 loss: 1.82199346e-06
Iter: 1494 loss: 1.81813584e-06
Iter: 1495 loss: 1.81518658e-06
Iter: 1496 loss: 1.82825431e-06
Iter: 1497 loss: 1.81457119e-06
Iter: 1498 loss: 1.81201767e-06
Iter: 1499 loss: 1.8127505e-06
Iter: 1500 loss: 1.81011933e-06
Iter: 1501 loss: 1.8070217e-06
Iter: 1502 loss: 1.8119523e-06
Iter: 1503 loss: 1.80554491e-06
Iter: 1504 loss: 1.8021799e-06
Iter: 1505 loss: 1.81246173e-06
Iter: 1506 loss: 1.80113818e-06
Iter: 1507 loss: 1.79876156e-06
Iter: 1508 loss: 1.83821385e-06
Iter: 1509 loss: 1.79876372e-06
Iter: 1510 loss: 1.7965308e-06
Iter: 1511 loss: 1.7971222e-06
Iter: 1512 loss: 1.7948571e-06
Iter: 1513 loss: 1.79273707e-06
Iter: 1514 loss: 1.81003679e-06
Iter: 1515 loss: 1.79259473e-06
Iter: 1516 loss: 1.7904523e-06
Iter: 1517 loss: 1.79092933e-06
Iter: 1518 loss: 1.78895812e-06
Iter: 1519 loss: 1.78626044e-06
Iter: 1520 loss: 1.79120718e-06
Iter: 1521 loss: 1.78507037e-06
Iter: 1522 loss: 1.78246205e-06
Iter: 1523 loss: 1.79554718e-06
Iter: 1524 loss: 1.78204232e-06
Iter: 1525 loss: 1.77950517e-06
Iter: 1526 loss: 1.77910806e-06
Iter: 1527 loss: 1.77736138e-06
Iter: 1528 loss: 1.77438869e-06
Iter: 1529 loss: 1.78482605e-06
Iter: 1530 loss: 1.77360926e-06
Iter: 1531 loss: 1.77064749e-06
Iter: 1532 loss: 1.78145638e-06
Iter: 1533 loss: 1.76997173e-06
Iter: 1534 loss: 1.7671382e-06
Iter: 1535 loss: 1.77769232e-06
Iter: 1536 loss: 1.7665019e-06
Iter: 1537 loss: 1.76408207e-06
Iter: 1538 loss: 1.76260596e-06
Iter: 1539 loss: 1.76161711e-06
Iter: 1540 loss: 1.75891137e-06
Iter: 1541 loss: 1.75892751e-06
Iter: 1542 loss: 1.75665787e-06
Iter: 1543 loss: 1.76128822e-06
Iter: 1544 loss: 1.7557943e-06
Iter: 1545 loss: 1.75371815e-06
Iter: 1546 loss: 1.75125751e-06
Iter: 1547 loss: 1.75094249e-06
Iter: 1548 loss: 1.74926822e-06
Iter: 1549 loss: 1.74878903e-06
Iter: 1550 loss: 1.74713614e-06
Iter: 1551 loss: 1.74480817e-06
Iter: 1552 loss: 1.74476577e-06
Iter: 1553 loss: 1.74195691e-06
Iter: 1554 loss: 1.75235778e-06
Iter: 1555 loss: 1.74134709e-06
Iter: 1556 loss: 1.73840635e-06
Iter: 1557 loss: 1.75164723e-06
Iter: 1558 loss: 1.73783542e-06
Iter: 1559 loss: 1.73566934e-06
Iter: 1560 loss: 1.73819512e-06
Iter: 1561 loss: 1.73438116e-06
Iter: 1562 loss: 1.73196941e-06
Iter: 1563 loss: 1.73533795e-06
Iter: 1564 loss: 1.73074238e-06
Iter: 1565 loss: 1.72779733e-06
Iter: 1566 loss: 1.74798117e-06
Iter: 1567 loss: 1.72752607e-06
Iter: 1568 loss: 1.72551859e-06
Iter: 1569 loss: 1.72444311e-06
Iter: 1570 loss: 1.72356476e-06
Iter: 1571 loss: 1.72049567e-06
Iter: 1572 loss: 1.73661374e-06
Iter: 1573 loss: 1.72006264e-06
Iter: 1574 loss: 1.71712713e-06
Iter: 1575 loss: 1.72324962e-06
Iter: 1576 loss: 1.71607849e-06
Iter: 1577 loss: 1.71321449e-06
Iter: 1578 loss: 1.72088596e-06
Iter: 1579 loss: 1.71233785e-06
Iter: 1580 loss: 1.70959515e-06
Iter: 1581 loss: 1.70875933e-06
Iter: 1582 loss: 1.70710121e-06
Iter: 1583 loss: 1.70577641e-06
Iter: 1584 loss: 1.70504222e-06
Iter: 1585 loss: 1.70355668e-06
Iter: 1586 loss: 1.70255225e-06
Iter: 1587 loss: 1.70201201e-06
Iter: 1588 loss: 1.69985026e-06
Iter: 1589 loss: 1.70558474e-06
Iter: 1590 loss: 1.69907605e-06
Iter: 1591 loss: 1.69655652e-06
Iter: 1592 loss: 1.70553665e-06
Iter: 1593 loss: 1.6959068e-06
Iter: 1594 loss: 1.69385703e-06
Iter: 1595 loss: 1.69154532e-06
Iter: 1596 loss: 1.69122677e-06
Iter: 1597 loss: 1.68808378e-06
Iter: 1598 loss: 1.71721081e-06
Iter: 1599 loss: 1.68794259e-06
Iter: 1600 loss: 1.68536064e-06
Iter: 1601 loss: 1.69652367e-06
Iter: 1602 loss: 1.68481881e-06
Iter: 1603 loss: 1.68266206e-06
Iter: 1604 loss: 1.68492306e-06
Iter: 1605 loss: 1.68141219e-06
Iter: 1606 loss: 1.67928374e-06
Iter: 1607 loss: 1.68448196e-06
Iter: 1608 loss: 1.67857866e-06
Iter: 1609 loss: 1.67576241e-06
Iter: 1610 loss: 1.6827787e-06
Iter: 1611 loss: 1.67482358e-06
Iter: 1612 loss: 1.67247993e-06
Iter: 1613 loss: 1.67372161e-06
Iter: 1614 loss: 1.67092776e-06
Iter: 1615 loss: 1.66802556e-06
Iter: 1616 loss: 1.6901339e-06
Iter: 1617 loss: 1.66787368e-06
Iter: 1618 loss: 1.66542668e-06
Iter: 1619 loss: 1.66685152e-06
Iter: 1620 loss: 1.66380028e-06
Iter: 1621 loss: 1.66105076e-06
Iter: 1622 loss: 1.68001031e-06
Iter: 1623 loss: 1.66080872e-06
Iter: 1624 loss: 1.65828487e-06
Iter: 1625 loss: 1.66884683e-06
Iter: 1626 loss: 1.65775396e-06
Iter: 1627 loss: 1.65604695e-06
Iter: 1628 loss: 1.65395522e-06
Iter: 1629 loss: 1.65368863e-06
Iter: 1630 loss: 1.65096242e-06
Iter: 1631 loss: 1.66024131e-06
Iter: 1632 loss: 1.65020356e-06
Iter: 1633 loss: 1.64730773e-06
Iter: 1634 loss: 1.66013137e-06
Iter: 1635 loss: 1.64672565e-06
Iter: 1636 loss: 1.64423386e-06
Iter: 1637 loss: 1.66316795e-06
Iter: 1638 loss: 1.64402843e-06
Iter: 1639 loss: 1.6423802e-06
Iter: 1640 loss: 1.63980542e-06
Iter: 1641 loss: 1.63969207e-06
Iter: 1642 loss: 1.63653226e-06
Iter: 1643 loss: 1.65650658e-06
Iter: 1644 loss: 1.63613993e-06
Iter: 1645 loss: 1.63329457e-06
Iter: 1646 loss: 1.64881601e-06
Iter: 1647 loss: 1.63288485e-06
Iter: 1648 loss: 1.63053858e-06
Iter: 1649 loss: 1.63040932e-06
Iter: 1650 loss: 1.62862375e-06
Iter: 1651 loss: 1.62597917e-06
Iter: 1652 loss: 1.65656843e-06
Iter: 1653 loss: 1.62597007e-06
Iter: 1654 loss: 1.62413153e-06
Iter: 1655 loss: 1.62492813e-06
Iter: 1656 loss: 1.62290439e-06
Iter: 1657 loss: 1.6204599e-06
Iter: 1658 loss: 1.62602805e-06
Iter: 1659 loss: 1.61951812e-06
Iter: 1660 loss: 1.6169729e-06
Iter: 1661 loss: 1.64080177e-06
Iter: 1662 loss: 1.61682192e-06
Iter: 1663 loss: 1.61548724e-06
Iter: 1664 loss: 1.61699143e-06
Iter: 1665 loss: 1.61473884e-06
Iter: 1666 loss: 1.61252e-06
Iter: 1667 loss: 1.6104176e-06
Iter: 1668 loss: 1.60986713e-06
Iter: 1669 loss: 1.60708601e-06
Iter: 1670 loss: 1.62286858e-06
Iter: 1671 loss: 1.60672698e-06
Iter: 1672 loss: 1.60434593e-06
Iter: 1673 loss: 1.6038266e-06
Iter: 1674 loss: 1.60234345e-06
Iter: 1675 loss: 1.59934257e-06
Iter: 1676 loss: 1.61835339e-06
Iter: 1677 loss: 1.59895205e-06
Iter: 1678 loss: 1.59655337e-06
Iter: 1679 loss: 1.60608874e-06
Iter: 1680 loss: 1.5960261e-06
Iter: 1681 loss: 1.59357865e-06
Iter: 1682 loss: 1.60625746e-06
Iter: 1683 loss: 1.59318904e-06
Iter: 1684 loss: 1.59108731e-06
Iter: 1685 loss: 1.59362185e-06
Iter: 1686 loss: 1.59007504e-06
Iter: 1687 loss: 1.58790499e-06
Iter: 1688 loss: 1.59598108e-06
Iter: 1689 loss: 1.58734406e-06
Iter: 1690 loss: 1.58489343e-06
Iter: 1691 loss: 1.58888565e-06
Iter: 1692 loss: 1.58383955e-06
Iter: 1693 loss: 1.58186958e-06
Iter: 1694 loss: 1.58987916e-06
Iter: 1695 loss: 1.58141233e-06
Iter: 1696 loss: 1.57929026e-06
Iter: 1697 loss: 1.58766807e-06
Iter: 1698 loss: 1.5787457e-06
Iter: 1699 loss: 1.57688589e-06
Iter: 1700 loss: 1.57983447e-06
Iter: 1701 loss: 1.57603699e-06
Iter: 1702 loss: 1.57377497e-06
Iter: 1703 loss: 1.57965712e-06
Iter: 1704 loss: 1.57304021e-06
Iter: 1705 loss: 1.57132081e-06
Iter: 1706 loss: 1.57271495e-06
Iter: 1707 loss: 1.57027534e-06
Iter: 1708 loss: 1.56806504e-06
Iter: 1709 loss: 1.57908721e-06
Iter: 1710 loss: 1.56763258e-06
Iter: 1711 loss: 1.56566216e-06
Iter: 1712 loss: 1.5670546e-06
Iter: 1713 loss: 1.56441058e-06
Iter: 1714 loss: 1.56205431e-06
Iter: 1715 loss: 1.56473834e-06
Iter: 1716 loss: 1.56079477e-06
Iter: 1717 loss: 1.55814689e-06
Iter: 1718 loss: 1.55889461e-06
Iter: 1719 loss: 1.55632392e-06
Iter: 1720 loss: 1.55363307e-06
Iter: 1721 loss: 1.5536159e-06
Iter: 1722 loss: 1.55175098e-06
Iter: 1723 loss: 1.55876e-06
Iter: 1724 loss: 1.55125167e-06
Iter: 1725 loss: 1.54943473e-06
Iter: 1726 loss: 1.54906479e-06
Iter: 1727 loss: 1.54786687e-06
Iter: 1728 loss: 1.54516033e-06
Iter: 1729 loss: 1.56303156e-06
Iter: 1730 loss: 1.54490522e-06
Iter: 1731 loss: 1.54273084e-06
Iter: 1732 loss: 1.54439317e-06
Iter: 1733 loss: 1.54143754e-06
Iter: 1734 loss: 1.53930773e-06
Iter: 1735 loss: 1.54766315e-06
Iter: 1736 loss: 1.53879421e-06
Iter: 1737 loss: 1.53628628e-06
Iter: 1738 loss: 1.54043175e-06
Iter: 1739 loss: 1.53519386e-06
Iter: 1740 loss: 1.53287897e-06
Iter: 1741 loss: 1.54521615e-06
Iter: 1742 loss: 1.53257702e-06
Iter: 1743 loss: 1.53092878e-06
Iter: 1744 loss: 1.54114718e-06
Iter: 1745 loss: 1.53075166e-06
Iter: 1746 loss: 1.52925156e-06
Iter: 1747 loss: 1.52908774e-06
Iter: 1748 loss: 1.52794087e-06
Iter: 1749 loss: 1.52588927e-06
Iter: 1750 loss: 1.53603719e-06
Iter: 1751 loss: 1.52555958e-06
Iter: 1752 loss: 1.52368966e-06
Iter: 1753 loss: 1.52408e-06
Iter: 1754 loss: 1.52232599e-06
Iter: 1755 loss: 1.5199081e-06
Iter: 1756 loss: 1.52027428e-06
Iter: 1757 loss: 1.51799497e-06
Iter: 1758 loss: 1.5151287e-06
Iter: 1759 loss: 1.52440145e-06
Iter: 1760 loss: 1.51433801e-06
Iter: 1761 loss: 1.511866e-06
Iter: 1762 loss: 1.53959604e-06
Iter: 1763 loss: 1.51179688e-06
Iter: 1764 loss: 1.50952951e-06
Iter: 1765 loss: 1.5120421e-06
Iter: 1766 loss: 1.50843653e-06
Iter: 1767 loss: 1.50623691e-06
Iter: 1768 loss: 1.51502832e-06
Iter: 1769 loss: 1.50583173e-06
Iter: 1770 loss: 1.50390463e-06
Iter: 1771 loss: 1.51079723e-06
Iter: 1772 loss: 1.5033869e-06
Iter: 1773 loss: 1.50152118e-06
Iter: 1774 loss: 1.50117205e-06
Iter: 1775 loss: 1.4999473e-06
Iter: 1776 loss: 1.49759023e-06
Iter: 1777 loss: 1.51762208e-06
Iter: 1778 loss: 1.49747393e-06
Iter: 1779 loss: 1.49542643e-06
Iter: 1780 loss: 1.49642699e-06
Iter: 1781 loss: 1.49406708e-06
Iter: 1782 loss: 1.49191601e-06
Iter: 1783 loss: 1.50578489e-06
Iter: 1784 loss: 1.49170364e-06
Iter: 1785 loss: 1.48995e-06
Iter: 1786 loss: 1.49797711e-06
Iter: 1787 loss: 1.4896284e-06
Iter: 1788 loss: 1.48802553e-06
Iter: 1789 loss: 1.4896566e-06
Iter: 1790 loss: 1.48718186e-06
Iter: 1791 loss: 1.48510071e-06
Iter: 1792 loss: 1.48980348e-06
Iter: 1793 loss: 1.4842833e-06
Iter: 1794 loss: 1.48224694e-06
Iter: 1795 loss: 1.48294089e-06
Iter: 1796 loss: 1.48081608e-06
Iter: 1797 loss: 1.47835544e-06
Iter: 1798 loss: 1.4795354e-06
Iter: 1799 loss: 1.47675905e-06
Iter: 1800 loss: 1.47354558e-06
Iter: 1801 loss: 1.48942604e-06
Iter: 1802 loss: 1.47300284e-06
Iter: 1803 loss: 1.47069045e-06
Iter: 1804 loss: 1.48413233e-06
Iter: 1805 loss: 1.47043829e-06
Iter: 1806 loss: 1.46828143e-06
Iter: 1807 loss: 1.47791411e-06
Iter: 1808 loss: 1.46780837e-06
Iter: 1809 loss: 1.46608431e-06
Iter: 1810 loss: 1.46622608e-06
Iter: 1811 loss: 1.46476918e-06
Iter: 1812 loss: 1.46247874e-06
Iter: 1813 loss: 1.48251229e-06
Iter: 1814 loss: 1.46228e-06
Iter: 1815 loss: 1.46075649e-06
Iter: 1816 loss: 1.45936599e-06
Iter: 1817 loss: 1.4589607e-06
Iter: 1818 loss: 1.45637262e-06
Iter: 1819 loss: 1.47579135e-06
Iter: 1820 loss: 1.45611853e-06
Iter: 1821 loss: 1.45409012e-06
Iter: 1822 loss: 1.45970307e-06
Iter: 1823 loss: 1.45345973e-06
Iter: 1824 loss: 1.45168087e-06
Iter: 1825 loss: 1.45741672e-06
Iter: 1826 loss: 1.45116644e-06
Iter: 1827 loss: 1.44904311e-06
Iter: 1828 loss: 1.45456397e-06
Iter: 1829 loss: 1.44836008e-06
Iter: 1830 loss: 1.44665114e-06
Iter: 1831 loss: 1.45156059e-06
Iter: 1832 loss: 1.4460993e-06
Iter: 1833 loss: 1.44439298e-06
Iter: 1834 loss: 1.44863589e-06
Iter: 1835 loss: 1.44374371e-06
Iter: 1836 loss: 1.441942e-06
Iter: 1837 loss: 1.44116075e-06
Iter: 1838 loss: 1.44025171e-06
Iter: 1839 loss: 1.43767011e-06
Iter: 1840 loss: 1.44334228e-06
Iter: 1841 loss: 1.43667774e-06
Iter: 1842 loss: 1.43410693e-06
Iter: 1843 loss: 1.44066371e-06
Iter: 1844 loss: 1.43322757e-06
Iter: 1845 loss: 1.43095303e-06
Iter: 1846 loss: 1.45202807e-06
Iter: 1847 loss: 1.43082229e-06
Iter: 1848 loss: 1.42893225e-06
Iter: 1849 loss: 1.43234388e-06
Iter: 1850 loss: 1.42816361e-06
Iter: 1851 loss: 1.42588954e-06
Iter: 1852 loss: 1.4277058e-06
Iter: 1853 loss: 1.42447459e-06
Iter: 1854 loss: 1.42259353e-06
Iter: 1855 loss: 1.44930175e-06
Iter: 1856 loss: 1.42262525e-06
Iter: 1857 loss: 1.42110855e-06
Iter: 1858 loss: 1.41976523e-06
Iter: 1859 loss: 1.41945384e-06
Iter: 1860 loss: 1.41725081e-06
Iter: 1861 loss: 1.4352463e-06
Iter: 1862 loss: 1.41709734e-06
Iter: 1863 loss: 1.41539181e-06
Iter: 1864 loss: 1.41980672e-06
Iter: 1865 loss: 1.4148369e-06
Iter: 1866 loss: 1.41314911e-06
Iter: 1867 loss: 1.42029376e-06
Iter: 1868 loss: 1.41285773e-06
Iter: 1869 loss: 1.41127373e-06
Iter: 1870 loss: 1.41382839e-06
Iter: 1871 loss: 1.41053761e-06
Iter: 1872 loss: 1.40890211e-06
Iter: 1873 loss: 1.4088231e-06
Iter: 1874 loss: 1.40753241e-06
Iter: 1875 loss: 1.40566215e-06
Iter: 1876 loss: 1.42609e-06
Iter: 1877 loss: 1.40560246e-06
Iter: 1878 loss: 1.4040013e-06
Iter: 1879 loss: 1.40222346e-06
Iter: 1880 loss: 1.40193924e-06
Iter: 1881 loss: 1.39953863e-06
Iter: 1882 loss: 1.40708744e-06
Iter: 1883 loss: 1.39888959e-06
Iter: 1884 loss: 1.39665394e-06
Iter: 1885 loss: 1.40036479e-06
Iter: 1886 loss: 1.39559143e-06
Iter: 1887 loss: 1.39318263e-06
Iter: 1888 loss: 1.4137355e-06
Iter: 1889 loss: 1.39309009e-06
Iter: 1890 loss: 1.39129133e-06
Iter: 1891 loss: 1.3970083e-06
Iter: 1892 loss: 1.39077633e-06
Iter: 1893 loss: 1.38900714e-06
Iter: 1894 loss: 1.38905091e-06
Iter: 1895 loss: 1.38757537e-06
Iter: 1896 loss: 1.38568e-06
Iter: 1897 loss: 1.4081354e-06
Iter: 1898 loss: 1.38565065e-06
Iter: 1899 loss: 1.38409496e-06
Iter: 1900 loss: 1.38426856e-06
Iter: 1901 loss: 1.38291603e-06
Iter: 1902 loss: 1.3809921e-06
Iter: 1903 loss: 1.39480289e-06
Iter: 1904 loss: 1.38082476e-06
Iter: 1905 loss: 1.37913923e-06
Iter: 1906 loss: 1.38268911e-06
Iter: 1907 loss: 1.37848292e-06
Iter: 1908 loss: 1.37677398e-06
Iter: 1909 loss: 1.38067708e-06
Iter: 1910 loss: 1.37606867e-06
Iter: 1911 loss: 1.3741062e-06
Iter: 1912 loss: 1.37770257e-06
Iter: 1913 loss: 1.37328732e-06
Iter: 1914 loss: 1.37135612e-06
Iter: 1915 loss: 1.37317306e-06
Iter: 1916 loss: 1.37030679e-06
Iter: 1917 loss: 1.36869767e-06
Iter: 1918 loss: 1.38437395e-06
Iter: 1919 loss: 1.3686456e-06
Iter: 1920 loss: 1.36706262e-06
Iter: 1921 loss: 1.36739982e-06
Iter: 1922 loss: 1.36594895e-06
Iter: 1923 loss: 1.36393362e-06
Iter: 1924 loss: 1.3641469e-06
Iter: 1925 loss: 1.362386e-06
Iter: 1926 loss: 1.35990126e-06
Iter: 1927 loss: 1.3668332e-06
Iter: 1928 loss: 1.35902224e-06
Iter: 1929 loss: 1.35693733e-06
Iter: 1930 loss: 1.37956613e-06
Iter: 1931 loss: 1.3569354e-06
Iter: 1932 loss: 1.3552127e-06
Iter: 1933 loss: 1.35716687e-06
Iter: 1934 loss: 1.35423488e-06
Iter: 1935 loss: 1.3525023e-06
Iter: 1936 loss: 1.35621133e-06
Iter: 1937 loss: 1.35176538e-06
Iter: 1938 loss: 1.34984407e-06
Iter: 1939 loss: 1.35889661e-06
Iter: 1940 loss: 1.349493e-06
Iter: 1941 loss: 1.34761831e-06
Iter: 1942 loss: 1.35129244e-06
Iter: 1943 loss: 1.34683148e-06
Iter: 1944 loss: 1.34496383e-06
Iter: 1945 loss: 1.35681262e-06
Iter: 1946 loss: 1.34477568e-06
Iter: 1947 loss: 1.34307379e-06
Iter: 1948 loss: 1.34473635e-06
Iter: 1949 loss: 1.34214588e-06
Iter: 1950 loss: 1.34050117e-06
Iter: 1951 loss: 1.34523941e-06
Iter: 1952 loss: 1.33998287e-06
Iter: 1953 loss: 1.33828121e-06
Iter: 1954 loss: 1.34323147e-06
Iter: 1955 loss: 1.33772176e-06
Iter: 1956 loss: 1.33624098e-06
Iter: 1957 loss: 1.33587423e-06
Iter: 1958 loss: 1.33490562e-06
Iter: 1959 loss: 1.33277285e-06
Iter: 1960 loss: 1.34392462e-06
Iter: 1961 loss: 1.33241349e-06
Iter: 1962 loss: 1.33067101e-06
Iter: 1963 loss: 1.34417473e-06
Iter: 1964 loss: 1.33052231e-06
Iter: 1965 loss: 1.32926971e-06
Iter: 1966 loss: 1.32757464e-06
Iter: 1967 loss: 1.32752984e-06
Iter: 1968 loss: 1.32516925e-06
Iter: 1969 loss: 1.32973946e-06
Iter: 1970 loss: 1.32418586e-06
Iter: 1971 loss: 1.32176092e-06
Iter: 1972 loss: 1.34135485e-06
Iter: 1973 loss: 1.32159312e-06
Iter: 1974 loss: 1.31997331e-06
Iter: 1975 loss: 1.32956848e-06
Iter: 1976 loss: 1.31980278e-06
Iter: 1977 loss: 1.31833087e-06
Iter: 1978 loss: 1.31628076e-06
Iter: 1979 loss: 1.3162005e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2.8
+ date
Wed Nov  4 16:15:37 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.4/300_300_300_1 --function f2 --psi 3 --alpha 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3dae009d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3dae126a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3b9843510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3b984b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3b984bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3b983c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3b97d5ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3b97be268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3b97be598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3806176a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff380617ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff380617158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff38067d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff38067dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff394064bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff394064b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff394064e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3805c5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3805fc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3805c5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3805c5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3804ff378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3805c5ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3804c80d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3804c8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3803dda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3803e02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3b984b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3803970d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff380397620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff380359268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff380359378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff380357950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3803596a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3802e2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3802f8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.008084032
test_loss: 0.010508617
train_loss: 0.0069341
test_loss: 0.009969144
train_loss: 0.006092714
test_loss: 0.0098213395
train_loss: 0.0066456255
test_loss: 0.009985905
train_loss: 0.0062555983
test_loss: 0.009638023
train_loss: 0.0057349806
test_loss: 0.009424394
train_loss: 0.0056510614
test_loss: 0.009597507
train_loss: 0.005917128
test_loss: 0.009580135
train_loss: 0.0063653262
test_loss: 0.009660085
train_loss: 0.0065362537
test_loss: 0.009358633
train_loss: 0.00569129
test_loss: 0.009373302
train_loss: 0.005923785
test_loss: 0.009271212
train_loss: 0.0054059746
test_loss: 0.009398622
train_loss: 0.0056407857
test_loss: 0.009585746
train_loss: 0.005938129
test_loss: 0.009207248
train_loss: 0.0059585515
test_loss: 0.0094602285
train_loss: 0.0056522433
test_loss: 0.009551192
train_loss: 0.0058113197
test_loss: 0.009326677
train_loss: 0.005488245
test_loss: 0.0091323
train_loss: 0.005639436
test_loss: 0.009258102
train_loss: 0.0061190273
test_loss: 0.009177459
train_loss: 0.005651986
test_loss: 0.009439124
train_loss: 0.0055106278
test_loss: 0.009207225
train_loss: 0.005680945
test_loss: 0.009496354
train_loss: 0.0051242705
test_loss: 0.009209224
train_loss: 0.0062748063
test_loss: 0.0092048915
train_loss: 0.0051587075
test_loss: 0.0090034995
train_loss: 0.0059940056
test_loss: 0.009242899
train_loss: 0.0053891744
test_loss: 0.009185521
train_loss: 0.0053064153
test_loss: 0.0092122
train_loss: 0.005214592
test_loss: 0.009028161
train_loss: 0.005023626
test_loss: 0.009156305
train_loss: 0.005331132
test_loss: 0.009264465
train_loss: 0.005648353
test_loss: 0.009076336
train_loss: 0.005473314
test_loss: 0.009083294
train_loss: 0.005439216
test_loss: 0.009348758
train_loss: 0.005356755
test_loss: 0.009043672
train_loss: 0.005360962
test_loss: 0.009139839
train_loss: 0.00589398
test_loss: 0.0090383
train_loss: 0.0053759967
test_loss: 0.0092652505
train_loss: 0.004908934
test_loss: 0.009179829
train_loss: 0.005653726
test_loss: 0.008892167
train_loss: 0.0055778963
test_loss: 0.009142839
train_loss: 0.0052323886
test_loss: 0.009260065
train_loss: 0.0057434123
test_loss: 0.009200336
train_loss: 0.004970584
test_loss: 0.008904074
train_loss: 0.005421578
test_loss: 0.009027799
train_loss: 0.0049152784
test_loss: 0.008981787
train_loss: 0.0053033894
test_loss: 0.0090975035
train_loss: 0.0057674036
test_loss: 0.008912352
train_loss: 0.005381832
test_loss: 0.009017977
train_loss: 0.004974058
test_loss: 0.008922265
train_loss: 0.004875015
test_loss: 0.009028189
train_loss: 0.0054335743
test_loss: 0.009021448
train_loss: 0.004924628
test_loss: 0.008851746
train_loss: 0.00524911
test_loss: 0.009025073
train_loss: 0.0046808957
test_loss: 0.008855974
train_loss: 0.004999333
test_loss: 0.009071581
train_loss: 0.004907628
test_loss: 0.008892176
train_loss: 0.0048123486
test_loss: 0.008905307
train_loss: 0.004765188
test_loss: 0.009056234
train_loss: 0.004926048
test_loss: 0.008978439
train_loss: 0.0047740303
test_loss: 0.008803917
train_loss: 0.005080806
test_loss: 0.008922488
train_loss: 0.0048531084
test_loss: 0.009012895
train_loss: 0.005347461
test_loss: 0.008908216
train_loss: 0.004813179
test_loss: 0.008929821
train_loss: 0.0049772062
test_loss: 0.008950428
train_loss: 0.004992336
test_loss: 0.0089317355
train_loss: 0.005206681
test_loss: 0.008943197
train_loss: 0.0050206003
test_loss: 0.0088355
train_loss: 0.0050156703
test_loss: 0.008799292
train_loss: 0.004670352
test_loss: 0.008816193
train_loss: 0.005156093
test_loss: 0.009083467
train_loss: 0.004690419
test_loss: 0.008924904
train_loss: 0.0052458723
test_loss: 0.009044805
train_loss: 0.0050268993
test_loss: 0.008964608
train_loss: 0.0048495
test_loss: 0.008816407
train_loss: 0.004733622
test_loss: 0.008948869
train_loss: 0.005175284
test_loss: 0.008906806
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.8/300_300_300_1 --optimizer lbfgs --function f2 --psi 3 --alpha 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801cff510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801cff950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801beb268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801ba42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801ba4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801ba4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801b7ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801b37378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801b276a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801b27510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801ad7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801b37400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97e00e5d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9801afb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97e00d0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97e008e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97e002a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97e0030840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dffe7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97e00301e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dffc9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dffc96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dff806a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dff80840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dff80488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dff37d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dff1c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dff58d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dfeb92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dfeb96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dff58e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dff5d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dfe53b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dff5dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dfdf6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97dfe0c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.39372668e-05
Iter: 2 loss: 4.91184292e-05
Iter: 3 loss: 4.40158037e-05
Iter: 4 loss: 4.3144435e-05
Iter: 5 loss: 3.61793e-05
Iter: 6 loss: 6.27974368e-05
Iter: 7 loss: 3.44566e-05
Iter: 8 loss: 2.99990388e-05
Iter: 9 loss: 4.60995689e-05
Iter: 10 loss: 2.88795291e-05
Iter: 11 loss: 2.62112044e-05
Iter: 12 loss: 4.89687664e-05
Iter: 13 loss: 2.60598808e-05
Iter: 14 loss: 2.42664428e-05
Iter: 15 loss: 2.37704153e-05
Iter: 16 loss: 2.26744851e-05
Iter: 17 loss: 2.0690808e-05
Iter: 18 loss: 3.72398717e-05
Iter: 19 loss: 2.05678e-05
Iter: 20 loss: 1.92074713e-05
Iter: 21 loss: 2.32369603e-05
Iter: 22 loss: 1.87933965e-05
Iter: 23 loss: 1.7622544e-05
Iter: 24 loss: 2.0533389e-05
Iter: 25 loss: 1.72121145e-05
Iter: 26 loss: 1.62008037e-05
Iter: 27 loss: 2.29562884e-05
Iter: 28 loss: 1.60970703e-05
Iter: 29 loss: 1.54158806e-05
Iter: 30 loss: 1.58276816e-05
Iter: 31 loss: 1.49777616e-05
Iter: 32 loss: 1.43294601e-05
Iter: 33 loss: 1.65709371e-05
Iter: 34 loss: 1.41600303e-05
Iter: 35 loss: 1.36093786e-05
Iter: 36 loss: 1.79988856e-05
Iter: 37 loss: 1.35718947e-05
Iter: 38 loss: 1.31770348e-05
Iter: 39 loss: 1.36586777e-05
Iter: 40 loss: 1.29705259e-05
Iter: 41 loss: 1.25666611e-05
Iter: 42 loss: 1.34127022e-05
Iter: 43 loss: 1.24067128e-05
Iter: 44 loss: 1.21658741e-05
Iter: 45 loss: 1.21596368e-05
Iter: 46 loss: 1.19428196e-05
Iter: 47 loss: 1.17573582e-05
Iter: 48 loss: 1.16981928e-05
Iter: 49 loss: 1.14316399e-05
Iter: 50 loss: 1.27580133e-05
Iter: 51 loss: 1.13865663e-05
Iter: 52 loss: 1.11521258e-05
Iter: 53 loss: 1.23746959e-05
Iter: 54 loss: 1.11150994e-05
Iter: 55 loss: 1.09308312e-05
Iter: 56 loss: 1.09363245e-05
Iter: 57 loss: 1.07850665e-05
Iter: 58 loss: 1.05892032e-05
Iter: 59 loss: 1.15720977e-05
Iter: 60 loss: 1.05566114e-05
Iter: 61 loss: 1.03767234e-05
Iter: 62 loss: 1.12948746e-05
Iter: 63 loss: 1.0347695e-05
Iter: 64 loss: 1.01978403e-05
Iter: 65 loss: 1.0174288e-05
Iter: 66 loss: 1.00709485e-05
Iter: 67 loss: 9.89597538e-06
Iter: 68 loss: 1.08842141e-05
Iter: 69 loss: 9.87176281e-06
Iter: 70 loss: 9.7224929e-06
Iter: 71 loss: 1.03648945e-05
Iter: 72 loss: 9.6919448e-06
Iter: 73 loss: 9.57298653e-06
Iter: 74 loss: 9.54778e-06
Iter: 75 loss: 9.46977525e-06
Iter: 76 loss: 9.34759282e-06
Iter: 77 loss: 1.10833807e-05
Iter: 78 loss: 9.34732725e-06
Iter: 79 loss: 9.25027325e-06
Iter: 80 loss: 9.45786633e-06
Iter: 81 loss: 9.21216633e-06
Iter: 82 loss: 9.1075517e-06
Iter: 83 loss: 9.55724317e-06
Iter: 84 loss: 9.08574748e-06
Iter: 85 loss: 9.00305622e-06
Iter: 86 loss: 9.17901798e-06
Iter: 87 loss: 8.97041355e-06
Iter: 88 loss: 8.88825252e-06
Iter: 89 loss: 8.91272248e-06
Iter: 90 loss: 8.82931545e-06
Iter: 91 loss: 8.73292265e-06
Iter: 92 loss: 9.57247266e-06
Iter: 93 loss: 8.72799865e-06
Iter: 94 loss: 8.65123366e-06
Iter: 95 loss: 8.72664805e-06
Iter: 96 loss: 8.60763066e-06
Iter: 97 loss: 8.52672929e-06
Iter: 98 loss: 8.56015595e-06
Iter: 99 loss: 8.47089723e-06
Iter: 100 loss: 8.38770575e-06
Iter: 101 loss: 9.33476258e-06
Iter: 102 loss: 8.38640153e-06
Iter: 103 loss: 8.3166251e-06
Iter: 104 loss: 8.42428381e-06
Iter: 105 loss: 8.28371049e-06
Iter: 106 loss: 8.21660069e-06
Iter: 107 loss: 8.25067e-06
Iter: 108 loss: 8.1717053e-06
Iter: 109 loss: 8.0944119e-06
Iter: 110 loss: 8.36828804e-06
Iter: 111 loss: 8.07481683e-06
Iter: 112 loss: 8.00586076e-06
Iter: 113 loss: 8.4936064e-06
Iter: 114 loss: 7.99967e-06
Iter: 115 loss: 7.94090556e-06
Iter: 116 loss: 8.03096e-06
Iter: 117 loss: 7.91312232e-06
Iter: 118 loss: 7.86693636e-06
Iter: 119 loss: 8.46415423e-06
Iter: 120 loss: 7.86658529e-06
Iter: 121 loss: 7.82413917e-06
Iter: 122 loss: 7.76880916e-06
Iter: 123 loss: 7.76544402e-06
Iter: 124 loss: 7.70601582e-06
Iter: 125 loss: 8.08016e-06
Iter: 126 loss: 7.69925555e-06
Iter: 127 loss: 7.64788638e-06
Iter: 128 loss: 7.79136553e-06
Iter: 129 loss: 7.6312981e-06
Iter: 130 loss: 7.57951375e-06
Iter: 131 loss: 7.69883445e-06
Iter: 132 loss: 7.56031295e-06
Iter: 133 loss: 7.51410244e-06
Iter: 134 loss: 7.71311807e-06
Iter: 135 loss: 7.50483787e-06
Iter: 136 loss: 7.45966463e-06
Iter: 137 loss: 7.47955346e-06
Iter: 138 loss: 7.42860266e-06
Iter: 139 loss: 7.38040444e-06
Iter: 140 loss: 7.50897516e-06
Iter: 141 loss: 7.36438187e-06
Iter: 142 loss: 7.31683122e-06
Iter: 143 loss: 7.73071406e-06
Iter: 144 loss: 7.31442378e-06
Iter: 145 loss: 7.27840143e-06
Iter: 146 loss: 7.25995233e-06
Iter: 147 loss: 7.24314486e-06
Iter: 148 loss: 7.18914725e-06
Iter: 149 loss: 7.3277788e-06
Iter: 150 loss: 7.17055264e-06
Iter: 151 loss: 7.12173915e-06
Iter: 152 loss: 7.29646035e-06
Iter: 153 loss: 7.1090285e-06
Iter: 154 loss: 7.07308754e-06
Iter: 155 loss: 7.61742558e-06
Iter: 156 loss: 7.07304025e-06
Iter: 157 loss: 7.04394188e-06
Iter: 158 loss: 7.04242575e-06
Iter: 159 loss: 7.02042053e-06
Iter: 160 loss: 6.98191707e-06
Iter: 161 loss: 7.13932059e-06
Iter: 162 loss: 6.97357154e-06
Iter: 163 loss: 6.94072196e-06
Iter: 164 loss: 6.97263204e-06
Iter: 165 loss: 6.92217827e-06
Iter: 166 loss: 6.88730825e-06
Iter: 167 loss: 6.9788166e-06
Iter: 168 loss: 6.87568127e-06
Iter: 169 loss: 6.83638063e-06
Iter: 170 loss: 7.05735965e-06
Iter: 171 loss: 6.8309655e-06
Iter: 172 loss: 6.80383027e-06
Iter: 173 loss: 6.79627647e-06
Iter: 174 loss: 6.7797705e-06
Iter: 175 loss: 6.74203875e-06
Iter: 176 loss: 6.96541974e-06
Iter: 177 loss: 6.73718159e-06
Iter: 178 loss: 6.70483951e-06
Iter: 179 loss: 6.77931348e-06
Iter: 180 loss: 6.6928751e-06
Iter: 181 loss: 6.66092365e-06
Iter: 182 loss: 6.70578629e-06
Iter: 183 loss: 6.64524578e-06
Iter: 184 loss: 6.60982414e-06
Iter: 185 loss: 6.89653962e-06
Iter: 186 loss: 6.60754631e-06
Iter: 187 loss: 6.58425051e-06
Iter: 188 loss: 6.5463887e-06
Iter: 189 loss: 6.54611358e-06
Iter: 190 loss: 6.50942775e-06
Iter: 191 loss: 7.02908437e-06
Iter: 192 loss: 6.50929769e-06
Iter: 193 loss: 6.4869887e-06
Iter: 194 loss: 6.68631674e-06
Iter: 195 loss: 6.48590139e-06
Iter: 196 loss: 6.46314129e-06
Iter: 197 loss: 6.42907298e-06
Iter: 198 loss: 6.4283131e-06
Iter: 199 loss: 6.39919881e-06
Iter: 200 loss: 6.63073388e-06
Iter: 201 loss: 6.39717928e-06
Iter: 202 loss: 6.36819823e-06
Iter: 203 loss: 6.4002561e-06
Iter: 204 loss: 6.35248398e-06
Iter: 205 loss: 6.32721822e-06
Iter: 206 loss: 6.43992462e-06
Iter: 207 loss: 6.32234e-06
Iter: 208 loss: 6.29605074e-06
Iter: 209 loss: 6.36144887e-06
Iter: 210 loss: 6.28669659e-06
Iter: 211 loss: 6.26053406e-06
Iter: 212 loss: 6.30759314e-06
Iter: 213 loss: 6.24927543e-06
Iter: 214 loss: 6.22456355e-06
Iter: 215 loss: 6.25409484e-06
Iter: 216 loss: 6.21158415e-06
Iter: 217 loss: 6.18280637e-06
Iter: 218 loss: 6.3695893e-06
Iter: 219 loss: 6.17961814e-06
Iter: 220 loss: 6.1537994e-06
Iter: 221 loss: 6.18079048e-06
Iter: 222 loss: 6.13962629e-06
Iter: 223 loss: 6.11249652e-06
Iter: 224 loss: 6.22443486e-06
Iter: 225 loss: 6.10654797e-06
Iter: 226 loss: 6.08093569e-06
Iter: 227 loss: 6.13988686e-06
Iter: 228 loss: 6.07132324e-06
Iter: 229 loss: 6.04640263e-06
Iter: 230 loss: 6.0927905e-06
Iter: 231 loss: 6.03562376e-06
Iter: 232 loss: 6.01597822e-06
Iter: 233 loss: 6.01596321e-06
Iter: 234 loss: 5.99975374e-06
Iter: 235 loss: 5.984155e-06
Iter: 236 loss: 5.98051611e-06
Iter: 237 loss: 5.95733718e-06
Iter: 238 loss: 5.96524023e-06
Iter: 239 loss: 5.94075755e-06
Iter: 240 loss: 5.91108255e-06
Iter: 241 loss: 6.18198328e-06
Iter: 242 loss: 5.90980244e-06
Iter: 243 loss: 5.88758803e-06
Iter: 244 loss: 5.96706e-06
Iter: 245 loss: 5.88184685e-06
Iter: 246 loss: 5.86401165e-06
Iter: 247 loss: 5.87403838e-06
Iter: 248 loss: 5.85235421e-06
Iter: 249 loss: 5.82691246e-06
Iter: 250 loss: 5.96816153e-06
Iter: 251 loss: 5.82314806e-06
Iter: 252 loss: 5.80599226e-06
Iter: 253 loss: 5.78398613e-06
Iter: 254 loss: 5.78231902e-06
Iter: 255 loss: 5.75750528e-06
Iter: 256 loss: 6.11157429e-06
Iter: 257 loss: 5.75753347e-06
Iter: 258 loss: 5.73743773e-06
Iter: 259 loss: 5.77436958e-06
Iter: 260 loss: 5.72879799e-06
Iter: 261 loss: 5.70739348e-06
Iter: 262 loss: 5.73679245e-06
Iter: 263 loss: 5.69665e-06
Iter: 264 loss: 5.67704865e-06
Iter: 265 loss: 5.8016808e-06
Iter: 266 loss: 5.67481084e-06
Iter: 267 loss: 5.65734854e-06
Iter: 268 loss: 5.69218764e-06
Iter: 269 loss: 5.65026494e-06
Iter: 270 loss: 5.63226149e-06
Iter: 271 loss: 5.75007243e-06
Iter: 272 loss: 5.63050753e-06
Iter: 273 loss: 5.61406614e-06
Iter: 274 loss: 5.6048284e-06
Iter: 275 loss: 5.59766568e-06
Iter: 276 loss: 5.57603198e-06
Iter: 277 loss: 5.58717738e-06
Iter: 278 loss: 5.56164741e-06
Iter: 279 loss: 5.53492555e-06
Iter: 280 loss: 5.71492819e-06
Iter: 281 loss: 5.53231257e-06
Iter: 282 loss: 5.51121548e-06
Iter: 283 loss: 5.66032713e-06
Iter: 284 loss: 5.50930918e-06
Iter: 285 loss: 5.49548031e-06
Iter: 286 loss: 5.48767412e-06
Iter: 287 loss: 5.48148819e-06
Iter: 288 loss: 5.45921785e-06
Iter: 289 loss: 5.61306888e-06
Iter: 290 loss: 5.45706962e-06
Iter: 291 loss: 5.44175418e-06
Iter: 292 loss: 5.45828561e-06
Iter: 293 loss: 5.43318856e-06
Iter: 294 loss: 5.41288227e-06
Iter: 295 loss: 5.42999e-06
Iter: 296 loss: 5.4006191e-06
Iter: 297 loss: 5.3846361e-06
Iter: 298 loss: 5.61931029e-06
Iter: 299 loss: 5.3845979e-06
Iter: 300 loss: 5.36969856e-06
Iter: 301 loss: 5.35290201e-06
Iter: 302 loss: 5.35061781e-06
Iter: 303 loss: 5.32823e-06
Iter: 304 loss: 5.47821401e-06
Iter: 305 loss: 5.32587728e-06
Iter: 306 loss: 5.30975558e-06
Iter: 307 loss: 5.43789429e-06
Iter: 308 loss: 5.30870784e-06
Iter: 309 loss: 5.29381578e-06
Iter: 310 loss: 5.29802674e-06
Iter: 311 loss: 5.28323744e-06
Iter: 312 loss: 5.26647273e-06
Iter: 313 loss: 5.32687136e-06
Iter: 314 loss: 5.2623127e-06
Iter: 315 loss: 5.2460714e-06
Iter: 316 loss: 5.25011865e-06
Iter: 317 loss: 5.23430299e-06
Iter: 318 loss: 5.21578886e-06
Iter: 319 loss: 5.25707674e-06
Iter: 320 loss: 5.20877711e-06
Iter: 321 loss: 5.19175865e-06
Iter: 322 loss: 5.41538247e-06
Iter: 323 loss: 5.19169862e-06
Iter: 324 loss: 5.17827903e-06
Iter: 325 loss: 5.16519731e-06
Iter: 326 loss: 5.16248429e-06
Iter: 327 loss: 5.14471185e-06
Iter: 328 loss: 5.26644726e-06
Iter: 329 loss: 5.14291241e-06
Iter: 330 loss: 5.12653196e-06
Iter: 331 loss: 5.16787304e-06
Iter: 332 loss: 5.12060342e-06
Iter: 333 loss: 5.10623704e-06
Iter: 334 loss: 5.12219e-06
Iter: 335 loss: 5.09826896e-06
Iter: 336 loss: 5.08238e-06
Iter: 337 loss: 5.16829095e-06
Iter: 338 loss: 5.07988534e-06
Iter: 339 loss: 5.06514925e-06
Iter: 340 loss: 5.1158363e-06
Iter: 341 loss: 5.0611543e-06
Iter: 342 loss: 5.04760919e-06
Iter: 343 loss: 5.05119306e-06
Iter: 344 loss: 5.03793399e-06
Iter: 345 loss: 5.02455532e-06
Iter: 346 loss: 5.02456533e-06
Iter: 347 loss: 5.01372142e-06
Iter: 348 loss: 4.99515227e-06
Iter: 349 loss: 4.9952514e-06
Iter: 350 loss: 4.98075497e-06
Iter: 351 loss: 5.16895307e-06
Iter: 352 loss: 4.98057125e-06
Iter: 353 loss: 4.96895154e-06
Iter: 354 loss: 4.96427538e-06
Iter: 355 loss: 4.95803215e-06
Iter: 356 loss: 4.94133292e-06
Iter: 357 loss: 5.00281521e-06
Iter: 358 loss: 4.9370683e-06
Iter: 359 loss: 4.92288564e-06
Iter: 360 loss: 5.04295349e-06
Iter: 361 loss: 4.92213121e-06
Iter: 362 loss: 4.91050923e-06
Iter: 363 loss: 4.91472474e-06
Iter: 364 loss: 4.90226466e-06
Iter: 365 loss: 4.88701608e-06
Iter: 366 loss: 4.88730529e-06
Iter: 367 loss: 4.87476427e-06
Iter: 368 loss: 4.85963619e-06
Iter: 369 loss: 4.85949e-06
Iter: 370 loss: 4.84956536e-06
Iter: 371 loss: 4.84184238e-06
Iter: 372 loss: 4.83876602e-06
Iter: 373 loss: 4.82162795e-06
Iter: 374 loss: 4.89036e-06
Iter: 375 loss: 4.81768166e-06
Iter: 376 loss: 4.80319886e-06
Iter: 377 loss: 4.91194396e-06
Iter: 378 loss: 4.80207291e-06
Iter: 379 loss: 4.79202936e-06
Iter: 380 loss: 4.80339168e-06
Iter: 381 loss: 4.78662423e-06
Iter: 382 loss: 4.77405729e-06
Iter: 383 loss: 4.8389611e-06
Iter: 384 loss: 4.7721287e-06
Iter: 385 loss: 4.75973138e-06
Iter: 386 loss: 4.75739398e-06
Iter: 387 loss: 4.74909e-06
Iter: 388 loss: 4.73570071e-06
Iter: 389 loss: 4.75085108e-06
Iter: 390 loss: 4.72862303e-06
Iter: 391 loss: 4.71145859e-06
Iter: 392 loss: 4.77928097e-06
Iter: 393 loss: 4.70769282e-06
Iter: 394 loss: 4.69165934e-06
Iter: 395 loss: 4.76974901e-06
Iter: 396 loss: 4.68892813e-06
Iter: 397 loss: 4.67826521e-06
Iter: 398 loss: 4.70353962e-06
Iter: 399 loss: 4.67433392e-06
Iter: 400 loss: 4.66024e-06
Iter: 401 loss: 4.68997587e-06
Iter: 402 loss: 4.65480662e-06
Iter: 403 loss: 4.64277491e-06
Iter: 404 loss: 4.65024914e-06
Iter: 405 loss: 4.63513061e-06
Iter: 406 loss: 4.62063599e-06
Iter: 407 loss: 4.6804762e-06
Iter: 408 loss: 4.61739319e-06
Iter: 409 loss: 4.60383762e-06
Iter: 410 loss: 4.67437167e-06
Iter: 411 loss: 4.60168121e-06
Iter: 412 loss: 4.5896154e-06
Iter: 413 loss: 4.58740033e-06
Iter: 414 loss: 4.57931674e-06
Iter: 415 loss: 4.56719272e-06
Iter: 416 loss: 4.74691842e-06
Iter: 417 loss: 4.56718863e-06
Iter: 418 loss: 4.55720919e-06
Iter: 419 loss: 4.56344878e-06
Iter: 420 loss: 4.5507918e-06
Iter: 421 loss: 4.53915891e-06
Iter: 422 loss: 4.60793626e-06
Iter: 423 loss: 4.53771e-06
Iter: 424 loss: 4.52767381e-06
Iter: 425 loss: 4.53022949e-06
Iter: 426 loss: 4.52019685e-06
Iter: 427 loss: 4.50795324e-06
Iter: 428 loss: 4.50443758e-06
Iter: 429 loss: 4.49704066e-06
Iter: 430 loss: 4.48224637e-06
Iter: 431 loss: 4.58250361e-06
Iter: 432 loss: 4.48087212e-06
Iter: 433 loss: 4.46754575e-06
Iter: 434 loss: 4.54203882e-06
Iter: 435 loss: 4.46558397e-06
Iter: 436 loss: 4.45464411e-06
Iter: 437 loss: 4.46474678e-06
Iter: 438 loss: 4.44834e-06
Iter: 439 loss: 4.434743e-06
Iter: 440 loss: 4.49945946e-06
Iter: 441 loss: 4.43227509e-06
Iter: 442 loss: 4.42092551e-06
Iter: 443 loss: 4.42623514e-06
Iter: 444 loss: 4.41334669e-06
Iter: 445 loss: 4.40003851e-06
Iter: 446 loss: 4.42426335e-06
Iter: 447 loss: 4.39430642e-06
Iter: 448 loss: 4.38193092e-06
Iter: 449 loss: 4.53366738e-06
Iter: 450 loss: 4.38174857e-06
Iter: 451 loss: 4.37279e-06
Iter: 452 loss: 4.3647965e-06
Iter: 453 loss: 4.36243909e-06
Iter: 454 loss: 4.34821368e-06
Iter: 455 loss: 4.49046456e-06
Iter: 456 loss: 4.34787e-06
Iter: 457 loss: 4.33754758e-06
Iter: 458 loss: 4.37327435e-06
Iter: 459 loss: 4.33495916e-06
Iter: 460 loss: 4.32663091e-06
Iter: 461 loss: 4.33847708e-06
Iter: 462 loss: 4.32246543e-06
Iter: 463 loss: 4.31174067e-06
Iter: 464 loss: 4.33028845e-06
Iter: 465 loss: 4.30689579e-06
Iter: 466 loss: 4.29704733e-06
Iter: 467 loss: 4.28982867e-06
Iter: 468 loss: 4.28645535e-06
Iter: 469 loss: 4.26975839e-06
Iter: 470 loss: 4.36198707e-06
Iter: 471 loss: 4.26741417e-06
Iter: 472 loss: 4.25609642e-06
Iter: 473 loss: 4.32716797e-06
Iter: 474 loss: 4.25472945e-06
Iter: 475 loss: 4.24300561e-06
Iter: 476 loss: 4.26582756e-06
Iter: 477 loss: 4.23801248e-06
Iter: 478 loss: 4.22726362e-06
Iter: 479 loss: 4.25212647e-06
Iter: 480 loss: 4.22331368e-06
Iter: 481 loss: 4.21246477e-06
Iter: 482 loss: 4.24899554e-06
Iter: 483 loss: 4.20947254e-06
Iter: 484 loss: 4.19925436e-06
Iter: 485 loss: 4.21480581e-06
Iter: 486 loss: 4.19429807e-06
Iter: 487 loss: 4.18290665e-06
Iter: 488 loss: 4.20014976e-06
Iter: 489 loss: 4.17731735e-06
Iter: 490 loss: 4.16459898e-06
Iter: 491 loss: 4.28645944e-06
Iter: 492 loss: 4.16409512e-06
Iter: 493 loss: 4.15637714e-06
Iter: 494 loss: 4.16654439e-06
Iter: 495 loss: 4.15240083e-06
Iter: 496 loss: 4.14180067e-06
Iter: 497 loss: 4.18126729e-06
Iter: 498 loss: 4.13918133e-06
Iter: 499 loss: 4.13072848e-06
Iter: 500 loss: 4.13365069e-06
Iter: 501 loss: 4.12470399e-06
Iter: 502 loss: 4.11343399e-06
Iter: 503 loss: 4.15777777e-06
Iter: 504 loss: 4.11097153e-06
Iter: 505 loss: 4.10111807e-06
Iter: 506 loss: 4.12104509e-06
Iter: 507 loss: 4.09697714e-06
Iter: 508 loss: 4.0866903e-06
Iter: 509 loss: 4.0809191e-06
Iter: 510 loss: 4.07645666e-06
Iter: 511 loss: 4.06292565e-06
Iter: 512 loss: 4.13661519e-06
Iter: 513 loss: 4.06077652e-06
Iter: 514 loss: 4.05017727e-06
Iter: 515 loss: 4.20505739e-06
Iter: 516 loss: 4.05017e-06
Iter: 517 loss: 4.04348566e-06
Iter: 518 loss: 4.03313607e-06
Iter: 519 loss: 4.03299873e-06
Iter: 520 loss: 4.02040314e-06
Iter: 521 loss: 4.11960264e-06
Iter: 522 loss: 4.01951e-06
Iter: 523 loss: 4.00928775e-06
Iter: 524 loss: 4.02954e-06
Iter: 525 loss: 4.00509e-06
Iter: 526 loss: 3.99457531e-06
Iter: 527 loss: 4.01321086e-06
Iter: 528 loss: 3.99007058e-06
Iter: 529 loss: 3.98002567e-06
Iter: 530 loss: 4.0742284e-06
Iter: 531 loss: 3.97967233e-06
Iter: 532 loss: 3.97174472e-06
Iter: 533 loss: 3.98388693e-06
Iter: 534 loss: 3.9679162e-06
Iter: 535 loss: 3.95936786e-06
Iter: 536 loss: 3.992046e-06
Iter: 537 loss: 3.95741608e-06
Iter: 538 loss: 3.94872541e-06
Iter: 539 loss: 3.94876042e-06
Iter: 540 loss: 3.94169547e-06
Iter: 541 loss: 3.93225491e-06
Iter: 542 loss: 3.9662591e-06
Iter: 543 loss: 3.92981838e-06
Iter: 544 loss: 3.91899175e-06
Iter: 545 loss: 3.94232939e-06
Iter: 546 loss: 3.91465619e-06
Iter: 547 loss: 3.90442892e-06
Iter: 548 loss: 3.91911e-06
Iter: 549 loss: 3.89939851e-06
Iter: 550 loss: 3.88834451e-06
Iter: 551 loss: 3.90778132e-06
Iter: 552 loss: 3.88345e-06
Iter: 553 loss: 3.87129194e-06
Iter: 554 loss: 3.95831466e-06
Iter: 555 loss: 3.87024284e-06
Iter: 556 loss: 3.8603639e-06
Iter: 557 loss: 3.90491323e-06
Iter: 558 loss: 3.85850853e-06
Iter: 559 loss: 3.85170279e-06
Iter: 560 loss: 3.84235864e-06
Iter: 561 loss: 3.84187751e-06
Iter: 562 loss: 3.83116503e-06
Iter: 563 loss: 3.96021824e-06
Iter: 564 loss: 3.831095e-06
Iter: 565 loss: 3.82288226e-06
Iter: 566 loss: 3.84184523e-06
Iter: 567 loss: 3.81989958e-06
Iter: 568 loss: 3.81082145e-06
Iter: 569 loss: 3.83730367e-06
Iter: 570 loss: 3.80804954e-06
Iter: 571 loss: 3.79970879e-06
Iter: 572 loss: 3.85136582e-06
Iter: 573 loss: 3.79870562e-06
Iter: 574 loss: 3.79155608e-06
Iter: 575 loss: 3.78736513e-06
Iter: 576 loss: 3.78427035e-06
Iter: 577 loss: 3.77406332e-06
Iter: 578 loss: 3.85114026e-06
Iter: 579 loss: 3.77326865e-06
Iter: 580 loss: 3.76586013e-06
Iter: 581 loss: 3.76198886e-06
Iter: 582 loss: 3.7585562e-06
Iter: 583 loss: 3.74757724e-06
Iter: 584 loss: 3.79225753e-06
Iter: 585 loss: 3.74510046e-06
Iter: 586 loss: 3.73528223e-06
Iter: 587 loss: 3.79018866e-06
Iter: 588 loss: 3.73391367e-06
Iter: 589 loss: 3.72587e-06
Iter: 590 loss: 3.71989472e-06
Iter: 591 loss: 3.71727128e-06
Iter: 592 loss: 3.70724342e-06
Iter: 593 loss: 3.78631512e-06
Iter: 594 loss: 3.70652583e-06
Iter: 595 loss: 3.69605368e-06
Iter: 596 loss: 3.73050352e-06
Iter: 597 loss: 3.69331974e-06
Iter: 598 loss: 3.68527367e-06
Iter: 599 loss: 3.69357758e-06
Iter: 600 loss: 3.6808533e-06
Iter: 601 loss: 3.67242455e-06
Iter: 602 loss: 3.67991106e-06
Iter: 603 loss: 3.66739778e-06
Iter: 604 loss: 3.65891356e-06
Iter: 605 loss: 3.76609842e-06
Iter: 606 loss: 3.65876235e-06
Iter: 607 loss: 3.65121127e-06
Iter: 608 loss: 3.65812866e-06
Iter: 609 loss: 3.64677089e-06
Iter: 610 loss: 3.63862864e-06
Iter: 611 loss: 3.69720988e-06
Iter: 612 loss: 3.63793174e-06
Iter: 613 loss: 3.63118329e-06
Iter: 614 loss: 3.62687751e-06
Iter: 615 loss: 3.6241081e-06
Iter: 616 loss: 3.61598245e-06
Iter: 617 loss: 3.68669475e-06
Iter: 618 loss: 3.61557932e-06
Iter: 619 loss: 3.60864806e-06
Iter: 620 loss: 3.60513081e-06
Iter: 621 loss: 3.60183e-06
Iter: 622 loss: 3.59218211e-06
Iter: 623 loss: 3.6392903e-06
Iter: 624 loss: 3.59035653e-06
Iter: 625 loss: 3.58347506e-06
Iter: 626 loss: 3.6075694e-06
Iter: 627 loss: 3.58164698e-06
Iter: 628 loss: 3.57317958e-06
Iter: 629 loss: 3.57929844e-06
Iter: 630 loss: 3.56783357e-06
Iter: 631 loss: 3.55950601e-06
Iter: 632 loss: 3.58369834e-06
Iter: 633 loss: 3.55700422e-06
Iter: 634 loss: 3.54861504e-06
Iter: 635 loss: 3.6018796e-06
Iter: 636 loss: 3.54753752e-06
Iter: 637 loss: 3.5390749e-06
Iter: 638 loss: 3.54185522e-06
Iter: 639 loss: 3.53309224e-06
Iter: 640 loss: 3.52384041e-06
Iter: 641 loss: 3.54023678e-06
Iter: 642 loss: 3.51989229e-06
Iter: 643 loss: 3.51183576e-06
Iter: 644 loss: 3.55409634e-06
Iter: 645 loss: 3.51057861e-06
Iter: 646 loss: 3.50254845e-06
Iter: 647 loss: 3.54576378e-06
Iter: 648 loss: 3.50131404e-06
Iter: 649 loss: 3.49418224e-06
Iter: 650 loss: 3.5049884e-06
Iter: 651 loss: 3.49077732e-06
Iter: 652 loss: 3.48410708e-06
Iter: 653 loss: 3.51369022e-06
Iter: 654 loss: 3.48270237e-06
Iter: 655 loss: 3.4764048e-06
Iter: 656 loss: 3.47683931e-06
Iter: 657 loss: 3.47143759e-06
Iter: 658 loss: 3.46364436e-06
Iter: 659 loss: 3.49052493e-06
Iter: 660 loss: 3.46169099e-06
Iter: 661 loss: 3.45290437e-06
Iter: 662 loss: 3.47263176e-06
Iter: 663 loss: 3.44952969e-06
Iter: 664 loss: 3.44195018e-06
Iter: 665 loss: 3.44946852e-06
Iter: 666 loss: 3.43759552e-06
Iter: 667 loss: 3.42932867e-06
Iter: 668 loss: 3.48347476e-06
Iter: 669 loss: 3.42841986e-06
Iter: 670 loss: 3.42041972e-06
Iter: 671 loss: 3.42709086e-06
Iter: 672 loss: 3.41568125e-06
Iter: 673 loss: 3.40665838e-06
Iter: 674 loss: 3.44222281e-06
Iter: 675 loss: 3.40466704e-06
Iter: 676 loss: 3.39755206e-06
Iter: 677 loss: 3.45578951e-06
Iter: 678 loss: 3.39703388e-06
Iter: 679 loss: 3.39121812e-06
Iter: 680 loss: 3.38340919e-06
Iter: 681 loss: 3.38290874e-06
Iter: 682 loss: 3.37472193e-06
Iter: 683 loss: 3.42560884e-06
Iter: 684 loss: 3.37390679e-06
Iter: 685 loss: 3.36699077e-06
Iter: 686 loss: 3.42401222e-06
Iter: 687 loss: 3.36670337e-06
Iter: 688 loss: 3.35964955e-06
Iter: 689 loss: 3.35531013e-06
Iter: 690 loss: 3.3525298e-06
Iter: 691 loss: 3.34594961e-06
Iter: 692 loss: 3.41433611e-06
Iter: 693 loss: 3.34571951e-06
Iter: 694 loss: 3.33981757e-06
Iter: 695 loss: 3.33680168e-06
Iter: 696 loss: 3.33400203e-06
Iter: 697 loss: 3.32532591e-06
Iter: 698 loss: 3.35432264e-06
Iter: 699 loss: 3.32301397e-06
Iter: 700 loss: 3.31605702e-06
Iter: 701 loss: 3.35725144e-06
Iter: 702 loss: 3.31518663e-06
Iter: 703 loss: 3.30825878e-06
Iter: 704 loss: 3.30482089e-06
Iter: 705 loss: 3.30162902e-06
Iter: 706 loss: 3.29295517e-06
Iter: 707 loss: 3.32883155e-06
Iter: 708 loss: 3.29104637e-06
Iter: 709 loss: 3.28197621e-06
Iter: 710 loss: 3.32342847e-06
Iter: 711 loss: 3.28029819e-06
Iter: 712 loss: 3.2728276e-06
Iter: 713 loss: 3.2880821e-06
Iter: 714 loss: 3.26991494e-06
Iter: 715 loss: 3.26300142e-06
Iter: 716 loss: 3.30834837e-06
Iter: 717 loss: 3.26232794e-06
Iter: 718 loss: 3.25619612e-06
Iter: 719 loss: 3.25692508e-06
Iter: 720 loss: 3.25159726e-06
Iter: 721 loss: 3.24344e-06
Iter: 722 loss: 3.25081169e-06
Iter: 723 loss: 3.23872746e-06
Iter: 724 loss: 3.23384552e-06
Iter: 725 loss: 3.23325094e-06
Iter: 726 loss: 3.22866754e-06
Iter: 727 loss: 3.22104461e-06
Iter: 728 loss: 3.22109395e-06
Iter: 729 loss: 3.21360312e-06
Iter: 730 loss: 3.2650596e-06
Iter: 731 loss: 3.21293669e-06
Iter: 732 loss: 3.20613435e-06
Iter: 733 loss: 3.22345068e-06
Iter: 734 loss: 3.20371964e-06
Iter: 735 loss: 3.1970153e-06
Iter: 736 loss: 3.19702963e-06
Iter: 737 loss: 3.19173523e-06
Iter: 738 loss: 3.18292177e-06
Iter: 739 loss: 3.23426821e-06
Iter: 740 loss: 3.18179696e-06
Iter: 741 loss: 3.17417107e-06
Iter: 742 loss: 3.19373203e-06
Iter: 743 loss: 3.17161675e-06
Iter: 744 loss: 3.16518367e-06
Iter: 745 loss: 3.18524508e-06
Iter: 746 loss: 3.16335922e-06
Iter: 747 loss: 3.15696116e-06
Iter: 748 loss: 3.15712487e-06
Iter: 749 loss: 3.15182615e-06
Iter: 750 loss: 3.14226213e-06
Iter: 751 loss: 3.22920891e-06
Iter: 752 loss: 3.14172667e-06
Iter: 753 loss: 3.13648366e-06
Iter: 754 loss: 3.1409395e-06
Iter: 755 loss: 3.13328928e-06
Iter: 756 loss: 3.12505472e-06
Iter: 757 loss: 3.14261047e-06
Iter: 758 loss: 3.12198017e-06
Iter: 759 loss: 3.11573058e-06
Iter: 760 loss: 3.13918e-06
Iter: 761 loss: 3.11428e-06
Iter: 762 loss: 3.10921587e-06
Iter: 763 loss: 3.14472254e-06
Iter: 764 loss: 3.10869655e-06
Iter: 765 loss: 3.10324504e-06
Iter: 766 loss: 3.10052064e-06
Iter: 767 loss: 3.09780103e-06
Iter: 768 loss: 3.09048301e-06
Iter: 769 loss: 3.09626193e-06
Iter: 770 loss: 3.08602648e-06
Iter: 771 loss: 3.07931828e-06
Iter: 772 loss: 3.16817886e-06
Iter: 773 loss: 3.07931759e-06
Iter: 774 loss: 3.07379514e-06
Iter: 775 loss: 3.07415803e-06
Iter: 776 loss: 3.06949983e-06
Iter: 777 loss: 3.06199036e-06
Iter: 778 loss: 3.06217953e-06
Iter: 779 loss: 3.05606545e-06
Iter: 780 loss: 3.04915625e-06
Iter: 781 loss: 3.04920127e-06
Iter: 782 loss: 3.04329296e-06
Iter: 783 loss: 3.039901e-06
Iter: 784 loss: 3.0375279e-06
Iter: 785 loss: 3.03004754e-06
Iter: 786 loss: 3.07010168e-06
Iter: 787 loss: 3.0290264e-06
Iter: 788 loss: 3.02270269e-06
Iter: 789 loss: 3.0550234e-06
Iter: 790 loss: 3.02170179e-06
Iter: 791 loss: 3.01558725e-06
Iter: 792 loss: 3.02215722e-06
Iter: 793 loss: 3.01208911e-06
Iter: 794 loss: 3.00586862e-06
Iter: 795 loss: 3.02359149e-06
Iter: 796 loss: 3.00405259e-06
Iter: 797 loss: 2.99736098e-06
Iter: 798 loss: 3.02434114e-06
Iter: 799 loss: 2.99585417e-06
Iter: 800 loss: 2.99071394e-06
Iter: 801 loss: 3.01142495e-06
Iter: 802 loss: 2.98943269e-06
Iter: 803 loss: 2.98297641e-06
Iter: 804 loss: 2.98427267e-06
Iter: 805 loss: 2.97809493e-06
Iter: 806 loss: 2.97255497e-06
Iter: 807 loss: 2.97309975e-06
Iter: 808 loss: 2.96826261e-06
Iter: 809 loss: 2.96224061e-06
Iter: 810 loss: 3.05176218e-06
Iter: 811 loss: 2.96220628e-06
Iter: 812 loss: 2.95741233e-06
Iter: 813 loss: 2.95785344e-06
Iter: 814 loss: 2.95367363e-06
Iter: 815 loss: 2.94689198e-06
Iter: 816 loss: 2.9554335e-06
Iter: 817 loss: 2.94337724e-06
Iter: 818 loss: 2.936898e-06
Iter: 819 loss: 2.96661506e-06
Iter: 820 loss: 2.93569337e-06
Iter: 821 loss: 2.92906861e-06
Iter: 822 loss: 2.95165887e-06
Iter: 823 loss: 2.92726736e-06
Iter: 824 loss: 2.92066647e-06
Iter: 825 loss: 2.9225107e-06
Iter: 826 loss: 2.91589504e-06
Iter: 827 loss: 2.90938715e-06
Iter: 828 loss: 2.98813166e-06
Iter: 829 loss: 2.90933303e-06
Iter: 830 loss: 2.9042892e-06
Iter: 831 loss: 2.91174638e-06
Iter: 832 loss: 2.9018247e-06
Iter: 833 loss: 2.89616901e-06
Iter: 834 loss: 2.89982199e-06
Iter: 835 loss: 2.89266882e-06
Iter: 836 loss: 2.88720321e-06
Iter: 837 loss: 2.95637301e-06
Iter: 838 loss: 2.88717411e-06
Iter: 839 loss: 2.88286242e-06
Iter: 840 loss: 2.88772412e-06
Iter: 841 loss: 2.88059573e-06
Iter: 842 loss: 2.87537182e-06
Iter: 843 loss: 2.88514957e-06
Iter: 844 loss: 2.87307648e-06
Iter: 845 loss: 2.86759928e-06
Iter: 846 loss: 2.8675945e-06
Iter: 847 loss: 2.8632644e-06
Iter: 848 loss: 2.855307e-06
Iter: 849 loss: 2.88180445e-06
Iter: 850 loss: 2.85308693e-06
Iter: 851 loss: 2.84784051e-06
Iter: 852 loss: 2.90739717e-06
Iter: 853 loss: 2.84768521e-06
Iter: 854 loss: 2.84263206e-06
Iter: 855 loss: 2.83722784e-06
Iter: 856 loss: 2.83642703e-06
Iter: 857 loss: 2.82913788e-06
Iter: 858 loss: 2.85640795e-06
Iter: 859 loss: 2.82742531e-06
Iter: 860 loss: 2.82150177e-06
Iter: 861 loss: 2.84942394e-06
Iter: 862 loss: 2.82051701e-06
Iter: 863 loss: 2.81480902e-06
Iter: 864 loss: 2.83992676e-06
Iter: 865 loss: 2.81364328e-06
Iter: 866 loss: 2.80842119e-06
Iter: 867 loss: 2.80909171e-06
Iter: 868 loss: 2.80454196e-06
Iter: 869 loss: 2.79866663e-06
Iter: 870 loss: 2.86571913e-06
Iter: 871 loss: 2.79856363e-06
Iter: 872 loss: 2.79397932e-06
Iter: 873 loss: 2.79097867e-06
Iter: 874 loss: 2.78911602e-06
Iter: 875 loss: 2.78367474e-06
Iter: 876 loss: 2.84387352e-06
Iter: 877 loss: 2.78352081e-06
Iter: 878 loss: 2.77848403e-06
Iter: 879 loss: 2.78672451e-06
Iter: 880 loss: 2.77612889e-06
Iter: 881 loss: 2.77142067e-06
Iter: 882 loss: 2.77177969e-06
Iter: 883 loss: 2.76766718e-06
Iter: 884 loss: 2.76127275e-06
Iter: 885 loss: 2.79688857e-06
Iter: 886 loss: 2.76035325e-06
Iter: 887 loss: 2.75547927e-06
Iter: 888 loss: 2.76026503e-06
Iter: 889 loss: 2.75269554e-06
Iter: 890 loss: 2.74734975e-06
Iter: 891 loss: 2.77067033e-06
Iter: 892 loss: 2.74630702e-06
Iter: 893 loss: 2.740328e-06
Iter: 894 loss: 2.75364641e-06
Iter: 895 loss: 2.73810656e-06
Iter: 896 loss: 2.73293199e-06
Iter: 897 loss: 2.73818569e-06
Iter: 898 loss: 2.73011437e-06
Iter: 899 loss: 2.72412262e-06
Iter: 900 loss: 2.72781517e-06
Iter: 901 loss: 2.72032139e-06
Iter: 902 loss: 2.71409226e-06
Iter: 903 loss: 2.80907398e-06
Iter: 904 loss: 2.71409181e-06
Iter: 905 loss: 2.7095773e-06
Iter: 906 loss: 2.71457407e-06
Iter: 907 loss: 2.70726377e-06
Iter: 908 loss: 2.70154305e-06
Iter: 909 loss: 2.71009321e-06
Iter: 910 loss: 2.69902557e-06
Iter: 911 loss: 2.69311727e-06
Iter: 912 loss: 2.73868045e-06
Iter: 913 loss: 2.69270549e-06
Iter: 914 loss: 2.68901704e-06
Iter: 915 loss: 2.69460725e-06
Iter: 916 loss: 2.6871503e-06
Iter: 917 loss: 2.68218423e-06
Iter: 918 loss: 2.69976408e-06
Iter: 919 loss: 2.6808134e-06
Iter: 920 loss: 2.67658879e-06
Iter: 921 loss: 2.67361361e-06
Iter: 922 loss: 2.67210044e-06
Iter: 923 loss: 2.66583879e-06
Iter: 924 loss: 2.68686631e-06
Iter: 925 loss: 2.66408324e-06
Iter: 926 loss: 2.65821154e-06
Iter: 927 loss: 2.69143743e-06
Iter: 928 loss: 2.65745871e-06
Iter: 929 loss: 2.65275207e-06
Iter: 930 loss: 2.6615653e-06
Iter: 931 loss: 2.65058657e-06
Iter: 932 loss: 2.6459611e-06
Iter: 933 loss: 2.66644474e-06
Iter: 934 loss: 2.64504297e-06
Iter: 935 loss: 2.6403809e-06
Iter: 936 loss: 2.64753271e-06
Iter: 937 loss: 2.63823222e-06
Iter: 938 loss: 2.63274433e-06
Iter: 939 loss: 2.63330617e-06
Iter: 940 loss: 2.62839558e-06
Iter: 941 loss: 2.62211051e-06
Iter: 942 loss: 2.64016171e-06
Iter: 943 loss: 2.62007029e-06
Iter: 944 loss: 2.61545824e-06
Iter: 945 loss: 2.61542618e-06
Iter: 946 loss: 2.61163405e-06
Iter: 947 loss: 2.60688512e-06
Iter: 948 loss: 2.60645152e-06
Iter: 949 loss: 2.60175375e-06
Iter: 950 loss: 2.6017342e-06
Iter: 951 loss: 2.59807848e-06
Iter: 952 loss: 2.59992521e-06
Iter: 953 loss: 2.59565286e-06
Iter: 954 loss: 2.59107105e-06
Iter: 955 loss: 2.61315654e-06
Iter: 956 loss: 2.59023136e-06
Iter: 957 loss: 2.58613727e-06
Iter: 958 loss: 2.58653949e-06
Iter: 959 loss: 2.58299087e-06
Iter: 960 loss: 2.57740157e-06
Iter: 961 loss: 2.57885631e-06
Iter: 962 loss: 2.57328929e-06
Iter: 963 loss: 2.56669e-06
Iter: 964 loss: 2.59820172e-06
Iter: 965 loss: 2.56552062e-06
Iter: 966 loss: 2.55941563e-06
Iter: 967 loss: 2.60330876e-06
Iter: 968 loss: 2.55885516e-06
Iter: 969 loss: 2.55493615e-06
Iter: 970 loss: 2.55641658e-06
Iter: 971 loss: 2.55222881e-06
Iter: 972 loss: 2.5465738e-06
Iter: 973 loss: 2.57190641e-06
Iter: 974 loss: 2.54546467e-06
Iter: 975 loss: 2.54086626e-06
Iter: 976 loss: 2.54448764e-06
Iter: 977 loss: 2.53802978e-06
Iter: 978 loss: 2.53262033e-06
Iter: 979 loss: 2.54057363e-06
Iter: 980 loss: 2.53005965e-06
Iter: 981 loss: 2.52479822e-06
Iter: 982 loss: 2.57821489e-06
Iter: 983 loss: 2.52460586e-06
Iter: 984 loss: 2.51989104e-06
Iter: 985 loss: 2.52777295e-06
Iter: 986 loss: 2.51777942e-06
Iter: 987 loss: 2.51294068e-06
Iter: 988 loss: 2.52058953e-06
Iter: 989 loss: 2.51062e-06
Iter: 990 loss: 2.50520293e-06
Iter: 991 loss: 2.54876772e-06
Iter: 992 loss: 2.50484072e-06
Iter: 993 loss: 2.50144376e-06
Iter: 994 loss: 2.50290486e-06
Iter: 995 loss: 2.49917298e-06
Iter: 996 loss: 2.4947883e-06
Iter: 997 loss: 2.51076858e-06
Iter: 998 loss: 2.49377149e-06
Iter: 999 loss: 2.48953825e-06
Iter: 1000 loss: 2.48580318e-06
Iter: 1001 loss: 2.4847302e-06
Iter: 1002 loss: 2.47856565e-06
Iter: 1003 loss: 2.51263282e-06
Iter: 1004 loss: 2.47762591e-06
Iter: 1005 loss: 2.47182061e-06
Iter: 1006 loss: 2.48095739e-06
Iter: 1007 loss: 2.4691351e-06
Iter: 1008 loss: 2.46433e-06
Iter: 1009 loss: 2.53724511e-06
Iter: 1010 loss: 2.46434024e-06
Iter: 1011 loss: 2.46081208e-06
Iter: 1012 loss: 2.45464958e-06
Iter: 1013 loss: 2.45464253e-06
Iter: 1014 loss: 2.45017873e-06
Iter: 1015 loss: 2.45005e-06
Iter: 1016 loss: 2.4465453e-06
Iter: 1017 loss: 2.44255239e-06
Iter: 1018 loss: 2.44211378e-06
Iter: 1019 loss: 2.43575209e-06
Iter: 1020 loss: 2.47263165e-06
Iter: 1021 loss: 2.43492104e-06
Iter: 1022 loss: 2.43123759e-06
Iter: 1023 loss: 2.4710796e-06
Iter: 1024 loss: 2.43107047e-06
Iter: 1025 loss: 2.42752867e-06
Iter: 1026 loss: 2.42510123e-06
Iter: 1027 loss: 2.42385067e-06
Iter: 1028 loss: 2.42022975e-06
Iter: 1029 loss: 2.47398339e-06
Iter: 1030 loss: 2.42027454e-06
Iter: 1031 loss: 2.4170854e-06
Iter: 1032 loss: 2.41409543e-06
Iter: 1033 loss: 2.41349289e-06
Iter: 1034 loss: 2.40843747e-06
Iter: 1035 loss: 2.42235183e-06
Iter: 1036 loss: 2.40689542e-06
Iter: 1037 loss: 2.40212557e-06
Iter: 1038 loss: 2.4248543e-06
Iter: 1039 loss: 2.40131931e-06
Iter: 1040 loss: 2.39701035e-06
Iter: 1041 loss: 2.39651149e-06
Iter: 1042 loss: 2.39327824e-06
Iter: 1043 loss: 2.38846701e-06
Iter: 1044 loss: 2.40394502e-06
Iter: 1045 loss: 2.38708571e-06
Iter: 1046 loss: 2.38211805e-06
Iter: 1047 loss: 2.40517647e-06
Iter: 1048 loss: 2.38117173e-06
Iter: 1049 loss: 2.37653308e-06
Iter: 1050 loss: 2.40053259e-06
Iter: 1051 loss: 2.37571521e-06
Iter: 1052 loss: 2.37211725e-06
Iter: 1053 loss: 2.36914457e-06
Iter: 1054 loss: 2.36797496e-06
Iter: 1055 loss: 2.36246615e-06
Iter: 1056 loss: 2.39690985e-06
Iter: 1057 loss: 2.36173537e-06
Iter: 1058 loss: 2.35655307e-06
Iter: 1059 loss: 2.37073482e-06
Iter: 1060 loss: 2.35490484e-06
Iter: 1061 loss: 2.3507464e-06
Iter: 1062 loss: 2.37234553e-06
Iter: 1063 loss: 2.3501434e-06
Iter: 1064 loss: 2.34627055e-06
Iter: 1065 loss: 2.36109577e-06
Iter: 1066 loss: 2.34541722e-06
Iter: 1067 loss: 2.34215395e-06
Iter: 1068 loss: 2.3455932e-06
Iter: 1069 loss: 2.3403245e-06
Iter: 1070 loss: 2.33620244e-06
Iter: 1071 loss: 2.35226116e-06
Iter: 1072 loss: 2.33518244e-06
Iter: 1073 loss: 2.33116907e-06
Iter: 1074 loss: 2.3336429e-06
Iter: 1075 loss: 2.32861612e-06
Iter: 1076 loss: 2.32480716e-06
Iter: 1077 loss: 2.33026344e-06
Iter: 1078 loss: 2.32303728e-06
Iter: 1079 loss: 2.31794547e-06
Iter: 1080 loss: 2.33973765e-06
Iter: 1081 loss: 2.31707531e-06
Iter: 1082 loss: 2.31282638e-06
Iter: 1083 loss: 2.32370735e-06
Iter: 1084 loss: 2.31131253e-06
Iter: 1085 loss: 2.30709361e-06
Iter: 1086 loss: 2.30453452e-06
Iter: 1087 loss: 2.30282558e-06
Iter: 1088 loss: 2.2982324e-06
Iter: 1089 loss: 2.29816487e-06
Iter: 1090 loss: 2.29446641e-06
Iter: 1091 loss: 2.30044498e-06
Iter: 1092 loss: 2.2927195e-06
Iter: 1093 loss: 2.28883027e-06
Iter: 1094 loss: 2.28596809e-06
Iter: 1095 loss: 2.28458157e-06
Iter: 1096 loss: 2.27974942e-06
Iter: 1097 loss: 2.33164178e-06
Iter: 1098 loss: 2.27963505e-06
Iter: 1099 loss: 2.27552346e-06
Iter: 1100 loss: 2.28962199e-06
Iter: 1101 loss: 2.27444116e-06
Iter: 1102 loss: 2.27035162e-06
Iter: 1103 loss: 2.2818358e-06
Iter: 1104 loss: 2.26914153e-06
Iter: 1105 loss: 2.26502561e-06
Iter: 1106 loss: 2.27841474e-06
Iter: 1107 loss: 2.26382735e-06
Iter: 1108 loss: 2.2604595e-06
Iter: 1109 loss: 2.26873703e-06
Iter: 1110 loss: 2.25940494e-06
Iter: 1111 loss: 2.25577e-06
Iter: 1112 loss: 2.26437396e-06
Iter: 1113 loss: 2.25445774e-06
Iter: 1114 loss: 2.2507038e-06
Iter: 1115 loss: 2.24901805e-06
Iter: 1116 loss: 2.24705605e-06
Iter: 1117 loss: 2.24229802e-06
Iter: 1118 loss: 2.26143129e-06
Iter: 1119 loss: 2.24124551e-06
Iter: 1120 loss: 2.23648681e-06
Iter: 1121 loss: 2.26308612e-06
Iter: 1122 loss: 2.23584516e-06
Iter: 1123 loss: 2.23149482e-06
Iter: 1124 loss: 2.2313036e-06
Iter: 1125 loss: 2.22787e-06
Iter: 1126 loss: 2.22315043e-06
Iter: 1127 loss: 2.24602172e-06
Iter: 1128 loss: 2.22238214e-06
Iter: 1129 loss: 2.21844311e-06
Iter: 1130 loss: 2.24542873e-06
Iter: 1131 loss: 2.2180227e-06
Iter: 1132 loss: 2.21408823e-06
Iter: 1133 loss: 2.21317759e-06
Iter: 1134 loss: 2.21068058e-06
Iter: 1135 loss: 2.20657512e-06
Iter: 1136 loss: 2.2148281e-06
Iter: 1137 loss: 2.20493894e-06
Iter: 1138 loss: 2.20047491e-06
Iter: 1139 loss: 2.22947665e-06
Iter: 1140 loss: 2.20008678e-06
Iter: 1141 loss: 2.1958906e-06
Iter: 1142 loss: 2.21247751e-06
Iter: 1143 loss: 2.19486037e-06
Iter: 1144 loss: 2.19143021e-06
Iter: 1145 loss: 2.19858293e-06
Iter: 1146 loss: 2.19010417e-06
Iter: 1147 loss: 2.18606874e-06
Iter: 1148 loss: 2.19683807e-06
Iter: 1149 loss: 2.18470655e-06
Iter: 1150 loss: 2.1814742e-06
Iter: 1151 loss: 2.18935634e-06
Iter: 1152 loss: 2.18026685e-06
Iter: 1153 loss: 2.17625234e-06
Iter: 1154 loss: 2.18127616e-06
Iter: 1155 loss: 2.17420848e-06
Iter: 1156 loss: 2.17025786e-06
Iter: 1157 loss: 2.16991475e-06
Iter: 1158 loss: 2.16694502e-06
Iter: 1159 loss: 2.16232252e-06
Iter: 1160 loss: 2.21243363e-06
Iter: 1161 loss: 2.16218064e-06
Iter: 1162 loss: 2.15859609e-06
Iter: 1163 loss: 2.16303852e-06
Iter: 1164 loss: 2.1567264e-06
Iter: 1165 loss: 2.15219507e-06
Iter: 1166 loss: 2.16631634e-06
Iter: 1167 loss: 2.15089085e-06
Iter: 1168 loss: 2.14741704e-06
Iter: 1169 loss: 2.14853253e-06
Iter: 1170 loss: 2.14492e-06
Iter: 1171 loss: 2.14067018e-06
Iter: 1172 loss: 2.20011702e-06
Iter: 1173 loss: 2.14068109e-06
Iter: 1174 loss: 2.13823841e-06
Iter: 1175 loss: 2.13543854e-06
Iter: 1176 loss: 2.13508747e-06
Iter: 1177 loss: 2.13034946e-06
Iter: 1178 loss: 2.14649549e-06
Iter: 1179 loss: 2.12906025e-06
Iter: 1180 loss: 2.12593795e-06
Iter: 1181 loss: 2.16463263e-06
Iter: 1182 loss: 2.12590339e-06
Iter: 1183 loss: 2.12273608e-06
Iter: 1184 loss: 2.12266e-06
Iter: 1185 loss: 2.12010491e-06
Iter: 1186 loss: 2.11643373e-06
Iter: 1187 loss: 2.13122621e-06
Iter: 1188 loss: 2.11558563e-06
Iter: 1189 loss: 2.11166889e-06
Iter: 1190 loss: 2.12109035e-06
Iter: 1191 loss: 2.11028214e-06
Iter: 1192 loss: 2.10703138e-06
Iter: 1193 loss: 2.11226552e-06
Iter: 1194 loss: 2.10551411e-06
Iter: 1195 loss: 2.10183384e-06
Iter: 1196 loss: 2.11474435e-06
Iter: 1197 loss: 2.10086228e-06
Iter: 1198 loss: 2.09736982e-06
Iter: 1199 loss: 2.09628661e-06
Iter: 1200 loss: 2.09417522e-06
Iter: 1201 loss: 2.09025256e-06
Iter: 1202 loss: 2.11643692e-06
Iter: 1203 loss: 2.08989331e-06
Iter: 1204 loss: 2.08591905e-06
Iter: 1205 loss: 2.09579775e-06
Iter: 1206 loss: 2.08447477e-06
Iter: 1207 loss: 2.08037181e-06
Iter: 1208 loss: 2.08765027e-06
Iter: 1209 loss: 2.07849598e-06
Iter: 1210 loss: 2.07481594e-06
Iter: 1211 loss: 2.0882369e-06
Iter: 1212 loss: 2.07388575e-06
Iter: 1213 loss: 2.07043718e-06
Iter: 1214 loss: 2.09599102e-06
Iter: 1215 loss: 2.07021276e-06
Iter: 1216 loss: 2.06747836e-06
Iter: 1217 loss: 2.06248842e-06
Iter: 1218 loss: 2.17516777e-06
Iter: 1219 loss: 2.06246159e-06
Iter: 1220 loss: 2.05835295e-06
Iter: 1221 loss: 2.12241775e-06
Iter: 1222 loss: 2.05837728e-06
Iter: 1223 loss: 2.05463016e-06
Iter: 1224 loss: 2.07219227e-06
Iter: 1225 loss: 2.05399e-06
Iter: 1226 loss: 2.05138394e-06
Iter: 1227 loss: 2.05110382e-06
Iter: 1228 loss: 2.04921707e-06
Iter: 1229 loss: 2.04550088e-06
Iter: 1230 loss: 2.06905111e-06
Iter: 1231 loss: 2.04510934e-06
Iter: 1232 loss: 2.04225489e-06
Iter: 1233 loss: 2.04558091e-06
Iter: 1234 loss: 2.0407615e-06
Iter: 1235 loss: 2.03780905e-06
Iter: 1236 loss: 2.04116918e-06
Iter: 1237 loss: 2.03619038e-06
Iter: 1238 loss: 2.03205809e-06
Iter: 1239 loss: 2.04376443e-06
Iter: 1240 loss: 2.03078071e-06
Iter: 1241 loss: 2.02719207e-06
Iter: 1242 loss: 2.03336208e-06
Iter: 1243 loss: 2.02557453e-06
Iter: 1244 loss: 2.02168189e-06
Iter: 1245 loss: 2.02993215e-06
Iter: 1246 loss: 2.02014439e-06
Iter: 1247 loss: 2.01611692e-06
Iter: 1248 loss: 2.0496816e-06
Iter: 1249 loss: 2.01585772e-06
Iter: 1250 loss: 2.01318494e-06
Iter: 1251 loss: 2.01176863e-06
Iter: 1252 loss: 2.01060061e-06
Iter: 1253 loss: 2.00670115e-06
Iter: 1254 loss: 2.0390994e-06
Iter: 1255 loss: 2.0065213e-06
Iter: 1256 loss: 2.00327531e-06
Iter: 1257 loss: 2.00884824e-06
Iter: 1258 loss: 2.00174304e-06
Iter: 1259 loss: 1.99852752e-06
Iter: 1260 loss: 2.00010254e-06
Iter: 1261 loss: 1.99631268e-06
Iter: 1262 loss: 1.9926606e-06
Iter: 1263 loss: 2.03300124e-06
Iter: 1264 loss: 1.99259512e-06
Iter: 1265 loss: 1.98935504e-06
Iter: 1266 loss: 1.99492661e-06
Iter: 1267 loss: 1.98786415e-06
Iter: 1268 loss: 1.98514545e-06
Iter: 1269 loss: 1.98491466e-06
Iter: 1270 loss: 1.98294401e-06
Iter: 1271 loss: 1.97907684e-06
Iter: 1272 loss: 2.0110474e-06
Iter: 1273 loss: 1.97886493e-06
Iter: 1274 loss: 1.97595591e-06
Iter: 1275 loss: 1.97680606e-06
Iter: 1276 loss: 1.97385862e-06
Iter: 1277 loss: 1.97022609e-06
Iter: 1278 loss: 1.97414715e-06
Iter: 1279 loss: 1.9682634e-06
Iter: 1280 loss: 1.96440715e-06
Iter: 1281 loss: 1.98632233e-06
Iter: 1282 loss: 1.96390783e-06
Iter: 1283 loss: 1.95986536e-06
Iter: 1284 loss: 1.9652241e-06
Iter: 1285 loss: 1.95785901e-06
Iter: 1286 loss: 1.95444841e-06
Iter: 1287 loss: 1.96316614e-06
Iter: 1288 loss: 1.95325174e-06
Iter: 1289 loss: 1.94975837e-06
Iter: 1290 loss: 1.97036434e-06
Iter: 1291 loss: 1.94927134e-06
Iter: 1292 loss: 1.94591189e-06
Iter: 1293 loss: 1.94852146e-06
Iter: 1294 loss: 1.94395034e-06
Iter: 1295 loss: 1.94056702e-06
Iter: 1296 loss: 1.95529e-06
Iter: 1297 loss: 1.93995038e-06
Iter: 1298 loss: 1.93650385e-06
Iter: 1299 loss: 1.94748645e-06
Iter: 1300 loss: 1.93546271e-06
Iter: 1301 loss: 1.93268579e-06
Iter: 1302 loss: 1.93654364e-06
Iter: 1303 loss: 1.93126493e-06
Iter: 1304 loss: 1.92787957e-06
Iter: 1305 loss: 1.95527809e-06
Iter: 1306 loss: 1.92768448e-06
Iter: 1307 loss: 1.92513676e-06
Iter: 1308 loss: 1.92313519e-06
Iter: 1309 loss: 1.92234074e-06
Iter: 1310 loss: 1.91920117e-06
Iter: 1311 loss: 1.9307297e-06
Iter: 1312 loss: 1.91834715e-06
Iter: 1313 loss: 1.91496474e-06
Iter: 1314 loss: 1.9332474e-06
Iter: 1315 loss: 1.91438039e-06
Iter: 1316 loss: 1.91167555e-06
Iter: 1317 loss: 1.9090453e-06
Iter: 1318 loss: 1.90847754e-06
Iter: 1319 loss: 1.90429478e-06
Iter: 1320 loss: 1.92823745e-06
Iter: 1321 loss: 1.90374089e-06
Iter: 1322 loss: 1.90077719e-06
Iter: 1323 loss: 1.90931814e-06
Iter: 1324 loss: 1.89991943e-06
Iter: 1325 loss: 1.89584034e-06
Iter: 1326 loss: 1.90139474e-06
Iter: 1327 loss: 1.89378557e-06
Iter: 1328 loss: 1.89019545e-06
Iter: 1329 loss: 1.8980669e-06
Iter: 1330 loss: 1.8887805e-06
Iter: 1331 loss: 1.88542617e-06
Iter: 1332 loss: 1.91451568e-06
Iter: 1333 loss: 1.8852038e-06
Iter: 1334 loss: 1.88230194e-06
Iter: 1335 loss: 1.88249862e-06
Iter: 1336 loss: 1.88004378e-06
Iter: 1337 loss: 1.87717069e-06
Iter: 1338 loss: 1.90327603e-06
Iter: 1339 loss: 1.87698561e-06
Iter: 1340 loss: 1.8743566e-06
Iter: 1341 loss: 1.87676721e-06
Iter: 1342 loss: 1.87289163e-06
Iter: 1343 loss: 1.86982697e-06
Iter: 1344 loss: 1.88433216e-06
Iter: 1345 loss: 1.869372e-06
Iter: 1346 loss: 1.86651096e-06
Iter: 1347 loss: 1.87707974e-06
Iter: 1348 loss: 1.8657945e-06
Iter: 1349 loss: 1.86376315e-06
Iter: 1350 loss: 1.86010186e-06
Iter: 1351 loss: 1.86008765e-06
Iter: 1352 loss: 1.85623605e-06
Iter: 1353 loss: 1.9069455e-06
Iter: 1354 loss: 1.85620786e-06
Iter: 1355 loss: 1.85337592e-06
Iter: 1356 loss: 1.86243642e-06
Iter: 1357 loss: 1.85258841e-06
Iter: 1358 loss: 1.84979899e-06
Iter: 1359 loss: 1.84819555e-06
Iter: 1360 loss: 1.84695023e-06
Iter: 1361 loss: 1.84319288e-06
Iter: 1362 loss: 1.85060912e-06
Iter: 1363 loss: 1.84163321e-06
Iter: 1364 loss: 1.83827615e-06
Iter: 1365 loss: 1.8710889e-06
Iter: 1366 loss: 1.83808663e-06
Iter: 1367 loss: 1.8352855e-06
Iter: 1368 loss: 1.8402485e-06
Iter: 1369 loss: 1.83401619e-06
Iter: 1370 loss: 1.83072007e-06
Iter: 1371 loss: 1.83597672e-06
Iter: 1372 loss: 1.82926362e-06
Iter: 1373 loss: 1.82585904e-06
Iter: 1374 loss: 1.84027601e-06
Iter: 1375 loss: 1.82518374e-06
Iter: 1376 loss: 1.82193241e-06
Iter: 1377 loss: 1.83419479e-06
Iter: 1378 loss: 1.82115673e-06
Iter: 1379 loss: 1.81859923e-06
Iter: 1380 loss: 1.81994415e-06
Iter: 1381 loss: 1.8168754e-06
Iter: 1382 loss: 1.8132705e-06
Iter: 1383 loss: 1.84099429e-06
Iter: 1384 loss: 1.81306473e-06
Iter: 1385 loss: 1.81064229e-06
Iter: 1386 loss: 1.81504993e-06
Iter: 1387 loss: 1.80961547e-06
Iter: 1388 loss: 1.80659595e-06
Iter: 1389 loss: 1.80773532e-06
Iter: 1390 loss: 1.80444965e-06
Iter: 1391 loss: 1.8013061e-06
Iter: 1392 loss: 1.80807888e-06
Iter: 1393 loss: 1.80013262e-06
Iter: 1394 loss: 1.79664335e-06
Iter: 1395 loss: 1.8113052e-06
Iter: 1396 loss: 1.79592826e-06
Iter: 1397 loss: 1.79278845e-06
Iter: 1398 loss: 1.80963116e-06
Iter: 1399 loss: 1.79235269e-06
Iter: 1400 loss: 1.79028609e-06
Iter: 1401 loss: 1.78623213e-06
Iter: 1402 loss: 1.86431657e-06
Iter: 1403 loss: 1.78617938e-06
Iter: 1404 loss: 1.78143864e-06
Iter: 1405 loss: 1.80787151e-06
Iter: 1406 loss: 1.78079176e-06
Iter: 1407 loss: 1.77738946e-06
Iter: 1408 loss: 1.80216637e-06
Iter: 1409 loss: 1.77711763e-06
Iter: 1410 loss: 1.77368884e-06
Iter: 1411 loss: 1.78272489e-06
Iter: 1412 loss: 1.77261882e-06
Iter: 1413 loss: 1.76969729e-06
Iter: 1414 loss: 1.7727051e-06
Iter: 1415 loss: 1.76807691e-06
Iter: 1416 loss: 1.76472281e-06
Iter: 1417 loss: 1.79008362e-06
Iter: 1418 loss: 1.76453159e-06
Iter: 1419 loss: 1.76176411e-06
Iter: 1420 loss: 1.76399305e-06
Iter: 1421 loss: 1.76011054e-06
Iter: 1422 loss: 1.75715877e-06
Iter: 1423 loss: 1.78014022e-06
Iter: 1424 loss: 1.75697687e-06
Iter: 1425 loss: 1.75446007e-06
Iter: 1426 loss: 1.75830087e-06
Iter: 1427 loss: 1.75319337e-06
Iter: 1428 loss: 1.75095704e-06
Iter: 1429 loss: 1.75595619e-06
Iter: 1430 loss: 1.75000685e-06
Iter: 1431 loss: 1.74745992e-06
Iter: 1432 loss: 1.75271396e-06
Iter: 1433 loss: 1.74643731e-06
Iter: 1434 loss: 1.74373599e-06
Iter: 1435 loss: 1.74423099e-06
Iter: 1436 loss: 1.74162869e-06
Iter: 1437 loss: 1.73920796e-06
Iter: 1438 loss: 1.73917829e-06
Iter: 1439 loss: 1.73741796e-06
Iter: 1440 loss: 1.73460603e-06
Iter: 1441 loss: 1.73461319e-06
Iter: 1442 loss: 1.7308688e-06
Iter: 1443 loss: 1.74408547e-06
Iter: 1444 loss: 1.72992611e-06
Iter: 1445 loss: 1.72656928e-06
Iter: 1446 loss: 1.72768455e-06
Iter: 1447 loss: 1.72413138e-06
Iter: 1448 loss: 1.72042087e-06
Iter: 1449 loss: 1.75664491e-06
Iter: 1450 loss: 1.72026625e-06
Iter: 1451 loss: 1.71770603e-06
Iter: 1452 loss: 1.73251283e-06
Iter: 1453 loss: 1.71729596e-06
Iter: 1454 loss: 1.7148659e-06
Iter: 1455 loss: 1.71667898e-06
Iter: 1456 loss: 1.71333795e-06
Iter: 1457 loss: 1.71074862e-06
Iter: 1458 loss: 1.71993418e-06
Iter: 1459 loss: 1.71008833e-06
Iter: 1460 loss: 1.70716135e-06
Iter: 1461 loss: 1.71422039e-06
Iter: 1462 loss: 1.70608496e-06
Iter: 1463 loss: 1.70314036e-06
Iter: 1464 loss: 1.71646138e-06
Iter: 1465 loss: 1.70254066e-06
Iter: 1466 loss: 1.70013345e-06
Iter: 1467 loss: 1.71095405e-06
Iter: 1468 loss: 1.69965017e-06
Iter: 1469 loss: 1.69727616e-06
Iter: 1470 loss: 1.69704128e-06
Iter: 1471 loss: 1.6954267e-06
Iter: 1472 loss: 1.69300608e-06
Iter: 1473 loss: 1.70556541e-06
Iter: 1474 loss: 1.69268355e-06
Iter: 1475 loss: 1.69021894e-06
Iter: 1476 loss: 1.69285852e-06
Iter: 1477 loss: 1.68885538e-06
Iter: 1478 loss: 1.68633233e-06
Iter: 1479 loss: 1.6958179e-06
Iter: 1480 loss: 1.68569682e-06
Iter: 1481 loss: 1.68315421e-06
Iter: 1482 loss: 1.69384509e-06
Iter: 1483 loss: 1.68261238e-06
Iter: 1484 loss: 1.68036729e-06
Iter: 1485 loss: 1.67818462e-06
Iter: 1486 loss: 1.67760572e-06
Iter: 1487 loss: 1.67391067e-06
Iter: 1488 loss: 1.68135477e-06
Iter: 1489 loss: 1.67234623e-06
Iter: 1490 loss: 1.66883819e-06
Iter: 1491 loss: 1.6807478e-06
Iter: 1492 loss: 1.6677742e-06
Iter: 1493 loss: 1.66450263e-06
Iter: 1494 loss: 1.67944e-06
Iter: 1495 loss: 1.66385701e-06
Iter: 1496 loss: 1.66134248e-06
Iter: 1497 loss: 1.67989106e-06
Iter: 1498 loss: 1.66120014e-06
Iter: 1499 loss: 1.65868255e-06
Iter: 1500 loss: 1.66562916e-06
Iter: 1501 loss: 1.65791471e-06
Iter: 1502 loss: 1.65591746e-06
Iter: 1503 loss: 1.66201016e-06
Iter: 1504 loss: 1.65523863e-06
Iter: 1505 loss: 1.65273264e-06
Iter: 1506 loss: 1.65809047e-06
Iter: 1507 loss: 1.65172423e-06
Iter: 1508 loss: 1.64946914e-06
Iter: 1509 loss: 1.6495386e-06
Iter: 1510 loss: 1.64766902e-06
Iter: 1511 loss: 1.64510095e-06
Iter: 1512 loss: 1.65961615e-06
Iter: 1513 loss: 1.64464177e-06
Iter: 1514 loss: 1.64221478e-06
Iter: 1515 loss: 1.65117592e-06
Iter: 1516 loss: 1.64156279e-06
Iter: 1517 loss: 1.63945776e-06
Iter: 1518 loss: 1.64150754e-06
Iter: 1519 loss: 1.63820084e-06
Iter: 1520 loss: 1.63566767e-06
Iter: 1521 loss: 1.65020526e-06
Iter: 1522 loss: 1.63534651e-06
Iter: 1523 loss: 1.63315303e-06
Iter: 1524 loss: 1.63297648e-06
Iter: 1525 loss: 1.63128834e-06
Iter: 1526 loss: 1.62860829e-06
Iter: 1527 loss: 1.64048788e-06
Iter: 1528 loss: 1.6281017e-06
Iter: 1529 loss: 1.62577362e-06
Iter: 1530 loss: 1.631891e-06
Iter: 1531 loss: 1.62497838e-06
Iter: 1532 loss: 1.62227946e-06
Iter: 1533 loss: 1.62835158e-06
Iter: 1534 loss: 1.62131846e-06
Iter: 1535 loss: 1.6189465e-06
Iter: 1536 loss: 1.62124059e-06
Iter: 1537 loss: 1.61768889e-06
Iter: 1538 loss: 1.6146289e-06
Iter: 1539 loss: 1.62151935e-06
Iter: 1540 loss: 1.613481e-06
Iter: 1541 loss: 1.61077071e-06
Iter: 1542 loss: 1.62209506e-06
Iter: 1543 loss: 1.61013679e-06
Iter: 1544 loss: 1.60748596e-06
Iter: 1545 loss: 1.63001869e-06
Iter: 1546 loss: 1.6073576e-06
Iter: 1547 loss: 1.60538e-06
Iter: 1548 loss: 1.60589946e-06
Iter: 1549 loss: 1.60400032e-06
Iter: 1550 loss: 1.60189938e-06
Iter: 1551 loss: 1.62534729e-06
Iter: 1552 loss: 1.60187e-06
Iter: 1553 loss: 1.60012553e-06
Iter: 1554 loss: 1.599504e-06
Iter: 1555 loss: 1.59851868e-06
Iter: 1556 loss: 1.59642343e-06
Iter: 1557 loss: 1.59598108e-06
Iter: 1558 loss: 1.594575e-06
Iter: 1559 loss: 1.59178148e-06
Iter: 1560 loss: 1.63218328e-06
Iter: 1561 loss: 1.59177534e-06
Iter: 1562 loss: 1.59003616e-06
Iter: 1563 loss: 1.58891862e-06
Iter: 1564 loss: 1.58823332e-06
Iter: 1565 loss: 1.58581588e-06
Iter: 1566 loss: 1.60751938e-06
Iter: 1567 loss: 1.58565888e-06
Iter: 1568 loss: 1.58377566e-06
Iter: 1569 loss: 1.58435228e-06
Iter: 1570 loss: 1.58244143e-06
Iter: 1571 loss: 1.57981651e-06
Iter: 1572 loss: 1.58320586e-06
Iter: 1573 loss: 1.57841623e-06
Iter: 1574 loss: 1.57574664e-06
Iter: 1575 loss: 1.58258467e-06
Iter: 1576 loss: 1.5747512e-06
Iter: 1577 loss: 1.57173167e-06
Iter: 1578 loss: 1.58931289e-06
Iter: 1579 loss: 1.57132e-06
Iter: 1580 loss: 1.56914894e-06
Iter: 1581 loss: 1.57095121e-06
Iter: 1582 loss: 1.56780379e-06
Iter: 1583 loss: 1.56543251e-06
Iter: 1584 loss: 1.5705109e-06
Iter: 1585 loss: 1.5645212e-06
Iter: 1586 loss: 1.56177293e-06
Iter: 1587 loss: 1.57081524e-06
Iter: 1588 loss: 1.561011e-06
Iter: 1589 loss: 1.5587558e-06
Iter: 1590 loss: 1.57711531e-06
Iter: 1591 loss: 1.55860312e-06
Iter: 1592 loss: 1.55618068e-06
Iter: 1593 loss: 1.5576851e-06
Iter: 1594 loss: 1.55464841e-06
Iter: 1595 loss: 1.55231135e-06
Iter: 1596 loss: 1.57090585e-06
Iter: 1597 loss: 1.55221437e-06
Iter: 1598 loss: 1.55031103e-06
Iter: 1599 loss: 1.55216992e-06
Iter: 1600 loss: 1.54927466e-06
Iter: 1601 loss: 1.547377e-06
Iter: 1602 loss: 1.54590975e-06
Iter: 1603 loss: 1.54532358e-06
Iter: 1604 loss: 1.54291297e-06
Iter: 1605 loss: 1.57446038e-06
Iter: 1606 loss: 1.54294844e-06
Iter: 1607 loss: 1.54106601e-06
Iter: 1608 loss: 1.54663962e-06
Iter: 1609 loss: 1.54053805e-06
Iter: 1610 loss: 1.53857479e-06
Iter: 1611 loss: 1.53708982e-06
Iter: 1612 loss: 1.53644487e-06
Iter: 1613 loss: 1.53393535e-06
Iter: 1614 loss: 1.5466112e-06
Iter: 1615 loss: 1.53356189e-06
Iter: 1616 loss: 1.53060591e-06
Iter: 1617 loss: 1.53742099e-06
Iter: 1618 loss: 1.529462e-06
Iter: 1619 loss: 1.52718076e-06
Iter: 1620 loss: 1.52683128e-06
Iter: 1621 loss: 1.52524422e-06
Iter: 1622 loss: 1.52198686e-06
Iter: 1623 loss: 1.54168913e-06
Iter: 1624 loss: 1.52156542e-06
Iter: 1625 loss: 1.51927907e-06
Iter: 1626 loss: 1.51983613e-06
Iter: 1627 loss: 1.51753511e-06
Iter: 1628 loss: 1.5145979e-06
Iter: 1629 loss: 1.53263272e-06
Iter: 1630 loss: 1.51416589e-06
Iter: 1631 loss: 1.51203756e-06
Iter: 1632 loss: 1.52878442e-06
Iter: 1633 loss: 1.51183701e-06
Iter: 1634 loss: 1.50984215e-06
Iter: 1635 loss: 1.51581526e-06
Iter: 1636 loss: 1.50923961e-06
Iter: 1637 loss: 1.50729693e-06
Iter: 1638 loss: 1.51025131e-06
Iter: 1639 loss: 1.50636424e-06
Iter: 1640 loss: 1.50390292e-06
Iter: 1641 loss: 1.51362804e-06
Iter: 1642 loss: 1.50338246e-06
Iter: 1643 loss: 1.50157575e-06
Iter: 1644 loss: 1.50066421e-06
Iter: 1645 loss: 1.49991604e-06
Iter: 1646 loss: 1.4974421e-06
Iter: 1647 loss: 1.51232575e-06
Iter: 1648 loss: 1.49715333e-06
Iter: 1649 loss: 1.49499056e-06
Iter: 1650 loss: 1.50157655e-06
Iter: 1651 loss: 1.4943663e-06
Iter: 1652 loss: 1.49244408e-06
Iter: 1653 loss: 1.49831851e-06
Iter: 1654 loss: 1.4917789e-06
Iter: 1655 loss: 1.48988124e-06
Iter: 1656 loss: 1.49378729e-06
Iter: 1657 loss: 1.48906531e-06
Iter: 1658 loss: 1.48645222e-06
Iter: 1659 loss: 1.48401455e-06
Iter: 1660 loss: 1.48331628e-06
Iter: 1661 loss: 1.48079653e-06
Iter: 1662 loss: 1.49740595e-06
Iter: 1663 loss: 1.48048719e-06
Iter: 1664 loss: 1.47795481e-06
Iter: 1665 loss: 1.49109383e-06
Iter: 1666 loss: 1.47752007e-06
Iter: 1667 loss: 1.47555795e-06
Iter: 1668 loss: 1.48065101e-06
Iter: 1669 loss: 1.47487515e-06
Iter: 1670 loss: 1.4730324e-06
Iter: 1671 loss: 1.47073081e-06
Iter: 1672 loss: 1.47053947e-06
Iter: 1673 loss: 1.46776392e-06
Iter: 1674 loss: 1.50611436e-06
Iter: 1675 loss: 1.46773777e-06
Iter: 1676 loss: 1.46586899e-06
Iter: 1677 loss: 1.48694085e-06
Iter: 1678 loss: 1.46586808e-06
Iter: 1679 loss: 1.46441471e-06
Iter: 1680 loss: 1.4623547e-06
Iter: 1681 loss: 1.46232514e-06
Iter: 1682 loss: 1.46014963e-06
Iter: 1683 loss: 1.47592186e-06
Iter: 1684 loss: 1.4600007e-06
Iter: 1685 loss: 1.45808986e-06
Iter: 1686 loss: 1.46391903e-06
Iter: 1687 loss: 1.45751801e-06
Iter: 1688 loss: 1.45576246e-06
Iter: 1689 loss: 1.45532749e-06
Iter: 1690 loss: 1.45427703e-06
Iter: 1691 loss: 1.451863e-06
Iter: 1692 loss: 1.462706e-06
Iter: 1693 loss: 1.45140211e-06
Iter: 1694 loss: 1.44894705e-06
Iter: 1695 loss: 1.46102752e-06
Iter: 1696 loss: 1.44856972e-06
Iter: 1697 loss: 1.44640376e-06
Iter: 1698 loss: 1.44552041e-06
Iter: 1699 loss: 1.44433716e-06
Iter: 1700 loss: 1.44193928e-06
Iter: 1701 loss: 1.45117406e-06
Iter: 1702 loss: 1.4413863e-06
Iter: 1703 loss: 1.43913223e-06
Iter: 1704 loss: 1.44237094e-06
Iter: 1705 loss: 1.43810337e-06
Iter: 1706 loss: 1.43597106e-06
Iter: 1707 loss: 1.4600862e-06
Iter: 1708 loss: 1.43592206e-06
Iter: 1709 loss: 1.43420857e-06
Iter: 1710 loss: 1.43422551e-06
Iter: 1711 loss: 1.43280431e-06
Iter: 1712 loss: 1.43035038e-06
Iter: 1713 loss: 1.43675891e-06
Iter: 1714 loss: 1.42947533e-06
Iter: 1715 loss: 1.42730346e-06
Iter: 1716 loss: 1.44622538e-06
Iter: 1717 loss: 1.42720012e-06
Iter: 1718 loss: 1.425081e-06
Iter: 1719 loss: 1.42786723e-06
Iter: 1720 loss: 1.42406566e-06
Iter: 1721 loss: 1.42210069e-06
Iter: 1722 loss: 1.43006696e-06
Iter: 1723 loss: 1.4217112e-06
Iter: 1724 loss: 1.42000204e-06
Iter: 1725 loss: 1.41973339e-06
Iter: 1726 loss: 1.41850671e-06
Iter: 1727 loss: 1.4163594e-06
Iter: 1728 loss: 1.42085423e-06
Iter: 1729 loss: 1.41552277e-06
Iter: 1730 loss: 1.4131474e-06
Iter: 1731 loss: 1.43554405e-06
Iter: 1732 loss: 1.41303951e-06
Iter: 1733 loss: 1.41131477e-06
Iter: 1734 loss: 1.40977374e-06
Iter: 1735 loss: 1.40927705e-06
Iter: 1736 loss: 1.40689372e-06
Iter: 1737 loss: 1.41866872e-06
Iter: 1738 loss: 1.40646284e-06
Iter: 1739 loss: 1.40387749e-06
Iter: 1740 loss: 1.41677151e-06
Iter: 1741 loss: 1.40345639e-06
Iter: 1742 loss: 1.40173438e-06
Iter: 1743 loss: 1.4012071e-06
Iter: 1744 loss: 1.40024235e-06
Iter: 1745 loss: 1.39791064e-06
Iter: 1746 loss: 1.40141879e-06
Iter: 1747 loss: 1.39667361e-06
Iter: 1748 loss: 1.39378812e-06
Iter: 1749 loss: 1.40302313e-06
Iter: 1750 loss: 1.39292547e-06
Iter: 1751 loss: 1.39070153e-06
Iter: 1752 loss: 1.40851887e-06
Iter: 1753 loss: 1.39058147e-06
Iter: 1754 loss: 1.38849543e-06
Iter: 1755 loss: 1.39947e-06
Iter: 1756 loss: 1.38825021e-06
Iter: 1757 loss: 1.38666189e-06
Iter: 1758 loss: 1.38879329e-06
Iter: 1759 loss: 1.38588246e-06
Iter: 1760 loss: 1.38398286e-06
Iter: 1761 loss: 1.38691632e-06
Iter: 1762 loss: 1.38307837e-06
Iter: 1763 loss: 1.38121709e-06
Iter: 1764 loss: 1.38351675e-06
Iter: 1765 loss: 1.38023074e-06
Iter: 1766 loss: 1.37791847e-06
Iter: 1767 loss: 1.37936695e-06
Iter: 1768 loss: 1.37642292e-06
Iter: 1769 loss: 1.37446978e-06
Iter: 1770 loss: 1.37441759e-06
Iter: 1771 loss: 1.37299105e-06
Iter: 1772 loss: 1.37224356e-06
Iter: 1773 loss: 1.37157235e-06
Iter: 1774 loss: 1.36930259e-06
Iter: 1775 loss: 1.37234781e-06
Iter: 1776 loss: 1.36816379e-06
Iter: 1777 loss: 1.3658929e-06
Iter: 1778 loss: 1.38981477e-06
Iter: 1779 loss: 1.36588926e-06
Iter: 1780 loss: 1.3640049e-06
Iter: 1781 loss: 1.36357198e-06
Iter: 1782 loss: 1.36231711e-06
Iter: 1783 loss: 1.36006861e-06
Iter: 1784 loss: 1.36556059e-06
Iter: 1785 loss: 1.35919686e-06
Iter: 1786 loss: 1.35694586e-06
Iter: 1787 loss: 1.36575818e-06
Iter: 1788 loss: 1.35636833e-06
Iter: 1789 loss: 1.35418907e-06
Iter: 1790 loss: 1.36369204e-06
Iter: 1791 loss: 1.3537308e-06
Iter: 1792 loss: 1.35201321e-06
Iter: 1793 loss: 1.35401069e-06
Iter: 1794 loss: 1.35111793e-06
Iter: 1795 loss: 1.34870879e-06
Iter: 1796 loss: 1.36643257e-06
Iter: 1797 loss: 1.34847107e-06
Iter: 1798 loss: 1.34707273e-06
Iter: 1799 loss: 1.34456286e-06
Iter: 1800 loss: 1.40521752e-06
Iter: 1801 loss: 1.34454058e-06
Iter: 1802 loss: 1.34206755e-06
Iter: 1803 loss: 1.36249901e-06
Iter: 1804 loss: 1.34191885e-06
Iter: 1805 loss: 1.33991387e-06
Iter: 1806 loss: 1.34914353e-06
Iter: 1807 loss: 1.33957599e-06
Iter: 1808 loss: 1.33765559e-06
Iter: 1809 loss: 1.3400936e-06
Iter: 1810 loss: 1.33666572e-06
Iter: 1811 loss: 1.33478363e-06
Iter: 1812 loss: 1.33996582e-06
Iter: 1813 loss: 1.33419849e-06
Iter: 1814 loss: 1.33184301e-06
Iter: 1815 loss: 1.3395038e-06
Iter: 1816 loss: 1.33121284e-06
Iter: 1817 loss: 1.32935224e-06
Iter: 1818 loss: 1.32872538e-06
Iter: 1819 loss: 1.32766263e-06
Iter: 1820 loss: 1.32557034e-06
Iter: 1821 loss: 1.35167625e-06
Iter: 1822 loss: 1.32554896e-06
Iter: 1823 loss: 1.32411162e-06
Iter: 1824 loss: 1.32877369e-06
Iter: 1825 loss: 1.32370724e-06
Iter: 1826 loss: 1.32209107e-06
Iter: 1827 loss: 1.32003402e-06
Iter: 1828 loss: 1.3198877e-06
Iter: 1829 loss: 1.31722163e-06
Iter: 1830 loss: 1.32556966e-06
Iter: 1831 loss: 1.31648085e-06
Iter: 1832 loss: 1.31423678e-06
Iter: 1833 loss: 1.34095922e-06
Iter: 1834 loss: 1.31419267e-06
Iter: 1835 loss: 1.31231764e-06
Iter: 1836 loss: 1.32042419e-06
Iter: 1837 loss: 1.31195702e-06
Iter: 1838 loss: 1.31038632e-06
Iter: 1839 loss: 1.30922535e-06
Iter: 1840 loss: 1.30870626e-06
Iter: 1841 loss: 1.30647561e-06
Iter: 1842 loss: 1.31872901e-06
Iter: 1843 loss: 1.3061333e-06
Iter: 1844 loss: 1.30415583e-06
Iter: 1845 loss: 1.30772378e-06
Iter: 1846 loss: 1.30335695e-06
Iter: 1847 loss: 1.30140211e-06
Iter: 1848 loss: 1.30307785e-06
Iter: 1849 loss: 1.30035187e-06
Iter: 1850 loss: 1.29847865e-06
Iter: 1851 loss: 1.32192531e-06
Iter: 1852 loss: 1.29847172e-06
Iter: 1853 loss: 1.29690966e-06
Iter: 1854 loss: 1.29803743e-06
Iter: 1855 loss: 1.29594923e-06
Iter: 1856 loss: 1.29412228e-06
Iter: 1857 loss: 1.29764601e-06
Iter: 1858 loss: 1.29336513e-06
Iter: 1859 loss: 1.29138016e-06
Iter: 1860 loss: 1.30125829e-06
Iter: 1861 loss: 1.29103751e-06
Iter: 1862 loss: 1.28921351e-06
Iter: 1863 loss: 1.28915315e-06
Iter: 1864 loss: 1.28767783e-06
Iter: 1865 loss: 1.28575653e-06
Iter: 1866 loss: 1.29826162e-06
Iter: 1867 loss: 1.2854922e-06
Iter: 1868 loss: 1.28384283e-06
Iter: 1869 loss: 1.29214857e-06
Iter: 1870 loss: 1.28348984e-06
Iter: 1871 loss: 1.28208308e-06
Iter: 1872 loss: 1.27963892e-06
Iter: 1873 loss: 1.27962755e-06
Iter: 1874 loss: 1.27806766e-06
Iter: 1875 loss: 1.27799422e-06
Iter: 1876 loss: 1.27629596e-06
Iter: 1877 loss: 1.27907867e-06
Iter: 1878 loss: 1.2755072e-06
Iter: 1879 loss: 1.27392673e-06
Iter: 1880 loss: 1.27345766e-06
Iter: 1881 loss: 1.27252474e-06
Iter: 1882 loss: 1.27034173e-06
Iter: 1883 loss: 1.27382566e-06
Iter: 1884 loss: 1.26926125e-06
Iter: 1885 loss: 1.26712268e-06
Iter: 1886 loss: 1.28244892e-06
Iter: 1887 loss: 1.26696682e-06
Iter: 1888 loss: 1.26497662e-06
Iter: 1889 loss: 1.26995155e-06
Iter: 1890 loss: 1.2643427e-06
Iter: 1891 loss: 1.26226359e-06
Iter: 1892 loss: 1.2661867e-06
Iter: 1893 loss: 1.26141322e-06
Iter: 1894 loss: 1.25957069e-06
Iter: 1895 loss: 1.2745877e-06
Iter: 1896 loss: 1.25945076e-06
Iter: 1897 loss: 1.25799454e-06
Iter: 1898 loss: 1.25847521e-06
Iter: 1899 loss: 1.2569692e-06
Iter: 1900 loss: 1.255136e-06
Iter: 1901 loss: 1.25781708e-06
Iter: 1902 loss: 1.25427891e-06
Iter: 1903 loss: 1.2522421e-06
Iter: 1904 loss: 1.27015539e-06
Iter: 1905 loss: 1.25218787e-06
Iter: 1906 loss: 1.25093015e-06
Iter: 1907 loss: 1.25076701e-06
Iter: 1908 loss: 1.24981079e-06
Iter: 1909 loss: 1.24781138e-06
Iter: 1910 loss: 1.25537815e-06
Iter: 1911 loss: 1.24728217e-06
Iter: 1912 loss: 1.24556789e-06
Iter: 1913 loss: 1.25518147e-06
Iter: 1914 loss: 1.24532858e-06
Iter: 1915 loss: 1.24391454e-06
Iter: 1916 loss: 1.24534085e-06
Iter: 1917 loss: 1.24321525e-06
Iter: 1918 loss: 1.24149642e-06
Iter: 1919 loss: 1.25662336e-06
Iter: 1920 loss: 1.24140752e-06
Iter: 1921 loss: 1.24030737e-06
Iter: 1922 loss: 1.23805194e-06
Iter: 1923 loss: 1.27760768e-06
Iter: 1924 loss: 1.23795633e-06
Iter: 1925 loss: 1.23585562e-06
Iter: 1926 loss: 1.24942767e-06
Iter: 1927 loss: 1.23561972e-06
Iter: 1928 loss: 1.23358814e-06
Iter: 1929 loss: 1.23302982e-06
Iter: 1930 loss: 1.23184373e-06
Iter: 1931 loss: 1.22919914e-06
Iter: 1932 loss: 1.2490915e-06
Iter: 1933 loss: 1.22898791e-06
Iter: 1934 loss: 1.22713743e-06
Iter: 1935 loss: 1.23729399e-06
Iter: 1936 loss: 1.22684094e-06
Iter: 1937 loss: 1.22472852e-06
Iter: 1938 loss: 1.23055293e-06
Iter: 1939 loss: 1.22403321e-06
Iter: 1940 loss: 1.22246342e-06
Iter: 1941 loss: 1.2241444e-06
Iter: 1942 loss: 1.22163885e-06
Iter: 1943 loss: 1.21969208e-06
Iter: 1944 loss: 1.22871336e-06
Iter: 1945 loss: 1.21932e-06
Iter: 1946 loss: 1.21743255e-06
Iter: 1947 loss: 1.21910239e-06
Iter: 1948 loss: 1.21637913e-06
Iter: 1949 loss: 1.21451217e-06
Iter: 1950 loss: 1.22608742e-06
Iter: 1951 loss: 1.21428013e-06
Iter: 1952 loss: 1.21266817e-06
Iter: 1953 loss: 1.21605535e-06
Iter: 1954 loss: 1.2120654e-06
Iter: 1955 loss: 1.21025164e-06
Iter: 1956 loss: 1.21559378e-06
Iter: 1957 loss: 1.20973789e-06
Iter: 1958 loss: 1.208409e-06
Iter: 1959 loss: 1.22233462e-06
Iter: 1960 loss: 1.20839911e-06
Iter: 1961 loss: 1.20738366e-06
Iter: 1962 loss: 1.20562959e-06
Iter: 1963 loss: 1.24893097e-06
Iter: 1964 loss: 1.20562504e-06
Iter: 1965 loss: 1.20354173e-06
Iter: 1966 loss: 1.22082361e-06
Iter: 1967 loss: 1.20339507e-06
Iter: 1968 loss: 1.20180255e-06
Iter: 1969 loss: 1.20174013e-06
Iter: 1970 loss: 1.20051823e-06
Iter: 1971 loss: 1.19852643e-06
Iter: 1972 loss: 1.20492416e-06
Iter: 1973 loss: 1.19789911e-06
Iter: 1974 loss: 1.19605625e-06
Iter: 1975 loss: 1.19779429e-06
Iter: 1976 loss: 1.19491858e-06
Iter: 1977 loss: 1.19278388e-06
Iter: 1978 loss: 1.20412255e-06
Iter: 1979 loss: 1.19248512e-06
Iter: 1980 loss: 1.19060383e-06
Iter: 1981 loss: 1.20576954e-06
Iter: 1982 loss: 1.19049923e-06
Iter: 1983 loss: 1.18900198e-06
Iter: 1984 loss: 1.18891126e-06
Iter: 1985 loss: 1.18781008e-06
Iter: 1986 loss: 1.18585911e-06
Iter: 1987 loss: 1.18704702e-06
Iter: 1988 loss: 1.18464777e-06
Iter: 1989 loss: 1.18269872e-06
Iter: 1990 loss: 1.21060111e-06
Iter: 1991 loss: 1.18269281e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi3
+ date
Wed Nov  4 16:55:15 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi2.8/300_300_300_1 --function f2 --psi 3 --alpha 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67eb23c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67eb23c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7c6d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7cbe048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7cbeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7c9e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7c17bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7c17f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7cbe950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7b9dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7b9d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7bd3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7bfc158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a010a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a00a3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a0157268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a0157510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a015e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a015e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a015e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a00e87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a00e88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a00e8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a015e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67803dd0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a00089d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67a001dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67c7c60840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67803a4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6780382730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67803829d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6780325048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6780325730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67803a02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67802cb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67802a5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0065102926
test_loss: 0.009415637
train_loss: 0.0054224106
test_loss: 0.00907916
train_loss: 0.0059515364
test_loss: 0.009242956
train_loss: 0.0048847157
test_loss: 0.009133447
train_loss: 0.005360433
test_loss: 0.009078701
train_loss: 0.0048321546
test_loss: 0.008926429
train_loss: 0.005822726
test_loss: 0.009058848
train_loss: 0.004654677
test_loss: 0.008743705
train_loss: 0.0048053823
test_loss: 0.008993943
train_loss: 0.004745933
test_loss: 0.008811655
train_loss: 0.0056352774
test_loss: 0.009067216
train_loss: 0.005097232
test_loss: 0.008956668
train_loss: 0.0049372553
test_loss: 0.008763963
train_loss: 0.0048231306
test_loss: 0.008836596
train_loss: 0.0054859975
test_loss: 0.008758882
train_loss: 0.005382647
test_loss: 0.0086151855
train_loss: 0.0046619084
test_loss: 0.008881067
train_loss: 0.0049625346
test_loss: 0.008661249
train_loss: 0.0052851783
test_loss: 0.008854777
train_loss: 0.0045477995
test_loss: 0.008740545
train_loss: 0.0052318284
test_loss: 0.008931931
train_loss: 0.004641539
test_loss: 0.008740246
train_loss: 0.0045750896
test_loss: 0.008716968
train_loss: 0.004914731
test_loss: 0.008880679
train_loss: 0.0062188413
test_loss: 0.008816987
train_loss: 0.004705796
test_loss: 0.008656237
train_loss: 0.005039233
test_loss: 0.008845099
train_loss: 0.004757095
test_loss: 0.008725577
train_loss: 0.00514
test_loss: 0.008786541
train_loss: 0.0048852125
test_loss: 0.008788902
train_loss: 0.00494421
test_loss: 0.008667658
train_loss: 0.0053522857
test_loss: 0.00878757
train_loss: 0.004393135
test_loss: 0.008568341
train_loss: 0.0044780737
test_loss: 0.008712371
train_loss: 0.0051972736
test_loss: 0.008773575
train_loss: 0.0047486043
test_loss: 0.00864365
train_loss: 0.004713364
test_loss: 0.008644578
train_loss: 0.00501749
test_loss: 0.008640944
train_loss: 0.0046101
test_loss: 0.008621944
train_loss: 0.004792038
test_loss: 0.008608042
train_loss: 0.004647512
test_loss: 0.00873289
train_loss: 0.0051921513
test_loss: 0.008665295
train_loss: 0.004431952
test_loss: 0.008656664
train_loss: 0.004764457
test_loss: 0.008575062
train_loss: 0.004684018
test_loss: 0.008728982
train_loss: 0.004743432
test_loss: 0.008718515
train_loss: 0.0049419743
test_loss: 0.008723056
train_loss: 0.0050753327
test_loss: 0.008851073
train_loss: 0.004404609
test_loss: 0.008754933
train_loss: 0.004614166
test_loss: 0.008716117
train_loss: 0.005058123
test_loss: 0.008709649
train_loss: 0.004558293
test_loss: 0.00861934
train_loss: 0.004625377
test_loss: 0.008789995
train_loss: 0.004407308
test_loss: 0.008539241
train_loss: 0.0049057463
test_loss: 0.008750577
train_loss: 0.0046909545
test_loss: 0.008648049
train_loss: 0.0045680343
test_loss: 0.008563891
train_loss: 0.0051408433
test_loss: 0.008571728
train_loss: 0.004647051
test_loss: 0.008601964
train_loss: 0.0048074424
test_loss: 0.008818413
train_loss: 0.0047375495
test_loss: 0.008790512
train_loss: 0.004786603
test_loss: 0.008705306
train_loss: 0.004439594
test_loss: 0.008537594
train_loss: 0.004529833
test_loss: 0.008459708
train_loss: 0.004510388
test_loss: 0.008566089
train_loss: 0.0042554648
test_loss: 0.008526443
train_loss: 0.004643255
test_loss: 0.008716061
train_loss: 0.004561574
test_loss: 0.008670847
train_loss: 0.004770889
test_loss: 0.008682037
train_loss: 0.0042276047
test_loss: 0.008554519
train_loss: 0.0045847613
test_loss: 0.008693885
train_loss: 0.0047904053
test_loss: 0.0086544445
train_loss: 0.004463207
test_loss: 0.00845058
train_loss: 0.0043867165
test_loss: 0.0085745575
train_loss: 0.004319151
test_loss: 0.008554996
train_loss: 0.0048103984
test_loss: 0.008609178
train_loss: 0.004562948
test_loss: 0.00853092
train_loss: 0.004327689
test_loss: 0.008465692
train_loss: 0.004679252
test_loss: 0.008516981
train_loss: 0.004465554
test_loss: 0.008563857
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi3/300_300_300_1 --optimizer lbfgs --function f2 --psi 3 --alpha 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi3_phi3/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af25c9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af2541a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af24aa488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af24d5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af24b61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af24b66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af243dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af243d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af243d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af23b0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af25d69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af236d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af2389b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af236d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af239fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af2356f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0ace1540d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0ace1047b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0ace11f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0ace11f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0ace0db8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0ace07aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0ace07ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0af2421510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0ace06c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0ace06c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0ace06c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0ace06c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0acdfd6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0acdf9f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0acdf9fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0acdf60158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0acdf60378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0acdf18840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0acdf60730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0acdeed840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.65766646e-05
Iter: 2 loss: 3.33196331e-05
Iter: 3 loss: 2.83180761e-05
Iter: 4 loss: 2.8244398e-05
Iter: 5 loss: 2.41702946e-05
Iter: 6 loss: 4.08779924e-05
Iter: 7 loss: 2.32663479e-05
Iter: 8 loss: 2.06150107e-05
Iter: 9 loss: 4.13199887e-05
Iter: 10 loss: 2.04292483e-05
Iter: 11 loss: 1.89955317e-05
Iter: 12 loss: 2.43583818e-05
Iter: 13 loss: 1.86469188e-05
Iter: 14 loss: 1.75400583e-05
Iter: 15 loss: 1.76320318e-05
Iter: 16 loss: 1.6681357e-05
Iter: 17 loss: 1.55124e-05
Iter: 18 loss: 2.23673524e-05
Iter: 19 loss: 1.53600122e-05
Iter: 20 loss: 1.4345409e-05
Iter: 21 loss: 1.83811044e-05
Iter: 22 loss: 1.41156906e-05
Iter: 23 loss: 1.335664e-05
Iter: 24 loss: 1.39420936e-05
Iter: 25 loss: 1.28959455e-05
Iter: 26 loss: 1.22714318e-05
Iter: 27 loss: 1.78850241e-05
Iter: 28 loss: 1.22419888e-05
Iter: 29 loss: 1.16943374e-05
Iter: 30 loss: 1.32909699e-05
Iter: 31 loss: 1.15249113e-05
Iter: 32 loss: 1.11448644e-05
Iter: 33 loss: 1.11343352e-05
Iter: 34 loss: 1.08372606e-05
Iter: 35 loss: 1.04251849e-05
Iter: 36 loss: 1.42678236e-05
Iter: 37 loss: 1.04081291e-05
Iter: 38 loss: 1.01203659e-05
Iter: 39 loss: 1.1086142e-05
Iter: 40 loss: 1.00419693e-05
Iter: 41 loss: 9.77278796e-06
Iter: 42 loss: 9.9269364e-06
Iter: 43 loss: 9.59779481e-06
Iter: 44 loss: 9.32882631e-06
Iter: 45 loss: 1.07105388e-05
Iter: 46 loss: 9.28549707e-06
Iter: 47 loss: 9.06915739e-06
Iter: 48 loss: 1.22343e-05
Iter: 49 loss: 9.06897549e-06
Iter: 50 loss: 8.95611356e-06
Iter: 51 loss: 8.86030102e-06
Iter: 52 loss: 8.82893619e-06
Iter: 53 loss: 8.61558146e-06
Iter: 54 loss: 9.55126234e-06
Iter: 55 loss: 8.57263058e-06
Iter: 56 loss: 8.45227623e-06
Iter: 57 loss: 8.52720768e-06
Iter: 58 loss: 8.37526386e-06
Iter: 59 loss: 8.2217739e-06
Iter: 60 loss: 9.28202189e-06
Iter: 61 loss: 8.20728928e-06
Iter: 62 loss: 8.09106e-06
Iter: 63 loss: 8.28972679e-06
Iter: 64 loss: 8.03909461e-06
Iter: 65 loss: 7.91625553e-06
Iter: 66 loss: 8.05211312e-06
Iter: 67 loss: 7.84958866e-06
Iter: 68 loss: 7.74526598e-06
Iter: 69 loss: 8.9824016e-06
Iter: 70 loss: 7.74392811e-06
Iter: 71 loss: 7.65919049e-06
Iter: 72 loss: 7.69650615e-06
Iter: 73 loss: 7.60159128e-06
Iter: 74 loss: 7.49828587e-06
Iter: 75 loss: 7.52894584e-06
Iter: 76 loss: 7.42406473e-06
Iter: 77 loss: 7.31543514e-06
Iter: 78 loss: 8.11043901e-06
Iter: 79 loss: 7.30655211e-06
Iter: 80 loss: 7.21938341e-06
Iter: 81 loss: 7.52457663e-06
Iter: 82 loss: 7.19659602e-06
Iter: 83 loss: 7.11607481e-06
Iter: 84 loss: 7.29698195e-06
Iter: 85 loss: 7.08558491e-06
Iter: 86 loss: 7.00966893e-06
Iter: 87 loss: 7.26720646e-06
Iter: 88 loss: 6.98906751e-06
Iter: 89 loss: 6.91347304e-06
Iter: 90 loss: 7.64338893e-06
Iter: 91 loss: 6.91055629e-06
Iter: 92 loss: 6.86685416e-06
Iter: 93 loss: 6.81343681e-06
Iter: 94 loss: 6.80860512e-06
Iter: 95 loss: 6.74299099e-06
Iter: 96 loss: 7.30343572e-06
Iter: 97 loss: 6.73927661e-06
Iter: 98 loss: 6.68393614e-06
Iter: 99 loss: 6.74345665e-06
Iter: 100 loss: 6.65355492e-06
Iter: 101 loss: 6.59670241e-06
Iter: 102 loss: 6.79595087e-06
Iter: 103 loss: 6.58206454e-06
Iter: 104 loss: 6.53385314e-06
Iter: 105 loss: 6.7723804e-06
Iter: 106 loss: 6.52553081e-06
Iter: 107 loss: 6.47860634e-06
Iter: 108 loss: 6.50649872e-06
Iter: 109 loss: 6.44827469e-06
Iter: 110 loss: 6.40033568e-06
Iter: 111 loss: 6.52071776e-06
Iter: 112 loss: 6.38373604e-06
Iter: 113 loss: 6.33069703e-06
Iter: 114 loss: 6.69556312e-06
Iter: 115 loss: 6.3257894e-06
Iter: 116 loss: 6.28692806e-06
Iter: 117 loss: 6.23554206e-06
Iter: 118 loss: 6.23237202e-06
Iter: 119 loss: 6.17692967e-06
Iter: 120 loss: 6.4208316e-06
Iter: 121 loss: 6.16587295e-06
Iter: 122 loss: 6.11647192e-06
Iter: 123 loss: 6.45575574e-06
Iter: 124 loss: 6.1117662e-06
Iter: 125 loss: 6.06905724e-06
Iter: 126 loss: 6.17434853e-06
Iter: 127 loss: 6.05393871e-06
Iter: 128 loss: 6.01583679e-06
Iter: 129 loss: 6.11192718e-06
Iter: 130 loss: 6.00241401e-06
Iter: 131 loss: 5.96342579e-06
Iter: 132 loss: 6.38710708e-06
Iter: 133 loss: 5.96247e-06
Iter: 134 loss: 5.9392537e-06
Iter: 135 loss: 5.9152635e-06
Iter: 136 loss: 5.91077105e-06
Iter: 137 loss: 5.87393606e-06
Iter: 138 loss: 6.04279376e-06
Iter: 139 loss: 5.86686565e-06
Iter: 140 loss: 5.83404608e-06
Iter: 141 loss: 5.97278722e-06
Iter: 142 loss: 5.8268497e-06
Iter: 143 loss: 5.79845891e-06
Iter: 144 loss: 5.80806409e-06
Iter: 145 loss: 5.77821947e-06
Iter: 146 loss: 5.74424757e-06
Iter: 147 loss: 5.97487724e-06
Iter: 148 loss: 5.74076512e-06
Iter: 149 loss: 5.71105556e-06
Iter: 150 loss: 5.75269451e-06
Iter: 151 loss: 5.69624171e-06
Iter: 152 loss: 5.66788e-06
Iter: 153 loss: 5.73910029e-06
Iter: 154 loss: 5.65790378e-06
Iter: 155 loss: 5.62733658e-06
Iter: 156 loss: 5.74137266e-06
Iter: 157 loss: 5.61995421e-06
Iter: 158 loss: 5.58919692e-06
Iter: 159 loss: 5.66613e-06
Iter: 160 loss: 5.57840303e-06
Iter: 161 loss: 5.55452425e-06
Iter: 162 loss: 5.52028541e-06
Iter: 163 loss: 5.51920948e-06
Iter: 164 loss: 5.47885247e-06
Iter: 165 loss: 5.79756215e-06
Iter: 166 loss: 5.47609352e-06
Iter: 167 loss: 5.44521663e-06
Iter: 168 loss: 5.71500232e-06
Iter: 169 loss: 5.44375871e-06
Iter: 170 loss: 5.42090947e-06
Iter: 171 loss: 5.46237288e-06
Iter: 172 loss: 5.41095278e-06
Iter: 173 loss: 5.38515e-06
Iter: 174 loss: 5.58267493e-06
Iter: 175 loss: 5.38334461e-06
Iter: 176 loss: 5.36450625e-06
Iter: 177 loss: 5.35897107e-06
Iter: 178 loss: 5.34746414e-06
Iter: 179 loss: 5.32031845e-06
Iter: 180 loss: 5.33937055e-06
Iter: 181 loss: 5.30343459e-06
Iter: 182 loss: 5.28068e-06
Iter: 183 loss: 5.28062e-06
Iter: 184 loss: 5.26273288e-06
Iter: 185 loss: 5.2475466e-06
Iter: 186 loss: 5.24257939e-06
Iter: 187 loss: 5.2171481e-06
Iter: 188 loss: 5.37269307e-06
Iter: 189 loss: 5.21412358e-06
Iter: 190 loss: 5.1910506e-06
Iter: 191 loss: 5.2571213e-06
Iter: 192 loss: 5.18390789e-06
Iter: 193 loss: 5.16088085e-06
Iter: 194 loss: 5.17769922e-06
Iter: 195 loss: 5.14662315e-06
Iter: 196 loss: 5.12314136e-06
Iter: 197 loss: 5.26381154e-06
Iter: 198 loss: 5.12021597e-06
Iter: 199 loss: 5.09572783e-06
Iter: 200 loss: 5.13355e-06
Iter: 201 loss: 5.08430094e-06
Iter: 202 loss: 5.06526067e-06
Iter: 203 loss: 5.0523e-06
Iter: 204 loss: 5.04531181e-06
Iter: 205 loss: 5.01304385e-06
Iter: 206 loss: 5.18370871e-06
Iter: 207 loss: 5.00802616e-06
Iter: 208 loss: 4.9855671e-06
Iter: 209 loss: 5.07994537e-06
Iter: 210 loss: 4.98074314e-06
Iter: 211 loss: 4.95720406e-06
Iter: 212 loss: 5.1093989e-06
Iter: 213 loss: 4.95465338e-06
Iter: 214 loss: 4.93721927e-06
Iter: 215 loss: 5.01843624e-06
Iter: 216 loss: 4.93399739e-06
Iter: 217 loss: 4.91975879e-06
Iter: 218 loss: 4.91930405e-06
Iter: 219 loss: 4.90824641e-06
Iter: 220 loss: 4.88979276e-06
Iter: 221 loss: 4.92043e-06
Iter: 222 loss: 4.8811944e-06
Iter: 223 loss: 4.86043064e-06
Iter: 224 loss: 4.93878451e-06
Iter: 225 loss: 4.85538567e-06
Iter: 226 loss: 4.83689473e-06
Iter: 227 loss: 4.95112135e-06
Iter: 228 loss: 4.83458189e-06
Iter: 229 loss: 4.820296e-06
Iter: 230 loss: 4.81196594e-06
Iter: 231 loss: 4.80602921e-06
Iter: 232 loss: 4.78736683e-06
Iter: 233 loss: 4.96070425e-06
Iter: 234 loss: 4.78633501e-06
Iter: 235 loss: 4.76974856e-06
Iter: 236 loss: 4.78036782e-06
Iter: 237 loss: 4.75904881e-06
Iter: 238 loss: 4.74100943e-06
Iter: 239 loss: 4.77731055e-06
Iter: 240 loss: 4.73370756e-06
Iter: 241 loss: 4.71475141e-06
Iter: 242 loss: 4.86282534e-06
Iter: 243 loss: 4.71335943e-06
Iter: 244 loss: 4.6995292e-06
Iter: 245 loss: 4.69122369e-06
Iter: 246 loss: 4.68553026e-06
Iter: 247 loss: 4.66674373e-06
Iter: 248 loss: 4.67882319e-06
Iter: 249 loss: 4.65469657e-06
Iter: 250 loss: 4.63569631e-06
Iter: 251 loss: 4.87414491e-06
Iter: 252 loss: 4.63553943e-06
Iter: 253 loss: 4.61918853e-06
Iter: 254 loss: 4.72298143e-06
Iter: 255 loss: 4.61746549e-06
Iter: 256 loss: 4.60330193e-06
Iter: 257 loss: 4.59637431e-06
Iter: 258 loss: 4.5897541e-06
Iter: 259 loss: 4.57239912e-06
Iter: 260 loss: 4.64770346e-06
Iter: 261 loss: 4.5688239e-06
Iter: 262 loss: 4.55187728e-06
Iter: 263 loss: 4.57838087e-06
Iter: 264 loss: 4.54412657e-06
Iter: 265 loss: 4.52928043e-06
Iter: 266 loss: 4.59785315e-06
Iter: 267 loss: 4.52658696e-06
Iter: 268 loss: 4.51146252e-06
Iter: 269 loss: 4.55369e-06
Iter: 270 loss: 4.50678181e-06
Iter: 271 loss: 4.49104118e-06
Iter: 272 loss: 4.49403797e-06
Iter: 273 loss: 4.47939783e-06
Iter: 274 loss: 4.46299418e-06
Iter: 275 loss: 4.59903549e-06
Iter: 276 loss: 4.46201466e-06
Iter: 277 loss: 4.44727812e-06
Iter: 278 loss: 4.46322383e-06
Iter: 279 loss: 4.43930548e-06
Iter: 280 loss: 4.42073633e-06
Iter: 281 loss: 4.44558964e-06
Iter: 282 loss: 4.41149405e-06
Iter: 283 loss: 4.39833093e-06
Iter: 284 loss: 4.56084308e-06
Iter: 285 loss: 4.39824544e-06
Iter: 286 loss: 4.38705956e-06
Iter: 287 loss: 4.37806921e-06
Iter: 288 loss: 4.37469862e-06
Iter: 289 loss: 4.35655693e-06
Iter: 290 loss: 4.38788629e-06
Iter: 291 loss: 4.34849335e-06
Iter: 292 loss: 4.33511286e-06
Iter: 293 loss: 4.47021193e-06
Iter: 294 loss: 4.33466357e-06
Iter: 295 loss: 4.31870603e-06
Iter: 296 loss: 4.33812465e-06
Iter: 297 loss: 4.31039189e-06
Iter: 298 loss: 4.29741704e-06
Iter: 299 loss: 4.33111609e-06
Iter: 300 loss: 4.293046e-06
Iter: 301 loss: 4.2808465e-06
Iter: 302 loss: 4.27016312e-06
Iter: 303 loss: 4.26697443e-06
Iter: 304 loss: 4.25257895e-06
Iter: 305 loss: 4.25243707e-06
Iter: 306 loss: 4.24296e-06
Iter: 307 loss: 4.241846e-06
Iter: 308 loss: 4.23502297e-06
Iter: 309 loss: 4.22033554e-06
Iter: 310 loss: 4.27798841e-06
Iter: 311 loss: 4.21702e-06
Iter: 312 loss: 4.20233118e-06
Iter: 313 loss: 4.22351832e-06
Iter: 314 loss: 4.19518892e-06
Iter: 315 loss: 4.18123e-06
Iter: 316 loss: 4.20741935e-06
Iter: 317 loss: 4.17533738e-06
Iter: 318 loss: 4.15933755e-06
Iter: 319 loss: 4.27100576e-06
Iter: 320 loss: 4.15785689e-06
Iter: 321 loss: 4.14686292e-06
Iter: 322 loss: 4.14324359e-06
Iter: 323 loss: 4.1369517e-06
Iter: 324 loss: 4.12371173e-06
Iter: 325 loss: 4.23863276e-06
Iter: 326 loss: 4.12305462e-06
Iter: 327 loss: 4.11160499e-06
Iter: 328 loss: 4.12619374e-06
Iter: 329 loss: 4.10574285e-06
Iter: 330 loss: 4.09375252e-06
Iter: 331 loss: 4.10498933e-06
Iter: 332 loss: 4.08685582e-06
Iter: 333 loss: 4.07652624e-06
Iter: 334 loss: 4.0765226e-06
Iter: 335 loss: 4.06704612e-06
Iter: 336 loss: 4.05529954e-06
Iter: 337 loss: 4.05419223e-06
Iter: 338 loss: 4.03987406e-06
Iter: 339 loss: 4.03207559e-06
Iter: 340 loss: 4.02570731e-06
Iter: 341 loss: 4.01299849e-06
Iter: 342 loss: 4.01269062e-06
Iter: 343 loss: 4.00150657e-06
Iter: 344 loss: 4.01767329e-06
Iter: 345 loss: 3.996141e-06
Iter: 346 loss: 3.98385782e-06
Iter: 347 loss: 4.03610557e-06
Iter: 348 loss: 3.98126576e-06
Iter: 349 loss: 3.97122221e-06
Iter: 350 loss: 3.98026032e-06
Iter: 351 loss: 3.96530913e-06
Iter: 352 loss: 3.95121788e-06
Iter: 353 loss: 4.00236331e-06
Iter: 354 loss: 3.94770905e-06
Iter: 355 loss: 3.93791561e-06
Iter: 356 loss: 3.9473839e-06
Iter: 357 loss: 3.93229857e-06
Iter: 358 loss: 3.91728827e-06
Iter: 359 loss: 3.97611029e-06
Iter: 360 loss: 3.91378535e-06
Iter: 361 loss: 3.9039769e-06
Iter: 362 loss: 3.90771356e-06
Iter: 363 loss: 3.89733395e-06
Iter: 364 loss: 3.8850103e-06
Iter: 365 loss: 3.97403e-06
Iter: 366 loss: 3.88407079e-06
Iter: 367 loss: 3.87334512e-06
Iter: 368 loss: 3.88504577e-06
Iter: 369 loss: 3.86756619e-06
Iter: 370 loss: 3.85802878e-06
Iter: 371 loss: 3.95347797e-06
Iter: 372 loss: 3.85768499e-06
Iter: 373 loss: 3.84898294e-06
Iter: 374 loss: 3.86019565e-06
Iter: 375 loss: 3.8446592e-06
Iter: 376 loss: 3.83612314e-06
Iter: 377 loss: 3.8299886e-06
Iter: 378 loss: 3.82700182e-06
Iter: 379 loss: 3.81304403e-06
Iter: 380 loss: 3.83363204e-06
Iter: 381 loss: 3.80629899e-06
Iter: 382 loss: 3.79288917e-06
Iter: 383 loss: 3.88717535e-06
Iter: 384 loss: 3.79163453e-06
Iter: 385 loss: 3.78004734e-06
Iter: 386 loss: 3.81528571e-06
Iter: 387 loss: 3.77658853e-06
Iter: 388 loss: 3.76255321e-06
Iter: 389 loss: 3.80494657e-06
Iter: 390 loss: 3.75829018e-06
Iter: 391 loss: 3.74955425e-06
Iter: 392 loss: 3.75383433e-06
Iter: 393 loss: 3.74359411e-06
Iter: 394 loss: 3.73091211e-06
Iter: 395 loss: 3.79729204e-06
Iter: 396 loss: 3.72899945e-06
Iter: 397 loss: 3.71804936e-06
Iter: 398 loss: 3.74451861e-06
Iter: 399 loss: 3.71410624e-06
Iter: 400 loss: 3.70564567e-06
Iter: 401 loss: 3.72889508e-06
Iter: 402 loss: 3.70284647e-06
Iter: 403 loss: 3.69192503e-06
Iter: 404 loss: 3.70861289e-06
Iter: 405 loss: 3.6867018e-06
Iter: 406 loss: 3.67718098e-06
Iter: 407 loss: 3.6948154e-06
Iter: 408 loss: 3.67312714e-06
Iter: 409 loss: 3.66332392e-06
Iter: 410 loss: 3.79273592e-06
Iter: 411 loss: 3.66331483e-06
Iter: 412 loss: 3.65781511e-06
Iter: 413 loss: 3.65894e-06
Iter: 414 loss: 3.6537233e-06
Iter: 415 loss: 3.64381162e-06
Iter: 416 loss: 3.64065909e-06
Iter: 417 loss: 3.63479899e-06
Iter: 418 loss: 3.62390438e-06
Iter: 419 loss: 3.6653762e-06
Iter: 420 loss: 3.62123092e-06
Iter: 421 loss: 3.61068169e-06
Iter: 422 loss: 3.61749653e-06
Iter: 423 loss: 3.60400645e-06
Iter: 424 loss: 3.59130127e-06
Iter: 425 loss: 3.61437469e-06
Iter: 426 loss: 3.58584657e-06
Iter: 427 loss: 3.57707268e-06
Iter: 428 loss: 3.57691169e-06
Iter: 429 loss: 3.56892838e-06
Iter: 430 loss: 3.56701776e-06
Iter: 431 loss: 3.56196892e-06
Iter: 432 loss: 3.55105203e-06
Iter: 433 loss: 3.56764986e-06
Iter: 434 loss: 3.54594567e-06
Iter: 435 loss: 3.53617247e-06
Iter: 436 loss: 3.61114644e-06
Iter: 437 loss: 3.53556811e-06
Iter: 438 loss: 3.52564462e-06
Iter: 439 loss: 3.53509199e-06
Iter: 440 loss: 3.520051e-06
Iter: 441 loss: 3.51070707e-06
Iter: 442 loss: 3.53175665e-06
Iter: 443 loss: 3.50734672e-06
Iter: 444 loss: 3.49853076e-06
Iter: 445 loss: 3.56809687e-06
Iter: 446 loss: 3.49791571e-06
Iter: 447 loss: 3.4910056e-06
Iter: 448 loss: 3.5080825e-06
Iter: 449 loss: 3.48846743e-06
Iter: 450 loss: 3.48063463e-06
Iter: 451 loss: 3.50558776e-06
Iter: 452 loss: 3.47828927e-06
Iter: 453 loss: 3.47116611e-06
Iter: 454 loss: 3.46664547e-06
Iter: 455 loss: 3.46386969e-06
Iter: 456 loss: 3.45303079e-06
Iter: 457 loss: 3.49118363e-06
Iter: 458 loss: 3.45008948e-06
Iter: 459 loss: 3.4388936e-06
Iter: 460 loss: 3.4738498e-06
Iter: 461 loss: 3.43555985e-06
Iter: 462 loss: 3.42619865e-06
Iter: 463 loss: 3.44131763e-06
Iter: 464 loss: 3.42181193e-06
Iter: 465 loss: 3.41034047e-06
Iter: 466 loss: 3.42993985e-06
Iter: 467 loss: 3.40527322e-06
Iter: 468 loss: 3.39596977e-06
Iter: 469 loss: 3.43811416e-06
Iter: 470 loss: 3.39415055e-06
Iter: 471 loss: 3.382881e-06
Iter: 472 loss: 3.427715e-06
Iter: 473 loss: 3.38039467e-06
Iter: 474 loss: 3.3728918e-06
Iter: 475 loss: 3.36688345e-06
Iter: 476 loss: 3.36472704e-06
Iter: 477 loss: 3.35398659e-06
Iter: 478 loss: 3.43933243e-06
Iter: 479 loss: 3.35327445e-06
Iter: 480 loss: 3.34368497e-06
Iter: 481 loss: 3.38297014e-06
Iter: 482 loss: 3.34154697e-06
Iter: 483 loss: 3.33405433e-06
Iter: 484 loss: 3.34050628e-06
Iter: 485 loss: 3.32970785e-06
Iter: 486 loss: 3.32240802e-06
Iter: 487 loss: 3.39991675e-06
Iter: 488 loss: 3.32222794e-06
Iter: 489 loss: 3.31502383e-06
Iter: 490 loss: 3.31384217e-06
Iter: 491 loss: 3.30874468e-06
Iter: 492 loss: 3.30139073e-06
Iter: 493 loss: 3.33471917e-06
Iter: 494 loss: 3.29985892e-06
Iter: 495 loss: 3.29211753e-06
Iter: 496 loss: 3.29072441e-06
Iter: 497 loss: 3.28537817e-06
Iter: 498 loss: 3.27524367e-06
Iter: 499 loss: 3.31874435e-06
Iter: 500 loss: 3.27325324e-06
Iter: 501 loss: 3.26500322e-06
Iter: 502 loss: 3.28386341e-06
Iter: 503 loss: 3.26193162e-06
Iter: 504 loss: 3.25263613e-06
Iter: 505 loss: 3.28924352e-06
Iter: 506 loss: 3.25055248e-06
Iter: 507 loss: 3.24197185e-06
Iter: 508 loss: 3.24154257e-06
Iter: 509 loss: 3.2349767e-06
Iter: 510 loss: 3.22599226e-06
Iter: 511 loss: 3.32489844e-06
Iter: 512 loss: 3.22577807e-06
Iter: 513 loss: 3.2173441e-06
Iter: 514 loss: 3.23030827e-06
Iter: 515 loss: 3.21335983e-06
Iter: 516 loss: 3.20385243e-06
Iter: 517 loss: 3.2064097e-06
Iter: 518 loss: 3.1970028e-06
Iter: 519 loss: 3.18759339e-06
Iter: 520 loss: 3.23271752e-06
Iter: 521 loss: 3.185786e-06
Iter: 522 loss: 3.17860577e-06
Iter: 523 loss: 3.26328563e-06
Iter: 524 loss: 3.17854233e-06
Iter: 525 loss: 3.17272338e-06
Iter: 526 loss: 3.16766295e-06
Iter: 527 loss: 3.1661782e-06
Iter: 528 loss: 3.15833336e-06
Iter: 529 loss: 3.15836837e-06
Iter: 530 loss: 3.15423063e-06
Iter: 531 loss: 3.14439649e-06
Iter: 532 loss: 3.26057784e-06
Iter: 533 loss: 3.14354929e-06
Iter: 534 loss: 3.13410783e-06
Iter: 535 loss: 3.23123027e-06
Iter: 536 loss: 3.13370515e-06
Iter: 537 loss: 3.12611201e-06
Iter: 538 loss: 3.15869579e-06
Iter: 539 loss: 3.12463908e-06
Iter: 540 loss: 3.11783469e-06
Iter: 541 loss: 3.11271742e-06
Iter: 542 loss: 3.11050053e-06
Iter: 543 loss: 3.10127461e-06
Iter: 544 loss: 3.16095975e-06
Iter: 545 loss: 3.10018595e-06
Iter: 546 loss: 3.09099232e-06
Iter: 547 loss: 3.11236454e-06
Iter: 548 loss: 3.08768313e-06
Iter: 549 loss: 3.07969481e-06
Iter: 550 loss: 3.11755e-06
Iter: 551 loss: 3.07817618e-06
Iter: 552 loss: 3.07155415e-06
Iter: 553 loss: 3.09182951e-06
Iter: 554 loss: 3.06957554e-06
Iter: 555 loss: 3.06272659e-06
Iter: 556 loss: 3.08642939e-06
Iter: 557 loss: 3.06086395e-06
Iter: 558 loss: 3.05419894e-06
Iter: 559 loss: 3.05105073e-06
Iter: 560 loss: 3.04786181e-06
Iter: 561 loss: 3.03984689e-06
Iter: 562 loss: 3.08424478e-06
Iter: 563 loss: 3.03861179e-06
Iter: 564 loss: 3.03219758e-06
Iter: 565 loss: 3.1132e-06
Iter: 566 loss: 3.03220077e-06
Iter: 567 loss: 3.02775766e-06
Iter: 568 loss: 3.02435433e-06
Iter: 569 loss: 3.02284116e-06
Iter: 570 loss: 3.01527302e-06
Iter: 571 loss: 3.05305184e-06
Iter: 572 loss: 3.01393493e-06
Iter: 573 loss: 3.00748502e-06
Iter: 574 loss: 3.00771922e-06
Iter: 575 loss: 3.0023748e-06
Iter: 576 loss: 2.99413227e-06
Iter: 577 loss: 2.99820772e-06
Iter: 578 loss: 2.98855184e-06
Iter: 579 loss: 2.97920042e-06
Iter: 580 loss: 3.05095432e-06
Iter: 581 loss: 2.97848555e-06
Iter: 582 loss: 2.9718733e-06
Iter: 583 loss: 3.01405771e-06
Iter: 584 loss: 2.97113183e-06
Iter: 585 loss: 2.96526582e-06
Iter: 586 loss: 2.96163807e-06
Iter: 587 loss: 2.95925133e-06
Iter: 588 loss: 2.95058862e-06
Iter: 589 loss: 2.98353416e-06
Iter: 590 loss: 2.94850179e-06
Iter: 591 loss: 2.9409548e-06
Iter: 592 loss: 2.98723535e-06
Iter: 593 loss: 2.94002211e-06
Iter: 594 loss: 2.93351604e-06
Iter: 595 loss: 2.95906284e-06
Iter: 596 loss: 2.93203721e-06
Iter: 597 loss: 2.92658137e-06
Iter: 598 loss: 2.92598202e-06
Iter: 599 loss: 2.92205777e-06
Iter: 600 loss: 2.91380911e-06
Iter: 601 loss: 2.96636426e-06
Iter: 602 loss: 2.91290235e-06
Iter: 603 loss: 2.90784396e-06
Iter: 604 loss: 2.91385959e-06
Iter: 605 loss: 2.90517301e-06
Iter: 606 loss: 2.89753234e-06
Iter: 607 loss: 2.93724247e-06
Iter: 608 loss: 2.89630475e-06
Iter: 609 loss: 2.89160153e-06
Iter: 610 loss: 2.88546926e-06
Iter: 611 loss: 2.88504907e-06
Iter: 612 loss: 2.87822832e-06
Iter: 613 loss: 2.98218129e-06
Iter: 614 loss: 2.87825287e-06
Iter: 615 loss: 2.87302146e-06
Iter: 616 loss: 2.86828754e-06
Iter: 617 loss: 2.86696309e-06
Iter: 618 loss: 2.85890019e-06
Iter: 619 loss: 2.8735858e-06
Iter: 620 loss: 2.85524607e-06
Iter: 621 loss: 2.84668363e-06
Iter: 622 loss: 2.87000557e-06
Iter: 623 loss: 2.84385396e-06
Iter: 624 loss: 2.83665099e-06
Iter: 625 loss: 2.89964692e-06
Iter: 626 loss: 2.83634631e-06
Iter: 627 loss: 2.82924657e-06
Iter: 628 loss: 2.83746522e-06
Iter: 629 loss: 2.82556607e-06
Iter: 630 loss: 2.81956682e-06
Iter: 631 loss: 2.82631663e-06
Iter: 632 loss: 2.81636812e-06
Iter: 633 loss: 2.80859854e-06
Iter: 634 loss: 2.85985061e-06
Iter: 635 loss: 2.80784025e-06
Iter: 636 loss: 2.80173867e-06
Iter: 637 loss: 2.82381029e-06
Iter: 638 loss: 2.80007362e-06
Iter: 639 loss: 2.79506776e-06
Iter: 640 loss: 2.79505025e-06
Iter: 641 loss: 2.79093229e-06
Iter: 642 loss: 2.78391826e-06
Iter: 643 loss: 2.8420568e-06
Iter: 644 loss: 2.78335392e-06
Iter: 645 loss: 2.77789377e-06
Iter: 646 loss: 2.800522e-06
Iter: 647 loss: 2.77673507e-06
Iter: 648 loss: 2.77238132e-06
Iter: 649 loss: 2.77265281e-06
Iter: 650 loss: 2.76895889e-06
Iter: 651 loss: 2.76166406e-06
Iter: 652 loss: 2.76607784e-06
Iter: 653 loss: 2.75707316e-06
Iter: 654 loss: 2.75043658e-06
Iter: 655 loss: 2.79187771e-06
Iter: 656 loss: 2.74964714e-06
Iter: 657 loss: 2.74353852e-06
Iter: 658 loss: 2.76805849e-06
Iter: 659 loss: 2.74220565e-06
Iter: 660 loss: 2.73716978e-06
Iter: 661 loss: 2.73106934e-06
Iter: 662 loss: 2.73053e-06
Iter: 663 loss: 2.72281159e-06
Iter: 664 loss: 2.76681681e-06
Iter: 665 loss: 2.72170746e-06
Iter: 666 loss: 2.71458566e-06
Iter: 667 loss: 2.73295063e-06
Iter: 668 loss: 2.71213e-06
Iter: 669 loss: 2.70612691e-06
Iter: 670 loss: 2.7529311e-06
Iter: 671 loss: 2.70569444e-06
Iter: 672 loss: 2.69989e-06
Iter: 673 loss: 2.7026108e-06
Iter: 674 loss: 2.69590237e-06
Iter: 675 loss: 2.69001612e-06
Iter: 676 loss: 2.71501585e-06
Iter: 677 loss: 2.68895155e-06
Iter: 678 loss: 2.68248959e-06
Iter: 679 loss: 2.70288956e-06
Iter: 680 loss: 2.68060649e-06
Iter: 681 loss: 2.6748794e-06
Iter: 682 loss: 2.68522035e-06
Iter: 683 loss: 2.67233099e-06
Iter: 684 loss: 2.66770803e-06
Iter: 685 loss: 2.73122396e-06
Iter: 686 loss: 2.66769939e-06
Iter: 687 loss: 2.66399775e-06
Iter: 688 loss: 2.65693939e-06
Iter: 689 loss: 2.80146151e-06
Iter: 690 loss: 2.6568764e-06
Iter: 691 loss: 2.65044537e-06
Iter: 692 loss: 2.68087092e-06
Iter: 693 loss: 2.64924029e-06
Iter: 694 loss: 2.64191567e-06
Iter: 695 loss: 2.6683249e-06
Iter: 696 loss: 2.64007804e-06
Iter: 697 loss: 2.63516517e-06
Iter: 698 loss: 2.64960227e-06
Iter: 699 loss: 2.6336686e-06
Iter: 700 loss: 2.62796675e-06
Iter: 701 loss: 2.6403718e-06
Iter: 702 loss: 2.62589333e-06
Iter: 703 loss: 2.62005551e-06
Iter: 704 loss: 2.63049469e-06
Iter: 705 loss: 2.61760397e-06
Iter: 706 loss: 2.61193782e-06
Iter: 707 loss: 2.60669822e-06
Iter: 708 loss: 2.60528964e-06
Iter: 709 loss: 2.59763033e-06
Iter: 710 loss: 2.70007854e-06
Iter: 711 loss: 2.59758781e-06
Iter: 712 loss: 2.59260582e-06
Iter: 713 loss: 2.6257253e-06
Iter: 714 loss: 2.59199533e-06
Iter: 715 loss: 2.58691716e-06
Iter: 716 loss: 2.58395312e-06
Iter: 717 loss: 2.58175828e-06
Iter: 718 loss: 2.57692204e-06
Iter: 719 loss: 2.57682586e-06
Iter: 720 loss: 2.5728109e-06
Iter: 721 loss: 2.57216311e-06
Iter: 722 loss: 2.56926751e-06
Iter: 723 loss: 2.56389353e-06
Iter: 724 loss: 2.60642446e-06
Iter: 725 loss: 2.5634572e-06
Iter: 726 loss: 2.55930649e-06
Iter: 727 loss: 2.55723489e-06
Iter: 728 loss: 2.55520172e-06
Iter: 729 loss: 2.54878478e-06
Iter: 730 loss: 2.55655618e-06
Iter: 731 loss: 2.54548559e-06
Iter: 732 loss: 2.5389e-06
Iter: 733 loss: 2.56355e-06
Iter: 734 loss: 2.53734902e-06
Iter: 735 loss: 2.53161306e-06
Iter: 736 loss: 2.57980719e-06
Iter: 737 loss: 2.53132271e-06
Iter: 738 loss: 2.52664472e-06
Iter: 739 loss: 2.52328618e-06
Iter: 740 loss: 2.52163773e-06
Iter: 741 loss: 2.51532128e-06
Iter: 742 loss: 2.55895065e-06
Iter: 743 loss: 2.51478514e-06
Iter: 744 loss: 2.50959692e-06
Iter: 745 loss: 2.52043355e-06
Iter: 746 loss: 2.50760604e-06
Iter: 747 loss: 2.5017066e-06
Iter: 748 loss: 2.50148196e-06
Iter: 749 loss: 2.496766e-06
Iter: 750 loss: 2.49021514e-06
Iter: 751 loss: 2.53977896e-06
Iter: 752 loss: 2.48977904e-06
Iter: 753 loss: 2.48484457e-06
Iter: 754 loss: 2.51863639e-06
Iter: 755 loss: 2.48441825e-06
Iter: 756 loss: 2.47989e-06
Iter: 757 loss: 2.48521224e-06
Iter: 758 loss: 2.47756816e-06
Iter: 759 loss: 2.47307662e-06
Iter: 760 loss: 2.50194307e-06
Iter: 761 loss: 2.47261028e-06
Iter: 762 loss: 2.4682779e-06
Iter: 763 loss: 2.46904619e-06
Iter: 764 loss: 2.46499e-06
Iter: 765 loss: 2.46057971e-06
Iter: 766 loss: 2.47873163e-06
Iter: 767 loss: 2.45958881e-06
Iter: 768 loss: 2.45483807e-06
Iter: 769 loss: 2.45815954e-06
Iter: 770 loss: 2.45180536e-06
Iter: 771 loss: 2.44599642e-06
Iter: 772 loss: 2.45335673e-06
Iter: 773 loss: 2.44304647e-06
Iter: 774 loss: 2.43669388e-06
Iter: 775 loss: 2.45414844e-06
Iter: 776 loss: 2.43473778e-06
Iter: 777 loss: 2.42991837e-06
Iter: 778 loss: 2.49700952e-06
Iter: 779 loss: 2.42985925e-06
Iter: 780 loss: 2.42590204e-06
Iter: 781 loss: 2.4197866e-06
Iter: 782 loss: 2.41971748e-06
Iter: 783 loss: 2.41338284e-06
Iter: 784 loss: 2.43145337e-06
Iter: 785 loss: 2.41139105e-06
Iter: 786 loss: 2.40400323e-06
Iter: 787 loss: 2.453864e-06
Iter: 788 loss: 2.40330746e-06
Iter: 789 loss: 2.39809197e-06
Iter: 790 loss: 2.39885776e-06
Iter: 791 loss: 2.3942971e-06
Iter: 792 loss: 2.38938719e-06
Iter: 793 loss: 2.45170622e-06
Iter: 794 loss: 2.38937e-06
Iter: 795 loss: 2.38454959e-06
Iter: 796 loss: 2.39079168e-06
Iter: 797 loss: 2.38221901e-06
Iter: 798 loss: 2.37828726e-06
Iter: 799 loss: 2.40314898e-06
Iter: 800 loss: 2.37771292e-06
Iter: 801 loss: 2.37414952e-06
Iter: 802 loss: 2.37700897e-06
Iter: 803 loss: 2.37210133e-06
Iter: 804 loss: 2.36723531e-06
Iter: 805 loss: 2.36491201e-06
Iter: 806 loss: 2.36264987e-06
Iter: 807 loss: 2.3580958e-06
Iter: 808 loss: 2.35804441e-06
Iter: 809 loss: 2.35450079e-06
Iter: 810 loss: 2.34890786e-06
Iter: 811 loss: 2.34884737e-06
Iter: 812 loss: 2.34257254e-06
Iter: 813 loss: 2.39578867e-06
Iter: 814 loss: 2.34224672e-06
Iter: 815 loss: 2.33828678e-06
Iter: 816 loss: 2.35378798e-06
Iter: 817 loss: 2.33732317e-06
Iter: 818 loss: 2.33276796e-06
Iter: 819 loss: 2.34035747e-06
Iter: 820 loss: 2.33064907e-06
Iter: 821 loss: 2.3259106e-06
Iter: 822 loss: 2.32918592e-06
Iter: 823 loss: 2.32284583e-06
Iter: 824 loss: 2.31733566e-06
Iter: 825 loss: 2.32658431e-06
Iter: 826 loss: 2.31480112e-06
Iter: 827 loss: 2.30876685e-06
Iter: 828 loss: 2.34964568e-06
Iter: 829 loss: 2.30822161e-06
Iter: 830 loss: 2.30348678e-06
Iter: 831 loss: 2.32634147e-06
Iter: 832 loss: 2.30259593e-06
Iter: 833 loss: 2.29906482e-06
Iter: 834 loss: 2.32121374e-06
Iter: 835 loss: 2.29875286e-06
Iter: 836 loss: 2.29520219e-06
Iter: 837 loss: 2.29293482e-06
Iter: 838 loss: 2.29154648e-06
Iter: 839 loss: 2.2872498e-06
Iter: 840 loss: 2.31788408e-06
Iter: 841 loss: 2.2868187e-06
Iter: 842 loss: 2.28276622e-06
Iter: 843 loss: 2.28497584e-06
Iter: 844 loss: 2.28005729e-06
Iter: 845 loss: 2.27486271e-06
Iter: 846 loss: 2.28043245e-06
Iter: 847 loss: 2.27214218e-06
Iter: 848 loss: 2.26674138e-06
Iter: 849 loss: 2.28294766e-06
Iter: 850 loss: 2.26507063e-06
Iter: 851 loss: 2.25945496e-06
Iter: 852 loss: 2.29320358e-06
Iter: 853 loss: 2.25874783e-06
Iter: 854 loss: 2.25459553e-06
Iter: 855 loss: 2.25628901e-06
Iter: 856 loss: 2.25183885e-06
Iter: 857 loss: 2.24659584e-06
Iter: 858 loss: 2.26274824e-06
Iter: 859 loss: 2.24501309e-06
Iter: 860 loss: 2.24017913e-06
Iter: 861 loss: 2.27976443e-06
Iter: 862 loss: 2.23981988e-06
Iter: 863 loss: 2.23617826e-06
Iter: 864 loss: 2.2316824e-06
Iter: 865 loss: 2.23126494e-06
Iter: 866 loss: 2.22530252e-06
Iter: 867 loss: 2.23526308e-06
Iter: 868 loss: 2.2225895e-06
Iter: 869 loss: 2.21668552e-06
Iter: 870 loss: 2.28103636e-06
Iter: 871 loss: 2.21655273e-06
Iter: 872 loss: 2.21187975e-06
Iter: 873 loss: 2.24566656e-06
Iter: 874 loss: 2.21147025e-06
Iter: 875 loss: 2.20838979e-06
Iter: 876 loss: 2.21312598e-06
Iter: 877 loss: 2.20698143e-06
Iter: 878 loss: 2.20310926e-06
Iter: 879 loss: 2.20501897e-06
Iter: 880 loss: 2.20060474e-06
Iter: 881 loss: 2.19599292e-06
Iter: 882 loss: 2.2129293e-06
Iter: 883 loss: 2.19482354e-06
Iter: 884 loss: 2.1901833e-06
Iter: 885 loss: 2.20377501e-06
Iter: 886 loss: 2.18875948e-06
Iter: 887 loss: 2.18449077e-06
Iter: 888 loss: 2.18172454e-06
Iter: 889 loss: 2.18004425e-06
Iter: 890 loss: 2.17462e-06
Iter: 891 loss: 2.22247e-06
Iter: 892 loss: 2.1743449e-06
Iter: 893 loss: 2.1702283e-06
Iter: 894 loss: 2.18774289e-06
Iter: 895 loss: 2.16935291e-06
Iter: 896 loss: 2.16521084e-06
Iter: 897 loss: 2.16362537e-06
Iter: 898 loss: 2.16136846e-06
Iter: 899 loss: 2.15668479e-06
Iter: 900 loss: 2.20300444e-06
Iter: 901 loss: 2.1565113e-06
Iter: 902 loss: 2.15276327e-06
Iter: 903 loss: 2.1582905e-06
Iter: 904 loss: 2.15098612e-06
Iter: 905 loss: 2.14570832e-06
Iter: 906 loss: 2.15016962e-06
Iter: 907 loss: 2.14267743e-06
Iter: 908 loss: 2.13745216e-06
Iter: 909 loss: 2.13925659e-06
Iter: 910 loss: 2.13382373e-06
Iter: 911 loss: 2.13130261e-06
Iter: 912 loss: 2.13077101e-06
Iter: 913 loss: 2.12746318e-06
Iter: 914 loss: 2.12658733e-06
Iter: 915 loss: 2.12469354e-06
Iter: 916 loss: 2.12092277e-06
Iter: 917 loss: 2.13146222e-06
Iter: 918 loss: 2.11970746e-06
Iter: 919 loss: 2.11631732e-06
Iter: 920 loss: 2.13073645e-06
Iter: 921 loss: 2.11555516e-06
Iter: 922 loss: 2.1118683e-06
Iter: 923 loss: 2.11320503e-06
Iter: 924 loss: 2.10939379e-06
Iter: 925 loss: 2.10484359e-06
Iter: 926 loss: 2.12150735e-06
Iter: 927 loss: 2.10377061e-06
Iter: 928 loss: 2.09961513e-06
Iter: 929 loss: 2.10975418e-06
Iter: 930 loss: 2.09809014e-06
Iter: 931 loss: 2.09377913e-06
Iter: 932 loss: 2.09348127e-06
Iter: 933 loss: 2.0902753e-06
Iter: 934 loss: 2.08534107e-06
Iter: 935 loss: 2.13356225e-06
Iter: 936 loss: 2.08517054e-06
Iter: 937 loss: 2.08126517e-06
Iter: 938 loss: 2.09514383e-06
Iter: 939 loss: 2.08028473e-06
Iter: 940 loss: 2.07681524e-06
Iter: 941 loss: 2.07663311e-06
Iter: 942 loss: 2.07392736e-06
Iter: 943 loss: 2.06969867e-06
Iter: 944 loss: 2.1050721e-06
Iter: 945 loss: 2.06944105e-06
Iter: 946 loss: 2.06548839e-06
Iter: 947 loss: 2.06872778e-06
Iter: 948 loss: 2.06312416e-06
Iter: 949 loss: 2.05914444e-06
Iter: 950 loss: 2.06379491e-06
Iter: 951 loss: 2.0570028e-06
Iter: 952 loss: 2.0526013e-06
Iter: 953 loss: 2.08354845e-06
Iter: 954 loss: 2.05222045e-06
Iter: 955 loss: 2.04722824e-06
Iter: 956 loss: 2.05856736e-06
Iter: 957 loss: 2.04532034e-06
Iter: 958 loss: 2.04228695e-06
Iter: 959 loss: 2.03824038e-06
Iter: 960 loss: 2.03792206e-06
Iter: 961 loss: 2.03394961e-06
Iter: 962 loss: 2.03397235e-06
Iter: 963 loss: 2.03041554e-06
Iter: 964 loss: 2.03146647e-06
Iter: 965 loss: 2.02787032e-06
Iter: 966 loss: 2.02399951e-06
Iter: 967 loss: 2.03516356e-06
Iter: 968 loss: 2.02278261e-06
Iter: 969 loss: 2.01897683e-06
Iter: 970 loss: 2.02884848e-06
Iter: 971 loss: 2.01781859e-06
Iter: 972 loss: 2.0138973e-06
Iter: 973 loss: 2.02429e-06
Iter: 974 loss: 2.01263651e-06
Iter: 975 loss: 2.00834029e-06
Iter: 976 loss: 2.01242074e-06
Iter: 977 loss: 2.0058983e-06
Iter: 978 loss: 2.00176692e-06
Iter: 979 loss: 2.02605747e-06
Iter: 980 loss: 2.00120644e-06
Iter: 981 loss: 1.99672604e-06
Iter: 982 loss: 2.00260138e-06
Iter: 983 loss: 1.99436386e-06
Iter: 984 loss: 1.9905408e-06
Iter: 985 loss: 2.00552267e-06
Iter: 986 loss: 1.98967382e-06
Iter: 987 loss: 1.98613611e-06
Iter: 988 loss: 1.99732904e-06
Iter: 989 loss: 1.98512635e-06
Iter: 990 loss: 1.98127373e-06
Iter: 991 loss: 1.98954e-06
Iter: 992 loss: 1.97972213e-06
Iter: 993 loss: 1.97688792e-06
Iter: 994 loss: 1.98337852e-06
Iter: 995 loss: 1.97575037e-06
Iter: 996 loss: 1.97198733e-06
Iter: 997 loss: 1.99612532e-06
Iter: 998 loss: 1.97159102e-06
Iter: 999 loss: 1.96905557e-06
Iter: 1000 loss: 1.96495421e-06
Iter: 1001 loss: 1.96493147e-06
Iter: 1002 loss: 1.96071278e-06
Iter: 1003 loss: 1.98099974e-06
Iter: 1004 loss: 1.95996608e-06
Iter: 1005 loss: 1.95625125e-06
Iter: 1006 loss: 1.9915758e-06
Iter: 1007 loss: 1.95607527e-06
Iter: 1008 loss: 1.95321354e-06
Iter: 1009 loss: 1.95015173e-06
Iter: 1010 loss: 1.94958284e-06
Iter: 1011 loss: 1.9456927e-06
Iter: 1012 loss: 1.96921656e-06
Iter: 1013 loss: 1.94524478e-06
Iter: 1014 loss: 1.94161748e-06
Iter: 1015 loss: 1.94901622e-06
Iter: 1016 loss: 1.94018548e-06
Iter: 1017 loss: 1.93611822e-06
Iter: 1018 loss: 1.94811059e-06
Iter: 1019 loss: 1.93481151e-06
Iter: 1020 loss: 1.93130018e-06
Iter: 1021 loss: 1.94008931e-06
Iter: 1022 loss: 1.93002e-06
Iter: 1023 loss: 1.92664629e-06
Iter: 1024 loss: 1.94712356e-06
Iter: 1025 loss: 1.926192e-06
Iter: 1026 loss: 1.92300263e-06
Iter: 1027 loss: 1.92342895e-06
Iter: 1028 loss: 1.92052039e-06
Iter: 1029 loss: 1.91682534e-06
Iter: 1030 loss: 1.9338122e-06
Iter: 1031 loss: 1.91612298e-06
Iter: 1032 loss: 1.91252843e-06
Iter: 1033 loss: 1.92573361e-06
Iter: 1034 loss: 1.91164781e-06
Iter: 1035 loss: 1.90826427e-06
Iter: 1036 loss: 1.91600748e-06
Iter: 1037 loss: 1.90701837e-06
Iter: 1038 loss: 1.90319565e-06
Iter: 1039 loss: 1.92424341e-06
Iter: 1040 loss: 1.90260789e-06
Iter: 1041 loss: 1.89995774e-06
Iter: 1042 loss: 1.89693458e-06
Iter: 1043 loss: 1.89662887e-06
Iter: 1044 loss: 1.89281889e-06
Iter: 1045 loss: 1.90959554e-06
Iter: 1046 loss: 1.89211096e-06
Iter: 1047 loss: 1.88834724e-06
Iter: 1048 loss: 1.90072228e-06
Iter: 1049 loss: 1.88743445e-06
Iter: 1050 loss: 1.88377464e-06
Iter: 1051 loss: 1.90910805e-06
Iter: 1052 loss: 1.88347974e-06
Iter: 1053 loss: 1.88106458e-06
Iter: 1054 loss: 1.87810201e-06
Iter: 1055 loss: 1.87785827e-06
Iter: 1056 loss: 1.87313981e-06
Iter: 1057 loss: 1.88881461e-06
Iter: 1058 loss: 1.8718365e-06
Iter: 1059 loss: 1.86813554e-06
Iter: 1060 loss: 1.8917026e-06
Iter: 1061 loss: 1.86770251e-06
Iter: 1062 loss: 1.86370335e-06
Iter: 1063 loss: 1.86833188e-06
Iter: 1064 loss: 1.86155364e-06
Iter: 1065 loss: 1.85833142e-06
Iter: 1066 loss: 1.87433636e-06
Iter: 1067 loss: 1.85777435e-06
Iter: 1068 loss: 1.85462852e-06
Iter: 1069 loss: 1.862511e-06
Iter: 1070 loss: 1.85347869e-06
Iter: 1071 loss: 1.85043586e-06
Iter: 1072 loss: 1.85680517e-06
Iter: 1073 loss: 1.84917326e-06
Iter: 1074 loss: 1.84654368e-06
Iter: 1075 loss: 1.86958675e-06
Iter: 1076 loss: 1.84635087e-06
Iter: 1077 loss: 1.84358282e-06
Iter: 1078 loss: 1.84306248e-06
Iter: 1079 loss: 1.84122086e-06
Iter: 1080 loss: 1.83812028e-06
Iter: 1081 loss: 1.85539045e-06
Iter: 1082 loss: 1.83757277e-06
Iter: 1083 loss: 1.83460111e-06
Iter: 1084 loss: 1.83187899e-06
Iter: 1085 loss: 1.83113639e-06
Iter: 1086 loss: 1.82687188e-06
Iter: 1087 loss: 1.83116492e-06
Iter: 1088 loss: 1.8244092e-06
Iter: 1089 loss: 1.82164047e-06
Iter: 1090 loss: 1.82146209e-06
Iter: 1091 loss: 1.81880091e-06
Iter: 1092 loss: 1.81870223e-06
Iter: 1093 loss: 1.8166736e-06
Iter: 1094 loss: 1.81318273e-06
Iter: 1095 loss: 1.82123699e-06
Iter: 1096 loss: 1.81196424e-06
Iter: 1097 loss: 1.80870825e-06
Iter: 1098 loss: 1.80711936e-06
Iter: 1099 loss: 1.80553934e-06
Iter: 1100 loss: 1.80160725e-06
Iter: 1101 loss: 1.85515546e-06
Iter: 1102 loss: 1.80159259e-06
Iter: 1103 loss: 1.7985061e-06
Iter: 1104 loss: 1.81076825e-06
Iter: 1105 loss: 1.79784763e-06
Iter: 1106 loss: 1.79506992e-06
Iter: 1107 loss: 1.79588312e-06
Iter: 1108 loss: 1.79303731e-06
Iter: 1109 loss: 1.78983566e-06
Iter: 1110 loss: 1.81507539e-06
Iter: 1111 loss: 1.78962068e-06
Iter: 1112 loss: 1.78690095e-06
Iter: 1113 loss: 1.79572498e-06
Iter: 1114 loss: 1.78621667e-06
Iter: 1115 loss: 1.78370101e-06
Iter: 1116 loss: 1.79098038e-06
Iter: 1117 loss: 1.7828828e-06
Iter: 1118 loss: 1.78012544e-06
Iter: 1119 loss: 1.77958395e-06
Iter: 1120 loss: 1.77778827e-06
Iter: 1121 loss: 1.77499373e-06
Iter: 1122 loss: 1.78396044e-06
Iter: 1123 loss: 1.77424101e-06
Iter: 1124 loss: 1.77078834e-06
Iter: 1125 loss: 1.7759113e-06
Iter: 1126 loss: 1.76900858e-06
Iter: 1127 loss: 1.7654063e-06
Iter: 1128 loss: 1.77166021e-06
Iter: 1129 loss: 1.76390154e-06
Iter: 1130 loss: 1.76046069e-06
Iter: 1131 loss: 1.77975528e-06
Iter: 1132 loss: 1.75997752e-06
Iter: 1133 loss: 1.75657817e-06
Iter: 1134 loss: 1.76774915e-06
Iter: 1135 loss: 1.75569903e-06
Iter: 1136 loss: 1.75242258e-06
Iter: 1137 loss: 1.75493381e-06
Iter: 1138 loss: 1.7504176e-06
Iter: 1139 loss: 1.74709771e-06
Iter: 1140 loss: 1.75080891e-06
Iter: 1141 loss: 1.74534216e-06
Iter: 1142 loss: 1.74098318e-06
Iter: 1143 loss: 1.74460581e-06
Iter: 1144 loss: 1.73839817e-06
Iter: 1145 loss: 1.73553735e-06
Iter: 1146 loss: 1.73541139e-06
Iter: 1147 loss: 1.73264266e-06
Iter: 1148 loss: 1.73213925e-06
Iter: 1149 loss: 1.73025762e-06
Iter: 1150 loss: 1.72761258e-06
Iter: 1151 loss: 1.75985883e-06
Iter: 1152 loss: 1.72753744e-06
Iter: 1153 loss: 1.72526893e-06
Iter: 1154 loss: 1.7279441e-06
Iter: 1155 loss: 1.72405475e-06
Iter: 1156 loss: 1.7214478e-06
Iter: 1157 loss: 1.72154137e-06
Iter: 1158 loss: 1.71944362e-06
Iter: 1159 loss: 1.71586123e-06
Iter: 1160 loss: 1.73562591e-06
Iter: 1161 loss: 1.71537499e-06
Iter: 1162 loss: 1.7124546e-06
Iter: 1163 loss: 1.71136082e-06
Iter: 1164 loss: 1.70970202e-06
Iter: 1165 loss: 1.70647525e-06
Iter: 1166 loss: 1.73312947e-06
Iter: 1167 loss: 1.70622059e-06
Iter: 1168 loss: 1.70346595e-06
Iter: 1169 loss: 1.71124134e-06
Iter: 1170 loss: 1.70264559e-06
Iter: 1171 loss: 1.69980956e-06
Iter: 1172 loss: 1.70410249e-06
Iter: 1173 loss: 1.6983704e-06
Iter: 1174 loss: 1.6957174e-06
Iter: 1175 loss: 1.7184e-06
Iter: 1176 loss: 1.69553357e-06
Iter: 1177 loss: 1.6931765e-06
Iter: 1178 loss: 1.6906597e-06
Iter: 1179 loss: 1.69026953e-06
Iter: 1180 loss: 1.68671625e-06
Iter: 1181 loss: 1.70293231e-06
Iter: 1182 loss: 1.68598899e-06
Iter: 1183 loss: 1.6831433e-06
Iter: 1184 loss: 1.68643169e-06
Iter: 1185 loss: 1.68158704e-06
Iter: 1186 loss: 1.67870814e-06
Iter: 1187 loss: 1.7094859e-06
Iter: 1188 loss: 1.67862072e-06
Iter: 1189 loss: 1.67578764e-06
Iter: 1190 loss: 1.68272902e-06
Iter: 1191 loss: 1.67482597e-06
Iter: 1192 loss: 1.6727306e-06
Iter: 1193 loss: 1.68402471e-06
Iter: 1194 loss: 1.67242956e-06
Iter: 1195 loss: 1.67021233e-06
Iter: 1196 loss: 1.66825726e-06
Iter: 1197 loss: 1.66770883e-06
Iter: 1198 loss: 1.66418022e-06
Iter: 1199 loss: 1.66688483e-06
Iter: 1200 loss: 1.66200516e-06
Iter: 1201 loss: 1.65816891e-06
Iter: 1202 loss: 1.6931632e-06
Iter: 1203 loss: 1.65803385e-06
Iter: 1204 loss: 1.65504377e-06
Iter: 1205 loss: 1.66262112e-06
Iter: 1206 loss: 1.65404595e-06
Iter: 1207 loss: 1.65147526e-06
Iter: 1208 loss: 1.64845335e-06
Iter: 1209 loss: 1.64816265e-06
Iter: 1210 loss: 1.64572236e-06
Iter: 1211 loss: 1.64522635e-06
Iter: 1212 loss: 1.64353298e-06
Iter: 1213 loss: 1.6415396e-06
Iter: 1214 loss: 1.64127e-06
Iter: 1215 loss: 1.63795551e-06
Iter: 1216 loss: 1.66021846e-06
Iter: 1217 loss: 1.63760546e-06
Iter: 1218 loss: 1.63523123e-06
Iter: 1219 loss: 1.63501682e-06
Iter: 1220 loss: 1.6332674e-06
Iter: 1221 loss: 1.62981758e-06
Iter: 1222 loss: 1.63638083e-06
Iter: 1223 loss: 1.62837318e-06
Iter: 1224 loss: 1.62540493e-06
Iter: 1225 loss: 1.65635686e-06
Iter: 1226 loss: 1.62533286e-06
Iter: 1227 loss: 1.62240372e-06
Iter: 1228 loss: 1.62913011e-06
Iter: 1229 loss: 1.6213487e-06
Iter: 1230 loss: 1.61918365e-06
Iter: 1231 loss: 1.62447463e-06
Iter: 1232 loss: 1.61849425e-06
Iter: 1233 loss: 1.61583853e-06
Iter: 1234 loss: 1.61764956e-06
Iter: 1235 loss: 1.61416278e-06
Iter: 1236 loss: 1.6114e-06
Iter: 1237 loss: 1.61612638e-06
Iter: 1238 loss: 1.61018443e-06
Iter: 1239 loss: 1.60712318e-06
Iter: 1240 loss: 1.61102889e-06
Iter: 1241 loss: 1.60554373e-06
Iter: 1242 loss: 1.60258662e-06
Iter: 1243 loss: 1.62967331e-06
Iter: 1244 loss: 1.60251602e-06
Iter: 1245 loss: 1.59976719e-06
Iter: 1246 loss: 1.60105174e-06
Iter: 1247 loss: 1.59792307e-06
Iter: 1248 loss: 1.59478964e-06
Iter: 1249 loss: 1.59842989e-06
Iter: 1250 loss: 1.59318824e-06
Iter: 1251 loss: 1.59000592e-06
Iter: 1252 loss: 1.63154857e-06
Iter: 1253 loss: 1.58999876e-06
Iter: 1254 loss: 1.58786668e-06
Iter: 1255 loss: 1.58869693e-06
Iter: 1256 loss: 1.58645253e-06
Iter: 1257 loss: 1.58369915e-06
Iter: 1258 loss: 1.58914827e-06
Iter: 1259 loss: 1.58262787e-06
Iter: 1260 loss: 1.57908028e-06
Iter: 1261 loss: 1.58620469e-06
Iter: 1262 loss: 1.57771535e-06
Iter: 1263 loss: 1.57505337e-06
Iter: 1264 loss: 1.57457316e-06
Iter: 1265 loss: 1.57273371e-06
Iter: 1266 loss: 1.56950955e-06
Iter: 1267 loss: 1.61016555e-06
Iter: 1268 loss: 1.56945589e-06
Iter: 1269 loss: 1.56680665e-06
Iter: 1270 loss: 1.58104808e-06
Iter: 1271 loss: 1.56647229e-06
Iter: 1272 loss: 1.56416013e-06
Iter: 1273 loss: 1.56476267e-06
Iter: 1274 loss: 1.56254316e-06
Iter: 1275 loss: 1.55973896e-06
Iter: 1276 loss: 1.57714749e-06
Iter: 1277 loss: 1.55940063e-06
Iter: 1278 loss: 1.55739053e-06
Iter: 1279 loss: 1.55514681e-06
Iter: 1280 loss: 1.55489022e-06
Iter: 1281 loss: 1.55144107e-06
Iter: 1282 loss: 1.56431463e-06
Iter: 1283 loss: 1.55063697e-06
Iter: 1284 loss: 1.54753275e-06
Iter: 1285 loss: 1.56886063e-06
Iter: 1286 loss: 1.54721909e-06
Iter: 1287 loss: 1.54484269e-06
Iter: 1288 loss: 1.54909935e-06
Iter: 1289 loss: 1.5437804e-06
Iter: 1290 loss: 1.54093186e-06
Iter: 1291 loss: 1.54193071e-06
Iter: 1292 loss: 1.53897872e-06
Iter: 1293 loss: 1.53620601e-06
Iter: 1294 loss: 1.57956322e-06
Iter: 1295 loss: 1.53619442e-06
Iter: 1296 loss: 1.53411304e-06
Iter: 1297 loss: 1.53254655e-06
Iter: 1298 loss: 1.53189296e-06
Iter: 1299 loss: 1.52869848e-06
Iter: 1300 loss: 1.53858991e-06
Iter: 1301 loss: 1.52778307e-06
Iter: 1302 loss: 1.52471489e-06
Iter: 1303 loss: 1.5404338e-06
Iter: 1304 loss: 1.52423149e-06
Iter: 1305 loss: 1.52183384e-06
Iter: 1306 loss: 1.52195673e-06
Iter: 1307 loss: 1.51990344e-06
Iter: 1308 loss: 1.51711743e-06
Iter: 1309 loss: 1.5405351e-06
Iter: 1310 loss: 1.51696395e-06
Iter: 1311 loss: 1.51445261e-06
Iter: 1312 loss: 1.52676512e-06
Iter: 1313 loss: 1.51397171e-06
Iter: 1314 loss: 1.51194752e-06
Iter: 1315 loss: 1.51268705e-06
Iter: 1316 loss: 1.51053098e-06
Iter: 1317 loss: 1.50817652e-06
Iter: 1318 loss: 1.51824725e-06
Iter: 1319 loss: 1.50763321e-06
Iter: 1320 loss: 1.50504957e-06
Iter: 1321 loss: 1.50583253e-06
Iter: 1322 loss: 1.50316669e-06
Iter: 1323 loss: 1.50064534e-06
Iter: 1324 loss: 1.50411529e-06
Iter: 1325 loss: 1.49931816e-06
Iter: 1326 loss: 1.49617722e-06
Iter: 1327 loss: 1.50753272e-06
Iter: 1328 loss: 1.49545576e-06
Iter: 1329 loss: 1.49247671e-06
Iter: 1330 loss: 1.51414224e-06
Iter: 1331 loss: 1.49224081e-06
Iter: 1332 loss: 1.49019024e-06
Iter: 1333 loss: 1.49043785e-06
Iter: 1334 loss: 1.48865183e-06
Iter: 1335 loss: 1.48648587e-06
Iter: 1336 loss: 1.51007498e-06
Iter: 1337 loss: 1.48641789e-06
Iter: 1338 loss: 1.48434788e-06
Iter: 1339 loss: 1.48150548e-06
Iter: 1340 loss: 1.48134382e-06
Iter: 1341 loss: 1.4786358e-06
Iter: 1342 loss: 1.49438051e-06
Iter: 1343 loss: 1.47823789e-06
Iter: 1344 loss: 1.47568505e-06
Iter: 1345 loss: 1.48492438e-06
Iter: 1346 loss: 1.47506557e-06
Iter: 1347 loss: 1.47253695e-06
Iter: 1348 loss: 1.47569199e-06
Iter: 1349 loss: 1.47126036e-06
Iter: 1350 loss: 1.46892557e-06
Iter: 1351 loss: 1.49640732e-06
Iter: 1352 loss: 1.46885714e-06
Iter: 1353 loss: 1.46678838e-06
Iter: 1354 loss: 1.46863727e-06
Iter: 1355 loss: 1.46556033e-06
Iter: 1356 loss: 1.46335469e-06
Iter: 1357 loss: 1.46261232e-06
Iter: 1358 loss: 1.46135369e-06
Iter: 1359 loss: 1.45854426e-06
Iter: 1360 loss: 1.49433731e-06
Iter: 1361 loss: 1.45854631e-06
Iter: 1362 loss: 1.45677222e-06
Iter: 1363 loss: 1.45485319e-06
Iter: 1364 loss: 1.4545617e-06
Iter: 1365 loss: 1.4513148e-06
Iter: 1366 loss: 1.46143498e-06
Iter: 1367 loss: 1.45045101e-06
Iter: 1368 loss: 1.44795047e-06
Iter: 1369 loss: 1.46057255e-06
Iter: 1370 loss: 1.4475545e-06
Iter: 1371 loss: 1.44501018e-06
Iter: 1372 loss: 1.45383069e-06
Iter: 1373 loss: 1.44439286e-06
Iter: 1374 loss: 1.44214494e-06
Iter: 1375 loss: 1.44377179e-06
Iter: 1376 loss: 1.44072101e-06
Iter: 1377 loss: 1.43835769e-06
Iter: 1378 loss: 1.46048615e-06
Iter: 1379 loss: 1.43830243e-06
Iter: 1380 loss: 1.43627176e-06
Iter: 1381 loss: 1.43515194e-06
Iter: 1382 loss: 1.43426314e-06
Iter: 1383 loss: 1.43165926e-06
Iter: 1384 loss: 1.43684917e-06
Iter: 1385 loss: 1.43059287e-06
Iter: 1386 loss: 1.42816725e-06
Iter: 1387 loss: 1.4490106e-06
Iter: 1388 loss: 1.42795773e-06
Iter: 1389 loss: 1.42594479e-06
Iter: 1390 loss: 1.43535317e-06
Iter: 1391 loss: 1.42557155e-06
Iter: 1392 loss: 1.42370118e-06
Iter: 1393 loss: 1.42922727e-06
Iter: 1394 loss: 1.42323643e-06
Iter: 1395 loss: 1.42124759e-06
Iter: 1396 loss: 1.41882492e-06
Iter: 1397 loss: 1.41864609e-06
Iter: 1398 loss: 1.41600742e-06
Iter: 1399 loss: 1.44735372e-06
Iter: 1400 loss: 1.41598616e-06
Iter: 1401 loss: 1.41429541e-06
Iter: 1402 loss: 1.41759381e-06
Iter: 1403 loss: 1.41359351e-06
Iter: 1404 loss: 1.41116243e-06
Iter: 1405 loss: 1.41130022e-06
Iter: 1406 loss: 1.40925954e-06
Iter: 1407 loss: 1.40689394e-06
Iter: 1408 loss: 1.40996565e-06
Iter: 1409 loss: 1.40568477e-06
Iter: 1410 loss: 1.40275608e-06
Iter: 1411 loss: 1.42011856e-06
Iter: 1412 loss: 1.40233e-06
Iter: 1413 loss: 1.3999985e-06
Iter: 1414 loss: 1.41421094e-06
Iter: 1415 loss: 1.39970643e-06
Iter: 1416 loss: 1.39748272e-06
Iter: 1417 loss: 1.39620909e-06
Iter: 1418 loss: 1.39534836e-06
Iter: 1419 loss: 1.39331269e-06
Iter: 1420 loss: 1.3933e-06
Iter: 1421 loss: 1.39159124e-06
Iter: 1422 loss: 1.38856763e-06
Iter: 1423 loss: 1.38856979e-06
Iter: 1424 loss: 1.38591872e-06
Iter: 1425 loss: 1.41217834e-06
Iter: 1426 loss: 1.38578889e-06
Iter: 1427 loss: 1.38380346e-06
Iter: 1428 loss: 1.40432394e-06
Iter: 1429 loss: 1.38369455e-06
Iter: 1430 loss: 1.38229655e-06
Iter: 1431 loss: 1.38059738e-06
Iter: 1432 loss: 1.38045289e-06
Iter: 1433 loss: 1.37788936e-06
Iter: 1434 loss: 1.38999098e-06
Iter: 1435 loss: 1.37741313e-06
Iter: 1436 loss: 1.37531026e-06
Iter: 1437 loss: 1.37992765e-06
Iter: 1438 loss: 1.37461598e-06
Iter: 1439 loss: 1.37234383e-06
Iter: 1440 loss: 1.3756337e-06
Iter: 1441 loss: 1.37122947e-06
Iter: 1442 loss: 1.36844631e-06
Iter: 1443 loss: 1.38111909e-06
Iter: 1444 loss: 1.36792505e-06
Iter: 1445 loss: 1.36569417e-06
Iter: 1446 loss: 1.36716756e-06
Iter: 1447 loss: 1.36431152e-06
Iter: 1448 loss: 1.36169626e-06
Iter: 1449 loss: 1.36611573e-06
Iter: 1450 loss: 1.36046447e-06
Iter: 1451 loss: 1.35835285e-06
Iter: 1452 loss: 1.38906694e-06
Iter: 1453 loss: 1.35832875e-06
Iter: 1454 loss: 1.35649657e-06
Iter: 1455 loss: 1.35630148e-06
Iter: 1456 loss: 1.35493042e-06
Iter: 1457 loss: 1.35287678e-06
Iter: 1458 loss: 1.35929827e-06
Iter: 1459 loss: 1.35238167e-06
Iter: 1460 loss: 1.34994593e-06
Iter: 1461 loss: 1.35791856e-06
Iter: 1462 loss: 1.34934135e-06
Iter: 1463 loss: 1.34744096e-06
Iter: 1464 loss: 1.34973982e-06
Iter: 1465 loss: 1.34638208e-06
Iter: 1466 loss: 1.34409765e-06
Iter: 1467 loss: 1.36659617e-06
Iter: 1468 loss: 1.34402012e-06
Iter: 1469 loss: 1.34244465e-06
Iter: 1470 loss: 1.3407132e-06
Iter: 1471 loss: 1.34043592e-06
Iter: 1472 loss: 1.33801495e-06
Iter: 1473 loss: 1.34227048e-06
Iter: 1474 loss: 1.33693766e-06
Iter: 1475 loss: 1.33453545e-06
Iter: 1476 loss: 1.35546054e-06
Iter: 1477 loss: 1.33443416e-06
Iter: 1478 loss: 1.33248204e-06
Iter: 1479 loss: 1.33499816e-06
Iter: 1480 loss: 1.33145807e-06
Iter: 1481 loss: 1.32952266e-06
Iter: 1482 loss: 1.33697506e-06
Iter: 1483 loss: 1.32900982e-06
Iter: 1484 loss: 1.32699438e-06
Iter: 1485 loss: 1.32985974e-06
Iter: 1486 loss: 1.32610546e-06
Iter: 1487 loss: 1.32396121e-06
Iter: 1488 loss: 1.32965647e-06
Iter: 1489 loss: 1.32318382e-06
Iter: 1490 loss: 1.32101491e-06
Iter: 1491 loss: 1.32193793e-06
Iter: 1492 loss: 1.31945114e-06
Iter: 1493 loss: 1.31700301e-06
Iter: 1494 loss: 1.35383129e-06
Iter: 1495 loss: 1.31699767e-06
Iter: 1496 loss: 1.31539036e-06
Iter: 1497 loss: 1.31447712e-06
Iter: 1498 loss: 1.31381933e-06
Iter: 1499 loss: 1.31147181e-06
Iter: 1500 loss: 1.319381e-06
Iter: 1501 loss: 1.31083175e-06
Iter: 1502 loss: 1.30905505e-06
Iter: 1503 loss: 1.33114077e-06
Iter: 1504 loss: 1.30904562e-06
Iter: 1505 loss: 1.30762533e-06
Iter: 1506 loss: 1.30806416e-06
Iter: 1507 loss: 1.3065951e-06
Iter: 1508 loss: 1.30451872e-06
Iter: 1509 loss: 1.31111062e-06
Iter: 1510 loss: 1.30391754e-06
Iter: 1511 loss: 1.30216199e-06
Iter: 1512 loss: 1.30042554e-06
Iter: 1513 loss: 1.30008652e-06
Iter: 1514 loss: 1.29747878e-06
Iter: 1515 loss: 1.30368653e-06
Iter: 1516 loss: 1.29647719e-06
Iter: 1517 loss: 1.29396642e-06
Iter: 1518 loss: 1.32105947e-06
Iter: 1519 loss: 1.29395073e-06
Iter: 1520 loss: 1.29240573e-06
Iter: 1521 loss: 1.30061505e-06
Iter: 1522 loss: 1.29212322e-06
Iter: 1523 loss: 1.29061368e-06
Iter: 1524 loss: 1.28846068e-06
Iter: 1525 loss: 1.28837655e-06
Iter: 1526 loss: 1.28590591e-06
Iter: 1527 loss: 1.30451122e-06
Iter: 1528 loss: 1.28563943e-06
Iter: 1529 loss: 1.28338161e-06
Iter: 1530 loss: 1.28893407e-06
Iter: 1531 loss: 1.28250281e-06
Iter: 1532 loss: 1.28063937e-06
Iter: 1533 loss: 1.2860603e-06
Iter: 1534 loss: 1.28006e-06
Iter: 1535 loss: 1.27819555e-06
Iter: 1536 loss: 1.28945339e-06
Iter: 1537 loss: 1.27792873e-06
Iter: 1538 loss: 1.27623639e-06
Iter: 1539 loss: 1.27513113e-06
Iter: 1540 loss: 1.27452688e-06
Iter: 1541 loss: 1.27289661e-06
Iter: 1542 loss: 1.27294857e-06
Iter: 1543 loss: 1.27131148e-06
Iter: 1544 loss: 1.27148951e-06
Iter: 1545 loss: 1.2701031e-06
Iter: 1546 loss: 1.26841746e-06
Iter: 1547 loss: 1.27373028e-06
Iter: 1548 loss: 1.2679692e-06
Iter: 1549 loss: 1.26583473e-06
Iter: 1550 loss: 1.26604868e-06
Iter: 1551 loss: 1.26426789e-06
Iter: 1552 loss: 1.2617985e-06
Iter: 1553 loss: 1.2671095e-06
Iter: 1554 loss: 1.26078544e-06
Iter: 1555 loss: 1.25837494e-06
Iter: 1556 loss: 1.26574287e-06
Iter: 1557 loss: 1.25761903e-06
Iter: 1558 loss: 1.25556244e-06
Iter: 1559 loss: 1.27451972e-06
Iter: 1560 loss: 1.25541055e-06
Iter: 1561 loss: 1.25371594e-06
Iter: 1562 loss: 1.2559309e-06
Iter: 1563 loss: 1.25280781e-06
Iter: 1564 loss: 1.25114434e-06
Iter: 1565 loss: 1.25248505e-06
Iter: 1566 loss: 1.25012457e-06
Iter: 1567 loss: 1.24787323e-06
Iter: 1568 loss: 1.25497547e-06
Iter: 1569 loss: 1.24718349e-06
Iter: 1570 loss: 1.24529197e-06
Iter: 1571 loss: 1.26124451e-06
Iter: 1572 loss: 1.24518181e-06
Iter: 1573 loss: 1.24352664e-06
Iter: 1574 loss: 1.2431758e-06
Iter: 1575 loss: 1.24215524e-06
Iter: 1576 loss: 1.23996597e-06
Iter: 1577 loss: 1.25694032e-06
Iter: 1578 loss: 1.23979271e-06
Iter: 1579 loss: 1.23826635e-06
Iter: 1580 loss: 1.24056089e-06
Iter: 1581 loss: 1.23754887e-06
Iter: 1582 loss: 1.23585346e-06
Iter: 1583 loss: 1.24909934e-06
Iter: 1584 loss: 1.23571988e-06
Iter: 1585 loss: 1.23447739e-06
Iter: 1586 loss: 1.23244195e-06
Iter: 1587 loss: 1.23243353e-06
Iter: 1588 loss: 1.23031725e-06
Iter: 1589 loss: 1.23785321e-06
Iter: 1590 loss: 1.22977417e-06
Iter: 1591 loss: 1.22739823e-06
Iter: 1592 loss: 1.23903351e-06
Iter: 1593 loss: 1.22699237e-06
Iter: 1594 loss: 1.22519543e-06
Iter: 1595 loss: 1.22537904e-06
Iter: 1596 loss: 1.22374854e-06
Iter: 1597 loss: 1.22132326e-06
Iter: 1598 loss: 1.23137625e-06
Iter: 1599 loss: 1.22076199e-06
Iter: 1600 loss: 1.21896619e-06
Iter: 1601 loss: 1.2299879e-06
Iter: 1602 loss: 1.21873359e-06
Iter: 1603 loss: 1.21689618e-06
Iter: 1604 loss: 1.22005224e-06
Iter: 1605 loss: 1.21611379e-06
Iter: 1606 loss: 1.21452717e-06
Iter: 1607 loss: 1.21379151e-06
Iter: 1608 loss: 1.21299354e-06
Iter: 1609 loss: 1.21054245e-06
Iter: 1610 loss: 1.22722815e-06
Iter: 1611 loss: 1.21035146e-06
Iter: 1612 loss: 1.20847199e-06
Iter: 1613 loss: 1.22237429e-06
Iter: 1614 loss: 1.2083417e-06
Iter: 1615 loss: 1.20689538e-06
Iter: 1616 loss: 1.2057244e-06
Iter: 1617 loss: 1.2053174e-06
Iter: 1618 loss: 1.20373159e-06
Iter: 1619 loss: 1.20371328e-06
Iter: 1620 loss: 1.20241396e-06
Iter: 1621 loss: 1.20194272e-06
Iter: 1622 loss: 1.20125674e-06
Iter: 1623 loss: 1.19950903e-06
Iter: 1624 loss: 1.20664401e-06
Iter: 1625 loss: 1.19911124e-06
Iter: 1626 loss: 1.19754861e-06
Iter: 1627 loss: 1.19747028e-06
Iter: 1628 loss: 1.19628453e-06
Iter: 1629 loss: 1.19434048e-06
Iter: 1630 loss: 1.20160007e-06
Iter: 1631 loss: 1.19391268e-06
Iter: 1632 loss: 1.19214076e-06
Iter: 1633 loss: 1.20028233e-06
Iter: 1634 loss: 1.19188451e-06
Iter: 1635 loss: 1.19003471e-06
Iter: 1636 loss: 1.19247989e-06
Iter: 1637 loss: 1.18913499e-06
Iter: 1638 loss: 1.18712433e-06
Iter: 1639 loss: 1.18731555e-06
Iter: 1640 loss: 1.18548314e-06
Iter: 1641 loss: 1.18316e-06
Iter: 1642 loss: 1.21511334e-06
Iter: 1643 loss: 1.18317564e-06
Iter: 1644 loss: 1.1817242e-06
Iter: 1645 loss: 1.18550247e-06
Iter: 1646 loss: 1.18125763e-06
Iter: 1647 loss: 1.17972024e-06
Iter: 1648 loss: 1.17847821e-06
Iter: 1649 loss: 1.17803438e-06
Iter: 1650 loss: 1.17596096e-06
Iter: 1651 loss: 1.18737398e-06
Iter: 1652 loss: 1.17569641e-06
Iter: 1653 loss: 1.17384889e-06
Iter: 1654 loss: 1.19084859e-06
Iter: 1655 loss: 1.17375032e-06
Iter: 1656 loss: 1.17255865e-06
Iter: 1657 loss: 1.17275636e-06
Iter: 1658 loss: 1.17159925e-06
Iter: 1659 loss: 1.16995216e-06
Iter: 1660 loss: 1.18366461e-06
Iter: 1661 loss: 1.16988963e-06
Iter: 1662 loss: 1.16853153e-06
Iter: 1663 loss: 1.16658737e-06
Iter: 1664 loss: 1.1664838e-06
Iter: 1665 loss: 1.16455465e-06
Iter: 1666 loss: 1.17239983e-06
Iter: 1667 loss: 1.16413435e-06
Iter: 1668 loss: 1.1620773e-06
Iter: 1669 loss: 1.17123921e-06
Iter: 1670 loss: 1.16165779e-06
Iter: 1671 loss: 1.16017736e-06
Iter: 1672 loss: 1.16240949e-06
Iter: 1673 loss: 1.15940873e-06
Iter: 1674 loss: 1.15745718e-06
Iter: 1675 loss: 1.16004799e-06
Iter: 1676 loss: 1.15649118e-06
Iter: 1677 loss: 1.15462058e-06
Iter: 1678 loss: 1.17020363e-06
Iter: 1679 loss: 1.15449325e-06
Iter: 1680 loss: 1.1528864e-06
Iter: 1681 loss: 1.15393937e-06
Iter: 1682 loss: 1.15187208e-06
Iter: 1683 loss: 1.15012369e-06
Iter: 1684 loss: 1.15359603e-06
Iter: 1685 loss: 1.14942077e-06
Iter: 1686 loss: 1.14732279e-06
Iter: 1687 loss: 1.16268802e-06
Iter: 1688 loss: 1.14717079e-06
Iter: 1689 loss: 1.14588772e-06
Iter: 1690 loss: 1.14533918e-06
Iter: 1691 loss: 1.1447097e-06
Iter: 1692 loss: 1.14287354e-06
Iter: 1693 loss: 1.14799741e-06
Iter: 1694 loss: 1.14226941e-06
Iter: 1695 loss: 1.14048771e-06
Iter: 1696 loss: 1.16051024e-06
Iter: 1697 loss: 1.14040552e-06
Iter: 1698 loss: 1.13917531e-06
Iter: 1699 loss: 1.13897988e-06
Iter: 1700 loss: 1.13810506e-06
Iter: 1701 loss: 1.13648139e-06
Iter: 1702 loss: 1.15305e-06
Iter: 1703 loss: 1.1364009e-06
Iter: 1704 loss: 1.13511032e-06
Iter: 1705 loss: 1.13393151e-06
Iter: 1706 loss: 1.13358897e-06
Iter: 1707 loss: 1.13196506e-06
Iter: 1708 loss: 1.13424187e-06
Iter: 1709 loss: 1.13111378e-06
Iter: 1710 loss: 1.12917792e-06
Iter: 1711 loss: 1.14436466e-06
Iter: 1712 loss: 1.12904513e-06
Iter: 1713 loss: 1.12764815e-06
Iter: 1714 loss: 1.12924886e-06
Iter: 1715 loss: 1.12687815e-06
Iter: 1716 loss: 1.12520024e-06
Iter: 1717 loss: 1.12702151e-06
Iter: 1718 loss: 1.12429893e-06
Iter: 1719 loss: 1.1219754e-06
Iter: 1720 loss: 1.1342056e-06
Iter: 1721 loss: 1.12167845e-06
Iter: 1722 loss: 1.1200932e-06
Iter: 1723 loss: 1.12444843e-06
Iter: 1724 loss: 1.11961901e-06
Iter: 1725 loss: 1.11814529e-06
Iter: 1726 loss: 1.12009366e-06
Iter: 1727 loss: 1.11729037e-06
Iter: 1728 loss: 1.11527663e-06
Iter: 1729 loss: 1.12318628e-06
Iter: 1730 loss: 1.11481438e-06
Iter: 1731 loss: 1.11318798e-06
Iter: 1732 loss: 1.11545296e-06
Iter: 1733 loss: 1.11234374e-06
Iter: 1734 loss: 1.11069323e-06
Iter: 1735 loss: 1.11396139e-06
Iter: 1736 loss: 1.11000554e-06
Iter: 1737 loss: 1.10814676e-06
Iter: 1738 loss: 1.12770499e-06
Iter: 1739 loss: 1.10815813e-06
Iter: 1740 loss: 1.10703172e-06
Iter: 1741 loss: 1.10807832e-06
Iter: 1742 loss: 1.10636597e-06
Iter: 1743 loss: 1.10517863e-06
Iter: 1744 loss: 1.11435838e-06
Iter: 1745 loss: 1.10509393e-06
Iter: 1746 loss: 1.10403312e-06
Iter: 1747 loss: 1.1019456e-06
Iter: 1748 loss: 1.13804128e-06
Iter: 1749 loss: 1.10192082e-06
Iter: 1750 loss: 1.09974167e-06
Iter: 1751 loss: 1.11031989e-06
Iter: 1752 loss: 1.09934012e-06
Iter: 1753 loss: 1.0973074e-06
Iter: 1754 loss: 1.10435872e-06
Iter: 1755 loss: 1.09676228e-06
Iter: 1756 loss: 1.09495738e-06
Iter: 1757 loss: 1.10295196e-06
Iter: 1758 loss: 1.09459233e-06
Iter: 1759 loss: 1.09264431e-06
Iter: 1760 loss: 1.09488326e-06
Iter: 1761 loss: 1.09160123e-06
Iter: 1762 loss: 1.09010398e-06
Iter: 1763 loss: 1.09633061e-06
Iter: 1764 loss: 1.08973745e-06
Iter: 1765 loss: 1.08815539e-06
Iter: 1766 loss: 1.09500934e-06
Iter: 1767 loss: 1.08780864e-06
Iter: 1768 loss: 1.08639961e-06
Iter: 1769 loss: 1.08735026e-06
Iter: 1770 loss: 1.08549875e-06
Iter: 1771 loss: 1.08382153e-06
Iter: 1772 loss: 1.09273446e-06
Iter: 1773 loss: 1.08358358e-06
Iter: 1774 loss: 1.08187555e-06
Iter: 1775 loss: 1.08262157e-06
Iter: 1776 loss: 1.08072049e-06
Iter: 1777 loss: 1.079e-06
Iter: 1778 loss: 1.09042469e-06
Iter: 1779 loss: 1.07879532e-06
Iter: 1780 loss: 1.07706478e-06
Iter: 1781 loss: 1.0834018e-06
Iter: 1782 loss: 1.07663959e-06
Iter: 1783 loss: 1.07534356e-06
Iter: 1784 loss: 1.07562653e-06
Iter: 1785 loss: 1.07439871e-06
Iter: 1786 loss: 1.07262645e-06
Iter: 1787 loss: 1.08407073e-06
Iter: 1788 loss: 1.07244227e-06
Iter: 1789 loss: 1.07119195e-06
Iter: 1790 loss: 1.07017775e-06
Iter: 1791 loss: 1.06982907e-06
Iter: 1792 loss: 1.06793505e-06
Iter: 1793 loss: 1.07080496e-06
Iter: 1794 loss: 1.06695256e-06
Iter: 1795 loss: 1.0648846e-06
Iter: 1796 loss: 1.06831465e-06
Iter: 1797 loss: 1.06390121e-06
Iter: 1798 loss: 1.06224968e-06
Iter: 1799 loss: 1.06223752e-06
Iter: 1800 loss: 1.06070934e-06
Iter: 1801 loss: 1.05969536e-06
Iter: 1802 loss: 1.05912557e-06
Iter: 1803 loss: 1.05737683e-06
Iter: 1804 loss: 1.07339747e-06
Iter: 1805 loss: 1.0573674e-06
Iter: 1806 loss: 1.05598747e-06
Iter: 1807 loss: 1.05772551e-06
Iter: 1808 loss: 1.05522076e-06
Iter: 1809 loss: 1.05349613e-06
Iter: 1810 loss: 1.0590278e-06
Iter: 1811 loss: 1.0529385e-06
Iter: 1812 loss: 1.05152935e-06
Iter: 1813 loss: 1.0539527e-06
Iter: 1814 loss: 1.05092022e-06
Iter: 1815 loss: 1.04949368e-06
Iter: 1816 loss: 1.0646894e-06
Iter: 1817 loss: 1.04949459e-06
Iter: 1818 loss: 1.0484805e-06
Iter: 1819 loss: 1.04961782e-06
Iter: 1820 loss: 1.04789342e-06
Iter: 1821 loss: 1.04671551e-06
Iter: 1822 loss: 1.04889887e-06
Iter: 1823 loss: 1.04613946e-06
Iter: 1824 loss: 1.04479921e-06
Iter: 1825 loss: 1.04598644e-06
Iter: 1826 loss: 1.04404614e-06
Iter: 1827 loss: 1.04263438e-06
Iter: 1828 loss: 1.05411345e-06
Iter: 1829 loss: 1.04249284e-06
Iter: 1830 loss: 1.04133562e-06
Iter: 1831 loss: 1.0402938e-06
Iter: 1832 loss: 1.03998e-06
Iter: 1833 loss: 1.03795321e-06
Iter: 1834 loss: 1.04036314e-06
Iter: 1835 loss: 1.03677e-06
Iter: 1836 loss: 1.03489856e-06
Iter: 1837 loss: 1.04594892e-06
Iter: 1838 loss: 1.03463685e-06
Iter: 1839 loss: 1.03294792e-06
Iter: 1840 loss: 1.03517186e-06
Iter: 1841 loss: 1.0321229e-06
Iter: 1842 loss: 1.03039656e-06
Iter: 1843 loss: 1.03939169e-06
Iter: 1844 loss: 1.03013053e-06
Iter: 1845 loss: 1.02837259e-06
Iter: 1846 loss: 1.03673142e-06
Iter: 1847 loss: 1.02810577e-06
Iter: 1848 loss: 1.02666695e-06
Iter: 1849 loss: 1.02823662e-06
Iter: 1850 loss: 1.02586807e-06
Iter: 1851 loss: 1.0244139e-06
Iter: 1852 loss: 1.03066827e-06
Iter: 1853 loss: 1.0241298e-06
Iter: 1854 loss: 1.02246986e-06
Iter: 1855 loss: 1.02689592e-06
Iter: 1856 loss: 1.0219967e-06
Iter: 1857 loss: 1.02062245e-06
Iter: 1858 loss: 1.02815773e-06
Iter: 1859 loss: 1.0203803e-06
Iter: 1860 loss: 1.01916066e-06
Iter: 1861 loss: 1.01985825e-06
Iter: 1862 loss: 1.01829e-06
Iter: 1863 loss: 1.0169025e-06
Iter: 1864 loss: 1.01846217e-06
Iter: 1865 loss: 1.01621788e-06
Iter: 1866 loss: 1.01486182e-06
Iter: 1867 loss: 1.02822537e-06
Iter: 1868 loss: 1.0147952e-06
Iter: 1869 loss: 1.01367118e-06
Iter: 1870 loss: 1.01236344e-06
Iter: 1871 loss: 1.0121978e-06
Iter: 1872 loss: 1.01042747e-06
Iter: 1873 loss: 1.02399508e-06
Iter: 1874 loss: 1.01029798e-06
Iter: 1875 loss: 1.00877241e-06
Iter: 1876 loss: 1.0110939e-06
Iter: 1877 loss: 1.00800253e-06
Iter: 1878 loss: 1.00643308e-06
Iter: 1879 loss: 1.00536931e-06
Iter: 1880 loss: 1.00477268e-06
Iter: 1881 loss: 1.00261639e-06
Iter: 1882 loss: 1.01668547e-06
Iter: 1883 loss: 1.00238799e-06
Iter: 1884 loss: 1.00074067e-06
Iter: 1885 loss: 1.01114495e-06
Iter: 1886 loss: 1.00054717e-06
Iter: 1887 loss: 9.98949304e-07
Iter: 1888 loss: 1.00483646e-06
Iter: 1889 loss: 9.98600854e-07
Iter: 1890 loss: 9.97330062e-07
Iter: 1891 loss: 9.99546273e-07
Iter: 1892 loss: 9.96806079e-07
Iter: 1893 loss: 9.95476626e-07
Iter: 1894 loss: 1.00347222e-06
Iter: 1895 loss: 9.95295e-07
Iter: 1896 loss: 9.94013e-07
Iter: 1897 loss: 9.95642154e-07
Iter: 1898 loss: 9.93342155e-07
Iter: 1899 loss: 9.92067e-07
Iter: 1900 loss: 9.94781544e-07
Iter: 1901 loss: 9.91530101e-07
Iter: 1902 loss: 9.90100261e-07
Iter: 1903 loss: 9.94609309e-07
Iter: 1904 loss: 9.89662794e-07
Iter: 1905 loss: 9.88580268e-07
Iter: 1906 loss: 9.88702595e-07
Iter: 1907 loss: 9.87683393e-07
Iter: 1908 loss: 9.86139e-07
Iter: 1909 loss: 1.00078933e-06
Iter: 1910 loss: 9.86090186e-07
Iter: 1911 loss: 9.8502278e-07
Iter: 1912 loss: 9.83722e-07
Iter: 1913 loss: 9.83635687e-07
Iter: 1914 loss: 9.81994845e-07
Iter: 1915 loss: 9.93204935e-07
Iter: 1916 loss: 9.81794301e-07
Iter: 1917 loss: 9.80368668e-07
Iter: 1918 loss: 9.84115559e-07
Iter: 1919 loss: 9.79870265e-07
Iter: 1920 loss: 9.78485446e-07
Iter: 1921 loss: 9.84050189e-07
Iter: 1922 loss: 9.78190087e-07
Iter: 1923 loss: 9.77066179e-07
Iter: 1924 loss: 9.76212732e-07
Iter: 1925 loss: 9.75881107e-07
Iter: 1926 loss: 9.74234808e-07
Iter: 1927 loss: 9.94512902e-07
Iter: 1928 loss: 9.74218892e-07
Iter: 1929 loss: 9.729672e-07
Iter: 1930 loss: 9.79192464e-07
Iter: 1931 loss: 9.72749376e-07
Iter: 1932 loss: 9.71702889e-07
Iter: 1933 loss: 9.72742782e-07
Iter: 1934 loss: 9.71145937e-07
Iter: 1935 loss: 9.69752591e-07
Iter: 1936 loss: 9.75481e-07
Iter: 1937 loss: 9.6945223e-07
Iter: 1938 loss: 9.68393351e-07
Iter: 1939 loss: 9.68023414e-07
Iter: 1940 loss: 9.67388814e-07
Iter: 1941 loss: 9.66170887e-07
Iter: 1942 loss: 9.73328724e-07
Iter: 1943 loss: 9.65988647e-07
Iter: 1944 loss: 9.64478545e-07
Iter: 1945 loss: 9.65922823e-07
Iter: 1946 loss: 9.63619641e-07
Iter: 1947 loss: 9.6229337e-07
Iter: 1948 loss: 9.65817435e-07
Iter: 1949 loss: 9.61801106e-07
Iter: 1950 loss: 9.60415719e-07
Iter: 1951 loss: 9.66865e-07
Iter: 1952 loss: 9.6012991e-07
Iter: 1953 loss: 9.58766577e-07
Iter: 1954 loss: 9.59144245e-07
Iter: 1955 loss: 9.57734756e-07
Iter: 1956 loss: 9.56264103e-07
Iter: 1957 loss: 9.60366151e-07
Iter: 1958 loss: 9.55784799e-07
Iter: 1959 loss: 9.54308803e-07
Iter: 1960 loss: 9.58113e-07
Iter: 1961 loss: 9.53807785e-07
Iter: 1962 loss: 9.52335256e-07
Iter: 1963 loss: 9.56866643e-07
Iter: 1964 loss: 9.51931383e-07
Iter: 1965 loss: 9.50596529e-07
Iter: 1966 loss: 9.64148512e-07
Iter: 1967 loss: 9.50566744e-07
Iter: 1968 loss: 9.49560103e-07
Iter: 1969 loss: 9.51089646e-07
Iter: 1970 loss: 9.49076707e-07
Iter: 1971 loss: 9.47767433e-07
Iter: 1972 loss: 9.50395361e-07
Iter: 1973 loss: 9.47232138e-07
Iter: 1974 loss: 9.46113403e-07
Iter: 1975 loss: 9.47974513e-07
Iter: 1976 loss: 9.45628415e-07
Iter: 1977 loss: 9.44274461e-07
Iter: 1978 loss: 9.48932e-07
Iter: 1979 loss: 9.43852967e-07
Iter: 1980 loss: 9.42694669e-07
Iter: 1981 loss: 9.42318877e-07
Iter: 1982 loss: 9.4165739e-07
Iter: 1983 loss: 9.40227039e-07
Iter: 1984 loss: 9.5444193e-07
Iter: 1985 loss: 9.40189693e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi3_phi3/300_300_300_1
