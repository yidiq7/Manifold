script1 - testing
script2 - test zerolayer
script2a - 10000 points and batch, 5 epochs
script2b - 10000 points and batch, 50 epochs.  output in 2a
script2c - 10000 points and batch, 200 epochs - looks good
script3 - do a lot of zerolayer (k=1) runs
	  rewrite this one to match the runs below
script10 - like 2c with k=2,3
script11 - like 3 with k=2,3

These runs take about an hour each.
script4 - f1 grid psi = -3 etc.  
script5 - f1 grid psi = 0 1 - writes to output4
script6 - f1 grid psi = 2 3 - writes to output4

script7 - f2 grid psi = 0 1 - 500 epochs is really not needed
script8 - f2 grid psi = 2 3
script9 - f2 grid psi = -2 -1

script15 - f0 grid like script3 - 150 epochs
script16

script17 - f2 grid psi = 0 1 - 150 epochs
script18 - f2 grid psi = 2 3
script19 - f2 grid psi = -2 -1


data_fs1 - combine output3,10,11 ; 25->1; prepend 0 for f0; columns are
layers function psi phi train_time sigma_train sigma_test E_train E_test sigma_max_train sigma_max_test

data_nn1 - combine 4,5,6,17,18,19
data_nn2 - L_BFGS versions

Try polishing with max batch size ?
script20 - like script18 with batch size 50000
script21 - like script18 with batch size 100000
script22 - like script18 with 150_150_150_1 - worse
script23 - 300_300_300_1 and batch size 5000
script24 - 300_300_300_1 and batch size 20000
script25 - 300_300_300_1 with total number of points 1000,5000,20000

script30 - try L_BFGS
script104 - like 4 polishing with BFGS

script40 - try L_BFGS on 300_300_300_1 with initial model 

script44 - like 4 with 300_300_300_1 and 500_500_500_500_1 and 150 epochs
45, 46
script54 - like 17 with 300_300_300_1 and 500_500_500_500_1
55, 56   - combine the scripts with 44,45,46

script61 - try some more Adam first
also add a command line option to choose GPU and subset
script62 - take finished models from 61 and run L-BFGS.
script63 - combined 61/62, 300 epochs, 20000 pairs.
Tried 1000 and 100000 but was much too slow.
