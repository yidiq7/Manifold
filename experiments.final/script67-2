+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS=300_300_300_1
+ case $RUN in
+ PSI=1
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f2
+ case $fn in
+ OPT=--alpha
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0
+ date
Wed Nov  4 10:46:32 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f2 --psi 1 --alpha 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd167d3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd167d3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd1682b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd16856f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd167bb1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd27211f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd166ce6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd167069d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd16706e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd16664bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd1662ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd1662f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd16631488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd16631510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcb8135158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcb8144620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcb8144d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcb812a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd166ab620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcb80d49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcb80d4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc900ffd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc900c89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc900dcb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc900dc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcb807b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9002dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc90052488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc90052950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc7042d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc7042d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc70401c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc70401a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc900706a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc7031d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc7032df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0017408909
test_loss: 0.0017451486
train_loss: 0.0017308713
test_loss: 0.0019602503
train_loss: 0.0017037252
test_loss: 0.0016755988
train_loss: 0.001769519
test_loss: 0.001748712
train_loss: 0.0015558689
test_loss: 0.0016634098
train_loss: 0.0016683105
test_loss: 0.0017728929
train_loss: 0.0018693119
test_loss: 0.0020445148
train_loss: 0.0018174275
test_loss: 0.001668746
train_loss: 0.0016397671
test_loss: 0.0016270098
train_loss: 0.0015143924
test_loss: 0.0016501033
train_loss: 0.0016695436
test_loss: 0.0015502834
train_loss: 0.0016066618
test_loss: 0.0016553393
train_loss: 0.0016304143
test_loss: 0.0019212825
train_loss: 0.0016418862
test_loss: 0.001717901
train_loss: 0.0015236099
test_loss: 0.0016515184
train_loss: 0.0015958741
test_loss: 0.0016546111
train_loss: 0.0015601935
test_loss: 0.0018747628
train_loss: 0.0016031729
test_loss: 0.0017517074
train_loss: 0.0014732644
test_loss: 0.0016217842
train_loss: 0.0016853978
test_loss: 0.0015330733
train_loss: 0.0015689052
test_loss: 0.0018786427
train_loss: 0.0015984953
test_loss: 0.0016339372
train_loss: 0.0017975904
test_loss: 0.0018009599
train_loss: 0.0015453718
test_loss: 0.0017416495
train_loss: 0.0015788274
test_loss: 0.0017290501
train_loss: 0.001433365
test_loss: 0.001704571
train_loss: 0.0015187843
test_loss: 0.0016040393
train_loss: 0.0016245414
test_loss: 0.0017112163
train_loss: 0.0015811517
test_loss: 0.00161325
train_loss: 0.0016509397
test_loss: 0.0016901207
train_loss: 0.0016381596
test_loss: 0.0017486576
train_loss: 0.0017767138
test_loss: 0.0016682873
train_loss: 0.0015841122
test_loss: 0.0018090974
train_loss: 0.0015667122
test_loss: 0.0017353358
train_loss: 0.001585014
test_loss: 0.0018238733
train_loss: 0.0017270058
test_loss: 0.001734205
train_loss: 0.0015120544
test_loss: 0.0017927386
train_loss: 0.0016270401
test_loss: 0.0016545606
train_loss: 0.0016221306
test_loss: 0.0016671757
train_loss: 0.0016252532
test_loss: 0.0018074836
train_loss: 0.0015548347
test_loss: 0.0015950728
train_loss: 0.0015005997
test_loss: 0.0015921503
train_loss: 0.0015254472
test_loss: 0.0017028901
train_loss: 0.0016086514
test_loss: 0.0017565961
train_loss: 0.0015991849
test_loss: 0.0017487686
train_loss: 0.0014950897
test_loss: 0.0017991549
train_loss: 0.0014948193
test_loss: 0.0016435802
train_loss: 0.0014715263
test_loss: 0.0015608824
train_loss: 0.001521772
test_loss: 0.0017401228
train_loss: 0.0015244086
test_loss: 0.0015645293
train_loss: 0.0016127403
test_loss: 0.0018977018
train_loss: 0.0016026687
test_loss: 0.0017549763
train_loss: 0.0016613473
test_loss: 0.0015705981
train_loss: 0.0014054675
test_loss: 0.0016529178
train_loss: 0.0017249859
test_loss: 0.0018520778
train_loss: 0.0014072839
test_loss: 0.0016898157
train_loss: 0.0014785415
test_loss: 0.0015933929
train_loss: 0.0015119989
test_loss: 0.0016455547
train_loss: 0.0015085184
test_loss: 0.0016651382
train_loss: 0.0013853715
test_loss: 0.0015874532
train_loss: 0.0014587521
test_loss: 0.0016036254
train_loss: 0.0013922896
test_loss: 0.0017134841
train_loss: 0.0014823035
test_loss: 0.0015830857
train_loss: 0.0018096515
test_loss: 0.0016411535
train_loss: 0.0014730638
test_loss: 0.0016555523
train_loss: 0.0014938186
test_loss: 0.0017092916
train_loss: 0.0014432059
test_loss: 0.0015914867
train_loss: 0.0014963275
test_loss: 0.0015916539
train_loss: 0.0014185777
test_loss: 0.0015941153
train_loss: 0.0014279815
test_loss: 0.0016378489
train_loss: 0.0015578885
test_loss: 0.0016130531
train_loss: 0.0015894877
test_loss: 0.001568882
train_loss: 0.0014443533
test_loss: 0.0015997408
train_loss: 0.0014386423
test_loss: 0.0016060491
train_loss: 0.0014083195
test_loss: 0.0015611682
train_loss: 0.0014030257
test_loss: 0.0015149115
train_loss: 0.0014482763
test_loss: 0.0017467221
train_loss: 0.0014484477
test_loss: 0.0017008375
train_loss: 0.0014010032
test_loss: 0.0016725024
train_loss: 0.0014168436
test_loss: 0.0015783932
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0/300_300_300_1 --optimizer lbfgs --function f2 --psi 1 --alpha 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd02aa3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd02ad1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd02aa3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd8302de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd82f91488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd82f916a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd02a641e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd02a649d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd82f919d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd029dc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd029dc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd82f91840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd82f92d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcdc12f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcdc0ffd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcdc0ffd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcdc0d46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcdc0fb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcdc07b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd47eb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd47ebe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd47a9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd4752950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd47147b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd47148c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd4726f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd46dfb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd4685620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd4688730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd4688bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd46b6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd460ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd46539d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd45babf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd459ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efcd45ad2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.21622633e-06
Iter: 2 loss: 2.87517764e-06
Iter: 3 loss: 2.44469675e-06
Iter: 4 loss: 2.23842471e-06
Iter: 5 loss: 2.04775051e-06
Iter: 6 loss: 1.99833494e-06
Iter: 7 loss: 1.98328416e-06
Iter: 8 loss: 1.94678319e-06
Iter: 9 loss: 1.90668811e-06
Iter: 10 loss: 1.79922029e-06
Iter: 11 loss: 2.53448297e-06
Iter: 12 loss: 1.7742891e-06
Iter: 13 loss: 1.76360948e-06
Iter: 14 loss: 1.7432958e-06
Iter: 15 loss: 1.71441468e-06
Iter: 16 loss: 1.69600253e-06
Iter: 17 loss: 1.68464624e-06
Iter: 18 loss: 1.63408095e-06
Iter: 19 loss: 1.62083984e-06
Iter: 20 loss: 1.58932858e-06
Iter: 21 loss: 1.52193275e-06
Iter: 22 loss: 1.96488099e-06
Iter: 23 loss: 1.51485392e-06
Iter: 24 loss: 1.46189097e-06
Iter: 25 loss: 1.99052488e-06
Iter: 26 loss: 1.46017101e-06
Iter: 27 loss: 1.44064586e-06
Iter: 28 loss: 1.39603594e-06
Iter: 29 loss: 1.98042221e-06
Iter: 30 loss: 1.39312795e-06
Iter: 31 loss: 1.3623378e-06
Iter: 32 loss: 1.46299385e-06
Iter: 33 loss: 1.35379582e-06
Iter: 34 loss: 1.32589093e-06
Iter: 35 loss: 1.36558515e-06
Iter: 36 loss: 1.31223135e-06
Iter: 37 loss: 1.28523175e-06
Iter: 38 loss: 1.71022771e-06
Iter: 39 loss: 1.28523482e-06
Iter: 40 loss: 1.24865778e-06
Iter: 41 loss: 1.19688184e-06
Iter: 42 loss: 1.19501601e-06
Iter: 43 loss: 1.17780883e-06
Iter: 44 loss: 1.17529964e-06
Iter: 45 loss: 1.15453088e-06
Iter: 46 loss: 1.13870328e-06
Iter: 47 loss: 1.13199894e-06
Iter: 48 loss: 1.11479403e-06
Iter: 49 loss: 1.11450709e-06
Iter: 50 loss: 1.10091673e-06
Iter: 51 loss: 1.08470704e-06
Iter: 52 loss: 1.08336837e-06
Iter: 53 loss: 1.07513426e-06
Iter: 54 loss: 1.05467416e-06
Iter: 55 loss: 1.25495615e-06
Iter: 56 loss: 1.05203173e-06
Iter: 57 loss: 1.03019033e-06
Iter: 58 loss: 1.33961316e-06
Iter: 59 loss: 1.03012576e-06
Iter: 60 loss: 1.01068724e-06
Iter: 61 loss: 1.03743889e-06
Iter: 62 loss: 1.00101136e-06
Iter: 63 loss: 9.77311174e-07
Iter: 64 loss: 9.64181e-07
Iter: 65 loss: 9.53737299e-07
Iter: 66 loss: 9.32315459e-07
Iter: 67 loss: 9.59200179e-07
Iter: 68 loss: 9.2123463e-07
Iter: 69 loss: 8.99637087e-07
Iter: 70 loss: 9.32422211e-07
Iter: 71 loss: 8.89399587e-07
Iter: 72 loss: 8.71014265e-07
Iter: 73 loss: 1.14910983e-06
Iter: 74 loss: 8.71010172e-07
Iter: 75 loss: 8.4927467e-07
Iter: 76 loss: 9.06951357e-07
Iter: 77 loss: 8.4205e-07
Iter: 78 loss: 8.31002069e-07
Iter: 79 loss: 8.39198094e-07
Iter: 80 loss: 8.24242591e-07
Iter: 81 loss: 8.0480487e-07
Iter: 82 loss: 8.83121288e-07
Iter: 83 loss: 8.00472321e-07
Iter: 84 loss: 7.94024686e-07
Iter: 85 loss: 7.88005366e-07
Iter: 86 loss: 7.8648327e-07
Iter: 87 loss: 7.83936912e-07
Iter: 88 loss: 7.82073641e-07
Iter: 89 loss: 7.78291223e-07
Iter: 90 loss: 7.67011898e-07
Iter: 91 loss: 8.03075068e-07
Iter: 92 loss: 7.61518947e-07
Iter: 93 loss: 7.56961754e-07
Iter: 94 loss: 7.53942459e-07
Iter: 95 loss: 7.4607442e-07
Iter: 96 loss: 7.31223054e-07
Iter: 97 loss: 1.05684035e-06
Iter: 98 loss: 7.31163141e-07
Iter: 99 loss: 7.16850309e-07
Iter: 100 loss: 8.03652597e-07
Iter: 101 loss: 7.15088788e-07
Iter: 102 loss: 7.03982494e-07
Iter: 103 loss: 7.08918037e-07
Iter: 104 loss: 6.96458244e-07
Iter: 105 loss: 6.87081751e-07
Iter: 106 loss: 7.20421554e-07
Iter: 107 loss: 6.84707459e-07
Iter: 108 loss: 6.77407684e-07
Iter: 109 loss: 6.66533424e-07
Iter: 110 loss: 6.66291214e-07
Iter: 111 loss: 6.71104942e-07
Iter: 112 loss: 6.59986256e-07
Iter: 113 loss: 6.53986717e-07
Iter: 114 loss: 6.47016122e-07
Iter: 115 loss: 6.4619303e-07
Iter: 116 loss: 6.42961254e-07
Iter: 117 loss: 6.42161751e-07
Iter: 118 loss: 6.38867e-07
Iter: 119 loss: 6.30411535e-07
Iter: 120 loss: 6.99892325e-07
Iter: 121 loss: 6.28886369e-07
Iter: 122 loss: 6.23667233e-07
Iter: 123 loss: 6.28740395e-07
Iter: 124 loss: 6.20664878e-07
Iter: 125 loss: 6.16843067e-07
Iter: 126 loss: 6.1668095e-07
Iter: 127 loss: 6.11613586e-07
Iter: 128 loss: 6.08786763e-07
Iter: 129 loss: 6.0655475e-07
Iter: 130 loss: 6.01021725e-07
Iter: 131 loss: 6.01959641e-07
Iter: 132 loss: 5.96834468e-07
Iter: 133 loss: 5.94462335e-07
Iter: 134 loss: 5.92841729e-07
Iter: 135 loss: 5.91407115e-07
Iter: 136 loss: 5.87508111e-07
Iter: 137 loss: 6.12960662e-07
Iter: 138 loss: 5.86580086e-07
Iter: 139 loss: 5.81828317e-07
Iter: 140 loss: 6.23548885e-07
Iter: 141 loss: 5.81575705e-07
Iter: 142 loss: 5.78165441e-07
Iter: 143 loss: 5.78233823e-07
Iter: 144 loss: 5.75451168e-07
Iter: 145 loss: 5.70548252e-07
Iter: 146 loss: 5.77281867e-07
Iter: 147 loss: 5.68129281e-07
Iter: 148 loss: 5.61332286e-07
Iter: 149 loss: 6.41265046e-07
Iter: 150 loss: 5.61231047e-07
Iter: 151 loss: 5.5889484e-07
Iter: 152 loss: 5.55615316e-07
Iter: 153 loss: 5.55511178e-07
Iter: 154 loss: 5.54052747e-07
Iter: 155 loss: 5.53172299e-07
Iter: 156 loss: 5.52284746e-07
Iter: 157 loss: 5.49486231e-07
Iter: 158 loss: 5.55370889e-07
Iter: 159 loss: 5.47801051e-07
Iter: 160 loss: 5.45114744e-07
Iter: 161 loss: 5.45098715e-07
Iter: 162 loss: 5.41871373e-07
Iter: 163 loss: 5.45487751e-07
Iter: 164 loss: 5.40103315e-07
Iter: 165 loss: 5.3769844e-07
Iter: 166 loss: 5.38974575e-07
Iter: 167 loss: 5.3611393e-07
Iter: 168 loss: 5.3157089e-07
Iter: 169 loss: 5.43393696e-07
Iter: 170 loss: 5.30031343e-07
Iter: 171 loss: 5.27561e-07
Iter: 172 loss: 5.24008215e-07
Iter: 173 loss: 5.2391465e-07
Iter: 174 loss: 5.21328218e-07
Iter: 175 loss: 5.57876888e-07
Iter: 176 loss: 5.21322079e-07
Iter: 177 loss: 5.18890886e-07
Iter: 178 loss: 5.17971216e-07
Iter: 179 loss: 5.16611181e-07
Iter: 180 loss: 5.12623444e-07
Iter: 181 loss: 5.16195314e-07
Iter: 182 loss: 5.10267228e-07
Iter: 183 loss: 5.11507324e-07
Iter: 184 loss: 5.08912763e-07
Iter: 185 loss: 5.07570917e-07
Iter: 186 loss: 5.0411586e-07
Iter: 187 loss: 5.30604552e-07
Iter: 188 loss: 5.03417311e-07
Iter: 189 loss: 5.03906278e-07
Iter: 190 loss: 5.02009e-07
Iter: 191 loss: 5.01100317e-07
Iter: 192 loss: 4.98954535e-07
Iter: 193 loss: 5.23310405e-07
Iter: 194 loss: 4.98757117e-07
Iter: 195 loss: 4.96666189e-07
Iter: 196 loss: 4.94527512e-07
Iter: 197 loss: 4.94147685e-07
Iter: 198 loss: 4.90271702e-07
Iter: 199 loss: 5.04497507e-07
Iter: 200 loss: 4.89325203e-07
Iter: 201 loss: 4.86581939e-07
Iter: 202 loss: 4.86320403e-07
Iter: 203 loss: 4.85526584e-07
Iter: 204 loss: 4.83539111e-07
Iter: 205 loss: 5.01533748e-07
Iter: 206 loss: 4.83237955e-07
Iter: 207 loss: 4.82422649e-07
Iter: 208 loss: 4.8188474e-07
Iter: 209 loss: 4.81155382e-07
Iter: 210 loss: 4.7967518e-07
Iter: 211 loss: 5.10371592e-07
Iter: 212 loss: 4.79671144e-07
Iter: 213 loss: 4.78179572e-07
Iter: 214 loss: 4.79300184e-07
Iter: 215 loss: 4.77269737e-07
Iter: 216 loss: 4.74304187e-07
Iter: 217 loss: 4.75187875e-07
Iter: 218 loss: 4.7217921e-07
Iter: 219 loss: 4.7270683e-07
Iter: 220 loss: 4.70991836e-07
Iter: 221 loss: 4.70066567e-07
Iter: 222 loss: 4.67891027e-07
Iter: 223 loss: 4.9567177e-07
Iter: 224 loss: 4.6774619e-07
Iter: 225 loss: 4.6576e-07
Iter: 226 loss: 4.67053923e-07
Iter: 227 loss: 4.6450549e-07
Iter: 228 loss: 4.62841086e-07
Iter: 229 loss: 4.63328206e-07
Iter: 230 loss: 4.61663234e-07
Iter: 231 loss: 4.61691485e-07
Iter: 232 loss: 4.60618935e-07
Iter: 233 loss: 4.59606895e-07
Iter: 234 loss: 4.56854139e-07
Iter: 235 loss: 4.73767102e-07
Iter: 236 loss: 4.56125633e-07
Iter: 237 loss: 4.54520091e-07
Iter: 238 loss: 4.54458956e-07
Iter: 239 loss: 4.5275462e-07
Iter: 240 loss: 4.55145681e-07
Iter: 241 loss: 4.51916833e-07
Iter: 242 loss: 4.50988182e-07
Iter: 243 loss: 4.4932014e-07
Iter: 244 loss: 4.90102138e-07
Iter: 245 loss: 4.49305588e-07
Iter: 246 loss: 4.49199433e-07
Iter: 247 loss: 4.48416813e-07
Iter: 248 loss: 4.47653633e-07
Iter: 249 loss: 4.45731587e-07
Iter: 250 loss: 4.63639736e-07
Iter: 251 loss: 4.45459136e-07
Iter: 252 loss: 4.44254113e-07
Iter: 253 loss: 4.44237031e-07
Iter: 254 loss: 4.4270476e-07
Iter: 255 loss: 4.43229112e-07
Iter: 256 loss: 4.41635848e-07
Iter: 257 loss: 4.40224539e-07
Iter: 258 loss: 4.39015281e-07
Iter: 259 loss: 4.38646879e-07
Iter: 260 loss: 4.37356448e-07
Iter: 261 loss: 4.39429471e-07
Iter: 262 loss: 4.36782244e-07
Iter: 263 loss: 4.35846687e-07
Iter: 264 loss: 4.39168446e-07
Iter: 265 loss: 4.35588674e-07
Iter: 266 loss: 4.34384503e-07
Iter: 267 loss: 4.39704365e-07
Iter: 268 loss: 4.34143203e-07
Iter: 269 loss: 4.3340043e-07
Iter: 270 loss: 4.32413572e-07
Iter: 271 loss: 4.32350532e-07
Iter: 272 loss: 4.30890083e-07
Iter: 273 loss: 4.48747016e-07
Iter: 274 loss: 4.3088437e-07
Iter: 275 loss: 4.2988691e-07
Iter: 276 loss: 4.31021334e-07
Iter: 277 loss: 4.29371823e-07
Iter: 278 loss: 4.28340456e-07
Iter: 279 loss: 4.27412175e-07
Iter: 280 loss: 4.27147171e-07
Iter: 281 loss: 4.26660677e-07
Iter: 282 loss: 4.26406899e-07
Iter: 283 loss: 4.25721e-07
Iter: 284 loss: 4.24446313e-07
Iter: 285 loss: 4.53170173e-07
Iter: 286 loss: 4.24456971e-07
Iter: 287 loss: 4.23264254e-07
Iter: 288 loss: 4.25928022e-07
Iter: 289 loss: 4.22811581e-07
Iter: 290 loss: 4.21063959e-07
Iter: 291 loss: 4.32128445e-07
Iter: 292 loss: 4.20867309e-07
Iter: 293 loss: 4.20270538e-07
Iter: 294 loss: 4.18924117e-07
Iter: 295 loss: 4.37569042e-07
Iter: 296 loss: 4.18862243e-07
Iter: 297 loss: 4.17873849e-07
Iter: 298 loss: 4.17830364e-07
Iter: 299 loss: 4.17040212e-07
Iter: 300 loss: 4.21606416e-07
Iter: 301 loss: 4.16914474e-07
Iter: 302 loss: 4.16388929e-07
Iter: 303 loss: 4.15607616e-07
Iter: 304 loss: 4.15588801e-07
Iter: 305 loss: 4.14864616e-07
Iter: 306 loss: 4.22618655e-07
Iter: 307 loss: 4.14842e-07
Iter: 308 loss: 4.13972202e-07
Iter: 309 loss: 4.13866104e-07
Iter: 310 loss: 4.1323716e-07
Iter: 311 loss: 4.12037224e-07
Iter: 312 loss: 4.13778082e-07
Iter: 313 loss: 4.11482546e-07
Iter: 314 loss: 4.10453765e-07
Iter: 315 loss: 4.13781208e-07
Iter: 316 loss: 4.10174607e-07
Iter: 317 loss: 4.09394602e-07
Iter: 318 loss: 4.09388e-07
Iter: 319 loss: 4.0903717e-07
Iter: 320 loss: 4.08166272e-07
Iter: 321 loss: 4.17637807e-07
Iter: 322 loss: 4.0805233e-07
Iter: 323 loss: 4.08392054e-07
Iter: 324 loss: 4.07734035e-07
Iter: 325 loss: 4.0748904e-07
Iter: 326 loss: 4.06823602e-07
Iter: 327 loss: 4.1204197e-07
Iter: 328 loss: 4.06691441e-07
Iter: 329 loss: 4.0592e-07
Iter: 330 loss: 4.0499512e-07
Iter: 331 loss: 4.04896923e-07
Iter: 332 loss: 4.05308384e-07
Iter: 333 loss: 4.04311095e-07
Iter: 334 loss: 4.03707219e-07
Iter: 335 loss: 4.03028452e-07
Iter: 336 loss: 4.02933892e-07
Iter: 337 loss: 4.02357216e-07
Iter: 338 loss: 4.02127625e-07
Iter: 339 loss: 4.01790118e-07
Iter: 340 loss: 4.0117385e-07
Iter: 341 loss: 4.07195785e-07
Iter: 342 loss: 4.01152505e-07
Iter: 343 loss: 4.00615704e-07
Iter: 344 loss: 4.06214951e-07
Iter: 345 loss: 4.00624174e-07
Iter: 346 loss: 4.00370539e-07
Iter: 347 loss: 3.99671194e-07
Iter: 348 loss: 4.03582703e-07
Iter: 349 loss: 3.99453796e-07
Iter: 350 loss: 3.98274892e-07
Iter: 351 loss: 4.04411509e-07
Iter: 352 loss: 3.9808026e-07
Iter: 353 loss: 3.97303069e-07
Iter: 354 loss: 4.08767e-07
Iter: 355 loss: 3.9728684e-07
Iter: 356 loss: 3.96654656e-07
Iter: 357 loss: 3.95282797e-07
Iter: 358 loss: 4.17554304e-07
Iter: 359 loss: 3.95244541e-07
Iter: 360 loss: 3.95045902e-07
Iter: 361 loss: 3.94839e-07
Iter: 362 loss: 3.94365969e-07
Iter: 363 loss: 3.9403e-07
Iter: 364 loss: 3.93865605e-07
Iter: 365 loss: 3.93406424e-07
Iter: 366 loss: 3.93160576e-07
Iter: 367 loss: 3.92934055e-07
Iter: 368 loss: 3.9261721e-07
Iter: 369 loss: 3.92465296e-07
Iter: 370 loss: 3.9222391e-07
Iter: 371 loss: 3.9143211e-07
Iter: 372 loss: 3.92992945e-07
Iter: 373 loss: 3.9092879e-07
Iter: 374 loss: 3.89774243e-07
Iter: 375 loss: 3.97056453e-07
Iter: 376 loss: 3.89653195e-07
Iter: 377 loss: 3.89392483e-07
Iter: 378 loss: 3.89307843e-07
Iter: 379 loss: 3.88927162e-07
Iter: 380 loss: 3.88933472e-07
Iter: 381 loss: 3.88624329e-07
Iter: 382 loss: 3.88268e-07
Iter: 383 loss: 3.88230035e-07
Iter: 384 loss: 3.87965457e-07
Iter: 385 loss: 3.87513779e-07
Iter: 386 loss: 3.92630568e-07
Iter: 387 loss: 3.87490502e-07
Iter: 388 loss: 3.87121929e-07
Iter: 389 loss: 3.87461313e-07
Iter: 390 loss: 3.8688836e-07
Iter: 391 loss: 3.86397403e-07
Iter: 392 loss: 3.85484583e-07
Iter: 393 loss: 4.07715135e-07
Iter: 394 loss: 3.85480945e-07
Iter: 395 loss: 3.85045382e-07
Iter: 396 loss: 3.84963101e-07
Iter: 397 loss: 3.84405666e-07
Iter: 398 loss: 3.85035833e-07
Iter: 399 loss: 3.84098939e-07
Iter: 400 loss: 3.83873385e-07
Iter: 401 loss: 3.83762142e-07
Iter: 402 loss: 3.83627651e-07
Iter: 403 loss: 3.83276415e-07
Iter: 404 loss: 3.88260332e-07
Iter: 405 loss: 3.83278461e-07
Iter: 406 loss: 3.83072518e-07
Iter: 407 loss: 3.82448405e-07
Iter: 408 loss: 3.83799431e-07
Iter: 409 loss: 3.8208853e-07
Iter: 410 loss: 3.81184918e-07
Iter: 411 loss: 3.8361128e-07
Iter: 412 loss: 3.80886775e-07
Iter: 413 loss: 3.80116035e-07
Iter: 414 loss: 3.87360757e-07
Iter: 415 loss: 3.80069707e-07
Iter: 416 loss: 3.7936519e-07
Iter: 417 loss: 3.86921e-07
Iter: 418 loss: 3.79348279e-07
Iter: 419 loss: 3.79134576e-07
Iter: 420 loss: 3.78574612e-07
Iter: 421 loss: 3.81703586e-07
Iter: 422 loss: 3.78414882e-07
Iter: 423 loss: 3.77838461e-07
Iter: 424 loss: 3.77840564e-07
Iter: 425 loss: 3.77205652e-07
Iter: 426 loss: 3.78575805e-07
Iter: 427 loss: 3.76966454e-07
Iter: 428 loss: 3.76585717e-07
Iter: 429 loss: 3.77173393e-07
Iter: 430 loss: 3.76418143e-07
Iter: 431 loss: 3.75939237e-07
Iter: 432 loss: 3.779206e-07
Iter: 433 loss: 3.7584158e-07
Iter: 434 loss: 3.75485797e-07
Iter: 435 loss: 3.75293212e-07
Iter: 436 loss: 3.75145305e-07
Iter: 437 loss: 3.74778324e-07
Iter: 438 loss: 3.79888178e-07
Iter: 439 loss: 3.74778551e-07
Iter: 440 loss: 3.74347195e-07
Iter: 441 loss: 3.74577837e-07
Iter: 442 loss: 3.74065735e-07
Iter: 443 loss: 3.73708247e-07
Iter: 444 loss: 3.74251158e-07
Iter: 445 loss: 3.73530895e-07
Iter: 446 loss: 3.7308655e-07
Iter: 447 loss: 3.7229114e-07
Iter: 448 loss: 3.72292618e-07
Iter: 449 loss: 3.71821784e-07
Iter: 450 loss: 3.71767328e-07
Iter: 451 loss: 3.71280379e-07
Iter: 452 loss: 3.7321098e-07
Iter: 453 loss: 3.71135172e-07
Iter: 454 loss: 3.70860562e-07
Iter: 455 loss: 3.70180544e-07
Iter: 456 loss: 3.77256526e-07
Iter: 457 loss: 3.70112161e-07
Iter: 458 loss: 3.7022329e-07
Iter: 459 loss: 3.6979776e-07
Iter: 460 loss: 3.69558279e-07
Iter: 461 loss: 3.69088468e-07
Iter: 462 loss: 3.771558e-07
Iter: 463 loss: 3.69081249e-07
Iter: 464 loss: 3.68585802e-07
Iter: 465 loss: 3.76088735e-07
Iter: 466 loss: 3.6857989e-07
Iter: 467 loss: 3.68319462e-07
Iter: 468 loss: 3.6768094e-07
Iter: 469 loss: 3.75234578e-07
Iter: 470 loss: 3.67639814e-07
Iter: 471 loss: 3.67117934e-07
Iter: 472 loss: 3.73837793e-07
Iter: 473 loss: 3.67134191e-07
Iter: 474 loss: 3.66765477e-07
Iter: 475 loss: 3.69365978e-07
Iter: 476 loss: 3.66736913e-07
Iter: 477 loss: 3.66507692e-07
Iter: 478 loss: 3.66197128e-07
Iter: 479 loss: 3.66180643e-07
Iter: 480 loss: 3.65725953e-07
Iter: 481 loss: 3.65507162e-07
Iter: 482 loss: 3.65280727e-07
Iter: 483 loss: 3.64702856e-07
Iter: 484 loss: 3.66362258e-07
Iter: 485 loss: 3.64539289e-07
Iter: 486 loss: 3.64128425e-07
Iter: 487 loss: 3.70394105e-07
Iter: 488 loss: 3.64124389e-07
Iter: 489 loss: 3.63644091e-07
Iter: 490 loss: 3.64441405e-07
Iter: 491 loss: 3.63436357e-07
Iter: 492 loss: 3.6306821e-07
Iter: 493 loss: 3.62482126e-07
Iter: 494 loss: 3.62493978e-07
Iter: 495 loss: 3.62486389e-07
Iter: 496 loss: 3.62177843e-07
Iter: 497 loss: 3.62001373e-07
Iter: 498 loss: 3.61683448e-07
Iter: 499 loss: 3.69439704e-07
Iter: 500 loss: 3.61681714e-07
Iter: 501 loss: 3.61424298e-07
Iter: 502 loss: 3.6139653e-07
Iter: 503 loss: 3.61275625e-07
Iter: 504 loss: 3.6102557e-07
Iter: 505 loss: 3.61024206e-07
Iter: 506 loss: 3.60867887e-07
Iter: 507 loss: 3.60864931e-07
Iter: 508 loss: 3.60720094e-07
Iter: 509 loss: 3.60371502e-07
Iter: 510 loss: 3.62788342e-07
Iter: 511 loss: 3.60289931e-07
Iter: 512 loss: 3.59691967e-07
Iter: 513 loss: 3.60892159e-07
Iter: 514 loss: 3.59487842e-07
Iter: 515 loss: 3.59019623e-07
Iter: 516 loss: 3.59014308e-07
Iter: 517 loss: 3.5873137e-07
Iter: 518 loss: 3.58327327e-07
Iter: 519 loss: 3.58326758e-07
Iter: 520 loss: 3.58619616e-07
Iter: 521 loss: 3.5818735e-07
Iter: 522 loss: 3.58070707e-07
Iter: 523 loss: 3.57756306e-07
Iter: 524 loss: 3.60745332e-07
Iter: 525 loss: 3.57695654e-07
Iter: 526 loss: 3.57470043e-07
Iter: 527 loss: 3.57479394e-07
Iter: 528 loss: 3.57206e-07
Iter: 529 loss: 3.56653317e-07
Iter: 530 loss: 3.6699376e-07
Iter: 531 loss: 3.56641351e-07
Iter: 532 loss: 3.56372254e-07
Iter: 533 loss: 3.56326467e-07
Iter: 534 loss: 3.56042e-07
Iter: 535 loss: 3.55754537e-07
Iter: 536 loss: 3.55682857e-07
Iter: 537 loss: 3.5526611e-07
Iter: 538 loss: 3.55154782e-07
Iter: 539 loss: 3.54907229e-07
Iter: 540 loss: 3.55046097e-07
Iter: 541 loss: 3.54714416e-07
Iter: 542 loss: 3.54569352e-07
Iter: 543 loss: 3.54279564e-07
Iter: 544 loss: 3.60772617e-07
Iter: 545 loss: 3.54278768e-07
Iter: 546 loss: 3.54019619e-07
Iter: 547 loss: 3.53828966e-07
Iter: 548 loss: 3.53721077e-07
Iter: 549 loss: 3.53429471e-07
Iter: 550 loss: 3.52991606e-07
Iter: 551 loss: 3.52954146e-07
Iter: 552 loss: 3.52492748e-07
Iter: 553 loss: 3.51990622e-07
Iter: 554 loss: 3.51920164e-07
Iter: 555 loss: 3.51553382e-07
Iter: 556 loss: 3.5154244e-07
Iter: 557 loss: 3.51231193e-07
Iter: 558 loss: 3.52212396e-07
Iter: 559 loss: 3.51136123e-07
Iter: 560 loss: 3.50789492e-07
Iter: 561 loss: 3.50534748e-07
Iter: 562 loss: 3.50429104e-07
Iter: 563 loss: 3.50359699e-07
Iter: 564 loss: 3.50197297e-07
Iter: 565 loss: 3.50055217e-07
Iter: 566 loss: 3.49696506e-07
Iter: 567 loss: 3.52383097e-07
Iter: 568 loss: 3.49615107e-07
Iter: 569 loss: 3.49186109e-07
Iter: 570 loss: 3.49185257e-07
Iter: 571 loss: 3.49012453e-07
Iter: 572 loss: 3.4872312e-07
Iter: 573 loss: 3.48729714e-07
Iter: 574 loss: 3.48619608e-07
Iter: 575 loss: 3.48547957e-07
Iter: 576 loss: 3.48432366e-07
Iter: 577 loss: 3.48142152e-07
Iter: 578 loss: 3.49468422e-07
Iter: 579 loss: 3.48022411e-07
Iter: 580 loss: 3.47646761e-07
Iter: 581 loss: 3.48404853e-07
Iter: 582 loss: 3.47529578e-07
Iter: 583 loss: 3.47229843e-07
Iter: 584 loss: 3.47223931e-07
Iter: 585 loss: 3.4705414e-07
Iter: 586 loss: 3.46689376e-07
Iter: 587 loss: 3.51709559e-07
Iter: 588 loss: 3.46664336e-07
Iter: 589 loss: 3.46351385e-07
Iter: 590 loss: 3.47066049e-07
Iter: 591 loss: 3.46228916e-07
Iter: 592 loss: 3.46058101e-07
Iter: 593 loss: 3.46143963e-07
Iter: 594 loss: 3.45951179e-07
Iter: 595 loss: 3.4575595e-07
Iter: 596 loss: 3.48484264e-07
Iter: 597 loss: 3.45760157e-07
Iter: 598 loss: 3.45650221e-07
Iter: 599 loss: 3.45420744e-07
Iter: 600 loss: 3.48975362e-07
Iter: 601 loss: 3.45391697e-07
Iter: 602 loss: 3.45232337e-07
Iter: 603 loss: 3.45209685e-07
Iter: 604 loss: 3.45085027e-07
Iter: 605 loss: 3.44709207e-07
Iter: 606 loss: 3.46248527e-07
Iter: 607 loss: 3.44575085e-07
Iter: 608 loss: 3.44697241e-07
Iter: 609 loss: 3.44355271e-07
Iter: 610 loss: 3.44223167e-07
Iter: 611 loss: 3.43976978e-07
Iter: 612 loss: 3.48842065e-07
Iter: 613 loss: 3.43982805e-07
Iter: 614 loss: 3.43687816e-07
Iter: 615 loss: 3.43995367e-07
Iter: 616 loss: 3.43526466e-07
Iter: 617 loss: 3.43379753e-07
Iter: 618 loss: 3.43694609e-07
Iter: 619 loss: 3.43314127e-07
Iter: 620 loss: 3.43125635e-07
Iter: 621 loss: 3.43442707e-07
Iter: 622 loss: 3.43028233e-07
Iter: 623 loss: 3.42837893e-07
Iter: 624 loss: 3.42784404e-07
Iter: 625 loss: 3.42683251e-07
Iter: 626 loss: 3.42299074e-07
Iter: 627 loss: 3.42873108e-07
Iter: 628 loss: 3.4214429e-07
Iter: 629 loss: 3.4177981e-07
Iter: 630 loss: 3.42656676e-07
Iter: 631 loss: 3.41648729e-07
Iter: 632 loss: 3.41303462e-07
Iter: 633 loss: 3.44979895e-07
Iter: 634 loss: 3.41294367e-07
Iter: 635 loss: 3.41120597e-07
Iter: 636 loss: 3.40927556e-07
Iter: 637 loss: 3.40900215e-07
Iter: 638 loss: 3.40791615e-07
Iter: 639 loss: 3.4074327e-07
Iter: 640 loss: 3.40611109e-07
Iter: 641 loss: 3.40318024e-07
Iter: 642 loss: 3.42929695e-07
Iter: 643 loss: 3.40257316e-07
Iter: 644 loss: 3.40121289e-07
Iter: 645 loss: 3.40052623e-07
Iter: 646 loss: 3.39889823e-07
Iter: 647 loss: 3.3945787e-07
Iter: 648 loss: 3.4340917e-07
Iter: 649 loss: 3.39380222e-07
Iter: 650 loss: 3.39003691e-07
Iter: 651 loss: 3.40288352e-07
Iter: 652 loss: 3.38924792e-07
Iter: 653 loss: 3.38745508e-07
Iter: 654 loss: 3.38756536e-07
Iter: 655 loss: 3.38612153e-07
Iter: 656 loss: 3.38493237e-07
Iter: 657 loss: 3.38443783e-07
Iter: 658 loss: 3.38232923e-07
Iter: 659 loss: 3.39615326e-07
Iter: 660 loss: 3.38206519e-07
Iter: 661 loss: 3.3805469e-07
Iter: 662 loss: 3.38085982e-07
Iter: 663 loss: 3.37931738e-07
Iter: 664 loss: 3.3773324e-07
Iter: 665 loss: 3.38803432e-07
Iter: 666 loss: 3.37691745e-07
Iter: 667 loss: 3.37453798e-07
Iter: 668 loss: 3.37759161e-07
Iter: 669 loss: 3.37362735e-07
Iter: 670 loss: 3.37188681e-07
Iter: 671 loss: 3.38315516e-07
Iter: 672 loss: 3.37174527e-07
Iter: 673 loss: 3.36998198e-07
Iter: 674 loss: 3.37439815e-07
Iter: 675 loss: 3.36929787e-07
Iter: 676 loss: 3.36830112e-07
Iter: 677 loss: 3.36663959e-07
Iter: 678 loss: 3.36657934e-07
Iter: 679 loss: 3.36416178e-07
Iter: 680 loss: 3.39550695e-07
Iter: 681 loss: 3.36416264e-07
Iter: 682 loss: 3.36281914e-07
Iter: 683 loss: 3.3589231e-07
Iter: 684 loss: 3.38118383e-07
Iter: 685 loss: 3.35788172e-07
Iter: 686 loss: 3.35751707e-07
Iter: 687 loss: 3.35610366e-07
Iter: 688 loss: 3.35445463e-07
Iter: 689 loss: 3.35485026e-07
Iter: 690 loss: 3.35336182e-07
Iter: 691 loss: 3.35185632e-07
Iter: 692 loss: 3.36171524e-07
Iter: 693 loss: 3.35165424e-07
Iter: 694 loss: 3.35044859e-07
Iter: 695 loss: 3.35327115e-07
Iter: 696 loss: 3.34979859e-07
Iter: 697 loss: 3.34877029e-07
Iter: 698 loss: 3.3486765e-07
Iter: 699 loss: 3.34790116e-07
Iter: 700 loss: 3.34580307e-07
Iter: 701 loss: 3.34996173e-07
Iter: 702 loss: 3.34502687e-07
Iter: 703 loss: 3.34273324e-07
Iter: 704 loss: 3.34319651e-07
Iter: 705 loss: 3.34125048e-07
Iter: 706 loss: 3.3387397e-07
Iter: 707 loss: 3.33874397e-07
Iter: 708 loss: 3.3376341e-07
Iter: 709 loss: 3.33482262e-07
Iter: 710 loss: 3.3583791e-07
Iter: 711 loss: 3.33419791e-07
Iter: 712 loss: 3.33314262e-07
Iter: 713 loss: 3.33281434e-07
Iter: 714 loss: 3.33145e-07
Iter: 715 loss: 3.3343386e-07
Iter: 716 loss: 3.33083193e-07
Iter: 717 loss: 3.33015691e-07
Iter: 718 loss: 3.32891176e-07
Iter: 719 loss: 3.32889613e-07
Iter: 720 loss: 3.32731531e-07
Iter: 721 loss: 3.34974146e-07
Iter: 722 loss: 3.32724085e-07
Iter: 723 loss: 3.32620715e-07
Iter: 724 loss: 3.32353522e-07
Iter: 725 loss: 3.34258317e-07
Iter: 726 loss: 3.32294292e-07
Iter: 727 loss: 3.32058619e-07
Iter: 728 loss: 3.32042589e-07
Iter: 729 loss: 3.31833689e-07
Iter: 730 loss: 3.32456779e-07
Iter: 731 loss: 3.31776107e-07
Iter: 732 loss: 3.31612142e-07
Iter: 733 loss: 3.31767382e-07
Iter: 734 loss: 3.31528554e-07
Iter: 735 loss: 3.31330341e-07
Iter: 736 loss: 3.31196702e-07
Iter: 737 loss: 3.3113966e-07
Iter: 738 loss: 3.3109734e-07
Iter: 739 loss: 3.31055162e-07
Iter: 740 loss: 3.309612e-07
Iter: 741 loss: 3.30801555e-07
Iter: 742 loss: 3.30801072e-07
Iter: 743 loss: 3.30628723e-07
Iter: 744 loss: 3.30782882e-07
Iter: 745 loss: 3.30545106e-07
Iter: 746 loss: 3.30333876e-07
Iter: 747 loss: 3.31735748e-07
Iter: 748 loss: 3.30301134e-07
Iter: 749 loss: 3.30165335e-07
Iter: 750 loss: 3.30138846e-07
Iter: 751 loss: 3.30044941e-07
Iter: 752 loss: 3.29920397e-07
Iter: 753 loss: 3.31892295e-07
Iter: 754 loss: 3.29920908e-07
Iter: 755 loss: 3.29794545e-07
Iter: 756 loss: 3.2965113e-07
Iter: 757 loss: 3.2964482e-07
Iter: 758 loss: 3.29472243e-07
Iter: 759 loss: 3.29796649e-07
Iter: 760 loss: 3.29404088e-07
Iter: 761 loss: 3.29181546e-07
Iter: 762 loss: 3.31166859e-07
Iter: 763 loss: 3.29168358e-07
Iter: 764 loss: 3.29025625e-07
Iter: 765 loss: 3.29162901e-07
Iter: 766 loss: 3.28946271e-07
Iter: 767 loss: 3.28757579e-07
Iter: 768 loss: 3.28683939e-07
Iter: 769 loss: 3.28567751e-07
Iter: 770 loss: 3.28302747e-07
Iter: 771 loss: 3.287482e-07
Iter: 772 loss: 3.28174195e-07
Iter: 773 loss: 3.28128465e-07
Iter: 774 loss: 3.28034275e-07
Iter: 775 loss: 3.27978512e-07
Iter: 776 loss: 3.27855219e-07
Iter: 777 loss: 3.29072265e-07
Iter: 778 loss: 3.27836119e-07
Iter: 779 loss: 3.27727491e-07
Iter: 780 loss: 3.27716407e-07
Iter: 781 loss: 3.27621478e-07
Iter: 782 loss: 3.27481303e-07
Iter: 783 loss: 3.2747019e-07
Iter: 784 loss: 3.2732089e-07
Iter: 785 loss: 3.28550385e-07
Iter: 786 loss: 3.2732521e-07
Iter: 787 loss: 3.27175513e-07
Iter: 788 loss: 3.26926795e-07
Iter: 789 loss: 3.26928273e-07
Iter: 790 loss: 3.26649e-07
Iter: 791 loss: 3.27330383e-07
Iter: 792 loss: 3.26547223e-07
Iter: 793 loss: 3.26381041e-07
Iter: 794 loss: 3.26370298e-07
Iter: 795 loss: 3.26263915e-07
Iter: 796 loss: 3.26305383e-07
Iter: 797 loss: 3.26172653e-07
Iter: 798 loss: 3.26021848e-07
Iter: 799 loss: 3.26187546e-07
Iter: 800 loss: 3.25927317e-07
Iter: 801 loss: 3.25748488e-07
Iter: 802 loss: 3.25521881e-07
Iter: 803 loss: 3.25513042e-07
Iter: 804 loss: 3.25271941e-07
Iter: 805 loss: 3.28998794e-07
Iter: 806 loss: 3.25270889e-07
Iter: 807 loss: 3.25004692e-07
Iter: 808 loss: 3.25669475e-07
Iter: 809 loss: 3.24913429e-07
Iter: 810 loss: 3.24762539e-07
Iter: 811 loss: 3.24719196e-07
Iter: 812 loss: 3.24636886e-07
Iter: 813 loss: 3.24401469e-07
Iter: 814 loss: 3.26533268e-07
Iter: 815 loss: 3.24382682e-07
Iter: 816 loss: 3.2430043e-07
Iter: 817 loss: 3.24334252e-07
Iter: 818 loss: 3.2425055e-07
Iter: 819 loss: 3.24090195e-07
Iter: 820 loss: 3.24191603e-07
Iter: 821 loss: 3.24001604e-07
Iter: 822 loss: 3.23825248e-07
Iter: 823 loss: 3.23625471e-07
Iter: 824 loss: 3.23597845e-07
Iter: 825 loss: 3.23404322e-07
Iter: 826 loss: 3.23393806e-07
Iter: 827 loss: 3.23257183e-07
Iter: 828 loss: 3.23261759e-07
Iter: 829 loss: 3.23152562e-07
Iter: 830 loss: 3.22973648e-07
Iter: 831 loss: 3.23627404e-07
Iter: 832 loss: 3.22927747e-07
Iter: 833 loss: 3.22784445e-07
Iter: 834 loss: 3.22580945e-07
Iter: 835 loss: 3.22574067e-07
Iter: 836 loss: 3.22358176e-07
Iter: 837 loss: 3.24082833e-07
Iter: 838 loss: 3.22343396e-07
Iter: 839 loss: 3.22178607e-07
Iter: 840 loss: 3.24710413e-07
Iter: 841 loss: 3.22185855e-07
Iter: 842 loss: 3.22071401e-07
Iter: 843 loss: 3.21777748e-07
Iter: 844 loss: 3.23652756e-07
Iter: 845 loss: 3.2170766e-07
Iter: 846 loss: 3.21588971e-07
Iter: 847 loss: 3.2150075e-07
Iter: 848 loss: 3.21375524e-07
Iter: 849 loss: 3.21211246e-07
Iter: 850 loss: 3.21210194e-07
Iter: 851 loss: 3.21001352e-07
Iter: 852 loss: 3.23540036e-07
Iter: 853 loss: 3.21005359e-07
Iter: 854 loss: 3.2091657e-07
Iter: 855 loss: 3.20813655e-07
Iter: 856 loss: 3.20803196e-07
Iter: 857 loss: 3.20716708e-07
Iter: 858 loss: 3.20697751e-07
Iter: 859 loss: 3.20630591e-07
Iter: 860 loss: 3.20560673e-07
Iter: 861 loss: 3.20553283e-07
Iter: 862 loss: 3.20402563e-07
Iter: 863 loss: 3.20459975e-07
Iter: 864 loss: 3.20301297e-07
Iter: 865 loss: 3.20082222e-07
Iter: 866 loss: 3.19852802e-07
Iter: 867 loss: 3.19828359e-07
Iter: 868 loss: 3.19539595e-07
Iter: 869 loss: 3.20486834e-07
Iter: 870 loss: 3.19458e-07
Iter: 871 loss: 3.19297499e-07
Iter: 872 loss: 3.2005974e-07
Iter: 873 loss: 3.19259641e-07
Iter: 874 loss: 3.19167327e-07
Iter: 875 loss: 3.18977357e-07
Iter: 876 loss: 3.22773786e-07
Iter: 877 loss: 3.18968375e-07
Iter: 878 loss: 3.18785027e-07
Iter: 879 loss: 3.19414625e-07
Iter: 880 loss: 3.1872537e-07
Iter: 881 loss: 3.18640417e-07
Iter: 882 loss: 3.18602076e-07
Iter: 883 loss: 3.18516129e-07
Iter: 884 loss: 3.18275568e-07
Iter: 885 loss: 3.20100696e-07
Iter: 886 loss: 3.18224977e-07
Iter: 887 loss: 3.18150398e-07
Iter: 888 loss: 3.18098273e-07
Iter: 889 loss: 3.18015253e-07
Iter: 890 loss: 3.17828608e-07
Iter: 891 loss: 3.20069233e-07
Iter: 892 loss: 3.17812066e-07
Iter: 893 loss: 3.17685704e-07
Iter: 894 loss: 3.18585762e-07
Iter: 895 loss: 3.17672345e-07
Iter: 896 loss: 3.1751631e-07
Iter: 897 loss: 3.17866863e-07
Iter: 898 loss: 3.1744753e-07
Iter: 899 loss: 3.17337197e-07
Iter: 900 loss: 3.17107435e-07
Iter: 901 loss: 3.21364e-07
Iter: 902 loss: 3.17117554e-07
Iter: 903 loss: 3.16840612e-07
Iter: 904 loss: 3.19610592e-07
Iter: 905 loss: 3.16846581e-07
Iter: 906 loss: 3.16623129e-07
Iter: 907 loss: 3.18269713e-07
Iter: 908 loss: 3.16604655e-07
Iter: 909 loss: 3.16441628e-07
Iter: 910 loss: 3.16867954e-07
Iter: 911 loss: 3.16402463e-07
Iter: 912 loss: 3.16256632e-07
Iter: 913 loss: 3.16220564e-07
Iter: 914 loss: 3.16138085e-07
Iter: 915 loss: 3.16016042e-07
Iter: 916 loss: 3.16777516e-07
Iter: 917 loss: 3.15995493e-07
Iter: 918 loss: 3.15863019e-07
Iter: 919 loss: 3.16357784e-07
Iter: 920 loss: 3.15834882e-07
Iter: 921 loss: 3.15754278e-07
Iter: 922 loss: 3.15678e-07
Iter: 923 loss: 3.1565088e-07
Iter: 924 loss: 3.15544611e-07
Iter: 925 loss: 3.15537335e-07
Iter: 926 loss: 3.15449313e-07
Iter: 927 loss: 3.15230352e-07
Iter: 928 loss: 3.17780689e-07
Iter: 929 loss: 3.15210798e-07
Iter: 930 loss: 3.14942383e-07
Iter: 931 loss: 3.15771558e-07
Iter: 932 loss: 3.14868885e-07
Iter: 933 loss: 3.14758182e-07
Iter: 934 loss: 3.14690737e-07
Iter: 935 loss: 3.14635599e-07
Iter: 936 loss: 3.14490194e-07
Iter: 937 loss: 3.15384568e-07
Iter: 938 loss: 3.14466092e-07
Iter: 939 loss: 3.14464273e-07
Iter: 940 loss: 3.14406776e-07
Iter: 941 loss: 3.14332368e-07
Iter: 942 loss: 3.14329355e-07
Iter: 943 loss: 3.14273e-07
Iter: 944 loss: 3.14198701e-07
Iter: 945 loss: 3.14188384e-07
Iter: 946 loss: 3.14117983e-07
Iter: 947 loss: 3.13992302e-07
Iter: 948 loss: 3.13918065e-07
Iter: 949 loss: 3.13834875e-07
Iter: 950 loss: 3.13824501e-07
Iter: 951 loss: 3.13749297e-07
Iter: 952 loss: 3.13698507e-07
Iter: 953 loss: 3.13590135e-07
Iter: 954 loss: 3.13597866e-07
Iter: 955 loss: 3.13509361e-07
Iter: 956 loss: 3.14668341e-07
Iter: 957 loss: 3.13507257e-07
Iter: 958 loss: 3.13433e-07
Iter: 959 loss: 3.13534116e-07
Iter: 960 loss: 3.1339772e-07
Iter: 961 loss: 3.13333089e-07
Iter: 962 loss: 3.13197177e-07
Iter: 963 loss: 3.15139374e-07
Iter: 964 loss: 3.13172563e-07
Iter: 965 loss: 3.13163071e-07
Iter: 966 loss: 3.13107762e-07
Iter: 967 loss: 3.13036509e-07
Iter: 968 loss: 3.12918985e-07
Iter: 969 loss: 3.12919042e-07
Iter: 970 loss: 3.12852364e-07
Iter: 971 loss: 3.13879298e-07
Iter: 972 loss: 3.12845117e-07
Iter: 973 loss: 3.12785687e-07
Iter: 974 loss: 3.12807344e-07
Iter: 975 loss: 3.12737427e-07
Iter: 976 loss: 3.12645142e-07
Iter: 977 loss: 3.1266373e-07
Iter: 978 loss: 3.12571871e-07
Iter: 979 loss: 3.12510849e-07
Iter: 980 loss: 3.1241791e-07
Iter: 981 loss: 3.12410805e-07
Iter: 982 loss: 3.12239621e-07
Iter: 983 loss: 3.11900408e-07
Iter: 984 loss: 3.17813488e-07
Iter: 985 loss: 3.11904017e-07
Iter: 986 loss: 3.11619942e-07
Iter: 987 loss: 3.1162341e-07
Iter: 988 loss: 3.11607323e-07
Iter: 989 loss: 3.11556e-07
Iter: 990 loss: 3.11487611e-07
Iter: 991 loss: 3.11401521e-07
Iter: 992 loss: 3.13237933e-07
Iter: 993 loss: 3.11389215e-07
Iter: 994 loss: 3.11309378e-07
Iter: 995 loss: 3.11317649e-07
Iter: 996 loss: 3.11283372e-07
Iter: 997 loss: 3.11148824e-07
Iter: 998 loss: 3.11867751e-07
Iter: 999 loss: 3.11118015e-07
Iter: 1000 loss: 3.10996199e-07
Iter: 1001 loss: 3.1099313e-07
Iter: 1002 loss: 3.10851675e-07
Iter: 1003 loss: 3.10866653e-07
Iter: 1004 loss: 3.10758821e-07
Iter: 1005 loss: 3.10548785e-07
Iter: 1006 loss: 3.1128863e-07
Iter: 1007 loss: 3.10510757e-07
Iter: 1008 loss: 3.103805e-07
Iter: 1009 loss: 3.10763824e-07
Iter: 1010 loss: 3.10340226e-07
Iter: 1011 loss: 3.10214887e-07
Iter: 1012 loss: 3.10707435e-07
Iter: 1013 loss: 3.10180042e-07
Iter: 1014 loss: 3.10114842e-07
Iter: 1015 loss: 3.10022301e-07
Iter: 1016 loss: 3.1001963e-07
Iter: 1017 loss: 3.09911968e-07
Iter: 1018 loss: 3.0984404e-07
Iter: 1019 loss: 3.09802317e-07
Iter: 1020 loss: 3.09712618e-07
Iter: 1021 loss: 3.10389737e-07
Iter: 1022 loss: 3.09695e-07
Iter: 1023 loss: 3.09604161e-07
Iter: 1024 loss: 3.09590547e-07
Iter: 1025 loss: 3.09524552e-07
Iter: 1026 loss: 3.09443863e-07
Iter: 1027 loss: 3.10767888e-07
Iter: 1028 loss: 3.09443521e-07
Iter: 1029 loss: 3.09359933e-07
Iter: 1030 loss: 3.09253863e-07
Iter: 1031 loss: 3.09263413e-07
Iter: 1032 loss: 3.09127643e-07
Iter: 1033 loss: 3.09148902e-07
Iter: 1034 loss: 3.09034363e-07
Iter: 1035 loss: 3.0886855e-07
Iter: 1036 loss: 3.08914593e-07
Iter: 1037 loss: 3.08744802e-07
Iter: 1038 loss: 3.08794483e-07
Iter: 1039 loss: 3.08669684e-07
Iter: 1040 loss: 3.08594622e-07
Iter: 1041 loss: 3.08445578e-07
Iter: 1042 loss: 3.1068754e-07
Iter: 1043 loss: 3.08441116e-07
Iter: 1044 loss: 3.08368499e-07
Iter: 1045 loss: 3.08354799e-07
Iter: 1046 loss: 3.08282438e-07
Iter: 1047 loss: 3.0822423e-07
Iter: 1048 loss: 3.08200242e-07
Iter: 1049 loss: 3.08135043e-07
Iter: 1050 loss: 3.08746451e-07
Iter: 1051 loss: 3.08140301e-07
Iter: 1052 loss: 3.08060862e-07
Iter: 1053 loss: 3.08046083e-07
Iter: 1054 loss: 3.08004559e-07
Iter: 1055 loss: 3.07858073e-07
Iter: 1056 loss: 3.07941662e-07
Iter: 1057 loss: 3.07767579e-07
Iter: 1058 loss: 3.07631694e-07
Iter: 1059 loss: 3.0874611e-07
Iter: 1060 loss: 3.0761629e-07
Iter: 1061 loss: 3.0748194e-07
Iter: 1062 loss: 3.07313371e-07
Iter: 1063 loss: 3.07314878e-07
Iter: 1064 loss: 3.07084576e-07
Iter: 1065 loss: 3.07350859e-07
Iter: 1066 loss: 3.06983054e-07
Iter: 1067 loss: 3.06846857e-07
Iter: 1068 loss: 3.0787848e-07
Iter: 1069 loss: 3.06832277e-07
Iter: 1070 loss: 3.06800871e-07
Iter: 1071 loss: 3.06794647e-07
Iter: 1072 loss: 3.06743345e-07
Iter: 1073 loss: 3.06741811e-07
Iter: 1074 loss: 3.06706028e-07
Iter: 1075 loss: 3.06647507e-07
Iter: 1076 loss: 3.06606324e-07
Iter: 1077 loss: 3.06590692e-07
Iter: 1078 loss: 3.06473112e-07
Iter: 1079 loss: 3.07206392e-07
Iter: 1080 loss: 3.06461573e-07
Iter: 1081 loss: 3.06401546e-07
Iter: 1082 loss: 3.06325148e-07
Iter: 1083 loss: 3.0632043e-07
Iter: 1084 loss: 3.06184972e-07
Iter: 1085 loss: 3.06939569e-07
Iter: 1086 loss: 3.06166157e-07
Iter: 1087 loss: 3.06086235e-07
Iter: 1088 loss: 3.06161041e-07
Iter: 1089 loss: 3.06033826e-07
Iter: 1090 loss: 3.05957684e-07
Iter: 1091 loss: 3.06548088e-07
Iter: 1092 loss: 3.0594336e-07
Iter: 1093 loss: 3.05912408e-07
Iter: 1094 loss: 3.06135433e-07
Iter: 1095 loss: 3.0590769e-07
Iter: 1096 loss: 3.0588393e-07
Iter: 1097 loss: 3.05823846e-07
Iter: 1098 loss: 3.06688946e-07
Iter: 1099 loss: 3.05829644e-07
Iter: 1100 loss: 3.05745857e-07
Iter: 1101 loss: 3.05784397e-07
Iter: 1102 loss: 3.05689525e-07
Iter: 1103 loss: 3.05599855e-07
Iter: 1104 loss: 3.06707705e-07
Iter: 1105 loss: 3.05591357e-07
Iter: 1106 loss: 3.05515016e-07
Iter: 1107 loss: 3.06134041e-07
Iter: 1108 loss: 3.0550774e-07
Iter: 1109 loss: 3.05469342e-07
Iter: 1110 loss: 3.05452318e-07
Iter: 1111 loss: 3.05429637e-07
Iter: 1112 loss: 3.05376744e-07
Iter: 1113 loss: 3.05379956e-07
Iter: 1114 loss: 3.0534531e-07
Iter: 1115 loss: 3.05338631e-07
Iter: 1116 loss: 3.0531254e-07
Iter: 1117 loss: 3.05280224e-07
Iter: 1118 loss: 3.05784965e-07
Iter: 1119 loss: 3.05285198e-07
Iter: 1120 loss: 3.05252712e-07
Iter: 1121 loss: 3.05229719e-07
Iter: 1122 loss: 3.05226592e-07
Iter: 1123 loss: 3.05170886e-07
Iter: 1124 loss: 3.052875e-07
Iter: 1125 loss: 3.05139906e-07
Iter: 1126 loss: 3.05096023e-07
Iter: 1127 loss: 3.05303558e-07
Iter: 1128 loss: 3.05081755e-07
Iter: 1129 loss: 3.05030397e-07
Iter: 1130 loss: 3.04943768e-07
Iter: 1131 loss: 3.04942688e-07
Iter: 1132 loss: 3.04853131e-07
Iter: 1133 loss: 3.05154657e-07
Iter: 1134 loss: 3.04835339e-07
Iter: 1135 loss: 3.04788045e-07
Iter: 1136 loss: 3.05188621e-07
Iter: 1137 loss: 3.04781821e-07
Iter: 1138 loss: 3.04756298e-07
Iter: 1139 loss: 3.04750756e-07
Iter: 1140 loss: 3.04726598e-07
Iter: 1141 loss: 3.04667594e-07
Iter: 1142 loss: 3.05023605e-07
Iter: 1143 loss: 3.04651309e-07
Iter: 1144 loss: 3.04619874e-07
Iter: 1145 loss: 3.0461095e-07
Iter: 1146 loss: 3.04563144e-07
Iter: 1147 loss: 3.04514856e-07
Iter: 1148 loss: 3.04505051e-07
Iter: 1149 loss: 3.04478306e-07
Iter: 1150 loss: 3.04470859e-07
Iter: 1151 loss: 3.04451618e-07
Iter: 1152 loss: 3.04533643e-07
Iter: 1153 loss: 3.04436611e-07
Iter: 1154 loss: 3.04424873e-07
Iter: 1155 loss: 3.04412737e-07
Iter: 1156 loss: 3.04406626e-07
Iter: 1157 loss: 3.0435524e-07
Iter: 1158 loss: 3.04530801e-07
Iter: 1159 loss: 3.0435416e-07
Iter: 1160 loss: 3.04315904e-07
Iter: 1161 loss: 3.04264233e-07
Iter: 1162 loss: 3.04253689e-07
Iter: 1163 loss: 3.04201222e-07
Iter: 1164 loss: 3.04193406e-07
Iter: 1165 loss: 3.04139974e-07
Iter: 1166 loss: 3.0407881e-07
Iter: 1167 loss: 3.04105527e-07
Iter: 1168 loss: 3.04050019e-07
Iter: 1169 loss: 3.04161404e-07
Iter: 1170 loss: 3.04012758e-07
Iter: 1171 loss: 3.04001077e-07
Iter: 1172 loss: 3.03951538e-07
Iter: 1173 loss: 3.04525059e-07
Iter: 1174 loss: 3.03941505e-07
Iter: 1175 loss: 3.0389927e-07
Iter: 1176 loss: 3.03973394e-07
Iter: 1177 loss: 3.03867239e-07
Iter: 1178 loss: 3.03797094e-07
Iter: 1179 loss: 3.04162626e-07
Iter: 1180 loss: 3.03780837e-07
Iter: 1181 loss: 3.03722913e-07
Iter: 1182 loss: 3.03699977e-07
Iter: 1183 loss: 3.03667207e-07
Iter: 1184 loss: 3.03563496e-07
Iter: 1185 loss: 3.03873605e-07
Iter: 1186 loss: 3.03528282e-07
Iter: 1187 loss: 3.03438185e-07
Iter: 1188 loss: 3.038854e-07
Iter: 1189 loss: 3.03422382e-07
Iter: 1190 loss: 3.03377277e-07
Iter: 1191 loss: 3.03426305e-07
Iter: 1192 loss: 3.03366903e-07
Iter: 1193 loss: 3.03297099e-07
Iter: 1194 loss: 3.0349284e-07
Iter: 1195 loss: 3.03269985e-07
Iter: 1196 loss: 3.0323443e-07
Iter: 1197 loss: 3.03169287e-07
Iter: 1198 loss: 3.03163688e-07
Iter: 1199 loss: 3.03057391e-07
Iter: 1200 loss: 3.02952969e-07
Iter: 1201 loss: 3.02939668e-07
Iter: 1202 loss: 3.02767404e-07
Iter: 1203 loss: 3.04131e-07
Iter: 1204 loss: 3.02747765e-07
Iter: 1205 loss: 3.0273975e-07
Iter: 1206 loss: 3.02699135e-07
Iter: 1207 loss: 3.02650676e-07
Iter: 1208 loss: 3.02598863e-07
Iter: 1209 loss: 3.02586e-07
Iter: 1210 loss: 3.02526189e-07
Iter: 1211 loss: 3.02567088e-07
Iter: 1212 loss: 3.02496517e-07
Iter: 1213 loss: 3.02377146e-07
Iter: 1214 loss: 3.0321911e-07
Iter: 1215 loss: 3.02369955e-07
Iter: 1216 loss: 3.02309871e-07
Iter: 1217 loss: 3.02209457e-07
Iter: 1218 loss: 3.04191872e-07
Iter: 1219 loss: 3.02219462e-07
Iter: 1220 loss: 3.02054787e-07
Iter: 1221 loss: 3.03409649e-07
Iter: 1222 loss: 3.02047027e-07
Iter: 1223 loss: 3.01957243e-07
Iter: 1224 loss: 3.02104695e-07
Iter: 1225 loss: 3.01920409e-07
Iter: 1226 loss: 3.0180297e-07
Iter: 1227 loss: 3.02042451e-07
Iter: 1228 loss: 3.01763578e-07
Iter: 1229 loss: 3.01635083e-07
Iter: 1230 loss: 3.0170267e-07
Iter: 1231 loss: 3.01577444e-07
Iter: 1232 loss: 3.01445027e-07
Iter: 1233 loss: 3.02223924e-07
Iter: 1234 loss: 3.01430049e-07
Iter: 1235 loss: 3.01344045e-07
Iter: 1236 loss: 3.01292658e-07
Iter: 1237 loss: 3.01257046e-07
Iter: 1238 loss: 3.01096406e-07
Iter: 1239 loss: 3.01107946e-07
Iter: 1240 loss: 3.00978854e-07
Iter: 1241 loss: 3.00905924e-07
Iter: 1242 loss: 3.00853088e-07
Iter: 1243 loss: 3.00771262e-07
Iter: 1244 loss: 3.00654165e-07
Iter: 1245 loss: 3.00656325e-07
Iter: 1246 loss: 3.00593797e-07
Iter: 1247 loss: 3.00591807e-07
Iter: 1248 loss: 3.0053107e-07
Iter: 1249 loss: 3.00409852e-07
Iter: 1250 loss: 3.03008591e-07
Iter: 1251 loss: 3.00406327e-07
Iter: 1252 loss: 3.0034019e-07
Iter: 1253 loss: 3.01134e-07
Iter: 1254 loss: 3.00335614e-07
Iter: 1255 loss: 3.00261661e-07
Iter: 1256 loss: 3.00318163e-07
Iter: 1257 loss: 3.00207432e-07
Iter: 1258 loss: 3.00130182e-07
Iter: 1259 loss: 3.00176964e-07
Iter: 1260 loss: 3.00092182e-07
Iter: 1261 loss: 2.99950216e-07
Iter: 1262 loss: 3.00183e-07
Iter: 1263 loss: 2.9989053e-07
Iter: 1264 loss: 2.99796369e-07
Iter: 1265 loss: 2.9986046e-07
Iter: 1266 loss: 2.99726111e-07
Iter: 1267 loss: 2.99623139e-07
Iter: 1268 loss: 2.9995644e-07
Iter: 1269 loss: 2.99599662e-07
Iter: 1270 loss: 2.99504507e-07
Iter: 1271 loss: 2.99544951e-07
Iter: 1272 loss: 2.99440103e-07
Iter: 1273 loss: 2.99340741e-07
Iter: 1274 loss: 2.99471822e-07
Iter: 1275 loss: 2.99284238e-07
Iter: 1276 loss: 2.99185444e-07
Iter: 1277 loss: 2.99177088e-07
Iter: 1278 loss: 2.99105068e-07
Iter: 1279 loss: 2.98987089e-07
Iter: 1280 loss: 3.01404498e-07
Iter: 1281 loss: 2.98980808e-07
Iter: 1282 loss: 2.98822e-07
Iter: 1283 loss: 3.00863348e-07
Iter: 1284 loss: 2.98825682e-07
Iter: 1285 loss: 2.9875423e-07
Iter: 1286 loss: 2.98621e-07
Iter: 1287 loss: 3.01297348e-07
Iter: 1288 loss: 2.98623775e-07
Iter: 1289 loss: 2.98629914e-07
Iter: 1290 loss: 2.98565567e-07
Iter: 1291 loss: 2.98523e-07
Iter: 1292 loss: 2.98418911e-07
Iter: 1293 loss: 2.99698883e-07
Iter: 1294 loss: 2.98400892e-07
Iter: 1295 loss: 2.9830727e-07
Iter: 1296 loss: 2.99022219e-07
Iter: 1297 loss: 2.98307157e-07
Iter: 1298 loss: 2.98193868e-07
Iter: 1299 loss: 2.98443581e-07
Iter: 1300 loss: 2.98135774e-07
Iter: 1301 loss: 2.98038685e-07
Iter: 1302 loss: 2.97953108e-07
Iter: 1303 loss: 2.9792227e-07
Iter: 1304 loss: 2.97767485e-07
Iter: 1305 loss: 2.97606022e-07
Iter: 1306 loss: 2.97576634e-07
Iter: 1307 loss: 2.97389079e-07
Iter: 1308 loss: 2.98827217e-07
Iter: 1309 loss: 2.9738132e-07
Iter: 1310 loss: 2.97264535e-07
Iter: 1311 loss: 2.98325375e-07
Iter: 1312 loss: 2.9725706e-07
Iter: 1313 loss: 2.97200472e-07
Iter: 1314 loss: 2.97194674e-07
Iter: 1315 loss: 2.97137262e-07
Iter: 1316 loss: 2.97044608e-07
Iter: 1317 loss: 2.97041879e-07
Iter: 1318 loss: 2.96962e-07
Iter: 1319 loss: 2.96949622e-07
Iter: 1320 loss: 2.96907103e-07
Iter: 1321 loss: 2.9680146e-07
Iter: 1322 loss: 2.98294594e-07
Iter: 1323 loss: 2.96793843e-07
Iter: 1324 loss: 2.96731173e-07
Iter: 1325 loss: 2.96720799e-07
Iter: 1326 loss: 2.96644572e-07
Iter: 1327 loss: 2.96550752e-07
Iter: 1328 loss: 2.96546062e-07
Iter: 1329 loss: 2.9646128e-07
Iter: 1330 loss: 2.96378261e-07
Iter: 1331 loss: 2.963734e-07
Iter: 1332 loss: 2.96296577e-07
Iter: 1333 loss: 2.96265853e-07
Iter: 1334 loss: 2.96186414e-07
Iter: 1335 loss: 2.96387924e-07
Iter: 1336 loss: 2.96141735e-07
Iter: 1337 loss: 2.96068862e-07
Iter: 1338 loss: 2.96522302e-07
Iter: 1339 loss: 2.96062325e-07
Iter: 1340 loss: 2.95993914e-07
Iter: 1341 loss: 2.95903476e-07
Iter: 1342 loss: 2.95904556e-07
Iter: 1343 loss: 2.95837083e-07
Iter: 1344 loss: 2.95825259e-07
Iter: 1345 loss: 2.95774612e-07
Iter: 1346 loss: 2.95734424e-07
Iter: 1347 loss: 2.95707764e-07
Iter: 1348 loss: 2.95628411e-07
Iter: 1349 loss: 2.9650181e-07
Iter: 1350 loss: 2.9562176e-07
Iter: 1351 loss: 2.9555423e-07
Iter: 1352 loss: 2.95556049e-07
Iter: 1353 loss: 2.95492413e-07
Iter: 1354 loss: 2.95432585e-07
Iter: 1355 loss: 2.95685481e-07
Iter: 1356 loss: 2.95415191e-07
Iter: 1357 loss: 2.95324242e-07
Iter: 1358 loss: 2.95550819e-07
Iter: 1359 loss: 2.95297809e-07
Iter: 1360 loss: 2.95232894e-07
Iter: 1361 loss: 2.95122788e-07
Iter: 1362 loss: 2.95126796e-07
Iter: 1363 loss: 2.95023426e-07
Iter: 1364 loss: 2.94959221e-07
Iter: 1365 loss: 2.94923126e-07
Iter: 1366 loss: 2.94840277e-07
Iter: 1367 loss: 2.94833029e-07
Iter: 1368 loss: 2.94742e-07
Iter: 1369 loss: 2.94778175e-07
Iter: 1370 loss: 2.94681797e-07
Iter: 1371 loss: 2.94579706e-07
Iter: 1372 loss: 2.94710077e-07
Iter: 1373 loss: 2.945294e-07
Iter: 1374 loss: 2.94448e-07
Iter: 1375 loss: 2.94645758e-07
Iter: 1376 loss: 2.94416594e-07
Iter: 1377 loss: 2.94293216e-07
Iter: 1378 loss: 2.94558561e-07
Iter: 1379 loss: 2.94264339e-07
Iter: 1380 loss: 2.94172708e-07
Iter: 1381 loss: 2.94875178e-07
Iter: 1382 loss: 2.9417123e-07
Iter: 1383 loss: 2.94080337e-07
Iter: 1384 loss: 2.94143177e-07
Iter: 1385 loss: 2.94021532e-07
Iter: 1386 loss: 2.93931862e-07
Iter: 1387 loss: 2.93952866e-07
Iter: 1388 loss: 2.9386959e-07
Iter: 1389 loss: 2.93734672e-07
Iter: 1390 loss: 2.94774679e-07
Iter: 1391 loss: 2.93719154e-07
Iter: 1392 loss: 2.93624453e-07
Iter: 1393 loss: 2.9340319e-07
Iter: 1394 loss: 2.96129315e-07
Iter: 1395 loss: 2.93375336e-07
Iter: 1396 loss: 2.93159502e-07
Iter: 1397 loss: 2.9350656e-07
Iter: 1398 loss: 2.93040955e-07
Iter: 1399 loss: 2.92830521e-07
Iter: 1400 loss: 2.95121e-07
Iter: 1401 loss: 2.92812615e-07
Iter: 1402 loss: 2.9271763e-07
Iter: 1403 loss: 2.92708364e-07
Iter: 1404 loss: 2.92640436e-07
Iter: 1405 loss: 2.92622815e-07
Iter: 1406 loss: 2.92568956e-07
Iter: 1407 loss: 2.9247559e-07
Iter: 1408 loss: 2.92835921e-07
Iter: 1409 loss: 2.92459134e-07
Iter: 1410 loss: 2.92375574e-07
Iter: 1411 loss: 2.92446146e-07
Iter: 1412 loss: 2.92323364e-07
Iter: 1413 loss: 2.9218765e-07
Iter: 1414 loss: 2.92186684e-07
Iter: 1415 loss: 2.92076237e-07
Iter: 1416 loss: 2.91985373e-07
Iter: 1417 loss: 2.91973947e-07
Iter: 1418 loss: 2.9191159e-07
Iter: 1419 loss: 2.91788098e-07
Iter: 1420 loss: 2.91791139e-07
Iter: 1421 loss: 2.9171693e-07
Iter: 1422 loss: 2.91701554e-07
Iter: 1423 loss: 2.91651645e-07
Iter: 1424 loss: 2.91559701e-07
Iter: 1425 loss: 2.91555125e-07
Iter: 1426 loss: 2.91469576e-07
Iter: 1427 loss: 2.91394144e-07
Iter: 1428 loss: 2.91377944e-07
Iter: 1429 loss: 2.91216622e-07
Iter: 1430 loss: 2.91257606e-07
Iter: 1431 loss: 2.91080227e-07
Iter: 1432 loss: 2.90883804e-07
Iter: 1433 loss: 2.91860431e-07
Iter: 1434 loss: 2.90852938e-07
Iter: 1435 loss: 2.90786971e-07
Iter: 1436 loss: 2.907843e-07
Iter: 1437 loss: 2.90719413e-07
Iter: 1438 loss: 2.91080937e-07
Iter: 1439 loss: 2.90710119e-07
Iter: 1440 loss: 2.90657738e-07
Iter: 1441 loss: 2.90615731e-07
Iter: 1442 loss: 2.90619283e-07
Iter: 1443 loss: 2.90547064e-07
Iter: 1444 loss: 2.9117021e-07
Iter: 1445 loss: 2.90535183e-07
Iter: 1446 loss: 2.90481978e-07
Iter: 1447 loss: 2.90682067e-07
Iter: 1448 loss: 2.90462481e-07
Iter: 1449 loss: 2.90416409e-07
Iter: 1450 loss: 2.90289904e-07
Iter: 1451 loss: 2.92380776e-07
Iter: 1452 loss: 2.90280923e-07
Iter: 1453 loss: 2.90283964e-07
Iter: 1454 loss: 2.90242554e-07
Iter: 1455 loss: 2.90193185e-07
Iter: 1456 loss: 2.90154418e-07
Iter: 1457 loss: 2.90152144e-07
Iter: 1458 loss: 2.90094022e-07
Iter: 1459 loss: 2.90075775e-07
Iter: 1460 loss: 2.90038315e-07
Iter: 1461 loss: 2.89968256e-07
Iter: 1462 loss: 2.89967318e-07
Iter: 1463 loss: 2.89934974e-07
Iter: 1464 loss: 2.89855507e-07
Iter: 1465 loss: 2.90503635e-07
Iter: 1466 loss: 2.89842802e-07
Iter: 1467 loss: 2.89754894e-07
Iter: 1468 loss: 2.8985886e-07
Iter: 1469 loss: 2.89728916e-07
Iter: 1470 loss: 2.89667099e-07
Iter: 1471 loss: 2.9028854e-07
Iter: 1472 loss: 2.89670169e-07
Iter: 1473 loss: 2.89629327e-07
Iter: 1474 loss: 2.89607726e-07
Iter: 1475 loss: 2.89574558e-07
Iter: 1476 loss: 2.89508733e-07
Iter: 1477 loss: 2.89654821e-07
Iter: 1478 loss: 2.89481704e-07
Iter: 1479 loss: 2.89424577e-07
Iter: 1480 loss: 2.89628588e-07
Iter: 1481 loss: 2.89398201e-07
Iter: 1482 loss: 2.89349174e-07
Iter: 1483 loss: 2.89490373e-07
Iter: 1484 loss: 2.89330615e-07
Iter: 1485 loss: 2.89270133e-07
Iter: 1486 loss: 2.89326067e-07
Iter: 1487 loss: 2.89228183e-07
Iter: 1488 loss: 2.89142179e-07
Iter: 1489 loss: 2.89312538e-07
Iter: 1490 loss: 2.89099091e-07
Iter: 1491 loss: 2.89025905e-07
Iter: 1492 loss: 2.88948115e-07
Iter: 1493 loss: 2.88929158e-07
Iter: 1494 loss: 2.88874162e-07
Iter: 1495 loss: 2.88859837e-07
Iter: 1496 loss: 2.88779859e-07
Iter: 1497 loss: 2.88734384e-07
Iter: 1498 loss: 2.88721878e-07
Iter: 1499 loss: 2.88665774e-07
Iter: 1500 loss: 2.88763886e-07
Iter: 1501 loss: 2.88637551e-07
Iter: 1502 loss: 2.8855797e-07
Iter: 1503 loss: 2.88917136e-07
Iter: 1504 loss: 2.88542253e-07
Iter: 1505 loss: 2.88450821e-07
Iter: 1506 loss: 2.88468357e-07
Iter: 1507 loss: 2.88384285e-07
Iter: 1508 loss: 2.88297542e-07
Iter: 1509 loss: 2.88930522e-07
Iter: 1510 loss: 2.88285179e-07
Iter: 1511 loss: 2.88183742e-07
Iter: 1512 loss: 2.88273725e-07
Iter: 1513 loss: 2.88145316e-07
Iter: 1514 loss: 2.88051581e-07
Iter: 1515 loss: 2.88625898e-07
Iter: 1516 loss: 2.88042088e-07
Iter: 1517 loss: 2.87977059e-07
Iter: 1518 loss: 2.87987064e-07
Iter: 1519 loss: 2.87932892e-07
Iter: 1520 loss: 2.87877981e-07
Iter: 1521 loss: 2.87872979e-07
Iter: 1522 loss: 2.87832819e-07
Iter: 1523 loss: 2.87749145e-07
Iter: 1524 loss: 2.89135414e-07
Iter: 1525 loss: 2.87747866e-07
Iter: 1526 loss: 2.87658708e-07
Iter: 1527 loss: 2.87661e-07
Iter: 1528 loss: 2.87613034e-07
Iter: 1529 loss: 2.87542548e-07
Iter: 1530 loss: 2.87534505e-07
Iter: 1531 loss: 2.87435341e-07
Iter: 1532 loss: 2.87506282e-07
Iter: 1533 loss: 2.87360706e-07
Iter: 1534 loss: 2.87305028e-07
Iter: 1535 loss: 2.87268506e-07
Iter: 1536 loss: 2.87195718e-07
Iter: 1537 loss: 2.8720379e-07
Iter: 1538 loss: 2.87142257e-07
Iter: 1539 loss: 2.87056e-07
Iter: 1540 loss: 2.87365e-07
Iter: 1541 loss: 2.87039455e-07
Iter: 1542 loss: 2.86959789e-07
Iter: 1543 loss: 2.87059294e-07
Iter: 1544 loss: 2.86928298e-07
Iter: 1545 loss: 2.86846358e-07
Iter: 1546 loss: 2.87380971e-07
Iter: 1547 loss: 2.86829959e-07
Iter: 1548 loss: 2.86781585e-07
Iter: 1549 loss: 2.86651243e-07
Iter: 1550 loss: 2.89369069e-07
Iter: 1551 loss: 2.86648685e-07
Iter: 1552 loss: 2.86565694e-07
Iter: 1553 loss: 2.86564187e-07
Iter: 1554 loss: 2.86467127e-07
Iter: 1555 loss: 2.86320898e-07
Iter: 1556 loss: 2.86329225e-07
Iter: 1557 loss: 2.86223838e-07
Iter: 1558 loss: 2.8621605e-07
Iter: 1559 loss: 2.86094973e-07
Iter: 1560 loss: 2.8609125e-07
Iter: 1561 loss: 2.85997658e-07
Iter: 1562 loss: 2.85881839e-07
Iter: 1563 loss: 2.85829515e-07
Iter: 1564 loss: 2.85759597e-07
Iter: 1565 loss: 2.8570841e-07
Iter: 1566 loss: 2.85702498e-07
Iter: 1567 loss: 2.85631273e-07
Iter: 1568 loss: 2.8573217e-07
Iter: 1569 loss: 2.85595632e-07
Iter: 1570 loss: 2.85531e-07
Iter: 1571 loss: 2.85591909e-07
Iter: 1572 loss: 2.85478905e-07
Iter: 1573 loss: 2.85401882e-07
Iter: 1574 loss: 2.85536487e-07
Iter: 1575 loss: 2.8536e-07
Iter: 1576 loss: 2.85254231e-07
Iter: 1577 loss: 2.85721057e-07
Iter: 1578 loss: 2.8523371e-07
Iter: 1579 loss: 2.85155323e-07
Iter: 1580 loss: 2.85050731e-07
Iter: 1581 loss: 2.85045076e-07
Iter: 1582 loss: 2.84950659e-07
Iter: 1583 loss: 2.84940882e-07
Iter: 1584 loss: 2.84851296e-07
Iter: 1585 loss: 2.84927467e-07
Iter: 1586 loss: 2.84801473e-07
Iter: 1587 loss: 2.84749348e-07
Iter: 1588 loss: 2.84806248e-07
Iter: 1589 loss: 2.84707795e-07
Iter: 1590 loss: 2.8461119e-07
Iter: 1591 loss: 2.85419844e-07
Iter: 1592 loss: 2.84604454e-07
Iter: 1593 loss: 2.84542324e-07
Iter: 1594 loss: 2.8442696e-07
Iter: 1595 loss: 2.85780601e-07
Iter: 1596 loss: 2.84404052e-07
Iter: 1597 loss: 2.84298096e-07
Iter: 1598 loss: 2.85513408e-07
Iter: 1599 loss: 2.84302416e-07
Iter: 1600 loss: 2.84199444e-07
Iter: 1601 loss: 2.85054199e-07
Iter: 1602 loss: 2.8420402e-07
Iter: 1603 loss: 2.84149166e-07
Iter: 1604 loss: 2.8412029e-07
Iter: 1605 loss: 2.84093062e-07
Iter: 1606 loss: 2.83973208e-07
Iter: 1607 loss: 2.84128816e-07
Iter: 1608 loss: 2.83925203e-07
Iter: 1609 loss: 2.83811715e-07
Iter: 1610 loss: 2.84427671e-07
Iter: 1611 loss: 2.83792161e-07
Iter: 1612 loss: 2.83689587e-07
Iter: 1613 loss: 2.83675661e-07
Iter: 1614 loss: 2.83593721e-07
Iter: 1615 loss: 2.83499503e-07
Iter: 1616 loss: 2.84131147e-07
Iter: 1617 loss: 2.83480915e-07
Iter: 1618 loss: 2.83353984e-07
Iter: 1619 loss: 2.83677906e-07
Iter: 1620 loss: 2.83319224e-07
Iter: 1621 loss: 2.83218526e-07
Iter: 1622 loss: 2.83124223e-07
Iter: 1623 loss: 2.83095403e-07
Iter: 1624 loss: 2.83037821e-07
Iter: 1625 loss: 2.83006699e-07
Iter: 1626 loss: 2.82960173e-07
Iter: 1627 loss: 2.82820082e-07
Iter: 1628 loss: 2.84030762e-07
Iter: 1629 loss: 2.82795071e-07
Iter: 1630 loss: 2.82658561e-07
Iter: 1631 loss: 2.82972962e-07
Iter: 1632 loss: 2.82584779e-07
Iter: 1633 loss: 2.82498092e-07
Iter: 1634 loss: 2.82480585e-07
Iter: 1635 loss: 2.82409786e-07
Iter: 1636 loss: 2.82292831e-07
Iter: 1637 loss: 2.82292035e-07
Iter: 1638 loss: 2.82114456e-07
Iter: 1639 loss: 2.82622182e-07
Iter: 1640 loss: 2.82048973e-07
Iter: 1641 loss: 2.81897485e-07
Iter: 1642 loss: 2.83019972e-07
Iter: 1643 loss: 2.81886031e-07
Iter: 1644 loss: 2.81774817e-07
Iter: 1645 loss: 2.81769701e-07
Iter: 1646 loss: 2.81665649e-07
Iter: 1647 loss: 2.81528798e-07
Iter: 1648 loss: 2.81695804e-07
Iter: 1649 loss: 2.8144882e-07
Iter: 1650 loss: 2.813631e-07
Iter: 1651 loss: 2.81356165e-07
Iter: 1652 loss: 2.81293694e-07
Iter: 1653 loss: 2.81208827e-07
Iter: 1654 loss: 2.82996098e-07
Iter: 1655 loss: 2.81210021e-07
Iter: 1656 loss: 2.81166592e-07
Iter: 1657 loss: 2.81154513e-07
Iter: 1658 loss: 2.81095311e-07
Iter: 1659 loss: 2.8100925e-07
Iter: 1660 loss: 2.81018885e-07
Iter: 1661 loss: 2.8094135e-07
Iter: 1662 loss: 2.80858188e-07
Iter: 1663 loss: 2.80853328e-07
Iter: 1664 loss: 2.80809587e-07
Iter: 1665 loss: 2.80778181e-07
Iter: 1666 loss: 2.80719519e-07
Iter: 1667 loss: 2.80786196e-07
Iter: 1668 loss: 2.80681888e-07
Iter: 1669 loss: 2.80632776e-07
Iter: 1670 loss: 2.80734696e-07
Iter: 1671 loss: 2.8061163e-07
Iter: 1672 loss: 2.80543418e-07
Iter: 1673 loss: 2.8061055e-07
Iter: 1674 loss: 2.80505247e-07
Iter: 1675 loss: 2.80420437e-07
Iter: 1676 loss: 2.80868647e-07
Iter: 1677 loss: 2.80403071e-07
Iter: 1678 loss: 2.80340203e-07
Iter: 1679 loss: 2.80273383e-07
Iter: 1680 loss: 2.80264516e-07
Iter: 1681 loss: 2.80195934e-07
Iter: 1682 loss: 2.80202698e-07
Iter: 1683 loss: 2.80136192e-07
Iter: 1684 loss: 2.80052404e-07
Iter: 1685 loss: 2.80040979e-07
Iter: 1686 loss: 2.79998176e-07
Iter: 1687 loss: 2.80000137e-07
Iter: 1688 loss: 2.79939229e-07
Iter: 1689 loss: 2.79947301e-07
Iter: 1690 loss: 2.79890799e-07
Iter: 1691 loss: 2.79838304e-07
Iter: 1692 loss: 2.79777026e-07
Iter: 1693 loss: 2.79765374e-07
Iter: 1694 loss: 2.79692756e-07
Iter: 1695 loss: 2.80710879e-07
Iter: 1696 loss: 2.79697929e-07
Iter: 1697 loss: 2.79634548e-07
Iter: 1698 loss: 2.79653705e-07
Iter: 1699 loss: 2.7959311e-07
Iter: 1700 loss: 2.7948181e-07
Iter: 1701 loss: 2.79660867e-07
Iter: 1702 loss: 2.79426558e-07
Iter: 1703 loss: 2.79313326e-07
Iter: 1704 loss: 2.79839554e-07
Iter: 1705 loss: 2.79285928e-07
Iter: 1706 loss: 2.7920882e-07
Iter: 1707 loss: 2.79544054e-07
Iter: 1708 loss: 2.79162975e-07
Iter: 1709 loss: 2.79075152e-07
Iter: 1710 loss: 2.79040393e-07
Iter: 1711 loss: 2.78992673e-07
Iter: 1712 loss: 2.78923864e-07
Iter: 1713 loss: 2.79552324e-07
Iter: 1714 loss: 2.78914627e-07
Iter: 1715 loss: 2.78845562e-07
Iter: 1716 loss: 2.79297979e-07
Iter: 1717 loss: 2.78835e-07
Iter: 1718 loss: 2.78786217e-07
Iter: 1719 loss: 2.78778089e-07
Iter: 1720 loss: 2.7876473e-07
Iter: 1721 loss: 2.78681114e-07
Iter: 1722 loss: 2.79318698e-07
Iter: 1723 loss: 2.78679124e-07
Iter: 1724 loss: 2.7863473e-07
Iter: 1725 loss: 2.78525249e-07
Iter: 1726 loss: 2.79789731e-07
Iter: 1727 loss: 2.78510328e-07
Iter: 1728 loss: 2.78408663e-07
Iter: 1729 loss: 2.79284677e-07
Iter: 1730 loss: 2.78408493e-07
Iter: 1731 loss: 2.78315781e-07
Iter: 1732 loss: 2.79290418e-07
Iter: 1733 loss: 2.78319845e-07
Iter: 1734 loss: 2.78258199e-07
Iter: 1735 loss: 2.78226679e-07
Iter: 1736 loss: 2.78210507e-07
Iter: 1737 loss: 2.78139936e-07
Iter: 1738 loss: 2.78521725e-07
Iter: 1739 loss: 2.78132e-07
Iter: 1740 loss: 2.7807215e-07
Iter: 1741 loss: 2.78310324e-07
Iter: 1742 loss: 2.78049555e-07
Iter: 1743 loss: 2.77985976e-07
Iter: 1744 loss: 2.77932742e-07
Iter: 1745 loss: 2.77910431e-07
Iter: 1746 loss: 2.7782059e-07
Iter: 1747 loss: 2.7792845e-07
Iter: 1748 loss: 2.77779037e-07
Iter: 1749 loss: 2.77705482e-07
Iter: 1750 loss: 2.77687917e-07
Iter: 1751 loss: 2.7765077e-07
Iter: 1752 loss: 2.77546349e-07
Iter: 1753 loss: 2.79395522e-07
Iter: 1754 loss: 2.77538589e-07
Iter: 1755 loss: 2.77535861e-07
Iter: 1756 loss: 2.77506217e-07
Iter: 1757 loss: 2.77467677e-07
Iter: 1758 loss: 2.77389859e-07
Iter: 1759 loss: 2.7842907e-07
Iter: 1760 loss: 2.77386789e-07
Iter: 1761 loss: 2.77317611e-07
Iter: 1762 loss: 2.77392814e-07
Iter: 1763 loss: 2.77264434e-07
Iter: 1764 loss: 2.77180789e-07
Iter: 1765 loss: 2.77944395e-07
Iter: 1766 loss: 2.77178202e-07
Iter: 1767 loss: 2.77088787e-07
Iter: 1768 loss: 2.77076708e-07
Iter: 1769 loss: 2.77010315e-07
Iter: 1770 loss: 2.7690794e-07
Iter: 1771 loss: 2.77465e-07
Iter: 1772 loss: 2.76892962e-07
Iter: 1773 loss: 2.76826313e-07
Iter: 1774 loss: 2.77219101e-07
Iter: 1775 loss: 2.76811875e-07
Iter: 1776 loss: 2.76781549e-07
Iter: 1777 loss: 2.76825176e-07
Iter: 1778 loss: 2.76737779e-07
Iter: 1779 loss: 2.76697193e-07
Iter: 1780 loss: 2.76667521e-07
Iter: 1781 loss: 2.76648933e-07
Iter: 1782 loss: 2.76606443e-07
Iter: 1783 loss: 2.76597746e-07
Iter: 1784 loss: 2.76548917e-07
Iter: 1785 loss: 2.76461037e-07
Iter: 1786 loss: 2.77918531e-07
Iter: 1787 loss: 2.76451829e-07
Iter: 1788 loss: 2.76366393e-07
Iter: 1789 loss: 2.77139065e-07
Iter: 1790 loss: 2.76354655e-07
Iter: 1791 loss: 2.76234147e-07
Iter: 1792 loss: 2.76364318e-07
Iter: 1793 loss: 2.76184323e-07
Iter: 1794 loss: 2.76108949e-07
Iter: 1795 loss: 2.76111336e-07
Iter: 1796 loss: 2.76057619e-07
Iter: 1797 loss: 2.76021666e-07
Iter: 1798 loss: 2.76004499e-07
Iter: 1799 loss: 2.75970592e-07
Iter: 1800 loss: 2.75973093e-07
Iter: 1801 loss: 2.75947855e-07
Iter: 1802 loss: 2.75883792e-07
Iter: 1803 loss: 2.75993045e-07
Iter: 1804 loss: 2.75881149e-07
Iter: 1805 loss: 2.75815239e-07
Iter: 1806 loss: 2.76200808e-07
Iter: 1807 loss: 2.75799493e-07
Iter: 1808 loss: 2.75761067e-07
Iter: 1809 loss: 2.75785283e-07
Iter: 1810 loss: 2.75729235e-07
Iter: 1811 loss: 2.75658607e-07
Iter: 1812 loss: 2.75562627e-07
Iter: 1813 loss: 2.75555209e-07
Iter: 1814 loss: 2.7546912e-07
Iter: 1815 loss: 2.75477475e-07
Iter: 1816 loss: 2.7537186e-07
Iter: 1817 loss: 2.75479834e-07
Iter: 1818 loss: 2.75317205e-07
Iter: 1819 loss: 2.75261527e-07
Iter: 1820 loss: 2.75360151e-07
Iter: 1821 loss: 2.75226085e-07
Iter: 1822 loss: 2.75153582e-07
Iter: 1823 loss: 2.75649029e-07
Iter: 1824 loss: 2.75137523e-07
Iter: 1825 loss: 2.75092702e-07
Iter: 1826 loss: 2.74986576e-07
Iter: 1827 loss: 2.76194896e-07
Iter: 1828 loss: 2.74965e-07
Iter: 1829 loss: 2.74910406e-07
Iter: 1830 loss: 2.74885451e-07
Iter: 1831 loss: 2.74819399e-07
Iter: 1832 loss: 2.74807263e-07
Iter: 1833 loss: 2.74760055e-07
Iter: 1834 loss: 2.74659925e-07
Iter: 1835 loss: 2.7484856e-07
Iter: 1836 loss: 2.7462761e-07
Iter: 1837 loss: 2.74563376e-07
Iter: 1838 loss: 2.74915067e-07
Iter: 1839 loss: 2.74554736e-07
Iter: 1840 loss: 2.74499712e-07
Iter: 1841 loss: 2.74723163e-07
Iter: 1842 loss: 2.7448462e-07
Iter: 1843 loss: 2.74424337e-07
Iter: 1844 loss: 2.7435803e-07
Iter: 1845 loss: 2.74356182e-07
Iter: 1846 loss: 2.74253239e-07
Iter: 1847 loss: 2.74462622e-07
Iter: 1848 loss: 2.74218053e-07
Iter: 1849 loss: 2.74089246e-07
Iter: 1850 loss: 2.75031255e-07
Iter: 1851 loss: 2.74088706e-07
Iter: 1852 loss: 2.74007959e-07
Iter: 1853 loss: 2.73923661e-07
Iter: 1854 loss: 2.73905073e-07
Iter: 1855 loss: 2.73829926e-07
Iter: 1856 loss: 2.73816909e-07
Iter: 1857 loss: 2.73773594e-07
Iter: 1858 loss: 2.73654564e-07
Iter: 1859 loss: 2.75036712e-07
Iter: 1860 loss: 2.73647458e-07
Iter: 1861 loss: 2.73576802e-07
Iter: 1862 loss: 2.74507101e-07
Iter: 1863 loss: 2.73578109e-07
Iter: 1864 loss: 2.73520811e-07
Iter: 1865 loss: 2.73813697e-07
Iter: 1866 loss: 2.73503161e-07
Iter: 1867 loss: 2.73452883e-07
Iter: 1868 loss: 2.73528883e-07
Iter: 1869 loss: 2.73432761e-07
Iter: 1870 loss: 2.73385439e-07
Iter: 1871 loss: 2.73430686e-07
Iter: 1872 loss: 2.73347553e-07
Iter: 1873 loss: 2.73278033e-07
Iter: 1874 loss: 2.73729654e-07
Iter: 1875 loss: 2.73259928e-07
Iter: 1876 loss: 2.73197884e-07
Iter: 1877 loss: 2.73152381e-07
Iter: 1878 loss: 2.7310935e-07
Iter: 1879 loss: 2.73036e-07
Iter: 1880 loss: 2.73188419e-07
Iter: 1881 loss: 2.73013768e-07
Iter: 1882 loss: 2.7296727e-07
Iter: 1883 loss: 2.72959483e-07
Iter: 1884 loss: 2.72921653e-07
Iter: 1885 loss: 2.72854777e-07
Iter: 1886 loss: 2.72856056e-07
Iter: 1887 loss: 2.72822717e-07
Iter: 1888 loss: 2.7281618e-07
Iter: 1889 loss: 2.72779261e-07
Iter: 1890 loss: 2.72710736e-07
Iter: 1891 loss: 2.74134834e-07
Iter: 1892 loss: 2.72709173e-07
Iter: 1893 loss: 2.7260711e-07
Iter: 1894 loss: 2.72574823e-07
Iter: 1895 loss: 2.72521447e-07
Iter: 1896 loss: 2.72550494e-07
Iter: 1897 loss: 2.7245045e-07
Iter: 1898 loss: 2.72408386e-07
Iter: 1899 loss: 2.72369931e-07
Iter: 1900 loss: 2.7237482e-07
Iter: 1901 loss: 2.7228063e-07
Iter: 1902 loss: 2.72463126e-07
Iter: 1903 loss: 2.72249537e-07
Iter: 1904 loss: 2.721915e-07
Iter: 1905 loss: 2.73034289e-07
Iter: 1906 loss: 2.72183115e-07
Iter: 1907 loss: 2.72135935e-07
Iter: 1908 loss: 2.72066728e-07
Iter: 1909 loss: 2.72063687e-07
Iter: 1910 loss: 2.71970862e-07
Iter: 1911 loss: 2.71897079e-07
Iter: 1912 loss: 2.7186644e-07
Iter: 1913 loss: 2.71784074e-07
Iter: 1914 loss: 2.71776344e-07
Iter: 1915 loss: 2.71687099e-07
Iter: 1916 loss: 2.71863769e-07
Iter: 1917 loss: 2.71663367e-07
Iter: 1918 loss: 2.71614311e-07
Iter: 1919 loss: 2.7193488e-07
Iter: 1920 loss: 2.71611611e-07
Iter: 1921 loss: 2.71559884e-07
Iter: 1922 loss: 2.71651345e-07
Iter: 1923 loss: 2.71559827e-07
Iter: 1924 loss: 2.71522936e-07
Iter: 1925 loss: 2.71455662e-07
Iter: 1926 loss: 2.72337388e-07
Iter: 1927 loss: 2.71454326e-07
Iter: 1928 loss: 2.71381623e-07
Iter: 1929 loss: 2.72213356e-07
Iter: 1930 loss: 2.71375228e-07
Iter: 1931 loss: 2.71286979e-07
Iter: 1932 loss: 2.71375143e-07
Iter: 1933 loss: 2.71231414e-07
Iter: 1934 loss: 2.71147627e-07
Iter: 1935 loss: 2.71197109e-07
Iter: 1936 loss: 2.71104057e-07
Iter: 1937 loss: 2.71002875e-07
Iter: 1938 loss: 2.71482975e-07
Iter: 1939 loss: 2.70990569e-07
Iter: 1940 loss: 2.70877024e-07
Iter: 1941 loss: 2.71400978e-07
Iter: 1942 loss: 2.7086287e-07
Iter: 1943 loss: 2.70797187e-07
Iter: 1944 loss: 2.70746739e-07
Iter: 1945 loss: 2.70741793e-07
Iter: 1946 loss: 2.70663605e-07
Iter: 1947 loss: 2.70789059e-07
Iter: 1948 loss: 2.70619211e-07
Iter: 1949 loss: 2.70516296e-07
Iter: 1950 loss: 2.71860642e-07
Iter: 1951 loss: 2.7051783e-07
Iter: 1952 loss: 2.70439358e-07
Iter: 1953 loss: 2.70302849e-07
Iter: 1954 loss: 2.73463627e-07
Iter: 1955 loss: 2.70303985e-07
Iter: 1956 loss: 2.70157727e-07
Iter: 1957 loss: 2.70156534e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0.4
+ date
Wed Nov  4 11:36:17 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0/300_300_300_1 --function f2 --psi 1 --alpha 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff45008b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff45008b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4481f6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4481d3048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4481d3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4481d87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4481d8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4480e1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4480e1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff448130620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff448130a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4481308c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff448127598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff430152730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4301297b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff448042c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4480427b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4300b6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff4300f0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff43011c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff43011c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff430054f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3f460f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3f45d8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3f45d89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3f459b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3f45a2158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3f45a8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff430095950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff430096620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3f4540048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3f4547488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff44805c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3f4547158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3f44dfb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3f44d7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.025773335
test_loss: 0.026013643
train_loss: 0.015127495
test_loss: 0.015557684
train_loss: 0.010726957
test_loss: 0.01121156
train_loss: 0.008742104
test_loss: 0.0095005
train_loss: 0.007653534
test_loss: 0.008058624
train_loss: 0.0065572252
test_loss: 0.0072577875
train_loss: 0.0064641396
test_loss: 0.006939993
train_loss: 0.005601907
test_loss: 0.0063272147
train_loss: 0.005698756
test_loss: 0.0061849575
train_loss: 0.005303493
test_loss: 0.0058440175
train_loss: 0.0051754154
test_loss: 0.005570289
train_loss: 0.0049099755
test_loss: 0.0054454007
train_loss: 0.004938719
test_loss: 0.005520928
train_loss: 0.004643228
test_loss: 0.005078427
train_loss: 0.004666035
test_loss: 0.0050827456
train_loss: 0.004144752
test_loss: 0.00503039
train_loss: 0.004298977
test_loss: 0.0049127582
train_loss: 0.004315051
test_loss: 0.0052327598
train_loss: 0.004334326
test_loss: 0.0047995136
train_loss: 0.004371166
test_loss: 0.0047327727
train_loss: 0.0041101095
test_loss: 0.0046025096
train_loss: 0.0038693973
test_loss: 0.0044975467
train_loss: 0.003707436
test_loss: 0.0043532904
train_loss: 0.0038826296
test_loss: 0.0045114253
train_loss: 0.004234669
test_loss: 0.0043781945
train_loss: 0.0038162866
test_loss: 0.004441482
train_loss: 0.003715068
test_loss: 0.0046454915
train_loss: 0.0036962023
test_loss: 0.0042548007
train_loss: 0.0034893802
test_loss: 0.004207912
train_loss: 0.003620856
test_loss: 0.0042019705
train_loss: 0.0033978964
test_loss: 0.0044219494
train_loss: 0.003512472
test_loss: 0.004065541
train_loss: 0.0034584121
test_loss: 0.004122336
train_loss: 0.0033304533
test_loss: 0.0041046445
train_loss: 0.0038071456
test_loss: 0.0044535184
train_loss: 0.003446241
test_loss: 0.0040921513
train_loss: 0.0031374164
test_loss: 0.0039255326
train_loss: 0.003387513
test_loss: 0.0039570103
train_loss: 0.003285314
test_loss: 0.004117058
train_loss: 0.003258978
test_loss: 0.0039427807
train_loss: 0.003087485
test_loss: 0.0038108747
train_loss: 0.0031602301
test_loss: 0.003882994
train_loss: 0.0030543064
test_loss: 0.004070745
train_loss: 0.0035329033
test_loss: 0.0039869226
train_loss: 0.0034506163
test_loss: 0.004226947
train_loss: 0.0030938971
test_loss: 0.003936098
train_loss: 0.00306689
test_loss: 0.003994033
train_loss: 0.0033583348
test_loss: 0.0038916527
train_loss: 0.0030016627
test_loss: 0.0036890793
train_loss: 0.0031837209
test_loss: 0.0037181159
train_loss: 0.002955631
test_loss: 0.003624538
train_loss: 0.0031837483
test_loss: 0.0037713589
train_loss: 0.0030645253
test_loss: 0.003622522
train_loss: 0.003201733
test_loss: 0.003928981
train_loss: 0.0030740064
test_loss: 0.0034956287
train_loss: 0.0034456172
test_loss: 0.0038827928
train_loss: 0.0029897215
test_loss: 0.0037376762
train_loss: 0.0029117663
test_loss: 0.0037461394
train_loss: 0.0029657572
test_loss: 0.0037603998
train_loss: 0.0028533996
test_loss: 0.0036469388
train_loss: 0.003046046
test_loss: 0.0035148526
train_loss: 0.0032557936
test_loss: 0.0038237171
train_loss: 0.0028410456
test_loss: 0.003579061
train_loss: 0.0026424183
test_loss: 0.0034538282
train_loss: 0.00307374
test_loss: 0.0035628492
train_loss: 0.002949621
test_loss: 0.0036852728
train_loss: 0.0028802825
test_loss: 0.0037306051
train_loss: 0.0028183195
test_loss: 0.0034969833
train_loss: 0.0028940756
test_loss: 0.0035203898
train_loss: 0.0030784525
test_loss: 0.0036885
train_loss: 0.0026650608
test_loss: 0.0034336783
train_loss: 0.0027911186
test_loss: 0.0036415376
train_loss: 0.002717569
test_loss: 0.003324669
train_loss: 0.002657298
test_loss: 0.0034146472
train_loss: 0.002706448
test_loss: 0.0033883573
train_loss: 0.002783869
test_loss: 0.0033878724
train_loss: 0.0027690805
test_loss: 0.0034019123
train_loss: 0.0026851129
test_loss: 0.0034240137
train_loss: 0.0025970656
test_loss: 0.003368095
train_loss: 0.0029407062
test_loss: 0.0034048622
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.4/300_300_300_1 --optimizer lbfgs --function f2 --psi 1 --alpha 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21910d7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21910c0e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2190fb2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2190f581e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2190f58bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2190f71620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2190f716a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2190f1b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2190ee4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2190ebb048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2190ebb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2190e9a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2190e60e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21870946a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f218709f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f218704a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f218704f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2187008d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21870237b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2187008c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2187008a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2186f88510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2186f88e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2186efb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2186f50840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2186eb38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2186ec72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2186ec7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2186eb3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2186e576a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2186e57ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2186e577b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f212a985d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f212a9bee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f212a97b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f212a924840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.22153742e-05
Iter: 2 loss: 9.97210554e-06
Iter: 3 loss: 2.45407809e-05
Iter: 4 loss: 9.73111128e-06
Iter: 5 loss: 8.56697261e-06
Iter: 6 loss: 2.12324194e-05
Iter: 7 loss: 8.54141581e-06
Iter: 8 loss: 8.22302e-06
Iter: 9 loss: 8.23513074e-06
Iter: 10 loss: 7.97192934e-06
Iter: 11 loss: 7.49018091e-06
Iter: 12 loss: 1.13095984e-05
Iter: 13 loss: 7.45714487e-06
Iter: 14 loss: 7.27391307e-06
Iter: 15 loss: 7.09450342e-06
Iter: 16 loss: 7.05537968e-06
Iter: 17 loss: 6.77190428e-06
Iter: 18 loss: 7.86566e-06
Iter: 19 loss: 6.70516965e-06
Iter: 20 loss: 6.43188559e-06
Iter: 21 loss: 7.99595546e-06
Iter: 22 loss: 6.39485961e-06
Iter: 23 loss: 6.13998145e-06
Iter: 24 loss: 7.18417868e-06
Iter: 25 loss: 6.08439404e-06
Iter: 26 loss: 5.95548408e-06
Iter: 27 loss: 5.78344952e-06
Iter: 28 loss: 5.7738489e-06
Iter: 29 loss: 5.6204276e-06
Iter: 30 loss: 6.74878174e-06
Iter: 31 loss: 5.60790795e-06
Iter: 32 loss: 5.43888291e-06
Iter: 33 loss: 6.55126e-06
Iter: 34 loss: 5.42126418e-06
Iter: 35 loss: 5.34175e-06
Iter: 36 loss: 5.13067516e-06
Iter: 37 loss: 6.64414256e-06
Iter: 38 loss: 5.08475341e-06
Iter: 39 loss: 4.80768222e-06
Iter: 40 loss: 5.90412446e-06
Iter: 41 loss: 4.74454191e-06
Iter: 42 loss: 4.56336875e-06
Iter: 43 loss: 5.37502137e-06
Iter: 44 loss: 4.52800577e-06
Iter: 45 loss: 4.42603323e-06
Iter: 46 loss: 5.97275812e-06
Iter: 47 loss: 4.42595683e-06
Iter: 48 loss: 4.32952856e-06
Iter: 49 loss: 4.91524861e-06
Iter: 50 loss: 4.31767194e-06
Iter: 51 loss: 4.2597444e-06
Iter: 52 loss: 4.18422951e-06
Iter: 53 loss: 4.17933597e-06
Iter: 54 loss: 4.120825e-06
Iter: 55 loss: 4.10991561e-06
Iter: 56 loss: 4.07709285e-06
Iter: 57 loss: 4.00194e-06
Iter: 58 loss: 4.97957535e-06
Iter: 59 loss: 3.99694545e-06
Iter: 60 loss: 3.94675362e-06
Iter: 61 loss: 4.70103e-06
Iter: 62 loss: 3.94670769e-06
Iter: 63 loss: 3.90379319e-06
Iter: 64 loss: 4.12164763e-06
Iter: 65 loss: 3.8969356e-06
Iter: 66 loss: 3.86545071e-06
Iter: 67 loss: 3.90362675e-06
Iter: 68 loss: 3.84892064e-06
Iter: 69 loss: 3.81073119e-06
Iter: 70 loss: 3.73093985e-06
Iter: 71 loss: 5.08198855e-06
Iter: 72 loss: 3.728916e-06
Iter: 73 loss: 3.68489304e-06
Iter: 74 loss: 3.6790816e-06
Iter: 75 loss: 3.62503943e-06
Iter: 76 loss: 3.70240741e-06
Iter: 77 loss: 3.59858359e-06
Iter: 78 loss: 3.56684313e-06
Iter: 79 loss: 3.52564462e-06
Iter: 80 loss: 3.52296934e-06
Iter: 81 loss: 3.47550599e-06
Iter: 82 loss: 3.62325295e-06
Iter: 83 loss: 3.46171851e-06
Iter: 84 loss: 3.42189924e-06
Iter: 85 loss: 3.65637425e-06
Iter: 86 loss: 3.41678515e-06
Iter: 87 loss: 3.38751602e-06
Iter: 88 loss: 3.38725772e-06
Iter: 89 loss: 3.36911262e-06
Iter: 90 loss: 3.33433968e-06
Iter: 91 loss: 4.066781e-06
Iter: 92 loss: 3.33421e-06
Iter: 93 loss: 3.31481556e-06
Iter: 94 loss: 3.30975627e-06
Iter: 95 loss: 3.29594877e-06
Iter: 96 loss: 3.26494433e-06
Iter: 97 loss: 3.6964e-06
Iter: 98 loss: 3.26330155e-06
Iter: 99 loss: 3.23888707e-06
Iter: 100 loss: 3.37108827e-06
Iter: 101 loss: 3.23524819e-06
Iter: 102 loss: 3.21401103e-06
Iter: 103 loss: 3.51638823e-06
Iter: 104 loss: 3.21398056e-06
Iter: 105 loss: 3.20358208e-06
Iter: 106 loss: 3.17495301e-06
Iter: 107 loss: 3.34490687e-06
Iter: 108 loss: 3.16706928e-06
Iter: 109 loss: 3.1315335e-06
Iter: 110 loss: 3.36435414e-06
Iter: 111 loss: 3.1277882e-06
Iter: 112 loss: 3.10373571e-06
Iter: 113 loss: 3.10318092e-06
Iter: 114 loss: 3.091601e-06
Iter: 115 loss: 3.08165818e-06
Iter: 116 loss: 3.07853793e-06
Iter: 117 loss: 3.06105062e-06
Iter: 118 loss: 3.11838085e-06
Iter: 119 loss: 3.05622279e-06
Iter: 120 loss: 3.04205014e-06
Iter: 121 loss: 3.02746344e-06
Iter: 122 loss: 3.02474336e-06
Iter: 123 loss: 3.01150158e-06
Iter: 124 loss: 3.01061755e-06
Iter: 125 loss: 2.99346698e-06
Iter: 126 loss: 2.98447048e-06
Iter: 127 loss: 2.97663519e-06
Iter: 128 loss: 2.96226972e-06
Iter: 129 loss: 3.12396355e-06
Iter: 130 loss: 2.9619589e-06
Iter: 131 loss: 2.94642109e-06
Iter: 132 loss: 2.93885569e-06
Iter: 133 loss: 2.93131029e-06
Iter: 134 loss: 2.91980609e-06
Iter: 135 loss: 2.91208244e-06
Iter: 136 loss: 2.90774256e-06
Iter: 137 loss: 2.9050891e-06
Iter: 138 loss: 2.8997174e-06
Iter: 139 loss: 2.89244622e-06
Iter: 140 loss: 2.87380976e-06
Iter: 141 loss: 3.02969534e-06
Iter: 142 loss: 2.87064677e-06
Iter: 143 loss: 2.85079204e-06
Iter: 144 loss: 2.8931895e-06
Iter: 145 loss: 2.84309522e-06
Iter: 146 loss: 2.83312806e-06
Iter: 147 loss: 2.83182862e-06
Iter: 148 loss: 2.82035603e-06
Iter: 149 loss: 2.82558858e-06
Iter: 150 loss: 2.81254029e-06
Iter: 151 loss: 2.80352833e-06
Iter: 152 loss: 2.79273627e-06
Iter: 153 loss: 2.79160759e-06
Iter: 154 loss: 2.778404e-06
Iter: 155 loss: 2.86885233e-06
Iter: 156 loss: 2.77718073e-06
Iter: 157 loss: 2.76400397e-06
Iter: 158 loss: 2.82379233e-06
Iter: 159 loss: 2.76153833e-06
Iter: 160 loss: 2.75204411e-06
Iter: 161 loss: 2.86542218e-06
Iter: 162 loss: 2.7519111e-06
Iter: 163 loss: 2.7447e-06
Iter: 164 loss: 2.73336582e-06
Iter: 165 loss: 2.73320711e-06
Iter: 166 loss: 2.72399757e-06
Iter: 167 loss: 2.72352645e-06
Iter: 168 loss: 2.71826116e-06
Iter: 169 loss: 2.70517785e-06
Iter: 170 loss: 2.83013651e-06
Iter: 171 loss: 2.70333499e-06
Iter: 172 loss: 2.69271163e-06
Iter: 173 loss: 2.75276625e-06
Iter: 174 loss: 2.69112752e-06
Iter: 175 loss: 2.68215422e-06
Iter: 176 loss: 2.68215649e-06
Iter: 177 loss: 2.6766279e-06
Iter: 178 loss: 2.66295137e-06
Iter: 179 loss: 2.79460073e-06
Iter: 180 loss: 2.66113057e-06
Iter: 181 loss: 2.64744813e-06
Iter: 182 loss: 2.68319468e-06
Iter: 183 loss: 2.64287041e-06
Iter: 184 loss: 2.64328855e-06
Iter: 185 loss: 2.63655943e-06
Iter: 186 loss: 2.63266452e-06
Iter: 187 loss: 2.62340654e-06
Iter: 188 loss: 2.72693751e-06
Iter: 189 loss: 2.62255253e-06
Iter: 190 loss: 2.61143214e-06
Iter: 191 loss: 2.61183959e-06
Iter: 192 loss: 2.60273805e-06
Iter: 193 loss: 2.59549324e-06
Iter: 194 loss: 2.59520175e-06
Iter: 195 loss: 2.5873735e-06
Iter: 196 loss: 2.61860714e-06
Iter: 197 loss: 2.58553132e-06
Iter: 198 loss: 2.58004593e-06
Iter: 199 loss: 2.58151931e-06
Iter: 200 loss: 2.57598822e-06
Iter: 201 loss: 2.56707654e-06
Iter: 202 loss: 2.61227319e-06
Iter: 203 loss: 2.56558292e-06
Iter: 204 loss: 2.56042949e-06
Iter: 205 loss: 2.55863415e-06
Iter: 206 loss: 2.55560144e-06
Iter: 207 loss: 2.5472259e-06
Iter: 208 loss: 2.56353655e-06
Iter: 209 loss: 2.54368297e-06
Iter: 210 loss: 2.53846633e-06
Iter: 211 loss: 2.61587729e-06
Iter: 212 loss: 2.53847475e-06
Iter: 213 loss: 2.53312328e-06
Iter: 214 loss: 2.53378312e-06
Iter: 215 loss: 2.52898053e-06
Iter: 216 loss: 2.52396399e-06
Iter: 217 loss: 2.51816618e-06
Iter: 218 loss: 2.51750475e-06
Iter: 219 loss: 2.51112328e-06
Iter: 220 loss: 2.6070436e-06
Iter: 221 loss: 2.51110032e-06
Iter: 222 loss: 2.50334e-06
Iter: 223 loss: 2.50683183e-06
Iter: 224 loss: 2.49797199e-06
Iter: 225 loss: 2.49191044e-06
Iter: 226 loss: 2.48353217e-06
Iter: 227 loss: 2.48319384e-06
Iter: 228 loss: 2.47201388e-06
Iter: 229 loss: 2.51315487e-06
Iter: 230 loss: 2.46925765e-06
Iter: 231 loss: 2.47107937e-06
Iter: 232 loss: 2.465732e-06
Iter: 233 loss: 2.46293075e-06
Iter: 234 loss: 2.45662432e-06
Iter: 235 loss: 2.54054066e-06
Iter: 236 loss: 2.45619867e-06
Iter: 237 loss: 2.45191586e-06
Iter: 238 loss: 2.451726e-06
Iter: 239 loss: 2.44789544e-06
Iter: 240 loss: 2.43939166e-06
Iter: 241 loss: 2.5672523e-06
Iter: 242 loss: 2.43906879e-06
Iter: 243 loss: 2.43177487e-06
Iter: 244 loss: 2.45208321e-06
Iter: 245 loss: 2.42936449e-06
Iter: 246 loss: 2.42265492e-06
Iter: 247 loss: 2.42263741e-06
Iter: 248 loss: 2.41882367e-06
Iter: 249 loss: 2.43872682e-06
Iter: 250 loss: 2.41818452e-06
Iter: 251 loss: 2.41534144e-06
Iter: 252 loss: 2.41002954e-06
Iter: 253 loss: 2.52824225e-06
Iter: 254 loss: 2.41004682e-06
Iter: 255 loss: 2.40443273e-06
Iter: 256 loss: 2.42811166e-06
Iter: 257 loss: 2.40329359e-06
Iter: 258 loss: 2.39799328e-06
Iter: 259 loss: 2.47136268e-06
Iter: 260 loss: 2.39795736e-06
Iter: 261 loss: 2.39511292e-06
Iter: 262 loss: 2.38684447e-06
Iter: 263 loss: 2.42116585e-06
Iter: 264 loss: 2.38358848e-06
Iter: 265 loss: 2.37616814e-06
Iter: 266 loss: 2.46759555e-06
Iter: 267 loss: 2.37610539e-06
Iter: 268 loss: 2.37341692e-06
Iter: 269 loss: 2.37260247e-06
Iter: 270 loss: 2.37075847e-06
Iter: 271 loss: 2.36688561e-06
Iter: 272 loss: 2.42912847e-06
Iter: 273 loss: 2.36679671e-06
Iter: 274 loss: 2.363151e-06
Iter: 275 loss: 2.36313281e-06
Iter: 276 loss: 2.36053415e-06
Iter: 277 loss: 2.35430821e-06
Iter: 278 loss: 2.42213832e-06
Iter: 279 loss: 2.35363132e-06
Iter: 280 loss: 2.34653317e-06
Iter: 281 loss: 2.36523147e-06
Iter: 282 loss: 2.34411937e-06
Iter: 283 loss: 2.3419509e-06
Iter: 284 loss: 2.34056733e-06
Iter: 285 loss: 2.33780293e-06
Iter: 286 loss: 2.33379433e-06
Iter: 287 loss: 2.33368e-06
Iter: 288 loss: 2.3291725e-06
Iter: 289 loss: 2.35283733e-06
Iter: 290 loss: 2.32846833e-06
Iter: 291 loss: 2.32514799e-06
Iter: 292 loss: 2.32793218e-06
Iter: 293 loss: 2.32328057e-06
Iter: 294 loss: 2.31948616e-06
Iter: 295 loss: 2.37590325e-06
Iter: 296 loss: 2.31952095e-06
Iter: 297 loss: 2.31684567e-06
Iter: 298 loss: 2.30927071e-06
Iter: 299 loss: 2.34602294e-06
Iter: 300 loss: 2.30663272e-06
Iter: 301 loss: 2.29896659e-06
Iter: 302 loss: 2.33778451e-06
Iter: 303 loss: 2.29769944e-06
Iter: 304 loss: 2.29376269e-06
Iter: 305 loss: 2.2937113e-06
Iter: 306 loss: 2.28960698e-06
Iter: 307 loss: 2.30753244e-06
Iter: 308 loss: 2.28874978e-06
Iter: 309 loss: 2.28694535e-06
Iter: 310 loss: 2.28464205e-06
Iter: 311 loss: 2.28451381e-06
Iter: 312 loss: 2.27950636e-06
Iter: 313 loss: 2.29847478e-06
Iter: 314 loss: 2.27840519e-06
Iter: 315 loss: 2.27450573e-06
Iter: 316 loss: 2.26840802e-06
Iter: 317 loss: 2.26829229e-06
Iter: 318 loss: 2.26429688e-06
Iter: 319 loss: 2.2643012e-06
Iter: 320 loss: 2.25996041e-06
Iter: 321 loss: 2.27305304e-06
Iter: 322 loss: 2.25868052e-06
Iter: 323 loss: 2.25638632e-06
Iter: 324 loss: 2.25269969e-06
Iter: 325 loss: 2.25262443e-06
Iter: 326 loss: 2.24968744e-06
Iter: 327 loss: 2.24967721e-06
Iter: 328 loss: 2.24594555e-06
Iter: 329 loss: 2.24332939e-06
Iter: 330 loss: 2.24199266e-06
Iter: 331 loss: 2.23730513e-06
Iter: 332 loss: 2.25165695e-06
Iter: 333 loss: 2.23596476e-06
Iter: 334 loss: 2.23149118e-06
Iter: 335 loss: 2.23725715e-06
Iter: 336 loss: 2.22922881e-06
Iter: 337 loss: 2.22505287e-06
Iter: 338 loss: 2.23120969e-06
Iter: 339 loss: 2.22300196e-06
Iter: 340 loss: 2.22331823e-06
Iter: 341 loss: 2.22114022e-06
Iter: 342 loss: 2.21986238e-06
Iter: 343 loss: 2.21638197e-06
Iter: 344 loss: 2.23867778e-06
Iter: 345 loss: 2.21547043e-06
Iter: 346 loss: 2.21287019e-06
Iter: 347 loss: 2.21283472e-06
Iter: 348 loss: 2.20980792e-06
Iter: 349 loss: 2.20645256e-06
Iter: 350 loss: 2.20596712e-06
Iter: 351 loss: 2.20178708e-06
Iter: 352 loss: 2.20759739e-06
Iter: 353 loss: 2.19967433e-06
Iter: 354 loss: 2.19802314e-06
Iter: 355 loss: 2.1972719e-06
Iter: 356 loss: 2.1957037e-06
Iter: 357 loss: 2.19180129e-06
Iter: 358 loss: 2.22680819e-06
Iter: 359 loss: 2.19120625e-06
Iter: 360 loss: 2.18734203e-06
Iter: 361 loss: 2.20588845e-06
Iter: 362 loss: 2.18667856e-06
Iter: 363 loss: 2.18270134e-06
Iter: 364 loss: 2.22316112e-06
Iter: 365 loss: 2.18258333e-06
Iter: 366 loss: 2.1805381e-06
Iter: 367 loss: 2.1750534e-06
Iter: 368 loss: 2.21346545e-06
Iter: 369 loss: 2.17382444e-06
Iter: 370 loss: 2.1686551e-06
Iter: 371 loss: 2.22718018e-06
Iter: 372 loss: 2.16853823e-06
Iter: 373 loss: 2.16537842e-06
Iter: 374 loss: 2.19996491e-06
Iter: 375 loss: 2.16525268e-06
Iter: 376 loss: 2.16338435e-06
Iter: 377 loss: 2.18183732e-06
Iter: 378 loss: 2.16328863e-06
Iter: 379 loss: 2.16138915e-06
Iter: 380 loss: 2.1588678e-06
Iter: 381 loss: 2.15871387e-06
Iter: 382 loss: 2.15627733e-06
Iter: 383 loss: 2.1622659e-06
Iter: 384 loss: 2.15539376e-06
Iter: 385 loss: 2.15186856e-06
Iter: 386 loss: 2.16749163e-06
Iter: 387 loss: 2.1512044e-06
Iter: 388 loss: 2.14895636e-06
Iter: 389 loss: 2.14474358e-06
Iter: 390 loss: 2.23836332e-06
Iter: 391 loss: 2.14472948e-06
Iter: 392 loss: 2.14485453e-06
Iter: 393 loss: 2.14271836e-06
Iter: 394 loss: 2.14122156e-06
Iter: 395 loss: 2.13782778e-06
Iter: 396 loss: 2.18187915e-06
Iter: 397 loss: 2.13756493e-06
Iter: 398 loss: 2.13492967e-06
Iter: 399 loss: 2.15133173e-06
Iter: 400 loss: 2.13457133e-06
Iter: 401 loss: 2.13152634e-06
Iter: 402 loss: 2.14918191e-06
Iter: 403 loss: 2.13115982e-06
Iter: 404 loss: 2.12915256e-06
Iter: 405 loss: 2.12412215e-06
Iter: 406 loss: 2.16723629e-06
Iter: 407 loss: 2.12328428e-06
Iter: 408 loss: 2.11803058e-06
Iter: 409 loss: 2.16785929e-06
Iter: 410 loss: 2.11786255e-06
Iter: 411 loss: 2.11526503e-06
Iter: 412 loss: 2.11516749e-06
Iter: 413 loss: 2.11317411e-06
Iter: 414 loss: 2.11948168e-06
Iter: 415 loss: 2.11266e-06
Iter: 416 loss: 2.11088059e-06
Iter: 417 loss: 2.10912117e-06
Iter: 418 loss: 2.10874623e-06
Iter: 419 loss: 2.10709186e-06
Iter: 420 loss: 2.1272208e-06
Iter: 421 loss: 2.10706889e-06
Iter: 422 loss: 2.10505141e-06
Iter: 423 loss: 2.1024648e-06
Iter: 424 loss: 2.10227245e-06
Iter: 425 loss: 2.09942618e-06
Iter: 426 loss: 2.10202052e-06
Iter: 427 loss: 2.09772043e-06
Iter: 428 loss: 2.09468817e-06
Iter: 429 loss: 2.09470204e-06
Iter: 430 loss: 2.0930097e-06
Iter: 431 loss: 2.08915435e-06
Iter: 432 loss: 2.14272586e-06
Iter: 433 loss: 2.08896108e-06
Iter: 434 loss: 2.0867451e-06
Iter: 435 loss: 2.08666756e-06
Iter: 436 loss: 2.08420261e-06
Iter: 437 loss: 2.08631127e-06
Iter: 438 loss: 2.08274218e-06
Iter: 439 loss: 2.0808327e-06
Iter: 440 loss: 2.07769926e-06
Iter: 441 loss: 2.07765243e-06
Iter: 442 loss: 2.07301082e-06
Iter: 443 loss: 2.07995777e-06
Iter: 444 loss: 2.07080939e-06
Iter: 445 loss: 2.06993468e-06
Iter: 446 loss: 2.06819504e-06
Iter: 447 loss: 2.06660479e-06
Iter: 448 loss: 2.06478e-06
Iter: 449 loss: 2.06450795e-06
Iter: 450 loss: 2.06177128e-06
Iter: 451 loss: 2.06927916e-06
Iter: 452 loss: 2.06092818e-06
Iter: 453 loss: 2.05934384e-06
Iter: 454 loss: 2.07589119e-06
Iter: 455 loss: 2.05932429e-06
Iter: 456 loss: 2.05762467e-06
Iter: 457 loss: 2.05541573e-06
Iter: 458 loss: 2.05529454e-06
Iter: 459 loss: 2.05295237e-06
Iter: 460 loss: 2.05590345e-06
Iter: 461 loss: 2.0517241e-06
Iter: 462 loss: 2.04859953e-06
Iter: 463 loss: 2.08305164e-06
Iter: 464 loss: 2.04856315e-06
Iter: 465 loss: 2.04675189e-06
Iter: 466 loss: 2.04267485e-06
Iter: 467 loss: 2.08970687e-06
Iter: 468 loss: 2.04228581e-06
Iter: 469 loss: 2.04216758e-06
Iter: 470 loss: 2.04073808e-06
Iter: 471 loss: 2.03913328e-06
Iter: 472 loss: 2.0372147e-06
Iter: 473 loss: 2.03701802e-06
Iter: 474 loss: 2.03490231e-06
Iter: 475 loss: 2.0336081e-06
Iter: 476 loss: 2.0327393e-06
Iter: 477 loss: 2.02945898e-06
Iter: 478 loss: 2.03940886e-06
Iter: 479 loss: 2.02856017e-06
Iter: 480 loss: 2.02494539e-06
Iter: 481 loss: 2.07377525e-06
Iter: 482 loss: 2.02494448e-06
Iter: 483 loss: 2.02303681e-06
Iter: 484 loss: 2.02032288e-06
Iter: 485 loss: 2.02022557e-06
Iter: 486 loss: 2.01769535e-06
Iter: 487 loss: 2.04678508e-06
Iter: 488 loss: 2.01764055e-06
Iter: 489 loss: 2.01615876e-06
Iter: 490 loss: 2.02408614e-06
Iter: 491 loss: 2.01590069e-06
Iter: 492 loss: 2.01430043e-06
Iter: 493 loss: 2.01307671e-06
Iter: 494 loss: 2.01256216e-06
Iter: 495 loss: 2.01056127e-06
Iter: 496 loss: 2.01438843e-06
Iter: 497 loss: 2.00974137e-06
Iter: 498 loss: 2.00744853e-06
Iter: 499 loss: 2.03338664e-06
Iter: 500 loss: 2.00743716e-06
Iter: 501 loss: 2.0060877e-06
Iter: 502 loss: 2.00264822e-06
Iter: 503 loss: 2.03341278e-06
Iter: 504 loss: 2.00214299e-06
Iter: 505 loss: 2.00122668e-06
Iter: 506 loss: 2.00050249e-06
Iter: 507 loss: 1.99879605e-06
Iter: 508 loss: 1.99896954e-06
Iter: 509 loss: 1.99754936e-06
Iter: 510 loss: 1.99583451e-06
Iter: 511 loss: 1.99251735e-06
Iter: 512 loss: 2.05656306e-06
Iter: 513 loss: 1.99250144e-06
Iter: 514 loss: 1.99087458e-06
Iter: 515 loss: 1.99054512e-06
Iter: 516 loss: 1.98830094e-06
Iter: 517 loss: 1.99070064e-06
Iter: 518 loss: 1.98707221e-06
Iter: 519 loss: 1.98533235e-06
Iter: 520 loss: 1.98356133e-06
Iter: 521 loss: 1.98319708e-06
Iter: 522 loss: 1.98108137e-06
Iter: 523 loss: 1.98109615e-06
Iter: 524 loss: 1.97932104e-06
Iter: 525 loss: 1.98244288e-06
Iter: 526 loss: 1.97849113e-06
Iter: 527 loss: 1.97648387e-06
Iter: 528 loss: 1.97673e-06
Iter: 529 loss: 1.97497161e-06
Iter: 530 loss: 1.97321265e-06
Iter: 531 loss: 1.98635871e-06
Iter: 532 loss: 1.9731151e-06
Iter: 533 loss: 1.97102372e-06
Iter: 534 loss: 1.97437112e-06
Iter: 535 loss: 1.97008967e-06
Iter: 536 loss: 1.96852761e-06
Iter: 537 loss: 1.96578253e-06
Iter: 538 loss: 1.96578503e-06
Iter: 539 loss: 1.96538849e-06
Iter: 540 loss: 1.96435622e-06
Iter: 541 loss: 1.96303017e-06
Iter: 542 loss: 1.96000974e-06
Iter: 543 loss: 2.00474346e-06
Iter: 544 loss: 1.95987832e-06
Iter: 545 loss: 1.95721032e-06
Iter: 546 loss: 1.96190899e-06
Iter: 547 loss: 1.95609982e-06
Iter: 548 loss: 1.95500161e-06
Iter: 549 loss: 1.95443499e-06
Iter: 550 loss: 1.95316488e-06
Iter: 551 loss: 1.95032044e-06
Iter: 552 loss: 1.99102e-06
Iter: 553 loss: 1.9502088e-06
Iter: 554 loss: 1.94806921e-06
Iter: 555 loss: 1.96649466e-06
Iter: 556 loss: 1.94796894e-06
Iter: 557 loss: 1.9460781e-06
Iter: 558 loss: 1.95296889e-06
Iter: 559 loss: 1.94558652e-06
Iter: 560 loss: 1.94379572e-06
Iter: 561 loss: 1.94511586e-06
Iter: 562 loss: 1.94263657e-06
Iter: 563 loss: 1.9402828e-06
Iter: 564 loss: 1.94270388e-06
Iter: 565 loss: 1.93894152e-06
Iter: 566 loss: 1.9375741e-06
Iter: 567 loss: 1.93749702e-06
Iter: 568 loss: 1.93618894e-06
Iter: 569 loss: 1.93302071e-06
Iter: 570 loss: 1.96596784e-06
Iter: 571 loss: 1.93263941e-06
Iter: 572 loss: 1.92971152e-06
Iter: 573 loss: 1.93844585e-06
Iter: 574 loss: 1.9288675e-06
Iter: 575 loss: 1.92849643e-06
Iter: 576 loss: 1.92765879e-06
Iter: 577 loss: 1.92656034e-06
Iter: 578 loss: 1.92379343e-06
Iter: 579 loss: 1.94467066e-06
Iter: 580 loss: 1.92318021e-06
Iter: 581 loss: 1.92007747e-06
Iter: 582 loss: 1.92473931e-06
Iter: 583 loss: 1.91859363e-06
Iter: 584 loss: 1.9180452e-06
Iter: 585 loss: 1.91676872e-06
Iter: 586 loss: 1.91569552e-06
Iter: 587 loss: 1.91302502e-06
Iter: 588 loss: 1.934583e-06
Iter: 589 loss: 1.91250979e-06
Iter: 590 loss: 1.91024901e-06
Iter: 591 loss: 1.9338504e-06
Iter: 592 loss: 1.91019899e-06
Iter: 593 loss: 1.9079157e-06
Iter: 594 loss: 1.91821891e-06
Iter: 595 loss: 1.90749961e-06
Iter: 596 loss: 1.90607625e-06
Iter: 597 loss: 1.90639389e-06
Iter: 598 loss: 1.90499611e-06
Iter: 599 loss: 1.90278911e-06
Iter: 600 loss: 1.90561377e-06
Iter: 601 loss: 1.90167907e-06
Iter: 602 loss: 1.89905609e-06
Iter: 603 loss: 1.92366474e-06
Iter: 604 loss: 1.8989648e-06
Iter: 605 loss: 1.89721231e-06
Iter: 606 loss: 1.89483501e-06
Iter: 607 loss: 1.89469949e-06
Iter: 608 loss: 1.89218417e-06
Iter: 609 loss: 1.89288482e-06
Iter: 610 loss: 1.89030607e-06
Iter: 611 loss: 1.88779359e-06
Iter: 612 loss: 1.90374772e-06
Iter: 613 loss: 1.88751767e-06
Iter: 614 loss: 1.88585238e-06
Iter: 615 loss: 1.88582123e-06
Iter: 616 loss: 1.88459182e-06
Iter: 617 loss: 1.88149966e-06
Iter: 618 loss: 1.90529886e-06
Iter: 619 loss: 1.88091769e-06
Iter: 620 loss: 1.87796297e-06
Iter: 621 loss: 1.8954662e-06
Iter: 622 loss: 1.87758167e-06
Iter: 623 loss: 1.87554326e-06
Iter: 624 loss: 1.87546402e-06
Iter: 625 loss: 1.8743641e-06
Iter: 626 loss: 1.87174328e-06
Iter: 627 loss: 1.90718015e-06
Iter: 628 loss: 1.87161163e-06
Iter: 629 loss: 1.86962234e-06
Iter: 630 loss: 1.87993555e-06
Iter: 631 loss: 1.86927457e-06
Iter: 632 loss: 1.86715283e-06
Iter: 633 loss: 1.88567992e-06
Iter: 634 loss: 1.86705574e-06
Iter: 635 loss: 1.86575346e-06
Iter: 636 loss: 1.86303203e-06
Iter: 637 loss: 1.91428308e-06
Iter: 638 loss: 1.86303839e-06
Iter: 639 loss: 1.86068746e-06
Iter: 640 loss: 1.86067473e-06
Iter: 641 loss: 1.85906595e-06
Iter: 642 loss: 1.86329669e-06
Iter: 643 loss: 1.85855833e-06
Iter: 644 loss: 1.85689737e-06
Iter: 645 loss: 1.85527438e-06
Iter: 646 loss: 1.8549224e-06
Iter: 647 loss: 1.85286115e-06
Iter: 648 loss: 1.85689066e-06
Iter: 649 loss: 1.85198269e-06
Iter: 650 loss: 1.85101271e-06
Iter: 651 loss: 1.85086446e-06
Iter: 652 loss: 1.84957446e-06
Iter: 653 loss: 1.8468761e-06
Iter: 654 loss: 1.89290211e-06
Iter: 655 loss: 1.84675469e-06
Iter: 656 loss: 1.8442347e-06
Iter: 657 loss: 1.84585792e-06
Iter: 658 loss: 1.84265923e-06
Iter: 659 loss: 1.84226565e-06
Iter: 660 loss: 1.84106921e-06
Iter: 661 loss: 1.84022883e-06
Iter: 662 loss: 1.83798579e-06
Iter: 663 loss: 1.85557315e-06
Iter: 664 loss: 1.8375805e-06
Iter: 665 loss: 1.83549719e-06
Iter: 666 loss: 1.84158489e-06
Iter: 667 loss: 1.83489624e-06
Iter: 668 loss: 1.834111e-06
Iter: 669 loss: 1.83372163e-06
Iter: 670 loss: 1.83276711e-06
Iter: 671 loss: 1.8300567e-06
Iter: 672 loss: 1.84697933e-06
Iter: 673 loss: 1.82940335e-06
Iter: 674 loss: 1.8273538e-06
Iter: 675 loss: 1.82726706e-06
Iter: 676 loss: 1.82507881e-06
Iter: 677 loss: 1.82484087e-06
Iter: 678 loss: 1.82328e-06
Iter: 679 loss: 1.82147949e-06
Iter: 680 loss: 1.82607414e-06
Iter: 681 loss: 1.82088809e-06
Iter: 682 loss: 1.81908536e-06
Iter: 683 loss: 1.82441704e-06
Iter: 684 loss: 1.81851635e-06
Iter: 685 loss: 1.81753899e-06
Iter: 686 loss: 1.81752307e-06
Iter: 687 loss: 1.81652854e-06
Iter: 688 loss: 1.81460211e-06
Iter: 689 loss: 1.85477052e-06
Iter: 690 loss: 1.81458483e-06
Iter: 691 loss: 1.81260828e-06
Iter: 692 loss: 1.81125313e-06
Iter: 693 loss: 1.81058488e-06
Iter: 694 loss: 1.80926872e-06
Iter: 695 loss: 1.80884604e-06
Iter: 696 loss: 1.80696509e-06
Iter: 697 loss: 1.8069212e-06
Iter: 698 loss: 1.80544487e-06
Iter: 699 loss: 1.80410484e-06
Iter: 700 loss: 1.8024773e-06
Iter: 701 loss: 1.80235736e-06
Iter: 702 loss: 1.801385e-06
Iter: 703 loss: 1.80119468e-06
Iter: 704 loss: 1.7998118e-06
Iter: 705 loss: 1.79739948e-06
Iter: 706 loss: 1.79741687e-06
Iter: 707 loss: 1.79499011e-06
Iter: 708 loss: 1.79744325e-06
Iter: 709 loss: 1.79364815e-06
Iter: 710 loss: 1.79105382e-06
Iter: 711 loss: 1.79102415e-06
Iter: 712 loss: 1.78994412e-06
Iter: 713 loss: 1.78775258e-06
Iter: 714 loss: 1.83147984e-06
Iter: 715 loss: 1.78773234e-06
Iter: 716 loss: 1.78690402e-06
Iter: 717 loss: 1.78670541e-06
Iter: 718 loss: 1.78577307e-06
Iter: 719 loss: 1.78478319e-06
Iter: 720 loss: 1.78459561e-06
Iter: 721 loss: 1.78303299e-06
Iter: 722 loss: 1.78334858e-06
Iter: 723 loss: 1.78185098e-06
Iter: 724 loss: 1.77964466e-06
Iter: 725 loss: 1.78945152e-06
Iter: 726 loss: 1.77919298e-06
Iter: 727 loss: 1.77712354e-06
Iter: 728 loss: 1.78155074e-06
Iter: 729 loss: 1.77634092e-06
Iter: 730 loss: 1.77494917e-06
Iter: 731 loss: 1.77488232e-06
Iter: 732 loss: 1.77401114e-06
Iter: 733 loss: 1.77180345e-06
Iter: 734 loss: 1.78973369e-06
Iter: 735 loss: 1.7713902e-06
Iter: 736 loss: 1.77066329e-06
Iter: 737 loss: 1.77034872e-06
Iter: 738 loss: 1.76922697e-06
Iter: 739 loss: 1.76790013e-06
Iter: 740 loss: 1.76782873e-06
Iter: 741 loss: 1.76610524e-06
Iter: 742 loss: 1.76614799e-06
Iter: 743 loss: 1.76476078e-06
Iter: 744 loss: 1.7625697e-06
Iter: 745 loss: 1.76254537e-06
Iter: 746 loss: 1.76140315e-06
Iter: 747 loss: 1.75911941e-06
Iter: 748 loss: 1.80334973e-06
Iter: 749 loss: 1.75905507e-06
Iter: 750 loss: 1.75769367e-06
Iter: 751 loss: 1.75741559e-06
Iter: 752 loss: 1.75635932e-06
Iter: 753 loss: 1.75414937e-06
Iter: 754 loss: 1.79628796e-06
Iter: 755 loss: 1.75415062e-06
Iter: 756 loss: 1.75189007e-06
Iter: 757 loss: 1.75326113e-06
Iter: 758 loss: 1.75039736e-06
Iter: 759 loss: 1.748062e-06
Iter: 760 loss: 1.76100764e-06
Iter: 761 loss: 1.747704e-06
Iter: 762 loss: 1.74536456e-06
Iter: 763 loss: 1.76019944e-06
Iter: 764 loss: 1.7451132e-06
Iter: 765 loss: 1.74371985e-06
Iter: 766 loss: 1.76384719e-06
Iter: 767 loss: 1.74371326e-06
Iter: 768 loss: 1.74262573e-06
Iter: 769 loss: 1.74070726e-06
Iter: 770 loss: 1.74068839e-06
Iter: 771 loss: 1.73878016e-06
Iter: 772 loss: 1.73905801e-06
Iter: 773 loss: 1.73732906e-06
Iter: 774 loss: 1.73650244e-06
Iter: 775 loss: 1.73595049e-06
Iter: 776 loss: 1.73484045e-06
Iter: 777 loss: 1.7332228e-06
Iter: 778 loss: 1.73319518e-06
Iter: 779 loss: 1.73155149e-06
Iter: 780 loss: 1.73525586e-06
Iter: 781 loss: 1.73093838e-06
Iter: 782 loss: 1.72902969e-06
Iter: 783 loss: 1.74988304e-06
Iter: 784 loss: 1.72895386e-06
Iter: 785 loss: 1.72792591e-06
Iter: 786 loss: 1.72533692e-06
Iter: 787 loss: 1.75183584e-06
Iter: 788 loss: 1.72501018e-06
Iter: 789 loss: 1.72277237e-06
Iter: 790 loss: 1.74969034e-06
Iter: 791 loss: 1.72273235e-06
Iter: 792 loss: 1.7203057e-06
Iter: 793 loss: 1.73096055e-06
Iter: 794 loss: 1.71985323e-06
Iter: 795 loss: 1.71874126e-06
Iter: 796 loss: 1.71710951e-06
Iter: 797 loss: 1.71707529e-06
Iter: 798 loss: 1.71550141e-06
Iter: 799 loss: 1.7296386e-06
Iter: 800 loss: 1.71538932e-06
Iter: 801 loss: 1.71367196e-06
Iter: 802 loss: 1.7210175e-06
Iter: 803 loss: 1.71332658e-06
Iter: 804 loss: 1.71238173e-06
Iter: 805 loss: 1.7111945e-06
Iter: 806 loss: 1.71110355e-06
Iter: 807 loss: 1.70886801e-06
Iter: 808 loss: 1.71277702e-06
Iter: 809 loss: 1.70794931e-06
Iter: 810 loss: 1.70592421e-06
Iter: 811 loss: 1.72075147e-06
Iter: 812 loss: 1.70575277e-06
Iter: 813 loss: 1.70371504e-06
Iter: 814 loss: 1.70998e-06
Iter: 815 loss: 1.70313251e-06
Iter: 816 loss: 1.70200565e-06
Iter: 817 loss: 1.70073088e-06
Iter: 818 loss: 1.7005658e-06
Iter: 819 loss: 1.69921861e-06
Iter: 820 loss: 1.699113e-06
Iter: 821 loss: 1.69828991e-06
Iter: 822 loss: 1.69636326e-06
Iter: 823 loss: 1.71663658e-06
Iter: 824 loss: 1.69620421e-06
Iter: 825 loss: 1.69410214e-06
Iter: 826 loss: 1.69996201e-06
Iter: 827 loss: 1.69342889e-06
Iter: 828 loss: 1.69132068e-06
Iter: 829 loss: 1.723987e-06
Iter: 830 loss: 1.69131965e-06
Iter: 831 loss: 1.69015334e-06
Iter: 832 loss: 1.68778058e-06
Iter: 833 loss: 1.73206365e-06
Iter: 834 loss: 1.68778899e-06
Iter: 835 loss: 1.68564122e-06
Iter: 836 loss: 1.6918932e-06
Iter: 837 loss: 1.6849782e-06
Iter: 838 loss: 1.68449969e-06
Iter: 839 loss: 1.68388294e-06
Iter: 840 loss: 1.68327131e-06
Iter: 841 loss: 1.68164672e-06
Iter: 842 loss: 1.69439295e-06
Iter: 843 loss: 1.6813658e-06
Iter: 844 loss: 1.67959183e-06
Iter: 845 loss: 1.685293e-06
Iter: 846 loss: 1.679109e-06
Iter: 847 loss: 1.6769178e-06
Iter: 848 loss: 1.68307929e-06
Iter: 849 loss: 1.67627957e-06
Iter: 850 loss: 1.67447149e-06
Iter: 851 loss: 1.68552924e-06
Iter: 852 loss: 1.67429835e-06
Iter: 853 loss: 1.67260794e-06
Iter: 854 loss: 1.68135625e-06
Iter: 855 loss: 1.67232452e-06
Iter: 856 loss: 1.67136363e-06
Iter: 857 loss: 1.66985114e-06
Iter: 858 loss: 1.66982909e-06
Iter: 859 loss: 1.66910945e-06
Iter: 860 loss: 1.66882387e-06
Iter: 861 loss: 1.66799259e-06
Iter: 862 loss: 1.6660174e-06
Iter: 863 loss: 1.68691622e-06
Iter: 864 loss: 1.66579457e-06
Iter: 865 loss: 1.66399354e-06
Iter: 866 loss: 1.68300517e-06
Iter: 867 loss: 1.66398695e-06
Iter: 868 loss: 1.66206064e-06
Iter: 869 loss: 1.6672077e-06
Iter: 870 loss: 1.66139614e-06
Iter: 871 loss: 1.66028371e-06
Iter: 872 loss: 1.65858103e-06
Iter: 873 loss: 1.65855022e-06
Iter: 874 loss: 1.65684924e-06
Iter: 875 loss: 1.66087216e-06
Iter: 876 loss: 1.65622532e-06
Iter: 877 loss: 1.65595713e-06
Iter: 878 loss: 1.65533265e-06
Iter: 879 loss: 1.65456822e-06
Iter: 880 loss: 1.65285212e-06
Iter: 881 loss: 1.67961753e-06
Iter: 882 loss: 1.65275776e-06
Iter: 883 loss: 1.65101721e-06
Iter: 884 loss: 1.65121912e-06
Iter: 885 loss: 1.64969219e-06
Iter: 886 loss: 1.64764754e-06
Iter: 887 loss: 1.67632982e-06
Iter: 888 loss: 1.64764651e-06
Iter: 889 loss: 1.64648452e-06
Iter: 890 loss: 1.65284825e-06
Iter: 891 loss: 1.64631706e-06
Iter: 892 loss: 1.64519975e-06
Iter: 893 loss: 1.64931771e-06
Iter: 894 loss: 1.64487687e-06
Iter: 895 loss: 1.64406561e-06
Iter: 896 loss: 1.64247206e-06
Iter: 897 loss: 1.67950873e-06
Iter: 898 loss: 1.64247331e-06
Iter: 899 loss: 1.64155597e-06
Iter: 900 loss: 1.64136372e-06
Iter: 901 loss: 1.64051539e-06
Iter: 902 loss: 1.63857408e-06
Iter: 903 loss: 1.66046175e-06
Iter: 904 loss: 1.63834511e-06
Iter: 905 loss: 1.63639129e-06
Iter: 906 loss: 1.64591438e-06
Iter: 907 loss: 1.6360201e-06
Iter: 908 loss: 1.63427967e-06
Iter: 909 loss: 1.65903714e-06
Iter: 910 loss: 1.63426876e-06
Iter: 911 loss: 1.63357424e-06
Iter: 912 loss: 1.63222148e-06
Iter: 913 loss: 1.66206655e-06
Iter: 914 loss: 1.63222319e-06
Iter: 915 loss: 1.63172047e-06
Iter: 916 loss: 1.63144637e-06
Iter: 917 loss: 1.63080449e-06
Iter: 918 loss: 1.62907565e-06
Iter: 919 loss: 1.63946095e-06
Iter: 920 loss: 1.62861602e-06
Iter: 921 loss: 1.62658716e-06
Iter: 922 loss: 1.63110667e-06
Iter: 923 loss: 1.62588515e-06
Iter: 924 loss: 1.62409799e-06
Iter: 925 loss: 1.63839377e-06
Iter: 926 loss: 1.62399317e-06
Iter: 927 loss: 1.6226528e-06
Iter: 928 loss: 1.62593858e-06
Iter: 929 loss: 1.62212393e-06
Iter: 930 loss: 1.62055721e-06
Iter: 931 loss: 1.62848528e-06
Iter: 932 loss: 1.62030369e-06
Iter: 933 loss: 1.61922969e-06
Iter: 934 loss: 1.61850619e-06
Iter: 935 loss: 1.61810419e-06
Iter: 936 loss: 1.6171507e-06
Iter: 937 loss: 1.61706305e-06
Iter: 938 loss: 1.61618505e-06
Iter: 939 loss: 1.61424771e-06
Iter: 940 loss: 1.64424e-06
Iter: 941 loss: 1.61411549e-06
Iter: 942 loss: 1.61277353e-06
Iter: 943 loss: 1.63293271e-06
Iter: 944 loss: 1.61276103e-06
Iter: 945 loss: 1.61134835e-06
Iter: 946 loss: 1.61317121e-06
Iter: 947 loss: 1.61057858e-06
Iter: 948 loss: 1.60951924e-06
Iter: 949 loss: 1.60883008e-06
Iter: 950 loss: 1.60841307e-06
Iter: 951 loss: 1.60718901e-06
Iter: 952 loss: 1.6071524e-06
Iter: 953 loss: 1.60637137e-06
Iter: 954 loss: 1.60442301e-06
Iter: 955 loss: 1.62026799e-06
Iter: 956 loss: 1.60401328e-06
Iter: 957 loss: 1.60212676e-06
Iter: 958 loss: 1.610342e-06
Iter: 959 loss: 1.60177228e-06
Iter: 960 loss: 1.60015838e-06
Iter: 961 loss: 1.61284379e-06
Iter: 962 loss: 1.60011052e-06
Iter: 963 loss: 1.59892227e-06
Iter: 964 loss: 1.60113404e-06
Iter: 965 loss: 1.5984474e-06
Iter: 966 loss: 1.59693036e-06
Iter: 967 loss: 1.60179468e-06
Iter: 968 loss: 1.59649517e-06
Iter: 969 loss: 1.59537956e-06
Iter: 970 loss: 1.59476133e-06
Iter: 971 loss: 1.59425849e-06
Iter: 972 loss: 1.59286174e-06
Iter: 973 loss: 1.592849e-06
Iter: 974 loss: 1.59202204e-06
Iter: 975 loss: 1.5902815e-06
Iter: 976 loss: 1.61780019e-06
Iter: 977 loss: 1.5902292e-06
Iter: 978 loss: 1.58943703e-06
Iter: 979 loss: 1.58921353e-06
Iter: 980 loss: 1.58827493e-06
Iter: 981 loss: 1.58701823e-06
Iter: 982 loss: 1.58693967e-06
Iter: 983 loss: 1.58594753e-06
Iter: 984 loss: 1.58957653e-06
Iter: 985 loss: 1.58568525e-06
Iter: 986 loss: 1.58422324e-06
Iter: 987 loss: 1.59171577e-06
Iter: 988 loss: 1.58405669e-06
Iter: 989 loss: 1.58319563e-06
Iter: 990 loss: 1.58137163e-06
Iter: 991 loss: 1.61129753e-06
Iter: 992 loss: 1.58133867e-06
Iter: 993 loss: 1.57981276e-06
Iter: 994 loss: 1.5995987e-06
Iter: 995 loss: 1.57982458e-06
Iter: 996 loss: 1.57850343e-06
Iter: 997 loss: 1.58011915e-06
Iter: 998 loss: 1.57778732e-06
Iter: 999 loss: 1.57673821e-06
Iter: 1000 loss: 1.58692842e-06
Iter: 1001 loss: 1.57667421e-06
Iter: 1002 loss: 1.57570128e-06
Iter: 1003 loss: 1.57497834e-06
Iter: 1004 loss: 1.57471186e-06
Iter: 1005 loss: 1.57341856e-06
Iter: 1006 loss: 1.57595059e-06
Iter: 1007 loss: 1.57282204e-06
Iter: 1008 loss: 1.57101863e-06
Iter: 1009 loss: 1.5832702e-06
Iter: 1010 loss: 1.57081558e-06
Iter: 1011 loss: 1.56978103e-06
Iter: 1012 loss: 1.56801229e-06
Iter: 1013 loss: 1.56802309e-06
Iter: 1014 loss: 1.56672843e-06
Iter: 1015 loss: 1.56652027e-06
Iter: 1016 loss: 1.5658361e-06
Iter: 1017 loss: 1.564385e-06
Iter: 1018 loss: 1.58985335e-06
Iter: 1019 loss: 1.56438773e-06
Iter: 1020 loss: 1.56368083e-06
Iter: 1021 loss: 1.56355497e-06
Iter: 1022 loss: 1.56271904e-06
Iter: 1023 loss: 1.56120541e-06
Iter: 1024 loss: 1.59496335e-06
Iter: 1025 loss: 1.56119938e-06
Iter: 1026 loss: 1.55979842e-06
Iter: 1027 loss: 1.55962744e-06
Iter: 1028 loss: 1.55868202e-06
Iter: 1029 loss: 1.55680732e-06
Iter: 1030 loss: 1.57306329e-06
Iter: 1031 loss: 1.55673376e-06
Iter: 1032 loss: 1.55542375e-06
Iter: 1033 loss: 1.55513089e-06
Iter: 1034 loss: 1.55432485e-06
Iter: 1035 loss: 1.55317014e-06
Iter: 1036 loss: 1.55317241e-06
Iter: 1037 loss: 1.55225973e-06
Iter: 1038 loss: 1.5514471e-06
Iter: 1039 loss: 1.55123257e-06
Iter: 1040 loss: 1.55006046e-06
Iter: 1041 loss: 1.55730254e-06
Iter: 1042 loss: 1.549947e-06
Iter: 1043 loss: 1.54842667e-06
Iter: 1044 loss: 1.55115276e-06
Iter: 1045 loss: 1.54775432e-06
Iter: 1046 loss: 1.54668055e-06
Iter: 1047 loss: 1.54598297e-06
Iter: 1048 loss: 1.54557256e-06
Iter: 1049 loss: 1.54370082e-06
Iter: 1050 loss: 1.56820897e-06
Iter: 1051 loss: 1.5436849e-06
Iter: 1052 loss: 1.54284953e-06
Iter: 1053 loss: 1.54129054e-06
Iter: 1054 loss: 1.5762223e-06
Iter: 1055 loss: 1.54129589e-06
Iter: 1056 loss: 1.54104828e-06
Iter: 1057 loss: 1.54053168e-06
Iter: 1058 loss: 1.53995643e-06
Iter: 1059 loss: 1.53852284e-06
Iter: 1060 loss: 1.55200951e-06
Iter: 1061 loss: 1.53832491e-06
Iter: 1062 loss: 1.53710334e-06
Iter: 1063 loss: 1.54964869e-06
Iter: 1064 loss: 1.53709061e-06
Iter: 1065 loss: 1.53575536e-06
Iter: 1066 loss: 1.53800306e-06
Iter: 1067 loss: 1.53519977e-06
Iter: 1068 loss: 1.53401584e-06
Iter: 1069 loss: 1.53335793e-06
Iter: 1070 loss: 1.53282213e-06
Iter: 1071 loss: 1.53126882e-06
Iter: 1072 loss: 1.55232885e-06
Iter: 1073 loss: 1.53130316e-06
Iter: 1074 loss: 1.53017822e-06
Iter: 1075 loss: 1.52955033e-06
Iter: 1076 loss: 1.52907046e-06
Iter: 1077 loss: 1.52826908e-06
Iter: 1078 loss: 1.52818791e-06
Iter: 1079 loss: 1.52728262e-06
Iter: 1080 loss: 1.52674852e-06
Iter: 1081 loss: 1.52644157e-06
Iter: 1082 loss: 1.52546158e-06
Iter: 1083 loss: 1.52845314e-06
Iter: 1084 loss: 1.52520261e-06
Iter: 1085 loss: 1.5236825e-06
Iter: 1086 loss: 1.52567145e-06
Iter: 1087 loss: 1.52288965e-06
Iter: 1088 loss: 1.52194514e-06
Iter: 1089 loss: 1.52150346e-06
Iter: 1090 loss: 1.52103166e-06
Iter: 1091 loss: 1.51939366e-06
Iter: 1092 loss: 1.53932115e-06
Iter: 1093 loss: 1.51938275e-06
Iter: 1094 loss: 1.51865925e-06
Iter: 1095 loss: 1.51715199e-06
Iter: 1096 loss: 1.54571717e-06
Iter: 1097 loss: 1.51712334e-06
Iter: 1098 loss: 1.51624488e-06
Iter: 1099 loss: 1.5161836e-06
Iter: 1100 loss: 1.51527843e-06
Iter: 1101 loss: 1.51403981e-06
Iter: 1102 loss: 1.51401468e-06
Iter: 1103 loss: 1.51268625e-06
Iter: 1104 loss: 1.52232838e-06
Iter: 1105 loss: 1.51261838e-06
Iter: 1106 loss: 1.51155348e-06
Iter: 1107 loss: 1.51155837e-06
Iter: 1108 loss: 1.51069685e-06
Iter: 1109 loss: 1.50939172e-06
Iter: 1110 loss: 1.50923097e-06
Iter: 1111 loss: 1.50827736e-06
Iter: 1112 loss: 1.50775236e-06
Iter: 1113 loss: 1.50729738e-06
Iter: 1114 loss: 1.50655524e-06
Iter: 1115 loss: 1.50485255e-06
Iter: 1116 loss: 1.52491896e-06
Iter: 1117 loss: 1.50467599e-06
Iter: 1118 loss: 1.50380856e-06
Iter: 1119 loss: 1.50364235e-06
Iter: 1120 loss: 1.50265771e-06
Iter: 1121 loss: 1.50091978e-06
Iter: 1122 loss: 1.50091375e-06
Iter: 1123 loss: 1.49957225e-06
Iter: 1124 loss: 1.50172946e-06
Iter: 1125 loss: 1.49900188e-06
Iter: 1126 loss: 1.49833215e-06
Iter: 1127 loss: 1.49808727e-06
Iter: 1128 loss: 1.49751986e-06
Iter: 1129 loss: 1.49594678e-06
Iter: 1130 loss: 1.50707035e-06
Iter: 1131 loss: 1.49555831e-06
Iter: 1132 loss: 1.49360494e-06
Iter: 1133 loss: 1.49437733e-06
Iter: 1134 loss: 1.49227162e-06
Iter: 1135 loss: 1.49143386e-06
Iter: 1136 loss: 1.49097104e-06
Iter: 1137 loss: 1.48980428e-06
Iter: 1138 loss: 1.49030188e-06
Iter: 1139 loss: 1.48899471e-06
Iter: 1140 loss: 1.48792503e-06
Iter: 1141 loss: 1.49292578e-06
Iter: 1142 loss: 1.48775223e-06
Iter: 1143 loss: 1.4868624e-06
Iter: 1144 loss: 1.4871157e-06
Iter: 1145 loss: 1.48621302e-06
Iter: 1146 loss: 1.48526624e-06
Iter: 1147 loss: 1.49934465e-06
Iter: 1148 loss: 1.48526942e-06
Iter: 1149 loss: 1.48452727e-06
Iter: 1150 loss: 1.48290405e-06
Iter: 1151 loss: 1.50487131e-06
Iter: 1152 loss: 1.48279878e-06
Iter: 1153 loss: 1.48186973e-06
Iter: 1154 loss: 1.481749e-06
Iter: 1155 loss: 1.48065544e-06
Iter: 1156 loss: 1.47872652e-06
Iter: 1157 loss: 1.47869014e-06
Iter: 1158 loss: 1.4779589e-06
Iter: 1159 loss: 1.47788307e-06
Iter: 1160 loss: 1.47700371e-06
Iter: 1161 loss: 1.47601759e-06
Iter: 1162 loss: 1.47589822e-06
Iter: 1163 loss: 1.4749462e-06
Iter: 1164 loss: 1.47429398e-06
Iter: 1165 loss: 1.4739295e-06
Iter: 1166 loss: 1.47230526e-06
Iter: 1167 loss: 1.47903688e-06
Iter: 1168 loss: 1.47190656e-06
Iter: 1169 loss: 1.47065259e-06
Iter: 1170 loss: 1.47063406e-06
Iter: 1171 loss: 1.46976186e-06
Iter: 1172 loss: 1.46855223e-06
Iter: 1173 loss: 1.46854131e-06
Iter: 1174 loss: 1.46705975e-06
Iter: 1175 loss: 1.47794799e-06
Iter: 1176 loss: 1.46697903e-06
Iter: 1177 loss: 1.46595153e-06
Iter: 1178 loss: 1.47207322e-06
Iter: 1179 loss: 1.46581465e-06
Iter: 1180 loss: 1.46476464e-06
Iter: 1181 loss: 1.4654579e-06
Iter: 1182 loss: 1.46408877e-06
Iter: 1183 loss: 1.46319417e-06
Iter: 1184 loss: 1.46439322e-06
Iter: 1185 loss: 1.46276079e-06
Iter: 1186 loss: 1.46148841e-06
Iter: 1187 loss: 1.47006881e-06
Iter: 1188 loss: 1.46134607e-06
Iter: 1189 loss: 1.46060574e-06
Iter: 1190 loss: 1.45936212e-06
Iter: 1191 loss: 1.45932688e-06
Iter: 1192 loss: 1.45786282e-06
Iter: 1193 loss: 1.45785975e-06
Iter: 1194 loss: 1.45715603e-06
Iter: 1195 loss: 1.4554879e-06
Iter: 1196 loss: 1.47704418e-06
Iter: 1197 loss: 1.45535341e-06
Iter: 1198 loss: 1.45393039e-06
Iter: 1199 loss: 1.45925628e-06
Iter: 1200 loss: 1.45366539e-06
Iter: 1201 loss: 1.45284082e-06
Iter: 1202 loss: 1.45281638e-06
Iter: 1203 loss: 1.45193655e-06
Iter: 1204 loss: 1.45144372e-06
Iter: 1205 loss: 1.45105741e-06
Iter: 1206 loss: 1.44993578e-06
Iter: 1207 loss: 1.45166985e-06
Iter: 1208 loss: 1.44939645e-06
Iter: 1209 loss: 1.44805085e-06
Iter: 1210 loss: 1.45501895e-06
Iter: 1211 loss: 1.44780859e-06
Iter: 1212 loss: 1.44667956e-06
Iter: 1213 loss: 1.45318813e-06
Iter: 1214 loss: 1.44656337e-06
Iter: 1215 loss: 1.44570686e-06
Iter: 1216 loss: 1.44485944e-06
Iter: 1217 loss: 1.44469834e-06
Iter: 1218 loss: 1.444247e-06
Iter: 1219 loss: 1.44407011e-06
Iter: 1220 loss: 1.44357443e-06
Iter: 1221 loss: 1.44245541e-06
Iter: 1222 loss: 1.45555771e-06
Iter: 1223 loss: 1.44231524e-06
Iter: 1224 loss: 1.44160867e-06
Iter: 1225 loss: 1.44153887e-06
Iter: 1226 loss: 1.44070282e-06
Iter: 1227 loss: 1.4389509e-06
Iter: 1228 loss: 1.46708294e-06
Iter: 1229 loss: 1.43890099e-06
Iter: 1230 loss: 1.43731108e-06
Iter: 1231 loss: 1.43803459e-06
Iter: 1232 loss: 1.43620878e-06
Iter: 1233 loss: 1.43483305e-06
Iter: 1234 loss: 1.43481429e-06
Iter: 1235 loss: 1.433855e-06
Iter: 1236 loss: 1.44462524e-06
Iter: 1237 loss: 1.43380669e-06
Iter: 1238 loss: 1.43333818e-06
Iter: 1239 loss: 1.43240345e-06
Iter: 1240 loss: 1.45283116e-06
Iter: 1241 loss: 1.43240254e-06
Iter: 1242 loss: 1.43128432e-06
Iter: 1243 loss: 1.44063097e-06
Iter: 1244 loss: 1.43122008e-06
Iter: 1245 loss: 1.43033151e-06
Iter: 1246 loss: 1.43326929e-06
Iter: 1247 loss: 1.43008413e-06
Iter: 1248 loss: 1.42909926e-06
Iter: 1249 loss: 1.4292508e-06
Iter: 1250 loss: 1.42836507e-06
Iter: 1251 loss: 1.42735985e-06
Iter: 1252 loss: 1.43368061e-06
Iter: 1253 loss: 1.42719205e-06
Iter: 1254 loss: 1.42611748e-06
Iter: 1255 loss: 1.42773774e-06
Iter: 1256 loss: 1.42553313e-06
Iter: 1257 loss: 1.42480667e-06
Iter: 1258 loss: 1.42617068e-06
Iter: 1259 loss: 1.42442605e-06
Iter: 1260 loss: 1.42341901e-06
Iter: 1261 loss: 1.42920817e-06
Iter: 1262 loss: 1.42328281e-06
Iter: 1263 loss: 1.42278543e-06
Iter: 1264 loss: 1.42140152e-06
Iter: 1265 loss: 1.43056263e-06
Iter: 1266 loss: 1.42100794e-06
Iter: 1267 loss: 1.41917963e-06
Iter: 1268 loss: 1.42813269e-06
Iter: 1269 loss: 1.41886699e-06
Iter: 1270 loss: 1.41760711e-06
Iter: 1271 loss: 1.41754526e-06
Iter: 1272 loss: 1.41659211e-06
Iter: 1273 loss: 1.41525823e-06
Iter: 1274 loss: 1.41517478e-06
Iter: 1275 loss: 1.41410624e-06
Iter: 1276 loss: 1.42582826e-06
Iter: 1277 loss: 1.41406281e-06
Iter: 1278 loss: 1.41313535e-06
Iter: 1279 loss: 1.41688315e-06
Iter: 1280 loss: 1.41294095e-06
Iter: 1281 loss: 1.41214593e-06
Iter: 1282 loss: 1.41473333e-06
Iter: 1283 loss: 1.41192686e-06
Iter: 1284 loss: 1.41128976e-06
Iter: 1285 loss: 1.41113605e-06
Iter: 1286 loss: 1.41072576e-06
Iter: 1287 loss: 1.40959969e-06
Iter: 1288 loss: 1.41767418e-06
Iter: 1289 loss: 1.40952795e-06
Iter: 1290 loss: 1.40869747e-06
Iter: 1291 loss: 1.40751581e-06
Iter: 1292 loss: 1.40748932e-06
Iter: 1293 loss: 1.40706334e-06
Iter: 1294 loss: 1.40672796e-06
Iter: 1295 loss: 1.40625161e-06
Iter: 1296 loss: 1.40498776e-06
Iter: 1297 loss: 1.41429882e-06
Iter: 1298 loss: 1.40469433e-06
Iter: 1299 loss: 1.40342615e-06
Iter: 1300 loss: 1.4074966e-06
Iter: 1301 loss: 1.40306224e-06
Iter: 1302 loss: 1.4025635e-06
Iter: 1303 loss: 1.40239558e-06
Iter: 1304 loss: 1.40174905e-06
Iter: 1305 loss: 1.4007569e-06
Iter: 1306 loss: 1.40071222e-06
Iter: 1307 loss: 1.39953784e-06
Iter: 1308 loss: 1.40145039e-06
Iter: 1309 loss: 1.3990491e-06
Iter: 1310 loss: 1.39783742e-06
Iter: 1311 loss: 1.41239389e-06
Iter: 1312 loss: 1.39781901e-06
Iter: 1313 loss: 1.39702104e-06
Iter: 1314 loss: 1.39877648e-06
Iter: 1315 loss: 1.39674853e-06
Iter: 1316 loss: 1.39593851e-06
Iter: 1317 loss: 1.39598876e-06
Iter: 1318 loss: 1.39529902e-06
Iter: 1319 loss: 1.39452902e-06
Iter: 1320 loss: 1.39453437e-06
Iter: 1321 loss: 1.39391841e-06
Iter: 1322 loss: 1.39316262e-06
Iter: 1323 loss: 1.39313079e-06
Iter: 1324 loss: 1.39242331e-06
Iter: 1325 loss: 1.39242854e-06
Iter: 1326 loss: 1.39165672e-06
Iter: 1327 loss: 1.39016913e-06
Iter: 1328 loss: 1.41906912e-06
Iter: 1329 loss: 1.39014628e-06
Iter: 1330 loss: 1.38886389e-06
Iter: 1331 loss: 1.39016856e-06
Iter: 1332 loss: 1.38813664e-06
Iter: 1333 loss: 1.38706866e-06
Iter: 1334 loss: 1.40045654e-06
Iter: 1335 loss: 1.38709265e-06
Iter: 1336 loss: 1.38595317e-06
Iter: 1337 loss: 1.39022325e-06
Iter: 1338 loss: 1.38569965e-06
Iter: 1339 loss: 1.38502458e-06
Iter: 1340 loss: 1.38433052e-06
Iter: 1341 loss: 1.38426e-06
Iter: 1342 loss: 1.38327277e-06
Iter: 1343 loss: 1.39561973e-06
Iter: 1344 loss: 1.38322775e-06
Iter: 1345 loss: 1.38243502e-06
Iter: 1346 loss: 1.38275391e-06
Iter: 1347 loss: 1.38178143e-06
Iter: 1348 loss: 1.38063524e-06
Iter: 1349 loss: 1.38364408e-06
Iter: 1350 loss: 1.38028804e-06
Iter: 1351 loss: 1.37946085e-06
Iter: 1352 loss: 1.38611927e-06
Iter: 1353 loss: 1.37938218e-06
Iter: 1354 loss: 1.37860889e-06
Iter: 1355 loss: 1.37875031e-06
Iter: 1356 loss: 1.37803193e-06
Iter: 1357 loss: 1.37726806e-06
Iter: 1358 loss: 1.38011831e-06
Iter: 1359 loss: 1.37712391e-06
Iter: 1360 loss: 1.37612153e-06
Iter: 1361 loss: 1.37775669e-06
Iter: 1362 loss: 1.37569759e-06
Iter: 1363 loss: 1.37495135e-06
Iter: 1364 loss: 1.37343216e-06
Iter: 1365 loss: 1.39950612e-06
Iter: 1366 loss: 1.37338589e-06
Iter: 1367 loss: 1.37164125e-06
Iter: 1368 loss: 1.37883603e-06
Iter: 1369 loss: 1.37127086e-06
Iter: 1370 loss: 1.3707878e-06
Iter: 1371 loss: 1.37043844e-06
Iter: 1372 loss: 1.36986569e-06
Iter: 1373 loss: 1.36854078e-06
Iter: 1374 loss: 1.38384758e-06
Iter: 1375 loss: 1.36846393e-06
Iter: 1376 loss: 1.3675342e-06
Iter: 1377 loss: 1.36751169e-06
Iter: 1378 loss: 1.36667154e-06
Iter: 1379 loss: 1.3686647e-06
Iter: 1380 loss: 1.36635549e-06
Iter: 1381 loss: 1.36559265e-06
Iter: 1382 loss: 1.36765073e-06
Iter: 1383 loss: 1.36533504e-06
Iter: 1384 loss: 1.36460255e-06
Iter: 1385 loss: 1.36669223e-06
Iter: 1386 loss: 1.36433368e-06
Iter: 1387 loss: 1.36345511e-06
Iter: 1388 loss: 1.366486e-06
Iter: 1389 loss: 1.36325e-06
Iter: 1390 loss: 1.36254585e-06
Iter: 1391 loss: 1.36203096e-06
Iter: 1392 loss: 1.36175845e-06
Iter: 1393 loss: 1.3609681e-06
Iter: 1394 loss: 1.36091433e-06
Iter: 1395 loss: 1.36048675e-06
Iter: 1396 loss: 1.35930077e-06
Iter: 1397 loss: 1.36843198e-06
Iter: 1398 loss: 1.35906475e-06
Iter: 1399 loss: 1.35766913e-06
Iter: 1400 loss: 1.35946948e-06
Iter: 1401 loss: 1.35694472e-06
Iter: 1402 loss: 1.3563839e-06
Iter: 1403 loss: 1.35617665e-06
Iter: 1404 loss: 1.35526761e-06
Iter: 1405 loss: 1.354701e-06
Iter: 1406 loss: 1.35434948e-06
Iter: 1407 loss: 1.35332994e-06
Iter: 1408 loss: 1.35307778e-06
Iter: 1409 loss: 1.35246842e-06
Iter: 1410 loss: 1.35142386e-06
Iter: 1411 loss: 1.35136168e-06
Iter: 1412 loss: 1.35070502e-06
Iter: 1413 loss: 1.35026039e-06
Iter: 1414 loss: 1.35001028e-06
Iter: 1415 loss: 1.34886318e-06
Iter: 1416 loss: 1.35313212e-06
Iter: 1417 loss: 1.34857964e-06
Iter: 1418 loss: 1.34767356e-06
Iter: 1419 loss: 1.35516382e-06
Iter: 1420 loss: 1.34759375e-06
Iter: 1421 loss: 1.34693562e-06
Iter: 1422 loss: 1.3464803e-06
Iter: 1423 loss: 1.34620473e-06
Iter: 1424 loss: 1.34569882e-06
Iter: 1425 loss: 1.3456505e-06
Iter: 1426 loss: 1.34514676e-06
Iter: 1427 loss: 1.3440731e-06
Iter: 1428 loss: 1.36093786e-06
Iter: 1429 loss: 1.34403183e-06
Iter: 1430 loss: 1.34292759e-06
Iter: 1431 loss: 1.34312859e-06
Iter: 1432 loss: 1.34207494e-06
Iter: 1433 loss: 1.34055779e-06
Iter: 1434 loss: 1.34468519e-06
Iter: 1435 loss: 1.34003665e-06
Iter: 1436 loss: 1.33925948e-06
Iter: 1437 loss: 1.33901199e-06
Iter: 1438 loss: 1.33852177e-06
Iter: 1439 loss: 1.33746744e-06
Iter: 1440 loss: 1.35612208e-06
Iter: 1441 loss: 1.33745857e-06
Iter: 1442 loss: 1.3367569e-06
Iter: 1443 loss: 1.34698689e-06
Iter: 1444 loss: 1.33675189e-06
Iter: 1445 loss: 1.33599929e-06
Iter: 1446 loss: 1.33671313e-06
Iter: 1447 loss: 1.3355384e-06
Iter: 1448 loss: 1.33473941e-06
Iter: 1449 loss: 1.33653543e-06
Iter: 1450 loss: 1.33444837e-06
Iter: 1451 loss: 1.3335698e-06
Iter: 1452 loss: 1.33635297e-06
Iter: 1453 loss: 1.33326398e-06
Iter: 1454 loss: 1.3322308e-06
Iter: 1455 loss: 1.33439994e-06
Iter: 1456 loss: 1.33181e-06
Iter: 1457 loss: 1.33102822e-06
Iter: 1458 loss: 1.3334751e-06
Iter: 1459 loss: 1.33081721e-06
Iter: 1460 loss: 1.32976686e-06
Iter: 1461 loss: 1.3328355e-06
Iter: 1462 loss: 1.32947912e-06
Iter: 1463 loss: 1.32885248e-06
Iter: 1464 loss: 1.32799528e-06
Iter: 1465 loss: 1.32797027e-06
Iter: 1466 loss: 1.3268351e-06
Iter: 1467 loss: 1.32749312e-06
Iter: 1468 loss: 1.32609546e-06
Iter: 1469 loss: 1.32506625e-06
Iter: 1470 loss: 1.32506045e-06
Iter: 1471 loss: 1.32386606e-06
Iter: 1472 loss: 1.324496e-06
Iter: 1473 loss: 1.32307548e-06
Iter: 1474 loss: 1.32222203e-06
Iter: 1475 loss: 1.32145692e-06
Iter: 1476 loss: 1.32129082e-06
Iter: 1477 loss: 1.32091191e-06
Iter: 1478 loss: 1.32056653e-06
Iter: 1479 loss: 1.32011769e-06
Iter: 1480 loss: 1.31956892e-06
Iter: 1481 loss: 1.31951583e-06
Iter: 1482 loss: 1.31876664e-06
Iter: 1483 loss: 1.32469825e-06
Iter: 1484 loss: 1.31877e-06
Iter: 1485 loss: 1.31803927e-06
Iter: 1486 loss: 1.31842057e-06
Iter: 1487 loss: 1.31752313e-06
Iter: 1488 loss: 1.31646539e-06
Iter: 1489 loss: 1.31754075e-06
Iter: 1490 loss: 1.31591878e-06
Iter: 1491 loss: 1.31524394e-06
Iter: 1492 loss: 1.31525599e-06
Iter: 1493 loss: 1.31472791e-06
Iter: 1494 loss: 1.31357797e-06
Iter: 1495 loss: 1.33300364e-06
Iter: 1496 loss: 1.31355023e-06
Iter: 1497 loss: 1.31255024e-06
Iter: 1498 loss: 1.31377249e-06
Iter: 1499 loss: 1.31203456e-06
Iter: 1500 loss: 1.31100967e-06
Iter: 1501 loss: 1.31264e-06
Iter: 1502 loss: 1.31054253e-06
Iter: 1503 loss: 1.31003662e-06
Iter: 1504 loss: 1.30982494e-06
Iter: 1505 loss: 1.30924514e-06
Iter: 1506 loss: 1.30794319e-06
Iter: 1507 loss: 1.32731873e-06
Iter: 1508 loss: 1.30787748e-06
Iter: 1509 loss: 1.30694275e-06
Iter: 1510 loss: 1.31699039e-06
Iter: 1511 loss: 1.30689955e-06
Iter: 1512 loss: 1.30614512e-06
Iter: 1513 loss: 1.3107358e-06
Iter: 1514 loss: 1.30606259e-06
Iter: 1515 loss: 1.30555111e-06
Iter: 1516 loss: 1.30511899e-06
Iter: 1517 loss: 1.30499723e-06
Iter: 1518 loss: 1.30431977e-06
Iter: 1519 loss: 1.3043225e-06
Iter: 1520 loss: 1.30382796e-06
Iter: 1521 loss: 1.30335798e-06
Iter: 1522 loss: 1.30328135e-06
Iter: 1523 loss: 1.30235139e-06
Iter: 1524 loss: 1.3060087e-06
Iter: 1525 loss: 1.30216085e-06
Iter: 1526 loss: 1.30129763e-06
Iter: 1527 loss: 1.30562069e-06
Iter: 1528 loss: 1.30114142e-06
Iter: 1529 loss: 1.30035619e-06
Iter: 1530 loss: 1.29892578e-06
Iter: 1531 loss: 1.33154072e-06
Iter: 1532 loss: 1.29891805e-06
Iter: 1533 loss: 1.29777197e-06
Iter: 1534 loss: 1.3020242e-06
Iter: 1535 loss: 1.29755585e-06
Iter: 1536 loss: 1.29665273e-06
Iter: 1537 loss: 1.29776049e-06
Iter: 1538 loss: 1.29615592e-06
Iter: 1539 loss: 1.29578052e-06
Iter: 1540 loss: 1.29553575e-06
Iter: 1541 loss: 1.29510295e-06
Iter: 1542 loss: 1.29413229e-06
Iter: 1543 loss: 1.30784042e-06
Iter: 1544 loss: 1.29407476e-06
Iter: 1545 loss: 1.29324462e-06
Iter: 1546 loss: 1.30106696e-06
Iter: 1547 loss: 1.2932278e-06
Iter: 1548 loss: 1.29229534e-06
Iter: 1549 loss: 1.29428611e-06
Iter: 1550 loss: 1.29195905e-06
Iter: 1551 loss: 1.2914096e-06
Iter: 1552 loss: 1.29107559e-06
Iter: 1553 loss: 1.29087607e-06
Iter: 1554 loss: 1.29045372e-06
Iter: 1555 loss: 1.29038801e-06
Iter: 1556 loss: 1.28997169e-06
Iter: 1557 loss: 1.28930537e-06
Iter: 1558 loss: 1.30578644e-06
Iter: 1559 loss: 1.28929014e-06
Iter: 1560 loss: 1.2886137e-06
Iter: 1561 loss: 1.2978478e-06
Iter: 1562 loss: 1.28861006e-06
Iter: 1563 loss: 1.287999e-06
Iter: 1564 loss: 1.28845886e-06
Iter: 1565 loss: 1.28758052e-06
Iter: 1566 loss: 1.28685713e-06
Iter: 1567 loss: 1.28686179e-06
Iter: 1568 loss: 1.28621161e-06
Iter: 1569 loss: 1.28523266e-06
Iter: 1570 loss: 1.28501983e-06
Iter: 1571 loss: 1.28434e-06
Iter: 1572 loss: 1.28316765e-06
Iter: 1573 loss: 1.28851423e-06
Iter: 1574 loss: 1.28291731e-06
Iter: 1575 loss: 1.28258182e-06
Iter: 1576 loss: 1.28236115e-06
Iter: 1577 loss: 1.28198201e-06
Iter: 1578 loss: 1.28097804e-06
Iter: 1579 loss: 1.28792692e-06
Iter: 1580 loss: 1.28075908e-06
Iter: 1581 loss: 1.2800831e-06
Iter: 1582 loss: 1.28004604e-06
Iter: 1583 loss: 1.27934436e-06
Iter: 1584 loss: 1.27915507e-06
Iter: 1585 loss: 1.27861153e-06
Iter: 1586 loss: 1.27787916e-06
Iter: 1587 loss: 1.27787894e-06
Iter: 1588 loss: 1.2772897e-06
Iter: 1589 loss: 1.27660167e-06
Iter: 1590 loss: 1.27656881e-06
Iter: 1591 loss: 1.27604471e-06
Iter: 1592 loss: 1.27515989e-06
Iter: 1593 loss: 1.29573289e-06
Iter: 1594 loss: 1.27516046e-06
Iter: 1595 loss: 1.27455075e-06
Iter: 1596 loss: 1.2745511e-06
Iter: 1597 loss: 1.27394856e-06
Iter: 1598 loss: 1.27492683e-06
Iter: 1599 loss: 1.27365297e-06
Iter: 1600 loss: 1.27307203e-06
Iter: 1601 loss: 1.27326598e-06
Iter: 1602 loss: 1.27265457e-06
Iter: 1603 loss: 1.27181465e-06
Iter: 1604 loss: 1.27208409e-06
Iter: 1605 loss: 1.27125452e-06
Iter: 1606 loss: 1.27033809e-06
Iter: 1607 loss: 1.27668216e-06
Iter: 1608 loss: 1.27028829e-06
Iter: 1609 loss: 1.26925397e-06
Iter: 1610 loss: 1.2723815e-06
Iter: 1611 loss: 1.26897407e-06
Iter: 1612 loss: 1.26838643e-06
Iter: 1613 loss: 1.26742009e-06
Iter: 1614 loss: 1.26743521e-06
Iter: 1615 loss: 1.26691293e-06
Iter: 1616 loss: 1.26670352e-06
Iter: 1617 loss: 1.26621399e-06
Iter: 1618 loss: 1.26512123e-06
Iter: 1619 loss: 1.28220506e-06
Iter: 1620 loss: 1.26510781e-06
Iter: 1621 loss: 1.26425675e-06
Iter: 1622 loss: 1.26993359e-06
Iter: 1623 loss: 1.26413488e-06
Iter: 1624 loss: 1.26309931e-06
Iter: 1625 loss: 1.2675664e-06
Iter: 1626 loss: 1.26292434e-06
Iter: 1627 loss: 1.2621922e-06
Iter: 1628 loss: 1.26081636e-06
Iter: 1629 loss: 1.29009015e-06
Iter: 1630 loss: 1.26079726e-06
Iter: 1631 loss: 1.26004397e-06
Iter: 1632 loss: 1.25996257e-06
Iter: 1633 loss: 1.25907297e-06
Iter: 1634 loss: 1.25948418e-06
Iter: 1635 loss: 1.2584544e-06
Iter: 1636 loss: 1.25769861e-06
Iter: 1637 loss: 1.25895735e-06
Iter: 1638 loss: 1.25736551e-06
Iter: 1639 loss: 1.2564916e-06
Iter: 1640 loss: 1.25761778e-06
Iter: 1641 loss: 1.25606084e-06
Iter: 1642 loss: 1.2552407e-06
Iter: 1643 loss: 1.25503402e-06
Iter: 1644 loss: 1.25445331e-06
Iter: 1645 loss: 1.25401755e-06
Iter: 1646 loss: 1.25383383e-06
Iter: 1647 loss: 1.25316922e-06
Iter: 1648 loss: 1.25229565e-06
Iter: 1649 loss: 1.25226904e-06
Iter: 1650 loss: 1.25126826e-06
Iter: 1651 loss: 1.25358702e-06
Iter: 1652 loss: 1.25092924e-06
Iter: 1653 loss: 1.24988605e-06
Iter: 1654 loss: 1.26233488e-06
Iter: 1655 loss: 1.24986491e-06
Iter: 1656 loss: 1.24949747e-06
Iter: 1657 loss: 1.24875032e-06
Iter: 1658 loss: 1.26516682e-06
Iter: 1659 loss: 1.24872918e-06
Iter: 1660 loss: 1.24834492e-06
Iter: 1661 loss: 1.2482692e-06
Iter: 1662 loss: 1.24778705e-06
Iter: 1663 loss: 1.24672988e-06
Iter: 1664 loss: 1.26023622e-06
Iter: 1665 loss: 1.24665257e-06
Iter: 1666 loss: 1.24560552e-06
Iter: 1667 loss: 1.24748055e-06
Iter: 1668 loss: 1.24514725e-06
Iter: 1669 loss: 1.24445091e-06
Iter: 1670 loss: 1.24437076e-06
Iter: 1671 loss: 1.243817e-06
Iter: 1672 loss: 1.24275198e-06
Iter: 1673 loss: 1.26366513e-06
Iter: 1674 loss: 1.24272844e-06
Iter: 1675 loss: 1.24188955e-06
Iter: 1676 loss: 1.24815801e-06
Iter: 1677 loss: 1.24183316e-06
Iter: 1678 loss: 1.2410959e-06
Iter: 1679 loss: 1.24111966e-06
Iter: 1680 loss: 1.24051815e-06
Iter: 1681 loss: 1.23979885e-06
Iter: 1682 loss: 1.24706344e-06
Iter: 1683 loss: 1.23977657e-06
Iter: 1684 loss: 1.23897826e-06
Iter: 1685 loss: 1.24105452e-06
Iter: 1686 loss: 1.2386804e-06
Iter: 1687 loss: 1.23804148e-06
Iter: 1688 loss: 1.23669133e-06
Iter: 1689 loss: 1.26090322e-06
Iter: 1690 loss: 1.2367143e-06
Iter: 1691 loss: 1.2361636e-06
Iter: 1692 loss: 1.23597624e-06
Iter: 1693 loss: 1.23527548e-06
Iter: 1694 loss: 1.23584459e-06
Iter: 1695 loss: 1.23482323e-06
Iter: 1696 loss: 1.23440304e-06
Iter: 1697 loss: 1.23714244e-06
Iter: 1698 loss: 1.23435586e-06
Iter: 1699 loss: 1.23375912e-06
Iter: 1700 loss: 1.23332393e-06
Iter: 1701 loss: 1.23312589e-06
Iter: 1702 loss: 1.23245445e-06
Iter: 1703 loss: 1.23194422e-06
Iter: 1704 loss: 1.23175516e-06
Iter: 1705 loss: 1.23123993e-06
Iter: 1706 loss: 1.23105031e-06
Iter: 1707 loss: 1.23061091e-06
Iter: 1708 loss: 1.2297121e-06
Iter: 1709 loss: 1.24613484e-06
Iter: 1710 loss: 1.229693e-06
Iter: 1711 loss: 1.22889469e-06
Iter: 1712 loss: 1.23431573e-06
Iter: 1713 loss: 1.22883091e-06
Iter: 1714 loss: 1.22803112e-06
Iter: 1715 loss: 1.2280293e-06
Iter: 1716 loss: 1.22742506e-06
Iter: 1717 loss: 1.22680422e-06
Iter: 1718 loss: 1.23491805e-06
Iter: 1719 loss: 1.22679774e-06
Iter: 1720 loss: 1.22601318e-06
Iter: 1721 loss: 1.22666279e-06
Iter: 1722 loss: 1.22553661e-06
Iter: 1723 loss: 1.22491076e-06
Iter: 1724 loss: 1.22516053e-06
Iter: 1725 loss: 1.22440952e-06
Iter: 1726 loss: 1.22379356e-06
Iter: 1727 loss: 1.22379402e-06
Iter: 1728 loss: 1.22332517e-06
Iter: 1729 loss: 1.22270058e-06
Iter: 1730 loss: 1.22270876e-06
Iter: 1731 loss: 1.22194047e-06
Iter: 1732 loss: 1.2336418e-06
Iter: 1733 loss: 1.22192796e-06
Iter: 1734 loss: 1.22151619e-06
Iter: 1735 loss: 1.22041206e-06
Iter: 1736 loss: 1.22954157e-06
Iter: 1737 loss: 1.22026154e-06
Iter: 1738 loss: 1.21890514e-06
Iter: 1739 loss: 1.22014058e-06
Iter: 1740 loss: 1.21812832e-06
Iter: 1741 loss: 1.21729124e-06
Iter: 1742 loss: 1.21725134e-06
Iter: 1743 loss: 1.21634343e-06
Iter: 1744 loss: 1.22052802e-06
Iter: 1745 loss: 1.21615665e-06
Iter: 1746 loss: 1.21570088e-06
Iter: 1747 loss: 1.21474932e-06
Iter: 1748 loss: 1.23242421e-06
Iter: 1749 loss: 1.21470976e-06
Iter: 1750 loss: 1.21360665e-06
Iter: 1751 loss: 1.21575818e-06
Iter: 1752 loss: 1.21315759e-06
Iter: 1753 loss: 1.21228e-06
Iter: 1754 loss: 1.21227731e-06
Iter: 1755 loss: 1.21163862e-06
Iter: 1756 loss: 1.21400285e-06
Iter: 1757 loss: 1.21145649e-06
Iter: 1758 loss: 1.21072901e-06
Iter: 1759 loss: 1.21044548e-06
Iter: 1760 loss: 1.21006769e-06
Iter: 1761 loss: 1.20947789e-06
Iter: 1762 loss: 1.21331016e-06
Iter: 1763 loss: 1.20941877e-06
Iter: 1764 loss: 1.20861864e-06
Iter: 1765 loss: 1.20975267e-06
Iter: 1766 loss: 1.20826871e-06
Iter: 1767 loss: 1.2077702e-06
Iter: 1768 loss: 1.20877473e-06
Iter: 1769 loss: 1.20751145e-06
Iter: 1770 loss: 1.20671382e-06
Iter: 1771 loss: 1.20841935e-06
Iter: 1772 loss: 1.20636071e-06
Iter: 1773 loss: 1.20568802e-06
Iter: 1774 loss: 1.20428103e-06
Iter: 1775 loss: 1.22781489e-06
Iter: 1776 loss: 1.20420395e-06
Iter: 1777 loss: 1.20367713e-06
Iter: 1778 loss: 1.20351024e-06
Iter: 1779 loss: 1.20278924e-06
Iter: 1780 loss: 1.20477034e-06
Iter: 1781 loss: 1.20258278e-06
Iter: 1782 loss: 1.20215577e-06
Iter: 1783 loss: 1.2012149e-06
Iter: 1784 loss: 1.21220626e-06
Iter: 1785 loss: 1.20112736e-06
Iter: 1786 loss: 1.20017523e-06
Iter: 1787 loss: 1.20641505e-06
Iter: 1788 loss: 1.20006109e-06
Iter: 1789 loss: 1.19912443e-06
Iter: 1790 loss: 1.2037608e-06
Iter: 1791 loss: 1.19896515e-06
Iter: 1792 loss: 1.19821163e-06
Iter: 1793 loss: 1.20154277e-06
Iter: 1794 loss: 1.19809101e-06
Iter: 1795 loss: 1.19730009e-06
Iter: 1796 loss: 1.19675792e-06
Iter: 1797 loss: 1.19649121e-06
Iter: 1798 loss: 1.19589708e-06
Iter: 1799 loss: 1.19590788e-06
Iter: 1800 loss: 1.19539391e-06
Iter: 1801 loss: 1.19536901e-06
Iter: 1802 loss: 1.19498e-06
Iter: 1803 loss: 1.19444e-06
Iter: 1804 loss: 1.19792844e-06
Iter: 1805 loss: 1.19437129e-06
Iter: 1806 loss: 1.19375659e-06
Iter: 1807 loss: 1.19265292e-06
Iter: 1808 loss: 1.19264109e-06
Iter: 1809 loss: 1.19146443e-06
Iter: 1810 loss: 1.19154902e-06
Iter: 1811 loss: 1.19053129e-06
Iter: 1812 loss: 1.19058154e-06
Iter: 1813 loss: 1.19000515e-06
Iter: 1814 loss: 1.18950175e-06
Iter: 1815 loss: 1.18871367e-06
Iter: 1816 loss: 1.1886948e-06
Iter: 1817 loss: 1.18801904e-06
Iter: 1818 loss: 1.18763e-06
Iter: 1819 loss: 1.18733419e-06
Iter: 1820 loss: 1.1866648e-06
Iter: 1821 loss: 1.18664502e-06
Iter: 1822 loss: 1.18595699e-06
Iter: 1823 loss: 1.18618868e-06
Iter: 1824 loss: 1.18545734e-06
Iter: 1825 loss: 1.18450589e-06
Iter: 1826 loss: 1.18797698e-06
Iter: 1827 loss: 1.18430137e-06
Iter: 1828 loss: 1.18349817e-06
Iter: 1829 loss: 1.18295293e-06
Iter: 1830 loss: 1.18264313e-06
Iter: 1831 loss: 1.18227672e-06
Iter: 1832 loss: 1.18206924e-06
Iter: 1833 loss: 1.1816694e-06
Iter: 1834 loss: 1.18086416e-06
Iter: 1835 loss: 1.19846686e-06
Iter: 1836 loss: 1.18085734e-06
Iter: 1837 loss: 1.18044261e-06
Iter: 1838 loss: 1.18035882e-06
Iter: 1839 loss: 1.17998729e-06
Iter: 1840 loss: 1.17914306e-06
Iter: 1841 loss: 1.19172e-06
Iter: 1842 loss: 1.1790994e-06
Iter: 1843 loss: 1.17818308e-06
Iter: 1844 loss: 1.17899515e-06
Iter: 1845 loss: 1.1776574e-06
Iter: 1846 loss: 1.17734407e-06
Iter: 1847 loss: 1.17707373e-06
Iter: 1848 loss: 1.17652621e-06
Iter: 1849 loss: 1.1752968e-06
Iter: 1850 loss: 1.18976777e-06
Iter: 1851 loss: 1.17515344e-06
Iter: 1852 loss: 1.1742776e-06
Iter: 1853 loss: 1.17573529e-06
Iter: 1854 loss: 1.17384866e-06
Iter: 1855 loss: 1.17335696e-06
Iter: 1856 loss: 1.17323066e-06
Iter: 1857 loss: 1.17268132e-06
Iter: 1858 loss: 1.17255195e-06
Iter: 1859 loss: 1.17218815e-06
Iter: 1860 loss: 1.17144077e-06
Iter: 1861 loss: 1.17305308e-06
Iter: 1862 loss: 1.17114564e-06
Iter: 1863 loss: 1.17033983e-06
Iter: 1864 loss: 1.17101558e-06
Iter: 1865 loss: 1.1698537e-06
Iter: 1866 loss: 1.16895615e-06
Iter: 1867 loss: 1.18128969e-06
Iter: 1868 loss: 1.16897263e-06
Iter: 1869 loss: 1.16844762e-06
Iter: 1870 loss: 1.16785714e-06
Iter: 1871 loss: 1.16781212e-06
Iter: 1872 loss: 1.16702336e-06
Iter: 1873 loss: 1.17859565e-06
Iter: 1874 loss: 1.16701688e-06
Iter: 1875 loss: 1.16658089e-06
Iter: 1876 loss: 1.1656897e-06
Iter: 1877 loss: 1.17783713e-06
Iter: 1878 loss: 1.16563308e-06
Iter: 1879 loss: 1.1652412e-06
Iter: 1880 loss: 1.16510409e-06
Iter: 1881 loss: 1.16457215e-06
Iter: 1882 loss: 1.16418664e-06
Iter: 1883 loss: 1.16400167e-06
Iter: 1884 loss: 1.16343074e-06
Iter: 1885 loss: 1.16261663e-06
Iter: 1886 loss: 1.16255455e-06
Iter: 1887 loss: 1.16169895e-06
Iter: 1888 loss: 1.1753018e-06
Iter: 1889 loss: 1.1617218e-06
Iter: 1890 loss: 1.16099011e-06
Iter: 1891 loss: 1.164389e-06
Iter: 1892 loss: 1.16087165e-06
Iter: 1893 loss: 1.16028173e-06
Iter: 1894 loss: 1.16017657e-06
Iter: 1895 loss: 1.15975263e-06
Iter: 1896 loss: 1.1588927e-06
Iter: 1897 loss: 1.160784e-06
Iter: 1898 loss: 1.15853845e-06
Iter: 1899 loss: 1.15786838e-06
Iter: 1900 loss: 1.16671845e-06
Iter: 1901 loss: 1.15785747e-06
Iter: 1902 loss: 1.15727028e-06
Iter: 1903 loss: 1.15686521e-06
Iter: 1904 loss: 1.1566251e-06
Iter: 1905 loss: 1.15603098e-06
Iter: 1906 loss: 1.16391516e-06
Iter: 1907 loss: 1.1560436e-06
Iter: 1908 loss: 1.15543207e-06
Iter: 1909 loss: 1.15462399e-06
Iter: 1910 loss: 1.15456862e-06
Iter: 1911 loss: 1.15376008e-06
Iter: 1912 loss: 1.15669968e-06
Iter: 1913 loss: 1.15359467e-06
Iter: 1914 loss: 1.15261901e-06
Iter: 1915 loss: 1.15936245e-06
Iter: 1916 loss: 1.15250793e-06
Iter: 1917 loss: 1.15196644e-06
Iter: 1918 loss: 1.15091291e-06
Iter: 1919 loss: 1.17006618e-06
Iter: 1920 loss: 1.15089051e-06
Iter: 1921 loss: 1.14998318e-06
Iter: 1922 loss: 1.15687931e-06
Iter: 1923 loss: 1.14990166e-06
Iter: 1924 loss: 1.14925649e-06
Iter: 1925 loss: 1.15866987e-06
Iter: 1926 loss: 1.14927684e-06
Iter: 1927 loss: 1.1488163e-06
Iter: 1928 loss: 1.14874911e-06
Iter: 1929 loss: 1.14843579e-06
Iter: 1930 loss: 1.14782688e-06
Iter: 1931 loss: 1.14891145e-06
Iter: 1932 loss: 1.14753857e-06
Iter: 1933 loss: 1.14689703e-06
Iter: 1934 loss: 1.14981094e-06
Iter: 1935 loss: 1.14679187e-06
Iter: 1936 loss: 1.14607087e-06
Iter: 1937 loss: 1.1483512e-06
Iter: 1938 loss: 1.14589125e-06
Iter: 1939 loss: 1.14536033e-06
Iter: 1940 loss: 1.14613135e-06
Iter: 1941 loss: 1.14513045e-06
Iter: 1942 loss: 1.14441332e-06
Iter: 1943 loss: 1.14642376e-06
Iter: 1944 loss: 1.14417367e-06
Iter: 1945 loss: 1.14376553e-06
Iter: 1946 loss: 1.14333977e-06
Iter: 1947 loss: 1.14326497e-06
Iter: 1948 loss: 1.14279214e-06
Iter: 1949 loss: 1.1427253e-06
Iter: 1950 loss: 1.14238276e-06
Iter: 1951 loss: 1.1415159e-06
Iter: 1952 loss: 1.14851116e-06
Iter: 1953 loss: 1.14136049e-06
Iter: 1954 loss: 1.14031491e-06
Iter: 1955 loss: 1.1411064e-06
Iter: 1956 loss: 1.13967201e-06
Iter: 1957 loss: 1.13950387e-06
Iter: 1958 loss: 1.13905492e-06
Iter: 1959 loss: 1.1385647e-06
Iter: 1960 loss: 1.13820909e-06
Iter: 1961 loss: 1.13802514e-06
Iter: 1962 loss: 1.13739e-06
Iter: 1963 loss: 1.14048771e-06
Iter: 1964 loss: 1.13729698e-06
Iter: 1965 loss: 1.13674241e-06
Iter: 1966 loss: 1.13769875e-06
Iter: 1967 loss: 1.13652618e-06
Iter: 1968 loss: 1.13589363e-06
Iter: 1969 loss: 1.13965132e-06
Iter: 1970 loss: 1.13580404e-06
Iter: 1971 loss: 1.13529518e-06
Iter: 1972 loss: 1.13472049e-06
Iter: 1973 loss: 1.13465671e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0.8
+ date
Wed Nov  4 12:23:03 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.4/300_300_300_1 --function f2 --psi 1 --alpha 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c46d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c395840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c381510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c3221e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c3220d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c32a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c215bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c225158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c225488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c2879d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c287c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c2b7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c28fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c168730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c1517b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c1137b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c113bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c0e1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c050b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c05c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c05c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c03ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c03ee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c2e7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c0b2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e24483840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2c2256a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e244837b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e24445268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e24445510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e244e9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e244bc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e244ce840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e244f2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e24529c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6e2450cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.012578607
test_loss: 0.013664587
train_loss: 0.009000587
test_loss: 0.010176893
train_loss: 0.0079796035
test_loss: 0.0092078475
train_loss: 0.0072559426
test_loss: 0.008401513
train_loss: 0.0077204737
test_loss: 0.008404614
train_loss: 0.0065130442
test_loss: 0.0077635925
train_loss: 0.0069091357
test_loss: 0.0077193743
train_loss: 0.006297763
test_loss: 0.007546863
train_loss: 0.007126618
test_loss: 0.00786004
train_loss: 0.006368989
test_loss: 0.007636831
train_loss: 0.0063180607
test_loss: 0.0075049405
train_loss: 0.0064737434
test_loss: 0.007477151
train_loss: 0.005555494
test_loss: 0.0069224155
train_loss: 0.006336487
test_loss: 0.0072820797
train_loss: 0.006491275
test_loss: 0.0075327232
train_loss: 0.0054556984
test_loss: 0.0071768975
train_loss: 0.005899935
test_loss: 0.0072494363
train_loss: 0.005354113
test_loss: 0.0069650384
train_loss: 0.005593228
test_loss: 0.006854046
train_loss: 0.005396582
test_loss: 0.006941855
train_loss: 0.0052240426
test_loss: 0.006668549
train_loss: 0.0054381005
test_loss: 0.007181909
train_loss: 0.0050817244
test_loss: 0.0065319114
train_loss: 0.005353799
test_loss: 0.0067596664
train_loss: 0.0051762825
test_loss: 0.0065326607
train_loss: 0.005111619
test_loss: 0.0065512103
train_loss: 0.0049294443
test_loss: 0.006313477
train_loss: 0.0054141073
test_loss: 0.0065350463
train_loss: 0.0052554645
test_loss: 0.0064904243
train_loss: 0.005250054
test_loss: 0.0065720645
train_loss: 0.004958652
test_loss: 0.006426018
train_loss: 0.0057115806
test_loss: 0.0066715325
train_loss: 0.004846871
test_loss: 0.006330903
train_loss: 0.004833698
test_loss: 0.0066485517
train_loss: 0.005233013
test_loss: 0.006302136
train_loss: 0.005016965
test_loss: 0.0062842383
train_loss: 0.005226631
test_loss: 0.0065573454
train_loss: 0.0047522075
test_loss: 0.0061837872
train_loss: 0.0047595715
test_loss: 0.006120181
train_loss: 0.0049566855
test_loss: 0.006190904
train_loss: 0.0051111984
test_loss: 0.006400717
train_loss: 0.004737907
test_loss: 0.00641883
train_loss: 0.004793543
test_loss: 0.006211887
train_loss: 0.004963058
test_loss: 0.00616267
train_loss: 0.0047389604
test_loss: 0.006105148
train_loss: 0.004914956
test_loss: 0.006276726
train_loss: 0.0049412013
test_loss: 0.006278387
train_loss: 0.0049543446
test_loss: 0.0061695674
train_loss: 0.005337754
test_loss: 0.0063368054
train_loss: 0.00463983
test_loss: 0.0061984607
train_loss: 0.004693686
test_loss: 0.0061021675
train_loss: 0.0046041165
test_loss: 0.006057338
train_loss: 0.0044892477
test_loss: 0.0058813104
train_loss: 0.0045562703
test_loss: 0.006123945
train_loss: 0.004656005
test_loss: 0.0060856887
train_loss: 0.0047234264
test_loss: 0.0060730935
train_loss: 0.0043925648
test_loss: 0.0061812554
train_loss: 0.004649847
test_loss: 0.0062283706
train_loss: 0.004596889
test_loss: 0.0060368488
train_loss: 0.004651913
test_loss: 0.006045717
train_loss: 0.0044887867
test_loss: 0.006063396
train_loss: 0.0045877118
test_loss: 0.005940143
train_loss: 0.0047242474
test_loss: 0.005916524
train_loss: 0.004255638
test_loss: 0.005989491
train_loss: 0.00449581
test_loss: 0.0060800314
train_loss: 0.00492333
test_loss: 0.0060834396
train_loss: 0.004341284
test_loss: 0.0058714445
train_loss: 0.004315868
test_loss: 0.006110326
train_loss: 0.0043828306
test_loss: 0.006260372
train_loss: 0.0043157125
test_loss: 0.0057345796
train_loss: 0.004098038
test_loss: 0.005836624
train_loss: 0.004560184
test_loss: 0.005985591
train_loss: 0.004342855
test_loss: 0.005794782
train_loss: 0.004258117
test_loss: 0.0059287474
train_loss: 0.0042712423
test_loss: 0.005939766
train_loss: 0.0041021383
test_loss: 0.0058282823
train_loss: 0.004158356
test_loss: 0.005769244
train_loss: 0.004594193
test_loss: 0.006081395
train_loss: 0.0042570005
test_loss: 0.005775667
train_loss: 0.0042395857
test_loss: 0.0058527063
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.8/300_300_300_1 --optimizer lbfgs --function f2 --psi 1 --alpha 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efddbebf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efddbebf7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb47397b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb46de1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb46de9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb46e9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb46e91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb4675378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb467a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb46350d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb4635048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb45f0158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb45f0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb45ba6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb45c28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb4577ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb4577c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb4543d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb44fbae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb4510268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb4510598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb44b8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb447f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb4420620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb4420a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb43e88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb43f9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb43f9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb43a8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb43637b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb43d36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb43632f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb432fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb4635158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb432fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efdb42fc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.18568491e-05
Iter: 2 loss: 2.70573564e-05
Iter: 3 loss: 2.70245364e-05
Iter: 4 loss: 2.44334951e-05
Iter: 5 loss: 2.45399224e-05
Iter: 6 loss: 2.23921325e-05
Iter: 7 loss: 2.0546051e-05
Iter: 8 loss: 2.85519727e-05
Iter: 9 loss: 2.01697731e-05
Iter: 10 loss: 1.8373059e-05
Iter: 11 loss: 2.76142055e-05
Iter: 12 loss: 1.80837014e-05
Iter: 13 loss: 1.71595857e-05
Iter: 14 loss: 1.66962673e-05
Iter: 15 loss: 1.62600809e-05
Iter: 16 loss: 1.53984838e-05
Iter: 17 loss: 2.90853241e-05
Iter: 18 loss: 1.53984911e-05
Iter: 19 loss: 1.46466009e-05
Iter: 20 loss: 1.54982918e-05
Iter: 21 loss: 1.42425661e-05
Iter: 22 loss: 1.37078969e-05
Iter: 23 loss: 1.4506968e-05
Iter: 24 loss: 1.34507973e-05
Iter: 25 loss: 1.28352549e-05
Iter: 26 loss: 1.63806981e-05
Iter: 27 loss: 1.27531557e-05
Iter: 28 loss: 1.2321836e-05
Iter: 29 loss: 1.56980714e-05
Iter: 30 loss: 1.22913152e-05
Iter: 31 loss: 1.19026554e-05
Iter: 32 loss: 1.14645245e-05
Iter: 33 loss: 1.14063696e-05
Iter: 34 loss: 1.09138e-05
Iter: 35 loss: 1.1486849e-05
Iter: 36 loss: 1.06515545e-05
Iter: 37 loss: 1.02296544e-05
Iter: 38 loss: 1.32135074e-05
Iter: 39 loss: 1.01915866e-05
Iter: 40 loss: 9.93217327e-06
Iter: 41 loss: 9.92325658e-06
Iter: 42 loss: 9.78809476e-06
Iter: 43 loss: 9.53780182e-06
Iter: 44 loss: 1.52439425e-05
Iter: 45 loss: 9.53732524e-06
Iter: 46 loss: 9.38562152e-06
Iter: 47 loss: 9.37243203e-06
Iter: 48 loss: 9.23652715e-06
Iter: 49 loss: 9.03582531e-06
Iter: 50 loss: 9.03076489e-06
Iter: 51 loss: 8.82578843e-06
Iter: 52 loss: 9.0537269e-06
Iter: 53 loss: 8.71461816e-06
Iter: 54 loss: 8.58703788e-06
Iter: 55 loss: 8.5770489e-06
Iter: 56 loss: 8.450108e-06
Iter: 57 loss: 8.39259519e-06
Iter: 58 loss: 8.32914e-06
Iter: 59 loss: 8.1979e-06
Iter: 60 loss: 8.53317e-06
Iter: 61 loss: 8.15265776e-06
Iter: 62 loss: 8.01917122e-06
Iter: 63 loss: 9.42306724e-06
Iter: 64 loss: 8.01579063e-06
Iter: 65 loss: 7.93033814e-06
Iter: 66 loss: 8.05141281e-06
Iter: 67 loss: 7.88838224e-06
Iter: 68 loss: 7.7976556e-06
Iter: 69 loss: 7.80584742e-06
Iter: 70 loss: 7.72743624e-06
Iter: 71 loss: 7.61300453e-06
Iter: 72 loss: 7.77502191e-06
Iter: 73 loss: 7.55667634e-06
Iter: 74 loss: 7.45771831e-06
Iter: 75 loss: 8.41611745e-06
Iter: 76 loss: 7.45405941e-06
Iter: 77 loss: 7.36125321e-06
Iter: 78 loss: 7.94355492e-06
Iter: 79 loss: 7.35070171e-06
Iter: 80 loss: 7.29911153e-06
Iter: 81 loss: 7.2159205e-06
Iter: 82 loss: 7.21539482e-06
Iter: 83 loss: 7.16778823e-06
Iter: 84 loss: 7.1586478e-06
Iter: 85 loss: 7.11334314e-06
Iter: 86 loss: 7.01549e-06
Iter: 87 loss: 8.50463948e-06
Iter: 88 loss: 7.0116339e-06
Iter: 89 loss: 6.91176319e-06
Iter: 90 loss: 7.35643425e-06
Iter: 91 loss: 6.89226636e-06
Iter: 92 loss: 6.84708539e-06
Iter: 93 loss: 6.84325914e-06
Iter: 94 loss: 6.8038521e-06
Iter: 95 loss: 6.73097156e-06
Iter: 96 loss: 8.3963514e-06
Iter: 97 loss: 6.7308315e-06
Iter: 98 loss: 6.67820086e-06
Iter: 99 loss: 7.26594317e-06
Iter: 100 loss: 6.67705535e-06
Iter: 101 loss: 6.62029e-06
Iter: 102 loss: 6.7576e-06
Iter: 103 loss: 6.59986472e-06
Iter: 104 loss: 6.55960412e-06
Iter: 105 loss: 6.53160623e-06
Iter: 106 loss: 6.5172926e-06
Iter: 107 loss: 6.4498e-06
Iter: 108 loss: 6.80953326e-06
Iter: 109 loss: 6.43960175e-06
Iter: 110 loss: 6.38688971e-06
Iter: 111 loss: 6.40309281e-06
Iter: 112 loss: 6.34939806e-06
Iter: 113 loss: 6.32999945e-06
Iter: 114 loss: 6.31644e-06
Iter: 115 loss: 6.29213628e-06
Iter: 116 loss: 6.24117365e-06
Iter: 117 loss: 7.09794585e-06
Iter: 118 loss: 6.23988217e-06
Iter: 119 loss: 6.19755974e-06
Iter: 120 loss: 6.54969426e-06
Iter: 121 loss: 6.19512366e-06
Iter: 122 loss: 6.15473e-06
Iter: 123 loss: 6.33875879e-06
Iter: 124 loss: 6.14697183e-06
Iter: 125 loss: 6.11935047e-06
Iter: 126 loss: 6.08397613e-06
Iter: 127 loss: 6.08135724e-06
Iter: 128 loss: 6.03920398e-06
Iter: 129 loss: 6.35228844e-06
Iter: 130 loss: 6.03579065e-06
Iter: 131 loss: 5.99552141e-06
Iter: 132 loss: 6.22749485e-06
Iter: 133 loss: 5.9900658e-06
Iter: 134 loss: 5.96524114e-06
Iter: 135 loss: 5.92601509e-06
Iter: 136 loss: 5.92558627e-06
Iter: 137 loss: 5.90632726e-06
Iter: 138 loss: 5.89902629e-06
Iter: 139 loss: 5.87758086e-06
Iter: 140 loss: 5.83573e-06
Iter: 141 loss: 6.69267592e-06
Iter: 142 loss: 5.83540441e-06
Iter: 143 loss: 5.7960051e-06
Iter: 144 loss: 5.90728405e-06
Iter: 145 loss: 5.78328763e-06
Iter: 146 loss: 5.74849037e-06
Iter: 147 loss: 6.02175805e-06
Iter: 148 loss: 5.74610203e-06
Iter: 149 loss: 5.71978899e-06
Iter: 150 loss: 5.86156875e-06
Iter: 151 loss: 5.71591409e-06
Iter: 152 loss: 5.69188796e-06
Iter: 153 loss: 5.80541746e-06
Iter: 154 loss: 5.68745918e-06
Iter: 155 loss: 5.67099551e-06
Iter: 156 loss: 5.64423181e-06
Iter: 157 loss: 5.64418224e-06
Iter: 158 loss: 5.62153309e-06
Iter: 159 loss: 5.62142122e-06
Iter: 160 loss: 5.59815089e-06
Iter: 161 loss: 5.5850187e-06
Iter: 162 loss: 5.57498424e-06
Iter: 163 loss: 5.55324414e-06
Iter: 164 loss: 5.56452142e-06
Iter: 165 loss: 5.53864538e-06
Iter: 166 loss: 5.51968333e-06
Iter: 167 loss: 5.51896483e-06
Iter: 168 loss: 5.50188906e-06
Iter: 169 loss: 5.47071249e-06
Iter: 170 loss: 6.2157892e-06
Iter: 171 loss: 5.47080344e-06
Iter: 172 loss: 5.44968316e-06
Iter: 173 loss: 5.7599882e-06
Iter: 174 loss: 5.44963677e-06
Iter: 175 loss: 5.42681573e-06
Iter: 176 loss: 5.4494808e-06
Iter: 177 loss: 5.41406553e-06
Iter: 178 loss: 5.39407074e-06
Iter: 179 loss: 5.37759297e-06
Iter: 180 loss: 5.37186406e-06
Iter: 181 loss: 5.34090395e-06
Iter: 182 loss: 5.48939124e-06
Iter: 183 loss: 5.33542425e-06
Iter: 184 loss: 5.31862042e-06
Iter: 185 loss: 5.31826527e-06
Iter: 186 loss: 5.30417583e-06
Iter: 187 loss: 5.30429134e-06
Iter: 188 loss: 5.29293766e-06
Iter: 189 loss: 5.27260909e-06
Iter: 190 loss: 5.29619501e-06
Iter: 191 loss: 5.26172653e-06
Iter: 192 loss: 5.24333609e-06
Iter: 193 loss: 5.30810303e-06
Iter: 194 loss: 5.23855306e-06
Iter: 195 loss: 5.21770744e-06
Iter: 196 loss: 5.32938429e-06
Iter: 197 loss: 5.21465472e-06
Iter: 198 loss: 5.20256e-06
Iter: 199 loss: 5.18010893e-06
Iter: 200 loss: 5.68815176e-06
Iter: 201 loss: 5.1800389e-06
Iter: 202 loss: 5.16832824e-06
Iter: 203 loss: 5.1664465e-06
Iter: 204 loss: 5.15221518e-06
Iter: 205 loss: 5.13489067e-06
Iter: 206 loss: 5.13324449e-06
Iter: 207 loss: 5.11268536e-06
Iter: 208 loss: 5.17814397e-06
Iter: 209 loss: 5.10679956e-06
Iter: 210 loss: 5.08626317e-06
Iter: 211 loss: 5.31674686e-06
Iter: 212 loss: 5.08587618e-06
Iter: 213 loss: 5.07477762e-06
Iter: 214 loss: 5.05263415e-06
Iter: 215 loss: 5.48746266e-06
Iter: 216 loss: 5.05249864e-06
Iter: 217 loss: 5.03035426e-06
Iter: 218 loss: 5.12629185e-06
Iter: 219 loss: 5.02589683e-06
Iter: 220 loss: 5.01567138e-06
Iter: 221 loss: 5.01402747e-06
Iter: 222 loss: 5.0045428e-06
Iter: 223 loss: 4.99077032e-06
Iter: 224 loss: 4.9903424e-06
Iter: 225 loss: 4.97386554e-06
Iter: 226 loss: 5.05818753e-06
Iter: 227 loss: 4.97120118e-06
Iter: 228 loss: 4.95819313e-06
Iter: 229 loss: 4.99185626e-06
Iter: 230 loss: 4.95383165e-06
Iter: 231 loss: 4.93826883e-06
Iter: 232 loss: 4.99841553e-06
Iter: 233 loss: 4.93463722e-06
Iter: 234 loss: 4.92161507e-06
Iter: 235 loss: 4.90217235e-06
Iter: 236 loss: 4.90181083e-06
Iter: 237 loss: 4.88850856e-06
Iter: 238 loss: 4.88817523e-06
Iter: 239 loss: 4.87573834e-06
Iter: 240 loss: 4.89726835e-06
Iter: 241 loss: 4.87023135e-06
Iter: 242 loss: 4.85903411e-06
Iter: 243 loss: 4.85640885e-06
Iter: 244 loss: 4.84922293e-06
Iter: 245 loss: 4.83636632e-06
Iter: 246 loss: 4.83626718e-06
Iter: 247 loss: 4.82706855e-06
Iter: 248 loss: 4.8105012e-06
Iter: 249 loss: 5.21632228e-06
Iter: 250 loss: 4.81051757e-06
Iter: 251 loss: 4.7926651e-06
Iter: 252 loss: 4.79983692e-06
Iter: 253 loss: 4.78025413e-06
Iter: 254 loss: 4.78034235e-06
Iter: 255 loss: 4.77092635e-06
Iter: 256 loss: 4.76300602e-06
Iter: 257 loss: 4.74920625e-06
Iter: 258 loss: 4.74923581e-06
Iter: 259 loss: 4.7372373e-06
Iter: 260 loss: 4.79102e-06
Iter: 261 loss: 4.7348708e-06
Iter: 262 loss: 4.72167e-06
Iter: 263 loss: 4.7722624e-06
Iter: 264 loss: 4.71860585e-06
Iter: 265 loss: 4.70753366e-06
Iter: 266 loss: 4.75591742e-06
Iter: 267 loss: 4.70524401e-06
Iter: 268 loss: 4.69553152e-06
Iter: 269 loss: 4.68317376e-06
Iter: 270 loss: 4.68214057e-06
Iter: 271 loss: 4.66980055e-06
Iter: 272 loss: 4.80632934e-06
Iter: 273 loss: 4.66957317e-06
Iter: 274 loss: 4.65658e-06
Iter: 275 loss: 4.68225744e-06
Iter: 276 loss: 4.6513469e-06
Iter: 277 loss: 4.6415521e-06
Iter: 278 loss: 4.64576715e-06
Iter: 279 loss: 4.6349387e-06
Iter: 280 loss: 4.62191201e-06
Iter: 281 loss: 4.75926754e-06
Iter: 282 loss: 4.62156368e-06
Iter: 283 loss: 4.61329319e-06
Iter: 284 loss: 4.60086812e-06
Iter: 285 loss: 4.60056526e-06
Iter: 286 loss: 4.58652175e-06
Iter: 287 loss: 4.61389891e-06
Iter: 288 loss: 4.580731e-06
Iter: 289 loss: 4.57590795e-06
Iter: 290 loss: 4.57366605e-06
Iter: 291 loss: 4.56702219e-06
Iter: 292 loss: 4.55762574e-06
Iter: 293 loss: 4.55718236e-06
Iter: 294 loss: 4.54715155e-06
Iter: 295 loss: 4.54983592e-06
Iter: 296 loss: 4.53998e-06
Iter: 297 loss: 4.52857603e-06
Iter: 298 loss: 4.52849599e-06
Iter: 299 loss: 4.52024324e-06
Iter: 300 loss: 4.51802953e-06
Iter: 301 loss: 4.51284905e-06
Iter: 302 loss: 4.50096559e-06
Iter: 303 loss: 4.5253073e-06
Iter: 304 loss: 4.49612435e-06
Iter: 305 loss: 4.48361334e-06
Iter: 306 loss: 4.52359745e-06
Iter: 307 loss: 4.48014362e-06
Iter: 308 loss: 4.47120237e-06
Iter: 309 loss: 4.59180319e-06
Iter: 310 loss: 4.47125649e-06
Iter: 311 loss: 4.46491504e-06
Iter: 312 loss: 4.45322803e-06
Iter: 313 loss: 4.73614455e-06
Iter: 314 loss: 4.4532344e-06
Iter: 315 loss: 4.44584475e-06
Iter: 316 loss: 4.44502575e-06
Iter: 317 loss: 4.43856152e-06
Iter: 318 loss: 4.42414148e-06
Iter: 319 loss: 4.63329161e-06
Iter: 320 loss: 4.42351438e-06
Iter: 321 loss: 4.40838858e-06
Iter: 322 loss: 4.4214753e-06
Iter: 323 loss: 4.39953328e-06
Iter: 324 loss: 4.39596079e-06
Iter: 325 loss: 4.39168662e-06
Iter: 326 loss: 4.38392453e-06
Iter: 327 loss: 4.377599e-06
Iter: 328 loss: 4.37538711e-06
Iter: 329 loss: 4.36629398e-06
Iter: 330 loss: 4.3684513e-06
Iter: 331 loss: 4.35960465e-06
Iter: 332 loss: 4.35148104e-06
Iter: 333 loss: 4.35140646e-06
Iter: 334 loss: 4.3434793e-06
Iter: 335 loss: 4.33637933e-06
Iter: 336 loss: 4.33440528e-06
Iter: 337 loss: 4.32591514e-06
Iter: 338 loss: 4.36524624e-06
Iter: 339 loss: 4.32428e-06
Iter: 340 loss: 4.31492117e-06
Iter: 341 loss: 4.33038531e-06
Iter: 342 loss: 4.31069475e-06
Iter: 343 loss: 4.30083128e-06
Iter: 344 loss: 4.3736859e-06
Iter: 345 loss: 4.30003502e-06
Iter: 346 loss: 4.29223837e-06
Iter: 347 loss: 4.29460488e-06
Iter: 348 loss: 4.28668136e-06
Iter: 349 loss: 4.27843406e-06
Iter: 350 loss: 4.34550293e-06
Iter: 351 loss: 4.27788245e-06
Iter: 352 loss: 4.26984707e-06
Iter: 353 loss: 4.2677957e-06
Iter: 354 loss: 4.26258475e-06
Iter: 355 loss: 4.25247663e-06
Iter: 356 loss: 4.24431e-06
Iter: 357 loss: 4.2413476e-06
Iter: 358 loss: 4.22864514e-06
Iter: 359 loss: 4.33326295e-06
Iter: 360 loss: 4.22782614e-06
Iter: 361 loss: 4.21722689e-06
Iter: 362 loss: 4.35490347e-06
Iter: 363 loss: 4.21707e-06
Iter: 364 loss: 4.21126424e-06
Iter: 365 loss: 4.19976777e-06
Iter: 366 loss: 4.4209969e-06
Iter: 367 loss: 4.19962726e-06
Iter: 368 loss: 4.18933951e-06
Iter: 369 loss: 4.24088e-06
Iter: 370 loss: 4.1877347e-06
Iter: 371 loss: 4.17743195e-06
Iter: 372 loss: 4.27845271e-06
Iter: 373 loss: 4.17709543e-06
Iter: 374 loss: 4.17233741e-06
Iter: 375 loss: 4.16158491e-06
Iter: 376 loss: 4.30247837e-06
Iter: 377 loss: 4.16079502e-06
Iter: 378 loss: 4.15285649e-06
Iter: 379 loss: 4.15258319e-06
Iter: 380 loss: 4.14504757e-06
Iter: 381 loss: 4.1525841e-06
Iter: 382 loss: 4.14086389e-06
Iter: 383 loss: 4.13273119e-06
Iter: 384 loss: 4.1677049e-06
Iter: 385 loss: 4.13118e-06
Iter: 386 loss: 4.1241542e-06
Iter: 387 loss: 4.13660109e-06
Iter: 388 loss: 4.12116333e-06
Iter: 389 loss: 4.11206838e-06
Iter: 390 loss: 4.13685029e-06
Iter: 391 loss: 4.10905159e-06
Iter: 392 loss: 4.10191114e-06
Iter: 393 loss: 4.09526638e-06
Iter: 394 loss: 4.09357745e-06
Iter: 395 loss: 4.08181859e-06
Iter: 396 loss: 4.11231122e-06
Iter: 397 loss: 4.07788e-06
Iter: 398 loss: 4.07387461e-06
Iter: 399 loss: 4.07168227e-06
Iter: 400 loss: 4.06717118e-06
Iter: 401 loss: 4.05737683e-06
Iter: 402 loss: 4.19510388e-06
Iter: 403 loss: 4.05682931e-06
Iter: 404 loss: 4.04655202e-06
Iter: 405 loss: 4.06646905e-06
Iter: 406 loss: 4.04227603e-06
Iter: 407 loss: 4.03733839e-06
Iter: 408 loss: 4.03603417e-06
Iter: 409 loss: 4.03140348e-06
Iter: 410 loss: 4.02175147e-06
Iter: 411 loss: 4.19229673e-06
Iter: 412 loss: 4.02159912e-06
Iter: 413 loss: 4.01090665e-06
Iter: 414 loss: 4.0446439e-06
Iter: 415 loss: 4.00787303e-06
Iter: 416 loss: 4.00106637e-06
Iter: 417 loss: 4.00098452e-06
Iter: 418 loss: 3.99486225e-06
Iter: 419 loss: 3.98508973e-06
Iter: 420 loss: 3.98500379e-06
Iter: 421 loss: 3.97883605e-06
Iter: 422 loss: 3.97842723e-06
Iter: 423 loss: 3.97336908e-06
Iter: 424 loss: 3.96952237e-06
Iter: 425 loss: 3.96782525e-06
Iter: 426 loss: 3.95959e-06
Iter: 427 loss: 3.98932116e-06
Iter: 428 loss: 3.95742336e-06
Iter: 429 loss: 3.95064217e-06
Iter: 430 loss: 3.94723975e-06
Iter: 431 loss: 3.94404424e-06
Iter: 432 loss: 3.93897426e-06
Iter: 433 loss: 3.93842038e-06
Iter: 434 loss: 3.9324982e-06
Iter: 435 loss: 3.92523134e-06
Iter: 436 loss: 3.92455422e-06
Iter: 437 loss: 3.91760204e-06
Iter: 438 loss: 3.91123149e-06
Iter: 439 loss: 3.90953483e-06
Iter: 440 loss: 3.9039478e-06
Iter: 441 loss: 3.90311197e-06
Iter: 442 loss: 3.89659635e-06
Iter: 443 loss: 3.90080959e-06
Iter: 444 loss: 3.89254456e-06
Iter: 445 loss: 3.886385e-06
Iter: 446 loss: 3.88096851e-06
Iter: 447 loss: 3.87931595e-06
Iter: 448 loss: 3.8713556e-06
Iter: 449 loss: 3.87138925e-06
Iter: 450 loss: 3.86413285e-06
Iter: 451 loss: 3.86854526e-06
Iter: 452 loss: 3.85946078e-06
Iter: 453 loss: 3.8533517e-06
Iter: 454 loss: 3.87344608e-06
Iter: 455 loss: 3.85159683e-06
Iter: 456 loss: 3.84375e-06
Iter: 457 loss: 3.86304282e-06
Iter: 458 loss: 3.84091027e-06
Iter: 459 loss: 3.83406314e-06
Iter: 460 loss: 3.83320275e-06
Iter: 461 loss: 3.82834423e-06
Iter: 462 loss: 3.82041571e-06
Iter: 463 loss: 3.88153603e-06
Iter: 464 loss: 3.81980317e-06
Iter: 465 loss: 3.81395171e-06
Iter: 466 loss: 3.82103144e-06
Iter: 467 loss: 3.81073937e-06
Iter: 468 loss: 3.80297388e-06
Iter: 469 loss: 3.86857482e-06
Iter: 470 loss: 3.80253505e-06
Iter: 471 loss: 3.79790299e-06
Iter: 472 loss: 3.78892946e-06
Iter: 473 loss: 3.97585791e-06
Iter: 474 loss: 3.78886557e-06
Iter: 475 loss: 3.77962147e-06
Iter: 476 loss: 3.80416077e-06
Iter: 477 loss: 3.7765235e-06
Iter: 478 loss: 3.77157198e-06
Iter: 479 loss: 3.77108063e-06
Iter: 480 loss: 3.76582761e-06
Iter: 481 loss: 3.75972127e-06
Iter: 482 loss: 3.75904415e-06
Iter: 483 loss: 3.75170271e-06
Iter: 484 loss: 3.75411196e-06
Iter: 485 loss: 3.74649653e-06
Iter: 486 loss: 3.74246565e-06
Iter: 487 loss: 3.74126012e-06
Iter: 488 loss: 3.73694434e-06
Iter: 489 loss: 3.72909813e-06
Iter: 490 loss: 3.92116181e-06
Iter: 491 loss: 3.72911836e-06
Iter: 492 loss: 3.72432987e-06
Iter: 493 loss: 3.72417207e-06
Iter: 494 loss: 3.71948431e-06
Iter: 495 loss: 3.71369e-06
Iter: 496 loss: 3.71306328e-06
Iter: 497 loss: 3.70594944e-06
Iter: 498 loss: 3.71919123e-06
Iter: 499 loss: 3.70296152e-06
Iter: 500 loss: 3.69487884e-06
Iter: 501 loss: 3.74667616e-06
Iter: 502 loss: 3.69403301e-06
Iter: 503 loss: 3.6871445e-06
Iter: 504 loss: 3.72966451e-06
Iter: 505 loss: 3.68631731e-06
Iter: 506 loss: 3.68109932e-06
Iter: 507 loss: 3.68486099e-06
Iter: 508 loss: 3.6779129e-06
Iter: 509 loss: 3.67268285e-06
Iter: 510 loss: 3.66406857e-06
Iter: 511 loss: 3.66399445e-06
Iter: 512 loss: 3.65469532e-06
Iter: 513 loss: 3.73061039e-06
Iter: 514 loss: 3.65410278e-06
Iter: 515 loss: 3.64778111e-06
Iter: 516 loss: 3.64775951e-06
Iter: 517 loss: 3.64269204e-06
Iter: 518 loss: 3.6346089e-06
Iter: 519 loss: 3.6344843e-06
Iter: 520 loss: 3.62651963e-06
Iter: 521 loss: 3.65075653e-06
Iter: 522 loss: 3.6242543e-06
Iter: 523 loss: 3.61932189e-06
Iter: 524 loss: 3.61914636e-06
Iter: 525 loss: 3.61425236e-06
Iter: 526 loss: 3.60560693e-06
Iter: 527 loss: 3.8192311e-06
Iter: 528 loss: 3.60563627e-06
Iter: 529 loss: 3.60065496e-06
Iter: 530 loss: 3.60068452e-06
Iter: 531 loss: 3.59544765e-06
Iter: 532 loss: 3.59019214e-06
Iter: 533 loss: 3.58913849e-06
Iter: 534 loss: 3.58231136e-06
Iter: 535 loss: 3.58703778e-06
Iter: 536 loss: 3.57800468e-06
Iter: 537 loss: 3.57268209e-06
Iter: 538 loss: 3.5727071e-06
Iter: 539 loss: 3.5664491e-06
Iter: 540 loss: 3.5642845e-06
Iter: 541 loss: 3.56075861e-06
Iter: 542 loss: 3.55450106e-06
Iter: 543 loss: 3.58129273e-06
Iter: 544 loss: 3.55318525e-06
Iter: 545 loss: 3.54724125e-06
Iter: 546 loss: 3.54534905e-06
Iter: 547 loss: 3.54184976e-06
Iter: 548 loss: 3.53499809e-06
Iter: 549 loss: 3.5669334e-06
Iter: 550 loss: 3.53371411e-06
Iter: 551 loss: 3.52836355e-06
Iter: 552 loss: 3.611902e-06
Iter: 553 loss: 3.52841357e-06
Iter: 554 loss: 3.52464644e-06
Iter: 555 loss: 3.51861877e-06
Iter: 556 loss: 3.51857216e-06
Iter: 557 loss: 3.51220751e-06
Iter: 558 loss: 3.52590814e-06
Iter: 559 loss: 3.50970458e-06
Iter: 560 loss: 3.50335017e-06
Iter: 561 loss: 3.50339519e-06
Iter: 562 loss: 3.49943434e-06
Iter: 563 loss: 3.49327684e-06
Iter: 564 loss: 3.493127e-06
Iter: 565 loss: 3.48858498e-06
Iter: 566 loss: 3.48854837e-06
Iter: 567 loss: 3.4843606e-06
Iter: 568 loss: 3.48037884e-06
Iter: 569 loss: 3.47939522e-06
Iter: 570 loss: 3.47375953e-06
Iter: 571 loss: 3.47874743e-06
Iter: 572 loss: 3.47031414e-06
Iter: 573 loss: 3.46613137e-06
Iter: 574 loss: 3.46593265e-06
Iter: 575 loss: 3.46137062e-06
Iter: 576 loss: 3.45616399e-06
Iter: 577 loss: 3.45557373e-06
Iter: 578 loss: 3.45019635e-06
Iter: 579 loss: 3.45260628e-06
Iter: 580 loss: 3.44660089e-06
Iter: 581 loss: 3.43974557e-06
Iter: 582 loss: 3.48422918e-06
Iter: 583 loss: 3.43901092e-06
Iter: 584 loss: 3.43308557e-06
Iter: 585 loss: 3.45154922e-06
Iter: 586 loss: 3.43132228e-06
Iter: 587 loss: 3.42724206e-06
Iter: 588 loss: 3.48691e-06
Iter: 589 loss: 3.42723024e-06
Iter: 590 loss: 3.42423664e-06
Iter: 591 loss: 3.41666691e-06
Iter: 592 loss: 3.49566835e-06
Iter: 593 loss: 3.41586065e-06
Iter: 594 loss: 3.40989959e-06
Iter: 595 loss: 3.47156856e-06
Iter: 596 loss: 3.40971928e-06
Iter: 597 loss: 3.40361271e-06
Iter: 598 loss: 3.43693205e-06
Iter: 599 loss: 3.40277029e-06
Iter: 600 loss: 3.3988415e-06
Iter: 601 loss: 3.39091594e-06
Iter: 602 loss: 3.53772202e-06
Iter: 603 loss: 3.39077678e-06
Iter: 604 loss: 3.38769973e-06
Iter: 605 loss: 3.38589598e-06
Iter: 606 loss: 3.38269865e-06
Iter: 607 loss: 3.37747406e-06
Iter: 608 loss: 3.37743154e-06
Iter: 609 loss: 3.37211395e-06
Iter: 610 loss: 3.40699057e-06
Iter: 611 loss: 3.37151914e-06
Iter: 612 loss: 3.36643143e-06
Iter: 613 loss: 3.40330257e-06
Iter: 614 loss: 3.36599123e-06
Iter: 615 loss: 3.36304947e-06
Iter: 616 loss: 3.3563656e-06
Iter: 617 loss: 3.44097498e-06
Iter: 618 loss: 3.35593222e-06
Iter: 619 loss: 3.34855145e-06
Iter: 620 loss: 3.37210713e-06
Iter: 621 loss: 3.34636547e-06
Iter: 622 loss: 3.34045126e-06
Iter: 623 loss: 3.41531677e-06
Iter: 624 loss: 3.34037099e-06
Iter: 625 loss: 3.33602929e-06
Iter: 626 loss: 3.36499261e-06
Iter: 627 loss: 3.33565436e-06
Iter: 628 loss: 3.33191406e-06
Iter: 629 loss: 3.33115122e-06
Iter: 630 loss: 3.32872833e-06
Iter: 631 loss: 3.32291779e-06
Iter: 632 loss: 3.3316187e-06
Iter: 633 loss: 3.32011768e-06
Iter: 634 loss: 3.31543356e-06
Iter: 635 loss: 3.34146307e-06
Iter: 636 loss: 3.31477418e-06
Iter: 637 loss: 3.30963235e-06
Iter: 638 loss: 3.33075286e-06
Iter: 639 loss: 3.30854346e-06
Iter: 640 loss: 3.30502257e-06
Iter: 641 loss: 3.2979633e-06
Iter: 642 loss: 3.42903058e-06
Iter: 643 loss: 3.29785507e-06
Iter: 644 loss: 3.29483782e-06
Iter: 645 loss: 3.29345539e-06
Iter: 646 loss: 3.29002523e-06
Iter: 647 loss: 3.28439319e-06
Iter: 648 loss: 3.28433157e-06
Iter: 649 loss: 3.27939983e-06
Iter: 650 loss: 3.32672221e-06
Iter: 651 loss: 3.27917951e-06
Iter: 652 loss: 3.27415523e-06
Iter: 653 loss: 3.2918208e-06
Iter: 654 loss: 3.2727371e-06
Iter: 655 loss: 3.2694029e-06
Iter: 656 loss: 3.26301597e-06
Iter: 657 loss: 3.39786266e-06
Iter: 658 loss: 3.26301233e-06
Iter: 659 loss: 3.25578708e-06
Iter: 660 loss: 3.27065209e-06
Iter: 661 loss: 3.25287488e-06
Iter: 662 loss: 3.24526832e-06
Iter: 663 loss: 3.28227497e-06
Iter: 664 loss: 3.24393545e-06
Iter: 665 loss: 3.23898462e-06
Iter: 666 loss: 3.23889049e-06
Iter: 667 loss: 3.23522977e-06
Iter: 668 loss: 3.23561949e-06
Iter: 669 loss: 3.23234644e-06
Iter: 670 loss: 3.22745609e-06
Iter: 671 loss: 3.23797644e-06
Iter: 672 loss: 3.22556298e-06
Iter: 673 loss: 3.2212215e-06
Iter: 674 loss: 3.23979202e-06
Iter: 675 loss: 3.22022493e-06
Iter: 676 loss: 3.21594825e-06
Iter: 677 loss: 3.24053417e-06
Iter: 678 loss: 3.21535708e-06
Iter: 679 loss: 3.21184552e-06
Iter: 680 loss: 3.20550953e-06
Iter: 681 loss: 3.36057678e-06
Iter: 682 loss: 3.20553022e-06
Iter: 683 loss: 3.20019308e-06
Iter: 684 loss: 3.27846715e-06
Iter: 685 loss: 3.20015783e-06
Iter: 686 loss: 3.19530955e-06
Iter: 687 loss: 3.20538288e-06
Iter: 688 loss: 3.19340461e-06
Iter: 689 loss: 3.18907814e-06
Iter: 690 loss: 3.18576076e-06
Iter: 691 loss: 3.18437628e-06
Iter: 692 loss: 3.17917807e-06
Iter: 693 loss: 3.17899094e-06
Iter: 694 loss: 3.17636795e-06
Iter: 695 loss: 3.17033118e-06
Iter: 696 loss: 3.24411189e-06
Iter: 697 loss: 3.16978708e-06
Iter: 698 loss: 3.16326032e-06
Iter: 699 loss: 3.16915384e-06
Iter: 700 loss: 3.1594036e-06
Iter: 701 loss: 3.15436409e-06
Iter: 702 loss: 3.15428542e-06
Iter: 703 loss: 3.14926956e-06
Iter: 704 loss: 3.16771502e-06
Iter: 705 loss: 3.14805402e-06
Iter: 706 loss: 3.14490853e-06
Iter: 707 loss: 3.14097383e-06
Iter: 708 loss: 3.14065937e-06
Iter: 709 loss: 3.13563828e-06
Iter: 710 loss: 3.20376898e-06
Iter: 711 loss: 3.13561668e-06
Iter: 712 loss: 3.13151395e-06
Iter: 713 loss: 3.14089721e-06
Iter: 714 loss: 3.12998191e-06
Iter: 715 loss: 3.12650536e-06
Iter: 716 loss: 3.13185683e-06
Iter: 717 loss: 3.12488282e-06
Iter: 718 loss: 3.12046291e-06
Iter: 719 loss: 3.11791564e-06
Iter: 720 loss: 3.11609074e-06
Iter: 721 loss: 3.11153235e-06
Iter: 722 loss: 3.11141207e-06
Iter: 723 loss: 3.10837481e-06
Iter: 724 loss: 3.10406267e-06
Iter: 725 loss: 3.10396103e-06
Iter: 726 loss: 3.10097448e-06
Iter: 727 loss: 3.10074984e-06
Iter: 728 loss: 3.09787947e-06
Iter: 729 loss: 3.09174652e-06
Iter: 730 loss: 3.18591765e-06
Iter: 731 loss: 3.09149209e-06
Iter: 732 loss: 3.0853771e-06
Iter: 733 loss: 3.10380801e-06
Iter: 734 loss: 3.08346966e-06
Iter: 735 loss: 3.07776236e-06
Iter: 736 loss: 3.07875644e-06
Iter: 737 loss: 3.07348751e-06
Iter: 738 loss: 3.06906986e-06
Iter: 739 loss: 3.06889115e-06
Iter: 740 loss: 3.06458e-06
Iter: 741 loss: 3.07850746e-06
Iter: 742 loss: 3.06332572e-06
Iter: 743 loss: 3.06018592e-06
Iter: 744 loss: 3.05633671e-06
Iter: 745 loss: 3.05591857e-06
Iter: 746 loss: 3.05279673e-06
Iter: 747 loss: 3.05265098e-06
Iter: 748 loss: 3.04933974e-06
Iter: 749 loss: 3.0449346e-06
Iter: 750 loss: 3.04472269e-06
Iter: 751 loss: 3.03989555e-06
Iter: 752 loss: 3.07150685e-06
Iter: 753 loss: 3.03943352e-06
Iter: 754 loss: 3.03530783e-06
Iter: 755 loss: 3.04021751e-06
Iter: 756 loss: 3.03309207e-06
Iter: 757 loss: 3.0281808e-06
Iter: 758 loss: 3.06259471e-06
Iter: 759 loss: 3.02775675e-06
Iter: 760 loss: 3.0242195e-06
Iter: 761 loss: 3.02410126e-06
Iter: 762 loss: 3.02139824e-06
Iter: 763 loss: 3.01740374e-06
Iter: 764 loss: 3.07653954e-06
Iter: 765 loss: 3.01736e-06
Iter: 766 loss: 3.01456612e-06
Iter: 767 loss: 3.00837451e-06
Iter: 768 loss: 3.10172572e-06
Iter: 769 loss: 3.00813554e-06
Iter: 770 loss: 3.00252145e-06
Iter: 771 loss: 3.01291971e-06
Iter: 772 loss: 3.00011743e-06
Iter: 773 loss: 2.9939597e-06
Iter: 774 loss: 3.01770501e-06
Iter: 775 loss: 2.99254407e-06
Iter: 776 loss: 2.98970235e-06
Iter: 777 loss: 2.9891969e-06
Iter: 778 loss: 2.98670034e-06
Iter: 779 loss: 2.98248438e-06
Iter: 780 loss: 2.98253e-06
Iter: 781 loss: 2.97798579e-06
Iter: 782 loss: 2.99975864e-06
Iter: 783 loss: 2.97721294e-06
Iter: 784 loss: 2.97278439e-06
Iter: 785 loss: 3.00564557e-06
Iter: 786 loss: 2.97240194e-06
Iter: 787 loss: 2.96971484e-06
Iter: 788 loss: 2.96501048e-06
Iter: 789 loss: 2.96501e-06
Iter: 790 loss: 2.96152098e-06
Iter: 791 loss: 2.96148573e-06
Iter: 792 loss: 2.95782593e-06
Iter: 793 loss: 2.95687482e-06
Iter: 794 loss: 2.95461405e-06
Iter: 795 loss: 2.94993197e-06
Iter: 796 loss: 2.97950692e-06
Iter: 797 loss: 2.94938536e-06
Iter: 798 loss: 2.94558663e-06
Iter: 799 loss: 2.95890072e-06
Iter: 800 loss: 2.94460142e-06
Iter: 801 loss: 2.94079928e-06
Iter: 802 loss: 2.9471671e-06
Iter: 803 loss: 2.93900848e-06
Iter: 804 loss: 2.93587959e-06
Iter: 805 loss: 2.93311086e-06
Iter: 806 loss: 2.93230437e-06
Iter: 807 loss: 2.92708137e-06
Iter: 808 loss: 2.93320136e-06
Iter: 809 loss: 2.92428967e-06
Iter: 810 loss: 2.91959213e-06
Iter: 811 loss: 2.97739462e-06
Iter: 812 loss: 2.91951278e-06
Iter: 813 loss: 2.91530841e-06
Iter: 814 loss: 2.94418192e-06
Iter: 815 loss: 2.91491256e-06
Iter: 816 loss: 2.91219067e-06
Iter: 817 loss: 2.90676508e-06
Iter: 818 loss: 3.00636657e-06
Iter: 819 loss: 2.90669664e-06
Iter: 820 loss: 2.90493153e-06
Iter: 821 loss: 2.90379694e-06
Iter: 822 loss: 2.90100252e-06
Iter: 823 loss: 2.89509353e-06
Iter: 824 loss: 2.99487078e-06
Iter: 825 loss: 2.89497984e-06
Iter: 826 loss: 2.8893146e-06
Iter: 827 loss: 2.89729451e-06
Iter: 828 loss: 2.88655747e-06
Iter: 829 loss: 2.88311912e-06
Iter: 830 loss: 2.88269462e-06
Iter: 831 loss: 2.87956937e-06
Iter: 832 loss: 2.87858825e-06
Iter: 833 loss: 2.87660782e-06
Iter: 834 loss: 2.87348666e-06
Iter: 835 loss: 2.90846606e-06
Iter: 836 loss: 2.87345665e-06
Iter: 837 loss: 2.87045714e-06
Iter: 838 loss: 2.86716727e-06
Iter: 839 loss: 2.86666545e-06
Iter: 840 loss: 2.86210411e-06
Iter: 841 loss: 2.88093725e-06
Iter: 842 loss: 2.86122213e-06
Iter: 843 loss: 2.85742431e-06
Iter: 844 loss: 2.86276781e-06
Iter: 845 loss: 2.8556e-06
Iter: 846 loss: 2.85120723e-06
Iter: 847 loss: 2.85047508e-06
Iter: 848 loss: 2.84744738e-06
Iter: 849 loss: 2.84371254e-06
Iter: 850 loss: 2.84360272e-06
Iter: 851 loss: 2.84006137e-06
Iter: 852 loss: 2.84745306e-06
Iter: 853 loss: 2.83860459e-06
Iter: 854 loss: 2.83566533e-06
Iter: 855 loss: 2.830737e-06
Iter: 856 loss: 2.83069039e-06
Iter: 857 loss: 2.82877022e-06
Iter: 858 loss: 2.82759197e-06
Iter: 859 loss: 2.82496376e-06
Iter: 860 loss: 2.82036854e-06
Iter: 861 loss: 2.82035353e-06
Iter: 862 loss: 2.81567941e-06
Iter: 863 loss: 2.82096426e-06
Iter: 864 loss: 2.81314237e-06
Iter: 865 loss: 2.81139796e-06
Iter: 866 loss: 2.81055941e-06
Iter: 867 loss: 2.80822337e-06
Iter: 868 loss: 2.80421727e-06
Iter: 869 loss: 2.80418431e-06
Iter: 870 loss: 2.80103609e-06
Iter: 871 loss: 2.80098834e-06
Iter: 872 loss: 2.79826463e-06
Iter: 873 loss: 2.79392953e-06
Iter: 874 loss: 2.79386268e-06
Iter: 875 loss: 2.78960169e-06
Iter: 876 loss: 2.79296682e-06
Iter: 877 loss: 2.78702169e-06
Iter: 878 loss: 2.78202174e-06
Iter: 879 loss: 2.83432269e-06
Iter: 880 loss: 2.78190464e-06
Iter: 881 loss: 2.7782803e-06
Iter: 882 loss: 2.77834306e-06
Iter: 883 loss: 2.77529216e-06
Iter: 884 loss: 2.77150775e-06
Iter: 885 loss: 2.8130944e-06
Iter: 886 loss: 2.77146364e-06
Iter: 887 loss: 2.76718129e-06
Iter: 888 loss: 2.76977744e-06
Iter: 889 loss: 2.76448486e-06
Iter: 890 loss: 2.76113838e-06
Iter: 891 loss: 2.7608944e-06
Iter: 892 loss: 2.75826596e-06
Iter: 893 loss: 2.75408843e-06
Iter: 894 loss: 2.78248672e-06
Iter: 895 loss: 2.75370439e-06
Iter: 896 loss: 2.74933427e-06
Iter: 897 loss: 2.7715414e-06
Iter: 898 loss: 2.74864624e-06
Iter: 899 loss: 2.74602098e-06
Iter: 900 loss: 2.7413198e-06
Iter: 901 loss: 2.84918269e-06
Iter: 902 loss: 2.74123613e-06
Iter: 903 loss: 2.73684827e-06
Iter: 904 loss: 2.78154062e-06
Iter: 905 loss: 2.73671185e-06
Iter: 906 loss: 2.7325998e-06
Iter: 907 loss: 2.76353262e-06
Iter: 908 loss: 2.73230489e-06
Iter: 909 loss: 2.72996385e-06
Iter: 910 loss: 2.72773696e-06
Iter: 911 loss: 2.72714487e-06
Iter: 912 loss: 2.72404304e-06
Iter: 913 loss: 2.76597871e-06
Iter: 914 loss: 2.7239978e-06
Iter: 915 loss: 2.72143438e-06
Iter: 916 loss: 2.71767067e-06
Iter: 917 loss: 2.71753e-06
Iter: 918 loss: 2.71308272e-06
Iter: 919 loss: 2.71196745e-06
Iter: 920 loss: 2.70913552e-06
Iter: 921 loss: 2.7032861e-06
Iter: 922 loss: 2.74056583e-06
Iter: 923 loss: 2.70272039e-06
Iter: 924 loss: 2.69981138e-06
Iter: 925 loss: 2.69944417e-06
Iter: 926 loss: 2.69737916e-06
Iter: 927 loss: 2.69382781e-06
Iter: 928 loss: 2.78190578e-06
Iter: 929 loss: 2.69380644e-06
Iter: 930 loss: 2.68975123e-06
Iter: 931 loss: 2.71097088e-06
Iter: 932 loss: 2.68916165e-06
Iter: 933 loss: 2.68530835e-06
Iter: 934 loss: 2.69146244e-06
Iter: 935 loss: 2.68352937e-06
Iter: 936 loss: 2.68081089e-06
Iter: 937 loss: 2.6808284e-06
Iter: 938 loss: 2.67817609e-06
Iter: 939 loss: 2.67274754e-06
Iter: 940 loss: 2.77104778e-06
Iter: 941 loss: 2.67270798e-06
Iter: 942 loss: 2.66863071e-06
Iter: 943 loss: 2.68752228e-06
Iter: 944 loss: 2.66792449e-06
Iter: 945 loss: 2.66523375e-06
Iter: 946 loss: 2.66513825e-06
Iter: 947 loss: 2.66325446e-06
Iter: 948 loss: 2.65979952e-06
Iter: 949 loss: 2.73573505e-06
Iter: 950 loss: 2.65984363e-06
Iter: 951 loss: 2.65665744e-06
Iter: 952 loss: 2.68618646e-06
Iter: 953 loss: 2.65659673e-06
Iter: 954 loss: 2.65316203e-06
Iter: 955 loss: 2.65437848e-06
Iter: 956 loss: 2.65069707e-06
Iter: 957 loss: 2.64710889e-06
Iter: 958 loss: 2.64507617e-06
Iter: 959 loss: 2.6434252e-06
Iter: 960 loss: 2.63941411e-06
Iter: 961 loss: 2.6633611e-06
Iter: 962 loss: 2.63892e-06
Iter: 963 loss: 2.63578931e-06
Iter: 964 loss: 2.63579113e-06
Iter: 965 loss: 2.63347283e-06
Iter: 966 loss: 2.63030142e-06
Iter: 967 loss: 2.63014567e-06
Iter: 968 loss: 2.62613867e-06
Iter: 969 loss: 2.62781145e-06
Iter: 970 loss: 2.62336721e-06
Iter: 971 loss: 2.62120443e-06
Iter: 972 loss: 2.62061303e-06
Iter: 973 loss: 2.61833384e-06
Iter: 974 loss: 2.61797777e-06
Iter: 975 loss: 2.61644936e-06
Iter: 976 loss: 2.61311106e-06
Iter: 977 loss: 2.61935384e-06
Iter: 978 loss: 2.61166042e-06
Iter: 979 loss: 2.6085529e-06
Iter: 980 loss: 2.61133664e-06
Iter: 981 loss: 2.60673141e-06
Iter: 982 loss: 2.60465e-06
Iter: 983 loss: 2.60441402e-06
Iter: 984 loss: 2.60274896e-06
Iter: 985 loss: 2.59916965e-06
Iter: 986 loss: 2.65444532e-06
Iter: 987 loss: 2.59900503e-06
Iter: 988 loss: 2.59564968e-06
Iter: 989 loss: 2.61371611e-06
Iter: 990 loss: 2.59517105e-06
Iter: 991 loss: 2.59136868e-06
Iter: 992 loss: 2.60794309e-06
Iter: 993 loss: 2.59056014e-06
Iter: 994 loss: 2.58808814e-06
Iter: 995 loss: 2.58463797e-06
Iter: 996 loss: 2.5844995e-06
Iter: 997 loss: 2.58103819e-06
Iter: 998 loss: 2.61974401e-06
Iter: 999 loss: 2.5810009e-06
Iter: 1000 loss: 2.57747206e-06
Iter: 1001 loss: 2.59061949e-06
Iter: 1002 loss: 2.5767024e-06
Iter: 1003 loss: 2.5742188e-06
Iter: 1004 loss: 2.56993781e-06
Iter: 1005 loss: 2.56994053e-06
Iter: 1006 loss: 2.56635667e-06
Iter: 1007 loss: 2.61831428e-06
Iter: 1008 loss: 2.56638668e-06
Iter: 1009 loss: 2.56307931e-06
Iter: 1010 loss: 2.57986767e-06
Iter: 1011 loss: 2.56250905e-06
Iter: 1012 loss: 2.56058183e-06
Iter: 1013 loss: 2.55866325e-06
Iter: 1014 loss: 2.55817213e-06
Iter: 1015 loss: 2.55446957e-06
Iter: 1016 loss: 2.57231e-06
Iter: 1017 loss: 2.55383429e-06
Iter: 1018 loss: 2.55129839e-06
Iter: 1019 loss: 2.57752026e-06
Iter: 1020 loss: 2.55125019e-06
Iter: 1021 loss: 2.54895872e-06
Iter: 1022 loss: 2.54751126e-06
Iter: 1023 loss: 2.54658744e-06
Iter: 1024 loss: 2.54349652e-06
Iter: 1025 loss: 2.54408224e-06
Iter: 1026 loss: 2.54123756e-06
Iter: 1027 loss: 2.53854137e-06
Iter: 1028 loss: 2.53857843e-06
Iter: 1029 loss: 2.53617e-06
Iter: 1030 loss: 2.53530561e-06
Iter: 1031 loss: 2.53402186e-06
Iter: 1032 loss: 2.53115832e-06
Iter: 1033 loss: 2.52941709e-06
Iter: 1034 loss: 2.52825612e-06
Iter: 1035 loss: 2.52532936e-06
Iter: 1036 loss: 2.52525251e-06
Iter: 1037 loss: 2.52221707e-06
Iter: 1038 loss: 2.52408404e-06
Iter: 1039 loss: 2.52028167e-06
Iter: 1040 loss: 2.51775691e-06
Iter: 1041 loss: 2.5153206e-06
Iter: 1042 loss: 2.51475331e-06
Iter: 1043 loss: 2.51306096e-06
Iter: 1044 loss: 2.51247093e-06
Iter: 1045 loss: 2.51042729e-06
Iter: 1046 loss: 2.50731387e-06
Iter: 1047 loss: 2.50730272e-06
Iter: 1048 loss: 2.50414655e-06
Iter: 1049 loss: 2.51068559e-06
Iter: 1050 loss: 2.50287439e-06
Iter: 1051 loss: 2.50058974e-06
Iter: 1052 loss: 2.50056246e-06
Iter: 1053 loss: 2.498502e-06
Iter: 1054 loss: 2.49668392e-06
Iter: 1055 loss: 2.49618779e-06
Iter: 1056 loss: 2.49293498e-06
Iter: 1057 loss: 2.50333164e-06
Iter: 1058 loss: 2.49198456e-06
Iter: 1059 loss: 2.48897186e-06
Iter: 1060 loss: 2.48822607e-06
Iter: 1061 loss: 2.48637321e-06
Iter: 1062 loss: 2.48320839e-06
Iter: 1063 loss: 2.48313063e-06
Iter: 1064 loss: 2.48144443e-06
Iter: 1065 loss: 2.47818343e-06
Iter: 1066 loss: 2.54654742e-06
Iter: 1067 loss: 2.47815319e-06
Iter: 1068 loss: 2.47436287e-06
Iter: 1069 loss: 2.47659023e-06
Iter: 1070 loss: 2.47188632e-06
Iter: 1071 loss: 2.47105686e-06
Iter: 1072 loss: 2.46958666e-06
Iter: 1073 loss: 2.46771037e-06
Iter: 1074 loss: 2.46443233e-06
Iter: 1075 loss: 2.54442421e-06
Iter: 1076 loss: 2.46440186e-06
Iter: 1077 loss: 2.46091804e-06
Iter: 1078 loss: 2.46629838e-06
Iter: 1079 loss: 2.45928777e-06
Iter: 1080 loss: 2.45670549e-06
Iter: 1081 loss: 2.45653382e-06
Iter: 1082 loss: 2.4546996e-06
Iter: 1083 loss: 2.4511121e-06
Iter: 1084 loss: 2.52107316e-06
Iter: 1085 loss: 2.4510673e-06
Iter: 1086 loss: 2.44748639e-06
Iter: 1087 loss: 2.4664937e-06
Iter: 1088 loss: 2.44693706e-06
Iter: 1089 loss: 2.4441656e-06
Iter: 1090 loss: 2.48819151e-06
Iter: 1091 loss: 2.44418811e-06
Iter: 1092 loss: 2.44271064e-06
Iter: 1093 loss: 2.43925956e-06
Iter: 1094 loss: 2.48625224e-06
Iter: 1095 loss: 2.43908244e-06
Iter: 1096 loss: 2.43536829e-06
Iter: 1097 loss: 2.46691343e-06
Iter: 1098 loss: 2.43519662e-06
Iter: 1099 loss: 2.43243767e-06
Iter: 1100 loss: 2.44507828e-06
Iter: 1101 loss: 2.43195382e-06
Iter: 1102 loss: 2.42956e-06
Iter: 1103 loss: 2.4360379e-06
Iter: 1104 loss: 2.42871351e-06
Iter: 1105 loss: 2.42608985e-06
Iter: 1106 loss: 2.42459237e-06
Iter: 1107 loss: 2.42343e-06
Iter: 1108 loss: 2.41998805e-06
Iter: 1109 loss: 2.42266651e-06
Iter: 1110 loss: 2.41791031e-06
Iter: 1111 loss: 2.41643374e-06
Iter: 1112 loss: 2.41583712e-06
Iter: 1113 loss: 2.41390512e-06
Iter: 1114 loss: 2.41242196e-06
Iter: 1115 loss: 2.41179396e-06
Iter: 1116 loss: 2.40912595e-06
Iter: 1117 loss: 2.40764757e-06
Iter: 1118 loss: 2.40645522e-06
Iter: 1119 loss: 2.40460759e-06
Iter: 1120 loss: 2.40393865e-06
Iter: 1121 loss: 2.40239569e-06
Iter: 1122 loss: 2.39873089e-06
Iter: 1123 loss: 2.44002968e-06
Iter: 1124 loss: 2.39841347e-06
Iter: 1125 loss: 2.39542624e-06
Iter: 1126 loss: 2.42932083e-06
Iter: 1127 loss: 2.39535575e-06
Iter: 1128 loss: 2.39226028e-06
Iter: 1129 loss: 2.40343206e-06
Iter: 1130 loss: 2.39142582e-06
Iter: 1131 loss: 2.38932103e-06
Iter: 1132 loss: 2.38571079e-06
Iter: 1133 loss: 2.3857242e-06
Iter: 1134 loss: 2.38275152e-06
Iter: 1135 loss: 2.42677834e-06
Iter: 1136 loss: 2.3827738e-06
Iter: 1137 loss: 2.37980794e-06
Iter: 1138 loss: 2.38432722e-06
Iter: 1139 loss: 2.37842028e-06
Iter: 1140 loss: 2.37550148e-06
Iter: 1141 loss: 2.37977429e-06
Iter: 1142 loss: 2.37414247e-06
Iter: 1143 loss: 2.37119366e-06
Iter: 1144 loss: 2.38151074e-06
Iter: 1145 loss: 2.37044787e-06
Iter: 1146 loss: 2.36760934e-06
Iter: 1147 loss: 2.3661155e-06
Iter: 1148 loss: 2.36486312e-06
Iter: 1149 loss: 2.36276105e-06
Iter: 1150 loss: 2.36247843e-06
Iter: 1151 loss: 2.36019173e-06
Iter: 1152 loss: 2.35934954e-06
Iter: 1153 loss: 2.3580335e-06
Iter: 1154 loss: 2.35580364e-06
Iter: 1155 loss: 2.35835159e-06
Iter: 1156 loss: 2.35454581e-06
Iter: 1157 loss: 2.35199195e-06
Iter: 1158 loss: 2.38828e-06
Iter: 1159 loss: 2.35197035e-06
Iter: 1160 loss: 2.35026164e-06
Iter: 1161 loss: 2.34751724e-06
Iter: 1162 loss: 2.34750496e-06
Iter: 1163 loss: 2.3448506e-06
Iter: 1164 loss: 2.36626329e-06
Iter: 1165 loss: 2.34466665e-06
Iter: 1166 loss: 2.34160439e-06
Iter: 1167 loss: 2.34997697e-06
Iter: 1168 loss: 2.34067079e-06
Iter: 1169 loss: 2.33850119e-06
Iter: 1170 loss: 2.33506967e-06
Iter: 1171 loss: 2.33502442e-06
Iter: 1172 loss: 2.33150558e-06
Iter: 1173 loss: 2.34178651e-06
Iter: 1174 loss: 2.33038918e-06
Iter: 1175 loss: 2.32869343e-06
Iter: 1176 loss: 2.32818638e-06
Iter: 1177 loss: 2.32663888e-06
Iter: 1178 loss: 2.32324e-06
Iter: 1179 loss: 2.3757791e-06
Iter: 1180 loss: 2.32310208e-06
Iter: 1181 loss: 2.319548e-06
Iter: 1182 loss: 2.33102696e-06
Iter: 1183 loss: 2.31853664e-06
Iter: 1184 loss: 2.31556373e-06
Iter: 1185 loss: 2.34683102e-06
Iter: 1186 loss: 2.31553395e-06
Iter: 1187 loss: 2.31321496e-06
Iter: 1188 loss: 2.32341063e-06
Iter: 1189 loss: 2.31274112e-06
Iter: 1190 loss: 2.31063223e-06
Iter: 1191 loss: 2.31612216e-06
Iter: 1192 loss: 2.30997102e-06
Iter: 1193 loss: 2.30782348e-06
Iter: 1194 loss: 2.30462911e-06
Iter: 1195 loss: 2.30457908e-06
Iter: 1196 loss: 2.30352794e-06
Iter: 1197 loss: 2.3026189e-06
Iter: 1198 loss: 2.30118076e-06
Iter: 1199 loss: 2.2978079e-06
Iter: 1200 loss: 2.33824244e-06
Iter: 1201 loss: 2.29751959e-06
Iter: 1202 loss: 2.29471925e-06
Iter: 1203 loss: 2.33408741e-06
Iter: 1204 loss: 2.29468924e-06
Iter: 1205 loss: 2.29165789e-06
Iter: 1206 loss: 2.29545094e-06
Iter: 1207 loss: 2.29008128e-06
Iter: 1208 loss: 2.28779413e-06
Iter: 1209 loss: 2.28521958e-06
Iter: 1210 loss: 2.28488125e-06
Iter: 1211 loss: 2.28131739e-06
Iter: 1212 loss: 2.29328612e-06
Iter: 1213 loss: 2.28033559e-06
Iter: 1214 loss: 2.27799433e-06
Iter: 1215 loss: 2.27792725e-06
Iter: 1216 loss: 2.27581268e-06
Iter: 1217 loss: 2.27450346e-06
Iter: 1218 loss: 2.27367832e-06
Iter: 1219 loss: 2.27084615e-06
Iter: 1220 loss: 2.27165651e-06
Iter: 1221 loss: 2.26887209e-06
Iter: 1222 loss: 2.26641509e-06
Iter: 1223 loss: 2.3013215e-06
Iter: 1224 loss: 2.26642715e-06
Iter: 1225 loss: 2.26366092e-06
Iter: 1226 loss: 2.26593647e-06
Iter: 1227 loss: 2.26203952e-06
Iter: 1228 loss: 2.25989766e-06
Iter: 1229 loss: 2.25983513e-06
Iter: 1230 loss: 2.25816575e-06
Iter: 1231 loss: 2.25567237e-06
Iter: 1232 loss: 2.25566805e-06
Iter: 1233 loss: 2.25374288e-06
Iter: 1234 loss: 2.25358372e-06
Iter: 1235 loss: 2.25215354e-06
Iter: 1236 loss: 2.24891392e-06
Iter: 1237 loss: 2.25213262e-06
Iter: 1238 loss: 2.24709e-06
Iter: 1239 loss: 2.24505766e-06
Iter: 1240 loss: 2.24503924e-06
Iter: 1241 loss: 2.24317569e-06
Iter: 1242 loss: 2.24056043e-06
Iter: 1243 loss: 2.24044925e-06
Iter: 1244 loss: 2.23747702e-06
Iter: 1245 loss: 2.23898132e-06
Iter: 1246 loss: 2.23542338e-06
Iter: 1247 loss: 2.23218649e-06
Iter: 1248 loss: 2.23808411e-06
Iter: 1249 loss: 2.23081861e-06
Iter: 1250 loss: 2.22916333e-06
Iter: 1251 loss: 2.22864583e-06
Iter: 1252 loss: 2.22721155e-06
Iter: 1253 loss: 2.22407152e-06
Iter: 1254 loss: 2.27533155e-06
Iter: 1255 loss: 2.22395965e-06
Iter: 1256 loss: 2.22085737e-06
Iter: 1257 loss: 2.2386107e-06
Iter: 1258 loss: 2.22041263e-06
Iter: 1259 loss: 2.21809296e-06
Iter: 1260 loss: 2.25384133e-06
Iter: 1261 loss: 2.21808614e-06
Iter: 1262 loss: 2.21639266e-06
Iter: 1263 loss: 2.21308642e-06
Iter: 1264 loss: 2.27728424e-06
Iter: 1265 loss: 2.21303935e-06
Iter: 1266 loss: 2.21018945e-06
Iter: 1267 loss: 2.23648613e-06
Iter: 1268 loss: 2.21008645e-06
Iter: 1269 loss: 2.20682591e-06
Iter: 1270 loss: 2.21169057e-06
Iter: 1271 loss: 2.20520155e-06
Iter: 1272 loss: 2.20234278e-06
Iter: 1273 loss: 2.20642733e-06
Iter: 1274 loss: 2.20105539e-06
Iter: 1275 loss: 2.19889239e-06
Iter: 1276 loss: 2.19885897e-06
Iter: 1277 loss: 2.1972744e-06
Iter: 1278 loss: 2.19689559e-06
Iter: 1279 loss: 2.19588401e-06
Iter: 1280 loss: 2.19340154e-06
Iter: 1281 loss: 2.19567551e-06
Iter: 1282 loss: 2.19196863e-06
Iter: 1283 loss: 2.18959349e-06
Iter: 1284 loss: 2.18978835e-06
Iter: 1285 loss: 2.1877388e-06
Iter: 1286 loss: 2.18441232e-06
Iter: 1287 loss: 2.19285357e-06
Iter: 1288 loss: 2.18327477e-06
Iter: 1289 loss: 2.18069977e-06
Iter: 1290 loss: 2.1806195e-06
Iter: 1291 loss: 2.17854085e-06
Iter: 1292 loss: 2.17614138e-06
Iter: 1293 loss: 2.17584238e-06
Iter: 1294 loss: 2.17289767e-06
Iter: 1295 loss: 2.1726953e-06
Iter: 1296 loss: 2.17043635e-06
Iter: 1297 loss: 2.16753097e-06
Iter: 1298 loss: 2.16752392e-06
Iter: 1299 loss: 2.16441845e-06
Iter: 1300 loss: 2.17035176e-06
Iter: 1301 loss: 2.16312719e-06
Iter: 1302 loss: 2.16099e-06
Iter: 1303 loss: 2.15897762e-06
Iter: 1304 loss: 2.15850332e-06
Iter: 1305 loss: 2.15675709e-06
Iter: 1306 loss: 2.15630962e-06
Iter: 1307 loss: 2.15508953e-06
Iter: 1308 loss: 2.15236037e-06
Iter: 1309 loss: 2.18747891e-06
Iter: 1310 loss: 2.15221417e-06
Iter: 1311 loss: 2.15075511e-06
Iter: 1312 loss: 2.15040473e-06
Iter: 1313 loss: 2.14886222e-06
Iter: 1314 loss: 2.14631768e-06
Iter: 1315 loss: 2.14629699e-06
Iter: 1316 loss: 2.14360489e-06
Iter: 1317 loss: 2.1443e-06
Iter: 1318 loss: 2.1416854e-06
Iter: 1319 loss: 2.13868543e-06
Iter: 1320 loss: 2.16111675e-06
Iter: 1321 loss: 2.13846238e-06
Iter: 1322 loss: 2.13607154e-06
Iter: 1323 loss: 2.14508782e-06
Iter: 1324 loss: 2.13545195e-06
Iter: 1325 loss: 2.13281783e-06
Iter: 1326 loss: 2.13563635e-06
Iter: 1327 loss: 2.13131443e-06
Iter: 1328 loss: 2.12805162e-06
Iter: 1329 loss: 2.15464456e-06
Iter: 1330 loss: 2.12783016e-06
Iter: 1331 loss: 2.12574628e-06
Iter: 1332 loss: 2.12447549e-06
Iter: 1333 loss: 2.12364307e-06
Iter: 1334 loss: 2.12098985e-06
Iter: 1335 loss: 2.12546411e-06
Iter: 1336 loss: 2.11983206e-06
Iter: 1337 loss: 2.11776614e-06
Iter: 1338 loss: 2.11765814e-06
Iter: 1339 loss: 2.11624547e-06
Iter: 1340 loss: 2.11368297e-06
Iter: 1341 loss: 2.16983108e-06
Iter: 1342 loss: 2.11366796e-06
Iter: 1343 loss: 2.11163297e-06
Iter: 1344 loss: 2.14059605e-06
Iter: 1345 loss: 2.11162023e-06
Iter: 1346 loss: 2.10933467e-06
Iter: 1347 loss: 2.10779717e-06
Iter: 1348 loss: 2.10689723e-06
Iter: 1349 loss: 2.10460234e-06
Iter: 1350 loss: 2.11548195e-06
Iter: 1351 loss: 2.10418671e-06
Iter: 1352 loss: 2.10165217e-06
Iter: 1353 loss: 2.11392262e-06
Iter: 1354 loss: 2.10116013e-06
Iter: 1355 loss: 2.09971176e-06
Iter: 1356 loss: 2.09642576e-06
Iter: 1357 loss: 2.14026545e-06
Iter: 1358 loss: 2.09630525e-06
Iter: 1359 loss: 2.09281143e-06
Iter: 1360 loss: 2.11349516e-06
Iter: 1361 loss: 2.09238601e-06
Iter: 1362 loss: 2.08958522e-06
Iter: 1363 loss: 2.11405313e-06
Iter: 1364 loss: 2.0894513e-06
Iter: 1365 loss: 2.08704387e-06
Iter: 1366 loss: 2.08964752e-06
Iter: 1367 loss: 2.08568827e-06
Iter: 1368 loss: 2.08322172e-06
Iter: 1369 loss: 2.09836116e-06
Iter: 1370 loss: 2.08294205e-06
Iter: 1371 loss: 2.08126949e-06
Iter: 1372 loss: 2.07976791e-06
Iter: 1373 loss: 2.07932226e-06
Iter: 1374 loss: 2.07751054e-06
Iter: 1375 loss: 2.07749144e-06
Iter: 1376 loss: 2.07575749e-06
Iter: 1377 loss: 2.07435505e-06
Iter: 1378 loss: 2.07382982e-06
Iter: 1379 loss: 2.07179437e-06
Iter: 1380 loss: 2.07615449e-06
Iter: 1381 loss: 2.07106132e-06
Iter: 1382 loss: 2.06838058e-06
Iter: 1383 loss: 2.08624192e-06
Iter: 1384 loss: 2.06807181e-06
Iter: 1385 loss: 2.06657751e-06
Iter: 1386 loss: 2.06501272e-06
Iter: 1387 loss: 2.06481741e-06
Iter: 1388 loss: 2.06267305e-06
Iter: 1389 loss: 2.06265668e-06
Iter: 1390 loss: 2.06120967e-06
Iter: 1391 loss: 2.05843753e-06
Iter: 1392 loss: 2.11752172e-06
Iter: 1393 loss: 2.05844435e-06
Iter: 1394 loss: 2.05546939e-06
Iter: 1395 loss: 2.05745255e-06
Iter: 1396 loss: 2.05368542e-06
Iter: 1397 loss: 2.05006836e-06
Iter: 1398 loss: 2.06836421e-06
Iter: 1399 loss: 2.04948515e-06
Iter: 1400 loss: 2.04776507e-06
Iter: 1401 loss: 2.04763728e-06
Iter: 1402 loss: 2.04621324e-06
Iter: 1403 loss: 2.04450475e-06
Iter: 1404 loss: 2.04430626e-06
Iter: 1405 loss: 2.04218645e-06
Iter: 1406 loss: 2.04723938e-06
Iter: 1407 loss: 2.04138269e-06
Iter: 1408 loss: 2.03888862e-06
Iter: 1409 loss: 2.05621518e-06
Iter: 1410 loss: 2.03864056e-06
Iter: 1411 loss: 2.03679974e-06
Iter: 1412 loss: 2.04482944e-06
Iter: 1413 loss: 2.03635591e-06
Iter: 1414 loss: 2.03447689e-06
Iter: 1415 loss: 2.03148329e-06
Iter: 1416 loss: 2.03145601e-06
Iter: 1417 loss: 2.02938054e-06
Iter: 1418 loss: 2.02925776e-06
Iter: 1419 loss: 2.02718638e-06
Iter: 1420 loss: 2.02617457e-06
Iter: 1421 loss: 2.02514138e-06
Iter: 1422 loss: 2.02337924e-06
Iter: 1423 loss: 2.03979607e-06
Iter: 1424 loss: 2.02330125e-06
Iter: 1425 loss: 2.02144361e-06
Iter: 1426 loss: 2.02296906e-06
Iter: 1427 loss: 2.0203579e-06
Iter: 1428 loss: 2.01872081e-06
Iter: 1429 loss: 2.01602143e-06
Iter: 1430 loss: 2.01606599e-06
Iter: 1431 loss: 2.01263197e-06
Iter: 1432 loss: 2.01975558e-06
Iter: 1433 loss: 2.01120042e-06
Iter: 1434 loss: 2.00907675e-06
Iter: 1435 loss: 2.0090115e-06
Iter: 1436 loss: 2.00670274e-06
Iter: 1437 loss: 2.00882664e-06
Iter: 1438 loss: 2.00536624e-06
Iter: 1439 loss: 2.00361774e-06
Iter: 1440 loss: 2.00236764e-06
Iter: 1441 loss: 2.00176237e-06
Iter: 1442 loss: 2.00022851e-06
Iter: 1443 loss: 1.99999454e-06
Iter: 1444 loss: 1.99862052e-06
Iter: 1445 loss: 1.99800934e-06
Iter: 1446 loss: 1.99729129e-06
Iter: 1447 loss: 1.99537317e-06
Iter: 1448 loss: 1.99910232e-06
Iter: 1449 loss: 1.99459464e-06
Iter: 1450 loss: 1.99210263e-06
Iter: 1451 loss: 2.00067802e-06
Iter: 1452 loss: 1.99146871e-06
Iter: 1453 loss: 1.98908106e-06
Iter: 1454 loss: 1.99953524e-06
Iter: 1455 loss: 1.98858515e-06
Iter: 1456 loss: 1.98694875e-06
Iter: 1457 loss: 1.98732641e-06
Iter: 1458 loss: 1.98572502e-06
Iter: 1459 loss: 1.98368025e-06
Iter: 1460 loss: 2.00590898e-06
Iter: 1461 loss: 1.98357839e-06
Iter: 1462 loss: 1.98231328e-06
Iter: 1463 loss: 1.97956729e-06
Iter: 1464 loss: 2.02480624e-06
Iter: 1465 loss: 1.97942768e-06
Iter: 1466 loss: 1.97643794e-06
Iter: 1467 loss: 1.98527687e-06
Iter: 1468 loss: 1.97551094e-06
Iter: 1469 loss: 1.97277495e-06
Iter: 1470 loss: 1.97873851e-06
Iter: 1471 loss: 1.97169493e-06
Iter: 1472 loss: 1.97002191e-06
Iter: 1473 loss: 1.9699296e-06
Iter: 1474 loss: 1.9682634e-06
Iter: 1475 loss: 1.96797191e-06
Iter: 1476 loss: 1.96688234e-06
Iter: 1477 loss: 1.96494921e-06
Iter: 1478 loss: 1.96419091e-06
Iter: 1479 loss: 1.96315113e-06
Iter: 1480 loss: 1.96056044e-06
Iter: 1481 loss: 1.98692055e-06
Iter: 1482 loss: 1.96047722e-06
Iter: 1483 loss: 1.95766961e-06
Iter: 1484 loss: 1.96196311e-06
Iter: 1485 loss: 1.95628536e-06
Iter: 1486 loss: 1.95442135e-06
Iter: 1487 loss: 1.95563734e-06
Iter: 1488 loss: 1.95325856e-06
Iter: 1489 loss: 1.95073289e-06
Iter: 1490 loss: 1.97257191e-06
Iter: 1491 loss: 1.9505635e-06
Iter: 1492 loss: 1.94917311e-06
Iter: 1493 loss: 1.95281928e-06
Iter: 1494 loss: 1.9486979e-06
Iter: 1495 loss: 1.94723e-06
Iter: 1496 loss: 1.94684958e-06
Iter: 1497 loss: 1.94593622e-06
Iter: 1498 loss: 1.94371364e-06
Iter: 1499 loss: 1.96408973e-06
Iter: 1500 loss: 1.94357949e-06
Iter: 1501 loss: 1.94217796e-06
Iter: 1502 loss: 1.94000381e-06
Iter: 1503 loss: 1.93995493e-06
Iter: 1504 loss: 1.93758115e-06
Iter: 1505 loss: 1.93851565e-06
Iter: 1506 loss: 1.9358863e-06
Iter: 1507 loss: 1.93283063e-06
Iter: 1508 loss: 1.9496722e-06
Iter: 1509 loss: 1.93238384e-06
Iter: 1510 loss: 1.93067035e-06
Iter: 1511 loss: 1.93056212e-06
Iter: 1512 loss: 1.92906555e-06
Iter: 1513 loss: 1.92591756e-06
Iter: 1514 loss: 1.98263479e-06
Iter: 1515 loss: 1.92586685e-06
Iter: 1516 loss: 1.923215e-06
Iter: 1517 loss: 1.9373208e-06
Iter: 1518 loss: 1.92280959e-06
Iter: 1519 loss: 1.92124526e-06
Iter: 1520 loss: 1.9212016e-06
Iter: 1521 loss: 1.91990785e-06
Iter: 1522 loss: 1.91744857e-06
Iter: 1523 loss: 1.96638712e-06
Iter: 1524 loss: 1.91741788e-06
Iter: 1525 loss: 1.91544359e-06
Iter: 1526 loss: 1.91544814e-06
Iter: 1527 loss: 1.91372874e-06
Iter: 1528 loss: 1.91660683e-06
Iter: 1529 loss: 1.91293975e-06
Iter: 1530 loss: 1.91153708e-06
Iter: 1531 loss: 1.91460958e-06
Iter: 1532 loss: 1.91094296e-06
Iter: 1533 loss: 1.90928586e-06
Iter: 1534 loss: 1.91502113e-06
Iter: 1535 loss: 1.90884657e-06
Iter: 1536 loss: 1.90711717e-06
Iter: 1537 loss: 1.90973196e-06
Iter: 1538 loss: 1.90636479e-06
Iter: 1539 loss: 1.9043971e-06
Iter: 1540 loss: 1.90306855e-06
Iter: 1541 loss: 1.90235482e-06
Iter: 1542 loss: 1.89966136e-06
Iter: 1543 loss: 1.90410765e-06
Iter: 1544 loss: 1.8983892e-06
Iter: 1545 loss: 1.89573609e-06
Iter: 1546 loss: 1.91102e-06
Iter: 1547 loss: 1.89543118e-06
Iter: 1548 loss: 1.89303216e-06
Iter: 1549 loss: 1.91850654e-06
Iter: 1550 loss: 1.89300579e-06
Iter: 1551 loss: 1.8917159e-06
Iter: 1552 loss: 1.88956983e-06
Iter: 1553 loss: 1.88958302e-06
Iter: 1554 loss: 1.88748459e-06
Iter: 1555 loss: 1.90058449e-06
Iter: 1556 loss: 1.88724869e-06
Iter: 1557 loss: 1.88486365e-06
Iter: 1558 loss: 1.89818479e-06
Iter: 1559 loss: 1.88452373e-06
Iter: 1560 loss: 1.88321519e-06
Iter: 1561 loss: 1.88184447e-06
Iter: 1562 loss: 1.88159891e-06
Iter: 1563 loss: 1.88032493e-06
Iter: 1564 loss: 1.88021988e-06
Iter: 1565 loss: 1.87909507e-06
Iter: 1566 loss: 1.87748742e-06
Iter: 1567 loss: 1.87746446e-06
Iter: 1568 loss: 1.87557623e-06
Iter: 1569 loss: 1.88770514e-06
Iter: 1570 loss: 1.87539786e-06
Iter: 1571 loss: 1.87335218e-06
Iter: 1572 loss: 1.87511012e-06
Iter: 1573 loss: 1.87214175e-06
Iter: 1574 loss: 1.87003468e-06
Iter: 1575 loss: 1.87749879e-06
Iter: 1576 loss: 1.86948921e-06
Iter: 1577 loss: 1.86764294e-06
Iter: 1578 loss: 1.86630405e-06
Iter: 1579 loss: 1.86568548e-06
Iter: 1580 loss: 1.86310251e-06
Iter: 1581 loss: 1.86833e-06
Iter: 1582 loss: 1.86209638e-06
Iter: 1583 loss: 1.85975887e-06
Iter: 1584 loss: 1.87466048e-06
Iter: 1585 loss: 1.85953547e-06
Iter: 1586 loss: 1.85729016e-06
Iter: 1587 loss: 1.87681701e-06
Iter: 1588 loss: 1.85712634e-06
Iter: 1589 loss: 1.8558394e-06
Iter: 1590 loss: 1.85437568e-06
Iter: 1591 loss: 1.854213e-06
Iter: 1592 loss: 1.85264071e-06
Iter: 1593 loss: 1.87453008e-06
Iter: 1594 loss: 1.85265912e-06
Iter: 1595 loss: 1.85105785e-06
Iter: 1596 loss: 1.85245835e-06
Iter: 1597 loss: 1.85011231e-06
Iter: 1598 loss: 1.84845283e-06
Iter: 1599 loss: 1.84684768e-06
Iter: 1600 loss: 1.84650253e-06
Iter: 1601 loss: 1.84517228e-06
Iter: 1602 loss: 1.84479518e-06
Iter: 1603 loss: 1.84373835e-06
Iter: 1604 loss: 1.84119062e-06
Iter: 1605 loss: 1.87806427e-06
Iter: 1606 loss: 1.84104715e-06
Iter: 1607 loss: 1.83942461e-06
Iter: 1608 loss: 1.83940233e-06
Iter: 1609 loss: 1.83774716e-06
Iter: 1610 loss: 1.83714735e-06
Iter: 1611 loss: 1.83631357e-06
Iter: 1612 loss: 1.83466273e-06
Iter: 1613 loss: 1.84073099e-06
Iter: 1614 loss: 1.83426766e-06
Iter: 1615 loss: 1.83238853e-06
Iter: 1616 loss: 1.83106556e-06
Iter: 1617 loss: 1.83041948e-06
Iter: 1618 loss: 1.82795179e-06
Iter: 1619 loss: 1.8338402e-06
Iter: 1620 loss: 1.82705901e-06
Iter: 1621 loss: 1.82498218e-06
Iter: 1622 loss: 1.83979682e-06
Iter: 1623 loss: 1.82478118e-06
Iter: 1624 loss: 1.82283611e-06
Iter: 1625 loss: 1.84001397e-06
Iter: 1626 loss: 1.8227513e-06
Iter: 1627 loss: 1.82172153e-06
Iter: 1628 loss: 1.81953237e-06
Iter: 1629 loss: 1.85799934e-06
Iter: 1630 loss: 1.81948519e-06
Iter: 1631 loss: 1.81839619e-06
Iter: 1632 loss: 1.81798544e-06
Iter: 1633 loss: 1.81683481e-06
Iter: 1634 loss: 1.81476844e-06
Iter: 1635 loss: 1.86435796e-06
Iter: 1636 loss: 1.81475309e-06
Iter: 1637 loss: 1.81324435e-06
Iter: 1638 loss: 1.8132298e-06
Iter: 1639 loss: 1.81191558e-06
Iter: 1640 loss: 1.81064252e-06
Iter: 1641 loss: 1.81035114e-06
Iter: 1642 loss: 1.80892732e-06
Iter: 1643 loss: 1.82448161e-06
Iter: 1644 loss: 1.8089288e-06
Iter: 1645 loss: 1.80749328e-06
Iter: 1646 loss: 1.80662016e-06
Iter: 1647 loss: 1.80602956e-06
Iter: 1648 loss: 1.80418385e-06
Iter: 1649 loss: 1.80692768e-06
Iter: 1650 loss: 1.80333575e-06
Iter: 1651 loss: 1.80095356e-06
Iter: 1652 loss: 1.80588802e-06
Iter: 1653 loss: 1.80001553e-06
Iter: 1654 loss: 1.79778738e-06
Iter: 1655 loss: 1.79820233e-06
Iter: 1656 loss: 1.79620451e-06
Iter: 1657 loss: 1.79462052e-06
Iter: 1658 loss: 1.79455174e-06
Iter: 1659 loss: 1.79290419e-06
Iter: 1660 loss: 1.79393214e-06
Iter: 1661 loss: 1.7917863e-06
Iter: 1662 loss: 1.79019821e-06
Iter: 1663 loss: 1.79109645e-06
Iter: 1664 loss: 1.78912956e-06
Iter: 1665 loss: 1.78733478e-06
Iter: 1666 loss: 1.8165241e-06
Iter: 1667 loss: 1.78733978e-06
Iter: 1668 loss: 1.78621838e-06
Iter: 1669 loss: 1.78451569e-06
Iter: 1670 loss: 1.78449045e-06
Iter: 1671 loss: 1.78304049e-06
Iter: 1672 loss: 1.78306459e-06
Iter: 1673 loss: 1.7817855e-06
Iter: 1674 loss: 1.77964557e-06
Iter: 1675 loss: 1.77962886e-06
Iter: 1676 loss: 1.77823335e-06
Iter: 1677 loss: 1.77822244e-06
Iter: 1678 loss: 1.77681932e-06
Iter: 1679 loss: 1.77566653e-06
Iter: 1680 loss: 1.7752451e-06
Iter: 1681 loss: 1.77341735e-06
Iter: 1682 loss: 1.77466825e-06
Iter: 1683 loss: 1.77224024e-06
Iter: 1684 loss: 1.77008485e-06
Iter: 1685 loss: 1.78320204e-06
Iter: 1686 loss: 1.76982132e-06
Iter: 1687 loss: 1.76786557e-06
Iter: 1688 loss: 1.76847607e-06
Iter: 1689 loss: 1.76654e-06
Iter: 1690 loss: 1.76424737e-06
Iter: 1691 loss: 1.76994149e-06
Iter: 1692 loss: 1.76344292e-06
Iter: 1693 loss: 1.76126412e-06
Iter: 1694 loss: 1.76444496e-06
Iter: 1695 loss: 1.76015635e-06
Iter: 1696 loss: 1.75811988e-06
Iter: 1697 loss: 1.78184621e-06
Iter: 1698 loss: 1.75806804e-06
Iter: 1699 loss: 1.7558973e-06
Iter: 1700 loss: 1.7619268e-06
Iter: 1701 loss: 1.75516277e-06
Iter: 1702 loss: 1.75392756e-06
Iter: 1703 loss: 1.7548407e-06
Iter: 1704 loss: 1.75310822e-06
Iter: 1705 loss: 1.75139269e-06
Iter: 1706 loss: 1.76668163e-06
Iter: 1707 loss: 1.75131117e-06
Iter: 1708 loss: 1.75046102e-06
Iter: 1709 loss: 1.74939851e-06
Iter: 1710 loss: 1.74933746e-06
Iter: 1711 loss: 1.74733464e-06
Iter: 1712 loss: 1.75928403e-06
Iter: 1713 loss: 1.74711704e-06
Iter: 1714 loss: 1.74568e-06
Iter: 1715 loss: 1.74392051e-06
Iter: 1716 loss: 1.7438033e-06
Iter: 1717 loss: 1.74253364e-06
Iter: 1718 loss: 1.74248635e-06
Iter: 1719 loss: 1.74127058e-06
Iter: 1720 loss: 1.74042771e-06
Iter: 1721 loss: 1.73995704e-06
Iter: 1722 loss: 1.73827095e-06
Iter: 1723 loss: 1.74259731e-06
Iter: 1724 loss: 1.73771241e-06
Iter: 1725 loss: 1.73579792e-06
Iter: 1726 loss: 1.7378e-06
Iter: 1727 loss: 1.73471494e-06
Iter: 1728 loss: 1.73232468e-06
Iter: 1729 loss: 1.73353226e-06
Iter: 1730 loss: 1.73073806e-06
Iter: 1731 loss: 1.72828277e-06
Iter: 1732 loss: 1.73903368e-06
Iter: 1733 loss: 1.72778732e-06
Iter: 1734 loss: 1.72661294e-06
Iter: 1735 loss: 1.7265437e-06
Iter: 1736 loss: 1.72524574e-06
Iter: 1737 loss: 1.72421403e-06
Iter: 1738 loss: 1.72386876e-06
Iter: 1739 loss: 1.72216244e-06
Iter: 1740 loss: 1.72699083e-06
Iter: 1741 loss: 1.7217094e-06
Iter: 1742 loss: 1.71984254e-06
Iter: 1743 loss: 1.73256308e-06
Iter: 1744 loss: 1.71967611e-06
Iter: 1745 loss: 1.71850354e-06
Iter: 1746 loss: 1.7173893e-06
Iter: 1747 loss: 1.7171044e-06
Iter: 1748 loss: 1.71545662e-06
Iter: 1749 loss: 1.71548299e-06
Iter: 1750 loss: 1.71450802e-06
Iter: 1751 loss: 1.71213173e-06
Iter: 1752 loss: 1.73526678e-06
Iter: 1753 loss: 1.71182751e-06
Iter: 1754 loss: 1.710633e-06
Iter: 1755 loss: 1.71042404e-06
Iter: 1756 loss: 1.70885141e-06
Iter: 1757 loss: 1.70791657e-06
Iter: 1758 loss: 1.70735984e-06
Iter: 1759 loss: 1.70555313e-06
Iter: 1760 loss: 1.70431292e-06
Iter: 1761 loss: 1.70365911e-06
Iter: 1762 loss: 1.70113208e-06
Iter: 1763 loss: 1.71095326e-06
Iter: 1764 loss: 1.70052431e-06
Iter: 1765 loss: 1.69816064e-06
Iter: 1766 loss: 1.70889814e-06
Iter: 1767 loss: 1.69776e-06
Iter: 1768 loss: 1.69605164e-06
Iter: 1769 loss: 1.72246473e-06
Iter: 1770 loss: 1.69604255e-06
Iter: 1771 loss: 1.69471332e-06
Iter: 1772 loss: 1.69707459e-06
Iter: 1773 loss: 1.69413806e-06
Iter: 1774 loss: 1.69265149e-06
Iter: 1775 loss: 1.69335783e-06
Iter: 1776 loss: 1.69165492e-06
Iter: 1777 loss: 1.69012662e-06
Iter: 1778 loss: 1.69883208e-06
Iter: 1779 loss: 1.68989345e-06
Iter: 1780 loss: 1.68825045e-06
Iter: 1781 loss: 1.69252053e-06
Iter: 1782 loss: 1.68766371e-06
Iter: 1783 loss: 1.6865763e-06
Iter: 1784 loss: 1.68659039e-06
Iter: 1785 loss: 1.68573729e-06
Iter: 1786 loss: 1.68387191e-06
Iter: 1787 loss: 1.69481314e-06
Iter: 1788 loss: 1.6836342e-06
Iter: 1789 loss: 1.68222414e-06
Iter: 1790 loss: 1.68038832e-06
Iter: 1791 loss: 1.68028112e-06
Iter: 1792 loss: 1.67868029e-06
Iter: 1793 loss: 1.67868734e-06
Iter: 1794 loss: 1.67714859e-06
Iter: 1795 loss: 1.67766973e-06
Iter: 1796 loss: 1.67603491e-06
Iter: 1797 loss: 1.67439714e-06
Iter: 1798 loss: 1.67312021e-06
Iter: 1799 loss: 1.67254984e-06
Iter: 1800 loss: 1.6704596e-06
Iter: 1801 loss: 1.67699068e-06
Iter: 1802 loss: 1.66977122e-06
Iter: 1803 loss: 1.66823111e-06
Iter: 1804 loss: 1.6924912e-06
Iter: 1805 loss: 1.66824657e-06
Iter: 1806 loss: 1.66650807e-06
Iter: 1807 loss: 1.66797417e-06
Iter: 1808 loss: 1.66543191e-06
Iter: 1809 loss: 1.66403868e-06
Iter: 1810 loss: 1.6674162e-06
Iter: 1811 loss: 1.66351253e-06
Iter: 1812 loss: 1.66197128e-06
Iter: 1813 loss: 1.67160192e-06
Iter: 1814 loss: 1.66180803e-06
Iter: 1815 loss: 1.66056248e-06
Iter: 1816 loss: 1.66293398e-06
Iter: 1817 loss: 1.66003633e-06
Iter: 1818 loss: 1.6587303e-06
Iter: 1819 loss: 1.65901815e-06
Iter: 1820 loss: 1.6577726e-06
Iter: 1821 loss: 1.65628126e-06
Iter: 1822 loss: 1.67094072e-06
Iter: 1823 loss: 1.65619758e-06
Iter: 1824 loss: 1.65485403e-06
Iter: 1825 loss: 1.65316942e-06
Iter: 1826 loss: 1.6530505e-06
Iter: 1827 loss: 1.65136294e-06
Iter: 1828 loss: 1.66332848e-06
Iter: 1829 loss: 1.65116489e-06
Iter: 1830 loss: 1.64935602e-06
Iter: 1831 loss: 1.65610891e-06
Iter: 1832 loss: 1.64899711e-06
Iter: 1833 loss: 1.6477461e-06
Iter: 1834 loss: 1.6461795e-06
Iter: 1835 loss: 1.64606377e-06
Iter: 1836 loss: 1.64412336e-06
Iter: 1837 loss: 1.64669621e-06
Iter: 1838 loss: 1.64314167e-06
Iter: 1839 loss: 1.64209246e-06
Iter: 1840 loss: 1.64192022e-06
Iter: 1841 loss: 1.64058918e-06
Iter: 1842 loss: 1.64033577e-06
Iter: 1843 loss: 1.63942786e-06
Iter: 1844 loss: 1.63781783e-06
Iter: 1845 loss: 1.63965808e-06
Iter: 1846 loss: 1.63696109e-06
Iter: 1847 loss: 1.63532127e-06
Iter: 1848 loss: 1.65715494e-06
Iter: 1849 loss: 1.63530581e-06
Iter: 1850 loss: 1.63412415e-06
Iter: 1851 loss: 1.63277e-06
Iter: 1852 loss: 1.63265327e-06
Iter: 1853 loss: 1.63077539e-06
Iter: 1854 loss: 1.64618712e-06
Iter: 1855 loss: 1.63064362e-06
Iter: 1856 loss: 1.62940228e-06
Iter: 1857 loss: 1.63345328e-06
Iter: 1858 loss: 1.6289689e-06
Iter: 1859 loss: 1.62771494e-06
Iter: 1860 loss: 1.62893161e-06
Iter: 1861 loss: 1.6269737e-06
Iter: 1862 loss: 1.62565095e-06
Iter: 1863 loss: 1.6277113e-06
Iter: 1864 loss: 1.62500089e-06
Iter: 1865 loss: 1.62346487e-06
Iter: 1866 loss: 1.63826803e-06
Iter: 1867 loss: 1.62339313e-06
Iter: 1868 loss: 1.62252377e-06
Iter: 1869 loss: 1.62028687e-06
Iter: 1870 loss: 1.64090579e-06
Iter: 1871 loss: 1.61993023e-06
Iter: 1872 loss: 1.61716855e-06
Iter: 1873 loss: 1.62619119e-06
Iter: 1874 loss: 1.61639605e-06
Iter: 1875 loss: 1.61487606e-06
Iter: 1876 loss: 1.61476396e-06
Iter: 1877 loss: 1.61326307e-06
Iter: 1878 loss: 1.61701735e-06
Iter: 1879 loss: 1.61268588e-06
Iter: 1880 loss: 1.61163166e-06
Iter: 1881 loss: 1.61055971e-06
Iter: 1882 loss: 1.6103229e-06
Iter: 1883 loss: 1.60890136e-06
Iter: 1884 loss: 1.608905e-06
Iter: 1885 loss: 1.60798993e-06
Iter: 1886 loss: 1.60604031e-06
Iter: 1887 loss: 1.6396491e-06
Iter: 1888 loss: 1.60599745e-06
Iter: 1889 loss: 1.60479215e-06
Iter: 1890 loss: 1.60470563e-06
Iter: 1891 loss: 1.60356535e-06
Iter: 1892 loss: 1.60299533e-06
Iter: 1893 loss: 1.60243121e-06
Iter: 1894 loss: 1.60093532e-06
Iter: 1895 loss: 1.60795059e-06
Iter: 1896 loss: 1.60061802e-06
Iter: 1897 loss: 1.599347e-06
Iter: 1898 loss: 1.60276193e-06
Iter: 1899 loss: 1.59895751e-06
Iter: 1900 loss: 1.59744081e-06
Iter: 1901 loss: 1.60287937e-06
Iter: 1902 loss: 1.59708486e-06
Iter: 1903 loss: 1.59580202e-06
Iter: 1904 loss: 1.59535557e-06
Iter: 1905 loss: 1.59466504e-06
Iter: 1906 loss: 1.5928307e-06
Iter: 1907 loss: 1.59309639e-06
Iter: 1908 loss: 1.5915e-06
Iter: 1909 loss: 1.59149226e-06
Iter: 1910 loss: 1.59062529e-06
Iter: 1911 loss: 1.5897831e-06
Iter: 1912 loss: 1.58798161e-06
Iter: 1913 loss: 1.61846424e-06
Iter: 1914 loss: 1.58792261e-06
Iter: 1915 loss: 1.58631201e-06
Iter: 1916 loss: 1.59840374e-06
Iter: 1917 loss: 1.58615512e-06
Iter: 1918 loss: 1.58447347e-06
Iter: 1919 loss: 1.59126034e-06
Iter: 1920 loss: 1.58409762e-06
Iter: 1921 loss: 1.58307216e-06
Iter: 1922 loss: 1.58177158e-06
Iter: 1923 loss: 1.58166699e-06
Iter: 1924 loss: 1.58004298e-06
Iter: 1925 loss: 1.58003706e-06
Iter: 1926 loss: 1.5788878e-06
Iter: 1927 loss: 1.57785485e-06
Iter: 1928 loss: 1.57756585e-06
Iter: 1929 loss: 1.57597708e-06
Iter: 1930 loss: 1.58534249e-06
Iter: 1931 loss: 1.5757978e-06
Iter: 1932 loss: 1.57436875e-06
Iter: 1933 loss: 1.57756961e-06
Iter: 1934 loss: 1.57382624e-06
Iter: 1935 loss: 1.57237696e-06
Iter: 1936 loss: 1.57918089e-06
Iter: 1937 loss: 1.57210502e-06
Iter: 1938 loss: 1.57092916e-06
Iter: 1939 loss: 1.56932697e-06
Iter: 1940 loss: 1.56924307e-06
Iter: 1941 loss: 1.56736519e-06
Iter: 1942 loss: 1.57669547e-06
Iter: 1943 loss: 1.56701981e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi1.2
+ date
Wed Nov  4 13:08:49 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi0.8/300_300_300_1 --function f2 --psi 1 --alpha 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4637158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4661c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4512730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e45212f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4521bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e44d2400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e43fdea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4402730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4402400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e44439d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4443ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4463d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4463a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e439bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e430dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e44906a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e44907b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e42e09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4240950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e426b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e426b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4225598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e426dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e433f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e433fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e413e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4521400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e414e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e414e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e41781e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e40c6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e40e7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e40e7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e40e7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4072b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e40569d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.013396837
test_loss: 0.01456506
train_loss: 0.012754284
test_loss: 0.012313104
train_loss: 0.009660734
test_loss: 0.011336498
train_loss: 0.009699752
test_loss: 0.010412694
train_loss: 0.008354457
test_loss: 0.010063738
train_loss: 0.008451498
test_loss: 0.010110975
train_loss: 0.007883326
test_loss: 0.009585862
train_loss: 0.007955058
test_loss: 0.009910378
train_loss: 0.00802654
test_loss: 0.009253476
train_loss: 0.008355822
test_loss: 0.009487196
train_loss: 0.007537598
test_loss: 0.0092999535
train_loss: 0.006909178
test_loss: 0.009072502
train_loss: 0.0076558967
test_loss: 0.009385286
train_loss: 0.006889322
test_loss: 0.008635542
train_loss: 0.007118754
test_loss: 0.008964186
train_loss: 0.0067412835
test_loss: 0.00924524
train_loss: 0.0071618
test_loss: 0.008981278
train_loss: 0.007053774
test_loss: 0.00900288
train_loss: 0.006142507
test_loss: 0.008506016
train_loss: 0.0070103696
test_loss: 0.008735347
train_loss: 0.0067517506
test_loss: 0.00878031
train_loss: 0.007145618
test_loss: 0.008769819
train_loss: 0.006867718
test_loss: 0.00906658
train_loss: 0.009126073
test_loss: 0.010024005
train_loss: 0.0070301685
test_loss: 0.008817381
train_loss: 0.006349331
test_loss: 0.0084748175
train_loss: 0.006340352
test_loss: 0.008493499
train_loss: 0.0061032134
test_loss: 0.008349404
train_loss: 0.0061740037
test_loss: 0.008394873
train_loss: 0.005962003
test_loss: 0.008270207
train_loss: 0.006033752
test_loss: 0.008253529
train_loss: 0.006639554
test_loss: 0.008325183
train_loss: 0.0060075745
test_loss: 0.0086478805
train_loss: 0.0070993705
test_loss: 0.008544047
train_loss: 0.0061619715
test_loss: 0.00819507
train_loss: 0.0059227888
test_loss: 0.008174422
train_loss: 0.0070382226
test_loss: 0.008962655
train_loss: 0.0060362006
test_loss: 0.008196588
train_loss: 0.0061215097
test_loss: 0.0080710575
train_loss: 0.006056152
test_loss: 0.008260937
train_loss: 0.0058998717
test_loss: 0.007993518
train_loss: 0.0059055123
test_loss: 0.008147612
train_loss: 0.0062659523
test_loss: 0.008473661
train_loss: 0.005705634
test_loss: 0.008333855
train_loss: 0.0066529196
test_loss: 0.008287664
train_loss: 0.006093316
test_loss: 0.008126232
train_loss: 0.0061198287
test_loss: 0.008185134
train_loss: 0.0059456094
test_loss: 0.008071419
train_loss: 0.0067358045
test_loss: 0.008257709
train_loss: 0.0061359443
test_loss: 0.0081684375
train_loss: 0.0058770366
test_loss: 0.008231613
train_loss: 0.006631361
test_loss: 0.008565212
train_loss: 0.0058198324
test_loss: 0.008097507
train_loss: 0.005871485
test_loss: 0.008101388
train_loss: 0.005731291
test_loss: 0.007927923
train_loss: 0.0060030357
test_loss: 0.007923628
train_loss: 0.005669848
test_loss: 0.008159807
train_loss: 0.0060506286
test_loss: 0.007906586
train_loss: 0.005407854
test_loss: 0.007669926
train_loss: 0.0060020513
test_loss: 0.008149807
train_loss: 0.0060126474
test_loss: 0.008104424
train_loss: 0.0058571724
test_loss: 0.007859711
train_loss: 0.0054888874
test_loss: 0.007859546
train_loss: 0.005771215
test_loss: 0.007922391
train_loss: 0.0059089605
test_loss: 0.008009086
train_loss: 0.006051555
test_loss: 0.007970417
train_loss: 0.005181493
test_loss: 0.007908116
train_loss: 0.0059150667
test_loss: 0.008059797
train_loss: 0.0055566784
test_loss: 0.007898559
train_loss: 0.0058431453
test_loss: 0.008105978
train_loss: 0.005738404
test_loss: 0.007916251
train_loss: 0.005133438
test_loss: 0.007850022
train_loss: 0.0052340375
test_loss: 0.0077869045
train_loss: 0.005223927
test_loss: 0.008071395
train_loss: 0.00516026
test_loss: 0.0076044984
train_loss: 0.005533983
test_loss: 0.008028246
train_loss: 0.0055048084
test_loss: 0.007957366
train_loss: 0.0054678787
test_loss: 0.008134939
train_loss: 0.005539745
test_loss: 0.007864364
train_loss: 0.005512924
test_loss: 0.007988283
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.2/300_300_300_1 --optimizer lbfgs --function f2 --psi 1 --alpha 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac45df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac4bfd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac3f0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac37a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac37ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac3a0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac365a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac3122f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac304620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac3049d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac37a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac3126a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a30d2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a30b4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a30d2d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a306aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a306ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a302ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2fdb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a302b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2f95d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2fa9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2f65ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2f11950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2f11d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2f432f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac3b4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2f11510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2ea3378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2ea3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2ea3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2e16598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2e16b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1ac312510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2dd3c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1a2d86f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.11556058e-05
Iter: 2 loss: 5.19295791e-05
Iter: 3 loss: 0.00020761753
Iter: 4 loss: 5.12507904e-05
Iter: 5 loss: 4.28806416e-05
Iter: 6 loss: 9.26089851e-05
Iter: 7 loss: 4.1737625e-05
Iter: 8 loss: 3.79486082e-05
Iter: 9 loss: 4.20259094e-05
Iter: 10 loss: 3.58852594e-05
Iter: 11 loss: 3.19326282e-05
Iter: 12 loss: 6.35701144e-05
Iter: 13 loss: 3.1671163e-05
Iter: 14 loss: 3.00239917e-05
Iter: 15 loss: 2.8941542e-05
Iter: 16 loss: 2.83134e-05
Iter: 17 loss: 2.65994659e-05
Iter: 18 loss: 4.27344785e-05
Iter: 19 loss: 2.65309573e-05
Iter: 20 loss: 2.4951767e-05
Iter: 21 loss: 2.8637096e-05
Iter: 22 loss: 2.43643881e-05
Iter: 23 loss: 2.33456667e-05
Iter: 24 loss: 2.49553923e-05
Iter: 25 loss: 2.28722583e-05
Iter: 26 loss: 2.16281151e-05
Iter: 27 loss: 2.74996819e-05
Iter: 28 loss: 2.14009287e-05
Iter: 29 loss: 2.05770648e-05
Iter: 30 loss: 2.00064587e-05
Iter: 31 loss: 1.97081572e-05
Iter: 32 loss: 1.89292368e-05
Iter: 33 loss: 1.9731935e-05
Iter: 34 loss: 1.84959918e-05
Iter: 35 loss: 1.79672388e-05
Iter: 36 loss: 2.18489949e-05
Iter: 37 loss: 1.79231756e-05
Iter: 38 loss: 1.74527013e-05
Iter: 39 loss: 1.84783639e-05
Iter: 40 loss: 1.72704695e-05
Iter: 41 loss: 1.68189181e-05
Iter: 42 loss: 1.74550041e-05
Iter: 43 loss: 1.65972797e-05
Iter: 44 loss: 1.61636235e-05
Iter: 45 loss: 1.65591227e-05
Iter: 46 loss: 1.59117608e-05
Iter: 47 loss: 1.56177121e-05
Iter: 48 loss: 1.56063506e-05
Iter: 49 loss: 1.53391065e-05
Iter: 50 loss: 1.51722707e-05
Iter: 51 loss: 1.50654141e-05
Iter: 52 loss: 1.47557275e-05
Iter: 53 loss: 1.53391848e-05
Iter: 54 loss: 1.46249149e-05
Iter: 55 loss: 1.43519555e-05
Iter: 56 loss: 1.81697469e-05
Iter: 57 loss: 1.43509733e-05
Iter: 58 loss: 1.41511973e-05
Iter: 59 loss: 1.4051403e-05
Iter: 60 loss: 1.39567346e-05
Iter: 61 loss: 1.37146772e-05
Iter: 62 loss: 1.54785794e-05
Iter: 63 loss: 1.36943645e-05
Iter: 64 loss: 1.34739721e-05
Iter: 65 loss: 1.38293935e-05
Iter: 66 loss: 1.33727062e-05
Iter: 67 loss: 1.31864081e-05
Iter: 68 loss: 1.32477462e-05
Iter: 69 loss: 1.3054083e-05
Iter: 70 loss: 1.28883385e-05
Iter: 71 loss: 1.53005931e-05
Iter: 72 loss: 1.28881666e-05
Iter: 73 loss: 1.27288895e-05
Iter: 74 loss: 1.26684126e-05
Iter: 75 loss: 1.25813631e-05
Iter: 76 loss: 1.24130738e-05
Iter: 77 loss: 1.25562874e-05
Iter: 78 loss: 1.23138689e-05
Iter: 79 loss: 1.21864878e-05
Iter: 80 loss: 1.21845524e-05
Iter: 81 loss: 1.20554896e-05
Iter: 82 loss: 1.22532656e-05
Iter: 83 loss: 1.19942943e-05
Iter: 84 loss: 1.18903899e-05
Iter: 85 loss: 1.18898706e-05
Iter: 86 loss: 1.18070957e-05
Iter: 87 loss: 1.16835963e-05
Iter: 88 loss: 1.3538539e-05
Iter: 89 loss: 1.16833453e-05
Iter: 90 loss: 1.16070314e-05
Iter: 91 loss: 1.14430222e-05
Iter: 92 loss: 1.40013553e-05
Iter: 93 loss: 1.14374079e-05
Iter: 94 loss: 1.12741036e-05
Iter: 95 loss: 1.19986089e-05
Iter: 96 loss: 1.12419539e-05
Iter: 97 loss: 1.11385234e-05
Iter: 98 loss: 1.11355657e-05
Iter: 99 loss: 1.10645387e-05
Iter: 100 loss: 1.10219971e-05
Iter: 101 loss: 1.09927787e-05
Iter: 102 loss: 1.08921759e-05
Iter: 103 loss: 1.11245581e-05
Iter: 104 loss: 1.08547138e-05
Iter: 105 loss: 1.07721535e-05
Iter: 106 loss: 1.17428353e-05
Iter: 107 loss: 1.0770982e-05
Iter: 108 loss: 1.07041124e-05
Iter: 109 loss: 1.05924119e-05
Iter: 110 loss: 1.05920253e-05
Iter: 111 loss: 1.04882347e-05
Iter: 112 loss: 1.10685005e-05
Iter: 113 loss: 1.04735182e-05
Iter: 114 loss: 1.03998254e-05
Iter: 115 loss: 1.12237103e-05
Iter: 116 loss: 1.03985531e-05
Iter: 117 loss: 1.03285547e-05
Iter: 118 loss: 1.0301258e-05
Iter: 119 loss: 1.02636586e-05
Iter: 120 loss: 1.01830638e-05
Iter: 121 loss: 1.02941904e-05
Iter: 122 loss: 1.01430069e-05
Iter: 123 loss: 1.00895541e-05
Iter: 124 loss: 1.00850866e-05
Iter: 125 loss: 1.00365814e-05
Iter: 126 loss: 9.97676125e-06
Iter: 127 loss: 9.97124334e-06
Iter: 128 loss: 9.91705747e-06
Iter: 129 loss: 1.07435135e-05
Iter: 130 loss: 9.91698835e-06
Iter: 131 loss: 9.86342093e-06
Iter: 132 loss: 9.817928e-06
Iter: 133 loss: 9.80317509e-06
Iter: 134 loss: 9.73893293e-06
Iter: 135 loss: 9.76628871e-06
Iter: 136 loss: 9.69458051e-06
Iter: 137 loss: 9.64850733e-06
Iter: 138 loss: 9.64408082e-06
Iter: 139 loss: 9.60509806e-06
Iter: 140 loss: 9.55489941e-06
Iter: 141 loss: 9.5515752e-06
Iter: 142 loss: 9.49474452e-06
Iter: 143 loss: 9.66942935e-06
Iter: 144 loss: 9.47812532e-06
Iter: 145 loss: 9.42013503e-06
Iter: 146 loss: 9.92035e-06
Iter: 147 loss: 9.41709732e-06
Iter: 148 loss: 9.38003359e-06
Iter: 149 loss: 9.34054879e-06
Iter: 150 loss: 9.33374122e-06
Iter: 151 loss: 9.26231587e-06
Iter: 152 loss: 9.38849553e-06
Iter: 153 loss: 9.23083371e-06
Iter: 154 loss: 9.16296722e-06
Iter: 155 loss: 9.30067836e-06
Iter: 156 loss: 9.13538315e-06
Iter: 157 loss: 9.08298352e-06
Iter: 158 loss: 9.08278707e-06
Iter: 159 loss: 9.03655746e-06
Iter: 160 loss: 9.05229172e-06
Iter: 161 loss: 9.00391115e-06
Iter: 162 loss: 8.9560408e-06
Iter: 163 loss: 8.99567203e-06
Iter: 164 loss: 8.92728e-06
Iter: 165 loss: 8.90440515e-06
Iter: 166 loss: 8.89805233e-06
Iter: 167 loss: 8.87007627e-06
Iter: 168 loss: 8.80698644e-06
Iter: 169 loss: 9.67744381e-06
Iter: 170 loss: 8.80349762e-06
Iter: 171 loss: 8.75061869e-06
Iter: 172 loss: 8.9917512e-06
Iter: 173 loss: 8.74055149e-06
Iter: 174 loss: 8.70289568e-06
Iter: 175 loss: 8.70276e-06
Iter: 176 loss: 8.67777817e-06
Iter: 177 loss: 8.62978413e-06
Iter: 178 loss: 9.64191622e-06
Iter: 179 loss: 8.62957404e-06
Iter: 180 loss: 8.58161729e-06
Iter: 181 loss: 8.77864932e-06
Iter: 182 loss: 8.57112263e-06
Iter: 183 loss: 8.53350866e-06
Iter: 184 loss: 9.00764371e-06
Iter: 185 loss: 8.53322126e-06
Iter: 186 loss: 8.50078504e-06
Iter: 187 loss: 8.47009051e-06
Iter: 188 loss: 8.46266448e-06
Iter: 189 loss: 8.41991823e-06
Iter: 190 loss: 8.58930616e-06
Iter: 191 loss: 8.41001565e-06
Iter: 192 loss: 8.3621735e-06
Iter: 193 loss: 8.6881264e-06
Iter: 194 loss: 8.35756146e-06
Iter: 195 loss: 8.33153899e-06
Iter: 196 loss: 8.30726458e-06
Iter: 197 loss: 8.30123281e-06
Iter: 198 loss: 8.25830648e-06
Iter: 199 loss: 8.35751416e-06
Iter: 200 loss: 8.24259405e-06
Iter: 201 loss: 8.20328933e-06
Iter: 202 loss: 8.42013833e-06
Iter: 203 loss: 8.19776687e-06
Iter: 204 loss: 8.16173451e-06
Iter: 205 loss: 8.54999871e-06
Iter: 206 loss: 8.16069132e-06
Iter: 207 loss: 8.14007853e-06
Iter: 208 loss: 8.10422625e-06
Iter: 209 loss: 8.10428173e-06
Iter: 210 loss: 8.06961725e-06
Iter: 211 loss: 8.24527706e-06
Iter: 212 loss: 8.06391199e-06
Iter: 213 loss: 8.02616341e-06
Iter: 214 loss: 8.27193071e-06
Iter: 215 loss: 8.02221803e-06
Iter: 216 loss: 7.99959707e-06
Iter: 217 loss: 7.96262157e-06
Iter: 218 loss: 7.96235508e-06
Iter: 219 loss: 7.92320952e-06
Iter: 220 loss: 8.21293906e-06
Iter: 221 loss: 7.91997718e-06
Iter: 222 loss: 7.88390571e-06
Iter: 223 loss: 8.11333121e-06
Iter: 224 loss: 7.87995486e-06
Iter: 225 loss: 7.8578214e-06
Iter: 226 loss: 7.81561903e-06
Iter: 227 loss: 8.73090357e-06
Iter: 228 loss: 7.81561539e-06
Iter: 229 loss: 7.79267248e-06
Iter: 230 loss: 7.78805224e-06
Iter: 231 loss: 7.76382058e-06
Iter: 232 loss: 7.76693e-06
Iter: 233 loss: 7.74525051e-06
Iter: 234 loss: 7.71537725e-06
Iter: 235 loss: 7.69773487e-06
Iter: 236 loss: 7.68533937e-06
Iter: 237 loss: 7.6541246e-06
Iter: 238 loss: 7.93688105e-06
Iter: 239 loss: 7.65270852e-06
Iter: 240 loss: 7.6277056e-06
Iter: 241 loss: 7.87733916e-06
Iter: 242 loss: 7.62677246e-06
Iter: 243 loss: 7.60899502e-06
Iter: 244 loss: 7.61607816e-06
Iter: 245 loss: 7.59666227e-06
Iter: 246 loss: 7.56850068e-06
Iter: 247 loss: 7.64327342e-06
Iter: 248 loss: 7.55920883e-06
Iter: 249 loss: 7.53540826e-06
Iter: 250 loss: 7.52166352e-06
Iter: 251 loss: 7.51155812e-06
Iter: 252 loss: 7.47816102e-06
Iter: 253 loss: 7.60778585e-06
Iter: 254 loss: 7.47036847e-06
Iter: 255 loss: 7.44620229e-06
Iter: 256 loss: 7.82390816e-06
Iter: 257 loss: 7.44617319e-06
Iter: 258 loss: 7.42568591e-06
Iter: 259 loss: 7.3934948e-06
Iter: 260 loss: 7.39322468e-06
Iter: 261 loss: 7.36254697e-06
Iter: 262 loss: 7.41989743e-06
Iter: 263 loss: 7.34955393e-06
Iter: 264 loss: 7.32467561e-06
Iter: 265 loss: 7.32470789e-06
Iter: 266 loss: 7.30334432e-06
Iter: 267 loss: 7.33098068e-06
Iter: 268 loss: 7.29249814e-06
Iter: 269 loss: 7.27321276e-06
Iter: 270 loss: 7.26033977e-06
Iter: 271 loss: 7.25305154e-06
Iter: 272 loss: 7.23022276e-06
Iter: 273 loss: 7.22927098e-06
Iter: 274 loss: 7.21630295e-06
Iter: 275 loss: 7.18902766e-06
Iter: 276 loss: 7.64809738e-06
Iter: 277 loss: 7.1883278e-06
Iter: 278 loss: 7.16067234e-06
Iter: 279 loss: 7.27338e-06
Iter: 280 loss: 7.15448732e-06
Iter: 281 loss: 7.13545296e-06
Iter: 282 loss: 7.13500276e-06
Iter: 283 loss: 7.11907342e-06
Iter: 284 loss: 7.10193217e-06
Iter: 285 loss: 7.09924643e-06
Iter: 286 loss: 7.07981053e-06
Iter: 287 loss: 7.33137495e-06
Iter: 288 loss: 7.07959498e-06
Iter: 289 loss: 7.06500714e-06
Iter: 290 loss: 7.0365e-06
Iter: 291 loss: 7.62573836e-06
Iter: 292 loss: 7.03633077e-06
Iter: 293 loss: 7.00861892e-06
Iter: 294 loss: 7.16054728e-06
Iter: 295 loss: 7.00459304e-06
Iter: 296 loss: 6.98567055e-06
Iter: 297 loss: 6.98564645e-06
Iter: 298 loss: 6.97239602e-06
Iter: 299 loss: 6.94781647e-06
Iter: 300 loss: 7.50766048e-06
Iter: 301 loss: 6.94776418e-06
Iter: 302 loss: 6.92248841e-06
Iter: 303 loss: 6.99612474e-06
Iter: 304 loss: 6.91468222e-06
Iter: 305 loss: 6.89646959e-06
Iter: 306 loss: 7.15605e-06
Iter: 307 loss: 6.89628905e-06
Iter: 308 loss: 6.87800639e-06
Iter: 309 loss: 6.87358e-06
Iter: 310 loss: 6.86207841e-06
Iter: 311 loss: 6.83975395e-06
Iter: 312 loss: 6.89548415e-06
Iter: 313 loss: 6.83195e-06
Iter: 314 loss: 6.81189977e-06
Iter: 315 loss: 7.0406104e-06
Iter: 316 loss: 6.81146412e-06
Iter: 317 loss: 6.79569439e-06
Iter: 318 loss: 6.78284869e-06
Iter: 319 loss: 6.77807338e-06
Iter: 320 loss: 6.75443789e-06
Iter: 321 loss: 6.78661399e-06
Iter: 322 loss: 6.74249259e-06
Iter: 323 loss: 6.73126169e-06
Iter: 324 loss: 6.72880333e-06
Iter: 325 loss: 6.71585804e-06
Iter: 326 loss: 6.68943585e-06
Iter: 327 loss: 7.16927752e-06
Iter: 328 loss: 6.6890143e-06
Iter: 329 loss: 6.66973847e-06
Iter: 330 loss: 6.91040395e-06
Iter: 331 loss: 6.66952747e-06
Iter: 332 loss: 6.6505545e-06
Iter: 333 loss: 6.64900244e-06
Iter: 334 loss: 6.63496576e-06
Iter: 335 loss: 6.61459035e-06
Iter: 336 loss: 6.6288062e-06
Iter: 337 loss: 6.60192154e-06
Iter: 338 loss: 6.58622503e-06
Iter: 339 loss: 6.58494719e-06
Iter: 340 loss: 6.57383953e-06
Iter: 341 loss: 6.55731128e-06
Iter: 342 loss: 6.55687518e-06
Iter: 343 loss: 6.53607867e-06
Iter: 344 loss: 6.53900361e-06
Iter: 345 loss: 6.52028757e-06
Iter: 346 loss: 6.50271886e-06
Iter: 347 loss: 6.50232778e-06
Iter: 348 loss: 6.48560672e-06
Iter: 349 loss: 6.50026914e-06
Iter: 350 loss: 6.47598245e-06
Iter: 351 loss: 6.45973796e-06
Iter: 352 loss: 6.4714277e-06
Iter: 353 loss: 6.44987813e-06
Iter: 354 loss: 6.4297692e-06
Iter: 355 loss: 6.65615153e-06
Iter: 356 loss: 6.4295341e-06
Iter: 357 loss: 6.41767656e-06
Iter: 358 loss: 6.39600876e-06
Iter: 359 loss: 6.90062097e-06
Iter: 360 loss: 6.39595146e-06
Iter: 361 loss: 6.37679477e-06
Iter: 362 loss: 6.37676749e-06
Iter: 363 loss: 6.35745118e-06
Iter: 364 loss: 6.3921234e-06
Iter: 365 loss: 6.34908247e-06
Iter: 366 loss: 6.33615309e-06
Iter: 367 loss: 6.32405499e-06
Iter: 368 loss: 6.32103365e-06
Iter: 369 loss: 6.30390286e-06
Iter: 370 loss: 6.54418682e-06
Iter: 371 loss: 6.30393333e-06
Iter: 372 loss: 6.2894419e-06
Iter: 373 loss: 6.29116448e-06
Iter: 374 loss: 6.27830877e-06
Iter: 375 loss: 6.26145e-06
Iter: 376 loss: 6.31454486e-06
Iter: 377 loss: 6.25651683e-06
Iter: 378 loss: 6.24065888e-06
Iter: 379 loss: 6.37312269e-06
Iter: 380 loss: 6.23983487e-06
Iter: 381 loss: 6.22589505e-06
Iter: 382 loss: 6.20681931e-06
Iter: 383 loss: 6.20593437e-06
Iter: 384 loss: 6.18581453e-06
Iter: 385 loss: 6.21513755e-06
Iter: 386 loss: 6.176233e-06
Iter: 387 loss: 6.15908266e-06
Iter: 388 loss: 6.42198393e-06
Iter: 389 loss: 6.1591054e-06
Iter: 390 loss: 6.14305554e-06
Iter: 391 loss: 6.1662613e-06
Iter: 392 loss: 6.13515203e-06
Iter: 393 loss: 6.12067e-06
Iter: 394 loss: 6.12738449e-06
Iter: 395 loss: 6.1107512e-06
Iter: 396 loss: 6.09132621e-06
Iter: 397 loss: 6.2924646e-06
Iter: 398 loss: 6.09072595e-06
Iter: 399 loss: 6.0802472e-06
Iter: 400 loss: 6.06254753e-06
Iter: 401 loss: 6.0625789e-06
Iter: 402 loss: 6.05513878e-06
Iter: 403 loss: 6.0511984e-06
Iter: 404 loss: 6.04259958e-06
Iter: 405 loss: 6.02433965e-06
Iter: 406 loss: 6.33148193e-06
Iter: 407 loss: 6.02393311e-06
Iter: 408 loss: 6.00559861e-06
Iter: 409 loss: 6.03984154e-06
Iter: 410 loss: 5.99792111e-06
Iter: 411 loss: 5.98275346e-06
Iter: 412 loss: 5.9827089e-06
Iter: 413 loss: 5.96991458e-06
Iter: 414 loss: 5.95777783e-06
Iter: 415 loss: 5.95490837e-06
Iter: 416 loss: 5.93880031e-06
Iter: 417 loss: 6.05756395e-06
Iter: 418 loss: 5.93740378e-06
Iter: 419 loss: 5.92203105e-06
Iter: 420 loss: 5.99421946e-06
Iter: 421 loss: 5.91914886e-06
Iter: 422 loss: 5.90859645e-06
Iter: 423 loss: 5.88671674e-06
Iter: 424 loss: 6.27846339e-06
Iter: 425 loss: 5.88637158e-06
Iter: 426 loss: 5.8653e-06
Iter: 427 loss: 5.98499e-06
Iter: 428 loss: 5.86256e-06
Iter: 429 loss: 5.84293048e-06
Iter: 430 loss: 5.92764673e-06
Iter: 431 loss: 5.8390051e-06
Iter: 432 loss: 5.8255473e-06
Iter: 433 loss: 6.01144848e-06
Iter: 434 loss: 5.82550047e-06
Iter: 435 loss: 5.81379481e-06
Iter: 436 loss: 5.80378401e-06
Iter: 437 loss: 5.80054939e-06
Iter: 438 loss: 5.78781055e-06
Iter: 439 loss: 5.9744707e-06
Iter: 440 loss: 5.78786739e-06
Iter: 441 loss: 5.77662104e-06
Iter: 442 loss: 5.77973196e-06
Iter: 443 loss: 5.768552e-06
Iter: 444 loss: 5.75691956e-06
Iter: 445 loss: 5.8267251e-06
Iter: 446 loss: 5.75547892e-06
Iter: 447 loss: 5.74288151e-06
Iter: 448 loss: 5.73646503e-06
Iter: 449 loss: 5.73055422e-06
Iter: 450 loss: 5.71575674e-06
Iter: 451 loss: 5.70733118e-06
Iter: 452 loss: 5.70096927e-06
Iter: 453 loss: 5.68096084e-06
Iter: 454 loss: 5.81550285e-06
Iter: 455 loss: 5.67900679e-06
Iter: 456 loss: 5.66485187e-06
Iter: 457 loss: 5.87786553e-06
Iter: 458 loss: 5.66481231e-06
Iter: 459 loss: 5.65596656e-06
Iter: 460 loss: 5.64264701e-06
Iter: 461 loss: 5.64238644e-06
Iter: 462 loss: 5.6304907e-06
Iter: 463 loss: 5.63026788e-06
Iter: 464 loss: 5.62037803e-06
Iter: 465 loss: 5.60572471e-06
Iter: 466 loss: 5.60533408e-06
Iter: 467 loss: 5.58911506e-06
Iter: 468 loss: 5.61198749e-06
Iter: 469 loss: 5.58130387e-06
Iter: 470 loss: 5.56367331e-06
Iter: 471 loss: 5.59228783e-06
Iter: 472 loss: 5.55561746e-06
Iter: 473 loss: 5.5401224e-06
Iter: 474 loss: 5.71468399e-06
Iter: 475 loss: 5.53988866e-06
Iter: 476 loss: 5.52373467e-06
Iter: 477 loss: 5.58659758e-06
Iter: 478 loss: 5.52001438e-06
Iter: 479 loss: 5.50953609e-06
Iter: 480 loss: 5.54532e-06
Iter: 481 loss: 5.50665936e-06
Iter: 482 loss: 5.49452216e-06
Iter: 483 loss: 5.49955257e-06
Iter: 484 loss: 5.48615208e-06
Iter: 485 loss: 5.47329182e-06
Iter: 486 loss: 5.46797583e-06
Iter: 487 loss: 5.46121191e-06
Iter: 488 loss: 5.45086414e-06
Iter: 489 loss: 5.45053172e-06
Iter: 490 loss: 5.4398306e-06
Iter: 491 loss: 5.43612487e-06
Iter: 492 loss: 5.43017541e-06
Iter: 493 loss: 5.41723239e-06
Iter: 494 loss: 5.43247916e-06
Iter: 495 loss: 5.41044028e-06
Iter: 496 loss: 5.39702705e-06
Iter: 497 loss: 5.47689342e-06
Iter: 498 loss: 5.39535631e-06
Iter: 499 loss: 5.38221e-06
Iter: 500 loss: 5.42696898e-06
Iter: 501 loss: 5.37862434e-06
Iter: 502 loss: 5.36752259e-06
Iter: 503 loss: 5.36830976e-06
Iter: 504 loss: 5.35870777e-06
Iter: 505 loss: 5.3485137e-06
Iter: 506 loss: 5.50669e-06
Iter: 507 loss: 5.34845367e-06
Iter: 508 loss: 5.3381109e-06
Iter: 509 loss: 5.32712238e-06
Iter: 510 loss: 5.32524609e-06
Iter: 511 loss: 5.31083742e-06
Iter: 512 loss: 5.31352e-06
Iter: 513 loss: 5.30017951e-06
Iter: 514 loss: 5.28342025e-06
Iter: 515 loss: 5.36169864e-06
Iter: 516 loss: 5.28024975e-06
Iter: 517 loss: 5.26945587e-06
Iter: 518 loss: 5.26938311e-06
Iter: 519 loss: 5.26012673e-06
Iter: 520 loss: 5.25455744e-06
Iter: 521 loss: 5.25063115e-06
Iter: 522 loss: 5.23907784e-06
Iter: 523 loss: 5.28378951e-06
Iter: 524 loss: 5.23636299e-06
Iter: 525 loss: 5.22417304e-06
Iter: 526 loss: 5.33127923e-06
Iter: 527 loss: 5.22366599e-06
Iter: 528 loss: 5.21731499e-06
Iter: 529 loss: 5.20761296e-06
Iter: 530 loss: 5.20737285e-06
Iter: 531 loss: 5.19555442e-06
Iter: 532 loss: 5.35292975e-06
Iter: 533 loss: 5.1954803e-06
Iter: 534 loss: 5.18765592e-06
Iter: 535 loss: 5.17555554e-06
Iter: 536 loss: 5.17537956e-06
Iter: 537 loss: 5.16105683e-06
Iter: 538 loss: 5.20073763e-06
Iter: 539 loss: 5.15632246e-06
Iter: 540 loss: 5.14037629e-06
Iter: 541 loss: 5.28013243e-06
Iter: 542 loss: 5.13953819e-06
Iter: 543 loss: 5.12989573e-06
Iter: 544 loss: 5.11580765e-06
Iter: 545 loss: 5.11536564e-06
Iter: 546 loss: 5.11092321e-06
Iter: 547 loss: 5.10709469e-06
Iter: 548 loss: 5.10069458e-06
Iter: 549 loss: 5.08567e-06
Iter: 550 loss: 5.28332839e-06
Iter: 551 loss: 5.08480207e-06
Iter: 552 loss: 5.06873539e-06
Iter: 553 loss: 5.1127563e-06
Iter: 554 loss: 5.0634344e-06
Iter: 555 loss: 5.04977197e-06
Iter: 556 loss: 5.10523114e-06
Iter: 557 loss: 5.04671971e-06
Iter: 558 loss: 5.03544652e-06
Iter: 559 loss: 5.12979113e-06
Iter: 560 loss: 5.03482079e-06
Iter: 561 loss: 5.02504327e-06
Iter: 562 loss: 5.08545963e-06
Iter: 563 loss: 5.0238823e-06
Iter: 564 loss: 5.01673367e-06
Iter: 565 loss: 5.02312469e-06
Iter: 566 loss: 5.01260547e-06
Iter: 567 loss: 5.00191254e-06
Iter: 568 loss: 5.02159082e-06
Iter: 569 loss: 4.9973878e-06
Iter: 570 loss: 4.98744294e-06
Iter: 571 loss: 4.98684267e-06
Iter: 572 loss: 4.97918e-06
Iter: 573 loss: 4.96886878e-06
Iter: 574 loss: 5.12762563e-06
Iter: 575 loss: 4.96885514e-06
Iter: 576 loss: 4.96024e-06
Iter: 577 loss: 4.95484437e-06
Iter: 578 loss: 4.95157747e-06
Iter: 579 loss: 4.94196502e-06
Iter: 580 loss: 4.93385778e-06
Iter: 581 loss: 4.93121524e-06
Iter: 582 loss: 4.92087565e-06
Iter: 583 loss: 4.92047457e-06
Iter: 584 loss: 4.91069613e-06
Iter: 585 loss: 4.93404423e-06
Iter: 586 loss: 4.90720595e-06
Iter: 587 loss: 4.89814283e-06
Iter: 588 loss: 4.88750129e-06
Iter: 589 loss: 4.8862812e-06
Iter: 590 loss: 4.87626949e-06
Iter: 591 loss: 4.87588e-06
Iter: 592 loss: 4.86848876e-06
Iter: 593 loss: 4.85598594e-06
Iter: 594 loss: 4.85593227e-06
Iter: 595 loss: 4.84119573e-06
Iter: 596 loss: 4.86230556e-06
Iter: 597 loss: 4.83400026e-06
Iter: 598 loss: 4.82734549e-06
Iter: 599 loss: 4.82605719e-06
Iter: 600 loss: 4.81740699e-06
Iter: 601 loss: 4.81775214e-06
Iter: 602 loss: 4.81058441e-06
Iter: 603 loss: 4.80215294e-06
Iter: 604 loss: 4.80024255e-06
Iter: 605 loss: 4.79476921e-06
Iter: 606 loss: 4.78266202e-06
Iter: 607 loss: 4.91560604e-06
Iter: 608 loss: 4.78232232e-06
Iter: 609 loss: 4.77447065e-06
Iter: 610 loss: 4.76992864e-06
Iter: 611 loss: 4.76654532e-06
Iter: 612 loss: 4.7563376e-06
Iter: 613 loss: 4.8281313e-06
Iter: 614 loss: 4.75539309e-06
Iter: 615 loss: 4.74502122e-06
Iter: 616 loss: 4.77133744e-06
Iter: 617 loss: 4.74122362e-06
Iter: 618 loss: 4.73293767e-06
Iter: 619 loss: 4.73136606e-06
Iter: 620 loss: 4.72562078e-06
Iter: 621 loss: 4.71370095e-06
Iter: 622 loss: 4.74024273e-06
Iter: 623 loss: 4.70917939e-06
Iter: 624 loss: 4.69991483e-06
Iter: 625 loss: 4.69977931e-06
Iter: 626 loss: 4.69459701e-06
Iter: 627 loss: 4.68338658e-06
Iter: 628 loss: 4.84692828e-06
Iter: 629 loss: 4.68287635e-06
Iter: 630 loss: 4.66949314e-06
Iter: 631 loss: 4.71213116e-06
Iter: 632 loss: 4.66572055e-06
Iter: 633 loss: 4.65646917e-06
Iter: 634 loss: 4.65637095e-06
Iter: 635 loss: 4.65004359e-06
Iter: 636 loss: 4.64209552e-06
Iter: 637 loss: 4.64151572e-06
Iter: 638 loss: 4.63385595e-06
Iter: 639 loss: 4.63378092e-06
Iter: 640 loss: 4.62593243e-06
Iter: 641 loss: 4.6255891e-06
Iter: 642 loss: 4.61955824e-06
Iter: 643 loss: 4.61178e-06
Iter: 644 loss: 4.6038449e-06
Iter: 645 loss: 4.6023124e-06
Iter: 646 loss: 4.59083276e-06
Iter: 647 loss: 4.66050096e-06
Iter: 648 loss: 4.58946033e-06
Iter: 649 loss: 4.57712076e-06
Iter: 650 loss: 4.64723598e-06
Iter: 651 loss: 4.57533906e-06
Iter: 652 loss: 4.56756106e-06
Iter: 653 loss: 4.5610077e-06
Iter: 654 loss: 4.55884174e-06
Iter: 655 loss: 4.54706242e-06
Iter: 656 loss: 4.60460433e-06
Iter: 657 loss: 4.54498604e-06
Iter: 658 loss: 4.53576513e-06
Iter: 659 loss: 4.67474774e-06
Iter: 660 loss: 4.53576604e-06
Iter: 661 loss: 4.53064422e-06
Iter: 662 loss: 4.51876758e-06
Iter: 663 loss: 4.66033543e-06
Iter: 664 loss: 4.51775395e-06
Iter: 665 loss: 4.50666e-06
Iter: 666 loss: 4.63062406e-06
Iter: 667 loss: 4.50654261e-06
Iter: 668 loss: 4.49824347e-06
Iter: 669 loss: 4.56883026e-06
Iter: 670 loss: 4.49774961e-06
Iter: 671 loss: 4.49125127e-06
Iter: 672 loss: 4.48847368e-06
Iter: 673 loss: 4.48522314e-06
Iter: 674 loss: 4.47662615e-06
Iter: 675 loss: 4.50651714e-06
Iter: 676 loss: 4.47447383e-06
Iter: 677 loss: 4.46513877e-06
Iter: 678 loss: 4.510905e-06
Iter: 679 loss: 4.46349713e-06
Iter: 680 loss: 4.45687192e-06
Iter: 681 loss: 4.4746007e-06
Iter: 682 loss: 4.45460773e-06
Iter: 683 loss: 4.44600209e-06
Iter: 684 loss: 4.46075728e-06
Iter: 685 loss: 4.44219313e-06
Iter: 686 loss: 4.43528188e-06
Iter: 687 loss: 4.42202281e-06
Iter: 688 loss: 4.69025099e-06
Iter: 689 loss: 4.42191504e-06
Iter: 690 loss: 4.4093249e-06
Iter: 691 loss: 4.50224434e-06
Iter: 692 loss: 4.40831218e-06
Iter: 693 loss: 4.39917039e-06
Iter: 694 loss: 4.51055712e-06
Iter: 695 loss: 4.39902806e-06
Iter: 696 loss: 4.39123869e-06
Iter: 697 loss: 4.39687847e-06
Iter: 698 loss: 4.38636152e-06
Iter: 699 loss: 4.3789928e-06
Iter: 700 loss: 4.37318158e-06
Iter: 701 loss: 4.37085828e-06
Iter: 702 loss: 4.36206074e-06
Iter: 703 loss: 4.49488e-06
Iter: 704 loss: 4.36205301e-06
Iter: 705 loss: 4.35418315e-06
Iter: 706 loss: 4.39394626e-06
Iter: 707 loss: 4.3528612e-06
Iter: 708 loss: 4.34747335e-06
Iter: 709 loss: 4.33641e-06
Iter: 710 loss: 4.54164319e-06
Iter: 711 loss: 4.33632704e-06
Iter: 712 loss: 4.32491879e-06
Iter: 713 loss: 4.42304645e-06
Iter: 714 loss: 4.32442084e-06
Iter: 715 loss: 4.3153691e-06
Iter: 716 loss: 4.41076463e-06
Iter: 717 loss: 4.31518629e-06
Iter: 718 loss: 4.31000353e-06
Iter: 719 loss: 4.30140653e-06
Iter: 720 loss: 4.30133196e-06
Iter: 721 loss: 4.29446482e-06
Iter: 722 loss: 4.29419e-06
Iter: 723 loss: 4.28795101e-06
Iter: 724 loss: 4.28395651e-06
Iter: 725 loss: 4.28153362e-06
Iter: 726 loss: 4.27433224e-06
Iter: 727 loss: 4.34714093e-06
Iter: 728 loss: 4.27424857e-06
Iter: 729 loss: 4.26882298e-06
Iter: 730 loss: 4.26274846e-06
Iter: 731 loss: 4.26193219e-06
Iter: 732 loss: 4.25219787e-06
Iter: 733 loss: 4.27024406e-06
Iter: 734 loss: 4.24810423e-06
Iter: 735 loss: 4.23907e-06
Iter: 736 loss: 4.24741256e-06
Iter: 737 loss: 4.2339525e-06
Iter: 738 loss: 4.22419453e-06
Iter: 739 loss: 4.2948177e-06
Iter: 740 loss: 4.22336097e-06
Iter: 741 loss: 4.21521418e-06
Iter: 742 loss: 4.28798376e-06
Iter: 743 loss: 4.21479035e-06
Iter: 744 loss: 4.20876404e-06
Iter: 745 loss: 4.20061178e-06
Iter: 746 loss: 4.2003212e-06
Iter: 747 loss: 4.19407479e-06
Iter: 748 loss: 4.19371281e-06
Iter: 749 loss: 4.18777154e-06
Iter: 750 loss: 4.18157651e-06
Iter: 751 loss: 4.18035324e-06
Iter: 752 loss: 4.17167303e-06
Iter: 753 loss: 4.17017509e-06
Iter: 754 loss: 4.16422063e-06
Iter: 755 loss: 4.16025705e-06
Iter: 756 loss: 4.15784234e-06
Iter: 757 loss: 4.15314753e-06
Iter: 758 loss: 4.14849046e-06
Iter: 759 loss: 4.14756687e-06
Iter: 760 loss: 4.14099713e-06
Iter: 761 loss: 4.19447133e-06
Iter: 762 loss: 4.14047736e-06
Iter: 763 loss: 4.13329963e-06
Iter: 764 loss: 4.12945064e-06
Iter: 765 loss: 4.12607415e-06
Iter: 766 loss: 4.11817928e-06
Iter: 767 loss: 4.11188739e-06
Iter: 768 loss: 4.10947769e-06
Iter: 769 loss: 4.10330358e-06
Iter: 770 loss: 4.10257508e-06
Iter: 771 loss: 4.09640688e-06
Iter: 772 loss: 4.09465702e-06
Iter: 773 loss: 4.09103e-06
Iter: 774 loss: 4.08233745e-06
Iter: 775 loss: 4.08316282e-06
Iter: 776 loss: 4.07564676e-06
Iter: 777 loss: 4.06636855e-06
Iter: 778 loss: 4.18330501e-06
Iter: 779 loss: 4.06634445e-06
Iter: 780 loss: 4.05919673e-06
Iter: 781 loss: 4.08839333e-06
Iter: 782 loss: 4.05769879e-06
Iter: 783 loss: 4.05145101e-06
Iter: 784 loss: 4.04925686e-06
Iter: 785 loss: 4.04564526e-06
Iter: 786 loss: 4.03766262e-06
Iter: 787 loss: 4.13259295e-06
Iter: 788 loss: 4.03763897e-06
Iter: 789 loss: 4.03131526e-06
Iter: 790 loss: 4.02408159e-06
Iter: 791 loss: 4.0231771e-06
Iter: 792 loss: 4.0148625e-06
Iter: 793 loss: 4.04864795e-06
Iter: 794 loss: 4.01314946e-06
Iter: 795 loss: 4.00531371e-06
Iter: 796 loss: 4.09820086e-06
Iter: 797 loss: 4.00520412e-06
Iter: 798 loss: 4.00033059e-06
Iter: 799 loss: 3.99508644e-06
Iter: 800 loss: 3.99413693e-06
Iter: 801 loss: 3.9884153e-06
Iter: 802 loss: 3.98832344e-06
Iter: 803 loss: 3.98449311e-06
Iter: 804 loss: 3.97530766e-06
Iter: 805 loss: 4.07684547e-06
Iter: 806 loss: 3.97444046e-06
Iter: 807 loss: 3.96429868e-06
Iter: 808 loss: 3.97877284e-06
Iter: 809 loss: 3.95924735e-06
Iter: 810 loss: 3.95021152e-06
Iter: 811 loss: 4.0743962e-06
Iter: 812 loss: 3.95020106e-06
Iter: 813 loss: 3.94237577e-06
Iter: 814 loss: 3.97633e-06
Iter: 815 loss: 3.94081235e-06
Iter: 816 loss: 3.93502751e-06
Iter: 817 loss: 3.93166056e-06
Iter: 818 loss: 3.92904167e-06
Iter: 819 loss: 3.9214865e-06
Iter: 820 loss: 3.96070936e-06
Iter: 821 loss: 3.92028232e-06
Iter: 822 loss: 3.9119559e-06
Iter: 823 loss: 3.95830375e-06
Iter: 824 loss: 3.91082722e-06
Iter: 825 loss: 3.90544392e-06
Iter: 826 loss: 3.90425157e-06
Iter: 827 loss: 3.90087462e-06
Iter: 828 loss: 3.89423349e-06
Iter: 829 loss: 3.97254553e-06
Iter: 830 loss: 3.89413253e-06
Iter: 831 loss: 3.88925673e-06
Iter: 832 loss: 3.8814328e-06
Iter: 833 loss: 3.88129683e-06
Iter: 834 loss: 3.87550881e-06
Iter: 835 loss: 3.87539694e-06
Iter: 836 loss: 3.87003456e-06
Iter: 837 loss: 3.87650925e-06
Iter: 838 loss: 3.86730335e-06
Iter: 839 loss: 3.86145166e-06
Iter: 840 loss: 3.85930571e-06
Iter: 841 loss: 3.85612748e-06
Iter: 842 loss: 3.84907617e-06
Iter: 843 loss: 3.95798361e-06
Iter: 844 loss: 3.84910345e-06
Iter: 845 loss: 3.84473833e-06
Iter: 846 loss: 3.83765973e-06
Iter: 847 loss: 3.83765291e-06
Iter: 848 loss: 3.82892813e-06
Iter: 849 loss: 3.83969791e-06
Iter: 850 loss: 3.82443159e-06
Iter: 851 loss: 3.81663949e-06
Iter: 852 loss: 3.90207515e-06
Iter: 853 loss: 3.81645077e-06
Iter: 854 loss: 3.80908205e-06
Iter: 855 loss: 3.83490215e-06
Iter: 856 loss: 3.80719916e-06
Iter: 857 loss: 3.80210577e-06
Iter: 858 loss: 3.7942857e-06
Iter: 859 loss: 3.79410426e-06
Iter: 860 loss: 3.78837422e-06
Iter: 861 loss: 3.78754885e-06
Iter: 862 loss: 3.78253753e-06
Iter: 863 loss: 3.7800819e-06
Iter: 864 loss: 3.77759261e-06
Iter: 865 loss: 3.77133665e-06
Iter: 866 loss: 3.78406139e-06
Iter: 867 loss: 3.76874777e-06
Iter: 868 loss: 3.76166304e-06
Iter: 869 loss: 3.82458074e-06
Iter: 870 loss: 3.76133312e-06
Iter: 871 loss: 3.75704349e-06
Iter: 872 loss: 3.76009461e-06
Iter: 873 loss: 3.75444165e-06
Iter: 874 loss: 3.74767319e-06
Iter: 875 loss: 3.7705986e-06
Iter: 876 loss: 3.74579531e-06
Iter: 877 loss: 3.74124e-06
Iter: 878 loss: 3.73782495e-06
Iter: 879 loss: 3.73640819e-06
Iter: 880 loss: 3.73030684e-06
Iter: 881 loss: 3.81331756e-06
Iter: 882 loss: 3.73032026e-06
Iter: 883 loss: 3.72507725e-06
Iter: 884 loss: 3.71911369e-06
Iter: 885 loss: 3.71836131e-06
Iter: 886 loss: 3.71078772e-06
Iter: 887 loss: 3.72662453e-06
Iter: 888 loss: 3.70776615e-06
Iter: 889 loss: 3.69981035e-06
Iter: 890 loss: 3.71380474e-06
Iter: 891 loss: 3.69627469e-06
Iter: 892 loss: 3.68900646e-06
Iter: 893 loss: 3.78170603e-06
Iter: 894 loss: 3.68902352e-06
Iter: 895 loss: 3.68229917e-06
Iter: 896 loss: 3.68553106e-06
Iter: 897 loss: 3.67795451e-06
Iter: 898 loss: 3.671488e-06
Iter: 899 loss: 3.68499218e-06
Iter: 900 loss: 3.66896052e-06
Iter: 901 loss: 3.66263703e-06
Iter: 902 loss: 3.74227648e-06
Iter: 903 loss: 3.6625986e-06
Iter: 904 loss: 3.65843039e-06
Iter: 905 loss: 3.65117535e-06
Iter: 906 loss: 3.65121e-06
Iter: 907 loss: 3.6463839e-06
Iter: 908 loss: 3.64612538e-06
Iter: 909 loss: 3.64105153e-06
Iter: 910 loss: 3.63724666e-06
Iter: 911 loss: 3.63554e-06
Iter: 912 loss: 3.63020263e-06
Iter: 913 loss: 3.63018307e-06
Iter: 914 loss: 3.6260717e-06
Iter: 915 loss: 3.61880734e-06
Iter: 916 loss: 3.61883531e-06
Iter: 917 loss: 3.61332195e-06
Iter: 918 loss: 3.69613508e-06
Iter: 919 loss: 3.61330876e-06
Iter: 920 loss: 3.60789704e-06
Iter: 921 loss: 3.60928379e-06
Iter: 922 loss: 3.60397303e-06
Iter: 923 loss: 3.59841169e-06
Iter: 924 loss: 3.60057402e-06
Iter: 925 loss: 3.59447358e-06
Iter: 926 loss: 3.58735565e-06
Iter: 927 loss: 3.59376145e-06
Iter: 928 loss: 3.58313059e-06
Iter: 929 loss: 3.57557155e-06
Iter: 930 loss: 3.66624295e-06
Iter: 931 loss: 3.57540148e-06
Iter: 932 loss: 3.56862301e-06
Iter: 933 loss: 3.58618695e-06
Iter: 934 loss: 3.56635769e-06
Iter: 935 loss: 3.56065493e-06
Iter: 936 loss: 3.56755459e-06
Iter: 937 loss: 3.55771294e-06
Iter: 938 loss: 3.55264729e-06
Iter: 939 loss: 3.6178667e-06
Iter: 940 loss: 3.55255816e-06
Iter: 941 loss: 3.54834197e-06
Iter: 942 loss: 3.54408917e-06
Iter: 943 loss: 3.54319195e-06
Iter: 944 loss: 3.53764085e-06
Iter: 945 loss: 3.56466285e-06
Iter: 946 loss: 3.53665246e-06
Iter: 947 loss: 3.53077257e-06
Iter: 948 loss: 3.56690202e-06
Iter: 949 loss: 3.53003179e-06
Iter: 950 loss: 3.52629195e-06
Iter: 951 loss: 3.52660436e-06
Iter: 952 loss: 3.52332722e-06
Iter: 953 loss: 3.5171397e-06
Iter: 954 loss: 3.53850828e-06
Iter: 955 loss: 3.51548488e-06
Iter: 956 loss: 3.51065341e-06
Iter: 957 loss: 3.5074961e-06
Iter: 958 loss: 3.50570122e-06
Iter: 959 loss: 3.50019377e-06
Iter: 960 loss: 3.50012488e-06
Iter: 961 loss: 3.49622655e-06
Iter: 962 loss: 3.49106313e-06
Iter: 963 loss: 3.49078846e-06
Iter: 964 loss: 3.48462618e-06
Iter: 965 loss: 3.48346339e-06
Iter: 966 loss: 3.47938703e-06
Iter: 967 loss: 3.47302262e-06
Iter: 968 loss: 3.47305831e-06
Iter: 969 loss: 3.46740535e-06
Iter: 970 loss: 3.48526623e-06
Iter: 971 loss: 3.46579964e-06
Iter: 972 loss: 3.46077309e-06
Iter: 973 loss: 3.4603197e-06
Iter: 974 loss: 3.45664216e-06
Iter: 975 loss: 3.45129683e-06
Iter: 976 loss: 3.53467067e-06
Iter: 977 loss: 3.45128046e-06
Iter: 978 loss: 3.4470645e-06
Iter: 979 loss: 3.44482578e-06
Iter: 980 loss: 3.44295154e-06
Iter: 981 loss: 3.43752936e-06
Iter: 982 loss: 3.45577519e-06
Iter: 983 loss: 3.4361492e-06
Iter: 984 loss: 3.4302268e-06
Iter: 985 loss: 3.47355558e-06
Iter: 986 loss: 3.42968497e-06
Iter: 987 loss: 3.42608928e-06
Iter: 988 loss: 3.42454814e-06
Iter: 989 loss: 3.42261387e-06
Iter: 990 loss: 3.41758323e-06
Iter: 991 loss: 3.45643093e-06
Iter: 992 loss: 3.41713303e-06
Iter: 993 loss: 3.41300301e-06
Iter: 994 loss: 3.40750171e-06
Iter: 995 loss: 3.40716e-06
Iter: 996 loss: 3.40177166e-06
Iter: 997 loss: 3.40183e-06
Iter: 998 loss: 3.39727808e-06
Iter: 999 loss: 3.3973588e-06
Iter: 1000 loss: 3.39368125e-06
Iter: 1001 loss: 3.38828136e-06
Iter: 1002 loss: 3.38254495e-06
Iter: 1003 loss: 3.38161408e-06
Iter: 1004 loss: 3.37348274e-06
Iter: 1005 loss: 3.41788018e-06
Iter: 1006 loss: 3.37230949e-06
Iter: 1007 loss: 3.36644e-06
Iter: 1008 loss: 3.41801615e-06
Iter: 1009 loss: 3.3661604e-06
Iter: 1010 loss: 3.36013568e-06
Iter: 1011 loss: 3.38180735e-06
Iter: 1012 loss: 3.35866298e-06
Iter: 1013 loss: 3.3550225e-06
Iter: 1014 loss: 3.35830759e-06
Iter: 1015 loss: 3.35279424e-06
Iter: 1016 loss: 3.34698916e-06
Iter: 1017 loss: 3.367408e-06
Iter: 1018 loss: 3.34551714e-06
Iter: 1019 loss: 3.34136644e-06
Iter: 1020 loss: 3.34248807e-06
Iter: 1021 loss: 3.33836306e-06
Iter: 1022 loss: 3.33242428e-06
Iter: 1023 loss: 3.3882759e-06
Iter: 1024 loss: 3.33219486e-06
Iter: 1025 loss: 3.32861919e-06
Iter: 1026 loss: 3.32463742e-06
Iter: 1027 loss: 3.32415448e-06
Iter: 1028 loss: 3.31896263e-06
Iter: 1029 loss: 3.36530161e-06
Iter: 1030 loss: 3.31872025e-06
Iter: 1031 loss: 3.31369733e-06
Iter: 1032 loss: 3.31378874e-06
Iter: 1033 loss: 3.3096444e-06
Iter: 1034 loss: 3.3046565e-06
Iter: 1035 loss: 3.33589378e-06
Iter: 1036 loss: 3.30414059e-06
Iter: 1037 loss: 3.29911791e-06
Iter: 1038 loss: 3.30900411e-06
Iter: 1039 loss: 3.29710247e-06
Iter: 1040 loss: 3.29220825e-06
Iter: 1041 loss: 3.29000727e-06
Iter: 1042 loss: 3.28759597e-06
Iter: 1043 loss: 3.28144961e-06
Iter: 1044 loss: 3.30409398e-06
Iter: 1045 loss: 3.2800383e-06
Iter: 1046 loss: 3.27496809e-06
Iter: 1047 loss: 3.3009087e-06
Iter: 1048 loss: 3.27422663e-06
Iter: 1049 loss: 3.26849022e-06
Iter: 1050 loss: 3.29260797e-06
Iter: 1051 loss: 3.2672865e-06
Iter: 1052 loss: 3.2633086e-06
Iter: 1053 loss: 3.26539725e-06
Iter: 1054 loss: 3.26062445e-06
Iter: 1055 loss: 3.25503584e-06
Iter: 1056 loss: 3.29221302e-06
Iter: 1057 loss: 3.25453084e-06
Iter: 1058 loss: 3.25078372e-06
Iter: 1059 loss: 3.25404017e-06
Iter: 1060 loss: 3.24848543e-06
Iter: 1061 loss: 3.24348321e-06
Iter: 1062 loss: 3.27176303e-06
Iter: 1063 loss: 3.24279904e-06
Iter: 1064 loss: 3.23925155e-06
Iter: 1065 loss: 3.23352333e-06
Iter: 1066 loss: 3.2335156e-06
Iter: 1067 loss: 3.22954e-06
Iter: 1068 loss: 3.22945516e-06
Iter: 1069 loss: 3.22529218e-06
Iter: 1070 loss: 3.22167671e-06
Iter: 1071 loss: 3.22054211e-06
Iter: 1072 loss: 3.21568632e-06
Iter: 1073 loss: 3.24228631e-06
Iter: 1074 loss: 3.21491143e-06
Iter: 1075 loss: 3.21015614e-06
Iter: 1076 loss: 3.23020186e-06
Iter: 1077 loss: 3.20908748e-06
Iter: 1078 loss: 3.20495474e-06
Iter: 1079 loss: 3.19843684e-06
Iter: 1080 loss: 3.19832679e-06
Iter: 1081 loss: 3.19115725e-06
Iter: 1082 loss: 3.23913582e-06
Iter: 1083 loss: 3.19046285e-06
Iter: 1084 loss: 3.1853474e-06
Iter: 1085 loss: 3.21355378e-06
Iter: 1086 loss: 3.18459843e-06
Iter: 1087 loss: 3.17887066e-06
Iter: 1088 loss: 3.19770834e-06
Iter: 1089 loss: 3.17721288e-06
Iter: 1090 loss: 3.17322429e-06
Iter: 1091 loss: 3.17907143e-06
Iter: 1092 loss: 3.17128092e-06
Iter: 1093 loss: 3.16717433e-06
Iter: 1094 loss: 3.20990694e-06
Iter: 1095 loss: 3.16703972e-06
Iter: 1096 loss: 3.16449859e-06
Iter: 1097 loss: 3.16350406e-06
Iter: 1098 loss: 3.1620375e-06
Iter: 1099 loss: 3.15769353e-06
Iter: 1100 loss: 3.17736976e-06
Iter: 1101 loss: 3.15683883e-06
Iter: 1102 loss: 3.15325474e-06
Iter: 1103 loss: 3.14965018e-06
Iter: 1104 loss: 3.14896806e-06
Iter: 1105 loss: 3.14420822e-06
Iter: 1106 loss: 3.18303319e-06
Iter: 1107 loss: 3.14387489e-06
Iter: 1108 loss: 3.13853616e-06
Iter: 1109 loss: 3.14790714e-06
Iter: 1110 loss: 3.13616465e-06
Iter: 1111 loss: 3.13181545e-06
Iter: 1112 loss: 3.13307351e-06
Iter: 1113 loss: 3.12862767e-06
Iter: 1114 loss: 3.12395787e-06
Iter: 1115 loss: 3.18876982e-06
Iter: 1116 loss: 3.12392922e-06
Iter: 1117 loss: 3.12027532e-06
Iter: 1118 loss: 3.11620852e-06
Iter: 1119 loss: 3.11566919e-06
Iter: 1120 loss: 3.11018584e-06
Iter: 1121 loss: 3.11659142e-06
Iter: 1122 loss: 3.10736505e-06
Iter: 1123 loss: 3.10398264e-06
Iter: 1124 loss: 3.10347809e-06
Iter: 1125 loss: 3.10017799e-06
Iter: 1126 loss: 3.09705274e-06
Iter: 1127 loss: 3.0962949e-06
Iter: 1128 loss: 3.0923793e-06
Iter: 1129 loss: 3.13070723e-06
Iter: 1130 loss: 3.09236384e-06
Iter: 1131 loss: 3.0886722e-06
Iter: 1132 loss: 3.09433653e-06
Iter: 1133 loss: 3.08692893e-06
Iter: 1134 loss: 3.0833171e-06
Iter: 1135 loss: 3.08365497e-06
Iter: 1136 loss: 3.08056292e-06
Iter: 1137 loss: 3.07567825e-06
Iter: 1138 loss: 3.11342637e-06
Iter: 1139 loss: 3.07540404e-06
Iter: 1140 loss: 3.07210121e-06
Iter: 1141 loss: 3.06912352e-06
Iter: 1142 loss: 3.06835841e-06
Iter: 1143 loss: 3.06317907e-06
Iter: 1144 loss: 3.09516713e-06
Iter: 1145 loss: 3.06264178e-06
Iter: 1146 loss: 3.05740696e-06
Iter: 1147 loss: 3.07440905e-06
Iter: 1148 loss: 3.05584376e-06
Iter: 1149 loss: 3.05191361e-06
Iter: 1150 loss: 3.04989362e-06
Iter: 1151 loss: 3.04805621e-06
Iter: 1152 loss: 3.04374453e-06
Iter: 1153 loss: 3.10740529e-06
Iter: 1154 loss: 3.04377727e-06
Iter: 1155 loss: 3.03991874e-06
Iter: 1156 loss: 3.03913703e-06
Iter: 1157 loss: 3.03663865e-06
Iter: 1158 loss: 3.03207685e-06
Iter: 1159 loss: 3.03429e-06
Iter: 1160 loss: 3.02901071e-06
Iter: 1161 loss: 3.02583794e-06
Iter: 1162 loss: 3.02544117e-06
Iter: 1163 loss: 3.02241483e-06
Iter: 1164 loss: 3.01955788e-06
Iter: 1165 loss: 3.01892987e-06
Iter: 1166 loss: 3.01557202e-06
Iter: 1167 loss: 3.06207266e-06
Iter: 1168 loss: 3.01556702e-06
Iter: 1169 loss: 3.01238515e-06
Iter: 1170 loss: 3.0087117e-06
Iter: 1171 loss: 3.00833858e-06
Iter: 1172 loss: 3.00428201e-06
Iter: 1173 loss: 3.01884029e-06
Iter: 1174 loss: 3.0033093e-06
Iter: 1175 loss: 2.99887233e-06
Iter: 1176 loss: 3.02023113e-06
Iter: 1177 loss: 2.99810017e-06
Iter: 1178 loss: 2.99482963e-06
Iter: 1179 loss: 2.99058547e-06
Iter: 1180 loss: 2.99035082e-06
Iter: 1181 loss: 2.98502778e-06
Iter: 1182 loss: 3.05844787e-06
Iter: 1183 loss: 2.98500572e-06
Iter: 1184 loss: 2.98079385e-06
Iter: 1185 loss: 2.987676e-06
Iter: 1186 loss: 2.97892598e-06
Iter: 1187 loss: 2.97515589e-06
Iter: 1188 loss: 2.97302245e-06
Iter: 1189 loss: 2.97134375e-06
Iter: 1190 loss: 2.96634289e-06
Iter: 1191 loss: 2.99168209e-06
Iter: 1192 loss: 2.96551957e-06
Iter: 1193 loss: 2.96120288e-06
Iter: 1194 loss: 3.00268266e-06
Iter: 1195 loss: 2.96103713e-06
Iter: 1196 loss: 2.95768132e-06
Iter: 1197 loss: 2.95537438e-06
Iter: 1198 loss: 2.95422205e-06
Iter: 1199 loss: 2.95113114e-06
Iter: 1200 loss: 2.95106565e-06
Iter: 1201 loss: 2.94797042e-06
Iter: 1202 loss: 2.94582242e-06
Iter: 1203 loss: 2.94477718e-06
Iter: 1204 loss: 2.94092706e-06
Iter: 1205 loss: 2.96320195e-06
Iter: 1206 loss: 2.94039091e-06
Iter: 1207 loss: 2.93622156e-06
Iter: 1208 loss: 2.94226811e-06
Iter: 1209 loss: 2.93429775e-06
Iter: 1210 loss: 2.9309042e-06
Iter: 1211 loss: 2.9263133e-06
Iter: 1212 loss: 2.9260982e-06
Iter: 1213 loss: 2.92182926e-06
Iter: 1214 loss: 2.9216103e-06
Iter: 1215 loss: 2.91805145e-06
Iter: 1216 loss: 2.91509923e-06
Iter: 1217 loss: 2.91408128e-06
Iter: 1218 loss: 2.91020046e-06
Iter: 1219 loss: 2.95168e-06
Iter: 1220 loss: 2.9100438e-06
Iter: 1221 loss: 2.90639696e-06
Iter: 1222 loss: 2.91099627e-06
Iter: 1223 loss: 2.90448497e-06
Iter: 1224 loss: 2.90051389e-06
Iter: 1225 loss: 2.89496438e-06
Iter: 1226 loss: 2.89475656e-06
Iter: 1227 loss: 2.88867022e-06
Iter: 1228 loss: 2.93881158e-06
Iter: 1229 loss: 2.88822434e-06
Iter: 1230 loss: 2.88395768e-06
Iter: 1231 loss: 2.92496202e-06
Iter: 1232 loss: 2.88380625e-06
Iter: 1233 loss: 2.88007345e-06
Iter: 1234 loss: 2.88808701e-06
Iter: 1235 loss: 2.87858711e-06
Iter: 1236 loss: 2.87525359e-06
Iter: 1237 loss: 2.88797241e-06
Iter: 1238 loss: 2.87437706e-06
Iter: 1239 loss: 2.87029115e-06
Iter: 1240 loss: 2.87246212e-06
Iter: 1241 loss: 2.8675945e-06
Iter: 1242 loss: 2.86418708e-06
Iter: 1243 loss: 2.87868625e-06
Iter: 1244 loss: 2.86349541e-06
Iter: 1245 loss: 2.85927445e-06
Iter: 1246 loss: 2.8642396e-06
Iter: 1247 loss: 2.8571144e-06
Iter: 1248 loss: 2.85335273e-06
Iter: 1249 loss: 2.85289548e-06
Iter: 1250 loss: 2.85021201e-06
Iter: 1251 loss: 2.84529506e-06
Iter: 1252 loss: 2.85507758e-06
Iter: 1253 loss: 2.84331054e-06
Iter: 1254 loss: 2.83871373e-06
Iter: 1255 loss: 2.83871577e-06
Iter: 1256 loss: 2.83616873e-06
Iter: 1257 loss: 2.83322015e-06
Iter: 1258 loss: 2.832875e-06
Iter: 1259 loss: 2.8288955e-06
Iter: 1260 loss: 2.86638601e-06
Iter: 1261 loss: 2.82870224e-06
Iter: 1262 loss: 2.82493147e-06
Iter: 1263 loss: 2.82332803e-06
Iter: 1264 loss: 2.82139331e-06
Iter: 1265 loss: 2.81685107e-06
Iter: 1266 loss: 2.82066185e-06
Iter: 1267 loss: 2.81410803e-06
Iter: 1268 loss: 2.81118241e-06
Iter: 1269 loss: 2.81101097e-06
Iter: 1270 loss: 2.80782569e-06
Iter: 1271 loss: 2.80686277e-06
Iter: 1272 loss: 2.80487484e-06
Iter: 1273 loss: 2.80115319e-06
Iter: 1274 loss: 2.8286031e-06
Iter: 1275 loss: 2.80079439e-06
Iter: 1276 loss: 2.79737878e-06
Iter: 1277 loss: 2.7975002e-06
Iter: 1278 loss: 2.7946046e-06
Iter: 1279 loss: 2.79083815e-06
Iter: 1280 loss: 2.8070865e-06
Iter: 1281 loss: 2.78998141e-06
Iter: 1282 loss: 2.785682e-06
Iter: 1283 loss: 2.79596316e-06
Iter: 1284 loss: 2.78415837e-06
Iter: 1285 loss: 2.78030984e-06
Iter: 1286 loss: 2.77945037e-06
Iter: 1287 loss: 2.77696768e-06
Iter: 1288 loss: 2.77202207e-06
Iter: 1289 loss: 2.78176185e-06
Iter: 1290 loss: 2.77004642e-06
Iter: 1291 loss: 2.76691367e-06
Iter: 1292 loss: 2.76662377e-06
Iter: 1293 loss: 2.76440119e-06
Iter: 1294 loss: 2.76052197e-06
Iter: 1295 loss: 2.8571053e-06
Iter: 1296 loss: 2.76053197e-06
Iter: 1297 loss: 2.75627303e-06
Iter: 1298 loss: 2.78395669e-06
Iter: 1299 loss: 2.75582784e-06
Iter: 1300 loss: 2.75204548e-06
Iter: 1301 loss: 2.77056574e-06
Iter: 1302 loss: 2.75133493e-06
Iter: 1303 loss: 2.7485512e-06
Iter: 1304 loss: 2.74477634e-06
Iter: 1305 loss: 2.74458171e-06
Iter: 1306 loss: 2.74057766e-06
Iter: 1307 loss: 2.80167114e-06
Iter: 1308 loss: 2.74059585e-06
Iter: 1309 loss: 2.73648311e-06
Iter: 1310 loss: 2.74374361e-06
Iter: 1311 loss: 2.73465184e-06
Iter: 1312 loss: 2.73184332e-06
Iter: 1313 loss: 2.73278897e-06
Iter: 1314 loss: 2.72984539e-06
Iter: 1315 loss: 2.72591933e-06
Iter: 1316 loss: 2.75860066e-06
Iter: 1317 loss: 2.72560555e-06
Iter: 1318 loss: 2.72309535e-06
Iter: 1319 loss: 2.72101283e-06
Iter: 1320 loss: 2.72023112e-06
Iter: 1321 loss: 2.71641875e-06
Iter: 1322 loss: 2.75267439e-06
Iter: 1323 loss: 2.71624208e-06
Iter: 1324 loss: 2.71305703e-06
Iter: 1325 loss: 2.71120234e-06
Iter: 1326 loss: 2.70994678e-06
Iter: 1327 loss: 2.70578221e-06
Iter: 1328 loss: 2.71402223e-06
Iter: 1329 loss: 2.703988e-06
Iter: 1330 loss: 2.70041801e-06
Iter: 1331 loss: 2.7566623e-06
Iter: 1332 loss: 2.70039845e-06
Iter: 1333 loss: 2.69753673e-06
Iter: 1334 loss: 2.69764223e-06
Iter: 1335 loss: 2.69529846e-06
Iter: 1336 loss: 2.69180146e-06
Iter: 1337 loss: 2.69133602e-06
Iter: 1338 loss: 2.68891699e-06
Iter: 1339 loss: 2.68560893e-06
Iter: 1340 loss: 2.68559e-06
Iter: 1341 loss: 2.6826192e-06
Iter: 1342 loss: 2.68144368e-06
Iter: 1343 loss: 2.67988617e-06
Iter: 1344 loss: 2.6770756e-06
Iter: 1345 loss: 2.71799e-06
Iter: 1346 loss: 2.67700602e-06
Iter: 1347 loss: 2.67427185e-06
Iter: 1348 loss: 2.67165979e-06
Iter: 1349 loss: 2.67092764e-06
Iter: 1350 loss: 2.66737834e-06
Iter: 1351 loss: 2.67331939e-06
Iter: 1352 loss: 2.66567713e-06
Iter: 1353 loss: 2.66184452e-06
Iter: 1354 loss: 2.70712894e-06
Iter: 1355 loss: 2.66179427e-06
Iter: 1356 loss: 2.65910694e-06
Iter: 1357 loss: 2.65558674e-06
Iter: 1358 loss: 2.65539165e-06
Iter: 1359 loss: 2.65188169e-06
Iter: 1360 loss: 2.68676104e-06
Iter: 1361 loss: 2.65175368e-06
Iter: 1362 loss: 2.64823552e-06
Iter: 1363 loss: 2.65068797e-06
Iter: 1364 loss: 2.64595155e-06
Iter: 1365 loss: 2.64277514e-06
Iter: 1366 loss: 2.64673849e-06
Iter: 1367 loss: 2.64109576e-06
Iter: 1368 loss: 2.63696529e-06
Iter: 1369 loss: 2.67279461e-06
Iter: 1370 loss: 2.63676543e-06
Iter: 1371 loss: 2.63363154e-06
Iter: 1372 loss: 2.63054653e-06
Iter: 1373 loss: 2.62993012e-06
Iter: 1374 loss: 2.62572576e-06
Iter: 1375 loss: 2.64033815e-06
Iter: 1376 loss: 2.62455956e-06
Iter: 1377 loss: 2.62071444e-06
Iter: 1378 loss: 2.6613202e-06
Iter: 1379 loss: 2.62058165e-06
Iter: 1380 loss: 2.61763512e-06
Iter: 1381 loss: 2.62196727e-06
Iter: 1382 loss: 2.61616833e-06
Iter: 1383 loss: 2.61324931e-06
Iter: 1384 loss: 2.63287575e-06
Iter: 1385 loss: 2.61290552e-06
Iter: 1386 loss: 2.61034825e-06
Iter: 1387 loss: 2.60746242e-06
Iter: 1388 loss: 2.60709862e-06
Iter: 1389 loss: 2.60417528e-06
Iter: 1390 loss: 2.62769572e-06
Iter: 1391 loss: 2.6039661e-06
Iter: 1392 loss: 2.60087882e-06
Iter: 1393 loss: 2.60930028e-06
Iter: 1394 loss: 2.59982312e-06
Iter: 1395 loss: 2.59723402e-06
Iter: 1396 loss: 2.59415424e-06
Iter: 1397 loss: 2.59383091e-06
Iter: 1398 loss: 2.59063199e-06
Iter: 1399 loss: 2.64168466e-06
Iter: 1400 loss: 2.59062426e-06
Iter: 1401 loss: 2.58742102e-06
Iter: 1402 loss: 2.58658974e-06
Iter: 1403 loss: 2.5845552e-06
Iter: 1404 loss: 2.5808381e-06
Iter: 1405 loss: 2.59982221e-06
Iter: 1406 loss: 2.58027535e-06
Iter: 1407 loss: 2.57686315e-06
Iter: 1408 loss: 2.59584021e-06
Iter: 1409 loss: 2.5764673e-06
Iter: 1410 loss: 2.57402189e-06
Iter: 1411 loss: 2.56991848e-06
Iter: 1412 loss: 2.56990097e-06
Iter: 1413 loss: 2.56604039e-06
Iter: 1414 loss: 2.60778825e-06
Iter: 1415 loss: 2.56591238e-06
Iter: 1416 loss: 2.56259978e-06
Iter: 1417 loss: 2.58805358e-06
Iter: 1418 loss: 2.56238241e-06
Iter: 1419 loss: 2.56016892e-06
Iter: 1420 loss: 2.56016665e-06
Iter: 1421 loss: 2.55851319e-06
Iter: 1422 loss: 2.55457826e-06
Iter: 1423 loss: 2.56080966e-06
Iter: 1424 loss: 2.55272266e-06
Iter: 1425 loss: 2.54958422e-06
Iter: 1426 loss: 2.5537031e-06
Iter: 1427 loss: 2.5479444e-06
Iter: 1428 loss: 2.54502697e-06
Iter: 1429 loss: 2.5824329e-06
Iter: 1430 loss: 2.54502174e-06
Iter: 1431 loss: 2.54247e-06
Iter: 1432 loss: 2.53914095e-06
Iter: 1433 loss: 2.53899816e-06
Iter: 1434 loss: 2.53469852e-06
Iter: 1435 loss: 2.54224665e-06
Iter: 1436 loss: 2.53277744e-06
Iter: 1437 loss: 2.52907444e-06
Iter: 1438 loss: 2.57500847e-06
Iter: 1439 loss: 2.52906375e-06
Iter: 1440 loss: 2.52576069e-06
Iter: 1441 loss: 2.52931977e-06
Iter: 1442 loss: 2.52402629e-06
Iter: 1443 loss: 2.52099426e-06
Iter: 1444 loss: 2.52740529e-06
Iter: 1445 loss: 2.51967276e-06
Iter: 1446 loss: 2.51627e-06
Iter: 1447 loss: 2.54025917e-06
Iter: 1448 loss: 2.51597839e-06
Iter: 1449 loss: 2.51371875e-06
Iter: 1450 loss: 2.50989569e-06
Iter: 1451 loss: 2.50989751e-06
Iter: 1452 loss: 2.50613675e-06
Iter: 1453 loss: 2.54562497e-06
Iter: 1454 loss: 2.50603762e-06
Iter: 1455 loss: 2.50206472e-06
Iter: 1456 loss: 2.51812071e-06
Iter: 1457 loss: 2.50116864e-06
Iter: 1458 loss: 2.49857726e-06
Iter: 1459 loss: 2.49659911e-06
Iter: 1460 loss: 2.49569598e-06
Iter: 1461 loss: 2.49241202e-06
Iter: 1462 loss: 2.5310519e-06
Iter: 1463 loss: 2.49234108e-06
Iter: 1464 loss: 2.48942138e-06
Iter: 1465 loss: 2.48788183e-06
Iter: 1466 loss: 2.48657147e-06
Iter: 1467 loss: 2.48366e-06
Iter: 1468 loss: 2.51605911e-06
Iter: 1469 loss: 2.48363699e-06
Iter: 1470 loss: 2.48072206e-06
Iter: 1471 loss: 2.47873641e-06
Iter: 1472 loss: 2.47762728e-06
Iter: 1473 loss: 2.47437356e-06
Iter: 1474 loss: 2.47641037e-06
Iter: 1475 loss: 2.47230491e-06
Iter: 1476 loss: 2.46781269e-06
Iter: 1477 loss: 2.48104834e-06
Iter: 1478 loss: 2.46649142e-06
Iter: 1479 loss: 2.46313266e-06
Iter: 1480 loss: 2.50109929e-06
Iter: 1481 loss: 2.46299624e-06
Iter: 1482 loss: 2.45984575e-06
Iter: 1483 loss: 2.46351556e-06
Iter: 1484 loss: 2.45810179e-06
Iter: 1485 loss: 2.45504225e-06
Iter: 1486 loss: 2.45942329e-06
Iter: 1487 loss: 2.45352294e-06
Iter: 1488 loss: 2.45037745e-06
Iter: 1489 loss: 2.48585729e-06
Iter: 1490 loss: 2.45030333e-06
Iter: 1491 loss: 2.44780631e-06
Iter: 1492 loss: 2.44352054e-06
Iter: 1493 loss: 2.44354032e-06
Iter: 1494 loss: 2.43953673e-06
Iter: 1495 loss: 2.45383853e-06
Iter: 1496 loss: 2.43850263e-06
Iter: 1497 loss: 2.43620343e-06
Iter: 1498 loss: 2.43596196e-06
Iter: 1499 loss: 2.43410773e-06
Iter: 1500 loss: 2.43205363e-06
Iter: 1501 loss: 2.43180421e-06
Iter: 1502 loss: 2.42939677e-06
Iter: 1503 loss: 2.45269189e-06
Iter: 1504 loss: 2.42935971e-06
Iter: 1505 loss: 2.42702163e-06
Iter: 1506 loss: 2.42291662e-06
Iter: 1507 loss: 2.5223851e-06
Iter: 1508 loss: 2.42292799e-06
Iter: 1509 loss: 2.41951e-06
Iter: 1510 loss: 2.46492982e-06
Iter: 1511 loss: 2.41949692e-06
Iter: 1512 loss: 2.41630664e-06
Iter: 1513 loss: 2.42542501e-06
Iter: 1514 loss: 2.41529483e-06
Iter: 1515 loss: 2.41261819e-06
Iter: 1516 loss: 2.40910276e-06
Iter: 1517 loss: 2.40879672e-06
Iter: 1518 loss: 2.40445979e-06
Iter: 1519 loss: 2.41504631e-06
Iter: 1520 loss: 2.40286e-06
Iter: 1521 loss: 2.39911901e-06
Iter: 1522 loss: 2.45674482e-06
Iter: 1523 loss: 2.39911333e-06
Iter: 1524 loss: 2.39600467e-06
Iter: 1525 loss: 2.40197141e-06
Iter: 1526 loss: 2.39468704e-06
Iter: 1527 loss: 2.39189603e-06
Iter: 1528 loss: 2.39623137e-06
Iter: 1529 loss: 2.39069914e-06
Iter: 1530 loss: 2.38806388e-06
Iter: 1531 loss: 2.41086764e-06
Iter: 1532 loss: 2.38791199e-06
Iter: 1533 loss: 2.38572807e-06
Iter: 1534 loss: 2.38290863e-06
Iter: 1535 loss: 2.38275425e-06
Iter: 1536 loss: 2.38014536e-06
Iter: 1537 loss: 2.38014013e-06
Iter: 1538 loss: 2.37737731e-06
Iter: 1539 loss: 2.37842619e-06
Iter: 1540 loss: 2.37549375e-06
Iter: 1541 loss: 2.3727423e-06
Iter: 1542 loss: 2.3687503e-06
Iter: 1543 loss: 2.3686689e-06
Iter: 1544 loss: 2.36542314e-06
Iter: 1545 loss: 2.36531446e-06
Iter: 1546 loss: 2.36208052e-06
Iter: 1547 loss: 2.36577898e-06
Iter: 1548 loss: 2.36046617e-06
Iter: 1549 loss: 2.35781795e-06
Iter: 1550 loss: 2.36493906e-06
Iter: 1551 loss: 2.35697212e-06
Iter: 1552 loss: 2.35386869e-06
Iter: 1553 loss: 2.36710753e-06
Iter: 1554 loss: 2.35319749e-06
Iter: 1555 loss: 2.35110383e-06
Iter: 1556 loss: 2.34781805e-06
Iter: 1557 loss: 2.34776144e-06
Iter: 1558 loss: 2.3434286e-06
Iter: 1559 loss: 2.36171218e-06
Iter: 1560 loss: 2.34248682e-06
Iter: 1561 loss: 2.33962e-06
Iter: 1562 loss: 2.38328357e-06
Iter: 1563 loss: 2.33962601e-06
Iter: 1564 loss: 2.33672245e-06
Iter: 1565 loss: 2.3358341e-06
Iter: 1566 loss: 2.33413448e-06
Iter: 1567 loss: 2.33101559e-06
Iter: 1568 loss: 2.33638661e-06
Iter: 1569 loss: 2.32962407e-06
Iter: 1570 loss: 2.327e-06
Iter: 1571 loss: 2.32697403e-06
Iter: 1572 loss: 2.32496e-06
Iter: 1573 loss: 2.32591083e-06
Iter: 1574 loss: 2.32360298e-06
Iter: 1575 loss: 2.32128923e-06
Iter: 1576 loss: 2.32933735e-06
Iter: 1577 loss: 2.32063053e-06
Iter: 1578 loss: 2.31806325e-06
Iter: 1579 loss: 2.31775289e-06
Iter: 1580 loss: 2.31589729e-06
Iter: 1581 loss: 2.31277681e-06
Iter: 1582 loss: 2.31221179e-06
Iter: 1583 loss: 2.31011018e-06
Iter: 1584 loss: 2.30729211e-06
Iter: 1585 loss: 2.30722344e-06
Iter: 1586 loss: 2.30446312e-06
Iter: 1587 loss: 2.30581372e-06
Iter: 1588 loss: 2.30262344e-06
Iter: 1589 loss: 2.29984403e-06
Iter: 1590 loss: 2.29824809e-06
Iter: 1591 loss: 2.29708075e-06
Iter: 1592 loss: 2.29371722e-06
Iter: 1593 loss: 2.3111852e-06
Iter: 1594 loss: 2.29322518e-06
Iter: 1595 loss: 2.290471e-06
Iter: 1596 loss: 2.32697198e-06
Iter: 1597 loss: 2.2904519e-06
Iter: 1598 loss: 2.28865088e-06
Iter: 1599 loss: 2.28633576e-06
Iter: 1600 loss: 2.28614454e-06
Iter: 1601 loss: 2.28343674e-06
Iter: 1602 loss: 2.31499121e-06
Iter: 1603 loss: 2.2834738e-06
Iter: 1604 loss: 2.28093745e-06
Iter: 1605 loss: 2.28334625e-06
Iter: 1606 loss: 2.27949135e-06
Iter: 1607 loss: 2.27724399e-06
Iter: 1608 loss: 2.28485828e-06
Iter: 1609 loss: 2.27666396e-06
Iter: 1610 loss: 2.27386658e-06
Iter: 1611 loss: 2.28307317e-06
Iter: 1612 loss: 2.27310693e-06
Iter: 1613 loss: 2.27088935e-06
Iter: 1614 loss: 2.26760221e-06
Iter: 1615 loss: 2.26754219e-06
Iter: 1616 loss: 2.26375664e-06
Iter: 1617 loss: 2.27753662e-06
Iter: 1618 loss: 2.26272755e-06
Iter: 1619 loss: 2.25929125e-06
Iter: 1620 loss: 2.29225384e-06
Iter: 1621 loss: 2.25919e-06
Iter: 1622 loss: 2.25612166e-06
Iter: 1623 loss: 2.25924896e-06
Iter: 1624 loss: 2.2544798e-06
Iter: 1625 loss: 2.25163308e-06
Iter: 1626 loss: 2.26080283e-06
Iter: 1627 loss: 2.25078384e-06
Iter: 1628 loss: 2.24783639e-06
Iter: 1629 loss: 2.26648717e-06
Iter: 1630 loss: 2.24743781e-06
Iter: 1631 loss: 2.24551718e-06
Iter: 1632 loss: 2.24152836e-06
Iter: 1633 loss: 2.3104235e-06
Iter: 1634 loss: 2.24140535e-06
Iter: 1635 loss: 2.23771985e-06
Iter: 1636 loss: 2.25758708e-06
Iter: 1637 loss: 2.23723396e-06
Iter: 1638 loss: 2.23408165e-06
Iter: 1639 loss: 2.26853899e-06
Iter: 1640 loss: 2.23405345e-06
Iter: 1641 loss: 2.23126563e-06
Iter: 1642 loss: 2.23264374e-06
Iter: 1643 loss: 2.22944095e-06
Iter: 1644 loss: 2.22695257e-06
Iter: 1645 loss: 2.2413933e-06
Iter: 1646 loss: 2.22668405e-06
Iter: 1647 loss: 2.22388053e-06
Iter: 1648 loss: 2.23271059e-06
Iter: 1649 loss: 2.2230488e-06
Iter: 1650 loss: 2.22111294e-06
Iter: 1651 loss: 2.21710502e-06
Iter: 1652 loss: 2.28560975e-06
Iter: 1653 loss: 2.21704659e-06
Iter: 1654 loss: 2.21246592e-06
Iter: 1655 loss: 2.24197584e-06
Iter: 1656 loss: 2.2119591e-06
Iter: 1657 loss: 2.20921675e-06
Iter: 1658 loss: 2.24347878e-06
Iter: 1659 loss: 2.20914012e-06
Iter: 1660 loss: 2.2067486e-06
Iter: 1661 loss: 2.20803759e-06
Iter: 1662 loss: 2.20512084e-06
Iter: 1663 loss: 2.2026079e-06
Iter: 1664 loss: 2.21269215e-06
Iter: 1665 loss: 2.20205652e-06
Iter: 1666 loss: 2.19953245e-06
Iter: 1667 loss: 2.21108894e-06
Iter: 1668 loss: 2.19908588e-06
Iter: 1669 loss: 2.19712433e-06
Iter: 1670 loss: 2.19466597e-06
Iter: 1671 loss: 2.19444428e-06
Iter: 1672 loss: 2.19128015e-06
Iter: 1673 loss: 2.20411903e-06
Iter: 1674 loss: 2.19056346e-06
Iter: 1675 loss: 2.18739251e-06
Iter: 1676 loss: 2.21939445e-06
Iter: 1677 loss: 2.18727041e-06
Iter: 1678 loss: 2.18494779e-06
Iter: 1679 loss: 2.18264449e-06
Iter: 1680 loss: 2.18219429e-06
Iter: 1681 loss: 2.17945e-06
Iter: 1682 loss: 2.19370258e-06
Iter: 1683 loss: 2.17901311e-06
Iter: 1684 loss: 2.17581692e-06
Iter: 1685 loss: 2.18935975e-06
Iter: 1686 loss: 2.17509705e-06
Iter: 1687 loss: 2.17326578e-06
Iter: 1688 loss: 2.18474e-06
Iter: 1689 loss: 2.1730375e-06
Iter: 1690 loss: 2.17115917e-06
Iter: 1691 loss: 2.16915441e-06
Iter: 1692 loss: 2.16884087e-06
Iter: 1693 loss: 2.16616013e-06
Iter: 1694 loss: 2.16688613e-06
Iter: 1695 loss: 2.16428e-06
Iter: 1696 loss: 2.16095191e-06
Iter: 1697 loss: 2.17113552e-06
Iter: 1698 loss: 2.15995328e-06
Iter: 1699 loss: 2.15688715e-06
Iter: 1700 loss: 2.1856597e-06
Iter: 1701 loss: 2.15675e-06
Iter: 1702 loss: 2.15410046e-06
Iter: 1703 loss: 2.15976434e-06
Iter: 1704 loss: 2.15303908e-06
Iter: 1705 loss: 2.15074306e-06
Iter: 1706 loss: 2.15090563e-06
Iter: 1707 loss: 2.14892862e-06
Iter: 1708 loss: 2.14594434e-06
Iter: 1709 loss: 2.16095259e-06
Iter: 1710 loss: 2.14548618e-06
Iter: 1711 loss: 2.14239208e-06
Iter: 1712 loss: 2.15981868e-06
Iter: 1713 loss: 2.1419246e-06
Iter: 1714 loss: 2.13968679e-06
Iter: 1715 loss: 2.13701082e-06
Iter: 1716 loss: 2.13677072e-06
Iter: 1717 loss: 2.13422095e-06
Iter: 1718 loss: 2.16552803e-06
Iter: 1719 loss: 2.13423e-06
Iter: 1720 loss: 2.13170665e-06
Iter: 1721 loss: 2.13760791e-06
Iter: 1722 loss: 2.13086378e-06
Iter: 1723 loss: 2.12865962e-06
Iter: 1724 loss: 2.13309931e-06
Iter: 1725 loss: 2.12780651e-06
Iter: 1726 loss: 2.12504733e-06
Iter: 1727 loss: 2.13616977e-06
Iter: 1728 loss: 2.12445821e-06
Iter: 1729 loss: 2.1225535e-06
Iter: 1730 loss: 2.12280975e-06
Iter: 1731 loss: 2.12108853e-06
Iter: 1732 loss: 2.11823453e-06
Iter: 1733 loss: 2.13493195e-06
Iter: 1734 loss: 2.11791348e-06
Iter: 1735 loss: 2.11610359e-06
Iter: 1736 loss: 2.11245469e-06
Iter: 1737 loss: 2.1757894e-06
Iter: 1738 loss: 2.11233646e-06
Iter: 1739 loss: 2.10863277e-06
Iter: 1740 loss: 2.13558224e-06
Iter: 1741 loss: 2.10830331e-06
Iter: 1742 loss: 2.1052374e-06
Iter: 1743 loss: 2.13422504e-06
Iter: 1744 loss: 2.10508915e-06
Iter: 1745 loss: 2.10286316e-06
Iter: 1746 loss: 2.10339863e-06
Iter: 1747 loss: 2.10123176e-06
Iter: 1748 loss: 2.0987361e-06
Iter: 1749 loss: 2.10146482e-06
Iter: 1750 loss: 2.09738846e-06
Iter: 1751 loss: 2.09509699e-06
Iter: 1752 loss: 2.09510654e-06
Iter: 1753 loss: 2.09318614e-06
Iter: 1754 loss: 2.09320933e-06
Iter: 1755 loss: 2.0916209e-06
Iter: 1756 loss: 2.08915935e-06
Iter: 1757 loss: 2.0878615e-06
Iter: 1758 loss: 2.08680808e-06
Iter: 1759 loss: 2.08412484e-06
Iter: 1760 loss: 2.08410756e-06
Iter: 1761 loss: 2.08164465e-06
Iter: 1762 loss: 2.0841253e-06
Iter: 1763 loss: 2.08029e-06
Iter: 1764 loss: 2.07788617e-06
Iter: 1765 loss: 2.08208735e-06
Iter: 1766 loss: 2.07690164e-06
Iter: 1767 loss: 2.07361859e-06
Iter: 1768 loss: 2.08690017e-06
Iter: 1769 loss: 2.07287826e-06
Iter: 1770 loss: 2.07064613e-06
Iter: 1771 loss: 2.07676021e-06
Iter: 1772 loss: 2.06991035e-06
Iter: 1773 loss: 2.06727509e-06
Iter: 1774 loss: 2.07611538e-06
Iter: 1775 loss: 2.0665052e-06
Iter: 1776 loss: 2.06458412e-06
Iter: 1777 loss: 2.06169534e-06
Iter: 1778 loss: 2.06163054e-06
Iter: 1779 loss: 2.05871856e-06
Iter: 1780 loss: 2.0828229e-06
Iter: 1781 loss: 2.05849437e-06
Iter: 1782 loss: 2.05584865e-06
Iter: 1783 loss: 2.06948062e-06
Iter: 1784 loss: 2.05546849e-06
Iter: 1785 loss: 2.05351898e-06
Iter: 1786 loss: 2.05018091e-06
Iter: 1787 loss: 2.1339024e-06
Iter: 1788 loss: 2.0501634e-06
Iter: 1789 loss: 2.04673029e-06
Iter: 1790 loss: 2.0851121e-06
Iter: 1791 loss: 2.04667322e-06
Iter: 1792 loss: 2.04399839e-06
Iter: 1793 loss: 2.06579921e-06
Iter: 1794 loss: 2.04390699e-06
Iter: 1795 loss: 2.04185699e-06
Iter: 1796 loss: 2.0405605e-06
Iter: 1797 loss: 2.03988816e-06
Iter: 1798 loss: 2.03715422e-06
Iter: 1799 loss: 2.04300477e-06
Iter: 1800 loss: 2.03613581e-06
Iter: 1801 loss: 2.03422087e-06
Iter: 1802 loss: 2.03421791e-06
Iter: 1803 loss: 2.03250102e-06
Iter: 1804 loss: 2.02989349e-06
Iter: 1805 loss: 2.02990987e-06
Iter: 1806 loss: 2.02741694e-06
Iter: 1807 loss: 2.06018808e-06
Iter: 1808 loss: 2.02741012e-06
Iter: 1809 loss: 2.02515412e-06
Iter: 1810 loss: 2.02624892e-06
Iter: 1811 loss: 2.02367301e-06
Iter: 1812 loss: 2.02128581e-06
Iter: 1813 loss: 2.02314777e-06
Iter: 1814 loss: 2.01985813e-06
Iter: 1815 loss: 2.01745866e-06
Iter: 1816 loss: 2.01746434e-06
Iter: 1817 loss: 2.0160428e-06
Iter: 1818 loss: 2.01357761e-06
Iter: 1819 loss: 2.01358534e-06
Iter: 1820 loss: 2.01152397e-06
Iter: 1821 loss: 2.03875379e-06
Iter: 1822 loss: 2.01151056e-06
Iter: 1823 loss: 2.00937666e-06
Iter: 1824 loss: 2.00614954e-06
Iter: 1825 loss: 2.00615136e-06
Iter: 1826 loss: 2.00263821e-06
Iter: 1827 loss: 2.01486682e-06
Iter: 1828 loss: 2.0017028e-06
Iter: 1829 loss: 1.99911574e-06
Iter: 1830 loss: 2.01702051e-06
Iter: 1831 loss: 1.99886063e-06
Iter: 1832 loss: 1.99638339e-06
Iter: 1833 loss: 2.01200487e-06
Iter: 1834 loss: 1.99611304e-06
Iter: 1835 loss: 1.99438614e-06
Iter: 1836 loss: 1.99150418e-06
Iter: 1837 loss: 1.99149963e-06
Iter: 1838 loss: 1.98850262e-06
Iter: 1839 loss: 2.01216358e-06
Iter: 1840 loss: 1.9882666e-06
Iter: 1841 loss: 1.98587395e-06
Iter: 1842 loss: 1.99910801e-06
Iter: 1843 loss: 1.98552652e-06
Iter: 1844 loss: 1.98324915e-06
Iter: 1845 loss: 1.98407565e-06
Iter: 1846 loss: 1.98169687e-06
Iter: 1847 loss: 1.97951658e-06
Iter: 1848 loss: 1.99169426e-06
Iter: 1849 loss: 1.97918189e-06
Iter: 1850 loss: 1.97664144e-06
Iter: 1851 loss: 1.98348107e-06
Iter: 1852 loss: 1.97571399e-06
Iter: 1853 loss: 1.97391182e-06
Iter: 1854 loss: 1.97831832e-06
Iter: 1855 loss: 1.97328609e-06
Iter: 1856 loss: 1.97088525e-06
Iter: 1857 loss: 1.97687e-06
Iter: 1858 loss: 1.97001646e-06
Iter: 1859 loss: 1.96811425e-06
Iter: 1860 loss: 1.96632391e-06
Iter: 1861 loss: 1.96588303e-06
Iter: 1862 loss: 1.96343535e-06
Iter: 1863 loss: 1.96343831e-06
Iter: 1864 loss: 1.96167139e-06
Iter: 1865 loss: 1.96028395e-06
Iter: 1866 loss: 1.95977054e-06
Iter: 1867 loss: 1.95688381e-06
Iter: 1868 loss: 1.95623716e-06
Iter: 1869 loss: 1.95445114e-06
Iter: 1870 loss: 1.95117104e-06
Iter: 1871 loss: 1.98032217e-06
Iter: 1872 loss: 1.9509805e-06
Iter: 1873 loss: 1.94889071e-06
Iter: 1874 loss: 1.94889935e-06
Iter: 1875 loss: 1.94734821e-06
Iter: 1876 loss: 1.94506129e-06
Iter: 1877 loss: 1.94504719e-06
Iter: 1878 loss: 1.94236418e-06
Iter: 1879 loss: 1.95843722e-06
Iter: 1880 loss: 1.94209679e-06
Iter: 1881 loss: 1.94010454e-06
Iter: 1882 loss: 1.95408211e-06
Iter: 1883 loss: 1.93993219e-06
Iter: 1884 loss: 1.93812821e-06
Iter: 1885 loss: 1.93688493e-06
Iter: 1886 loss: 1.93620417e-06
Iter: 1887 loss: 1.93414394e-06
Iter: 1888 loss: 1.93414917e-06
Iter: 1889 loss: 1.93272035e-06
Iter: 1890 loss: 1.93040228e-06
Iter: 1891 loss: 1.93042661e-06
Iter: 1892 loss: 1.92792822e-06
Iter: 1893 loss: 1.93836831e-06
Iter: 1894 loss: 1.92735797e-06
Iter: 1895 loss: 1.92478774e-06
Iter: 1896 loss: 1.94116274e-06
Iter: 1897 loss: 1.92446987e-06
Iter: 1898 loss: 1.92281e-06
Iter: 1899 loss: 1.92069842e-06
Iter: 1900 loss: 1.92059269e-06
Iter: 1901 loss: 1.91835488e-06
Iter: 1902 loss: 1.91831759e-06
Iter: 1903 loss: 1.91676099e-06
Iter: 1904 loss: 1.91401e-06
Iter: 1905 loss: 1.91404956e-06
Iter: 1906 loss: 1.91076151e-06
Iter: 1907 loss: 1.92280959e-06
Iter: 1908 loss: 1.91002209e-06
Iter: 1909 loss: 1.90830451e-06
Iter: 1910 loss: 1.90820947e-06
Iter: 1911 loss: 1.90671472e-06
Iter: 1912 loss: 1.90409253e-06
Iter: 1913 loss: 1.90410037e-06
Iter: 1914 loss: 1.9013994e-06
Iter: 1915 loss: 1.90701701e-06
Iter: 1916 loss: 1.90035928e-06
Iter: 1917 loss: 1.89833406e-06
Iter: 1918 loss: 1.89822617e-06
Iter: 1919 loss: 1.8967346e-06
Iter: 1920 loss: 1.89684079e-06
Iter: 1921 loss: 1.89557898e-06
Iter: 1922 loss: 1.89355956e-06
Iter: 1923 loss: 1.90551827e-06
Iter: 1924 loss: 1.89326931e-06
Iter: 1925 loss: 1.89175387e-06
Iter: 1926 loss: 1.88974036e-06
Iter: 1927 loss: 1.88955369e-06
Iter: 1928 loss: 1.88705758e-06
Iter: 1929 loss: 1.90607921e-06
Iter: 1930 loss: 1.88688409e-06
Iter: 1931 loss: 1.88468766e-06
Iter: 1932 loss: 1.89620982e-06
Iter: 1933 loss: 1.8843441e-06
Iter: 1934 loss: 1.88264426e-06
Iter: 1935 loss: 1.88008482e-06
Iter: 1936 loss: 1.88007471e-06
Iter: 1937 loss: 1.87732508e-06
Iter: 1938 loss: 1.90756418e-06
Iter: 1939 loss: 1.87725709e-06
Iter: 1940 loss: 1.87500291e-06
Iter: 1941 loss: 1.88245508e-06
Iter: 1942 loss: 1.87439127e-06
Iter: 1943 loss: 1.87256308e-06
Iter: 1944 loss: 1.87036392e-06
Iter: 1945 loss: 1.87013165e-06
Iter: 1946 loss: 1.86846432e-06
Iter: 1947 loss: 1.86819284e-06
Iter: 1948 loss: 1.86668126e-06
Iter: 1949 loss: 1.86416833e-06
Iter: 1950 loss: 1.86414331e-06
Iter: 1951 loss: 1.86204966e-06
Iter: 1952 loss: 1.87784042e-06
Iter: 1953 loss: 1.86184047e-06
Iter: 1954 loss: 1.85998101e-06
Iter: 1955 loss: 1.87456749e-06
Iter: 1956 loss: 1.85983379e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi1.6
+ date
Wed Nov  4 13:51:48 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.2/300_300_300_1 --function f2 --psi 1 --alpha 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f441d2672f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f441d267bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f441d1cb510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f441d1fc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f441d1fcae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f441d1e3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f8200bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f441d2256a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f441d225488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f8248620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f8248c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f82488c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f810fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f8144730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f81697b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f81ee840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f81bf510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f8092840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f8061730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f8092488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f80921e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f8059d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43e012cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43e0086840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43e0086a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f81b88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f44497cb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f818b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43f818bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394799510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43e00c7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43e00c88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43e00c8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43e00c8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394706ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394714bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0113233235
test_loss: 0.013268628
train_loss: 0.008509384
test_loss: 0.011336936
train_loss: 0.009053457
test_loss: 0.010506282
train_loss: 0.00810783
test_loss: 0.010101094
train_loss: 0.007506921
test_loss: 0.009913881
train_loss: 0.0073043588
test_loss: 0.009895082
train_loss: 0.006948162
test_loss: 0.009558784
train_loss: 0.0071622096
test_loss: 0.009550959
train_loss: 0.007689095
test_loss: 0.00970439
train_loss: 0.0073282504
test_loss: 0.0096752765
train_loss: 0.0069060284
test_loss: 0.009309109
train_loss: 0.007248245
test_loss: 0.009290616
train_loss: 0.006608732
test_loss: 0.009196524
train_loss: 0.006187856
test_loss: 0.009235597
train_loss: 0.006926696
test_loss: 0.008929872
train_loss: 0.0069767004
test_loss: 0.009509217
train_loss: 0.006432114
test_loss: 0.0092615895
train_loss: 0.007344586
test_loss: 0.009319832
train_loss: 0.0057558953
test_loss: 0.0088513065
train_loss: 0.00646678
test_loss: 0.009058956
train_loss: 0.0061964286
test_loss: 0.009044036
train_loss: 0.0067017963
test_loss: 0.009035006
train_loss: 0.006354958
test_loss: 0.0091552455
train_loss: 0.0063745375
test_loss: 0.009025097
train_loss: 0.0060469517
test_loss: 0.00881685
train_loss: 0.0063125635
test_loss: 0.00884895
train_loss: 0.006232138
test_loss: 0.009007053
train_loss: 0.00608289
test_loss: 0.008760497
train_loss: 0.006239911
test_loss: 0.009097073
train_loss: 0.0065248953
test_loss: 0.0090572145
train_loss: 0.0060137296
test_loss: 0.008848818
train_loss: 0.0057632783
test_loss: 0.00878765
train_loss: 0.005928577
test_loss: 0.008859589
train_loss: 0.005989228
test_loss: 0.008581479
train_loss: 0.0056220847
test_loss: 0.008806999
train_loss: 0.0059556235
test_loss: 0.008741003
train_loss: 0.0061273016
test_loss: 0.0086680865
train_loss: 0.0056440546
test_loss: 0.008664328
train_loss: 0.0057617202
test_loss: 0.008668321
train_loss: 0.00594756
test_loss: 0.008662486
train_loss: 0.0057298904
test_loss: 0.008607552
train_loss: 0.006027512
test_loss: 0.00888234
train_loss: 0.0057229428
test_loss: 0.008816254
train_loss: 0.0060803215
test_loss: 0.008814553
train_loss: 0.0060879323
test_loss: 0.0087788245
train_loss: 0.005583289
test_loss: 0.00864752
train_loss: 0.0060849776
test_loss: 0.008560212
train_loss: 0.0055740466
test_loss: 0.0084545445
train_loss: 0.0055536656
test_loss: 0.008457812
train_loss: 0.005932508
test_loss: 0.008728152
train_loss: 0.005536355
test_loss: 0.008703252
train_loss: 0.0059803985
test_loss: 0.0087787
train_loss: 0.005315363
test_loss: 0.008468417
train_loss: 0.0059388634
test_loss: 0.008565499
train_loss: 0.006077909
test_loss: 0.008513632
train_loss: 0.0058447123
test_loss: 0.008487872
train_loss: 0.005463413
test_loss: 0.008729735
train_loss: 0.0055849897
test_loss: 0.0084295
train_loss: 0.005803373
test_loss: 0.008628745
train_loss: 0.0058242353
test_loss: 0.008613813
train_loss: 0.00549462
test_loss: 0.008569706
train_loss: 0.0060454905
test_loss: 0.008554633
train_loss: 0.00545883
test_loss: 0.008440509
train_loss: 0.0052506384
test_loss: 0.0083408225
train_loss: 0.0053878045
test_loss: 0.00833024
train_loss: 0.0056843264
test_loss: 0.008375806
train_loss: 0.00612894
test_loss: 0.008563101
train_loss: 0.0054814783
test_loss: 0.008358469
train_loss: 0.0051457714
test_loss: 0.008333235
train_loss: 0.005595366
test_loss: 0.008485621
train_loss: 0.005311016
test_loss: 0.008383026
train_loss: 0.0052755643
test_loss: 0.0083056595
train_loss: 0.005246166
test_loss: 0.008335119
train_loss: 0.0057527465
test_loss: 0.008310581
train_loss: 0.0052737338
test_loss: 0.008493932
train_loss: 0.005179503
test_loss: 0.008367707
train_loss: 0.0054189134
test_loss: 0.008280693
train_loss: 0.0059686806
test_loss: 0.008362881
train_loss: 0.005462155
test_loss: 0.00825137
train_loss: 0.0053197523
test_loss: 0.008207528
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.6/300_300_300_1 --optimizer lbfgs --function f2 --psi 1 --alpha 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4007fc5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4007fc5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4007ede620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4007e6e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4007e6eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4007ea4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4007ea4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4007e04730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4007e08488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4007e089d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8c0a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4007e08bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8bb9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8ba7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8b39730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8b44510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8b44a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8b077b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8ace8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8ace9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8ace378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8a9d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8a52bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8a0e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8a5b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff89bd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff89dd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff89dd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff898b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff897bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff89b26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff897b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff8909950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff892b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff88dc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ff88d5ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.56835876e-05
Iter: 2 loss: 4.9825132e-05
Iter: 3 loss: 4.91607207e-05
Iter: 4 loss: 4.49224754e-05
Iter: 5 loss: 3.84624582e-05
Iter: 6 loss: 0.000105335901
Iter: 7 loss: 3.82811704e-05
Iter: 8 loss: 3.52435964e-05
Iter: 9 loss: 3.51556955e-05
Iter: 10 loss: 3.27841481e-05
Iter: 11 loss: 3.00439133e-05
Iter: 12 loss: 6.7375353e-05
Iter: 13 loss: 3.00325737e-05
Iter: 14 loss: 2.82999463e-05
Iter: 15 loss: 2.78444713e-05
Iter: 16 loss: 2.67638898e-05
Iter: 17 loss: 2.47377138e-05
Iter: 18 loss: 2.86826235e-05
Iter: 19 loss: 2.39015826e-05
Iter: 20 loss: 2.24056021e-05
Iter: 21 loss: 4.42723467e-05
Iter: 22 loss: 2.24023133e-05
Iter: 23 loss: 2.13953044e-05
Iter: 24 loss: 2.25612857e-05
Iter: 25 loss: 2.08594829e-05
Iter: 26 loss: 1.98700709e-05
Iter: 27 loss: 2.14507563e-05
Iter: 28 loss: 1.9411149e-05
Iter: 29 loss: 1.86508623e-05
Iter: 30 loss: 2.98167834e-05
Iter: 31 loss: 1.86500474e-05
Iter: 32 loss: 1.79948984e-05
Iter: 33 loss: 1.90848114e-05
Iter: 34 loss: 1.76978283e-05
Iter: 35 loss: 1.72674609e-05
Iter: 36 loss: 1.78646314e-05
Iter: 37 loss: 1.70540225e-05
Iter: 38 loss: 1.65088604e-05
Iter: 39 loss: 2.02779447e-05
Iter: 40 loss: 1.64572739e-05
Iter: 41 loss: 1.61167845e-05
Iter: 42 loss: 1.57716804e-05
Iter: 43 loss: 1.57044087e-05
Iter: 44 loss: 1.52881858e-05
Iter: 45 loss: 1.77257934e-05
Iter: 46 loss: 1.52336888e-05
Iter: 47 loss: 1.49084744e-05
Iter: 48 loss: 1.49084644e-05
Iter: 49 loss: 1.47276469e-05
Iter: 50 loss: 1.44643545e-05
Iter: 51 loss: 1.44570686e-05
Iter: 52 loss: 1.41703713e-05
Iter: 53 loss: 1.63337081e-05
Iter: 54 loss: 1.41482378e-05
Iter: 55 loss: 1.3864088e-05
Iter: 56 loss: 1.4334084e-05
Iter: 57 loss: 1.37345733e-05
Iter: 58 loss: 1.35082973e-05
Iter: 59 loss: 1.35654482e-05
Iter: 60 loss: 1.33437698e-05
Iter: 61 loss: 1.3095344e-05
Iter: 62 loss: 1.4093368e-05
Iter: 63 loss: 1.30396193e-05
Iter: 64 loss: 1.28205957e-05
Iter: 65 loss: 1.45273061e-05
Iter: 66 loss: 1.2805267e-05
Iter: 67 loss: 1.2623892e-05
Iter: 68 loss: 1.29244536e-05
Iter: 69 loss: 1.2541148e-05
Iter: 70 loss: 1.23875889e-05
Iter: 71 loss: 1.30857916e-05
Iter: 72 loss: 1.23580603e-05
Iter: 73 loss: 1.21835201e-05
Iter: 74 loss: 1.26949089e-05
Iter: 75 loss: 1.21297417e-05
Iter: 76 loss: 1.20201894e-05
Iter: 77 loss: 1.19819269e-05
Iter: 78 loss: 1.19199976e-05
Iter: 79 loss: 1.17674026e-05
Iter: 80 loss: 1.35114487e-05
Iter: 81 loss: 1.17647596e-05
Iter: 82 loss: 1.16490519e-05
Iter: 83 loss: 1.160584e-05
Iter: 84 loss: 1.15422481e-05
Iter: 85 loss: 1.14135928e-05
Iter: 86 loss: 1.16428746e-05
Iter: 87 loss: 1.13572578e-05
Iter: 88 loss: 1.12445705e-05
Iter: 89 loss: 1.12434936e-05
Iter: 90 loss: 1.11569489e-05
Iter: 91 loss: 1.10976825e-05
Iter: 92 loss: 1.10660258e-05
Iter: 93 loss: 1.0957443e-05
Iter: 94 loss: 1.09904486e-05
Iter: 95 loss: 1.08797112e-05
Iter: 96 loss: 1.07754422e-05
Iter: 97 loss: 1.07755332e-05
Iter: 98 loss: 1.06890784e-05
Iter: 99 loss: 1.07564756e-05
Iter: 100 loss: 1.06365869e-05
Iter: 101 loss: 1.05490608e-05
Iter: 102 loss: 1.04776391e-05
Iter: 103 loss: 1.04519495e-05
Iter: 104 loss: 1.03281791e-05
Iter: 105 loss: 1.09690718e-05
Iter: 106 loss: 1.03082184e-05
Iter: 107 loss: 1.02410449e-05
Iter: 108 loss: 1.02386712e-05
Iter: 109 loss: 1.01793012e-05
Iter: 110 loss: 1.01265887e-05
Iter: 111 loss: 1.01115029e-05
Iter: 112 loss: 1.00332172e-05
Iter: 113 loss: 1.07926444e-05
Iter: 114 loss: 1.00304751e-05
Iter: 115 loss: 9.96569e-06
Iter: 116 loss: 9.91060824e-06
Iter: 117 loss: 9.8925575e-06
Iter: 118 loss: 9.80983077e-06
Iter: 119 loss: 1.04480432e-05
Iter: 120 loss: 9.80363438e-06
Iter: 121 loss: 9.72776434e-06
Iter: 122 loss: 9.99076929e-06
Iter: 123 loss: 9.7075972e-06
Iter: 124 loss: 9.65689833e-06
Iter: 125 loss: 9.66342395e-06
Iter: 126 loss: 9.61841124e-06
Iter: 127 loss: 9.56881922e-06
Iter: 128 loss: 9.56792792e-06
Iter: 129 loss: 9.52869414e-06
Iter: 130 loss: 9.45293505e-06
Iter: 131 loss: 1.10126894e-05
Iter: 132 loss: 9.45235115e-06
Iter: 133 loss: 9.37094501e-06
Iter: 134 loss: 9.49336209e-06
Iter: 135 loss: 9.33183128e-06
Iter: 136 loss: 9.26127541e-06
Iter: 137 loss: 1.01319074e-05
Iter: 138 loss: 9.26053e-06
Iter: 139 loss: 9.20022103e-06
Iter: 140 loss: 9.39400343e-06
Iter: 141 loss: 9.18303158e-06
Iter: 142 loss: 9.13268e-06
Iter: 143 loss: 9.10996278e-06
Iter: 144 loss: 9.08476613e-06
Iter: 145 loss: 9.01461499e-06
Iter: 146 loss: 9.12341056e-06
Iter: 147 loss: 8.98176222e-06
Iter: 148 loss: 8.9278019e-06
Iter: 149 loss: 9.70744259e-06
Iter: 150 loss: 8.92767821e-06
Iter: 151 loss: 8.87979695e-06
Iter: 152 loss: 9.08209e-06
Iter: 153 loss: 8.86979706e-06
Iter: 154 loss: 8.83413304e-06
Iter: 155 loss: 8.81232518e-06
Iter: 156 loss: 8.79771687e-06
Iter: 157 loss: 8.74347279e-06
Iter: 158 loss: 9.07114463e-06
Iter: 159 loss: 8.73644058e-06
Iter: 160 loss: 8.68693951e-06
Iter: 161 loss: 8.79522213e-06
Iter: 162 loss: 8.66777191e-06
Iter: 163 loss: 8.62380512e-06
Iter: 164 loss: 8.73194e-06
Iter: 165 loss: 8.60792e-06
Iter: 166 loss: 8.56027691e-06
Iter: 167 loss: 8.81673623e-06
Iter: 168 loss: 8.55301823e-06
Iter: 169 loss: 8.51844379e-06
Iter: 170 loss: 8.60112777e-06
Iter: 171 loss: 8.50569904e-06
Iter: 172 loss: 8.46364856e-06
Iter: 173 loss: 8.61287663e-06
Iter: 174 loss: 8.45283103e-06
Iter: 175 loss: 8.42317149e-06
Iter: 176 loss: 8.36310574e-06
Iter: 177 loss: 9.48014713e-06
Iter: 178 loss: 8.3623072e-06
Iter: 179 loss: 8.30547106e-06
Iter: 180 loss: 8.55549661e-06
Iter: 181 loss: 8.29402597e-06
Iter: 182 loss: 8.25595271e-06
Iter: 183 loss: 8.25489769e-06
Iter: 184 loss: 8.22530819e-06
Iter: 185 loss: 8.19331308e-06
Iter: 186 loss: 8.18840272e-06
Iter: 187 loss: 8.14287705e-06
Iter: 188 loss: 8.15500607e-06
Iter: 189 loss: 8.10977508e-06
Iter: 190 loss: 8.0604741e-06
Iter: 191 loss: 8.56007318e-06
Iter: 192 loss: 8.05893069e-06
Iter: 193 loss: 8.03120201e-06
Iter: 194 loss: 8.43362795e-06
Iter: 195 loss: 8.0310474e-06
Iter: 196 loss: 8.00593807e-06
Iter: 197 loss: 7.97448229e-06
Iter: 198 loss: 7.97187477e-06
Iter: 199 loss: 7.93183244e-06
Iter: 200 loss: 8.05878e-06
Iter: 201 loss: 7.9202955e-06
Iter: 202 loss: 7.87888439e-06
Iter: 203 loss: 8.11926748e-06
Iter: 204 loss: 7.87354111e-06
Iter: 205 loss: 7.83944142e-06
Iter: 206 loss: 7.87359204e-06
Iter: 207 loss: 7.82014286e-06
Iter: 208 loss: 7.78689719e-06
Iter: 209 loss: 7.99394911e-06
Iter: 210 loss: 7.78308276e-06
Iter: 211 loss: 7.75088301e-06
Iter: 212 loss: 7.8042176e-06
Iter: 213 loss: 7.73628381e-06
Iter: 214 loss: 7.71208579e-06
Iter: 215 loss: 8.01005808e-06
Iter: 216 loss: 7.71178111e-06
Iter: 217 loss: 7.69305188e-06
Iter: 218 loss: 7.66307767e-06
Iter: 219 loss: 7.66277662e-06
Iter: 220 loss: 7.62499849e-06
Iter: 221 loss: 7.63390199e-06
Iter: 222 loss: 7.59723753e-06
Iter: 223 loss: 7.55581686e-06
Iter: 224 loss: 7.87308e-06
Iter: 225 loss: 7.55268684e-06
Iter: 226 loss: 7.52105825e-06
Iter: 227 loss: 7.84217082e-06
Iter: 228 loss: 7.51995094e-06
Iter: 229 loss: 7.49636683e-06
Iter: 230 loss: 7.48427783e-06
Iter: 231 loss: 7.47329614e-06
Iter: 232 loss: 7.44134604e-06
Iter: 233 loss: 7.47869444e-06
Iter: 234 loss: 7.42427164e-06
Iter: 235 loss: 7.39393136e-06
Iter: 236 loss: 7.59873092e-06
Iter: 237 loss: 7.39088864e-06
Iter: 238 loss: 7.35608046e-06
Iter: 239 loss: 7.48723414e-06
Iter: 240 loss: 7.34778814e-06
Iter: 241 loss: 7.32640456e-06
Iter: 242 loss: 7.30197462e-06
Iter: 243 loss: 7.29903422e-06
Iter: 244 loss: 7.27021461e-06
Iter: 245 loss: 7.59443e-06
Iter: 246 loss: 7.26967e-06
Iter: 247 loss: 7.2440339e-06
Iter: 248 loss: 7.3268684e-06
Iter: 249 loss: 7.2368216e-06
Iter: 250 loss: 7.21419246e-06
Iter: 251 loss: 7.23707672e-06
Iter: 252 loss: 7.20158096e-06
Iter: 253 loss: 7.17790454e-06
Iter: 254 loss: 7.46867681e-06
Iter: 255 loss: 7.17767807e-06
Iter: 256 loss: 7.16269278e-06
Iter: 257 loss: 7.14190355e-06
Iter: 258 loss: 7.14089128e-06
Iter: 259 loss: 7.10824042e-06
Iter: 260 loss: 7.2772973e-06
Iter: 261 loss: 7.10284348e-06
Iter: 262 loss: 7.0809474e-06
Iter: 263 loss: 7.05193816e-06
Iter: 264 loss: 7.05024831e-06
Iter: 265 loss: 7.01695217e-06
Iter: 266 loss: 7.18446654e-06
Iter: 267 loss: 7.01138288e-06
Iter: 268 loss: 6.98335816e-06
Iter: 269 loss: 7.30893726e-06
Iter: 270 loss: 6.98289932e-06
Iter: 271 loss: 6.96325787e-06
Iter: 272 loss: 7.02232091e-06
Iter: 273 loss: 6.95736e-06
Iter: 274 loss: 6.93905031e-06
Iter: 275 loss: 6.90801926e-06
Iter: 276 loss: 6.90801426e-06
Iter: 277 loss: 6.8790705e-06
Iter: 278 loss: 7.21321794e-06
Iter: 279 loss: 6.87857028e-06
Iter: 280 loss: 6.8535337e-06
Iter: 281 loss: 7.02826901e-06
Iter: 282 loss: 6.85112536e-06
Iter: 283 loss: 6.83323242e-06
Iter: 284 loss: 6.81746724e-06
Iter: 285 loss: 6.81272377e-06
Iter: 286 loss: 6.78850029e-06
Iter: 287 loss: 6.82268774e-06
Iter: 288 loss: 6.77652679e-06
Iter: 289 loss: 6.7597075e-06
Iter: 290 loss: 6.75804768e-06
Iter: 291 loss: 6.74483499e-06
Iter: 292 loss: 6.73474142e-06
Iter: 293 loss: 6.73066188e-06
Iter: 294 loss: 6.71176622e-06
Iter: 295 loss: 6.87802185e-06
Iter: 296 loss: 6.710864e-06
Iter: 297 loss: 6.69528026e-06
Iter: 298 loss: 6.66850656e-06
Iter: 299 loss: 6.66858705e-06
Iter: 300 loss: 6.64181789e-06
Iter: 301 loss: 6.76820082e-06
Iter: 302 loss: 6.63686751e-06
Iter: 303 loss: 6.61127342e-06
Iter: 304 loss: 6.76861691e-06
Iter: 305 loss: 6.60805972e-06
Iter: 306 loss: 6.58828685e-06
Iter: 307 loss: 6.58965473e-06
Iter: 308 loss: 6.57293913e-06
Iter: 309 loss: 6.54525866e-06
Iter: 310 loss: 6.60015303e-06
Iter: 311 loss: 6.53401821e-06
Iter: 312 loss: 6.51626897e-06
Iter: 313 loss: 6.51600567e-06
Iter: 314 loss: 6.50002585e-06
Iter: 315 loss: 6.48587547e-06
Iter: 316 loss: 6.4816868e-06
Iter: 317 loss: 6.46062472e-06
Iter: 318 loss: 6.47620936e-06
Iter: 319 loss: 6.44753436e-06
Iter: 320 loss: 6.43212161e-06
Iter: 321 loss: 6.43069825e-06
Iter: 322 loss: 6.41627139e-06
Iter: 323 loss: 6.39858536e-06
Iter: 324 loss: 6.39673635e-06
Iter: 325 loss: 6.37419771e-06
Iter: 326 loss: 6.38141182e-06
Iter: 327 loss: 6.35811784e-06
Iter: 328 loss: 6.34605294e-06
Iter: 329 loss: 6.34214439e-06
Iter: 330 loss: 6.3284906e-06
Iter: 331 loss: 6.31699641e-06
Iter: 332 loss: 6.31315606e-06
Iter: 333 loss: 6.2940594e-06
Iter: 334 loss: 6.32588308e-06
Iter: 335 loss: 6.28531234e-06
Iter: 336 loss: 6.26465135e-06
Iter: 337 loss: 6.44036072e-06
Iter: 338 loss: 6.26362635e-06
Iter: 339 loss: 6.24940685e-06
Iter: 340 loss: 6.24204313e-06
Iter: 341 loss: 6.23555843e-06
Iter: 342 loss: 6.21413483e-06
Iter: 343 loss: 6.23147844e-06
Iter: 344 loss: 6.2013969e-06
Iter: 345 loss: 6.17909518e-06
Iter: 346 loss: 6.32247247e-06
Iter: 347 loss: 6.17665137e-06
Iter: 348 loss: 6.15482213e-06
Iter: 349 loss: 6.25321172e-06
Iter: 350 loss: 6.1506471e-06
Iter: 351 loss: 6.13420252e-06
Iter: 352 loss: 6.13968859e-06
Iter: 353 loss: 6.12259646e-06
Iter: 354 loss: 6.10521693e-06
Iter: 355 loss: 6.29878105e-06
Iter: 356 loss: 6.10489542e-06
Iter: 357 loss: 6.08786922e-06
Iter: 358 loss: 6.07495076e-06
Iter: 359 loss: 6.06945969e-06
Iter: 360 loss: 6.04990419e-06
Iter: 361 loss: 6.10179404e-06
Iter: 362 loss: 6.04333172e-06
Iter: 363 loss: 6.02351656e-06
Iter: 364 loss: 6.27867666e-06
Iter: 365 loss: 6.02337741e-06
Iter: 366 loss: 6.01237116e-06
Iter: 367 loss: 5.99802797e-06
Iter: 368 loss: 5.99709801e-06
Iter: 369 loss: 5.9802951e-06
Iter: 370 loss: 6.09465678e-06
Iter: 371 loss: 5.97865028e-06
Iter: 372 loss: 5.95879374e-06
Iter: 373 loss: 6.01446709e-06
Iter: 374 loss: 5.95253869e-06
Iter: 375 loss: 5.93993354e-06
Iter: 376 loss: 5.92320521e-06
Iter: 377 loss: 5.92223932e-06
Iter: 378 loss: 5.90088439e-06
Iter: 379 loss: 6.05900732e-06
Iter: 380 loss: 5.89901629e-06
Iter: 381 loss: 5.87788509e-06
Iter: 382 loss: 5.9586846e-06
Iter: 383 loss: 5.87286377e-06
Iter: 384 loss: 5.85717498e-06
Iter: 385 loss: 5.85463886e-06
Iter: 386 loss: 5.84379359e-06
Iter: 387 loss: 5.82339908e-06
Iter: 388 loss: 5.86044916e-06
Iter: 389 loss: 5.81449103e-06
Iter: 390 loss: 5.79541029e-06
Iter: 391 loss: 5.93839104e-06
Iter: 392 loss: 5.79395601e-06
Iter: 393 loss: 5.77752871e-06
Iter: 394 loss: 5.86027636e-06
Iter: 395 loss: 5.77469746e-06
Iter: 396 loss: 5.7598254e-06
Iter: 397 loss: 5.75572267e-06
Iter: 398 loss: 5.74672094e-06
Iter: 399 loss: 5.7289908e-06
Iter: 400 loss: 5.96055406e-06
Iter: 401 loss: 5.72891759e-06
Iter: 402 loss: 5.71649616e-06
Iter: 403 loss: 5.70772636e-06
Iter: 404 loss: 5.70344855e-06
Iter: 405 loss: 5.68668702e-06
Iter: 406 loss: 5.82712801e-06
Iter: 407 loss: 5.68588257e-06
Iter: 408 loss: 5.67006919e-06
Iter: 409 loss: 5.69908298e-06
Iter: 410 loss: 5.66312974e-06
Iter: 411 loss: 5.65016262e-06
Iter: 412 loss: 5.65806477e-06
Iter: 413 loss: 5.64179209e-06
Iter: 414 loss: 5.62986315e-06
Iter: 415 loss: 5.62971309e-06
Iter: 416 loss: 5.62142213e-06
Iter: 417 loss: 5.60640183e-06
Iter: 418 loss: 5.96993596e-06
Iter: 419 loss: 5.60641911e-06
Iter: 420 loss: 5.58869442e-06
Iter: 421 loss: 5.60407307e-06
Iter: 422 loss: 5.57823796e-06
Iter: 423 loss: 5.55659608e-06
Iter: 424 loss: 5.65141863e-06
Iter: 425 loss: 5.55237693e-06
Iter: 426 loss: 5.53597283e-06
Iter: 427 loss: 5.78894105e-06
Iter: 428 loss: 5.53599602e-06
Iter: 429 loss: 5.52631582e-06
Iter: 430 loss: 5.50203595e-06
Iter: 431 loss: 5.73053694e-06
Iter: 432 loss: 5.49863671e-06
Iter: 433 loss: 5.47465788e-06
Iter: 434 loss: 5.60994386e-06
Iter: 435 loss: 5.47126047e-06
Iter: 436 loss: 5.45508829e-06
Iter: 437 loss: 5.63165304e-06
Iter: 438 loss: 5.45475586e-06
Iter: 439 loss: 5.43967235e-06
Iter: 440 loss: 5.51438643e-06
Iter: 441 loss: 5.43715532e-06
Iter: 442 loss: 5.42652833e-06
Iter: 443 loss: 5.42005091e-06
Iter: 444 loss: 5.4155762e-06
Iter: 445 loss: 5.40208839e-06
Iter: 446 loss: 5.5944065e-06
Iter: 447 loss: 5.40198e-06
Iter: 448 loss: 5.3904796e-06
Iter: 449 loss: 5.38091308e-06
Iter: 450 loss: 5.37757e-06
Iter: 451 loss: 5.3641229e-06
Iter: 452 loss: 5.55506904e-06
Iter: 453 loss: 5.36406742e-06
Iter: 454 loss: 5.3541753e-06
Iter: 455 loss: 5.35362233e-06
Iter: 456 loss: 5.34590845e-06
Iter: 457 loss: 5.33073035e-06
Iter: 458 loss: 5.40952e-06
Iter: 459 loss: 5.3282256e-06
Iter: 460 loss: 5.31747764e-06
Iter: 461 loss: 5.3122717e-06
Iter: 462 loss: 5.30696343e-06
Iter: 463 loss: 5.29246972e-06
Iter: 464 loss: 5.29029148e-06
Iter: 465 loss: 5.28004466e-06
Iter: 466 loss: 5.26000395e-06
Iter: 467 loss: 5.38395852e-06
Iter: 468 loss: 5.25758287e-06
Iter: 469 loss: 5.23942072e-06
Iter: 470 loss: 5.38625545e-06
Iter: 471 loss: 5.23834251e-06
Iter: 472 loss: 5.22489609e-06
Iter: 473 loss: 5.24420466e-06
Iter: 474 loss: 5.21840684e-06
Iter: 475 loss: 5.20521826e-06
Iter: 476 loss: 5.19710829e-06
Iter: 477 loss: 5.19177274e-06
Iter: 478 loss: 5.17574881e-06
Iter: 479 loss: 5.26787062e-06
Iter: 480 loss: 5.17344051e-06
Iter: 481 loss: 5.16095497e-06
Iter: 482 loss: 5.34998344e-06
Iter: 483 loss: 5.16092e-06
Iter: 484 loss: 5.15222e-06
Iter: 485 loss: 5.14070734e-06
Iter: 486 loss: 5.14009025e-06
Iter: 487 loss: 5.12986435e-06
Iter: 488 loss: 5.12981114e-06
Iter: 489 loss: 5.12067618e-06
Iter: 490 loss: 5.11323469e-06
Iter: 491 loss: 5.11065036e-06
Iter: 492 loss: 5.09912797e-06
Iter: 493 loss: 5.20305275e-06
Iter: 494 loss: 5.09862457e-06
Iter: 495 loss: 5.08802532e-06
Iter: 496 loss: 5.10236259e-06
Iter: 497 loss: 5.08284847e-06
Iter: 498 loss: 5.07303048e-06
Iter: 499 loss: 5.08694666e-06
Iter: 500 loss: 5.0683384e-06
Iter: 501 loss: 5.05425533e-06
Iter: 502 loss: 5.06876131e-06
Iter: 503 loss: 5.04635273e-06
Iter: 504 loss: 5.03330557e-06
Iter: 505 loss: 5.04113859e-06
Iter: 506 loss: 5.02486546e-06
Iter: 507 loss: 5.01089471e-06
Iter: 508 loss: 5.10260361e-06
Iter: 509 loss: 5.00929036e-06
Iter: 510 loss: 4.99649968e-06
Iter: 511 loss: 5.07685945e-06
Iter: 512 loss: 4.99514681e-06
Iter: 513 loss: 4.98487589e-06
Iter: 514 loss: 4.98299642e-06
Iter: 515 loss: 4.97624387e-06
Iter: 516 loss: 4.96393386e-06
Iter: 517 loss: 4.96800112e-06
Iter: 518 loss: 4.95521226e-06
Iter: 519 loss: 4.94253754e-06
Iter: 520 loss: 5.11675353e-06
Iter: 521 loss: 4.94256074e-06
Iter: 522 loss: 4.93046218e-06
Iter: 523 loss: 4.96209077e-06
Iter: 524 loss: 4.92635263e-06
Iter: 525 loss: 4.91738865e-06
Iter: 526 loss: 4.92418712e-06
Iter: 527 loss: 4.9121245e-06
Iter: 528 loss: 4.89957301e-06
Iter: 529 loss: 4.98488362e-06
Iter: 530 loss: 4.89852482e-06
Iter: 531 loss: 4.89021204e-06
Iter: 532 loss: 4.89224385e-06
Iter: 533 loss: 4.88419028e-06
Iter: 534 loss: 4.87452644e-06
Iter: 535 loss: 4.97392148e-06
Iter: 536 loss: 4.87428224e-06
Iter: 537 loss: 4.86703539e-06
Iter: 538 loss: 4.85347391e-06
Iter: 539 loss: 5.14485237e-06
Iter: 540 loss: 4.85345072e-06
Iter: 541 loss: 4.83932217e-06
Iter: 542 loss: 4.96436223e-06
Iter: 543 loss: 4.83865e-06
Iter: 544 loss: 4.82798578e-06
Iter: 545 loss: 4.86085855e-06
Iter: 546 loss: 4.82485029e-06
Iter: 547 loss: 4.81458e-06
Iter: 548 loss: 4.8111192e-06
Iter: 549 loss: 4.8051993e-06
Iter: 550 loss: 4.79124219e-06
Iter: 551 loss: 4.86859926e-06
Iter: 552 loss: 4.78944185e-06
Iter: 553 loss: 4.77975345e-06
Iter: 554 loss: 4.89484773e-06
Iter: 555 loss: 4.77967615e-06
Iter: 556 loss: 4.77198637e-06
Iter: 557 loss: 4.75827119e-06
Iter: 558 loss: 5.09805523e-06
Iter: 559 loss: 4.75826801e-06
Iter: 560 loss: 4.74285753e-06
Iter: 561 loss: 4.78820175e-06
Iter: 562 loss: 4.73823229e-06
Iter: 563 loss: 4.72972397e-06
Iter: 564 loss: 4.72953e-06
Iter: 565 loss: 4.72048396e-06
Iter: 566 loss: 4.71670592e-06
Iter: 567 loss: 4.71195835e-06
Iter: 568 loss: 4.7023459e-06
Iter: 569 loss: 4.76156902e-06
Iter: 570 loss: 4.7012154e-06
Iter: 571 loss: 4.69119823e-06
Iter: 572 loss: 4.71968769e-06
Iter: 573 loss: 4.68806229e-06
Iter: 574 loss: 4.68077e-06
Iter: 575 loss: 4.68384496e-06
Iter: 576 loss: 4.67579957e-06
Iter: 577 loss: 4.66500705e-06
Iter: 578 loss: 4.72926786e-06
Iter: 579 loss: 4.66359688e-06
Iter: 580 loss: 4.65660241e-06
Iter: 581 loss: 4.64237746e-06
Iter: 582 loss: 4.90235288e-06
Iter: 583 loss: 4.64215964e-06
Iter: 584 loss: 4.62787921e-06
Iter: 585 loss: 4.7461026e-06
Iter: 586 loss: 4.62699063e-06
Iter: 587 loss: 4.61559694e-06
Iter: 588 loss: 4.70206942e-06
Iter: 589 loss: 4.61478248e-06
Iter: 590 loss: 4.60598676e-06
Iter: 591 loss: 4.59624425e-06
Iter: 592 loss: 4.59496823e-06
Iter: 593 loss: 4.5845909e-06
Iter: 594 loss: 4.58455725e-06
Iter: 595 loss: 4.57568e-06
Iter: 596 loss: 4.59021066e-06
Iter: 597 loss: 4.57158148e-06
Iter: 598 loss: 4.56211137e-06
Iter: 599 loss: 4.56090538e-06
Iter: 600 loss: 4.55409281e-06
Iter: 601 loss: 4.5427696e-06
Iter: 602 loss: 4.59434386e-06
Iter: 603 loss: 4.54061819e-06
Iter: 604 loss: 4.53113807e-06
Iter: 605 loss: 4.66069741e-06
Iter: 606 loss: 4.53112807e-06
Iter: 607 loss: 4.52518361e-06
Iter: 608 loss: 4.51805636e-06
Iter: 609 loss: 4.51733922e-06
Iter: 610 loss: 4.51040341e-06
Iter: 611 loss: 4.51029064e-06
Iter: 612 loss: 4.50505968e-06
Iter: 613 loss: 4.49388244e-06
Iter: 614 loss: 4.66404663e-06
Iter: 615 loss: 4.49352729e-06
Iter: 616 loss: 4.48547826e-06
Iter: 617 loss: 4.48520677e-06
Iter: 618 loss: 4.4779481e-06
Iter: 619 loss: 4.46999411e-06
Iter: 620 loss: 4.46884542e-06
Iter: 621 loss: 4.45798332e-06
Iter: 622 loss: 4.4622293e-06
Iter: 623 loss: 4.45040359e-06
Iter: 624 loss: 4.43757108e-06
Iter: 625 loss: 4.52085897e-06
Iter: 626 loss: 4.43616409e-06
Iter: 627 loss: 4.42621376e-06
Iter: 628 loss: 4.50266907e-06
Iter: 629 loss: 4.42547935e-06
Iter: 630 loss: 4.41726661e-06
Iter: 631 loss: 4.42524197e-06
Iter: 632 loss: 4.41254815e-06
Iter: 633 loss: 4.40326539e-06
Iter: 634 loss: 4.42041528e-06
Iter: 635 loss: 4.39934183e-06
Iter: 636 loss: 4.38828556e-06
Iter: 637 loss: 4.46146259e-06
Iter: 638 loss: 4.38712095e-06
Iter: 639 loss: 4.3795103e-06
Iter: 640 loss: 4.37815834e-06
Iter: 641 loss: 4.37295603e-06
Iter: 642 loss: 4.36273922e-06
Iter: 643 loss: 4.39906034e-06
Iter: 644 loss: 4.36008668e-06
Iter: 645 loss: 4.35017864e-06
Iter: 646 loss: 4.45744035e-06
Iter: 647 loss: 4.34995309e-06
Iter: 648 loss: 4.3444561e-06
Iter: 649 loss: 4.33775313e-06
Iter: 650 loss: 4.33714e-06
Iter: 651 loss: 4.3289574e-06
Iter: 652 loss: 4.32895922e-06
Iter: 653 loss: 4.32318166e-06
Iter: 654 loss: 4.31122908e-06
Iter: 655 loss: 4.52472796e-06
Iter: 656 loss: 4.31104854e-06
Iter: 657 loss: 4.30331056e-06
Iter: 658 loss: 4.30317868e-06
Iter: 659 loss: 4.29565807e-06
Iter: 660 loss: 4.29251122e-06
Iter: 661 loss: 4.28857766e-06
Iter: 662 loss: 4.27886789e-06
Iter: 663 loss: 4.26893348e-06
Iter: 664 loss: 4.26707584e-06
Iter: 665 loss: 4.25389771e-06
Iter: 666 loss: 4.33747118e-06
Iter: 667 loss: 4.25243798e-06
Iter: 668 loss: 4.24034624e-06
Iter: 669 loss: 4.31829903e-06
Iter: 670 loss: 4.23902975e-06
Iter: 671 loss: 4.22951916e-06
Iter: 672 loss: 4.29136617e-06
Iter: 673 loss: 4.22852236e-06
Iter: 674 loss: 4.22199719e-06
Iter: 675 loss: 4.21571895e-06
Iter: 676 loss: 4.21431832e-06
Iter: 677 loss: 4.20167908e-06
Iter: 678 loss: 4.30435102e-06
Iter: 679 loss: 4.20087872e-06
Iter: 680 loss: 4.19437538e-06
Iter: 681 loss: 4.19338539e-06
Iter: 682 loss: 4.18880791e-06
Iter: 683 loss: 4.18089576e-06
Iter: 684 loss: 4.27290024e-06
Iter: 685 loss: 4.18073796e-06
Iter: 686 loss: 4.17327828e-06
Iter: 687 loss: 4.17847787e-06
Iter: 688 loss: 4.16862e-06
Iter: 689 loss: 4.1615026e-06
Iter: 690 loss: 4.17188858e-06
Iter: 691 loss: 4.15798377e-06
Iter: 692 loss: 4.14845317e-06
Iter: 693 loss: 4.19801154e-06
Iter: 694 loss: 4.14703e-06
Iter: 695 loss: 4.140079e-06
Iter: 696 loss: 4.13868702e-06
Iter: 697 loss: 4.13408179e-06
Iter: 698 loss: 4.12589407e-06
Iter: 699 loss: 4.15797513e-06
Iter: 700 loss: 4.124e-06
Iter: 701 loss: 4.11315477e-06
Iter: 702 loss: 4.13264388e-06
Iter: 703 loss: 4.10854e-06
Iter: 704 loss: 4.10002531e-06
Iter: 705 loss: 4.09216227e-06
Iter: 706 loss: 4.09016866e-06
Iter: 707 loss: 4.07826656e-06
Iter: 708 loss: 4.14468e-06
Iter: 709 loss: 4.07651123e-06
Iter: 710 loss: 4.06784511e-06
Iter: 711 loss: 4.14437636e-06
Iter: 712 loss: 4.06746449e-06
Iter: 713 loss: 4.05918945e-06
Iter: 714 loss: 4.07590824e-06
Iter: 715 loss: 4.05583069e-06
Iter: 716 loss: 4.04787e-06
Iter: 717 loss: 4.0568807e-06
Iter: 718 loss: 4.04367393e-06
Iter: 719 loss: 4.03466311e-06
Iter: 720 loss: 4.10904295e-06
Iter: 721 loss: 4.03402191e-06
Iter: 722 loss: 4.02778e-06
Iter: 723 loss: 4.02325804e-06
Iter: 724 loss: 4.02106161e-06
Iter: 725 loss: 4.01396028e-06
Iter: 726 loss: 4.0137129e-06
Iter: 727 loss: 4.00905537e-06
Iter: 728 loss: 4.00214594e-06
Iter: 729 loss: 4.00187037e-06
Iter: 730 loss: 3.99461533e-06
Iter: 731 loss: 4.06276558e-06
Iter: 732 loss: 3.9942779e-06
Iter: 733 loss: 3.98730026e-06
Iter: 734 loss: 3.99412693e-06
Iter: 735 loss: 3.98334714e-06
Iter: 736 loss: 3.97600525e-06
Iter: 737 loss: 3.9730935e-06
Iter: 738 loss: 3.96913765e-06
Iter: 739 loss: 3.96196538e-06
Iter: 740 loss: 4.07351627e-06
Iter: 741 loss: 3.96199357e-06
Iter: 742 loss: 3.95465e-06
Iter: 743 loss: 3.95254438e-06
Iter: 744 loss: 3.94813787e-06
Iter: 745 loss: 3.94004974e-06
Iter: 746 loss: 3.94556355e-06
Iter: 747 loss: 3.93492382e-06
Iter: 748 loss: 3.92468473e-06
Iter: 749 loss: 3.95197185e-06
Iter: 750 loss: 3.92105358e-06
Iter: 751 loss: 3.91202457e-06
Iter: 752 loss: 3.9976585e-06
Iter: 753 loss: 3.91176127e-06
Iter: 754 loss: 3.90398782e-06
Iter: 755 loss: 3.92687798e-06
Iter: 756 loss: 3.90162677e-06
Iter: 757 loss: 3.89507613e-06
Iter: 758 loss: 3.90343757e-06
Iter: 759 loss: 3.89174284e-06
Iter: 760 loss: 3.88453509e-06
Iter: 761 loss: 3.92932179e-06
Iter: 762 loss: 3.8837311e-06
Iter: 763 loss: 3.87719956e-06
Iter: 764 loss: 3.88145781e-06
Iter: 765 loss: 3.87310456e-06
Iter: 766 loss: 3.86601278e-06
Iter: 767 loss: 3.94201606e-06
Iter: 768 loss: 3.86585953e-06
Iter: 769 loss: 3.86075862e-06
Iter: 770 loss: 3.8548169e-06
Iter: 771 loss: 3.85409385e-06
Iter: 772 loss: 3.84784289e-06
Iter: 773 loss: 3.92121137e-06
Iter: 774 loss: 3.84775331e-06
Iter: 775 loss: 3.84118084e-06
Iter: 776 loss: 3.83627048e-06
Iter: 777 loss: 3.83406496e-06
Iter: 778 loss: 3.82702319e-06
Iter: 779 loss: 3.83306588e-06
Iter: 780 loss: 3.82289545e-06
Iter: 781 loss: 3.81619884e-06
Iter: 782 loss: 3.8162766e-06
Iter: 783 loss: 3.81017867e-06
Iter: 784 loss: 3.80762322e-06
Iter: 785 loss: 3.80445545e-06
Iter: 786 loss: 3.79663584e-06
Iter: 787 loss: 3.79807943e-06
Iter: 788 loss: 3.79086259e-06
Iter: 789 loss: 3.78149707e-06
Iter: 790 loss: 3.81050086e-06
Iter: 791 loss: 3.77876631e-06
Iter: 792 loss: 3.7703735e-06
Iter: 793 loss: 3.86296415e-06
Iter: 794 loss: 3.77025117e-06
Iter: 795 loss: 3.76235357e-06
Iter: 796 loss: 3.77360675e-06
Iter: 797 loss: 3.75875925e-06
Iter: 798 loss: 3.75301693e-06
Iter: 799 loss: 3.77328843e-06
Iter: 800 loss: 3.75162517e-06
Iter: 801 loss: 3.74489855e-06
Iter: 802 loss: 3.76056119e-06
Iter: 803 loss: 3.7423456e-06
Iter: 804 loss: 3.73627836e-06
Iter: 805 loss: 3.76820935e-06
Iter: 806 loss: 3.73540433e-06
Iter: 807 loss: 3.72904105e-06
Iter: 808 loss: 3.73094986e-06
Iter: 809 loss: 3.72463342e-06
Iter: 810 loss: 3.71790111e-06
Iter: 811 loss: 3.73644502e-06
Iter: 812 loss: 3.71567762e-06
Iter: 813 loss: 3.70968e-06
Iter: 814 loss: 3.75759851e-06
Iter: 815 loss: 3.70939097e-06
Iter: 816 loss: 3.70404587e-06
Iter: 817 loss: 3.69688973e-06
Iter: 818 loss: 3.69642476e-06
Iter: 819 loss: 3.68808355e-06
Iter: 820 loss: 3.70688804e-06
Iter: 821 loss: 3.68504584e-06
Iter: 822 loss: 3.67791654e-06
Iter: 823 loss: 3.67797929e-06
Iter: 824 loss: 3.67292841e-06
Iter: 825 loss: 3.66487529e-06
Iter: 826 loss: 3.66477389e-06
Iter: 827 loss: 3.65579535e-06
Iter: 828 loss: 3.67335201e-06
Iter: 829 loss: 3.65208712e-06
Iter: 830 loss: 3.64305083e-06
Iter: 831 loss: 3.69574946e-06
Iter: 832 loss: 3.64192874e-06
Iter: 833 loss: 3.6352285e-06
Iter: 834 loss: 3.69057852e-06
Iter: 835 loss: 3.63469235e-06
Iter: 836 loss: 3.62820765e-06
Iter: 837 loss: 3.63517302e-06
Iter: 838 loss: 3.62463652e-06
Iter: 839 loss: 3.61920274e-06
Iter: 840 loss: 3.63667505e-06
Iter: 841 loss: 3.61768321e-06
Iter: 842 loss: 3.61047114e-06
Iter: 843 loss: 3.62945525e-06
Iter: 844 loss: 3.60790864e-06
Iter: 845 loss: 3.60198919e-06
Iter: 846 loss: 3.62059041e-06
Iter: 847 loss: 3.60026706e-06
Iter: 848 loss: 3.59380306e-06
Iter: 849 loss: 3.60072727e-06
Iter: 850 loss: 3.59027422e-06
Iter: 851 loss: 3.58460443e-06
Iter: 852 loss: 3.61017123e-06
Iter: 853 loss: 3.58354691e-06
Iter: 854 loss: 3.57732506e-06
Iter: 855 loss: 3.59185219e-06
Iter: 856 loss: 3.57499721e-06
Iter: 857 loss: 3.57006252e-06
Iter: 858 loss: 3.56364944e-06
Iter: 859 loss: 3.56322494e-06
Iter: 860 loss: 3.55531938e-06
Iter: 861 loss: 3.62701257e-06
Iter: 862 loss: 3.55498287e-06
Iter: 863 loss: 3.54760459e-06
Iter: 864 loss: 3.58046418e-06
Iter: 865 loss: 3.54615e-06
Iter: 866 loss: 3.54011513e-06
Iter: 867 loss: 3.53336054e-06
Iter: 868 loss: 3.53249266e-06
Iter: 869 loss: 3.52436791e-06
Iter: 870 loss: 3.54072927e-06
Iter: 871 loss: 3.52101e-06
Iter: 872 loss: 3.51129233e-06
Iter: 873 loss: 3.55681e-06
Iter: 874 loss: 3.50946334e-06
Iter: 875 loss: 3.50180608e-06
Iter: 876 loss: 3.58346779e-06
Iter: 877 loss: 3.50158143e-06
Iter: 878 loss: 3.49535867e-06
Iter: 879 loss: 3.50376308e-06
Iter: 880 loss: 3.49229822e-06
Iter: 881 loss: 3.48674621e-06
Iter: 882 loss: 3.51066865e-06
Iter: 883 loss: 3.48559433e-06
Iter: 884 loss: 3.47906052e-06
Iter: 885 loss: 3.49271181e-06
Iter: 886 loss: 3.47649029e-06
Iter: 887 loss: 3.47117748e-06
Iter: 888 loss: 3.47691321e-06
Iter: 889 loss: 3.46836714e-06
Iter: 890 loss: 3.46183401e-06
Iter: 891 loss: 3.49451579e-06
Iter: 892 loss: 3.46064871e-06
Iter: 893 loss: 3.45598801e-06
Iter: 894 loss: 3.46403158e-06
Iter: 895 loss: 3.45389662e-06
Iter: 896 loss: 3.44798e-06
Iter: 897 loss: 3.4685263e-06
Iter: 898 loss: 3.44644513e-06
Iter: 899 loss: 3.44154932e-06
Iter: 900 loss: 3.43442548e-06
Iter: 901 loss: 3.43423881e-06
Iter: 902 loss: 3.42655221e-06
Iter: 903 loss: 3.50202913e-06
Iter: 904 loss: 3.42630369e-06
Iter: 905 loss: 3.41943223e-06
Iter: 906 loss: 3.45574836e-06
Iter: 907 loss: 3.41833515e-06
Iter: 908 loss: 3.41331861e-06
Iter: 909 loss: 3.40591851e-06
Iter: 910 loss: 3.40561064e-06
Iter: 911 loss: 3.39660028e-06
Iter: 912 loss: 3.41235341e-06
Iter: 913 loss: 3.39255166e-06
Iter: 914 loss: 3.38466407e-06
Iter: 915 loss: 3.46284241e-06
Iter: 916 loss: 3.38451628e-06
Iter: 917 loss: 3.37929396e-06
Iter: 918 loss: 3.44923615e-06
Iter: 919 loss: 3.37932943e-06
Iter: 920 loss: 3.37516349e-06
Iter: 921 loss: 3.37227198e-06
Iter: 922 loss: 3.37085953e-06
Iter: 923 loss: 3.3655831e-06
Iter: 924 loss: 3.43882266e-06
Iter: 925 loss: 3.36560151e-06
Iter: 926 loss: 3.36167727e-06
Iter: 927 loss: 3.35648588e-06
Iter: 928 loss: 3.35616778e-06
Iter: 929 loss: 3.34987703e-06
Iter: 930 loss: 3.38522955e-06
Iter: 931 loss: 3.349081e-06
Iter: 932 loss: 3.34293554e-06
Iter: 933 loss: 3.3624051e-06
Iter: 934 loss: 3.3411452e-06
Iter: 935 loss: 3.33587968e-06
Iter: 936 loss: 3.34886295e-06
Iter: 937 loss: 3.3341189e-06
Iter: 938 loss: 3.32870877e-06
Iter: 939 loss: 3.34300421e-06
Iter: 940 loss: 3.32683931e-06
Iter: 941 loss: 3.32135937e-06
Iter: 942 loss: 3.31859019e-06
Iter: 943 loss: 3.31600131e-06
Iter: 944 loss: 3.30873127e-06
Iter: 945 loss: 3.33730827e-06
Iter: 946 loss: 3.30708099e-06
Iter: 947 loss: 3.30034709e-06
Iter: 948 loss: 3.3729898e-06
Iter: 949 loss: 3.3002168e-06
Iter: 950 loss: 3.29569866e-06
Iter: 951 loss: 3.29086606e-06
Iter: 952 loss: 3.2900773e-06
Iter: 953 loss: 3.28265651e-06
Iter: 954 loss: 3.29358818e-06
Iter: 955 loss: 3.2791e-06
Iter: 956 loss: 3.27155976e-06
Iter: 957 loss: 3.30230023e-06
Iter: 958 loss: 3.26981763e-06
Iter: 959 loss: 3.26406644e-06
Iter: 960 loss: 3.35263758e-06
Iter: 961 loss: 3.26401232e-06
Iter: 962 loss: 3.25898964e-06
Iter: 963 loss: 3.26182931e-06
Iter: 964 loss: 3.25571773e-06
Iter: 965 loss: 3.2512587e-06
Iter: 966 loss: 3.28399096e-06
Iter: 967 loss: 3.2507769e-06
Iter: 968 loss: 3.24627808e-06
Iter: 969 loss: 3.24120333e-06
Iter: 970 loss: 3.24039229e-06
Iter: 971 loss: 3.23410904e-06
Iter: 972 loss: 3.25754468e-06
Iter: 973 loss: 3.23252925e-06
Iter: 974 loss: 3.22638675e-06
Iter: 975 loss: 3.2814537e-06
Iter: 976 loss: 3.22609867e-06
Iter: 977 loss: 3.22230153e-06
Iter: 978 loss: 3.21771654e-06
Iter: 979 loss: 3.21716811e-06
Iter: 980 loss: 3.21046537e-06
Iter: 981 loss: 3.26365944e-06
Iter: 982 loss: 3.21008815e-06
Iter: 983 loss: 3.20472373e-06
Iter: 984 loss: 3.20651725e-06
Iter: 985 loss: 3.20088702e-06
Iter: 986 loss: 3.19466949e-06
Iter: 987 loss: 3.19950459e-06
Iter: 988 loss: 3.1909085e-06
Iter: 989 loss: 3.18459297e-06
Iter: 990 loss: 3.18455432e-06
Iter: 991 loss: 3.18082084e-06
Iter: 992 loss: 3.17402578e-06
Iter: 993 loss: 3.34046695e-06
Iter: 994 loss: 3.17409877e-06
Iter: 995 loss: 3.16654928e-06
Iter: 996 loss: 3.17669537e-06
Iter: 997 loss: 3.16263549e-06
Iter: 998 loss: 3.15536363e-06
Iter: 999 loss: 3.22547157e-06
Iter: 1000 loss: 3.15507e-06
Iter: 1001 loss: 3.14949875e-06
Iter: 1002 loss: 3.19489118e-06
Iter: 1003 loss: 3.14913541e-06
Iter: 1004 loss: 3.14344948e-06
Iter: 1005 loss: 3.14756289e-06
Iter: 1006 loss: 3.1399486e-06
Iter: 1007 loss: 3.1352306e-06
Iter: 1008 loss: 3.15991065e-06
Iter: 1009 loss: 3.13457667e-06
Iter: 1010 loss: 3.12936277e-06
Iter: 1011 loss: 3.13159535e-06
Iter: 1012 loss: 3.12588554e-06
Iter: 1013 loss: 3.12087332e-06
Iter: 1014 loss: 3.12529983e-06
Iter: 1015 loss: 3.11793337e-06
Iter: 1016 loss: 3.11201325e-06
Iter: 1017 loss: 3.18361481e-06
Iter: 1018 loss: 3.11195981e-06
Iter: 1019 loss: 3.10822952e-06
Iter: 1020 loss: 3.10417863e-06
Iter: 1021 loss: 3.10361293e-06
Iter: 1022 loss: 3.09787902e-06
Iter: 1023 loss: 3.13620922e-06
Iter: 1024 loss: 3.09732468e-06
Iter: 1025 loss: 3.09195366e-06
Iter: 1026 loss: 3.09951338e-06
Iter: 1027 loss: 3.08931749e-06
Iter: 1028 loss: 3.08383142e-06
Iter: 1029 loss: 3.08398376e-06
Iter: 1030 loss: 3.07953405e-06
Iter: 1031 loss: 3.07481e-06
Iter: 1032 loss: 3.07455275e-06
Iter: 1033 loss: 3.07098776e-06
Iter: 1034 loss: 3.06593824e-06
Iter: 1035 loss: 3.06578795e-06
Iter: 1036 loss: 3.05993262e-06
Iter: 1037 loss: 3.0658482e-06
Iter: 1038 loss: 3.05669937e-06
Iter: 1039 loss: 3.05007211e-06
Iter: 1040 loss: 3.08059043e-06
Iter: 1041 loss: 3.04890227e-06
Iter: 1042 loss: 3.04405467e-06
Iter: 1043 loss: 3.0439694e-06
Iter: 1044 loss: 3.04115133e-06
Iter: 1045 loss: 3.0356e-06
Iter: 1046 loss: 3.142693e-06
Iter: 1047 loss: 3.03553952e-06
Iter: 1048 loss: 3.02942703e-06
Iter: 1049 loss: 3.10504765e-06
Iter: 1050 loss: 3.02933e-06
Iter: 1051 loss: 3.02524859e-06
Iter: 1052 loss: 3.02517356e-06
Iter: 1053 loss: 3.02199351e-06
Iter: 1054 loss: 3.01722048e-06
Iter: 1055 loss: 3.0355618e-06
Iter: 1056 loss: 3.01603313e-06
Iter: 1057 loss: 3.01112414e-06
Iter: 1058 loss: 3.03754132e-06
Iter: 1059 loss: 3.01043383e-06
Iter: 1060 loss: 3.0065994e-06
Iter: 1061 loss: 3.00417651e-06
Iter: 1062 loss: 3.00287547e-06
Iter: 1063 loss: 2.99739577e-06
Iter: 1064 loss: 3.02944136e-06
Iter: 1065 loss: 2.99665044e-06
Iter: 1066 loss: 2.99121962e-06
Iter: 1067 loss: 2.99692238e-06
Iter: 1068 loss: 2.98830855e-06
Iter: 1069 loss: 2.98321652e-06
Iter: 1070 loss: 2.99582348e-06
Iter: 1071 loss: 2.98141026e-06
Iter: 1072 loss: 2.97539918e-06
Iter: 1073 loss: 3.01189652e-06
Iter: 1074 loss: 2.97461406e-06
Iter: 1075 loss: 2.97042584e-06
Iter: 1076 loss: 2.96751296e-06
Iter: 1077 loss: 2.96591475e-06
Iter: 1078 loss: 2.9596e-06
Iter: 1079 loss: 2.97014913e-06
Iter: 1080 loss: 2.95661243e-06
Iter: 1081 loss: 2.95342807e-06
Iter: 1082 loss: 2.9527439e-06
Iter: 1083 loss: 2.94941e-06
Iter: 1084 loss: 2.946839e-06
Iter: 1085 loss: 2.94582287e-06
Iter: 1086 loss: 2.94141046e-06
Iter: 1087 loss: 2.94812162e-06
Iter: 1088 loss: 2.93939979e-06
Iter: 1089 loss: 2.93386165e-06
Iter: 1090 loss: 2.96820599e-06
Iter: 1091 loss: 2.93318385e-06
Iter: 1092 loss: 2.92846744e-06
Iter: 1093 loss: 2.92403979e-06
Iter: 1094 loss: 2.92291497e-06
Iter: 1095 loss: 2.9192e-06
Iter: 1096 loss: 2.9189373e-06
Iter: 1097 loss: 2.91557308e-06
Iter: 1098 loss: 2.90992466e-06
Iter: 1099 loss: 2.90989692e-06
Iter: 1100 loss: 2.90459911e-06
Iter: 1101 loss: 2.92287064e-06
Iter: 1102 loss: 2.90313301e-06
Iter: 1103 loss: 2.89850618e-06
Iter: 1104 loss: 2.94104939e-06
Iter: 1105 loss: 2.89833179e-06
Iter: 1106 loss: 2.89417676e-06
Iter: 1107 loss: 2.89234231e-06
Iter: 1108 loss: 2.89010495e-06
Iter: 1109 loss: 2.88463684e-06
Iter: 1110 loss: 2.91669653e-06
Iter: 1111 loss: 2.8838765e-06
Iter: 1112 loss: 2.87816283e-06
Iter: 1113 loss: 2.89001582e-06
Iter: 1114 loss: 2.87581224e-06
Iter: 1115 loss: 2.87140756e-06
Iter: 1116 loss: 2.86983322e-06
Iter: 1117 loss: 2.86739396e-06
Iter: 1118 loss: 2.86090744e-06
Iter: 1119 loss: 2.8837851e-06
Iter: 1120 loss: 2.85920555e-06
Iter: 1121 loss: 2.85588021e-06
Iter: 1122 loss: 2.85531905e-06
Iter: 1123 loss: 2.85298552e-06
Iter: 1124 loss: 2.84752196e-06
Iter: 1125 loss: 2.9144303e-06
Iter: 1126 loss: 2.84710973e-06
Iter: 1127 loss: 2.84074213e-06
Iter: 1128 loss: 2.87429748e-06
Iter: 1129 loss: 2.83967165e-06
Iter: 1130 loss: 2.83607028e-06
Iter: 1131 loss: 2.89147e-06
Iter: 1132 loss: 2.83607505e-06
Iter: 1133 loss: 2.83268059e-06
Iter: 1134 loss: 2.82679957e-06
Iter: 1135 loss: 2.82680958e-06
Iter: 1136 loss: 2.82201017e-06
Iter: 1137 loss: 2.88291085e-06
Iter: 1138 loss: 2.82197448e-06
Iter: 1139 loss: 2.81769303e-06
Iter: 1140 loss: 2.82612132e-06
Iter: 1141 loss: 2.8159443e-06
Iter: 1142 loss: 2.81162966e-06
Iter: 1143 loss: 2.80517315e-06
Iter: 1144 loss: 2.80509403e-06
Iter: 1145 loss: 2.79747428e-06
Iter: 1146 loss: 2.84262455e-06
Iter: 1147 loss: 2.79648157e-06
Iter: 1148 loss: 2.7918e-06
Iter: 1149 loss: 2.82603037e-06
Iter: 1150 loss: 2.7914773e-06
Iter: 1151 loss: 2.7867809e-06
Iter: 1152 loss: 2.8086788e-06
Iter: 1153 loss: 2.7859769e-06
Iter: 1154 loss: 2.78228481e-06
Iter: 1155 loss: 2.77988534e-06
Iter: 1156 loss: 2.77847585e-06
Iter: 1157 loss: 2.77383651e-06
Iter: 1158 loss: 2.79867982e-06
Iter: 1159 loss: 2.77310824e-06
Iter: 1160 loss: 2.76918e-06
Iter: 1161 loss: 2.81340749e-06
Iter: 1162 loss: 2.76905121e-06
Iter: 1163 loss: 2.76608171e-06
Iter: 1164 loss: 2.76376159e-06
Iter: 1165 loss: 2.76276455e-06
Iter: 1166 loss: 2.7576707e-06
Iter: 1167 loss: 2.77856157e-06
Iter: 1168 loss: 2.75658977e-06
Iter: 1169 loss: 2.75191201e-06
Iter: 1170 loss: 2.74841909e-06
Iter: 1171 loss: 2.74691433e-06
Iter: 1172 loss: 2.74224749e-06
Iter: 1173 loss: 2.81360644e-06
Iter: 1174 loss: 2.74221202e-06
Iter: 1175 loss: 2.73769956e-06
Iter: 1176 loss: 2.74429499e-06
Iter: 1177 loss: 2.73545356e-06
Iter: 1178 loss: 2.73170235e-06
Iter: 1179 loss: 2.72880015e-06
Iter: 1180 loss: 2.72757507e-06
Iter: 1181 loss: 2.72423631e-06
Iter: 1182 loss: 2.72396437e-06
Iter: 1183 loss: 2.7210956e-06
Iter: 1184 loss: 2.71601493e-06
Iter: 1185 loss: 2.84084535e-06
Iter: 1186 loss: 2.71602471e-06
Iter: 1187 loss: 2.71067847e-06
Iter: 1188 loss: 2.72724446e-06
Iter: 1189 loss: 2.70915439e-06
Iter: 1190 loss: 2.7050437e-06
Iter: 1191 loss: 2.74961576e-06
Iter: 1192 loss: 2.70493888e-06
Iter: 1193 loss: 2.70087935e-06
Iter: 1194 loss: 2.70729765e-06
Iter: 1195 loss: 2.69897464e-06
Iter: 1196 loss: 2.69576913e-06
Iter: 1197 loss: 2.69462384e-06
Iter: 1198 loss: 2.69272823e-06
Iter: 1199 loss: 2.68924305e-06
Iter: 1200 loss: 2.68915664e-06
Iter: 1201 loss: 2.68624285e-06
Iter: 1202 loss: 2.68200961e-06
Iter: 1203 loss: 2.68190615e-06
Iter: 1204 loss: 2.67759833e-06
Iter: 1205 loss: 2.69251655e-06
Iter: 1206 loss: 2.67656287e-06
Iter: 1207 loss: 2.67177461e-06
Iter: 1208 loss: 2.69008774e-06
Iter: 1209 loss: 2.6705934e-06
Iter: 1210 loss: 2.66705229e-06
Iter: 1211 loss: 2.67488167e-06
Iter: 1212 loss: 2.66576171e-06
Iter: 1213 loss: 2.66181928e-06
Iter: 1214 loss: 2.68277358e-06
Iter: 1215 loss: 2.66131929e-06
Iter: 1216 loss: 2.65733797e-06
Iter: 1217 loss: 2.65581639e-06
Iter: 1218 loss: 2.65361359e-06
Iter: 1219 loss: 2.64899177e-06
Iter: 1220 loss: 2.65763174e-06
Iter: 1221 loss: 2.64707069e-06
Iter: 1222 loss: 2.6428238e-06
Iter: 1223 loss: 2.69604789e-06
Iter: 1224 loss: 2.64277833e-06
Iter: 1225 loss: 2.63924221e-06
Iter: 1226 loss: 2.63662309e-06
Iter: 1227 loss: 2.63541597e-06
Iter: 1228 loss: 2.63101356e-06
Iter: 1229 loss: 2.63445e-06
Iter: 1230 loss: 2.62827461e-06
Iter: 1231 loss: 2.62307344e-06
Iter: 1232 loss: 2.6578914e-06
Iter: 1233 loss: 2.62255e-06
Iter: 1234 loss: 2.61833702e-06
Iter: 1235 loss: 2.65744211e-06
Iter: 1236 loss: 2.61814193e-06
Iter: 1237 loss: 2.61492733e-06
Iter: 1238 loss: 2.61435207e-06
Iter: 1239 loss: 2.61218793e-06
Iter: 1240 loss: 2.60883758e-06
Iter: 1241 loss: 2.66190273e-06
Iter: 1242 loss: 2.60884417e-06
Iter: 1243 loss: 2.60617526e-06
Iter: 1244 loss: 2.60141269e-06
Iter: 1245 loss: 2.60142519e-06
Iter: 1246 loss: 2.59667831e-06
Iter: 1247 loss: 2.60624483e-06
Iter: 1248 loss: 2.59470971e-06
Iter: 1249 loss: 2.58972386e-06
Iter: 1250 loss: 2.60156958e-06
Iter: 1251 loss: 2.58793807e-06
Iter: 1252 loss: 2.58395607e-06
Iter: 1253 loss: 2.58397358e-06
Iter: 1254 loss: 2.58106184e-06
Iter: 1255 loss: 2.57909096e-06
Iter: 1256 loss: 2.5780987e-06
Iter: 1257 loss: 2.57365855e-06
Iter: 1258 loss: 2.61326068e-06
Iter: 1259 loss: 2.57348029e-06
Iter: 1260 loss: 2.57064585e-06
Iter: 1261 loss: 2.56512067e-06
Iter: 1262 loss: 2.67656083e-06
Iter: 1263 loss: 2.56512453e-06
Iter: 1264 loss: 2.56146609e-06
Iter: 1265 loss: 2.56136559e-06
Iter: 1266 loss: 2.55798909e-06
Iter: 1267 loss: 2.56324097e-06
Iter: 1268 loss: 2.55648729e-06
Iter: 1269 loss: 2.55330951e-06
Iter: 1270 loss: 2.55086138e-06
Iter: 1271 loss: 2.54991369e-06
Iter: 1272 loss: 2.54612041e-06
Iter: 1273 loss: 2.54605607e-06
Iter: 1274 loss: 2.54265365e-06
Iter: 1275 loss: 2.54590418e-06
Iter: 1276 loss: 2.5406498e-06
Iter: 1277 loss: 2.53773123e-06
Iter: 1278 loss: 2.5575132e-06
Iter: 1279 loss: 2.53736425e-06
Iter: 1280 loss: 2.53412418e-06
Iter: 1281 loss: 2.53249709e-06
Iter: 1282 loss: 2.53090093e-06
Iter: 1283 loss: 2.52723135e-06
Iter: 1284 loss: 2.5273348e-06
Iter: 1285 loss: 2.52430618e-06
Iter: 1286 loss: 2.5197096e-06
Iter: 1287 loss: 2.54793167e-06
Iter: 1288 loss: 2.51925712e-06
Iter: 1289 loss: 2.51588722e-06
Iter: 1290 loss: 2.54717952e-06
Iter: 1291 loss: 2.51579468e-06
Iter: 1292 loss: 2.5125787e-06
Iter: 1293 loss: 2.512847e-06
Iter: 1294 loss: 2.51003439e-06
Iter: 1295 loss: 2.50623179e-06
Iter: 1296 loss: 2.51999381e-06
Iter: 1297 loss: 2.50523453e-06
Iter: 1298 loss: 2.50122525e-06
Iter: 1299 loss: 2.50888525e-06
Iter: 1300 loss: 2.49945833e-06
Iter: 1301 loss: 2.49560981e-06
Iter: 1302 loss: 2.49666209e-06
Iter: 1303 loss: 2.4928e-06
Iter: 1304 loss: 2.48886818e-06
Iter: 1305 loss: 2.48887909e-06
Iter: 1306 loss: 2.48607398e-06
Iter: 1307 loss: 2.48330389e-06
Iter: 1308 loss: 2.48264723e-06
Iter: 1309 loss: 2.47829576e-06
Iter: 1310 loss: 2.48598258e-06
Iter: 1311 loss: 2.47642538e-06
Iter: 1312 loss: 2.47278012e-06
Iter: 1313 loss: 2.52237282e-06
Iter: 1314 loss: 2.4727392e-06
Iter: 1315 loss: 2.46958371e-06
Iter: 1316 loss: 2.47798198e-06
Iter: 1317 loss: 2.46848413e-06
Iter: 1318 loss: 2.46562558e-06
Iter: 1319 loss: 2.46948503e-06
Iter: 1320 loss: 2.46417676e-06
Iter: 1321 loss: 2.4598985e-06
Iter: 1322 loss: 2.46933496e-06
Iter: 1323 loss: 2.45829551e-06
Iter: 1324 loss: 2.45518027e-06
Iter: 1325 loss: 2.45182423e-06
Iter: 1326 loss: 2.4512135e-06
Iter: 1327 loss: 2.44569583e-06
Iter: 1328 loss: 2.45206047e-06
Iter: 1329 loss: 2.44272269e-06
Iter: 1330 loss: 2.43708541e-06
Iter: 1331 loss: 2.47985895e-06
Iter: 1332 loss: 2.43658224e-06
Iter: 1333 loss: 2.43294289e-06
Iter: 1334 loss: 2.46285299e-06
Iter: 1335 loss: 2.43263412e-06
Iter: 1336 loss: 2.42868964e-06
Iter: 1337 loss: 2.43418572e-06
Iter: 1338 loss: 2.42674469e-06
Iter: 1339 loss: 2.42359215e-06
Iter: 1340 loss: 2.446184e-06
Iter: 1341 loss: 2.42334863e-06
Iter: 1342 loss: 2.4200981e-06
Iter: 1343 loss: 2.41965154e-06
Iter: 1344 loss: 2.41737257e-06
Iter: 1345 loss: 2.4137953e-06
Iter: 1346 loss: 2.41743624e-06
Iter: 1347 loss: 2.41169573e-06
Iter: 1348 loss: 2.40757572e-06
Iter: 1349 loss: 2.45775823e-06
Iter: 1350 loss: 2.40751797e-06
Iter: 1351 loss: 2.4048818e-06
Iter: 1352 loss: 2.40260829e-06
Iter: 1353 loss: 2.40196255e-06
Iter: 1354 loss: 2.39831684e-06
Iter: 1355 loss: 2.42289e-06
Iter: 1356 loss: 2.39793781e-06
Iter: 1357 loss: 2.39426777e-06
Iter: 1358 loss: 2.41150292e-06
Iter: 1359 loss: 2.39353358e-06
Iter: 1360 loss: 2.39109886e-06
Iter: 1361 loss: 2.38881967e-06
Iter: 1362 loss: 2.38825942e-06
Iter: 1363 loss: 2.3845555e-06
Iter: 1364 loss: 2.40367444e-06
Iter: 1365 loss: 2.38396569e-06
Iter: 1366 loss: 2.38029179e-06
Iter: 1367 loss: 2.39863698e-06
Iter: 1368 loss: 2.37967015e-06
Iter: 1369 loss: 2.37708491e-06
Iter: 1370 loss: 2.37568338e-06
Iter: 1371 loss: 2.37460927e-06
Iter: 1372 loss: 2.37069958e-06
Iter: 1373 loss: 2.37316385e-06
Iter: 1374 loss: 2.36830147e-06
Iter: 1375 loss: 2.36382107e-06
Iter: 1376 loss: 2.38525399e-06
Iter: 1377 loss: 2.36302913e-06
Iter: 1378 loss: 2.35948187e-06
Iter: 1379 loss: 2.40384657e-06
Iter: 1380 loss: 2.35949028e-06
Iter: 1381 loss: 2.3566181e-06
Iter: 1382 loss: 2.35376274e-06
Iter: 1383 loss: 2.3531793e-06
Iter: 1384 loss: 2.34963704e-06
Iter: 1385 loss: 2.34966728e-06
Iter: 1386 loss: 2.34676349e-06
Iter: 1387 loss: 2.3476357e-06
Iter: 1388 loss: 2.34471827e-06
Iter: 1389 loss: 2.34175241e-06
Iter: 1390 loss: 2.35114794e-06
Iter: 1391 loss: 2.34096865e-06
Iter: 1392 loss: 2.33780565e-06
Iter: 1393 loss: 2.35445941e-06
Iter: 1394 loss: 2.33738e-06
Iter: 1395 loss: 2.33492938e-06
Iter: 1396 loss: 2.33157334e-06
Iter: 1397 loss: 2.33140486e-06
Iter: 1398 loss: 2.32895e-06
Iter: 1399 loss: 2.32863044e-06
Iter: 1400 loss: 2.32635512e-06
Iter: 1401 loss: 2.32484172e-06
Iter: 1402 loss: 2.32391221e-06
Iter: 1403 loss: 2.3208338e-06
Iter: 1404 loss: 2.31919012e-06
Iter: 1405 loss: 2.3178045e-06
Iter: 1406 loss: 2.31463446e-06
Iter: 1407 loss: 2.31462627e-06
Iter: 1408 loss: 2.31149625e-06
Iter: 1409 loss: 2.31184754e-06
Iter: 1410 loss: 2.30915066e-06
Iter: 1411 loss: 2.30586875e-06
Iter: 1412 loss: 2.30434262e-06
Iter: 1413 loss: 2.30272099e-06
Iter: 1414 loss: 2.29748184e-06
Iter: 1415 loss: 2.31126546e-06
Iter: 1416 loss: 2.29569127e-06
Iter: 1417 loss: 2.29147167e-06
Iter: 1418 loss: 2.32547677e-06
Iter: 1419 loss: 2.29126067e-06
Iter: 1420 loss: 2.28772114e-06
Iter: 1421 loss: 2.32069078e-06
Iter: 1422 loss: 2.28752538e-06
Iter: 1423 loss: 2.28512181e-06
Iter: 1424 loss: 2.2847089e-06
Iter: 1425 loss: 2.28311933e-06
Iter: 1426 loss: 2.27997452e-06
Iter: 1427 loss: 2.29308489e-06
Iter: 1428 loss: 2.27926944e-06
Iter: 1429 loss: 2.27606188e-06
Iter: 1430 loss: 2.2863851e-06
Iter: 1431 loss: 2.27508622e-06
Iter: 1432 loss: 2.27226565e-06
Iter: 1433 loss: 2.27365831e-06
Iter: 1434 loss: 2.27038413e-06
Iter: 1435 loss: 2.267122e-06
Iter: 1436 loss: 2.31090144e-06
Iter: 1437 loss: 2.26710677e-06
Iter: 1438 loss: 2.26532165e-06
Iter: 1439 loss: 2.2641384e-06
Iter: 1440 loss: 2.26347424e-06
Iter: 1441 loss: 2.25998656e-06
Iter: 1442 loss: 2.26708698e-06
Iter: 1443 loss: 2.25852068e-06
Iter: 1444 loss: 2.2551651e-06
Iter: 1445 loss: 2.25411395e-06
Iter: 1446 loss: 2.25215172e-06
Iter: 1447 loss: 2.24783162e-06
Iter: 1448 loss: 2.27148189e-06
Iter: 1449 loss: 2.24717337e-06
Iter: 1450 loss: 2.2438262e-06
Iter: 1451 loss: 2.28322824e-06
Iter: 1452 loss: 2.24375049e-06
Iter: 1453 loss: 2.24146811e-06
Iter: 1454 loss: 2.23839697e-06
Iter: 1455 loss: 2.23821644e-06
Iter: 1456 loss: 2.23421148e-06
Iter: 1457 loss: 2.23855682e-06
Iter: 1458 loss: 2.23198231e-06
Iter: 1459 loss: 2.22801555e-06
Iter: 1460 loss: 2.24039786e-06
Iter: 1461 loss: 2.22678909e-06
Iter: 1462 loss: 2.22353037e-06
Iter: 1463 loss: 2.22345943e-06
Iter: 1464 loss: 2.22110634e-06
Iter: 1465 loss: 2.22078575e-06
Iter: 1466 loss: 2.21903088e-06
Iter: 1467 loss: 2.2159029e-06
Iter: 1468 loss: 2.21639584e-06
Iter: 1469 loss: 2.21344544e-06
Iter: 1470 loss: 2.21099435e-06
Iter: 1471 loss: 2.21071969e-06
Iter: 1472 loss: 2.20883089e-06
Iter: 1473 loss: 2.20565e-06
Iter: 1474 loss: 2.20563606e-06
Iter: 1475 loss: 2.20228731e-06
Iter: 1476 loss: 2.22648919e-06
Iter: 1477 loss: 2.20198763e-06
Iter: 1478 loss: 2.19879871e-06
Iter: 1479 loss: 2.20061384e-06
Iter: 1480 loss: 2.19663616e-06
Iter: 1481 loss: 2.19371213e-06
Iter: 1482 loss: 2.20756e-06
Iter: 1483 loss: 2.19308959e-06
Iter: 1484 loss: 2.18959963e-06
Iter: 1485 loss: 2.19599815e-06
Iter: 1486 loss: 2.18814375e-06
Iter: 1487 loss: 2.18510945e-06
Iter: 1488 loss: 2.18492733e-06
Iter: 1489 loss: 2.18268292e-06
Iter: 1490 loss: 2.17886145e-06
Iter: 1491 loss: 2.19830235e-06
Iter: 1492 loss: 2.1781882e-06
Iter: 1493 loss: 2.17497563e-06
Iter: 1494 loss: 2.20264201e-06
Iter: 1495 loss: 2.17489878e-06
Iter: 1496 loss: 2.17206707e-06
Iter: 1497 loss: 2.17124716e-06
Iter: 1498 loss: 2.16949093e-06
Iter: 1499 loss: 2.16603439e-06
Iter: 1500 loss: 2.16951094e-06
Iter: 1501 loss: 2.16403464e-06
Iter: 1502 loss: 2.16103285e-06
Iter: 1503 loss: 2.19529602e-06
Iter: 1504 loss: 2.16105354e-06
Iter: 1505 loss: 2.1581061e-06
Iter: 1506 loss: 2.16696276e-06
Iter: 1507 loss: 2.15727414e-06
Iter: 1508 loss: 2.15484351e-06
Iter: 1509 loss: 2.1533674e-06
Iter: 1510 loss: 2.15227828e-06
Iter: 1511 loss: 2.14973488e-06
Iter: 1512 loss: 2.1496603e-06
Iter: 1513 loss: 2.14778106e-06
Iter: 1514 loss: 2.1443243e-06
Iter: 1515 loss: 2.21956134e-06
Iter: 1516 loss: 2.14428974e-06
Iter: 1517 loss: 2.14073134e-06
Iter: 1518 loss: 2.14237707e-06
Iter: 1519 loss: 2.13829662e-06
Iter: 1520 loss: 2.13506519e-06
Iter: 1521 loss: 2.13509156e-06
Iter: 1522 loss: 2.13191265e-06
Iter: 1523 loss: 2.13297676e-06
Iter: 1524 loss: 2.12973578e-06
Iter: 1525 loss: 2.12647819e-06
Iter: 1526 loss: 2.13558224e-06
Iter: 1527 loss: 2.12540272e-06
Iter: 1528 loss: 2.12242e-06
Iter: 1529 loss: 2.15251748e-06
Iter: 1530 loss: 2.12231589e-06
Iter: 1531 loss: 2.12012901e-06
Iter: 1532 loss: 2.11806469e-06
Iter: 1533 loss: 2.11752376e-06
Iter: 1534 loss: 2.11419388e-06
Iter: 1535 loss: 2.12003147e-06
Iter: 1536 loss: 2.11273573e-06
Iter: 1537 loss: 2.10889516e-06
Iter: 1538 loss: 2.12023747e-06
Iter: 1539 loss: 2.10775102e-06
Iter: 1540 loss: 2.10392636e-06
Iter: 1541 loss: 2.15025102e-06
Iter: 1542 loss: 2.10392886e-06
Iter: 1543 loss: 2.1017654e-06
Iter: 1544 loss: 2.10100688e-06
Iter: 1545 loss: 2.09985728e-06
Iter: 1546 loss: 2.09725067e-06
Iter: 1547 loss: 2.13374e-06
Iter: 1548 loss: 2.0972343e-06
Iter: 1549 loss: 2.09551e-06
Iter: 1550 loss: 2.09346535e-06
Iter: 1551 loss: 2.09325663e-06
Iter: 1552 loss: 2.09065797e-06
Iter: 1553 loss: 2.11836618e-06
Iter: 1554 loss: 2.09062705e-06
Iter: 1555 loss: 2.08821234e-06
Iter: 1556 loss: 2.0840564e-06
Iter: 1557 loss: 2.08404344e-06
Iter: 1558 loss: 2.08033839e-06
Iter: 1559 loss: 2.08983101e-06
Iter: 1560 loss: 2.07903759e-06
Iter: 1561 loss: 2.07557378e-06
Iter: 1562 loss: 2.09497057e-06
Iter: 1563 loss: 2.07507264e-06
Iter: 1564 loss: 2.07145808e-06
Iter: 1565 loss: 2.09047425e-06
Iter: 1566 loss: 2.07089806e-06
Iter: 1567 loss: 2.06843447e-06
Iter: 1568 loss: 2.07056496e-06
Iter: 1569 loss: 2.06694131e-06
Iter: 1570 loss: 2.06381424e-06
Iter: 1571 loss: 2.079448e-06
Iter: 1572 loss: 2.06325581e-06
Iter: 1573 loss: 2.06045434e-06
Iter: 1574 loss: 2.0615114e-06
Iter: 1575 loss: 2.05845117e-06
Iter: 1576 loss: 2.05533979e-06
Iter: 1577 loss: 2.06333334e-06
Iter: 1578 loss: 2.05416927e-06
Iter: 1579 loss: 2.05180277e-06
Iter: 1580 loss: 2.0889006e-06
Iter: 1581 loss: 2.05177571e-06
Iter: 1582 loss: 2.04953403e-06
Iter: 1583 loss: 2.04785374e-06
Iter: 1584 loss: 2.04718367e-06
Iter: 1585 loss: 2.04469757e-06
Iter: 1586 loss: 2.07319454e-06
Iter: 1587 loss: 2.04470689e-06
Iter: 1588 loss: 2.0424136e-06
Iter: 1589 loss: 2.04298158e-06
Iter: 1590 loss: 2.04075877e-06
Iter: 1591 loss: 2.03822697e-06
Iter: 1592 loss: 2.04081198e-06
Iter: 1593 loss: 2.03677519e-06
Iter: 1594 loss: 2.03313448e-06
Iter: 1595 loss: 2.05202332e-06
Iter: 1596 loss: 2.03249147e-06
Iter: 1597 loss: 2.03033778e-06
Iter: 1598 loss: 2.02703495e-06
Iter: 1599 loss: 2.02692081e-06
Iter: 1600 loss: 2.0231455e-06
Iter: 1601 loss: 2.0376433e-06
Iter: 1602 loss: 2.02222e-06
Iter: 1603 loss: 2.01875491e-06
Iter: 1604 loss: 2.03005061e-06
Iter: 1605 loss: 2.01776334e-06
Iter: 1606 loss: 2.01568628e-06
Iter: 1607 loss: 2.01558169e-06
Iter: 1608 loss: 2.01399553e-06
Iter: 1609 loss: 2.01091711e-06
Iter: 1610 loss: 2.07137282e-06
Iter: 1611 loss: 2.01083276e-06
Iter: 1612 loss: 2.00725299e-06
Iter: 1613 loss: 2.02298429e-06
Iter: 1614 loss: 2.00642444e-06
Iter: 1615 loss: 2.00423574e-06
Iter: 1616 loss: 2.00421528e-06
Iter: 1617 loss: 2.00227123e-06
Iter: 1618 loss: 2.00031536e-06
Iter: 1619 loss: 1.99989836e-06
Iter: 1620 loss: 1.99693022e-06
Iter: 1621 loss: 2.02293313e-06
Iter: 1622 loss: 1.99676583e-06
Iter: 1623 loss: 1.99443116e-06
Iter: 1624 loss: 1.99643137e-06
Iter: 1625 loss: 1.99307078e-06
Iter: 1626 loss: 1.99052192e-06
Iter: 1627 loss: 2.00362138e-06
Iter: 1628 loss: 1.99014107e-06
Iter: 1629 loss: 1.98803946e-06
Iter: 1630 loss: 1.98977409e-06
Iter: 1631 loss: 1.9867598e-06
Iter: 1632 loss: 1.98438397e-06
Iter: 1633 loss: 1.98904445e-06
Iter: 1634 loss: 1.98339103e-06
Iter: 1635 loss: 1.98016596e-06
Iter: 1636 loss: 1.99039141e-06
Iter: 1637 loss: 1.97925738e-06
Iter: 1638 loss: 1.97675126e-06
Iter: 1639 loss: 1.97511736e-06
Iter: 1640 loss: 1.97411282e-06
Iter: 1641 loss: 1.9705567e-06
Iter: 1642 loss: 1.98102725e-06
Iter: 1643 loss: 1.96944757e-06
Iter: 1644 loss: 1.96605e-06
Iter: 1645 loss: 1.9797917e-06
Iter: 1646 loss: 1.96537576e-06
Iter: 1647 loss: 1.96244446e-06
Iter: 1648 loss: 1.99052943e-06
Iter: 1649 loss: 1.96231167e-06
Iter: 1650 loss: 1.95978691e-06
Iter: 1651 loss: 1.96174278e-06
Iter: 1652 loss: 1.95818711e-06
Iter: 1653 loss: 1.95583561e-06
Iter: 1654 loss: 1.95781445e-06
Iter: 1655 loss: 1.95443749e-06
Iter: 1656 loss: 1.95246935e-06
Iter: 1657 loss: 1.95236953e-06
Iter: 1658 loss: 1.95084613e-06
Iter: 1659 loss: 1.94784411e-06
Iter: 1660 loss: 2.00755949e-06
Iter: 1661 loss: 1.94782615e-06
Iter: 1662 loss: 1.94585664e-06
Iter: 1663 loss: 1.94570976e-06
Iter: 1664 loss: 1.943906e-06
Iter: 1665 loss: 1.94031327e-06
Iter: 1666 loss: 2.01790112e-06
Iter: 1667 loss: 1.94025552e-06
Iter: 1668 loss: 1.93766368e-06
Iter: 1669 loss: 1.97211102e-06
Iter: 1670 loss: 1.93765572e-06
Iter: 1671 loss: 1.93533151e-06
Iter: 1672 loss: 1.93905453e-06
Iter: 1673 loss: 1.93430151e-06
Iter: 1674 loss: 1.93187316e-06
Iter: 1675 loss: 1.93392975e-06
Iter: 1676 loss: 1.93048936e-06
Iter: 1677 loss: 1.92751418e-06
Iter: 1678 loss: 1.94793938e-06
Iter: 1679 loss: 1.92716789e-06
Iter: 1680 loss: 1.9251429e-06
Iter: 1681 loss: 1.92369316e-06
Iter: 1682 loss: 1.92286689e-06
Iter: 1683 loss: 1.91987783e-06
Iter: 1684 loss: 1.92702282e-06
Iter: 1685 loss: 1.91874142e-06
Iter: 1686 loss: 1.91596428e-06
Iter: 1687 loss: 1.91595836e-06
Iter: 1688 loss: 1.91419099e-06
Iter: 1689 loss: 1.91182517e-06
Iter: 1690 loss: 1.91167351e-06
Iter: 1691 loss: 1.90909145e-06
Iter: 1692 loss: 1.94276527e-06
Iter: 1693 loss: 1.90909122e-06
Iter: 1694 loss: 1.90652077e-06
Iter: 1695 loss: 1.90887158e-06
Iter: 1696 loss: 1.90500532e-06
Iter: 1697 loss: 1.90267792e-06
Iter: 1698 loss: 1.90500657e-06
Iter: 1699 loss: 1.90125093e-06
Iter: 1700 loss: 1.89913328e-06
Iter: 1701 loss: 1.92945868e-06
Iter: 1702 loss: 1.89910816e-06
Iter: 1703 loss: 1.89737034e-06
Iter: 1704 loss: 1.89449145e-06
Iter: 1705 loss: 1.89445473e-06
Iter: 1706 loss: 1.89146203e-06
Iter: 1707 loss: 1.90331389e-06
Iter: 1708 loss: 1.89079663e-06
Iter: 1709 loss: 1.88762829e-06
Iter: 1710 loss: 1.90929813e-06
Iter: 1711 loss: 1.88731258e-06
Iter: 1712 loss: 1.88513377e-06
Iter: 1713 loss: 1.88371178e-06
Iter: 1714 loss: 1.88273407e-06
Iter: 1715 loss: 1.87997148e-06
Iter: 1716 loss: 1.90902983e-06
Iter: 1717 loss: 1.87986802e-06
Iter: 1718 loss: 1.87779426e-06
Iter: 1719 loss: 1.88105105e-06
Iter: 1720 loss: 1.87678711e-06
Iter: 1721 loss: 1.87458284e-06
Iter: 1722 loss: 1.87552689e-06
Iter: 1723 loss: 1.873202e-06
Iter: 1724 loss: 1.87051864e-06
Iter: 1725 loss: 1.89467482e-06
Iter: 1726 loss: 1.87037199e-06
Iter: 1727 loss: 1.86802936e-06
Iter: 1728 loss: 1.87139358e-06
Iter: 1729 loss: 1.86688317e-06
Iter: 1730 loss: 1.86490615e-06
Iter: 1731 loss: 1.87467856e-06
Iter: 1732 loss: 1.86455293e-06
Iter: 1733 loss: 1.86210173e-06
Iter: 1734 loss: 1.8631772e-06
Iter: 1735 loss: 1.8604477e-06
Iter: 1736 loss: 1.858313e-06
Iter: 1737 loss: 1.85886609e-06
Iter: 1738 loss: 1.85677413e-06
Iter: 1739 loss: 1.85456474e-06
Iter: 1740 loss: 1.8545087e-06
Iter: 1741 loss: 1.85294812e-06
Iter: 1742 loss: 1.85031479e-06
Iter: 1743 loss: 1.8502667e-06
Iter: 1744 loss: 1.84744465e-06
Iter: 1745 loss: 1.85738531e-06
Iter: 1746 loss: 1.84662747e-06
Iter: 1747 loss: 1.84411681e-06
Iter: 1748 loss: 1.87431419e-06
Iter: 1749 loss: 1.84411749e-06
Iter: 1750 loss: 1.84227872e-06
Iter: 1751 loss: 1.84054443e-06
Iter: 1752 loss: 1.8401139e-06
Iter: 1753 loss: 1.8376827e-06
Iter: 1754 loss: 1.84145256e-06
Iter: 1755 loss: 1.83647865e-06
Iter: 1756 loss: 1.8334116e-06
Iter: 1757 loss: 1.86225918e-06
Iter: 1758 loss: 1.83332304e-06
Iter: 1759 loss: 1.83125132e-06
Iter: 1760 loss: 1.83004067e-06
Iter: 1761 loss: 1.8291197e-06
Iter: 1762 loss: 1.82664166e-06
Iter: 1763 loss: 1.82666304e-06
Iter: 1764 loss: 1.8248744e-06
Iter: 1765 loss: 1.82599672e-06
Iter: 1766 loss: 1.82367035e-06
Iter: 1767 loss: 1.82187057e-06
Iter: 1768 loss: 1.8366e-06
Iter: 1769 loss: 1.82176291e-06
Iter: 1770 loss: 1.82026145e-06
Iter: 1771 loss: 1.81822497e-06
Iter: 1772 loss: 1.81812106e-06
Iter: 1773 loss: 1.81550604e-06
Iter: 1774 loss: 1.8256062e-06
Iter: 1775 loss: 1.81491396e-06
Iter: 1776 loss: 1.81286748e-06
Iter: 1777 loss: 1.83653128e-06
Iter: 1778 loss: 1.81278608e-06
Iter: 1779 loss: 1.81121493e-06
Iter: 1780 loss: 1.80875281e-06
Iter: 1781 loss: 1.80872917e-06
Iter: 1782 loss: 1.80568804e-06
Iter: 1783 loss: 1.8113883e-06
Iter: 1784 loss: 1.80434643e-06
Iter: 1785 loss: 1.80222e-06
Iter: 1786 loss: 1.80216534e-06
Iter: 1787 loss: 1.80030815e-06
Iter: 1788 loss: 1.79914196e-06
Iter: 1789 loss: 1.79842823e-06
Iter: 1790 loss: 1.79590086e-06
Iter: 1791 loss: 1.79764231e-06
Iter: 1792 loss: 1.79427911e-06
Iter: 1793 loss: 1.7919e-06
Iter: 1794 loss: 1.82622512e-06
Iter: 1795 loss: 1.79192568e-06
Iter: 1796 loss: 1.78991377e-06
Iter: 1797 loss: 1.79425342e-06
Iter: 1798 loss: 1.78916991e-06
Iter: 1799 loss: 1.78718699e-06
Iter: 1800 loss: 1.79293659e-06
Iter: 1801 loss: 1.78650737e-06
Iter: 1802 loss: 1.78418031e-06
Iter: 1803 loss: 1.79068218e-06
Iter: 1804 loss: 1.78345579e-06
Iter: 1805 loss: 1.78167477e-06
Iter: 1806 loss: 1.78463529e-06
Iter: 1807 loss: 1.78088067e-06
Iter: 1808 loss: 1.77846277e-06
Iter: 1809 loss: 1.78232676e-06
Iter: 1810 loss: 1.77733568e-06
Iter: 1811 loss: 1.77514767e-06
Iter: 1812 loss: 1.77824165e-06
Iter: 1813 loss: 1.77411823e-06
Iter: 1814 loss: 1.77192555e-06
Iter: 1815 loss: 1.79915526e-06
Iter: 1816 loss: 1.77190236e-06
Iter: 1817 loss: 1.77063293e-06
Iter: 1818 loss: 1.76846765e-06
Iter: 1819 loss: 1.7684356e-06
Iter: 1820 loss: 1.76597655e-06
Iter: 1821 loss: 1.77151628e-06
Iter: 1822 loss: 1.76508854e-06
Iter: 1823 loss: 1.76278763e-06
Iter: 1824 loss: 1.79781887e-06
Iter: 1825 loss: 1.76280594e-06
Iter: 1826 loss: 1.76108767e-06
Iter: 1827 loss: 1.75888158e-06
Iter: 1828 loss: 1.75868911e-06
Iter: 1829 loss: 1.75590549e-06
Iter: 1830 loss: 1.76132028e-06
Iter: 1831 loss: 1.75475293e-06
Iter: 1832 loss: 1.75308878e-06
Iter: 1833 loss: 1.75301545e-06
Iter: 1834 loss: 1.75137257e-06
Iter: 1835 loss: 1.75081573e-06
Iter: 1836 loss: 1.74987429e-06
Iter: 1837 loss: 1.74779893e-06
Iter: 1838 loss: 1.76218452e-06
Iter: 1839 loss: 1.74760225e-06
Iter: 1840 loss: 1.74586467e-06
Iter: 1841 loss: 1.74704473e-06
Iter: 1842 loss: 1.74475531e-06
Iter: 1843 loss: 1.7430275e-06
Iter: 1844 loss: 1.74898548e-06
Iter: 1845 loss: 1.74255445e-06
Iter: 1846 loss: 1.7405024e-06
Iter: 1847 loss: 1.74322645e-06
Iter: 1848 loss: 1.73953924e-06
Iter: 1849 loss: 1.73739704e-06
Iter: 1850 loss: 1.74402339e-06
Iter: 1851 loss: 1.7367903e-06
Iter: 1852 loss: 1.73453941e-06
Iter: 1853 loss: 1.74277284e-06
Iter: 1854 loss: 1.73393619e-06
Iter: 1855 loss: 1.73196656e-06
Iter: 1856 loss: 1.73081344e-06
Iter: 1857 loss: 1.7298953e-06
Iter: 1858 loss: 1.72729142e-06
Iter: 1859 loss: 1.73532453e-06
Iter: 1860 loss: 1.72657303e-06
Iter: 1861 loss: 1.72393413e-06
Iter: 1862 loss: 1.75100286e-06
Iter: 1863 loss: 1.72386763e-06
Iter: 1864 loss: 1.72226612e-06
Iter: 1865 loss: 1.72052705e-06
Iter: 1866 loss: 1.72024181e-06
Iter: 1867 loss: 1.71785666e-06
Iter: 1868 loss: 1.72866908e-06
Iter: 1869 loss: 1.71740464e-06
Iter: 1870 loss: 1.71569809e-06
Iter: 1871 loss: 1.71571901e-06
Iter: 1872 loss: 1.71429178e-06
Iter: 1873 loss: 1.71290685e-06
Iter: 1874 loss: 1.71262832e-06
Iter: 1875 loss: 1.71029762e-06
Iter: 1876 loss: 1.71841805e-06
Iter: 1877 loss: 1.70977091e-06
Iter: 1878 loss: 1.70687599e-06
Iter: 1879 loss: 1.7107825e-06
Iter: 1880 loss: 1.70543592e-06
Iter: 1881 loss: 1.70341355e-06
Iter: 1882 loss: 1.7084152e-06
Iter: 1883 loss: 1.70263127e-06
Iter: 1884 loss: 1.70022417e-06
Iter: 1885 loss: 1.71314923e-06
Iter: 1886 loss: 1.69983923e-06
Iter: 1887 loss: 1.69816428e-06
Iter: 1888 loss: 1.70065732e-06
Iter: 1889 loss: 1.69734449e-06
Iter: 1890 loss: 1.69546286e-06
Iter: 1891 loss: 1.70271915e-06
Iter: 1892 loss: 1.69494695e-06
Iter: 1893 loss: 1.69310101e-06
Iter: 1894 loss: 1.69105181e-06
Iter: 1895 loss: 1.69071427e-06
Iter: 1896 loss: 1.68820759e-06
Iter: 1897 loss: 1.7081137e-06
Iter: 1898 loss: 1.68805218e-06
Iter: 1899 loss: 1.68567783e-06
Iter: 1900 loss: 1.69690406e-06
Iter: 1901 loss: 1.68531164e-06
Iter: 1902 loss: 1.68370411e-06
Iter: 1903 loss: 1.6812802e-06
Iter: 1904 loss: 1.6812271e-06
Iter: 1905 loss: 1.67969597e-06
Iter: 1906 loss: 1.67943915e-06
Iter: 1907 loss: 1.67775738e-06
Iter: 1908 loss: 1.67730673e-06
Iter: 1909 loss: 1.67622545e-06
Iter: 1910 loss: 1.67428516e-06
Iter: 1911 loss: 1.67709e-06
Iter: 1912 loss: 1.67341386e-06
Iter: 1913 loss: 1.67066401e-06
Iter: 1914 loss: 1.68153861e-06
Iter: 1915 loss: 1.67004441e-06
Iter: 1916 loss: 1.66834775e-06
Iter: 1917 loss: 1.66748919e-06
Iter: 1918 loss: 1.66671794e-06
Iter: 1919 loss: 1.66446171e-06
Iter: 1920 loss: 1.69509872e-06
Iter: 1921 loss: 1.66448501e-06
Iter: 1922 loss: 1.66267159e-06
Iter: 1923 loss: 1.66176085e-06
Iter: 1924 loss: 1.66086329e-06
Iter: 1925 loss: 1.65868846e-06
Iter: 1926 loss: 1.67331109e-06
Iter: 1927 loss: 1.65842664e-06
Iter: 1928 loss: 1.65637368e-06
Iter: 1929 loss: 1.65853726e-06
Iter: 1930 loss: 1.65530639e-06
Iter: 1931 loss: 1.6531817e-06
Iter: 1932 loss: 1.65351923e-06
Iter: 1933 loss: 1.6515603e-06
Iter: 1934 loss: 1.64940775e-06
Iter: 1935 loss: 1.68260874e-06
Iter: 1936 loss: 1.64934886e-06
Iter: 1937 loss: 1.64753692e-06
Iter: 1938 loss: 1.6486473e-06
Iter: 1939 loss: 1.64636094e-06
Iter: 1940 loss: 1.64436472e-06
Iter: 1941 loss: 1.64558321e-06
Iter: 1942 loss: 1.64301173e-06
Iter: 1943 loss: 1.64083974e-06
Iter: 1944 loss: 1.64084645e-06
Iter: 1945 loss: 1.63932282e-06
Iter: 1946 loss: 1.63753816e-06
Iter: 1947 loss: 1.63732716e-06
Iter: 1948 loss: 1.63532661e-06
Iter: 1949 loss: 1.64167864e-06
Iter: 1950 loss: 1.63475568e-06
Iter: 1951 loss: 1.63243203e-06
Iter: 1952 loss: 1.6459237e-06
Iter: 1953 loss: 1.6320962e-06
Iter: 1954 loss: 1.63069672e-06
Iter: 1955 loss: 1.6292895e-06
Iter: 1956 loss: 1.6289672e-06
Iter: 1957 loss: 1.62676702e-06
Iter: 1958 loss: 1.65621032e-06
Iter: 1959 loss: 1.62676008e-06
Iter: 1960 loss: 1.62506831e-06
Iter: 1961 loss: 1.62347169e-06
Iter: 1962 loss: 1.62312483e-06
Iter: 1963 loss: 1.62070296e-06
Iter: 1964 loss: 1.63187792e-06
Iter: 1965 loss: 1.62029573e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2
+ date
Wed Nov  4 14:32:46 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi1.6/300_300_300_1 --function f2 --psi 1 --alpha 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b64ae4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b64b979d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f631620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f631f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f631730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f5bf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f5baea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f5ba400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f50f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f50fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b184806a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f50fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b18433d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b183d7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f54c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f56a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b3f56a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b183a1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b18394730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b183a1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b182a4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1832a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b18333730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b18333b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b182469d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b182cf9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b181f1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b182c3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b182c3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b181e3c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1819c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1819c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1820ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1819c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b18178840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b18178158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.010292877
test_loss: 0.01244424
train_loss: 0.008069509
test_loss: 0.010770932
train_loss: 0.007941805
test_loss: 0.010145242
train_loss: 0.007219021
test_loss: 0.0101116095
train_loss: 0.0070495396
test_loss: 0.010164632
train_loss: 0.006837435
test_loss: 0.009701826
train_loss: 0.006940886
test_loss: 0.009774202
train_loss: 0.006681391
test_loss: 0.009430974
train_loss: 0.0063483887
test_loss: 0.00957121
train_loss: 0.0067826174
test_loss: 0.009707816
train_loss: 0.0066859256
test_loss: 0.009495416
train_loss: 0.006357069
test_loss: 0.009371561
train_loss: 0.0065651867
test_loss: 0.009574682
train_loss: 0.0063446485
test_loss: 0.009299709
train_loss: 0.0058413604
test_loss: 0.009211342
train_loss: 0.0059942836
test_loss: 0.009165167
train_loss: 0.0059259143
test_loss: 0.00930601
train_loss: 0.00603005
test_loss: 0.009187699
train_loss: 0.0062708627
test_loss: 0.009359174
train_loss: 0.006202372
test_loss: 0.009071583
train_loss: 0.0062639085
test_loss: 0.009430657
train_loss: 0.005904754
test_loss: 0.008999478
train_loss: 0.0060783885
test_loss: 0.009173621
train_loss: 0.005721125
test_loss: 0.00914138
train_loss: 0.006174352
test_loss: 0.009213928
train_loss: 0.005856387
test_loss: 0.009177856
train_loss: 0.0055218358
test_loss: 0.009009173
train_loss: 0.006014142
test_loss: 0.009202366
train_loss: 0.005595847
test_loss: 0.008994813
train_loss: 0.005487846
test_loss: 0.009101833
train_loss: 0.0059103663
test_loss: 0.009151117
train_loss: 0.005876193
test_loss: 0.009009443
train_loss: 0.0058383946
test_loss: 0.009065037
train_loss: 0.0057602455
test_loss: 0.008924595
train_loss: 0.00539572
test_loss: 0.008962558
train_loss: 0.0056903176
test_loss: 0.008801828
train_loss: 0.0057498463
test_loss: 0.009011583
train_loss: 0.0057107005
test_loss: 0.009035358
train_loss: 0.0054788734
test_loss: 0.008694678
train_loss: 0.0054829335
test_loss: 0.008957018
train_loss: 0.0054677525
test_loss: 0.009019779
train_loss: 0.006165987
test_loss: 0.00882169
train_loss: 0.005729322
test_loss: 0.008856946
train_loss: 0.0052282065
test_loss: 0.00866015
train_loss: 0.005425222
test_loss: 0.008651816
train_loss: 0.0054068556
test_loss: 0.008776804
train_loss: 0.0054089385
test_loss: 0.008663386
train_loss: 0.00599748
test_loss: 0.008927604
train_loss: 0.005191776
test_loss: 0.008640936
train_loss: 0.0053688623
test_loss: 0.008849504
train_loss: 0.0053730905
test_loss: 0.008735328
train_loss: 0.0055240868
test_loss: 0.008966788
train_loss: 0.005148944
test_loss: 0.008972543
train_loss: 0.0049613453
test_loss: 0.008819439
train_loss: 0.0053644013
test_loss: 0.008798409
train_loss: 0.005853294
test_loss: 0.008787096
train_loss: 0.004931583
test_loss: 0.008659523
train_loss: 0.005126101
test_loss: 0.0087165525
train_loss: 0.0055475095
test_loss: 0.008593088
train_loss: 0.0054240897
test_loss: 0.008761426
train_loss: 0.005716051
test_loss: 0.008975872
train_loss: 0.0053023943
test_loss: 0.008646549
train_loss: 0.005396921
test_loss: 0.008592779
train_loss: 0.0055686426
test_loss: 0.008887043
train_loss: 0.004920762
test_loss: 0.008756703
train_loss: 0.005406118
test_loss: 0.008895195
train_loss: 0.005347679
test_loss: 0.008634403
train_loss: 0.005621055
test_loss: 0.008793245
train_loss: 0.0051985756
test_loss: 0.008471893
train_loss: 0.005221688
test_loss: 0.00857386
train_loss: 0.005074814
test_loss: 0.008598756
train_loss: 0.005168014
test_loss: 0.008781992
train_loss: 0.0053655533
test_loss: 0.008617917
train_loss: 0.0050303745
test_loss: 0.00863043
train_loss: 0.0049180016
test_loss: 0.0087540755
train_loss: 0.00502359
test_loss: 0.0085080825
train_loss: 0.0055182716
test_loss: 0.008669377
train_loss: 0.0050809123
test_loss: 0.008694281
train_loss: 0.0051287934
test_loss: 0.008583613
train_loss: 0.0056462428
test_loss: 0.00868176
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2/300_300_300_1 --optimizer lbfgs --function f2 --psi 1 --alpha 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c4ca95620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c4ca952f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c4ca086a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c4ca08400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c4ca01730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c4ca29400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c4ca292f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c4c97d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c4c93e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c4cbc5048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c4c93ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48f59c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48f8cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48f41400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48ee5c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48f101e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48f10620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48e657b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48e84950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48e65488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48e49598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48e49e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48e0c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48e65ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48dc3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48d5ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48d1b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48d5a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48d36a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48cdc1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48d099d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48d097b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48c70bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48d09bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48c70048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c48c39268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.65825831e-05
Iter: 2 loss: 4.82192372e-05
Iter: 3 loss: 5.60948138e-05
Iter: 4 loss: 4.34058602e-05
Iter: 5 loss: 3.64604202e-05
Iter: 6 loss: 6.94259797e-05
Iter: 7 loss: 3.51461131e-05
Iter: 8 loss: 3.12398624e-05
Iter: 9 loss: 4.36235132e-05
Iter: 10 loss: 3.0119525e-05
Iter: 11 loss: 2.72333218e-05
Iter: 12 loss: 5.58509164e-05
Iter: 13 loss: 2.71377867e-05
Iter: 14 loss: 2.57108622e-05
Iter: 15 loss: 2.48054566e-05
Iter: 16 loss: 2.42436436e-05
Iter: 17 loss: 2.25423973e-05
Iter: 18 loss: 3.58556717e-05
Iter: 19 loss: 2.24230116e-05
Iter: 20 loss: 2.09930477e-05
Iter: 21 loss: 2.38452412e-05
Iter: 22 loss: 2.04078497e-05
Iter: 23 loss: 1.9234998e-05
Iter: 24 loss: 2.07739795e-05
Iter: 25 loss: 1.86387333e-05
Iter: 26 loss: 1.77341462e-05
Iter: 27 loss: 2.8029619e-05
Iter: 28 loss: 1.77191723e-05
Iter: 29 loss: 1.69871382e-05
Iter: 30 loss: 1.75995774e-05
Iter: 31 loss: 1.65514575e-05
Iter: 32 loss: 1.59253e-05
Iter: 33 loss: 1.73037715e-05
Iter: 34 loss: 1.56838032e-05
Iter: 35 loss: 1.51522809e-05
Iter: 36 loss: 1.98732669e-05
Iter: 37 loss: 1.51267168e-05
Iter: 38 loss: 1.47001938e-05
Iter: 39 loss: 1.50660762e-05
Iter: 40 loss: 1.44478163e-05
Iter: 41 loss: 1.40136335e-05
Iter: 42 loss: 1.43787674e-05
Iter: 43 loss: 1.3756041e-05
Iter: 44 loss: 1.33758476e-05
Iter: 45 loss: 1.6587046e-05
Iter: 46 loss: 1.33540361e-05
Iter: 47 loss: 1.30406079e-05
Iter: 48 loss: 1.60250147e-05
Iter: 49 loss: 1.30282369e-05
Iter: 50 loss: 1.28120837e-05
Iter: 51 loss: 1.26571404e-05
Iter: 52 loss: 1.25817724e-05
Iter: 53 loss: 1.23445852e-05
Iter: 54 loss: 1.59613337e-05
Iter: 55 loss: 1.23444725e-05
Iter: 56 loss: 1.21775774e-05
Iter: 57 loss: 1.19726838e-05
Iter: 58 loss: 1.1954111e-05
Iter: 59 loss: 1.17043646e-05
Iter: 60 loss: 1.27545918e-05
Iter: 61 loss: 1.16515257e-05
Iter: 62 loss: 1.1446743e-05
Iter: 63 loss: 1.31654915e-05
Iter: 64 loss: 1.14346767e-05
Iter: 65 loss: 1.1275305e-05
Iter: 66 loss: 1.14962331e-05
Iter: 67 loss: 1.11963727e-05
Iter: 68 loss: 1.10222982e-05
Iter: 69 loss: 1.12054195e-05
Iter: 70 loss: 1.09265929e-05
Iter: 71 loss: 1.07724318e-05
Iter: 72 loss: 1.24903581e-05
Iter: 73 loss: 1.07693886e-05
Iter: 74 loss: 1.06466068e-05
Iter: 75 loss: 1.06195221e-05
Iter: 76 loss: 1.05395802e-05
Iter: 77 loss: 1.03808597e-05
Iter: 78 loss: 1.083795e-05
Iter: 79 loss: 1.03310149e-05
Iter: 80 loss: 1.02241047e-05
Iter: 81 loss: 1.16842311e-05
Iter: 82 loss: 1.02236772e-05
Iter: 83 loss: 1.01310488e-05
Iter: 84 loss: 1.02047215e-05
Iter: 85 loss: 1.00752059e-05
Iter: 86 loss: 9.97613279e-06
Iter: 87 loss: 1.0684118e-05
Iter: 88 loss: 9.96754261e-06
Iter: 89 loss: 9.88772e-06
Iter: 90 loss: 9.89109321e-06
Iter: 91 loss: 9.82460915e-06
Iter: 92 loss: 9.72862654e-06
Iter: 93 loss: 9.82103484e-06
Iter: 94 loss: 9.67399683e-06
Iter: 95 loss: 9.59632598e-06
Iter: 96 loss: 9.5963378e-06
Iter: 97 loss: 9.52811752e-06
Iter: 98 loss: 9.42698534e-06
Iter: 99 loss: 9.42446877e-06
Iter: 100 loss: 9.31096838e-06
Iter: 101 loss: 9.55930227e-06
Iter: 102 loss: 9.26730627e-06
Iter: 103 loss: 9.16476347e-06
Iter: 104 loss: 9.81296671e-06
Iter: 105 loss: 9.15307282e-06
Iter: 106 loss: 9.06322202e-06
Iter: 107 loss: 9.72744238e-06
Iter: 108 loss: 9.05614706e-06
Iter: 109 loss: 8.99458428e-06
Iter: 110 loss: 9.00674422e-06
Iter: 111 loss: 8.94887216e-06
Iter: 112 loss: 8.86846283e-06
Iter: 113 loss: 9.0567728e-06
Iter: 114 loss: 8.83926623e-06
Iter: 115 loss: 8.76493323e-06
Iter: 116 loss: 9.43778286e-06
Iter: 117 loss: 8.76149716e-06
Iter: 118 loss: 8.71229531e-06
Iter: 119 loss: 8.78041101e-06
Iter: 120 loss: 8.68798088e-06
Iter: 121 loss: 8.62656634e-06
Iter: 122 loss: 8.98503095e-06
Iter: 123 loss: 8.61836543e-06
Iter: 124 loss: 8.56953375e-06
Iter: 125 loss: 8.5493366e-06
Iter: 126 loss: 8.52331232e-06
Iter: 127 loss: 8.458087e-06
Iter: 128 loss: 8.45775412e-06
Iter: 129 loss: 8.40553275e-06
Iter: 130 loss: 8.3639452e-06
Iter: 131 loss: 8.35657193e-06
Iter: 132 loss: 8.3170853e-06
Iter: 133 loss: 8.27254826e-06
Iter: 134 loss: 8.26668293e-06
Iter: 135 loss: 8.20098558e-06
Iter: 136 loss: 8.3324685e-06
Iter: 137 loss: 8.1743292e-06
Iter: 138 loss: 8.11624068e-06
Iter: 139 loss: 8.24510698e-06
Iter: 140 loss: 8.09414269e-06
Iter: 141 loss: 8.04234e-06
Iter: 142 loss: 8.83867324e-06
Iter: 143 loss: 8.04242427e-06
Iter: 144 loss: 8.00301132e-06
Iter: 145 loss: 7.97177654e-06
Iter: 146 loss: 7.95967117e-06
Iter: 147 loss: 7.91127786e-06
Iter: 148 loss: 8.18067747e-06
Iter: 149 loss: 7.90408285e-06
Iter: 150 loss: 7.85878547e-06
Iter: 151 loss: 8.12310373e-06
Iter: 152 loss: 7.85308384e-06
Iter: 153 loss: 7.8215071e-06
Iter: 154 loss: 7.85245356e-06
Iter: 155 loss: 7.80371192e-06
Iter: 156 loss: 7.76828165e-06
Iter: 157 loss: 8.11961e-06
Iter: 158 loss: 7.7672812e-06
Iter: 159 loss: 7.740211e-06
Iter: 160 loss: 7.70190763e-06
Iter: 161 loss: 7.7007553e-06
Iter: 162 loss: 7.65335426e-06
Iter: 163 loss: 7.89335081e-06
Iter: 164 loss: 7.64565448e-06
Iter: 165 loss: 7.60573084e-06
Iter: 166 loss: 7.77924834e-06
Iter: 167 loss: 7.5974167e-06
Iter: 168 loss: 7.56240388e-06
Iter: 169 loss: 7.57313092e-06
Iter: 170 loss: 7.53689847e-06
Iter: 171 loss: 7.49619358e-06
Iter: 172 loss: 7.63623393e-06
Iter: 173 loss: 7.48523689e-06
Iter: 174 loss: 7.43963619e-06
Iter: 175 loss: 7.7255263e-06
Iter: 176 loss: 7.43436703e-06
Iter: 177 loss: 7.40716814e-06
Iter: 178 loss: 7.39741927e-06
Iter: 179 loss: 7.38228937e-06
Iter: 180 loss: 7.3427268e-06
Iter: 181 loss: 7.35054709e-06
Iter: 182 loss: 7.31342152e-06
Iter: 183 loss: 7.27624683e-06
Iter: 184 loss: 7.78405865e-06
Iter: 185 loss: 7.27609677e-06
Iter: 186 loss: 7.24554775e-06
Iter: 187 loss: 7.37992241e-06
Iter: 188 loss: 7.23965422e-06
Iter: 189 loss: 7.21098877e-06
Iter: 190 loss: 7.2382818e-06
Iter: 191 loss: 7.1945442e-06
Iter: 192 loss: 7.16883142e-06
Iter: 193 loss: 7.49000355e-06
Iter: 194 loss: 7.16857e-06
Iter: 195 loss: 7.14527687e-06
Iter: 196 loss: 7.11995108e-06
Iter: 197 loss: 7.11596795e-06
Iter: 198 loss: 7.08678363e-06
Iter: 199 loss: 7.2143207e-06
Iter: 200 loss: 7.08100333e-06
Iter: 201 loss: 7.0511287e-06
Iter: 202 loss: 7.15154556e-06
Iter: 203 loss: 7.04278364e-06
Iter: 204 loss: 7.01385034e-06
Iter: 205 loss: 7.02983e-06
Iter: 206 loss: 6.99489556e-06
Iter: 207 loss: 6.9685957e-06
Iter: 208 loss: 7.18769297e-06
Iter: 209 loss: 6.96707548e-06
Iter: 210 loss: 6.94110577e-06
Iter: 211 loss: 6.95145354e-06
Iter: 212 loss: 6.92304639e-06
Iter: 213 loss: 6.89481067e-06
Iter: 214 loss: 6.92553112e-06
Iter: 215 loss: 6.87930606e-06
Iter: 216 loss: 6.84772294e-06
Iter: 217 loss: 7.02713533e-06
Iter: 218 loss: 6.8434756e-06
Iter: 219 loss: 6.81387883e-06
Iter: 220 loss: 6.96896859e-06
Iter: 221 loss: 6.80941957e-06
Iter: 222 loss: 6.78488323e-06
Iter: 223 loss: 6.78347533e-06
Iter: 224 loss: 6.76463378e-06
Iter: 225 loss: 6.73729392e-06
Iter: 226 loss: 6.75451247e-06
Iter: 227 loss: 6.71985708e-06
Iter: 228 loss: 6.70180589e-06
Iter: 229 loss: 6.6999396e-06
Iter: 230 loss: 6.68174e-06
Iter: 231 loss: 6.69343399e-06
Iter: 232 loss: 6.67021686e-06
Iter: 233 loss: 6.65088464e-06
Iter: 234 loss: 6.64538538e-06
Iter: 235 loss: 6.6337916e-06
Iter: 236 loss: 6.60631031e-06
Iter: 237 loss: 6.84200495e-06
Iter: 238 loss: 6.60463047e-06
Iter: 239 loss: 6.58436556e-06
Iter: 240 loss: 6.57448436e-06
Iter: 241 loss: 6.56470183e-06
Iter: 242 loss: 6.53512e-06
Iter: 243 loss: 6.63986748e-06
Iter: 244 loss: 6.52756171e-06
Iter: 245 loss: 6.50251513e-06
Iter: 246 loss: 6.74847843e-06
Iter: 247 loss: 6.50186121e-06
Iter: 248 loss: 6.48335208e-06
Iter: 249 loss: 6.46679837e-06
Iter: 250 loss: 6.46215767e-06
Iter: 251 loss: 6.44049e-06
Iter: 252 loss: 6.6781331e-06
Iter: 253 loss: 6.43991461e-06
Iter: 254 loss: 6.417657e-06
Iter: 255 loss: 6.4155e-06
Iter: 256 loss: 6.39906284e-06
Iter: 257 loss: 6.37644234e-06
Iter: 258 loss: 6.45659793e-06
Iter: 259 loss: 6.37034191e-06
Iter: 260 loss: 6.34966455e-06
Iter: 261 loss: 6.4716337e-06
Iter: 262 loss: 6.34694879e-06
Iter: 263 loss: 6.32612e-06
Iter: 264 loss: 6.35945116e-06
Iter: 265 loss: 6.31639887e-06
Iter: 266 loss: 6.29987426e-06
Iter: 267 loss: 6.34851085e-06
Iter: 268 loss: 6.29471378e-06
Iter: 269 loss: 6.27558848e-06
Iter: 270 loss: 6.38352594e-06
Iter: 271 loss: 6.2729473e-06
Iter: 272 loss: 6.25746725e-06
Iter: 273 loss: 6.24454469e-06
Iter: 274 loss: 6.24005042e-06
Iter: 275 loss: 6.21784056e-06
Iter: 276 loss: 6.20407263e-06
Iter: 277 loss: 6.19513139e-06
Iter: 278 loss: 6.16356783e-06
Iter: 279 loss: 6.3926891e-06
Iter: 280 loss: 6.16097e-06
Iter: 281 loss: 6.13809743e-06
Iter: 282 loss: 6.4113915e-06
Iter: 283 loss: 6.13770726e-06
Iter: 284 loss: 6.12073381e-06
Iter: 285 loss: 6.11898258e-06
Iter: 286 loss: 6.10656025e-06
Iter: 287 loss: 6.08610571e-06
Iter: 288 loss: 6.19559387e-06
Iter: 289 loss: 6.08286973e-06
Iter: 290 loss: 6.06195226e-06
Iter: 291 loss: 6.1180649e-06
Iter: 292 loss: 6.05507194e-06
Iter: 293 loss: 6.03869375e-06
Iter: 294 loss: 6.04333218e-06
Iter: 295 loss: 6.02689533e-06
Iter: 296 loss: 6.00906105e-06
Iter: 297 loss: 6.1810415e-06
Iter: 298 loss: 6.00835483e-06
Iter: 299 loss: 5.99113082e-06
Iter: 300 loss: 6.00874637e-06
Iter: 301 loss: 5.981507e-06
Iter: 302 loss: 5.96205746e-06
Iter: 303 loss: 5.99335e-06
Iter: 304 loss: 5.95304391e-06
Iter: 305 loss: 5.93587447e-06
Iter: 306 loss: 6.20371611e-06
Iter: 307 loss: 5.93585628e-06
Iter: 308 loss: 5.92490505e-06
Iter: 309 loss: 5.90225591e-06
Iter: 310 loss: 6.29126953e-06
Iter: 311 loss: 5.90175023e-06
Iter: 312 loss: 5.87697241e-06
Iter: 313 loss: 6.01741203e-06
Iter: 314 loss: 5.87364411e-06
Iter: 315 loss: 5.8524115e-06
Iter: 316 loss: 5.99794839e-06
Iter: 317 loss: 5.8502942e-06
Iter: 318 loss: 5.83681822e-06
Iter: 319 loss: 5.83613564e-06
Iter: 320 loss: 5.82581106e-06
Iter: 321 loss: 5.80535061e-06
Iter: 322 loss: 5.83250585e-06
Iter: 323 loss: 5.79514972e-06
Iter: 324 loss: 5.7764687e-06
Iter: 325 loss: 5.84009558e-06
Iter: 326 loss: 5.77138144e-06
Iter: 327 loss: 5.75032618e-06
Iter: 328 loss: 5.91449498e-06
Iter: 329 loss: 5.74897967e-06
Iter: 330 loss: 5.73387433e-06
Iter: 331 loss: 5.73426496e-06
Iter: 332 loss: 5.7221373e-06
Iter: 333 loss: 5.70573957e-06
Iter: 334 loss: 5.78692379e-06
Iter: 335 loss: 5.70286693e-06
Iter: 336 loss: 5.68576525e-06
Iter: 337 loss: 5.73988e-06
Iter: 338 loss: 5.68086e-06
Iter: 339 loss: 5.66632207e-06
Iter: 340 loss: 5.70518932e-06
Iter: 341 loss: 5.66133349e-06
Iter: 342 loss: 5.64702623e-06
Iter: 343 loss: 5.74578826e-06
Iter: 344 loss: 5.64569746e-06
Iter: 345 loss: 5.63470167e-06
Iter: 346 loss: 5.62485093e-06
Iter: 347 loss: 5.62201285e-06
Iter: 348 loss: 5.60253284e-06
Iter: 349 loss: 5.74060368e-06
Iter: 350 loss: 5.60077478e-06
Iter: 351 loss: 5.58890588e-06
Iter: 352 loss: 5.58620832e-06
Iter: 353 loss: 5.57849489e-06
Iter: 354 loss: 5.56182204e-06
Iter: 355 loss: 5.55870656e-06
Iter: 356 loss: 5.54732515e-06
Iter: 357 loss: 5.52591e-06
Iter: 358 loss: 5.65656865e-06
Iter: 359 loss: 5.52324309e-06
Iter: 360 loss: 5.50446657e-06
Iter: 361 loss: 5.59881209e-06
Iter: 362 loss: 5.50141749e-06
Iter: 363 loss: 5.48314756e-06
Iter: 364 loss: 5.56435953e-06
Iter: 365 loss: 5.47945274e-06
Iter: 366 loss: 5.46605952e-06
Iter: 367 loss: 5.47606805e-06
Iter: 368 loss: 5.45795137e-06
Iter: 369 loss: 5.44190243e-06
Iter: 370 loss: 5.50145432e-06
Iter: 371 loss: 5.43808164e-06
Iter: 372 loss: 5.42032603e-06
Iter: 373 loss: 5.48786193e-06
Iter: 374 loss: 5.41603822e-06
Iter: 375 loss: 5.40242399e-06
Iter: 376 loss: 5.41152622e-06
Iter: 377 loss: 5.3939475e-06
Iter: 378 loss: 5.38147287e-06
Iter: 379 loss: 5.38146696e-06
Iter: 380 loss: 5.37164578e-06
Iter: 381 loss: 5.35581421e-06
Iter: 382 loss: 5.35566141e-06
Iter: 383 loss: 5.34291485e-06
Iter: 384 loss: 5.34289848e-06
Iter: 385 loss: 5.33170532e-06
Iter: 386 loss: 5.31818569e-06
Iter: 387 loss: 5.31683872e-06
Iter: 388 loss: 5.30193938e-06
Iter: 389 loss: 5.38538234e-06
Iter: 390 loss: 5.29981935e-06
Iter: 391 loss: 5.28445116e-06
Iter: 392 loss: 5.33048842e-06
Iter: 393 loss: 5.27974134e-06
Iter: 394 loss: 5.2659143e-06
Iter: 395 loss: 5.27431e-06
Iter: 396 loss: 5.25697214e-06
Iter: 397 loss: 5.24142433e-06
Iter: 398 loss: 5.25153155e-06
Iter: 399 loss: 5.23158815e-06
Iter: 400 loss: 5.21466472e-06
Iter: 401 loss: 5.38710265e-06
Iter: 402 loss: 5.21417087e-06
Iter: 403 loss: 5.2022333e-06
Iter: 404 loss: 5.28177907e-06
Iter: 405 loss: 5.20101366e-06
Iter: 406 loss: 5.18948218e-06
Iter: 407 loss: 5.17804528e-06
Iter: 408 loss: 5.17561921e-06
Iter: 409 loss: 5.16072669e-06
Iter: 410 loss: 5.32152e-06
Iter: 411 loss: 5.16043565e-06
Iter: 412 loss: 5.14769727e-06
Iter: 413 loss: 5.18652132e-06
Iter: 414 loss: 5.1439074e-06
Iter: 415 loss: 5.13320765e-06
Iter: 416 loss: 5.15689408e-06
Iter: 417 loss: 5.12906581e-06
Iter: 418 loss: 5.11524559e-06
Iter: 419 loss: 5.13310897e-06
Iter: 420 loss: 5.10807058e-06
Iter: 421 loss: 5.09519623e-06
Iter: 422 loss: 5.10472364e-06
Iter: 423 loss: 5.0871713e-06
Iter: 424 loss: 5.07326513e-06
Iter: 425 loss: 5.17357967e-06
Iter: 426 loss: 5.07197183e-06
Iter: 427 loss: 5.05927619e-06
Iter: 428 loss: 5.08677e-06
Iter: 429 loss: 5.05426942e-06
Iter: 430 loss: 5.04338777e-06
Iter: 431 loss: 5.03075808e-06
Iter: 432 loss: 5.02915736e-06
Iter: 433 loss: 5.01084833e-06
Iter: 434 loss: 5.13214218e-06
Iter: 435 loss: 5.0089493e-06
Iter: 436 loss: 4.99566568e-06
Iter: 437 loss: 5.03712363e-06
Iter: 438 loss: 4.99180942e-06
Iter: 439 loss: 4.97712335e-06
Iter: 440 loss: 5.07026834e-06
Iter: 441 loss: 4.97537303e-06
Iter: 442 loss: 4.96581151e-06
Iter: 443 loss: 4.95772292e-06
Iter: 444 loss: 4.95492532e-06
Iter: 445 loss: 4.94079677e-06
Iter: 446 loss: 5.140143e-06
Iter: 447 loss: 4.94074902e-06
Iter: 448 loss: 4.93067409e-06
Iter: 449 loss: 4.94386859e-06
Iter: 450 loss: 4.92554727e-06
Iter: 451 loss: 4.91579567e-06
Iter: 452 loss: 4.96811299e-06
Iter: 453 loss: 4.91426545e-06
Iter: 454 loss: 4.90364391e-06
Iter: 455 loss: 4.90514276e-06
Iter: 456 loss: 4.89581e-06
Iter: 457 loss: 4.88479782e-06
Iter: 458 loss: 4.8917027e-06
Iter: 459 loss: 4.87763373e-06
Iter: 460 loss: 4.86291628e-06
Iter: 461 loss: 4.98324334e-06
Iter: 462 loss: 4.86207318e-06
Iter: 463 loss: 4.85204237e-06
Iter: 464 loss: 4.84892917e-06
Iter: 465 loss: 4.84306383e-06
Iter: 466 loss: 4.82977339e-06
Iter: 467 loss: 4.88524165e-06
Iter: 468 loss: 4.82700761e-06
Iter: 469 loss: 4.81341567e-06
Iter: 470 loss: 4.87447051e-06
Iter: 471 loss: 4.81077e-06
Iter: 472 loss: 4.80074777e-06
Iter: 473 loss: 4.80076642e-06
Iter: 474 loss: 4.79281e-06
Iter: 475 loss: 4.77853837e-06
Iter: 476 loss: 4.80538029e-06
Iter: 477 loss: 4.77237063e-06
Iter: 478 loss: 4.75739262e-06
Iter: 479 loss: 4.76797368e-06
Iter: 480 loss: 4.74811441e-06
Iter: 481 loss: 4.73611044e-06
Iter: 482 loss: 4.73583123e-06
Iter: 483 loss: 4.72422926e-06
Iter: 484 loss: 4.73797263e-06
Iter: 485 loss: 4.71819e-06
Iter: 486 loss: 4.70840951e-06
Iter: 487 loss: 4.76430068e-06
Iter: 488 loss: 4.70698706e-06
Iter: 489 loss: 4.69707811e-06
Iter: 490 loss: 4.70787745e-06
Iter: 491 loss: 4.69163797e-06
Iter: 492 loss: 4.68252711e-06
Iter: 493 loss: 4.69176166e-06
Iter: 494 loss: 4.67737163e-06
Iter: 495 loss: 4.6655191e-06
Iter: 496 loss: 4.73112914e-06
Iter: 497 loss: 4.66389702e-06
Iter: 498 loss: 4.65539551e-06
Iter: 499 loss: 4.6515961e-06
Iter: 500 loss: 4.64742288e-06
Iter: 501 loss: 4.63699917e-06
Iter: 502 loss: 4.75035586e-06
Iter: 503 loss: 4.63669494e-06
Iter: 504 loss: 4.62658591e-06
Iter: 505 loss: 4.62050593e-06
Iter: 506 loss: 4.61619038e-06
Iter: 507 loss: 4.60306501e-06
Iter: 508 loss: 4.64942195e-06
Iter: 509 loss: 4.59960484e-06
Iter: 510 loss: 4.58834302e-06
Iter: 511 loss: 4.59856301e-06
Iter: 512 loss: 4.58180284e-06
Iter: 513 loss: 4.56934595e-06
Iter: 514 loss: 4.71287194e-06
Iter: 515 loss: 4.56912949e-06
Iter: 516 loss: 4.559236e-06
Iter: 517 loss: 4.56950056e-06
Iter: 518 loss: 4.55387544e-06
Iter: 519 loss: 4.5428751e-06
Iter: 520 loss: 4.55398185e-06
Iter: 521 loss: 4.53673965e-06
Iter: 522 loss: 4.5281804e-06
Iter: 523 loss: 4.52783934e-06
Iter: 524 loss: 4.5213942e-06
Iter: 525 loss: 4.51133292e-06
Iter: 526 loss: 4.51122e-06
Iter: 527 loss: 4.50061589e-06
Iter: 528 loss: 4.56973839e-06
Iter: 529 loss: 4.49946765e-06
Iter: 530 loss: 4.48973606e-06
Iter: 531 loss: 4.52563e-06
Iter: 532 loss: 4.48738456e-06
Iter: 533 loss: 4.47947332e-06
Iter: 534 loss: 4.47082857e-06
Iter: 535 loss: 4.46946e-06
Iter: 536 loss: 4.45740761e-06
Iter: 537 loss: 4.52553832e-06
Iter: 538 loss: 4.45558771e-06
Iter: 539 loss: 4.4428507e-06
Iter: 540 loss: 4.5006127e-06
Iter: 541 loss: 4.44020225e-06
Iter: 542 loss: 4.431045e-06
Iter: 543 loss: 4.43290719e-06
Iter: 544 loss: 4.42417695e-06
Iter: 545 loss: 4.41437214e-06
Iter: 546 loss: 4.51774031e-06
Iter: 547 loss: 4.41409975e-06
Iter: 548 loss: 4.40568783e-06
Iter: 549 loss: 4.40105759e-06
Iter: 550 loss: 4.39717178e-06
Iter: 551 loss: 4.38566622e-06
Iter: 552 loss: 4.40553958e-06
Iter: 553 loss: 4.38061807e-06
Iter: 554 loss: 4.36880737e-06
Iter: 555 loss: 4.43120553e-06
Iter: 556 loss: 4.36695791e-06
Iter: 557 loss: 4.35828315e-06
Iter: 558 loss: 4.47854e-06
Iter: 559 loss: 4.35828497e-06
Iter: 560 loss: 4.35057791e-06
Iter: 561 loss: 4.34040203e-06
Iter: 562 loss: 4.33988771e-06
Iter: 563 loss: 4.33024252e-06
Iter: 564 loss: 4.40070835e-06
Iter: 565 loss: 4.32950037e-06
Iter: 566 loss: 4.31956641e-06
Iter: 567 loss: 4.3343407e-06
Iter: 568 loss: 4.31479566e-06
Iter: 569 loss: 4.30480259e-06
Iter: 570 loss: 4.30868295e-06
Iter: 571 loss: 4.29781812e-06
Iter: 572 loss: 4.28656222e-06
Iter: 573 loss: 4.39722498e-06
Iter: 574 loss: 4.28626117e-06
Iter: 575 loss: 4.27794112e-06
Iter: 576 loss: 4.29059673e-06
Iter: 577 loss: 4.27400118e-06
Iter: 578 loss: 4.26512634e-06
Iter: 579 loss: 4.26024053e-06
Iter: 580 loss: 4.25640519e-06
Iter: 581 loss: 4.24673271e-06
Iter: 582 loss: 4.24675409e-06
Iter: 583 loss: 4.23857637e-06
Iter: 584 loss: 4.24548216e-06
Iter: 585 loss: 4.23377514e-06
Iter: 586 loss: 4.22378798e-06
Iter: 587 loss: 4.22012772e-06
Iter: 588 loss: 4.21457207e-06
Iter: 589 loss: 4.20489687e-06
Iter: 590 loss: 4.20488777e-06
Iter: 591 loss: 4.19681146e-06
Iter: 592 loss: 4.20503693e-06
Iter: 593 loss: 4.19238722e-06
Iter: 594 loss: 4.18523905e-06
Iter: 595 loss: 4.24513883e-06
Iter: 596 loss: 4.18476066e-06
Iter: 597 loss: 4.17704587e-06
Iter: 598 loss: 4.17513866e-06
Iter: 599 loss: 4.17026558e-06
Iter: 600 loss: 4.16132298e-06
Iter: 601 loss: 4.16052171e-06
Iter: 602 loss: 4.15375871e-06
Iter: 603 loss: 4.14200122e-06
Iter: 604 loss: 4.2030747e-06
Iter: 605 loss: 4.14006217e-06
Iter: 606 loss: 4.12959071e-06
Iter: 607 loss: 4.22109497e-06
Iter: 608 loss: 4.1289918e-06
Iter: 609 loss: 4.12228474e-06
Iter: 610 loss: 4.12068903e-06
Iter: 611 loss: 4.11631345e-06
Iter: 612 loss: 4.10697976e-06
Iter: 613 loss: 4.10868233e-06
Iter: 614 loss: 4.0999148e-06
Iter: 615 loss: 4.08914593e-06
Iter: 616 loss: 4.210523e-06
Iter: 617 loss: 4.0889181e-06
Iter: 618 loss: 4.08051e-06
Iter: 619 loss: 4.11574638e-06
Iter: 620 loss: 4.0786922e-06
Iter: 621 loss: 4.0708419e-06
Iter: 622 loss: 4.06425443e-06
Iter: 623 loss: 4.06208528e-06
Iter: 624 loss: 4.05127594e-06
Iter: 625 loss: 4.126568e-06
Iter: 626 loss: 4.05029368e-06
Iter: 627 loss: 4.04051934e-06
Iter: 628 loss: 4.08095957e-06
Iter: 629 loss: 4.03830927e-06
Iter: 630 loss: 4.03048034e-06
Iter: 631 loss: 4.05686751e-06
Iter: 632 loss: 4.02826845e-06
Iter: 633 loss: 4.0205241e-06
Iter: 634 loss: 4.06128947e-06
Iter: 635 loss: 4.01930447e-06
Iter: 636 loss: 4.01347e-06
Iter: 637 loss: 4.00208501e-06
Iter: 638 loss: 4.23155734e-06
Iter: 639 loss: 4.00196222e-06
Iter: 640 loss: 3.99469491e-06
Iter: 641 loss: 3.99442342e-06
Iter: 642 loss: 3.98756583e-06
Iter: 643 loss: 3.9825436e-06
Iter: 644 loss: 3.98028e-06
Iter: 645 loss: 3.9709239e-06
Iter: 646 loss: 4.00209319e-06
Iter: 647 loss: 3.96838459e-06
Iter: 648 loss: 3.96098221e-06
Iter: 649 loss: 4.04884304e-06
Iter: 650 loss: 3.96081168e-06
Iter: 651 loss: 3.9542756e-06
Iter: 652 loss: 3.94390645e-06
Iter: 653 loss: 3.94385552e-06
Iter: 654 loss: 3.9338e-06
Iter: 655 loss: 3.98881275e-06
Iter: 656 loss: 3.93231949e-06
Iter: 657 loss: 3.92276888e-06
Iter: 658 loss: 3.93283335e-06
Iter: 659 loss: 3.91757976e-06
Iter: 660 loss: 3.90980813e-06
Iter: 661 loss: 3.9097622e-06
Iter: 662 loss: 3.90400055e-06
Iter: 663 loss: 3.89799925e-06
Iter: 664 loss: 3.89705747e-06
Iter: 665 loss: 3.88773333e-06
Iter: 666 loss: 3.94489e-06
Iter: 667 loss: 3.886787e-06
Iter: 668 loss: 3.87698856e-06
Iter: 669 loss: 3.91019103e-06
Iter: 670 loss: 3.87441651e-06
Iter: 671 loss: 3.86823285e-06
Iter: 672 loss: 3.86618e-06
Iter: 673 loss: 3.86281863e-06
Iter: 674 loss: 3.85434123e-06
Iter: 675 loss: 3.89769548e-06
Iter: 676 loss: 3.85298063e-06
Iter: 677 loss: 3.84424857e-06
Iter: 678 loss: 3.85745534e-06
Iter: 679 loss: 3.83989936e-06
Iter: 680 loss: 3.83319093e-06
Iter: 681 loss: 3.8404828e-06
Iter: 682 loss: 3.8294711e-06
Iter: 683 loss: 3.82102735e-06
Iter: 684 loss: 3.89593606e-06
Iter: 685 loss: 3.82062035e-06
Iter: 686 loss: 3.81339237e-06
Iter: 687 loss: 3.81372456e-06
Iter: 688 loss: 3.80770234e-06
Iter: 689 loss: 3.79925154e-06
Iter: 690 loss: 3.80845199e-06
Iter: 691 loss: 3.79452945e-06
Iter: 692 loss: 3.78631762e-06
Iter: 693 loss: 3.85498333e-06
Iter: 694 loss: 3.78587e-06
Iter: 695 loss: 3.77699462e-06
Iter: 696 loss: 3.78178447e-06
Iter: 697 loss: 3.77115362e-06
Iter: 698 loss: 3.76245634e-06
Iter: 699 loss: 3.78186724e-06
Iter: 700 loss: 3.75927198e-06
Iter: 701 loss: 3.75208742e-06
Iter: 702 loss: 3.85194426e-06
Iter: 703 loss: 3.75208174e-06
Iter: 704 loss: 3.74596357e-06
Iter: 705 loss: 3.74780984e-06
Iter: 706 loss: 3.74160209e-06
Iter: 707 loss: 3.7343907e-06
Iter: 708 loss: 3.75656168e-06
Iter: 709 loss: 3.73235184e-06
Iter: 710 loss: 3.72426257e-06
Iter: 711 loss: 3.74110596e-06
Iter: 712 loss: 3.72104387e-06
Iter: 713 loss: 3.71451e-06
Iter: 714 loss: 3.71175884e-06
Iter: 715 loss: 3.70822909e-06
Iter: 716 loss: 3.69773215e-06
Iter: 717 loss: 3.72246313e-06
Iter: 718 loss: 3.69384452e-06
Iter: 719 loss: 3.6850206e-06
Iter: 720 loss: 3.79663334e-06
Iter: 721 loss: 3.68493352e-06
Iter: 722 loss: 3.67774123e-06
Iter: 723 loss: 3.68399742e-06
Iter: 724 loss: 3.67356188e-06
Iter: 725 loss: 3.66694439e-06
Iter: 726 loss: 3.7011705e-06
Iter: 727 loss: 3.66589825e-06
Iter: 728 loss: 3.65912729e-06
Iter: 729 loss: 3.67065081e-06
Iter: 730 loss: 3.65615188e-06
Iter: 731 loss: 3.64884545e-06
Iter: 732 loss: 3.6515703e-06
Iter: 733 loss: 3.64371499e-06
Iter: 734 loss: 3.63522668e-06
Iter: 735 loss: 3.64487755e-06
Iter: 736 loss: 3.63045456e-06
Iter: 737 loss: 3.62568699e-06
Iter: 738 loss: 3.62520495e-06
Iter: 739 loss: 3.6196609e-06
Iter: 740 loss: 3.61468824e-06
Iter: 741 loss: 3.61328216e-06
Iter: 742 loss: 3.60616514e-06
Iter: 743 loss: 3.64942434e-06
Iter: 744 loss: 3.60536433e-06
Iter: 745 loss: 3.59879e-06
Iter: 746 loss: 3.61146795e-06
Iter: 747 loss: 3.5960179e-06
Iter: 748 loss: 3.59000092e-06
Iter: 749 loss: 3.58667467e-06
Iter: 750 loss: 3.5839505e-06
Iter: 751 loss: 3.57567774e-06
Iter: 752 loss: 3.65409824e-06
Iter: 753 loss: 3.57540944e-06
Iter: 754 loss: 3.56844521e-06
Iter: 755 loss: 3.58502575e-06
Iter: 756 loss: 3.5658577e-06
Iter: 757 loss: 3.5597559e-06
Iter: 758 loss: 3.55960401e-06
Iter: 759 loss: 3.55476459e-06
Iter: 760 loss: 3.54568829e-06
Iter: 761 loss: 3.58584111e-06
Iter: 762 loss: 3.54385702e-06
Iter: 763 loss: 3.53734708e-06
Iter: 764 loss: 3.61183038e-06
Iter: 765 loss: 3.53706e-06
Iter: 766 loss: 3.53223777e-06
Iter: 767 loss: 3.52558527e-06
Iter: 768 loss: 3.52523966e-06
Iter: 769 loss: 3.51762083e-06
Iter: 770 loss: 3.60435297e-06
Iter: 771 loss: 3.51740732e-06
Iter: 772 loss: 3.51143512e-06
Iter: 773 loss: 3.51707968e-06
Iter: 774 loss: 3.50794039e-06
Iter: 775 loss: 3.50170558e-06
Iter: 776 loss: 3.5189089e-06
Iter: 777 loss: 3.49973402e-06
Iter: 778 loss: 3.49293646e-06
Iter: 779 loss: 3.55051475e-06
Iter: 780 loss: 3.49259017e-06
Iter: 781 loss: 3.48844446e-06
Iter: 782 loss: 3.47985497e-06
Iter: 783 loss: 3.61752609e-06
Iter: 784 loss: 3.47951323e-06
Iter: 785 loss: 3.47040282e-06
Iter: 786 loss: 3.5269718e-06
Iter: 787 loss: 3.46939623e-06
Iter: 788 loss: 3.46247157e-06
Iter: 789 loss: 3.50879873e-06
Iter: 790 loss: 3.46169782e-06
Iter: 791 loss: 3.45480112e-06
Iter: 792 loss: 3.46940283e-06
Iter: 793 loss: 3.45210719e-06
Iter: 794 loss: 3.44684827e-06
Iter: 795 loss: 3.44213186e-06
Iter: 796 loss: 3.44085402e-06
Iter: 797 loss: 3.43408851e-06
Iter: 798 loss: 3.43407669e-06
Iter: 799 loss: 3.42857902e-06
Iter: 800 loss: 3.43268698e-06
Iter: 801 loss: 3.42514477e-06
Iter: 802 loss: 3.41811301e-06
Iter: 803 loss: 3.42815247e-06
Iter: 804 loss: 3.414709e-06
Iter: 805 loss: 3.40761744e-06
Iter: 806 loss: 3.46438583e-06
Iter: 807 loss: 3.40710608e-06
Iter: 808 loss: 3.40144538e-06
Iter: 809 loss: 3.40507177e-06
Iter: 810 loss: 3.3977667e-06
Iter: 811 loss: 3.39167013e-06
Iter: 812 loss: 3.4088248e-06
Iter: 813 loss: 3.38971859e-06
Iter: 814 loss: 3.38275413e-06
Iter: 815 loss: 3.43006673e-06
Iter: 816 loss: 3.3821475e-06
Iter: 817 loss: 3.37758433e-06
Iter: 818 loss: 3.37940128e-06
Iter: 819 loss: 3.37451729e-06
Iter: 820 loss: 3.36842572e-06
Iter: 821 loss: 3.3923227e-06
Iter: 822 loss: 3.36699759e-06
Iter: 823 loss: 3.36157882e-06
Iter: 824 loss: 3.35367122e-06
Iter: 825 loss: 3.35346795e-06
Iter: 826 loss: 3.34491051e-06
Iter: 827 loss: 3.40065367e-06
Iter: 828 loss: 3.34393371e-06
Iter: 829 loss: 3.33684466e-06
Iter: 830 loss: 3.3443996e-06
Iter: 831 loss: 3.33291655e-06
Iter: 832 loss: 3.32517811e-06
Iter: 833 loss: 3.42807903e-06
Iter: 834 loss: 3.32523e-06
Iter: 835 loss: 3.31922047e-06
Iter: 836 loss: 3.32514e-06
Iter: 837 loss: 3.31583533e-06
Iter: 838 loss: 3.30967987e-06
Iter: 839 loss: 3.31301271e-06
Iter: 840 loss: 3.30567377e-06
Iter: 841 loss: 3.29964314e-06
Iter: 842 loss: 3.38169298e-06
Iter: 843 loss: 3.29959266e-06
Iter: 844 loss: 3.29468912e-06
Iter: 845 loss: 3.29481122e-06
Iter: 846 loss: 3.2907451e-06
Iter: 847 loss: 3.28504348e-06
Iter: 848 loss: 3.31847423e-06
Iter: 849 loss: 3.28433134e-06
Iter: 850 loss: 3.27846465e-06
Iter: 851 loss: 3.29015279e-06
Iter: 852 loss: 3.27604312e-06
Iter: 853 loss: 3.27103226e-06
Iter: 854 loss: 3.29543104e-06
Iter: 855 loss: 3.27020052e-06
Iter: 856 loss: 3.26539202e-06
Iter: 857 loss: 3.26656e-06
Iter: 858 loss: 3.26191798e-06
Iter: 859 loss: 3.2562134e-06
Iter: 860 loss: 3.26156055e-06
Iter: 861 loss: 3.25293081e-06
Iter: 862 loss: 3.24636471e-06
Iter: 863 loss: 3.275979e-06
Iter: 864 loss: 3.245073e-06
Iter: 865 loss: 3.23856943e-06
Iter: 866 loss: 3.26477902e-06
Iter: 867 loss: 3.2371488e-06
Iter: 868 loss: 3.23198242e-06
Iter: 869 loss: 3.23356926e-06
Iter: 870 loss: 3.2283574e-06
Iter: 871 loss: 3.22161713e-06
Iter: 872 loss: 3.21978632e-06
Iter: 873 loss: 3.21560628e-06
Iter: 874 loss: 3.20804293e-06
Iter: 875 loss: 3.30537205e-06
Iter: 876 loss: 3.2080136e-06
Iter: 877 loss: 3.20200274e-06
Iter: 878 loss: 3.24493385e-06
Iter: 879 loss: 3.20155345e-06
Iter: 880 loss: 3.19694891e-06
Iter: 881 loss: 3.19326023e-06
Iter: 882 loss: 3.19186256e-06
Iter: 883 loss: 3.18679758e-06
Iter: 884 loss: 3.25307383e-06
Iter: 885 loss: 3.18673665e-06
Iter: 886 loss: 3.18180946e-06
Iter: 887 loss: 3.18605566e-06
Iter: 888 loss: 3.17875811e-06
Iter: 889 loss: 3.17305285e-06
Iter: 890 loss: 3.18450475e-06
Iter: 891 loss: 3.17080389e-06
Iter: 892 loss: 3.16461819e-06
Iter: 893 loss: 3.1921345e-06
Iter: 894 loss: 3.1634263e-06
Iter: 895 loss: 3.15885609e-06
Iter: 896 loss: 3.15591114e-06
Iter: 897 loss: 3.15428019e-06
Iter: 898 loss: 3.14737872e-06
Iter: 899 loss: 3.21920015e-06
Iter: 900 loss: 3.14711224e-06
Iter: 901 loss: 3.14257431e-06
Iter: 902 loss: 3.13874261e-06
Iter: 903 loss: 3.13737269e-06
Iter: 904 loss: 3.13018973e-06
Iter: 905 loss: 3.15455236e-06
Iter: 906 loss: 3.12821066e-06
Iter: 907 loss: 3.12216866e-06
Iter: 908 loss: 3.14804038e-06
Iter: 909 loss: 3.12094403e-06
Iter: 910 loss: 3.11512304e-06
Iter: 911 loss: 3.14933823e-06
Iter: 912 loss: 3.11430267e-06
Iter: 913 loss: 3.1096879e-06
Iter: 914 loss: 3.11000849e-06
Iter: 915 loss: 3.10626228e-06
Iter: 916 loss: 3.10022642e-06
Iter: 917 loss: 3.10248834e-06
Iter: 918 loss: 3.09604366e-06
Iter: 919 loss: 3.09106326e-06
Iter: 920 loss: 3.09069378e-06
Iter: 921 loss: 3.08640642e-06
Iter: 922 loss: 3.0843396e-06
Iter: 923 loss: 3.08225594e-06
Iter: 924 loss: 3.07762571e-06
Iter: 925 loss: 3.13041369e-06
Iter: 926 loss: 3.07758319e-06
Iter: 927 loss: 3.07348751e-06
Iter: 928 loss: 3.06881202e-06
Iter: 929 loss: 3.06813899e-06
Iter: 930 loss: 3.06199627e-06
Iter: 931 loss: 3.07052869e-06
Iter: 932 loss: 3.0589315e-06
Iter: 933 loss: 3.0531412e-06
Iter: 934 loss: 3.13053852e-06
Iter: 935 loss: 3.05313119e-06
Iter: 936 loss: 3.04883724e-06
Iter: 937 loss: 3.04927471e-06
Iter: 938 loss: 3.04560808e-06
Iter: 939 loss: 3.04068703e-06
Iter: 940 loss: 3.06159882e-06
Iter: 941 loss: 3.03967681e-06
Iter: 942 loss: 3.0341821e-06
Iter: 943 loss: 3.04586547e-06
Iter: 944 loss: 3.03203933e-06
Iter: 945 loss: 3.02657872e-06
Iter: 946 loss: 3.02883927e-06
Iter: 947 loss: 3.02284616e-06
Iter: 948 loss: 3.01545697e-06
Iter: 949 loss: 3.02534681e-06
Iter: 950 loss: 3.01177283e-06
Iter: 951 loss: 3.00635838e-06
Iter: 952 loss: 3.05002982e-06
Iter: 953 loss: 3.00601573e-06
Iter: 954 loss: 3.00038118e-06
Iter: 955 loss: 3.02006333e-06
Iter: 956 loss: 2.99889098e-06
Iter: 957 loss: 2.99396879e-06
Iter: 958 loss: 3.01137425e-06
Iter: 959 loss: 2.99266912e-06
Iter: 960 loss: 2.98863733e-06
Iter: 961 loss: 3.01430896e-06
Iter: 962 loss: 2.98817531e-06
Iter: 963 loss: 2.98447731e-06
Iter: 964 loss: 2.97769634e-06
Iter: 965 loss: 3.13350643e-06
Iter: 966 loss: 2.97770021e-06
Iter: 967 loss: 2.97202359e-06
Iter: 968 loss: 3.06125389e-06
Iter: 969 loss: 2.97199722e-06
Iter: 970 loss: 2.96748203e-06
Iter: 971 loss: 2.97410361e-06
Iter: 972 loss: 2.96524195e-06
Iter: 973 loss: 2.96043481e-06
Iter: 974 loss: 2.95800737e-06
Iter: 975 loss: 2.95559403e-06
Iter: 976 loss: 2.94910433e-06
Iter: 977 loss: 2.96940107e-06
Iter: 978 loss: 2.94734286e-06
Iter: 979 loss: 2.94139204e-06
Iter: 980 loss: 3.01650857e-06
Iter: 981 loss: 2.94136771e-06
Iter: 982 loss: 2.93772928e-06
Iter: 983 loss: 2.93651237e-06
Iter: 984 loss: 2.9344003e-06
Iter: 985 loss: 2.92919799e-06
Iter: 986 loss: 2.95300924e-06
Iter: 987 loss: 2.92822233e-06
Iter: 988 loss: 2.92222285e-06
Iter: 989 loss: 2.93024686e-06
Iter: 990 loss: 2.91911e-06
Iter: 991 loss: 2.91376205e-06
Iter: 992 loss: 2.92398909e-06
Iter: 993 loss: 2.91148012e-06
Iter: 994 loss: 2.90638718e-06
Iter: 995 loss: 2.91778201e-06
Iter: 996 loss: 2.90436469e-06
Iter: 997 loss: 2.89860736e-06
Iter: 998 loss: 2.96732628e-06
Iter: 999 loss: 2.8985e-06
Iter: 1000 loss: 2.89547461e-06
Iter: 1001 loss: 2.89200261e-06
Iter: 1002 loss: 2.89161608e-06
Iter: 1003 loss: 2.88658748e-06
Iter: 1004 loss: 2.91356082e-06
Iter: 1005 loss: 2.88589763e-06
Iter: 1006 loss: 2.88087972e-06
Iter: 1007 loss: 2.89052673e-06
Iter: 1008 loss: 2.87880084e-06
Iter: 1009 loss: 2.87370585e-06
Iter: 1010 loss: 2.87849525e-06
Iter: 1011 loss: 2.87088983e-06
Iter: 1012 loss: 2.8660761e-06
Iter: 1013 loss: 2.91941205e-06
Iter: 1014 loss: 2.86595218e-06
Iter: 1015 loss: 2.86192153e-06
Iter: 1016 loss: 2.85855731e-06
Iter: 1017 loss: 2.85737337e-06
Iter: 1018 loss: 2.85101351e-06
Iter: 1019 loss: 2.86509794e-06
Iter: 1020 loss: 2.84855378e-06
Iter: 1021 loss: 2.84306498e-06
Iter: 1022 loss: 2.86009913e-06
Iter: 1023 loss: 2.84145017e-06
Iter: 1024 loss: 2.8351019e-06
Iter: 1025 loss: 2.8746058e-06
Iter: 1026 loss: 2.83434929e-06
Iter: 1027 loss: 2.8303034e-06
Iter: 1028 loss: 2.83005784e-06
Iter: 1029 loss: 2.82708606e-06
Iter: 1030 loss: 2.82263136e-06
Iter: 1031 loss: 2.87033708e-06
Iter: 1032 loss: 2.82259634e-06
Iter: 1033 loss: 2.81823895e-06
Iter: 1034 loss: 2.82293195e-06
Iter: 1035 loss: 2.81594293e-06
Iter: 1036 loss: 2.81207576e-06
Iter: 1037 loss: 2.83680879e-06
Iter: 1038 loss: 2.81157963e-06
Iter: 1039 loss: 2.807378e-06
Iter: 1040 loss: 2.805e-06
Iter: 1041 loss: 2.80319136e-06
Iter: 1042 loss: 2.79787446e-06
Iter: 1043 loss: 2.79663982e-06
Iter: 1044 loss: 2.79313826e-06
Iter: 1045 loss: 2.7863266e-06
Iter: 1046 loss: 2.85179681e-06
Iter: 1047 loss: 2.78602374e-06
Iter: 1048 loss: 2.78174298e-06
Iter: 1049 loss: 2.80591803e-06
Iter: 1050 loss: 2.78106563e-06
Iter: 1051 loss: 2.77637491e-06
Iter: 1052 loss: 2.77585059e-06
Iter: 1053 loss: 2.77243e-06
Iter: 1054 loss: 2.76746505e-06
Iter: 1055 loss: 2.79216943e-06
Iter: 1056 loss: 2.76658648e-06
Iter: 1057 loss: 2.76171454e-06
Iter: 1058 loss: 2.787119e-06
Iter: 1059 loss: 2.76098035e-06
Iter: 1060 loss: 2.75722732e-06
Iter: 1061 loss: 2.75445927e-06
Iter: 1062 loss: 2.75316188e-06
Iter: 1063 loss: 2.7469805e-06
Iter: 1064 loss: 2.7610854e-06
Iter: 1065 loss: 2.74460399e-06
Iter: 1066 loss: 2.73911883e-06
Iter: 1067 loss: 2.80635368e-06
Iter: 1068 loss: 2.73902128e-06
Iter: 1069 loss: 2.73520277e-06
Iter: 1070 loss: 2.73339288e-06
Iter: 1071 loss: 2.73137016e-06
Iter: 1072 loss: 2.7275255e-06
Iter: 1073 loss: 2.72751413e-06
Iter: 1074 loss: 2.72433817e-06
Iter: 1075 loss: 2.72207603e-06
Iter: 1076 loss: 2.7209951e-06
Iter: 1077 loss: 2.71630961e-06
Iter: 1078 loss: 2.71134968e-06
Iter: 1079 loss: 2.71055387e-06
Iter: 1080 loss: 2.7060014e-06
Iter: 1081 loss: 2.70584337e-06
Iter: 1082 loss: 2.70153146e-06
Iter: 1083 loss: 2.70490955e-06
Iter: 1084 loss: 2.69882094e-06
Iter: 1085 loss: 2.69428438e-06
Iter: 1086 loss: 2.69653333e-06
Iter: 1087 loss: 2.69110751e-06
Iter: 1088 loss: 2.6856469e-06
Iter: 1089 loss: 2.72008833e-06
Iter: 1090 loss: 2.68504209e-06
Iter: 1091 loss: 2.68036e-06
Iter: 1092 loss: 2.70084092e-06
Iter: 1093 loss: 2.67938685e-06
Iter: 1094 loss: 2.67508176e-06
Iter: 1095 loss: 2.67241739e-06
Iter: 1096 loss: 2.67063547e-06
Iter: 1097 loss: 2.6656453e-06
Iter: 1098 loss: 2.71926729e-06
Iter: 1099 loss: 2.66548273e-06
Iter: 1100 loss: 2.661503e-06
Iter: 1101 loss: 2.67784799e-06
Iter: 1102 loss: 2.6606242e-06
Iter: 1103 loss: 2.65707104e-06
Iter: 1104 loss: 2.65501035e-06
Iter: 1105 loss: 2.65347e-06
Iter: 1106 loss: 2.64960113e-06
Iter: 1107 loss: 2.64959954e-06
Iter: 1108 loss: 2.64633627e-06
Iter: 1109 loss: 2.64692108e-06
Iter: 1110 loss: 2.64391679e-06
Iter: 1111 loss: 2.63979973e-06
Iter: 1112 loss: 2.66661095e-06
Iter: 1113 loss: 2.63940456e-06
Iter: 1114 loss: 2.63610627e-06
Iter: 1115 loss: 2.63178117e-06
Iter: 1116 loss: 2.6314151e-06
Iter: 1117 loss: 2.62582466e-06
Iter: 1118 loss: 2.64387336e-06
Iter: 1119 loss: 2.62414051e-06
Iter: 1120 loss: 2.61896571e-06
Iter: 1121 loss: 2.62482081e-06
Iter: 1122 loss: 2.61623791e-06
Iter: 1123 loss: 2.61077525e-06
Iter: 1124 loss: 2.65043127e-06
Iter: 1125 loss: 2.61031619e-06
Iter: 1126 loss: 2.60612569e-06
Iter: 1127 loss: 2.63998186e-06
Iter: 1128 loss: 2.60585443e-06
Iter: 1129 loss: 2.60256093e-06
Iter: 1130 loss: 2.59844546e-06
Iter: 1131 loss: 2.59808053e-06
Iter: 1132 loss: 2.59318722e-06
Iter: 1133 loss: 2.63381821e-06
Iter: 1134 loss: 2.5928548e-06
Iter: 1135 loss: 2.58805267e-06
Iter: 1136 loss: 2.60337356e-06
Iter: 1137 loss: 2.58667137e-06
Iter: 1138 loss: 2.5831223e-06
Iter: 1139 loss: 2.587155e-06
Iter: 1140 loss: 2.58132559e-06
Iter: 1141 loss: 2.57680358e-06
Iter: 1142 loss: 2.59959461e-06
Iter: 1143 loss: 2.576106e-06
Iter: 1144 loss: 2.57193597e-06
Iter: 1145 loss: 2.58439832e-06
Iter: 1146 loss: 2.57068086e-06
Iter: 1147 loss: 2.56762542e-06
Iter: 1148 loss: 2.58075079e-06
Iter: 1149 loss: 2.5669533e-06
Iter: 1150 loss: 2.5637089e-06
Iter: 1151 loss: 2.55987879e-06
Iter: 1152 loss: 2.55944246e-06
Iter: 1153 loss: 2.5549291e-06
Iter: 1154 loss: 2.59367789e-06
Iter: 1155 loss: 2.55469558e-06
Iter: 1156 loss: 2.55122472e-06
Iter: 1157 loss: 2.56545627e-06
Iter: 1158 loss: 2.55044552e-06
Iter: 1159 loss: 2.54702081e-06
Iter: 1160 loss: 2.54385964e-06
Iter: 1161 loss: 2.54308839e-06
Iter: 1162 loss: 2.53774988e-06
Iter: 1163 loss: 2.54797351e-06
Iter: 1164 loss: 2.5356735e-06
Iter: 1165 loss: 2.52974564e-06
Iter: 1166 loss: 2.54471342e-06
Iter: 1167 loss: 2.52769678e-06
Iter: 1168 loss: 2.52278323e-06
Iter: 1169 loss: 2.5635436e-06
Iter: 1170 loss: 2.52237214e-06
Iter: 1171 loss: 2.5178158e-06
Iter: 1172 loss: 2.53835788e-06
Iter: 1173 loss: 2.51701908e-06
Iter: 1174 loss: 2.51356187e-06
Iter: 1175 loss: 2.51602614e-06
Iter: 1176 loss: 2.51149e-06
Iter: 1177 loss: 2.50771518e-06
Iter: 1178 loss: 2.5464185e-06
Iter: 1179 loss: 2.50765606e-06
Iter: 1180 loss: 2.50480548e-06
Iter: 1181 loss: 2.50501125e-06
Iter: 1182 loss: 2.50252833e-06
Iter: 1183 loss: 2.49876825e-06
Iter: 1184 loss: 2.52353129e-06
Iter: 1185 loss: 2.49842333e-06
Iter: 1186 loss: 2.49555251e-06
Iter: 1187 loss: 2.49264349e-06
Iter: 1188 loss: 2.49206187e-06
Iter: 1189 loss: 2.48768856e-06
Iter: 1190 loss: 2.50995163e-06
Iter: 1191 loss: 2.48705919e-06
Iter: 1192 loss: 2.48242668e-06
Iter: 1193 loss: 2.49531286e-06
Iter: 1194 loss: 2.48110246e-06
Iter: 1195 loss: 2.47750245e-06
Iter: 1196 loss: 2.48342599e-06
Iter: 1197 loss: 2.47583966e-06
Iter: 1198 loss: 2.47153662e-06
Iter: 1199 loss: 2.47980211e-06
Iter: 1200 loss: 2.46982108e-06
Iter: 1201 loss: 2.46533773e-06
Iter: 1202 loss: 2.50244648e-06
Iter: 1203 loss: 2.46508353e-06
Iter: 1204 loss: 2.46221634e-06
Iter: 1205 loss: 2.45825208e-06
Iter: 1206 loss: 2.45807678e-06
Iter: 1207 loss: 2.4529611e-06
Iter: 1208 loss: 2.47028265e-06
Iter: 1209 loss: 2.4515125e-06
Iter: 1210 loss: 2.44678631e-06
Iter: 1211 loss: 2.46715035e-06
Iter: 1212 loss: 2.4458509e-06
Iter: 1213 loss: 2.44166404e-06
Iter: 1214 loss: 2.49462391e-06
Iter: 1215 loss: 2.44157377e-06
Iter: 1216 loss: 2.43891918e-06
Iter: 1217 loss: 2.4380638e-06
Iter: 1218 loss: 2.43653813e-06
Iter: 1219 loss: 2.43233285e-06
Iter: 1220 loss: 2.45395631e-06
Iter: 1221 loss: 2.43167e-06
Iter: 1222 loss: 2.42866463e-06
Iter: 1223 loss: 2.42987585e-06
Iter: 1224 loss: 2.42656552e-06
Iter: 1225 loss: 2.42252054e-06
Iter: 1226 loss: 2.43706154e-06
Iter: 1227 loss: 2.42161877e-06
Iter: 1228 loss: 2.41738485e-06
Iter: 1229 loss: 2.42343071e-06
Iter: 1230 loss: 2.4153444e-06
Iter: 1231 loss: 2.41144562e-06
Iter: 1232 loss: 2.41210091e-06
Iter: 1233 loss: 2.40852046e-06
Iter: 1234 loss: 2.40431496e-06
Iter: 1235 loss: 2.45902856e-06
Iter: 1236 loss: 2.4042215e-06
Iter: 1237 loss: 2.40087206e-06
Iter: 1238 loss: 2.4061992e-06
Iter: 1239 loss: 2.39927795e-06
Iter: 1240 loss: 2.39565975e-06
Iter: 1241 loss: 2.39553424e-06
Iter: 1242 loss: 2.39270526e-06
Iter: 1243 loss: 2.38828579e-06
Iter: 1244 loss: 2.42352439e-06
Iter: 1245 loss: 2.38796429e-06
Iter: 1246 loss: 2.38429129e-06
Iter: 1247 loss: 2.40206873e-06
Iter: 1248 loss: 2.38370944e-06
Iter: 1249 loss: 2.38060602e-06
Iter: 1250 loss: 2.37643712e-06
Iter: 1251 loss: 2.37626659e-06
Iter: 1252 loss: 2.37294034e-06
Iter: 1253 loss: 2.37271183e-06
Iter: 1254 loss: 2.36971209e-06
Iter: 1255 loss: 2.37085442e-06
Iter: 1256 loss: 2.36754977e-06
Iter: 1257 loss: 2.3644e-06
Iter: 1258 loss: 2.36179176e-06
Iter: 1259 loss: 2.36088135e-06
Iter: 1260 loss: 2.35698485e-06
Iter: 1261 loss: 2.41579164e-06
Iter: 1262 loss: 2.3569728e-06
Iter: 1263 loss: 2.35366838e-06
Iter: 1264 loss: 2.35462039e-06
Iter: 1265 loss: 2.35131847e-06
Iter: 1266 loss: 2.34736081e-06
Iter: 1267 loss: 2.36205142e-06
Iter: 1268 loss: 2.34641675e-06
Iter: 1269 loss: 2.34281606e-06
Iter: 1270 loss: 2.36213918e-06
Iter: 1271 loss: 2.34235472e-06
Iter: 1272 loss: 2.33941e-06
Iter: 1273 loss: 2.33578385e-06
Iter: 1274 loss: 2.33551054e-06
Iter: 1275 loss: 2.33029209e-06
Iter: 1276 loss: 2.34904314e-06
Iter: 1277 loss: 2.32901766e-06
Iter: 1278 loss: 2.3248117e-06
Iter: 1279 loss: 2.36599567e-06
Iter: 1280 loss: 2.32465345e-06
Iter: 1281 loss: 2.32084403e-06
Iter: 1282 loss: 2.32296861e-06
Iter: 1283 loss: 2.31835747e-06
Iter: 1284 loss: 2.31488e-06
Iter: 1285 loss: 2.3232783e-06
Iter: 1286 loss: 2.31362287e-06
Iter: 1287 loss: 2.31039417e-06
Iter: 1288 loss: 2.35138668e-06
Iter: 1289 loss: 2.31041531e-06
Iter: 1290 loss: 2.30751948e-06
Iter: 1291 loss: 2.30478122e-06
Iter: 1292 loss: 2.30418777e-06
Iter: 1293 loss: 2.30032265e-06
Iter: 1294 loss: 2.32503658e-06
Iter: 1295 loss: 2.29990042e-06
Iter: 1296 loss: 2.29644752e-06
Iter: 1297 loss: 2.30411183e-06
Iter: 1298 loss: 2.29522675e-06
Iter: 1299 loss: 2.29192119e-06
Iter: 1300 loss: 2.29200623e-06
Iter: 1301 loss: 2.28929321e-06
Iter: 1302 loss: 2.28546355e-06
Iter: 1303 loss: 2.29756643e-06
Iter: 1304 loss: 2.28436193e-06
Iter: 1305 loss: 2.28046156e-06
Iter: 1306 loss: 2.31628474e-06
Iter: 1307 loss: 2.2802808e-06
Iter: 1308 loss: 2.27762143e-06
Iter: 1309 loss: 2.27564237e-06
Iter: 1310 loss: 2.27475266e-06
Iter: 1311 loss: 2.27088185e-06
Iter: 1312 loss: 2.30897513e-06
Iter: 1313 loss: 2.2707959e-06
Iter: 1314 loss: 2.26767588e-06
Iter: 1315 loss: 2.26699194e-06
Iter: 1316 loss: 2.26498696e-06
Iter: 1317 loss: 2.26100246e-06
Iter: 1318 loss: 2.26615157e-06
Iter: 1319 loss: 2.25897816e-06
Iter: 1320 loss: 2.25535541e-06
Iter: 1321 loss: 2.27907049e-06
Iter: 1322 loss: 2.2549566e-06
Iter: 1323 loss: 2.25161853e-06
Iter: 1324 loss: 2.27666078e-06
Iter: 1325 loss: 2.25131271e-06
Iter: 1326 loss: 2.2486513e-06
Iter: 1327 loss: 2.25053464e-06
Iter: 1328 loss: 2.24694327e-06
Iter: 1329 loss: 2.24380415e-06
Iter: 1330 loss: 2.25791337e-06
Iter: 1331 loss: 2.24321957e-06
Iter: 1332 loss: 2.24025894e-06
Iter: 1333 loss: 2.23691814e-06
Iter: 1334 loss: 2.23651114e-06
Iter: 1335 loss: 2.23266647e-06
Iter: 1336 loss: 2.25572899e-06
Iter: 1337 loss: 2.23217648e-06
Iter: 1338 loss: 2.22861581e-06
Iter: 1339 loss: 2.24500718e-06
Iter: 1340 loss: 2.22794733e-06
Iter: 1341 loss: 2.22462427e-06
Iter: 1342 loss: 2.23538132e-06
Iter: 1343 loss: 2.22383414e-06
Iter: 1344 loss: 2.22108201e-06
Iter: 1345 loss: 2.21766049e-06
Iter: 1346 loss: 2.21743653e-06
Iter: 1347 loss: 2.21406253e-06
Iter: 1348 loss: 2.21395248e-06
Iter: 1349 loss: 2.21136611e-06
Iter: 1350 loss: 2.21001164e-06
Iter: 1351 loss: 2.20893935e-06
Iter: 1352 loss: 2.20554284e-06
Iter: 1353 loss: 2.2231302e-06
Iter: 1354 loss: 2.20492166e-06
Iter: 1355 loss: 2.20145512e-06
Iter: 1356 loss: 2.21018263e-06
Iter: 1357 loss: 2.20015409e-06
Iter: 1358 loss: 2.19741582e-06
Iter: 1359 loss: 2.19757862e-06
Iter: 1360 loss: 2.19515846e-06
Iter: 1361 loss: 2.19272329e-06
Iter: 1362 loss: 2.19263165e-06
Iter: 1363 loss: 2.19036338e-06
Iter: 1364 loss: 2.18752e-06
Iter: 1365 loss: 2.18728565e-06
Iter: 1366 loss: 2.18376385e-06
Iter: 1367 loss: 2.19066396e-06
Iter: 1368 loss: 2.18228388e-06
Iter: 1369 loss: 2.17895e-06
Iter: 1370 loss: 2.21209302e-06
Iter: 1371 loss: 2.1788328e-06
Iter: 1372 loss: 2.17643128e-06
Iter: 1373 loss: 2.17624779e-06
Iter: 1374 loss: 2.17446905e-06
Iter: 1375 loss: 2.17087381e-06
Iter: 1376 loss: 2.17201159e-06
Iter: 1377 loss: 2.16829176e-06
Iter: 1378 loss: 2.16423382e-06
Iter: 1379 loss: 2.19155709e-06
Iter: 1380 loss: 2.16373269e-06
Iter: 1381 loss: 2.16052968e-06
Iter: 1382 loss: 2.18363948e-06
Iter: 1383 loss: 2.16029275e-06
Iter: 1384 loss: 2.15731916e-06
Iter: 1385 loss: 2.1563942e-06
Iter: 1386 loss: 2.15467344e-06
Iter: 1387 loss: 2.15170212e-06
Iter: 1388 loss: 2.18182777e-06
Iter: 1389 loss: 2.151628e-06
Iter: 1390 loss: 2.14899114e-06
Iter: 1391 loss: 2.14967213e-06
Iter: 1392 loss: 2.14705778e-06
Iter: 1393 loss: 2.14398437e-06
Iter: 1394 loss: 2.15347359e-06
Iter: 1395 loss: 2.14304691e-06
Iter: 1396 loss: 2.1405267e-06
Iter: 1397 loss: 2.16941044e-06
Iter: 1398 loss: 2.14048305e-06
Iter: 1399 loss: 2.13825206e-06
Iter: 1400 loss: 2.13544e-06
Iter: 1401 loss: 2.13526982e-06
Iter: 1402 loss: 2.13196063e-06
Iter: 1403 loss: 2.1770004e-06
Iter: 1404 loss: 2.13194562e-06
Iter: 1405 loss: 2.12967734e-06
Iter: 1406 loss: 2.1269268e-06
Iter: 1407 loss: 2.12672103e-06
Iter: 1408 loss: 2.12320197e-06
Iter: 1409 loss: 2.12914574e-06
Iter: 1410 loss: 2.12168948e-06
Iter: 1411 loss: 2.1179776e-06
Iter: 1412 loss: 2.15502132e-06
Iter: 1413 loss: 2.11786391e-06
Iter: 1414 loss: 2.11472707e-06
Iter: 1415 loss: 2.11664315e-06
Iter: 1416 loss: 2.11270299e-06
Iter: 1417 loss: 2.10940789e-06
Iter: 1418 loss: 2.11659881e-06
Iter: 1419 loss: 2.10811368e-06
Iter: 1420 loss: 2.10483472e-06
Iter: 1421 loss: 2.10917347e-06
Iter: 1422 loss: 2.1031592e-06
Iter: 1423 loss: 2.09981044e-06
Iter: 1424 loss: 2.1373246e-06
Iter: 1425 loss: 2.09979953e-06
Iter: 1426 loss: 2.09712516e-06
Iter: 1427 loss: 2.10068356e-06
Iter: 1428 loss: 2.09577865e-06
Iter: 1429 loss: 2.09301015e-06
Iter: 1430 loss: 2.09968061e-06
Iter: 1431 loss: 2.09196969e-06
Iter: 1432 loss: 2.08829852e-06
Iter: 1433 loss: 2.09947166e-06
Iter: 1434 loss: 2.08717893e-06
Iter: 1435 loss: 2.0846303e-06
Iter: 1436 loss: 2.09736731e-06
Iter: 1437 loss: 2.08414258e-06
Iter: 1438 loss: 2.08175516e-06
Iter: 1439 loss: 2.08666461e-06
Iter: 1440 loss: 2.08078404e-06
Iter: 1441 loss: 2.0784405e-06
Iter: 1442 loss: 2.07617245e-06
Iter: 1443 loss: 2.07560129e-06
Iter: 1444 loss: 2.07181256e-06
Iter: 1445 loss: 2.08393203e-06
Iter: 1446 loss: 2.07071798e-06
Iter: 1447 loss: 2.0678649e-06
Iter: 1448 loss: 2.10153576e-06
Iter: 1449 loss: 2.06780533e-06
Iter: 1450 loss: 2.06502955e-06
Iter: 1451 loss: 2.06382515e-06
Iter: 1452 loss: 2.06246409e-06
Iter: 1453 loss: 2.05860351e-06
Iter: 1454 loss: 2.06738582e-06
Iter: 1455 loss: 2.05708784e-06
Iter: 1456 loss: 2.05458286e-06
Iter: 1457 loss: 2.08317988e-06
Iter: 1458 loss: 2.05445349e-06
Iter: 1459 loss: 2.05171182e-06
Iter: 1460 loss: 2.0510422e-06
Iter: 1461 loss: 2.04932303e-06
Iter: 1462 loss: 2.04585217e-06
Iter: 1463 loss: 2.05565766e-06
Iter: 1464 loss: 2.04469916e-06
Iter: 1465 loss: 2.04177491e-06
Iter: 1466 loss: 2.06032018e-06
Iter: 1467 loss: 2.04136472e-06
Iter: 1468 loss: 2.03829836e-06
Iter: 1469 loss: 2.0448233e-06
Iter: 1470 loss: 2.03706122e-06
Iter: 1471 loss: 2.03426384e-06
Iter: 1472 loss: 2.05079482e-06
Iter: 1473 loss: 2.03386617e-06
Iter: 1474 loss: 2.03157606e-06
Iter: 1475 loss: 2.03251238e-06
Iter: 1476 loss: 2.02995579e-06
Iter: 1477 loss: 2.02734736e-06
Iter: 1478 loss: 2.02904175e-06
Iter: 1479 loss: 2.02571937e-06
Iter: 1480 loss: 2.0224893e-06
Iter: 1481 loss: 2.0521295e-06
Iter: 1482 loss: 2.02227693e-06
Iter: 1483 loss: 2.01968396e-06
Iter: 1484 loss: 2.02003548e-06
Iter: 1485 loss: 2.01767034e-06
Iter: 1486 loss: 2.01455441e-06
Iter: 1487 loss: 2.01308694e-06
Iter: 1488 loss: 2.01153034e-06
Iter: 1489 loss: 2.0081261e-06
Iter: 1490 loss: 2.06330628e-06
Iter: 1491 loss: 2.00812565e-06
Iter: 1492 loss: 2.00519617e-06
Iter: 1493 loss: 2.01281182e-06
Iter: 1494 loss: 2.00428076e-06
Iter: 1495 loss: 2.0012767e-06
Iter: 1496 loss: 2.0004486e-06
Iter: 1497 loss: 1.99860324e-06
Iter: 1498 loss: 1.995189e-06
Iter: 1499 loss: 2.00312661e-06
Iter: 1500 loss: 1.99389069e-06
Iter: 1501 loss: 1.99116539e-06
Iter: 1502 loss: 1.99115016e-06
Iter: 1503 loss: 1.9890017e-06
Iter: 1504 loss: 1.98949419e-06
Iter: 1505 loss: 1.98747512e-06
Iter: 1506 loss: 1.98425869e-06
Iter: 1507 loss: 1.9973379e-06
Iter: 1508 loss: 1.98348471e-06
Iter: 1509 loss: 1.98087082e-06
Iter: 1510 loss: 1.98076441e-06
Iter: 1511 loss: 1.97873101e-06
Iter: 1512 loss: 1.97562895e-06
Iter: 1513 loss: 1.99327087e-06
Iter: 1514 loss: 1.97514214e-06
Iter: 1515 loss: 1.97224699e-06
Iter: 1516 loss: 1.97975669e-06
Iter: 1517 loss: 1.97122563e-06
Iter: 1518 loss: 1.96843575e-06
Iter: 1519 loss: 1.96980636e-06
Iter: 1520 loss: 1.96662631e-06
Iter: 1521 loss: 1.96369092e-06
Iter: 1522 loss: 1.97657778e-06
Iter: 1523 loss: 1.96312931e-06
Iter: 1524 loss: 1.96039105e-06
Iter: 1525 loss: 1.97929012e-06
Iter: 1526 loss: 1.96008523e-06
Iter: 1527 loss: 1.95776374e-06
Iter: 1528 loss: 1.9550946e-06
Iter: 1529 loss: 1.95475468e-06
Iter: 1530 loss: 1.95151e-06
Iter: 1531 loss: 1.96720475e-06
Iter: 1532 loss: 1.95088728e-06
Iter: 1533 loss: 1.94757013e-06
Iter: 1534 loss: 1.96899964e-06
Iter: 1535 loss: 1.9472418e-06
Iter: 1536 loss: 1.94491486e-06
Iter: 1537 loss: 1.94602626e-06
Iter: 1538 loss: 1.94335803e-06
Iter: 1539 loss: 1.94110089e-06
Iter: 1540 loss: 1.95652092e-06
Iter: 1541 loss: 1.94089762e-06
Iter: 1542 loss: 1.9384097e-06
Iter: 1543 loss: 1.94463018e-06
Iter: 1544 loss: 1.93749293e-06
Iter: 1545 loss: 1.93538381e-06
Iter: 1546 loss: 1.93462779e-06
Iter: 1547 loss: 1.93341725e-06
Iter: 1548 loss: 1.93038022e-06
Iter: 1549 loss: 1.95085249e-06
Iter: 1550 loss: 1.93003962e-06
Iter: 1551 loss: 1.92760285e-06
Iter: 1552 loss: 1.9277104e-06
Iter: 1553 loss: 1.92566335e-06
Iter: 1554 loss: 1.92220864e-06
Iter: 1555 loss: 1.92466223e-06
Iter: 1556 loss: 1.92002699e-06
Iter: 1557 loss: 1.91658341e-06
Iter: 1558 loss: 1.96366113e-06
Iter: 1559 loss: 1.91655772e-06
Iter: 1560 loss: 1.91399522e-06
Iter: 1561 loss: 1.92241941e-06
Iter: 1562 loss: 1.91330219e-06
Iter: 1563 loss: 1.91118124e-06
Iter: 1564 loss: 1.90948595e-06
Iter: 1565 loss: 1.90882906e-06
Iter: 1566 loss: 1.90554624e-06
Iter: 1567 loss: 1.92042e-06
Iter: 1568 loss: 1.90489777e-06
Iter: 1569 loss: 1.9018596e-06
Iter: 1570 loss: 1.92178868e-06
Iter: 1571 loss: 1.90161973e-06
Iter: 1572 loss: 1.89903142e-06
Iter: 1573 loss: 1.89745015e-06
Iter: 1574 loss: 1.89639e-06
Iter: 1575 loss: 1.89413322e-06
Iter: 1576 loss: 1.89411446e-06
Iter: 1577 loss: 1.89175501e-06
Iter: 1578 loss: 1.89246953e-06
Iter: 1579 loss: 1.89010939e-06
Iter: 1580 loss: 1.88791057e-06
Iter: 1581 loss: 1.88900174e-06
Iter: 1582 loss: 1.88635886e-06
Iter: 1583 loss: 1.88338174e-06
Iter: 1584 loss: 1.90227411e-06
Iter: 1585 loss: 1.8830666e-06
Iter: 1586 loss: 1.88055026e-06
Iter: 1587 loss: 1.88038121e-06
Iter: 1588 loss: 1.87846797e-06
Iter: 1589 loss: 1.87569265e-06
Iter: 1590 loss: 1.89070602e-06
Iter: 1591 loss: 1.87526587e-06
Iter: 1592 loss: 1.87228045e-06
Iter: 1593 loss: 1.8813746e-06
Iter: 1594 loss: 1.87135731e-06
Iter: 1595 loss: 1.86875127e-06
Iter: 1596 loss: 1.87249043e-06
Iter: 1597 loss: 1.86753482e-06
Iter: 1598 loss: 1.86487864e-06
Iter: 1599 loss: 1.86833881e-06
Iter: 1600 loss: 1.86357443e-06
Iter: 1601 loss: 1.8609968e-06
Iter: 1602 loss: 1.89669754e-06
Iter: 1603 loss: 1.86099965e-06
Iter: 1604 loss: 1.85914689e-06
Iter: 1605 loss: 1.85641238e-06
Iter: 1606 loss: 1.8562971e-06
Iter: 1607 loss: 1.85307556e-06
Iter: 1608 loss: 1.87001774e-06
Iter: 1609 loss: 1.8525601e-06
Iter: 1610 loss: 1.85003478e-06
Iter: 1611 loss: 1.87567798e-06
Iter: 1612 loss: 1.84991518e-06
Iter: 1613 loss: 1.84778764e-06
Iter: 1614 loss: 1.84898363e-06
Iter: 1615 loss: 1.84641897e-06
Iter: 1616 loss: 1.84392889e-06
Iter: 1617 loss: 1.85561271e-06
Iter: 1618 loss: 1.84351597e-06
Iter: 1619 loss: 1.84126361e-06
Iter: 1620 loss: 1.84055648e-06
Iter: 1621 loss: 1.83920019e-06
Iter: 1622 loss: 1.83647876e-06
Iter: 1623 loss: 1.83963198e-06
Iter: 1624 loss: 1.83496832e-06
Iter: 1625 loss: 1.83213172e-06
Iter: 1626 loss: 1.86441889e-06
Iter: 1627 loss: 1.83205509e-06
Iter: 1628 loss: 1.82964311e-06
Iter: 1629 loss: 1.82990914e-06
Iter: 1630 loss: 1.82782856e-06
Iter: 1631 loss: 1.82478789e-06
Iter: 1632 loss: 1.82912709e-06
Iter: 1633 loss: 1.82325687e-06
Iter: 1634 loss: 1.82071744e-06
Iter: 1635 loss: 1.85187923e-06
Iter: 1636 loss: 1.82072506e-06
Iter: 1637 loss: 1.81840164e-06
Iter: 1638 loss: 1.82139024e-06
Iter: 1639 loss: 1.81725318e-06
Iter: 1640 loss: 1.81469579e-06
Iter: 1641 loss: 1.81401811e-06
Iter: 1642 loss: 1.81250061e-06
Iter: 1643 loss: 1.80975428e-06
Iter: 1644 loss: 1.83514533e-06
Iter: 1645 loss: 1.80968073e-06
Iter: 1646 loss: 1.80708378e-06
Iter: 1647 loss: 1.81787777e-06
Iter: 1648 loss: 1.80653126e-06
Iter: 1649 loss: 1.80448501e-06
Iter: 1650 loss: 1.80784377e-06
Iter: 1651 loss: 1.80365373e-06
Iter: 1652 loss: 1.8011084e-06
Iter: 1653 loss: 1.80689233e-06
Iter: 1654 loss: 1.80018094e-06
Iter: 1655 loss: 1.79765107e-06
Iter: 1656 loss: 1.79850349e-06
Iter: 1657 loss: 1.79589847e-06
Iter: 1658 loss: 1.79348251e-06
Iter: 1659 loss: 1.80794154e-06
Iter: 1660 loss: 1.79313281e-06
Iter: 1661 loss: 1.79076756e-06
Iter: 1662 loss: 1.79663789e-06
Iter: 1663 loss: 1.78993787e-06
Iter: 1664 loss: 1.78774644e-06
Iter: 1665 loss: 1.78878577e-06
Iter: 1666 loss: 1.78636765e-06
Iter: 1667 loss: 1.78363643e-06
Iter: 1668 loss: 1.79217261e-06
Iter: 1669 loss: 1.78288678e-06
Iter: 1670 loss: 1.78009327e-06
Iter: 1671 loss: 1.79891754e-06
Iter: 1672 loss: 1.77978916e-06
Iter: 1673 loss: 1.77788161e-06
Iter: 1674 loss: 1.77490165e-06
Iter: 1675 loss: 1.77489846e-06
Iter: 1676 loss: 1.77170364e-06
Iter: 1677 loss: 1.80357506e-06
Iter: 1678 loss: 1.77164293e-06
Iter: 1679 loss: 1.76915364e-06
Iter: 1680 loss: 1.78777066e-06
Iter: 1681 loss: 1.76899823e-06
Iter: 1682 loss: 1.76702724e-06
Iter: 1683 loss: 1.76663593e-06
Iter: 1684 loss: 1.76533456e-06
Iter: 1685 loss: 1.76350943e-06
Iter: 1686 loss: 1.76355434e-06
Iter: 1687 loss: 1.76220613e-06
Iter: 1688 loss: 1.75968603e-06
Iter: 1689 loss: 1.8169336e-06
Iter: 1690 loss: 1.75970513e-06
Iter: 1691 loss: 1.75667651e-06
Iter: 1692 loss: 1.76692606e-06
Iter: 1693 loss: 1.75585512e-06
Iter: 1694 loss: 1.75365562e-06
Iter: 1695 loss: 1.76950675e-06
Iter: 1696 loss: 1.75346349e-06
Iter: 1697 loss: 1.75107152e-06
Iter: 1698 loss: 1.75061336e-06
Iter: 1699 loss: 1.74901038e-06
Iter: 1700 loss: 1.74601485e-06
Iter: 1701 loss: 1.75205469e-06
Iter: 1702 loss: 1.74481499e-06
Iter: 1703 loss: 1.74213619e-06
Iter: 1704 loss: 1.76836511e-06
Iter: 1705 loss: 1.74202387e-06
Iter: 1706 loss: 1.73962621e-06
Iter: 1707 loss: 1.74190427e-06
Iter: 1708 loss: 1.73825492e-06
Iter: 1709 loss: 1.7359082e-06
Iter: 1710 loss: 1.73937303e-06
Iter: 1711 loss: 1.73470664e-06
Iter: 1712 loss: 1.73213914e-06
Iter: 1713 loss: 1.75577861e-06
Iter: 1714 loss: 1.73209355e-06
Iter: 1715 loss: 1.7300182e-06
Iter: 1716 loss: 1.7291145e-06
Iter: 1717 loss: 1.72805574e-06
Iter: 1718 loss: 1.72600573e-06
Iter: 1719 loss: 1.74811066e-06
Iter: 1720 loss: 1.72597026e-06
Iter: 1721 loss: 1.72377099e-06
Iter: 1722 loss: 1.72648254e-06
Iter: 1723 loss: 1.72268017e-06
Iter: 1724 loss: 1.72064142e-06
Iter: 1725 loss: 1.71987949e-06
Iter: 1726 loss: 1.71869613e-06
Iter: 1727 loss: 1.71630745e-06
Iter: 1728 loss: 1.71631768e-06
Iter: 1729 loss: 1.71458339e-06
Iter: 1730 loss: 1.71228203e-06
Iter: 1731 loss: 1.71214651e-06
Iter: 1732 loss: 1.70916553e-06
Iter: 1733 loss: 1.71797478e-06
Iter: 1734 loss: 1.70826763e-06
Iter: 1735 loss: 1.70531075e-06
Iter: 1736 loss: 1.7145486e-06
Iter: 1737 loss: 1.70440831e-06
Iter: 1738 loss: 1.70241094e-06
Iter: 1739 loss: 1.7299916e-06
Iter: 1740 loss: 1.70240241e-06
Iter: 1741 loss: 1.70068847e-06
Iter: 1742 loss: 1.6977458e-06
Iter: 1743 loss: 1.69777775e-06
Iter: 1744 loss: 1.6948477e-06
Iter: 1745 loss: 1.72535385e-06
Iter: 1746 loss: 1.69475163e-06
Iter: 1747 loss: 1.69249097e-06
Iter: 1748 loss: 1.7026241e-06
Iter: 1749 loss: 1.69201007e-06
Iter: 1750 loss: 1.68984798e-06
Iter: 1751 loss: 1.68810334e-06
Iter: 1752 loss: 1.68754764e-06
Iter: 1753 loss: 1.68580209e-06
Iter: 1754 loss: 1.68566191e-06
Iter: 1755 loss: 1.6839424e-06
Iter: 1756 loss: 1.68203849e-06
Iter: 1757 loss: 1.68177644e-06
Iter: 1758 loss: 1.67975145e-06
Iter: 1759 loss: 1.69592352e-06
Iter: 1760 loss: 1.67958819e-06
Iter: 1761 loss: 1.67767905e-06
Iter: 1762 loss: 1.67890164e-06
Iter: 1763 loss: 1.6765141e-06
Iter: 1764 loss: 1.67434314e-06
Iter: 1765 loss: 1.67752e-06
Iter: 1766 loss: 1.67332234e-06
Iter: 1767 loss: 1.67103053e-06
Iter: 1768 loss: 1.68817201e-06
Iter: 1769 loss: 1.67089615e-06
Iter: 1770 loss: 1.66879147e-06
Iter: 1771 loss: 1.66836537e-06
Iter: 1772 loss: 1.66699579e-06
Iter: 1773 loss: 1.6642565e-06
Iter: 1774 loss: 1.6664336e-06
Iter: 1775 loss: 1.66260645e-06
Iter: 1776 loss: 1.65962183e-06
Iter: 1777 loss: 1.6723053e-06
Iter: 1778 loss: 1.65902907e-06
Iter: 1779 loss: 1.65627534e-06
Iter: 1780 loss: 1.66893437e-06
Iter: 1781 loss: 1.65574772e-06
Iter: 1782 loss: 1.65336371e-06
Iter: 1783 loss: 1.67283463e-06
Iter: 1784 loss: 1.65321205e-06
Iter: 1785 loss: 1.65179699e-06
Iter: 1786 loss: 1.65098072e-06
Iter: 1787 loss: 1.65036033e-06
Iter: 1788 loss: 1.64814821e-06
Iter: 1789 loss: 1.66905397e-06
Iter: 1790 loss: 1.64801713e-06
Iter: 1791 loss: 1.64609366e-06
Iter: 1792 loss: 1.64554012e-06
Iter: 1793 loss: 1.64434846e-06
Iter: 1794 loss: 1.64217238e-06
Iter: 1795 loss: 1.66042344e-06
Iter: 1796 loss: 1.64209587e-06
Iter: 1797 loss: 1.64029916e-06
Iter: 1798 loss: 1.63827053e-06
Iter: 1799 loss: 1.63807272e-06
Iter: 1800 loss: 1.63557581e-06
Iter: 1801 loss: 1.65107986e-06
Iter: 1802 loss: 1.63523566e-06
Iter: 1803 loss: 1.63264713e-06
Iter: 1804 loss: 1.64405765e-06
Iter: 1805 loss: 1.6321294e-06
Iter: 1806 loss: 1.6303477e-06
Iter: 1807 loss: 1.62883646e-06
Iter: 1808 loss: 1.62831066e-06
Iter: 1809 loss: 1.62601327e-06
Iter: 1810 loss: 1.65067445e-06
Iter: 1811 loss: 1.62594336e-06
Iter: 1812 loss: 1.62380707e-06
Iter: 1813 loss: 1.62758863e-06
Iter: 1814 loss: 1.62286267e-06
Iter: 1815 loss: 1.62079914e-06
Iter: 1816 loss: 1.62243578e-06
Iter: 1817 loss: 1.61952732e-06
Iter: 1818 loss: 1.61710318e-06
Iter: 1819 loss: 1.62099104e-06
Iter: 1820 loss: 1.61588719e-06
Iter: 1821 loss: 1.61343041e-06
Iter: 1822 loss: 1.63590289e-06
Iter: 1823 loss: 1.61332946e-06
Iter: 1824 loss: 1.61089201e-06
Iter: 1825 loss: 1.61692606e-06
Iter: 1826 loss: 1.61002345e-06
Iter: 1827 loss: 1.60823902e-06
Iter: 1828 loss: 1.61360367e-06
Iter: 1829 loss: 1.60769252e-06
Iter: 1830 loss: 1.6054463e-06
Iter: 1831 loss: 1.60805132e-06
Iter: 1832 loss: 1.60437628e-06
Iter: 1833 loss: 1.60225545e-06
Iter: 1834 loss: 1.6023165e-06
Iter: 1835 loss: 1.60052161e-06
Iter: 1836 loss: 1.59812362e-06
Iter: 1837 loss: 1.61516255e-06
Iter: 1838 loss: 1.59789602e-06
Iter: 1839 loss: 1.59583055e-06
Iter: 1840 loss: 1.60712011e-06
Iter: 1841 loss: 1.59553974e-06
Iter: 1842 loss: 1.59382307e-06
Iter: 1843 loss: 1.59221145e-06
Iter: 1844 loss: 1.59183537e-06
Iter: 1845 loss: 1.58924763e-06
Iter: 1846 loss: 1.61735102e-06
Iter: 1847 loss: 1.58916532e-06
Iter: 1848 loss: 1.58712714e-06
Iter: 1849 loss: 1.58570481e-06
Iter: 1850 loss: 1.58492321e-06
Iter: 1851 loss: 1.58243165e-06
Iter: 1852 loss: 1.59047272e-06
Iter: 1853 loss: 1.58174646e-06
Iter: 1854 loss: 1.57915383e-06
Iter: 1855 loss: 1.58403589e-06
Iter: 1856 loss: 1.57796899e-06
Iter: 1857 loss: 1.57634327e-06
Iter: 1858 loss: 1.57619695e-06
Iter: 1859 loss: 1.5749207e-06
Iter: 1860 loss: 1.57474415e-06
Iter: 1861 loss: 1.57386728e-06
Iter: 1862 loss: 1.57182944e-06
Iter: 1863 loss: 1.57751742e-06
Iter: 1864 loss: 1.57118336e-06
Iter: 1865 loss: 1.56897613e-06
Iter: 1866 loss: 1.57185707e-06
Iter: 1867 loss: 1.56781448e-06
Iter: 1868 loss: 1.56586361e-06
Iter: 1869 loss: 1.56849183e-06
Iter: 1870 loss: 1.56495707e-06
Iter: 1871 loss: 1.56280146e-06
Iter: 1872 loss: 1.58074636e-06
Iter: 1873 loss: 1.56263434e-06
Iter: 1874 loss: 1.56098645e-06
Iter: 1875 loss: 1.55978921e-06
Iter: 1876 loss: 1.5591861e-06
Iter: 1877 loss: 1.55667158e-06
Iter: 1878 loss: 1.56455803e-06
Iter: 1879 loss: 1.55590419e-06
Iter: 1880 loss: 1.55329758e-06
Iter: 1881 loss: 1.57630609e-06
Iter: 1882 loss: 1.55320959e-06
Iter: 1883 loss: 1.55169027e-06
Iter: 1884 loss: 1.55062298e-06
Iter: 1885 loss: 1.55015664e-06
Iter: 1886 loss: 1.54790723e-06
Iter: 1887 loss: 1.5564151e-06
Iter: 1888 loss: 1.54746283e-06
Iter: 1889 loss: 1.54503755e-06
Iter: 1890 loss: 1.55364069e-06
Iter: 1891 loss: 1.54443785e-06
Iter: 1892 loss: 1.54213762e-06
Iter: 1893 loss: 1.5449632e-06
Iter: 1894 loss: 1.54094027e-06
Iter: 1895 loss: 1.53912777e-06
Iter: 1896 loss: 1.5566402e-06
Iter: 1897 loss: 1.53903466e-06
Iter: 1898 loss: 1.5371279e-06
Iter: 1899 loss: 1.53878182e-06
Iter: 1900 loss: 1.53599558e-06
Iter: 1901 loss: 1.53394785e-06
Iter: 1902 loss: 1.53183237e-06
Iter: 1903 loss: 1.53148017e-06
Iter: 1904 loss: 1.52942732e-06
Iter: 1905 loss: 1.52938469e-06
Iter: 1906 loss: 1.52761118e-06
Iter: 1907 loss: 1.52954362e-06
Iter: 1908 loss: 1.52663824e-06
Iter: 1909 loss: 1.52458597e-06
Iter: 1910 loss: 1.52490838e-06
Iter: 1911 loss: 1.52304187e-06
Iter: 1912 loss: 1.52074699e-06
Iter: 1913 loss: 1.53562155e-06
Iter: 1914 loss: 1.52052621e-06
Iter: 1915 loss: 1.51846018e-06
Iter: 1916 loss: 1.52845359e-06
Iter: 1917 loss: 1.51810264e-06
Iter: 1918 loss: 1.51640711e-06
Iter: 1919 loss: 1.51534152e-06
Iter: 1920 loss: 1.51466543e-06
Iter: 1921 loss: 1.51230802e-06
Iter: 1922 loss: 1.52121095e-06
Iter: 1923 loss: 1.51174277e-06
Iter: 1924 loss: 1.50964456e-06
Iter: 1925 loss: 1.52633902e-06
Iter: 1926 loss: 1.50947767e-06
Iter: 1927 loss: 1.50777646e-06
Iter: 1928 loss: 1.50747792e-06
Iter: 1929 loss: 1.50639e-06
Iter: 1930 loss: 1.50434482e-06
Iter: 1931 loss: 1.52428424e-06
Iter: 1932 loss: 1.50426581e-06
Iter: 1933 loss: 1.50243568e-06
Iter: 1934 loss: 1.50697224e-06
Iter: 1935 loss: 1.50182723e-06
Iter: 1936 loss: 1.50044639e-06
Iter: 1937 loss: 1.49962875e-06
Iter: 1938 loss: 1.49905532e-06
Iter: 1939 loss: 1.49704681e-06
Iter: 1940 loss: 1.51747179e-06
Iter: 1941 loss: 1.49702419e-06
Iter: 1942 loss: 1.49567472e-06
Iter: 1943 loss: 1.4940282e-06
Iter: 1944 loss: 1.49383982e-06
Iter: 1945 loss: 1.49161497e-06
Iter: 1946 loss: 1.50023652e-06
Iter: 1947 loss: 1.49105085e-06
Iter: 1948 loss: 1.4888833e-06
Iter: 1949 loss: 1.50953292e-06
Iter: 1950 loss: 1.48875677e-06
Iter: 1951 loss: 1.4873433e-06
Iter: 1952 loss: 1.48576225e-06
Iter: 1953 loss: 1.48552704e-06
Iter: 1954 loss: 1.48312097e-06
Iter: 1955 loss: 1.49339951e-06
Iter: 1956 loss: 1.48256549e-06
Iter: 1957 loss: 1.48052379e-06
Iter: 1958 loss: 1.49920436e-06
Iter: 1959 loss: 1.48042545e-06
Iter: 1960 loss: 1.47879007e-06
Iter: 1961 loss: 1.47774904e-06
Iter: 1962 loss: 1.47705316e-06
Iter: 1963 loss: 1.47493256e-06
Iter: 1964 loss: 1.48161098e-06
Iter: 1965 loss: 1.47429535e-06
Iter: 1966 loss: 1.47265393e-06
Iter: 1967 loss: 1.4908519e-06
Iter: 1968 loss: 1.47254718e-06
Iter: 1969 loss: 1.47107028e-06
Iter: 1970 loss: 1.47379512e-06
Iter: 1971 loss: 1.47027913e-06
Iter: 1972 loss: 1.46891239e-06
Iter: 1973 loss: 1.47258038e-06
Iter: 1974 loss: 1.46845559e-06
Iter: 1975 loss: 1.4667055e-06
Iter: 1976 loss: 1.46643015e-06
Iter: 1977 loss: 1.46522893e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2.4
+ date
Wed Nov  4 15:13:57 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2/300_300_300_1 --function f2 --psi 1 --alpha 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fe2a551e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fe2a55ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fe298d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fe2974620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fe296c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fbc07f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fe2962ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fe29da620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fe29da400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fe29b98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fe29b6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f347ba8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f347ba840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fbc05e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fbc059d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa00ae950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa00aebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f347b0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f347436a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f3476e048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa0023598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa0032f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa0032ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f346db730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f346db488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f34645840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f34669268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f346696a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2003fb3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f3462d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f34597048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f34597400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f346b3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f34597598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f345e3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f345c8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.00859899
test_loss: 0.01135699
train_loss: 0.0071537415
test_loss: 0.00992794
train_loss: 0.0067920494
test_loss: 0.009614587
train_loss: 0.006418392
test_loss: 0.0093825925
train_loss: 0.005993435
test_loss: 0.009261269
train_loss: 0.0059124627
test_loss: 0.009104937
train_loss: 0.006565612
test_loss: 0.009199514
train_loss: 0.0067047277
test_loss: 0.009300203
train_loss: 0.0060205436
test_loss: 0.009241609
train_loss: 0.005786396
test_loss: 0.009014659
train_loss: 0.0056831543
test_loss: 0.008891763
train_loss: 0.005910105
test_loss: 0.009020326
train_loss: 0.005791554
test_loss: 0.008799263
train_loss: 0.0060527734
test_loss: 0.00892404
train_loss: 0.005755296
test_loss: 0.008783598
train_loss: 0.00606254
test_loss: 0.008901324
train_loss: 0.00607045
test_loss: 0.008808457
train_loss: 0.0058001224
test_loss: 0.00887084
train_loss: 0.005499848
test_loss: 0.008851784
train_loss: 0.0059825415
test_loss: 0.008821676
train_loss: 0.005684865
test_loss: 0.008955136
train_loss: 0.0053463876
test_loss: 0.008921646
train_loss: 0.005850679
test_loss: 0.008845377
train_loss: 0.0050657773
test_loss: 0.00856116
train_loss: 0.005500543
test_loss: 0.008665084
train_loss: 0.0054583405
test_loss: 0.008801212
train_loss: 0.0058482755
test_loss: 0.008649398
train_loss: 0.005987093
test_loss: 0.008678217
train_loss: 0.0053662583
test_loss: 0.008779942
train_loss: 0.0053809406
test_loss: 0.008680848
train_loss: 0.005208497
test_loss: 0.008553251
train_loss: 0.0053634313
test_loss: 0.008686742
train_loss: 0.00525963
test_loss: 0.008733894
train_loss: 0.005904748
test_loss: 0.008680354
train_loss: 0.005635682
test_loss: 0.009029809
train_loss: 0.0052482514
test_loss: 0.008673474
train_loss: 0.0054749115
test_loss: 0.008742485
train_loss: 0.0054329885
test_loss: 0.008905735
train_loss: 0.004844471
test_loss: 0.008624154
train_loss: 0.005057159
test_loss: 0.008531621
train_loss: 0.0050065382
test_loss: 0.008445839
train_loss: 0.005225124
test_loss: 0.008510015
train_loss: 0.004909605
test_loss: 0.008650308
train_loss: 0.005280785
test_loss: 0.0088185305
train_loss: 0.005633884
test_loss: 0.00870098
train_loss: 0.0051577324
test_loss: 0.008639603
train_loss: 0.0048604803
test_loss: 0.008509765
train_loss: 0.005077259
test_loss: 0.008422833
train_loss: 0.005123265
test_loss: 0.008548901
train_loss: 0.005163802
test_loss: 0.008429827
train_loss: 0.005562539
test_loss: 0.008560681
train_loss: 0.0053818524
test_loss: 0.008515071
train_loss: 0.005272132
test_loss: 0.008663682
train_loss: 0.005226325
test_loss: 0.008425367
train_loss: 0.0051360833
test_loss: 0.008534074
train_loss: 0.0054793046
test_loss: 0.008554285
train_loss: 0.004961705
test_loss: 0.008444165
train_loss: 0.0052356524
test_loss: 0.008598607
train_loss: 0.0053038886
test_loss: 0.008763007
train_loss: 0.004984317
test_loss: 0.0087542515
train_loss: 0.005158901
test_loss: 0.008591374
train_loss: 0.0050099534
test_loss: 0.00844824
train_loss: 0.0051801894
test_loss: 0.008498339
train_loss: 0.0049512233
test_loss: 0.008782983
train_loss: 0.0046475423
test_loss: 0.008458283
train_loss: 0.0051861973
test_loss: 0.008498677
train_loss: 0.005128934
test_loss: 0.0086260475
train_loss: 0.0051463563
test_loss: 0.008465854
train_loss: 0.00496794
test_loss: 0.008609104
train_loss: 0.005062096
test_loss: 0.008482965
train_loss: 0.0047430163
test_loss: 0.008453468
train_loss: 0.004909987
test_loss: 0.00854685
train_loss: 0.005007387
test_loss: 0.008567429
train_loss: 0.0048050513
test_loss: 0.008290534
train_loss: 0.005036205
test_loss: 0.00833251
train_loss: 0.0049209227
test_loss: 0.008466532
train_loss: 0.005383833
test_loss: 0.008557085
train_loss: 0.004971255
test_loss: 0.008312449
train_loss: 0.004845447
test_loss: 0.008348667
train_loss: 0.004991997
test_loss: 0.008505992
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.4/300_300_300_1 --optimizer lbfgs --function f2 --psi 1 --alpha 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd65ecfbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd65ec91950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd65ec63598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd65ec0a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd65ec78840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd65ec3c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd65ebf2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd621f0d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd621f1c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd621f1c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd621ec16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd621f0d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd621e7ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd621e9f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd621e4fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd621df47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd621df4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc5457b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc508950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc545730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc545ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc4c02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc545950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc428598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc44f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc409a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc414400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc409950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc3c32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc3c36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc409048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc35b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc354b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc3867b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc354d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd5fc2c7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.29170032e-05
Iter: 2 loss: 3.78992263e-05
Iter: 3 loss: 4.21819095e-05
Iter: 4 loss: 3.49155525e-05
Iter: 5 loss: 3.10593896e-05
Iter: 6 loss: 3.94167655e-05
Iter: 7 loss: 2.9568022e-05
Iter: 8 loss: 2.66029219e-05
Iter: 9 loss: 3.96556497e-05
Iter: 10 loss: 2.6013462e-05
Iter: 11 loss: 2.37623699e-05
Iter: 12 loss: 3.70589187e-05
Iter: 13 loss: 2.34718609e-05
Iter: 14 loss: 2.22278941e-05
Iter: 15 loss: 2.28264471e-05
Iter: 16 loss: 2.13904714e-05
Iter: 17 loss: 1.99525984e-05
Iter: 18 loss: 2.66360694e-05
Iter: 19 loss: 1.96884284e-05
Iter: 20 loss: 1.8527051e-05
Iter: 21 loss: 2.07793837e-05
Iter: 22 loss: 1.80427178e-05
Iter: 23 loss: 1.69530977e-05
Iter: 24 loss: 2.35027219e-05
Iter: 25 loss: 1.68167116e-05
Iter: 26 loss: 1.59646861e-05
Iter: 27 loss: 1.79865474e-05
Iter: 28 loss: 1.56555943e-05
Iter: 29 loss: 1.49557109e-05
Iter: 30 loss: 1.58495532e-05
Iter: 31 loss: 1.45959093e-05
Iter: 32 loss: 1.41165601e-05
Iter: 33 loss: 1.41159671e-05
Iter: 34 loss: 1.37509551e-05
Iter: 35 loss: 1.351434e-05
Iter: 36 loss: 1.33737549e-05
Iter: 37 loss: 1.29361852e-05
Iter: 38 loss: 1.46219572e-05
Iter: 39 loss: 1.28333022e-05
Iter: 40 loss: 1.24915068e-05
Iter: 41 loss: 1.44414425e-05
Iter: 42 loss: 1.24449925e-05
Iter: 43 loss: 1.21128469e-05
Iter: 44 loss: 1.42674371e-05
Iter: 45 loss: 1.20769928e-05
Iter: 46 loss: 1.18688758e-05
Iter: 47 loss: 1.22522397e-05
Iter: 48 loss: 1.1779739e-05
Iter: 49 loss: 1.15558814e-05
Iter: 50 loss: 1.25341685e-05
Iter: 51 loss: 1.15107696e-05
Iter: 52 loss: 1.13300812e-05
Iter: 53 loss: 1.13084225e-05
Iter: 54 loss: 1.11785066e-05
Iter: 55 loss: 1.09607554e-05
Iter: 56 loss: 1.17340423e-05
Iter: 57 loss: 1.09046177e-05
Iter: 58 loss: 1.07151436e-05
Iter: 59 loss: 1.20935765e-05
Iter: 60 loss: 1.0699081e-05
Iter: 61 loss: 1.05441286e-05
Iter: 62 loss: 1.05605268e-05
Iter: 63 loss: 1.04250084e-05
Iter: 64 loss: 1.02654712e-05
Iter: 65 loss: 1.18034268e-05
Iter: 66 loss: 1.02595113e-05
Iter: 67 loss: 1.01340793e-05
Iter: 68 loss: 1.01040114e-05
Iter: 69 loss: 1.00241923e-05
Iter: 70 loss: 9.87248495e-06
Iter: 71 loss: 1.06292391e-05
Iter: 72 loss: 9.84699091e-06
Iter: 73 loss: 9.70974543e-06
Iter: 74 loss: 1.0446216e-05
Iter: 75 loss: 9.68910263e-06
Iter: 76 loss: 9.59283443e-06
Iter: 77 loss: 9.5359228e-06
Iter: 78 loss: 9.49576315e-06
Iter: 79 loss: 9.35488242e-06
Iter: 80 loss: 1.00426469e-05
Iter: 81 loss: 9.33010233e-06
Iter: 82 loss: 9.24859796e-06
Iter: 83 loss: 9.24668711e-06
Iter: 84 loss: 9.18691148e-06
Iter: 85 loss: 9.11497773e-06
Iter: 86 loss: 9.10770177e-06
Iter: 87 loss: 9.01930798e-06
Iter: 88 loss: 9.89026194e-06
Iter: 89 loss: 9.01641579e-06
Iter: 90 loss: 8.9436553e-06
Iter: 91 loss: 8.92936077e-06
Iter: 92 loss: 8.88110208e-06
Iter: 93 loss: 8.79932577e-06
Iter: 94 loss: 8.91769741e-06
Iter: 95 loss: 8.75993828e-06
Iter: 96 loss: 8.6787677e-06
Iter: 97 loss: 9.4119323e-06
Iter: 98 loss: 8.67470226e-06
Iter: 99 loss: 8.60397085e-06
Iter: 100 loss: 8.71988232e-06
Iter: 101 loss: 8.57139548e-06
Iter: 102 loss: 8.50301e-06
Iter: 103 loss: 8.64345384e-06
Iter: 104 loss: 8.47576121e-06
Iter: 105 loss: 8.39813765e-06
Iter: 106 loss: 8.57895884e-06
Iter: 107 loss: 8.36960862e-06
Iter: 108 loss: 8.28503471e-06
Iter: 109 loss: 8.53536221e-06
Iter: 110 loss: 8.25930692e-06
Iter: 111 loss: 8.20301284e-06
Iter: 112 loss: 8.41204201e-06
Iter: 113 loss: 8.18903391e-06
Iter: 114 loss: 8.12303e-06
Iter: 115 loss: 8.24842755e-06
Iter: 116 loss: 8.09504309e-06
Iter: 117 loss: 8.03629518e-06
Iter: 118 loss: 8.05424497e-06
Iter: 119 loss: 7.99427198e-06
Iter: 120 loss: 7.94698826e-06
Iter: 121 loss: 7.94533207e-06
Iter: 122 loss: 7.90072136e-06
Iter: 123 loss: 7.91159437e-06
Iter: 124 loss: 7.86809687e-06
Iter: 125 loss: 7.82736e-06
Iter: 126 loss: 7.94687367e-06
Iter: 127 loss: 7.81470499e-06
Iter: 128 loss: 7.76424e-06
Iter: 129 loss: 7.8709445e-06
Iter: 130 loss: 7.74431919e-06
Iter: 131 loss: 7.69683083e-06
Iter: 132 loss: 7.73515058e-06
Iter: 133 loss: 7.6681581e-06
Iter: 134 loss: 7.61768e-06
Iter: 135 loss: 7.71181294e-06
Iter: 136 loss: 7.59604245e-06
Iter: 137 loss: 7.54640223e-06
Iter: 138 loss: 8.15193562e-06
Iter: 139 loss: 7.5459634e-06
Iter: 140 loss: 7.51280822e-06
Iter: 141 loss: 7.51463813e-06
Iter: 142 loss: 7.48675484e-06
Iter: 143 loss: 7.44137333e-06
Iter: 144 loss: 7.51854168e-06
Iter: 145 loss: 7.42110024e-06
Iter: 146 loss: 7.37638265e-06
Iter: 147 loss: 7.71155101e-06
Iter: 148 loss: 7.37256369e-06
Iter: 149 loss: 7.33596107e-06
Iter: 150 loss: 7.35332605e-06
Iter: 151 loss: 7.3109577e-06
Iter: 152 loss: 7.26754115e-06
Iter: 153 loss: 7.43742112e-06
Iter: 154 loss: 7.25751534e-06
Iter: 155 loss: 7.21514607e-06
Iter: 156 loss: 7.36602306e-06
Iter: 157 loss: 7.20431672e-06
Iter: 158 loss: 7.16796694e-06
Iter: 159 loss: 7.2375251e-06
Iter: 160 loss: 7.15284932e-06
Iter: 161 loss: 7.11825896e-06
Iter: 162 loss: 7.57898488e-06
Iter: 163 loss: 7.11816347e-06
Iter: 164 loss: 7.0981082e-06
Iter: 165 loss: 7.06347691e-06
Iter: 166 loss: 7.06350875e-06
Iter: 167 loss: 7.03218302e-06
Iter: 168 loss: 7.42229258e-06
Iter: 169 loss: 7.03200567e-06
Iter: 170 loss: 7.00133e-06
Iter: 171 loss: 6.99203e-06
Iter: 172 loss: 6.9738071e-06
Iter: 173 loss: 6.94232222e-06
Iter: 174 loss: 6.98291569e-06
Iter: 175 loss: 6.926276e-06
Iter: 176 loss: 6.88779755e-06
Iter: 177 loss: 7.13044756e-06
Iter: 178 loss: 6.88339424e-06
Iter: 179 loss: 6.8517511e-06
Iter: 180 loss: 7.00251212e-06
Iter: 181 loss: 6.84606584e-06
Iter: 182 loss: 6.8196623e-06
Iter: 183 loss: 6.80498943e-06
Iter: 184 loss: 6.79344157e-06
Iter: 185 loss: 6.75953515e-06
Iter: 186 loss: 6.90669503e-06
Iter: 187 loss: 6.75251113e-06
Iter: 188 loss: 6.72009264e-06
Iter: 189 loss: 6.9500411e-06
Iter: 190 loss: 6.71701946e-06
Iter: 191 loss: 6.6958778e-06
Iter: 192 loss: 6.6740663e-06
Iter: 193 loss: 6.6700195e-06
Iter: 194 loss: 6.63877108e-06
Iter: 195 loss: 7.07713343e-06
Iter: 196 loss: 6.6387729e-06
Iter: 197 loss: 6.61744571e-06
Iter: 198 loss: 6.64479376e-06
Iter: 199 loss: 6.60679598e-06
Iter: 200 loss: 6.58145e-06
Iter: 201 loss: 6.76232139e-06
Iter: 202 loss: 6.57930514e-06
Iter: 203 loss: 6.56072916e-06
Iter: 204 loss: 6.53022562e-06
Iter: 205 loss: 6.52999643e-06
Iter: 206 loss: 6.50522907e-06
Iter: 207 loss: 6.86037674e-06
Iter: 208 loss: 6.50518223e-06
Iter: 209 loss: 6.48280547e-06
Iter: 210 loss: 6.51043183e-06
Iter: 211 loss: 6.47109437e-06
Iter: 212 loss: 6.44687952e-06
Iter: 213 loss: 6.43904377e-06
Iter: 214 loss: 6.42488249e-06
Iter: 215 loss: 6.39339851e-06
Iter: 216 loss: 6.61860213e-06
Iter: 217 loss: 6.39074733e-06
Iter: 218 loss: 6.36719415e-06
Iter: 219 loss: 6.51443315e-06
Iter: 220 loss: 6.36443701e-06
Iter: 221 loss: 6.34207845e-06
Iter: 222 loss: 6.33249692e-06
Iter: 223 loss: 6.32075717e-06
Iter: 224 loss: 6.29470424e-06
Iter: 225 loss: 6.3651355e-06
Iter: 226 loss: 6.28595808e-06
Iter: 227 loss: 6.26180645e-06
Iter: 228 loss: 6.44781085e-06
Iter: 229 loss: 6.26007659e-06
Iter: 230 loss: 6.23780079e-06
Iter: 231 loss: 6.25621897e-06
Iter: 232 loss: 6.22437756e-06
Iter: 233 loss: 6.20062565e-06
Iter: 234 loss: 6.26072097e-06
Iter: 235 loss: 6.19242564e-06
Iter: 236 loss: 6.17214209e-06
Iter: 237 loss: 6.41294082e-06
Iter: 238 loss: 6.17183923e-06
Iter: 239 loss: 6.15493e-06
Iter: 240 loss: 6.15544286e-06
Iter: 241 loss: 6.14127066e-06
Iter: 242 loss: 6.11649284e-06
Iter: 243 loss: 6.20096534e-06
Iter: 244 loss: 6.10974666e-06
Iter: 245 loss: 6.0930538e-06
Iter: 246 loss: 6.09011568e-06
Iter: 247 loss: 6.07864104e-06
Iter: 248 loss: 6.05195055e-06
Iter: 249 loss: 6.23142387e-06
Iter: 250 loss: 6.04923616e-06
Iter: 251 loss: 6.02856835e-06
Iter: 252 loss: 6.07647553e-06
Iter: 253 loss: 6.02087312e-06
Iter: 254 loss: 6.00222302e-06
Iter: 255 loss: 5.98752285e-06
Iter: 256 loss: 5.9815784e-06
Iter: 257 loss: 5.9607346e-06
Iter: 258 loss: 5.96074278e-06
Iter: 259 loss: 5.94310495e-06
Iter: 260 loss: 5.9729764e-06
Iter: 261 loss: 5.93541e-06
Iter: 262 loss: 5.91418666e-06
Iter: 263 loss: 5.90307536e-06
Iter: 264 loss: 5.89327828e-06
Iter: 265 loss: 5.87000977e-06
Iter: 266 loss: 5.96528389e-06
Iter: 267 loss: 5.864983e-06
Iter: 268 loss: 5.84140616e-06
Iter: 269 loss: 6.01752618e-06
Iter: 270 loss: 5.83956626e-06
Iter: 271 loss: 5.82366874e-06
Iter: 272 loss: 5.85983025e-06
Iter: 273 loss: 5.81747918e-06
Iter: 274 loss: 5.80295773e-06
Iter: 275 loss: 5.86099623e-06
Iter: 276 loss: 5.79970128e-06
Iter: 277 loss: 5.78056915e-06
Iter: 278 loss: 5.80110373e-06
Iter: 279 loss: 5.77005176e-06
Iter: 280 loss: 5.75421564e-06
Iter: 281 loss: 5.78030085e-06
Iter: 282 loss: 5.74712294e-06
Iter: 283 loss: 5.72770614e-06
Iter: 284 loss: 5.78376785e-06
Iter: 285 loss: 5.72172485e-06
Iter: 286 loss: 5.70406701e-06
Iter: 287 loss: 5.73268153e-06
Iter: 288 loss: 5.69603708e-06
Iter: 289 loss: 5.67649886e-06
Iter: 290 loss: 5.81934546e-06
Iter: 291 loss: 5.67490133e-06
Iter: 292 loss: 5.66197468e-06
Iter: 293 loss: 5.64527818e-06
Iter: 294 loss: 5.64425409e-06
Iter: 295 loss: 5.62330297e-06
Iter: 296 loss: 5.77918172e-06
Iter: 297 loss: 5.62156674e-06
Iter: 298 loss: 5.60382023e-06
Iter: 299 loss: 5.68546238e-06
Iter: 300 loss: 5.60051922e-06
Iter: 301 loss: 5.58410102e-06
Iter: 302 loss: 5.60728e-06
Iter: 303 loss: 5.57580734e-06
Iter: 304 loss: 5.55896031e-06
Iter: 305 loss: 5.54383269e-06
Iter: 306 loss: 5.53938253e-06
Iter: 307 loss: 5.51841731e-06
Iter: 308 loss: 5.79837615e-06
Iter: 309 loss: 5.51830317e-06
Iter: 310 loss: 5.50094774e-06
Iter: 311 loss: 5.57447584e-06
Iter: 312 loss: 5.49718789e-06
Iter: 313 loss: 5.48345361e-06
Iter: 314 loss: 5.5179471e-06
Iter: 315 loss: 5.47845411e-06
Iter: 316 loss: 5.46227056e-06
Iter: 317 loss: 5.53373502e-06
Iter: 318 loss: 5.45895728e-06
Iter: 319 loss: 5.44670456e-06
Iter: 320 loss: 5.43184296e-06
Iter: 321 loss: 5.43057513e-06
Iter: 322 loss: 5.41375903e-06
Iter: 323 loss: 5.59210912e-06
Iter: 324 loss: 5.4134116e-06
Iter: 325 loss: 5.39943903e-06
Iter: 326 loss: 5.41943427e-06
Iter: 327 loss: 5.39260327e-06
Iter: 328 loss: 5.37692677e-06
Iter: 329 loss: 5.4297052e-06
Iter: 330 loss: 5.37273536e-06
Iter: 331 loss: 5.35795471e-06
Iter: 332 loss: 5.39052144e-06
Iter: 333 loss: 5.35253548e-06
Iter: 334 loss: 5.33587809e-06
Iter: 335 loss: 5.33988441e-06
Iter: 336 loss: 5.32388913e-06
Iter: 337 loss: 5.30599891e-06
Iter: 338 loss: 5.4106481e-06
Iter: 339 loss: 5.30378475e-06
Iter: 340 loss: 5.28624469e-06
Iter: 341 loss: 5.3631984e-06
Iter: 342 loss: 5.28283499e-06
Iter: 343 loss: 5.2680989e-06
Iter: 344 loss: 5.25978794e-06
Iter: 345 loss: 5.25338965e-06
Iter: 346 loss: 5.23416657e-06
Iter: 347 loss: 5.3166641e-06
Iter: 348 loss: 5.23034669e-06
Iter: 349 loss: 5.21651691e-06
Iter: 350 loss: 5.31442629e-06
Iter: 351 loss: 5.21541233e-06
Iter: 352 loss: 5.20102276e-06
Iter: 353 loss: 5.24281859e-06
Iter: 354 loss: 5.19649529e-06
Iter: 355 loss: 5.18440402e-06
Iter: 356 loss: 5.2320238e-06
Iter: 357 loss: 5.18168963e-06
Iter: 358 loss: 5.168803e-06
Iter: 359 loss: 5.17135641e-06
Iter: 360 loss: 5.15896954e-06
Iter: 361 loss: 5.14542353e-06
Iter: 362 loss: 5.13621126e-06
Iter: 363 loss: 5.13112809e-06
Iter: 364 loss: 5.11957251e-06
Iter: 365 loss: 5.11823328e-06
Iter: 366 loss: 5.10812242e-06
Iter: 367 loss: 5.0968797e-06
Iter: 368 loss: 5.09529855e-06
Iter: 369 loss: 5.07923869e-06
Iter: 370 loss: 5.20671483e-06
Iter: 371 loss: 5.07821096e-06
Iter: 372 loss: 5.06629658e-06
Iter: 373 loss: 5.06796459e-06
Iter: 374 loss: 5.05722619e-06
Iter: 375 loss: 5.04084073e-06
Iter: 376 loss: 5.11170356e-06
Iter: 377 loss: 5.03756928e-06
Iter: 378 loss: 5.02484909e-06
Iter: 379 loss: 5.07685399e-06
Iter: 380 loss: 5.0219578e-06
Iter: 381 loss: 5.0071958e-06
Iter: 382 loss: 5.02339572e-06
Iter: 383 loss: 4.99900671e-06
Iter: 384 loss: 4.98662575e-06
Iter: 385 loss: 4.98184363e-06
Iter: 386 loss: 4.97498968e-06
Iter: 387 loss: 4.95766e-06
Iter: 388 loss: 5.10931e-06
Iter: 389 loss: 4.95679e-06
Iter: 390 loss: 4.94291089e-06
Iter: 391 loss: 5.06546257e-06
Iter: 392 loss: 4.94212145e-06
Iter: 393 loss: 4.93288917e-06
Iter: 394 loss: 4.93135758e-06
Iter: 395 loss: 4.9249511e-06
Iter: 396 loss: 4.91057926e-06
Iter: 397 loss: 4.9670366e-06
Iter: 398 loss: 4.90715911e-06
Iter: 399 loss: 4.89636568e-06
Iter: 400 loss: 4.89523336e-06
Iter: 401 loss: 4.88750084e-06
Iter: 402 loss: 4.87228635e-06
Iter: 403 loss: 4.92937e-06
Iter: 404 loss: 4.8685265e-06
Iter: 405 loss: 4.85526471e-06
Iter: 406 loss: 4.95930362e-06
Iter: 407 loss: 4.85440069e-06
Iter: 408 loss: 4.84276461e-06
Iter: 409 loss: 4.83654276e-06
Iter: 410 loss: 4.83120311e-06
Iter: 411 loss: 4.81818824e-06
Iter: 412 loss: 4.88165097e-06
Iter: 413 loss: 4.81593088e-06
Iter: 414 loss: 4.80199105e-06
Iter: 415 loss: 4.83081658e-06
Iter: 416 loss: 4.79634127e-06
Iter: 417 loss: 4.78214361e-06
Iter: 418 loss: 4.81968164e-06
Iter: 419 loss: 4.77750564e-06
Iter: 420 loss: 4.76496643e-06
Iter: 421 loss: 4.85789042e-06
Iter: 422 loss: 4.76391733e-06
Iter: 423 loss: 4.75407887e-06
Iter: 424 loss: 4.74887929e-06
Iter: 425 loss: 4.74447597e-06
Iter: 426 loss: 4.72926286e-06
Iter: 427 loss: 4.75003708e-06
Iter: 428 loss: 4.72166039e-06
Iter: 429 loss: 4.71291241e-06
Iter: 430 loss: 4.71232761e-06
Iter: 431 loss: 4.70249324e-06
Iter: 432 loss: 4.69078e-06
Iter: 433 loss: 4.68961071e-06
Iter: 434 loss: 4.67534755e-06
Iter: 435 loss: 4.75052548e-06
Iter: 436 loss: 4.6731011e-06
Iter: 437 loss: 4.66098209e-06
Iter: 438 loss: 4.70680607e-06
Iter: 439 loss: 4.65804897e-06
Iter: 440 loss: 4.64777895e-06
Iter: 441 loss: 4.63766173e-06
Iter: 442 loss: 4.63555716e-06
Iter: 443 loss: 4.62497246e-06
Iter: 444 loss: 4.62479329e-06
Iter: 445 loss: 4.61513082e-06
Iter: 446 loss: 4.61930767e-06
Iter: 447 loss: 4.60845e-06
Iter: 448 loss: 4.59522062e-06
Iter: 449 loss: 4.59686953e-06
Iter: 450 loss: 4.5851325e-06
Iter: 451 loss: 4.57151782e-06
Iter: 452 loss: 4.68112739e-06
Iter: 453 loss: 4.57050874e-06
Iter: 454 loss: 4.55918962e-06
Iter: 455 loss: 4.59649027e-06
Iter: 456 loss: 4.55588361e-06
Iter: 457 loss: 4.54427936e-06
Iter: 458 loss: 4.56038742e-06
Iter: 459 loss: 4.53848452e-06
Iter: 460 loss: 4.52599943e-06
Iter: 461 loss: 4.58148907e-06
Iter: 462 loss: 4.52363474e-06
Iter: 463 loss: 4.51232791e-06
Iter: 464 loss: 4.52085214e-06
Iter: 465 loss: 4.5053448e-06
Iter: 466 loss: 4.49282152e-06
Iter: 467 loss: 4.52446193e-06
Iter: 468 loss: 4.48846549e-06
Iter: 469 loss: 4.4782887e-06
Iter: 470 loss: 4.47831962e-06
Iter: 471 loss: 4.47031925e-06
Iter: 472 loss: 4.45634e-06
Iter: 473 loss: 4.80147037e-06
Iter: 474 loss: 4.45641035e-06
Iter: 475 loss: 4.44499574e-06
Iter: 476 loss: 4.53601388e-06
Iter: 477 loss: 4.44439229e-06
Iter: 478 loss: 4.43265708e-06
Iter: 479 loss: 4.4487233e-06
Iter: 480 loss: 4.42683177e-06
Iter: 481 loss: 4.41552265e-06
Iter: 482 loss: 4.43285398e-06
Iter: 483 loss: 4.41027078e-06
Iter: 484 loss: 4.39731593e-06
Iter: 485 loss: 4.45969636e-06
Iter: 486 loss: 4.39510814e-06
Iter: 487 loss: 4.38369898e-06
Iter: 488 loss: 4.43837735e-06
Iter: 489 loss: 4.38181314e-06
Iter: 490 loss: 4.37296194e-06
Iter: 491 loss: 4.36084156e-06
Iter: 492 loss: 4.36032406e-06
Iter: 493 loss: 4.34576395e-06
Iter: 494 loss: 4.45311343e-06
Iter: 495 loss: 4.34460071e-06
Iter: 496 loss: 4.33447713e-06
Iter: 497 loss: 4.44078796e-06
Iter: 498 loss: 4.33419382e-06
Iter: 499 loss: 4.32646812e-06
Iter: 500 loss: 4.31763601e-06
Iter: 501 loss: 4.31654826e-06
Iter: 502 loss: 4.30388354e-06
Iter: 503 loss: 4.38545476e-06
Iter: 504 loss: 4.30237515e-06
Iter: 505 loss: 4.29040483e-06
Iter: 506 loss: 4.30914406e-06
Iter: 507 loss: 4.28460498e-06
Iter: 508 loss: 4.27726172e-06
Iter: 509 loss: 4.2772308e-06
Iter: 510 loss: 4.26978568e-06
Iter: 511 loss: 4.26084625e-06
Iter: 512 loss: 4.259869e-06
Iter: 513 loss: 4.24908694e-06
Iter: 514 loss: 4.26277666e-06
Iter: 515 loss: 4.24359769e-06
Iter: 516 loss: 4.23191932e-06
Iter: 517 loss: 4.30440923e-06
Iter: 518 loss: 4.2305046e-06
Iter: 519 loss: 4.21920276e-06
Iter: 520 loss: 4.24743257e-06
Iter: 521 loss: 4.21518416e-06
Iter: 522 loss: 4.2037e-06
Iter: 523 loss: 4.20896049e-06
Iter: 524 loss: 4.19600156e-06
Iter: 525 loss: 4.18681384e-06
Iter: 526 loss: 4.18682066e-06
Iter: 527 loss: 4.17934461e-06
Iter: 528 loss: 4.17637193e-06
Iter: 529 loss: 4.17248339e-06
Iter: 530 loss: 4.16204648e-06
Iter: 531 loss: 4.17298179e-06
Iter: 532 loss: 4.15643535e-06
Iter: 533 loss: 4.14491888e-06
Iter: 534 loss: 4.2229367e-06
Iter: 535 loss: 4.14381566e-06
Iter: 536 loss: 4.13513408e-06
Iter: 537 loss: 4.1911585e-06
Iter: 538 loss: 4.13419502e-06
Iter: 539 loss: 4.12693953e-06
Iter: 540 loss: 4.11536803e-06
Iter: 541 loss: 4.11523479e-06
Iter: 542 loss: 4.10778694e-06
Iter: 543 loss: 4.10754365e-06
Iter: 544 loss: 4.10018447e-06
Iter: 545 loss: 4.10032271e-06
Iter: 546 loss: 4.09430504e-06
Iter: 547 loss: 4.08240157e-06
Iter: 548 loss: 4.13577163e-06
Iter: 549 loss: 4.07997095e-06
Iter: 550 loss: 4.07166817e-06
Iter: 551 loss: 4.06827394e-06
Iter: 552 loss: 4.06374875e-06
Iter: 553 loss: 4.05147784e-06
Iter: 554 loss: 4.07278685e-06
Iter: 555 loss: 4.04593811e-06
Iter: 556 loss: 4.03537661e-06
Iter: 557 loss: 4.16420062e-06
Iter: 558 loss: 4.03529521e-06
Iter: 559 loss: 4.02749902e-06
Iter: 560 loss: 4.04270577e-06
Iter: 561 loss: 4.02442856e-06
Iter: 562 loss: 4.01572288e-06
Iter: 563 loss: 4.014566e-06
Iter: 564 loss: 4.00842964e-06
Iter: 565 loss: 3.99854071e-06
Iter: 566 loss: 4.13142834e-06
Iter: 567 loss: 3.99859027e-06
Iter: 568 loss: 3.99054306e-06
Iter: 569 loss: 3.98137763e-06
Iter: 570 loss: 3.98035172e-06
Iter: 571 loss: 3.96835185e-06
Iter: 572 loss: 3.99754254e-06
Iter: 573 loss: 3.96409632e-06
Iter: 574 loss: 3.95607276e-06
Iter: 575 loss: 3.95592633e-06
Iter: 576 loss: 3.94892504e-06
Iter: 577 loss: 3.93892606e-06
Iter: 578 loss: 3.9386332e-06
Iter: 579 loss: 3.92763877e-06
Iter: 580 loss: 4.00327826e-06
Iter: 581 loss: 3.92655465e-06
Iter: 582 loss: 3.91868252e-06
Iter: 583 loss: 4.01336911e-06
Iter: 584 loss: 3.91864023e-06
Iter: 585 loss: 3.91294134e-06
Iter: 586 loss: 3.90569767e-06
Iter: 587 loss: 3.90507512e-06
Iter: 588 loss: 3.89458955e-06
Iter: 589 loss: 3.94750168e-06
Iter: 590 loss: 3.89279876e-06
Iter: 591 loss: 3.88520493e-06
Iter: 592 loss: 3.89139723e-06
Iter: 593 loss: 3.88063518e-06
Iter: 594 loss: 3.8707567e-06
Iter: 595 loss: 3.88831859e-06
Iter: 596 loss: 3.86651072e-06
Iter: 597 loss: 3.85760404e-06
Iter: 598 loss: 3.97276e-06
Iter: 599 loss: 3.85747035e-06
Iter: 600 loss: 3.85119074e-06
Iter: 601 loss: 3.84518353e-06
Iter: 602 loss: 3.84376426e-06
Iter: 603 loss: 3.83304769e-06
Iter: 604 loss: 3.89467232e-06
Iter: 605 loss: 3.83162114e-06
Iter: 606 loss: 3.82213375e-06
Iter: 607 loss: 3.85455087e-06
Iter: 608 loss: 3.8197004e-06
Iter: 609 loss: 3.81118707e-06
Iter: 610 loss: 3.80702477e-06
Iter: 611 loss: 3.80307506e-06
Iter: 612 loss: 3.79345693e-06
Iter: 613 loss: 3.87007367e-06
Iter: 614 loss: 3.79282824e-06
Iter: 615 loss: 3.78532832e-06
Iter: 616 loss: 3.83488396e-06
Iter: 617 loss: 3.7845025e-06
Iter: 618 loss: 3.7778691e-06
Iter: 619 loss: 3.77470315e-06
Iter: 620 loss: 3.77162178e-06
Iter: 621 loss: 3.76436878e-06
Iter: 622 loss: 3.84496025e-06
Iter: 623 loss: 3.76418075e-06
Iter: 624 loss: 3.75679065e-06
Iter: 625 loss: 3.76184835e-06
Iter: 626 loss: 3.75202831e-06
Iter: 627 loss: 3.74564797e-06
Iter: 628 loss: 3.74581759e-06
Iter: 629 loss: 3.74049205e-06
Iter: 630 loss: 3.73094099e-06
Iter: 631 loss: 3.76368257e-06
Iter: 632 loss: 3.7283337e-06
Iter: 633 loss: 3.71880287e-06
Iter: 634 loss: 3.75639843e-06
Iter: 635 loss: 3.7166526e-06
Iter: 636 loss: 3.70854832e-06
Iter: 637 loss: 3.7188845e-06
Iter: 638 loss: 3.70416592e-06
Iter: 639 loss: 3.69553345e-06
Iter: 640 loss: 3.76654702e-06
Iter: 641 loss: 3.69493091e-06
Iter: 642 loss: 3.68781866e-06
Iter: 643 loss: 3.68962378e-06
Iter: 644 loss: 3.68253177e-06
Iter: 645 loss: 3.67447137e-06
Iter: 646 loss: 3.71563215e-06
Iter: 647 loss: 3.67318194e-06
Iter: 648 loss: 3.66657355e-06
Iter: 649 loss: 3.69392e-06
Iter: 650 loss: 3.66511313e-06
Iter: 651 loss: 3.6581514e-06
Iter: 652 loss: 3.64952166e-06
Iter: 653 loss: 3.64873631e-06
Iter: 654 loss: 3.63897334e-06
Iter: 655 loss: 3.7044374e-06
Iter: 656 loss: 3.63792901e-06
Iter: 657 loss: 3.62976652e-06
Iter: 658 loss: 3.70592988e-06
Iter: 659 loss: 3.62939727e-06
Iter: 660 loss: 3.62324386e-06
Iter: 661 loss: 3.61584262e-06
Iter: 662 loss: 3.615163e-06
Iter: 663 loss: 3.60903869e-06
Iter: 664 loss: 3.60824492e-06
Iter: 665 loss: 3.60459035e-06
Iter: 666 loss: 3.59523483e-06
Iter: 667 loss: 3.67802113e-06
Iter: 668 loss: 3.59371325e-06
Iter: 669 loss: 3.58244552e-06
Iter: 670 loss: 3.64582684e-06
Iter: 671 loss: 3.58089846e-06
Iter: 672 loss: 3.57287195e-06
Iter: 673 loss: 3.61560478e-06
Iter: 674 loss: 3.57164959e-06
Iter: 675 loss: 3.5639016e-06
Iter: 676 loss: 3.58327725e-06
Iter: 677 loss: 3.56121427e-06
Iter: 678 loss: 3.55325483e-06
Iter: 679 loss: 3.57374938e-06
Iter: 680 loss: 3.5503549e-06
Iter: 681 loss: 3.54359395e-06
Iter: 682 loss: 3.58793659e-06
Iter: 683 loss: 3.54283156e-06
Iter: 684 loss: 3.53657651e-06
Iter: 685 loss: 3.53461633e-06
Iter: 686 loss: 3.53077758e-06
Iter: 687 loss: 3.52250163e-06
Iter: 688 loss: 3.56291025e-06
Iter: 689 loss: 3.52100415e-06
Iter: 690 loss: 3.51316544e-06
Iter: 691 loss: 3.54141503e-06
Iter: 692 loss: 3.5111384e-06
Iter: 693 loss: 3.50416917e-06
Iter: 694 loss: 3.49882657e-06
Iter: 695 loss: 3.49663947e-06
Iter: 696 loss: 3.48893514e-06
Iter: 697 loss: 3.48894628e-06
Iter: 698 loss: 3.48201911e-06
Iter: 699 loss: 3.48998333e-06
Iter: 700 loss: 3.47819946e-06
Iter: 701 loss: 3.4724967e-06
Iter: 702 loss: 3.51817494e-06
Iter: 703 loss: 3.47200216e-06
Iter: 704 loss: 3.46684828e-06
Iter: 705 loss: 3.47390755e-06
Iter: 706 loss: 3.46429329e-06
Iter: 707 loss: 3.45809235e-06
Iter: 708 loss: 3.44955e-06
Iter: 709 loss: 3.44922478e-06
Iter: 710 loss: 3.43976922e-06
Iter: 711 loss: 3.47227569e-06
Iter: 712 loss: 3.43734155e-06
Iter: 713 loss: 3.4270538e-06
Iter: 714 loss: 3.47353011e-06
Iter: 715 loss: 3.42493809e-06
Iter: 716 loss: 3.41846089e-06
Iter: 717 loss: 3.48502704e-06
Iter: 718 loss: 3.4181553e-06
Iter: 719 loss: 3.41215423e-06
Iter: 720 loss: 3.40711267e-06
Iter: 721 loss: 3.40535644e-06
Iter: 722 loss: 3.39810094e-06
Iter: 723 loss: 3.49522384e-06
Iter: 724 loss: 3.39810413e-06
Iter: 725 loss: 3.39232793e-06
Iter: 726 loss: 3.38858604e-06
Iter: 727 loss: 3.38648806e-06
Iter: 728 loss: 3.37935353e-06
Iter: 729 loss: 3.43297711e-06
Iter: 730 loss: 3.37876918e-06
Iter: 731 loss: 3.37197616e-06
Iter: 732 loss: 3.38030645e-06
Iter: 733 loss: 3.36850371e-06
Iter: 734 loss: 3.36120138e-06
Iter: 735 loss: 3.36674543e-06
Iter: 736 loss: 3.35682307e-06
Iter: 737 loss: 3.35178288e-06
Iter: 738 loss: 3.35148889e-06
Iter: 739 loss: 3.34739752e-06
Iter: 740 loss: 3.34175047e-06
Iter: 741 loss: 3.34148899e-06
Iter: 742 loss: 3.33432308e-06
Iter: 743 loss: 3.41887971e-06
Iter: 744 loss: 3.33422531e-06
Iter: 745 loss: 3.32983336e-06
Iter: 746 loss: 3.32472155e-06
Iter: 747 loss: 3.3242261e-06
Iter: 748 loss: 3.31624642e-06
Iter: 749 loss: 3.32846162e-06
Iter: 750 loss: 3.31261026e-06
Iter: 751 loss: 3.30402918e-06
Iter: 752 loss: 3.32435548e-06
Iter: 753 loss: 3.30092371e-06
Iter: 754 loss: 3.29253453e-06
Iter: 755 loss: 3.32561672e-06
Iter: 756 loss: 3.29068098e-06
Iter: 757 loss: 3.28434226e-06
Iter: 758 loss: 3.35784239e-06
Iter: 759 loss: 3.28433111e-06
Iter: 760 loss: 3.27859857e-06
Iter: 761 loss: 3.27557859e-06
Iter: 762 loss: 3.27290809e-06
Iter: 763 loss: 3.26508371e-06
Iter: 764 loss: 3.3065885e-06
Iter: 765 loss: 3.26401732e-06
Iter: 766 loss: 3.25744622e-06
Iter: 767 loss: 3.28673718e-06
Iter: 768 loss: 3.25621568e-06
Iter: 769 loss: 3.25110318e-06
Iter: 770 loss: 3.24975281e-06
Iter: 771 loss: 3.24659868e-06
Iter: 772 loss: 3.23921677e-06
Iter: 773 loss: 3.30040075e-06
Iter: 774 loss: 3.23878749e-06
Iter: 775 loss: 3.2336925e-06
Iter: 776 loss: 3.23618974e-06
Iter: 777 loss: 3.23035647e-06
Iter: 778 loss: 3.22396136e-06
Iter: 779 loss: 3.2615344e-06
Iter: 780 loss: 3.22302458e-06
Iter: 781 loss: 3.21676475e-06
Iter: 782 loss: 3.23442327e-06
Iter: 783 loss: 3.21463608e-06
Iter: 784 loss: 3.20957452e-06
Iter: 785 loss: 3.22341702e-06
Iter: 786 loss: 3.20785648e-06
Iter: 787 loss: 3.20211439e-06
Iter: 788 loss: 3.20938511e-06
Iter: 789 loss: 3.19905257e-06
Iter: 790 loss: 3.19257879e-06
Iter: 791 loss: 3.19364653e-06
Iter: 792 loss: 3.18766888e-06
Iter: 793 loss: 3.18033199e-06
Iter: 794 loss: 3.19535661e-06
Iter: 795 loss: 3.17740478e-06
Iter: 796 loss: 3.16921705e-06
Iter: 797 loss: 3.18872458e-06
Iter: 798 loss: 3.16623891e-06
Iter: 799 loss: 3.16073738e-06
Iter: 800 loss: 3.16050364e-06
Iter: 801 loss: 3.15639204e-06
Iter: 802 loss: 3.15264424e-06
Iter: 803 loss: 3.15160332e-06
Iter: 804 loss: 3.14501267e-06
Iter: 805 loss: 3.16982732e-06
Iter: 806 loss: 3.14346244e-06
Iter: 807 loss: 3.1370314e-06
Iter: 808 loss: 3.16755632e-06
Iter: 809 loss: 3.1358702e-06
Iter: 810 loss: 3.13072223e-06
Iter: 811 loss: 3.13270039e-06
Iter: 812 loss: 3.12716952e-06
Iter: 813 loss: 3.12108887e-06
Iter: 814 loss: 3.15296415e-06
Iter: 815 loss: 3.12010025e-06
Iter: 816 loss: 3.11369035e-06
Iter: 817 loss: 3.12703696e-06
Iter: 818 loss: 3.11115673e-06
Iter: 819 loss: 3.10628275e-06
Iter: 820 loss: 3.16072828e-06
Iter: 821 loss: 3.10613359e-06
Iter: 822 loss: 3.10203541e-06
Iter: 823 loss: 3.09767074e-06
Iter: 824 loss: 3.0970441e-06
Iter: 825 loss: 3.09142979e-06
Iter: 826 loss: 3.14209046e-06
Iter: 827 loss: 3.09127222e-06
Iter: 828 loss: 3.08629683e-06
Iter: 829 loss: 3.08215249e-06
Iter: 830 loss: 3.080753e-06
Iter: 831 loss: 3.07404e-06
Iter: 832 loss: 3.0889305e-06
Iter: 833 loss: 3.07145115e-06
Iter: 834 loss: 3.06399352e-06
Iter: 835 loss: 3.08331892e-06
Iter: 836 loss: 3.06142238e-06
Iter: 837 loss: 3.05339609e-06
Iter: 838 loss: 3.05504227e-06
Iter: 839 loss: 3.04759942e-06
Iter: 840 loss: 3.03805018e-06
Iter: 841 loss: 3.07731079e-06
Iter: 842 loss: 3.03593856e-06
Iter: 843 loss: 3.02976218e-06
Iter: 844 loss: 3.0297349e-06
Iter: 845 loss: 3.02439253e-06
Iter: 846 loss: 3.0289068e-06
Iter: 847 loss: 3.02126637e-06
Iter: 848 loss: 3.01549449e-06
Iter: 849 loss: 3.02653689e-06
Iter: 850 loss: 3.0131273e-06
Iter: 851 loss: 3.00813781e-06
Iter: 852 loss: 3.03104184e-06
Iter: 853 loss: 3.00703505e-06
Iter: 854 loss: 3.00118654e-06
Iter: 855 loss: 3.01704813e-06
Iter: 856 loss: 2.99931389e-06
Iter: 857 loss: 2.99398062e-06
Iter: 858 loss: 3.01769705e-06
Iter: 859 loss: 2.99287694e-06
Iter: 860 loss: 2.98795703e-06
Iter: 861 loss: 2.98941882e-06
Iter: 862 loss: 2.98448276e-06
Iter: 863 loss: 2.97816041e-06
Iter: 864 loss: 2.98428722e-06
Iter: 865 loss: 2.97453926e-06
Iter: 866 loss: 2.96771373e-06
Iter: 867 loss: 3.02195576e-06
Iter: 868 loss: 2.96725284e-06
Iter: 869 loss: 2.96237727e-06
Iter: 870 loss: 2.96349617e-06
Iter: 871 loss: 2.95871587e-06
Iter: 872 loss: 2.95325549e-06
Iter: 873 loss: 2.9930311e-06
Iter: 874 loss: 2.95283417e-06
Iter: 875 loss: 2.94766255e-06
Iter: 876 loss: 2.95611608e-06
Iter: 877 loss: 2.94517758e-06
Iter: 878 loss: 2.93968105e-06
Iter: 879 loss: 2.93740209e-06
Iter: 880 loss: 2.93459061e-06
Iter: 881 loss: 2.9272353e-06
Iter: 882 loss: 2.94364918e-06
Iter: 883 loss: 2.92438108e-06
Iter: 884 loss: 2.91600531e-06
Iter: 885 loss: 2.93920448e-06
Iter: 886 loss: 2.91332117e-06
Iter: 887 loss: 2.90828916e-06
Iter: 888 loss: 2.90791741e-06
Iter: 889 loss: 2.90483786e-06
Iter: 890 loss: 2.89761147e-06
Iter: 891 loss: 2.97638235e-06
Iter: 892 loss: 2.89690024e-06
Iter: 893 loss: 2.89097466e-06
Iter: 894 loss: 2.89086847e-06
Iter: 895 loss: 2.886316e-06
Iter: 896 loss: 2.90854132e-06
Iter: 897 loss: 2.8855452e-06
Iter: 898 loss: 2.88178103e-06
Iter: 899 loss: 2.88601836e-06
Iter: 900 loss: 2.87975763e-06
Iter: 901 loss: 2.87493435e-06
Iter: 902 loss: 2.8805016e-06
Iter: 903 loss: 2.87238754e-06
Iter: 904 loss: 2.86643763e-06
Iter: 905 loss: 2.87357443e-06
Iter: 906 loss: 2.86320392e-06
Iter: 907 loss: 2.85802093e-06
Iter: 908 loss: 2.88100568e-06
Iter: 909 loss: 2.8570289e-06
Iter: 910 loss: 2.85089277e-06
Iter: 911 loss: 2.85744477e-06
Iter: 912 loss: 2.84750104e-06
Iter: 913 loss: 2.84139833e-06
Iter: 914 loss: 2.86888417e-06
Iter: 915 loss: 2.84032194e-06
Iter: 916 loss: 2.83514601e-06
Iter: 917 loss: 2.86404133e-06
Iter: 918 loss: 2.83452209e-06
Iter: 919 loss: 2.8300708e-06
Iter: 920 loss: 2.82455039e-06
Iter: 921 loss: 2.8240845e-06
Iter: 922 loss: 2.81783105e-06
Iter: 923 loss: 2.83940562e-06
Iter: 924 loss: 2.81610937e-06
Iter: 925 loss: 2.80925497e-06
Iter: 926 loss: 2.82884412e-06
Iter: 927 loss: 2.80709037e-06
Iter: 928 loss: 2.80031804e-06
Iter: 929 loss: 2.82213296e-06
Iter: 930 loss: 2.79842948e-06
Iter: 931 loss: 2.79356027e-06
Iter: 932 loss: 2.79355299e-06
Iter: 933 loss: 2.7900137e-06
Iter: 934 loss: 2.783843e-06
Iter: 935 loss: 2.78383914e-06
Iter: 936 loss: 2.77880326e-06
Iter: 937 loss: 2.77880099e-06
Iter: 938 loss: 2.77462505e-06
Iter: 939 loss: 2.78212929e-06
Iter: 940 loss: 2.7727308e-06
Iter: 941 loss: 2.76911305e-06
Iter: 942 loss: 2.76843662e-06
Iter: 943 loss: 2.76611945e-06
Iter: 944 loss: 2.76025776e-06
Iter: 945 loss: 2.79066467e-06
Iter: 946 loss: 2.75937623e-06
Iter: 947 loss: 2.75493971e-06
Iter: 948 loss: 2.75241609e-06
Iter: 949 loss: 2.75059665e-06
Iter: 950 loss: 2.74446666e-06
Iter: 951 loss: 2.78867401e-06
Iter: 952 loss: 2.74402e-06
Iter: 953 loss: 2.73882051e-06
Iter: 954 loss: 2.75251114e-06
Iter: 955 loss: 2.73698197e-06
Iter: 956 loss: 2.73200612e-06
Iter: 957 loss: 2.74058675e-06
Iter: 958 loss: 2.72974398e-06
Iter: 959 loss: 2.7252845e-06
Iter: 960 loss: 2.77299796e-06
Iter: 961 loss: 2.72516559e-06
Iter: 962 loss: 2.72176248e-06
Iter: 963 loss: 2.71673025e-06
Iter: 964 loss: 2.71657336e-06
Iter: 965 loss: 2.71068393e-06
Iter: 966 loss: 2.72246666e-06
Iter: 967 loss: 2.7082815e-06
Iter: 968 loss: 2.70163309e-06
Iter: 969 loss: 2.72220973e-06
Iter: 970 loss: 2.69965449e-06
Iter: 971 loss: 2.69456405e-06
Iter: 972 loss: 2.76334777e-06
Iter: 973 loss: 2.69454586e-06
Iter: 974 loss: 2.6898756e-06
Iter: 975 loss: 2.69279622e-06
Iter: 976 loss: 2.68684812e-06
Iter: 977 loss: 2.6818218e-06
Iter: 978 loss: 2.68551e-06
Iter: 979 loss: 2.67879796e-06
Iter: 980 loss: 2.67379619e-06
Iter: 981 loss: 2.67376e-06
Iter: 982 loss: 2.67096357e-06
Iter: 983 loss: 2.66917618e-06
Iter: 984 loss: 2.66814e-06
Iter: 985 loss: 2.66340203e-06
Iter: 986 loss: 2.67857376e-06
Iter: 987 loss: 2.66211737e-06
Iter: 988 loss: 2.6575176e-06
Iter: 989 loss: 2.66324196e-06
Iter: 990 loss: 2.65511926e-06
Iter: 991 loss: 2.64987921e-06
Iter: 992 loss: 2.65420204e-06
Iter: 993 loss: 2.64681125e-06
Iter: 994 loss: 2.64092796e-06
Iter: 995 loss: 2.67827363e-06
Iter: 996 loss: 2.64037681e-06
Iter: 997 loss: 2.63477705e-06
Iter: 998 loss: 2.6452085e-06
Iter: 999 loss: 2.63250331e-06
Iter: 1000 loss: 2.62833623e-06
Iter: 1001 loss: 2.65027029e-06
Iter: 1002 loss: 2.62778349e-06
Iter: 1003 loss: 2.62338631e-06
Iter: 1004 loss: 2.63207335e-06
Iter: 1005 loss: 2.62160302e-06
Iter: 1006 loss: 2.61707055e-06
Iter: 1007 loss: 2.61055129e-06
Iter: 1008 loss: 2.61035188e-06
Iter: 1009 loss: 2.60387537e-06
Iter: 1010 loss: 2.66545885e-06
Iter: 1011 loss: 2.60366414e-06
Iter: 1012 loss: 2.59833678e-06
Iter: 1013 loss: 2.60977686e-06
Iter: 1014 loss: 2.59632316e-06
Iter: 1015 loss: 2.5915092e-06
Iter: 1016 loss: 2.66475081e-06
Iter: 1017 loss: 2.59158219e-06
Iter: 1018 loss: 2.58880027e-06
Iter: 1019 loss: 2.58500404e-06
Iter: 1020 loss: 2.58493765e-06
Iter: 1021 loss: 2.57921033e-06
Iter: 1022 loss: 2.59362469e-06
Iter: 1023 loss: 2.57724218e-06
Iter: 1024 loss: 2.57274633e-06
Iter: 1025 loss: 2.63593506e-06
Iter: 1026 loss: 2.57278862e-06
Iter: 1027 loss: 2.56965495e-06
Iter: 1028 loss: 2.56796238e-06
Iter: 1029 loss: 2.56648173e-06
Iter: 1030 loss: 2.56210205e-06
Iter: 1031 loss: 2.61224773e-06
Iter: 1032 loss: 2.56203293e-06
Iter: 1033 loss: 2.55856276e-06
Iter: 1034 loss: 2.56155454e-06
Iter: 1035 loss: 2.55659302e-06
Iter: 1036 loss: 2.55259874e-06
Iter: 1037 loss: 2.55052896e-06
Iter: 1038 loss: 2.54867768e-06
Iter: 1039 loss: 2.54385145e-06
Iter: 1040 loss: 2.59722106e-06
Iter: 1041 loss: 2.54372526e-06
Iter: 1042 loss: 2.53988719e-06
Iter: 1043 loss: 2.54045267e-06
Iter: 1044 loss: 2.53687813e-06
Iter: 1045 loss: 2.53229064e-06
Iter: 1046 loss: 2.56451449e-06
Iter: 1047 loss: 2.53189046e-06
Iter: 1048 loss: 2.52781092e-06
Iter: 1049 loss: 2.53067765e-06
Iter: 1050 loss: 2.52521295e-06
Iter: 1051 loss: 2.52052587e-06
Iter: 1052 loss: 2.51596794e-06
Iter: 1053 loss: 2.51504434e-06
Iter: 1054 loss: 2.50874336e-06
Iter: 1055 loss: 2.54749511e-06
Iter: 1056 loss: 2.50813e-06
Iter: 1057 loss: 2.50298262e-06
Iter: 1058 loss: 2.55870714e-06
Iter: 1059 loss: 2.5029135e-06
Iter: 1060 loss: 2.49832124e-06
Iter: 1061 loss: 2.50179801e-06
Iter: 1062 loss: 2.49549976e-06
Iter: 1063 loss: 2.49121058e-06
Iter: 1064 loss: 2.49063442e-06
Iter: 1065 loss: 2.48762399e-06
Iter: 1066 loss: 2.48208426e-06
Iter: 1067 loss: 2.54613224e-06
Iter: 1068 loss: 2.48196648e-06
Iter: 1069 loss: 2.47800176e-06
Iter: 1070 loss: 2.48733522e-06
Iter: 1071 loss: 2.476585e-06
Iter: 1072 loss: 2.47245407e-06
Iter: 1073 loss: 2.47678986e-06
Iter: 1074 loss: 2.47029107e-06
Iter: 1075 loss: 2.46633408e-06
Iter: 1076 loss: 2.52830478e-06
Iter: 1077 loss: 2.46632703e-06
Iter: 1078 loss: 2.46375453e-06
Iter: 1079 loss: 2.45939373e-06
Iter: 1080 loss: 2.45942078e-06
Iter: 1081 loss: 2.45526508e-06
Iter: 1082 loss: 2.4889132e-06
Iter: 1083 loss: 2.45495562e-06
Iter: 1084 loss: 2.45087699e-06
Iter: 1085 loss: 2.45422598e-06
Iter: 1086 loss: 2.44846592e-06
Iter: 1087 loss: 2.4445e-06
Iter: 1088 loss: 2.46533841e-06
Iter: 1089 loss: 2.44383182e-06
Iter: 1090 loss: 2.43968361e-06
Iter: 1091 loss: 2.4414e-06
Iter: 1092 loss: 2.43682166e-06
Iter: 1093 loss: 2.43184513e-06
Iter: 1094 loss: 2.44574289e-06
Iter: 1095 loss: 2.43043633e-06
Iter: 1096 loss: 2.42587339e-06
Iter: 1097 loss: 2.42494957e-06
Iter: 1098 loss: 2.42199985e-06
Iter: 1099 loss: 2.41870566e-06
Iter: 1100 loss: 2.41847238e-06
Iter: 1101 loss: 2.4149449e-06
Iter: 1102 loss: 2.41579778e-06
Iter: 1103 loss: 2.41240718e-06
Iter: 1104 loss: 2.40822283e-06
Iter: 1105 loss: 2.4077267e-06
Iter: 1106 loss: 2.40463351e-06
Iter: 1107 loss: 2.40030295e-06
Iter: 1108 loss: 2.4377091e-06
Iter: 1109 loss: 2.40008558e-06
Iter: 1110 loss: 2.39597193e-06
Iter: 1111 loss: 2.40938152e-06
Iter: 1112 loss: 2.39484143e-06
Iter: 1113 loss: 2.39077895e-06
Iter: 1114 loss: 2.39299152e-06
Iter: 1115 loss: 2.38814027e-06
Iter: 1116 loss: 2.3850107e-06
Iter: 1117 loss: 2.38498114e-06
Iter: 1118 loss: 2.38234793e-06
Iter: 1119 loss: 2.38150574e-06
Iter: 1120 loss: 2.37998347e-06
Iter: 1121 loss: 2.37603786e-06
Iter: 1122 loss: 2.37878567e-06
Iter: 1123 loss: 2.37357608e-06
Iter: 1124 loss: 2.36981941e-06
Iter: 1125 loss: 2.40857207e-06
Iter: 1126 loss: 2.36976712e-06
Iter: 1127 loss: 2.3665641e-06
Iter: 1128 loss: 2.36510641e-06
Iter: 1129 loss: 2.36355754e-06
Iter: 1130 loss: 2.35941957e-06
Iter: 1131 loss: 2.37959739e-06
Iter: 1132 loss: 2.35872767e-06
Iter: 1133 loss: 2.35420907e-06
Iter: 1134 loss: 2.35772745e-06
Iter: 1135 loss: 2.35153743e-06
Iter: 1136 loss: 2.34731056e-06
Iter: 1137 loss: 2.3513935e-06
Iter: 1138 loss: 2.34484924e-06
Iter: 1139 loss: 2.34041227e-06
Iter: 1140 loss: 2.37522636e-06
Iter: 1141 loss: 2.34011941e-06
Iter: 1142 loss: 2.33654e-06
Iter: 1143 loss: 2.35834227e-06
Iter: 1144 loss: 2.33598303e-06
Iter: 1145 loss: 2.33245373e-06
Iter: 1146 loss: 2.32816546e-06
Iter: 1147 loss: 2.32775028e-06
Iter: 1148 loss: 2.32290495e-06
Iter: 1149 loss: 2.3361622e-06
Iter: 1150 loss: 2.32133289e-06
Iter: 1151 loss: 2.31751619e-06
Iter: 1152 loss: 2.31746253e-06
Iter: 1153 loss: 2.31455647e-06
Iter: 1154 loss: 2.3131106e-06
Iter: 1155 loss: 2.31169588e-06
Iter: 1156 loss: 2.30746923e-06
Iter: 1157 loss: 2.32149432e-06
Iter: 1158 loss: 2.30619867e-06
Iter: 1159 loss: 2.30310616e-06
Iter: 1160 loss: 2.30316482e-06
Iter: 1161 loss: 2.30075511e-06
Iter: 1162 loss: 2.29819761e-06
Iter: 1163 loss: 2.29790021e-06
Iter: 1164 loss: 2.29401212e-06
Iter: 1165 loss: 2.31252488e-06
Iter: 1166 loss: 2.29334705e-06
Iter: 1167 loss: 2.2893023e-06
Iter: 1168 loss: 2.29374018e-06
Iter: 1169 loss: 2.28717363e-06
Iter: 1170 loss: 2.28349518e-06
Iter: 1171 loss: 2.29463808e-06
Iter: 1172 loss: 2.28241811e-06
Iter: 1173 loss: 2.27861574e-06
Iter: 1174 loss: 2.28637259e-06
Iter: 1175 loss: 2.27711325e-06
Iter: 1176 loss: 2.27246687e-06
Iter: 1177 loss: 2.27716691e-06
Iter: 1178 loss: 2.26983252e-06
Iter: 1179 loss: 2.26575503e-06
Iter: 1180 loss: 2.27335477e-06
Iter: 1181 loss: 2.26400493e-06
Iter: 1182 loss: 2.25983513e-06
Iter: 1183 loss: 2.3015341e-06
Iter: 1184 loss: 2.25963367e-06
Iter: 1185 loss: 2.25559324e-06
Iter: 1186 loss: 2.25782469e-06
Iter: 1187 loss: 2.25299073e-06
Iter: 1188 loss: 2.24923269e-06
Iter: 1189 loss: 2.25143572e-06
Iter: 1190 loss: 2.24679115e-06
Iter: 1191 loss: 2.24277278e-06
Iter: 1192 loss: 2.26974726e-06
Iter: 1193 loss: 2.24236419e-06
Iter: 1194 loss: 2.23852908e-06
Iter: 1195 loss: 2.25428403e-06
Iter: 1196 loss: 2.23759844e-06
Iter: 1197 loss: 2.23456777e-06
Iter: 1198 loss: 2.23429788e-06
Iter: 1199 loss: 2.23195275e-06
Iter: 1200 loss: 2.22805579e-06
Iter: 1201 loss: 2.26849056e-06
Iter: 1202 loss: 2.22793437e-06
Iter: 1203 loss: 2.22436347e-06
Iter: 1204 loss: 2.2324632e-06
Iter: 1205 loss: 2.22306039e-06
Iter: 1206 loss: 2.22013387e-06
Iter: 1207 loss: 2.221861e-06
Iter: 1208 loss: 2.21823211e-06
Iter: 1209 loss: 2.21455321e-06
Iter: 1210 loss: 2.23358484e-06
Iter: 1211 loss: 2.21389132e-06
Iter: 1212 loss: 2.21032269e-06
Iter: 1213 loss: 2.21072e-06
Iter: 1214 loss: 2.20766105e-06
Iter: 1215 loss: 2.20440484e-06
Iter: 1216 loss: 2.21675464e-06
Iter: 1217 loss: 2.20360312e-06
Iter: 1218 loss: 2.19985327e-06
Iter: 1219 loss: 2.20981406e-06
Iter: 1220 loss: 2.1986084e-06
Iter: 1221 loss: 2.19517051e-06
Iter: 1222 loss: 2.19940421e-06
Iter: 1223 loss: 2.19346271e-06
Iter: 1224 loss: 2.18946957e-06
Iter: 1225 loss: 2.20044194e-06
Iter: 1226 loss: 2.18807872e-06
Iter: 1227 loss: 2.18411469e-06
Iter: 1228 loss: 2.22125891e-06
Iter: 1229 loss: 2.18398782e-06
Iter: 1230 loss: 2.18131072e-06
Iter: 1231 loss: 2.1787223e-06
Iter: 1232 loss: 2.17812794e-06
Iter: 1233 loss: 2.17416414e-06
Iter: 1234 loss: 2.18062e-06
Iter: 1235 loss: 2.17231263e-06
Iter: 1236 loss: 2.16897865e-06
Iter: 1237 loss: 2.22221206e-06
Iter: 1238 loss: 2.16897593e-06
Iter: 1239 loss: 2.1660112e-06
Iter: 1240 loss: 2.16562876e-06
Iter: 1241 loss: 2.16350827e-06
Iter: 1242 loss: 2.1603978e-06
Iter: 1243 loss: 2.17589968e-06
Iter: 1244 loss: 2.15991827e-06
Iter: 1245 loss: 2.1563e-06
Iter: 1246 loss: 2.1713131e-06
Iter: 1247 loss: 2.15550017e-06
Iter: 1248 loss: 2.15326736e-06
Iter: 1249 loss: 2.15161572e-06
Iter: 1250 loss: 2.15087516e-06
Iter: 1251 loss: 2.1466019e-06
Iter: 1252 loss: 2.17241836e-06
Iter: 1253 loss: 2.1461442e-06
Iter: 1254 loss: 2.1433209e-06
Iter: 1255 loss: 2.14204829e-06
Iter: 1256 loss: 2.14074475e-06
Iter: 1257 loss: 2.13698195e-06
Iter: 1258 loss: 2.1536091e-06
Iter: 1259 loss: 2.1361825e-06
Iter: 1260 loss: 2.13227e-06
Iter: 1261 loss: 2.14253441e-06
Iter: 1262 loss: 2.13088379e-06
Iter: 1263 loss: 2.12757413e-06
Iter: 1264 loss: 2.13692442e-06
Iter: 1265 loss: 2.12649502e-06
Iter: 1266 loss: 2.12273721e-06
Iter: 1267 loss: 2.1271203e-06
Iter: 1268 loss: 2.12066266e-06
Iter: 1269 loss: 2.11692964e-06
Iter: 1270 loss: 2.16505873e-06
Iter: 1271 loss: 2.11685892e-06
Iter: 1272 loss: 2.11446491e-06
Iter: 1273 loss: 2.11232964e-06
Iter: 1274 loss: 2.1116748e-06
Iter: 1275 loss: 2.10804183e-06
Iter: 1276 loss: 2.1147705e-06
Iter: 1277 loss: 2.10644521e-06
Iter: 1278 loss: 2.10251892e-06
Iter: 1279 loss: 2.12214172e-06
Iter: 1280 loss: 2.10185317e-06
Iter: 1281 loss: 2.09775908e-06
Iter: 1282 loss: 2.11484439e-06
Iter: 1283 loss: 2.09684231e-06
Iter: 1284 loss: 2.09413861e-06
Iter: 1285 loss: 2.0945231e-06
Iter: 1286 loss: 2.0919565e-06
Iter: 1287 loss: 2.08885854e-06
Iter: 1288 loss: 2.08882443e-06
Iter: 1289 loss: 2.08640131e-06
Iter: 1290 loss: 2.0848106e-06
Iter: 1291 loss: 2.08381152e-06
Iter: 1292 loss: 2.08060874e-06
Iter: 1293 loss: 2.09958353e-06
Iter: 1294 loss: 2.08024085e-06
Iter: 1295 loss: 2.07707831e-06
Iter: 1296 loss: 2.07958e-06
Iter: 1297 loss: 2.07527501e-06
Iter: 1298 loss: 2.0718885e-06
Iter: 1299 loss: 2.07418543e-06
Iter: 1300 loss: 2.06972709e-06
Iter: 1301 loss: 2.06610093e-06
Iter: 1302 loss: 2.0792836e-06
Iter: 1303 loss: 2.06519098e-06
Iter: 1304 loss: 2.06115851e-06
Iter: 1305 loss: 2.07577796e-06
Iter: 1306 loss: 2.06021446e-06
Iter: 1307 loss: 2.05659853e-06
Iter: 1308 loss: 2.06707296e-06
Iter: 1309 loss: 2.05554534e-06
Iter: 1310 loss: 2.05261108e-06
Iter: 1311 loss: 2.06516324e-06
Iter: 1312 loss: 2.05205629e-06
Iter: 1313 loss: 2.04880416e-06
Iter: 1314 loss: 2.05523156e-06
Iter: 1315 loss: 2.047507e-06
Iter: 1316 loss: 2.04489425e-06
Iter: 1317 loss: 2.04146772e-06
Iter: 1318 loss: 2.04111939e-06
Iter: 1319 loss: 2.03661e-06
Iter: 1320 loss: 2.077114e-06
Iter: 1321 loss: 2.03648142e-06
Iter: 1322 loss: 2.03347031e-06
Iter: 1323 loss: 2.05636752e-06
Iter: 1324 loss: 2.03326408e-06
Iter: 1325 loss: 2.0303728e-06
Iter: 1326 loss: 2.03003356e-06
Iter: 1327 loss: 2.02797287e-06
Iter: 1328 loss: 2.02562956e-06
Iter: 1329 loss: 2.02560705e-06
Iter: 1330 loss: 2.02333058e-06
Iter: 1331 loss: 2.01966418e-06
Iter: 1332 loss: 2.01957346e-06
Iter: 1333 loss: 2.01646503e-06
Iter: 1334 loss: 2.04175149e-06
Iter: 1335 loss: 2.01622765e-06
Iter: 1336 loss: 2.01308467e-06
Iter: 1337 loss: 2.0178868e-06
Iter: 1338 loss: 2.01150942e-06
Iter: 1339 loss: 2.00840032e-06
Iter: 1340 loss: 2.0062887e-06
Iter: 1341 loss: 2.00514046e-06
Iter: 1342 loss: 2.00112072e-06
Iter: 1343 loss: 2.01993839e-06
Iter: 1344 loss: 2.00037493e-06
Iter: 1345 loss: 1.99620195e-06
Iter: 1346 loss: 2.01012654e-06
Iter: 1347 loss: 1.99521105e-06
Iter: 1348 loss: 1.991913e-06
Iter: 1349 loss: 2.01339685e-06
Iter: 1350 loss: 1.99157421e-06
Iter: 1351 loss: 1.98875091e-06
Iter: 1352 loss: 1.99310171e-06
Iter: 1353 loss: 1.98743783e-06
Iter: 1354 loss: 1.9841309e-06
Iter: 1355 loss: 1.98945963e-06
Iter: 1356 loss: 1.98256294e-06
Iter: 1357 loss: 1.97964482e-06
Iter: 1358 loss: 2.01732291e-06
Iter: 1359 loss: 1.97967029e-06
Iter: 1360 loss: 1.97737063e-06
Iter: 1361 loss: 1.97338773e-06
Iter: 1362 loss: 1.97341706e-06
Iter: 1363 loss: 1.96990163e-06
Iter: 1364 loss: 1.99154238e-06
Iter: 1365 loss: 1.9694794e-06
Iter: 1366 loss: 1.96622955e-06
Iter: 1367 loss: 1.99274e-06
Iter: 1368 loss: 1.96594669e-06
Iter: 1369 loss: 1.96367751e-06
Iter: 1370 loss: 1.96820793e-06
Iter: 1371 loss: 1.96267183e-06
Iter: 1372 loss: 1.96016686e-06
Iter: 1373 loss: 1.96857286e-06
Iter: 1374 loss: 1.95944131e-06
Iter: 1375 loss: 1.95705275e-06
Iter: 1376 loss: 1.95535108e-06
Iter: 1377 loss: 1.95462667e-06
Iter: 1378 loss: 1.95131975e-06
Iter: 1379 loss: 1.96520841e-06
Iter: 1380 loss: 1.95068606e-06
Iter: 1381 loss: 1.947388e-06
Iter: 1382 loss: 1.96505766e-06
Iter: 1383 loss: 1.94687391e-06
Iter: 1384 loss: 1.94392487e-06
Iter: 1385 loss: 1.94439144e-06
Iter: 1386 loss: 1.9416525e-06
Iter: 1387 loss: 1.93788492e-06
Iter: 1388 loss: 1.93814662e-06
Iter: 1389 loss: 1.93490246e-06
Iter: 1390 loss: 1.93068513e-06
Iter: 1391 loss: 1.95865e-06
Iter: 1392 loss: 1.9302081e-06
Iter: 1393 loss: 1.92694915e-06
Iter: 1394 loss: 1.95038228e-06
Iter: 1395 loss: 1.92663811e-06
Iter: 1396 loss: 1.92371203e-06
Iter: 1397 loss: 1.92863786e-06
Iter: 1398 loss: 1.92244488e-06
Iter: 1399 loss: 1.9192421e-06
Iter: 1400 loss: 1.93002143e-06
Iter: 1401 loss: 1.9183949e-06
Iter: 1402 loss: 1.91537447e-06
Iter: 1403 loss: 1.92712241e-06
Iter: 1404 loss: 1.9146255e-06
Iter: 1405 loss: 1.91246863e-06
Iter: 1406 loss: 1.90906781e-06
Iter: 1407 loss: 1.90901847e-06
Iter: 1408 loss: 1.9066764e-06
Iter: 1409 loss: 1.90622245e-06
Iter: 1410 loss: 1.90422588e-06
Iter: 1411 loss: 1.90333219e-06
Iter: 1412 loss: 1.90238302e-06
Iter: 1413 loss: 1.89965806e-06
Iter: 1414 loss: 1.91109643e-06
Iter: 1415 loss: 1.89916454e-06
Iter: 1416 loss: 1.89638536e-06
Iter: 1417 loss: 1.89706566e-06
Iter: 1418 loss: 1.8943997e-06
Iter: 1419 loss: 1.89124376e-06
Iter: 1420 loss: 1.89694424e-06
Iter: 1421 loss: 1.88987769e-06
Iter: 1422 loss: 1.88731951e-06
Iter: 1423 loss: 1.91883305e-06
Iter: 1424 loss: 1.88728393e-06
Iter: 1425 loss: 1.88477838e-06
Iter: 1426 loss: 1.88182833e-06
Iter: 1427 loss: 1.88153945e-06
Iter: 1428 loss: 1.87804596e-06
Iter: 1429 loss: 1.89042976e-06
Iter: 1430 loss: 1.87714522e-06
Iter: 1431 loss: 1.87371825e-06
Iter: 1432 loss: 1.88063984e-06
Iter: 1433 loss: 1.87229728e-06
Iter: 1434 loss: 1.86866066e-06
Iter: 1435 loss: 1.88354682e-06
Iter: 1436 loss: 1.86787872e-06
Iter: 1437 loss: 1.86516343e-06
Iter: 1438 loss: 1.89904779e-06
Iter: 1439 loss: 1.86514865e-06
Iter: 1440 loss: 1.86313332e-06
Iter: 1441 loss: 1.8608697e-06
Iter: 1442 loss: 1.86060697e-06
Iter: 1443 loss: 1.85757949e-06
Iter: 1444 loss: 1.88789534e-06
Iter: 1445 loss: 1.85749434e-06
Iter: 1446 loss: 1.85531565e-06
Iter: 1447 loss: 1.85665408e-06
Iter: 1448 loss: 1.85387762e-06
Iter: 1449 loss: 1.85152498e-06
Iter: 1450 loss: 1.87901344e-06
Iter: 1451 loss: 1.85152953e-06
Iter: 1452 loss: 1.84946964e-06
Iter: 1453 loss: 1.84648559e-06
Iter: 1454 loss: 1.84636951e-06
Iter: 1455 loss: 1.84310784e-06
Iter: 1456 loss: 1.85854458e-06
Iter: 1457 loss: 1.842546e-06
Iter: 1458 loss: 1.8396787e-06
Iter: 1459 loss: 1.85533509e-06
Iter: 1460 loss: 1.83922214e-06
Iter: 1461 loss: 1.83687257e-06
Iter: 1462 loss: 1.83551765e-06
Iter: 1463 loss: 1.8344756e-06
Iter: 1464 loss: 1.83172222e-06
Iter: 1465 loss: 1.86048146e-06
Iter: 1466 loss: 1.83169141e-06
Iter: 1467 loss: 1.82918802e-06
Iter: 1468 loss: 1.83456336e-06
Iter: 1469 loss: 1.82833332e-06
Iter: 1470 loss: 1.82583858e-06
Iter: 1471 loss: 1.82385656e-06
Iter: 1472 loss: 1.82303711e-06
Iter: 1473 loss: 1.81975395e-06
Iter: 1474 loss: 1.83150826e-06
Iter: 1475 loss: 1.81892619e-06
Iter: 1476 loss: 1.81541259e-06
Iter: 1477 loss: 1.83121188e-06
Iter: 1478 loss: 1.8147673e-06
Iter: 1479 loss: 1.81171686e-06
Iter: 1480 loss: 1.83438806e-06
Iter: 1481 loss: 1.81145128e-06
Iter: 1482 loss: 1.80901804e-06
Iter: 1483 loss: 1.8083266e-06
Iter: 1484 loss: 1.80684833e-06
Iter: 1485 loss: 1.80393511e-06
Iter: 1486 loss: 1.82007807e-06
Iter: 1487 loss: 1.80349525e-06
Iter: 1488 loss: 1.8007e-06
Iter: 1489 loss: 1.81584414e-06
Iter: 1490 loss: 1.8002479e-06
Iter: 1491 loss: 1.7980733e-06
Iter: 1492 loss: 1.8016317e-06
Iter: 1493 loss: 1.79711321e-06
Iter: 1494 loss: 1.79441724e-06
Iter: 1495 loss: 1.79491406e-06
Iter: 1496 loss: 1.79244e-06
Iter: 1497 loss: 1.78949199e-06
Iter: 1498 loss: 1.79476956e-06
Iter: 1499 loss: 1.78811888e-06
Iter: 1500 loss: 1.78504411e-06
Iter: 1501 loss: 1.81189296e-06
Iter: 1502 loss: 1.78484959e-06
Iter: 1503 loss: 1.78231403e-06
Iter: 1504 loss: 1.780915e-06
Iter: 1505 loss: 1.77975619e-06
Iter: 1506 loss: 1.77669324e-06
Iter: 1507 loss: 1.79805306e-06
Iter: 1508 loss: 1.77642801e-06
Iter: 1509 loss: 1.77360778e-06
Iter: 1510 loss: 1.78460311e-06
Iter: 1511 loss: 1.77300649e-06
Iter: 1512 loss: 1.77071763e-06
Iter: 1513 loss: 1.7700313e-06
Iter: 1514 loss: 1.76877973e-06
Iter: 1515 loss: 1.76537606e-06
Iter: 1516 loss: 1.77209972e-06
Iter: 1517 loss: 1.76404444e-06
Iter: 1518 loss: 1.76104118e-06
Iter: 1519 loss: 1.78989103e-06
Iter: 1520 loss: 1.76097467e-06
Iter: 1521 loss: 1.75874698e-06
Iter: 1522 loss: 1.76893832e-06
Iter: 1523 loss: 1.75823686e-06
Iter: 1524 loss: 1.75604771e-06
Iter: 1525 loss: 1.75352284e-06
Iter: 1526 loss: 1.75318496e-06
Iter: 1527 loss: 1.75121659e-06
Iter: 1528 loss: 1.75101536e-06
Iter: 1529 loss: 1.74926231e-06
Iter: 1530 loss: 1.74972627e-06
Iter: 1531 loss: 1.74792558e-06
Iter: 1532 loss: 1.74590275e-06
Iter: 1533 loss: 1.74623221e-06
Iter: 1534 loss: 1.74437127e-06
Iter: 1535 loss: 1.74122181e-06
Iter: 1536 loss: 1.75603827e-06
Iter: 1537 loss: 1.74070124e-06
Iter: 1538 loss: 1.73859678e-06
Iter: 1539 loss: 1.73986075e-06
Iter: 1540 loss: 1.73718558e-06
Iter: 1541 loss: 1.73453759e-06
Iter: 1542 loss: 1.75565492e-06
Iter: 1543 loss: 1.73432068e-06
Iter: 1544 loss: 1.73189358e-06
Iter: 1545 loss: 1.72965213e-06
Iter: 1546 loss: 1.72909211e-06
Iter: 1547 loss: 1.72694877e-06
Iter: 1548 loss: 1.72693444e-06
Iter: 1549 loss: 1.72473904e-06
Iter: 1550 loss: 1.72254954e-06
Iter: 1551 loss: 1.72215766e-06
Iter: 1552 loss: 1.71931083e-06
Iter: 1553 loss: 1.7331223e-06
Iter: 1554 loss: 1.71883664e-06
Iter: 1555 loss: 1.71629756e-06
Iter: 1556 loss: 1.71536794e-06
Iter: 1557 loss: 1.71401575e-06
Iter: 1558 loss: 1.71181978e-06
Iter: 1559 loss: 1.71160514e-06
Iter: 1560 loss: 1.70986846e-06
Iter: 1561 loss: 1.7090681e-06
Iter: 1562 loss: 1.70816452e-06
Iter: 1563 loss: 1.70594785e-06
Iter: 1564 loss: 1.72398973e-06
Iter: 1565 loss: 1.70586532e-06
Iter: 1566 loss: 1.7036798e-06
Iter: 1567 loss: 1.70606415e-06
Iter: 1568 loss: 1.70247563e-06
Iter: 1569 loss: 1.70039175e-06
Iter: 1570 loss: 1.69851137e-06
Iter: 1571 loss: 1.69800273e-06
Iter: 1572 loss: 1.69509747e-06
Iter: 1573 loss: 1.73126205e-06
Iter: 1574 loss: 1.6950089e-06
Iter: 1575 loss: 1.69305258e-06
Iter: 1576 loss: 1.69548366e-06
Iter: 1577 loss: 1.6920493e-06
Iter: 1578 loss: 1.68968586e-06
Iter: 1579 loss: 1.695366e-06
Iter: 1580 loss: 1.68884014e-06
Iter: 1581 loss: 1.68641964e-06
Iter: 1582 loss: 1.6966552e-06
Iter: 1583 loss: 1.68590566e-06
Iter: 1584 loss: 1.68370343e-06
Iter: 1585 loss: 1.68483416e-06
Iter: 1586 loss: 1.68226154e-06
Iter: 1587 loss: 1.67969699e-06
Iter: 1588 loss: 1.69503176e-06
Iter: 1589 loss: 1.67934729e-06
Iter: 1590 loss: 1.67686312e-06
Iter: 1591 loss: 1.67900043e-06
Iter: 1592 loss: 1.67545204e-06
Iter: 1593 loss: 1.67305609e-06
Iter: 1594 loss: 1.6738901e-06
Iter: 1595 loss: 1.67141161e-06
Iter: 1596 loss: 1.66834843e-06
Iter: 1597 loss: 1.69468217e-06
Iter: 1598 loss: 1.66816562e-06
Iter: 1599 loss: 1.66554491e-06
Iter: 1600 loss: 1.67681083e-06
Iter: 1601 loss: 1.66505242e-06
Iter: 1602 loss: 1.66312225e-06
Iter: 1603 loss: 1.66593486e-06
Iter: 1604 loss: 1.66214511e-06
Iter: 1605 loss: 1.65973916e-06
Iter: 1606 loss: 1.67395376e-06
Iter: 1607 loss: 1.65949655e-06
Iter: 1608 loss: 1.65793608e-06
Iter: 1609 loss: 1.65631036e-06
Iter: 1610 loss: 1.65597351e-06
Iter: 1611 loss: 1.65325275e-06
Iter: 1612 loss: 1.65634583e-06
Iter: 1613 loss: 1.65180143e-06
Iter: 1614 loss: 1.64881283e-06
Iter: 1615 loss: 1.66470466e-06
Iter: 1616 loss: 1.64830499e-06
Iter: 1617 loss: 1.64559458e-06
Iter: 1618 loss: 1.66141672e-06
Iter: 1619 loss: 1.64523794e-06
Iter: 1620 loss: 1.64283131e-06
Iter: 1621 loss: 1.64241544e-06
Iter: 1622 loss: 1.64080359e-06
Iter: 1623 loss: 1.63836944e-06
Iter: 1624 loss: 1.66840414e-06
Iter: 1625 loss: 1.63837342e-06
Iter: 1626 loss: 1.63628863e-06
Iter: 1627 loss: 1.63640948e-06
Iter: 1628 loss: 1.63481786e-06
Iter: 1629 loss: 1.63255606e-06
Iter: 1630 loss: 1.64312416e-06
Iter: 1631 loss: 1.6321286e-06
Iter: 1632 loss: 1.62971958e-06
Iter: 1633 loss: 1.63427308e-06
Iter: 1634 loss: 1.62866888e-06
Iter: 1635 loss: 1.62636161e-06
Iter: 1636 loss: 1.62553351e-06
Iter: 1637 loss: 1.62421134e-06
Iter: 1638 loss: 1.62164315e-06
Iter: 1639 loss: 1.65351503e-06
Iter: 1640 loss: 1.62156186e-06
Iter: 1641 loss: 1.61911271e-06
Iter: 1642 loss: 1.62688207e-06
Iter: 1643 loss: 1.61844298e-06
Iter: 1644 loss: 1.61659671e-06
Iter: 1645 loss: 1.62243464e-06
Iter: 1646 loss: 1.61609751e-06
Iter: 1647 loss: 1.61420758e-06
Iter: 1648 loss: 1.61944229e-06
Iter: 1649 loss: 1.61356161e-06
Iter: 1650 loss: 1.61138257e-06
Iter: 1651 loss: 1.61045671e-06
Iter: 1652 loss: 1.60926663e-06
Iter: 1653 loss: 1.60713444e-06
Iter: 1654 loss: 1.60826232e-06
Iter: 1655 loss: 1.60563593e-06
Iter: 1656 loss: 1.60240756e-06
Iter: 1657 loss: 1.61857884e-06
Iter: 1658 loss: 1.60196112e-06
Iter: 1659 loss: 1.59942965e-06
Iter: 1660 loss: 1.61891751e-06
Iter: 1661 loss: 1.59917522e-06
Iter: 1662 loss: 1.5970478e-06
Iter: 1663 loss: 1.59753313e-06
Iter: 1664 loss: 1.59541173e-06
Iter: 1665 loss: 1.59271804e-06
Iter: 1666 loss: 1.60190234e-06
Iter: 1667 loss: 1.59191325e-06
Iter: 1668 loss: 1.5892208e-06
Iter: 1669 loss: 1.60282343e-06
Iter: 1670 loss: 1.58875082e-06
Iter: 1671 loss: 1.58682292e-06
Iter: 1672 loss: 1.58726402e-06
Iter: 1673 loss: 1.58540615e-06
Iter: 1674 loss: 1.58282342e-06
Iter: 1675 loss: 1.60228831e-06
Iter: 1676 loss: 1.58252783e-06
Iter: 1677 loss: 1.5807268e-06
Iter: 1678 loss: 1.58014029e-06
Iter: 1679 loss: 1.57913746e-06
Iter: 1680 loss: 1.57625982e-06
Iter: 1681 loss: 1.58246621e-06
Iter: 1682 loss: 1.57514546e-06
Iter: 1683 loss: 1.57276963e-06
Iter: 1684 loss: 1.6006245e-06
Iter: 1685 loss: 1.57274167e-06
Iter: 1686 loss: 1.57065e-06
Iter: 1687 loss: 1.57209331e-06
Iter: 1688 loss: 1.5692674e-06
Iter: 1689 loss: 1.56704277e-06
Iter: 1690 loss: 1.57429452e-06
Iter: 1691 loss: 1.56635247e-06
Iter: 1692 loss: 1.56424016e-06
Iter: 1693 loss: 1.58377054e-06
Iter: 1694 loss: 1.56414308e-06
Iter: 1695 loss: 1.56275473e-06
Iter: 1696 loss: 1.56025681e-06
Iter: 1697 loss: 1.61979347e-06
Iter: 1698 loss: 1.56026658e-06
Iter: 1699 loss: 1.55741702e-06
Iter: 1700 loss: 1.5625393e-06
Iter: 1701 loss: 1.55617352e-06
Iter: 1702 loss: 1.55378734e-06
Iter: 1703 loss: 1.58496414e-06
Iter: 1704 loss: 1.55376529e-06
Iter: 1705 loss: 1.55173598e-06
Iter: 1706 loss: 1.55426358e-06
Iter: 1707 loss: 1.55065368e-06
Iter: 1708 loss: 1.54845475e-06
Iter: 1709 loss: 1.55374778e-06
Iter: 1710 loss: 1.54762051e-06
Iter: 1711 loss: 1.54516624e-06
Iter: 1712 loss: 1.55276041e-06
Iter: 1713 loss: 1.54448912e-06
Iter: 1714 loss: 1.54208385e-06
Iter: 1715 loss: 1.54993563e-06
Iter: 1716 loss: 1.5414787e-06
Iter: 1717 loss: 1.53943392e-06
Iter: 1718 loss: 1.54353006e-06
Iter: 1719 loss: 1.53864448e-06
Iter: 1720 loss: 1.53647318e-06
Iter: 1721 loss: 1.54622671e-06
Iter: 1722 loss: 1.53601832e-06
Iter: 1723 loss: 1.53422081e-06
Iter: 1724 loss: 1.53268593e-06
Iter: 1725 loss: 1.53221e-06
Iter: 1726 loss: 1.52944267e-06
Iter: 1727 loss: 1.54791815e-06
Iter: 1728 loss: 1.52917983e-06
Iter: 1729 loss: 1.52754251e-06
Iter: 1730 loss: 1.55014584e-06
Iter: 1731 loss: 1.52754978e-06
Iter: 1732 loss: 1.52602365e-06
Iter: 1733 loss: 1.52414123e-06
Iter: 1734 loss: 1.52397411e-06
Iter: 1735 loss: 1.52188113e-06
Iter: 1736 loss: 1.5447572e-06
Iter: 1737 loss: 1.52182565e-06
Iter: 1738 loss: 1.52004304e-06
Iter: 1739 loss: 1.52033897e-06
Iter: 1740 loss: 1.51871632e-06
Iter: 1741 loss: 1.5167459e-06
Iter: 1742 loss: 1.51463678e-06
Iter: 1743 loss: 1.51436461e-06
Iter: 1744 loss: 1.510728e-06
Iter: 1745 loss: 1.524136e-06
Iter: 1746 loss: 1.5098924e-06
Iter: 1747 loss: 1.50717369e-06
Iter: 1748 loss: 1.54192e-06
Iter: 1749 loss: 1.50713856e-06
Iter: 1750 loss: 1.50498113e-06
Iter: 1751 loss: 1.50803044e-06
Iter: 1752 loss: 1.50400592e-06
Iter: 1753 loss: 1.50173616e-06
Iter: 1754 loss: 1.51000131e-06
Iter: 1755 loss: 1.50117194e-06
Iter: 1756 loss: 1.49923551e-06
Iter: 1757 loss: 1.50877713e-06
Iter: 1758 loss: 1.49888024e-06
Iter: 1759 loss: 1.49726463e-06
Iter: 1760 loss: 1.4966206e-06
Iter: 1761 loss: 1.49575885e-06
Iter: 1762 loss: 1.49352059e-06
Iter: 1763 loss: 1.50640426e-06
Iter: 1764 loss: 1.49317941e-06
Iter: 1765 loss: 1.49134723e-06
Iter: 1766 loss: 1.50077926e-06
Iter: 1767 loss: 1.49103084e-06
Iter: 1768 loss: 1.48921504e-06
Iter: 1769 loss: 1.4912506e-06
Iter: 1770 loss: 1.48820322e-06
Iter: 1771 loss: 1.48613913e-06
Iter: 1772 loss: 1.50019969e-06
Iter: 1773 loss: 1.4858922e-06
Iter: 1774 loss: 1.48422123e-06
Iter: 1775 loss: 1.48187883e-06
Iter: 1776 loss: 1.48181812e-06
Iter: 1777 loss: 1.47910873e-06
Iter: 1778 loss: 1.49688822e-06
Iter: 1779 loss: 1.47879553e-06
Iter: 1780 loss: 1.47645346e-06
Iter: 1781 loss: 1.49115135e-06
Iter: 1782 loss: 1.47623143e-06
Iter: 1783 loss: 1.47451283e-06
Iter: 1784 loss: 1.47222181e-06
Iter: 1785 loss: 1.47210255e-06
Iter: 1786 loss: 1.46946957e-06
Iter: 1787 loss: 1.47596324e-06
Iter: 1788 loss: 1.46857076e-06
Iter: 1789 loss: 1.46574621e-06
Iter: 1790 loss: 1.48513539e-06
Iter: 1791 loss: 1.46546836e-06
Iter: 1792 loss: 1.46352488e-06
Iter: 1793 loss: 1.47258731e-06
Iter: 1794 loss: 1.46314994e-06
Iter: 1795 loss: 1.46098239e-06
Iter: 1796 loss: 1.46153729e-06
Iter: 1797 loss: 1.45941817e-06
Iter: 1798 loss: 1.45704439e-06
Iter: 1799 loss: 1.47506285e-06
Iter: 1800 loss: 1.45690501e-06
Iter: 1801 loss: 1.45486626e-06
Iter: 1802 loss: 1.45938338e-06
Iter: 1803 loss: 1.45404476e-06
Iter: 1804 loss: 1.4521421e-06
Iter: 1805 loss: 1.45438776e-06
Iter: 1806 loss: 1.45114905e-06
Iter: 1807 loss: 1.44924877e-06
Iter: 1808 loss: 1.47213689e-06
Iter: 1809 loss: 1.44922524e-06
Iter: 1810 loss: 1.44766295e-06
Iter: 1811 loss: 1.44736623e-06
Iter: 1812 loss: 1.44635e-06
Iter: 1813 loss: 1.44438968e-06
Iter: 1814 loss: 1.45390106e-06
Iter: 1815 loss: 1.44397779e-06
Iter: 1816 loss: 1.4421737e-06
Iter: 1817 loss: 1.441932e-06
Iter: 1818 loss: 1.44058436e-06
Iter: 1819 loss: 1.43807949e-06
Iter: 1820 loss: 1.44024784e-06
Iter: 1821 loss: 1.43659406e-06
Iter: 1822 loss: 1.43377213e-06
Iter: 1823 loss: 1.44508613e-06
Iter: 1824 loss: 1.43319096e-06
Iter: 1825 loss: 1.43121758e-06
Iter: 1826 loss: 1.45774379e-06
Iter: 1827 loss: 1.43122293e-06
Iter: 1828 loss: 1.42954116e-06
Iter: 1829 loss: 1.42983697e-06
Iter: 1830 loss: 1.4282773e-06
Iter: 1831 loss: 1.42613487e-06
Iter: 1832 loss: 1.42889405e-06
Iter: 1833 loss: 1.42508907e-06
Iter: 1834 loss: 1.42288013e-06
Iter: 1835 loss: 1.43206034e-06
Iter: 1836 loss: 1.42238207e-06
Iter: 1837 loss: 1.42029012e-06
Iter: 1838 loss: 1.42717408e-06
Iter: 1839 loss: 1.4196936e-06
Iter: 1840 loss: 1.41732346e-06
Iter: 1841 loss: 1.417965e-06
Iter: 1842 loss: 1.41564431e-06
Iter: 1843 loss: 1.41354678e-06
Iter: 1844 loss: 1.44459329e-06
Iter: 1845 loss: 1.41354872e-06
Iter: 1846 loss: 1.4117619e-06
Iter: 1847 loss: 1.41224655e-06
Iter: 1848 loss: 1.41042528e-06
Iter: 1849 loss: 1.40902512e-06
Iter: 1850 loss: 1.40901557e-06
Iter: 1851 loss: 1.40790337e-06
Iter: 1852 loss: 1.40548968e-06
Iter: 1853 loss: 1.44371916e-06
Iter: 1854 loss: 1.40543716e-06
Iter: 1855 loss: 1.4033277e-06
Iter: 1856 loss: 1.43188913e-06
Iter: 1857 loss: 1.40327893e-06
Iter: 1858 loss: 1.40165e-06
Iter: 1859 loss: 1.40260636e-06
Iter: 1860 loss: 1.40051793e-06
Iter: 1861 loss: 1.39859344e-06
Iter: 1862 loss: 1.3998465e-06
Iter: 1863 loss: 1.39730685e-06
Iter: 1864 loss: 1.39503982e-06
Iter: 1865 loss: 1.40854331e-06
Iter: 1866 loss: 1.39473445e-06
Iter: 1867 loss: 1.3926865e-06
Iter: 1868 loss: 1.40529164e-06
Iter: 1869 loss: 1.39242809e-06
Iter: 1870 loss: 1.39102679e-06
Iter: 1871 loss: 1.38936537e-06
Iter: 1872 loss: 1.3891879e-06
Iter: 1873 loss: 1.38664598e-06
Iter: 1874 loss: 1.39592407e-06
Iter: 1875 loss: 1.38612199e-06
Iter: 1876 loss: 1.38375208e-06
Iter: 1877 loss: 1.38965538e-06
Iter: 1878 loss: 1.38294399e-06
Iter: 1879 loss: 1.38116911e-06
Iter: 1880 loss: 1.4068421e-06
Iter: 1881 loss: 1.38115058e-06
Iter: 1882 loss: 1.37973461e-06
Iter: 1883 loss: 1.38159248e-06
Iter: 1884 loss: 1.37890129e-06
Iter: 1885 loss: 1.37743496e-06
Iter: 1886 loss: 1.384602e-06
Iter: 1887 loss: 1.3771662e-06
Iter: 1888 loss: 1.37559277e-06
Iter: 1889 loss: 1.37497466e-06
Iter: 1890 loss: 1.37405596e-06
Iter: 1891 loss: 1.37233792e-06
Iter: 1892 loss: 1.37553684e-06
Iter: 1893 loss: 1.37155916e-06
Iter: 1894 loss: 1.36920175e-06
Iter: 1895 loss: 1.38101086e-06
Iter: 1896 loss: 1.36879544e-06
Iter: 1897 loss: 1.36723634e-06
Iter: 1898 loss: 1.36614608e-06
Iter: 1899 loss: 1.3656072e-06
Iter: 1900 loss: 1.36339543e-06
Iter: 1901 loss: 1.37882785e-06
Iter: 1902 loss: 1.36324707e-06
Iter: 1903 loss: 1.361145e-06
Iter: 1904 loss: 1.37096686e-06
Iter: 1905 loss: 1.36078029e-06
Iter: 1906 loss: 1.35921346e-06
Iter: 1907 loss: 1.3587935e-06
Iter: 1908 loss: 1.35781443e-06
Iter: 1909 loss: 1.35554296e-06
Iter: 1910 loss: 1.35954463e-06
Iter: 1911 loss: 1.3546022e-06
Iter: 1912 loss: 1.35213577e-06
Iter: 1913 loss: 1.35852258e-06
Iter: 1914 loss: 1.35128232e-06
Iter: 1915 loss: 1.34948425e-06
Iter: 1916 loss: 1.34948937e-06
Iter: 1917 loss: 1.34808226e-06
Iter: 1918 loss: 1.34974493e-06
Iter: 1919 loss: 1.3474048e-06
Iter: 1920 loss: 1.345594e-06
Iter: 1921 loss: 1.35116397e-06
Iter: 1922 loss: 1.34508059e-06
Iter: 1923 loss: 1.34338666e-06
Iter: 1924 loss: 1.34536e-06
Iter: 1925 loss: 1.34245317e-06
Iter: 1926 loss: 1.34091329e-06
Iter: 1927 loss: 1.35019764e-06
Iter: 1928 loss: 1.34074457e-06
Iter: 1929 loss: 1.33932349e-06
Iter: 1930 loss: 1.33884896e-06
Iter: 1931 loss: 1.33802382e-06
Iter: 1932 loss: 1.33593198e-06
Iter: 1933 loss: 1.34035974e-06
Iter: 1934 loss: 1.33516073e-06
Iter: 1935 loss: 1.33328251e-06
Iter: 1936 loss: 1.34966194e-06
Iter: 1937 loss: 1.33318031e-06
Iter: 1938 loss: 1.33144511e-06
Iter: 1939 loss: 1.33115361e-06
Iter: 1940 loss: 1.32995251e-06
Iter: 1941 loss: 1.32805144e-06
Iter: 1942 loss: 1.3373766e-06
Iter: 1943 loss: 1.32767059e-06
Iter: 1944 loss: 1.32608022e-06
Iter: 1945 loss: 1.33708249e-06
Iter: 1946 loss: 1.32593061e-06
Iter: 1947 loss: 1.32437413e-06
Iter: 1948 loss: 1.32347225e-06
Iter: 1949 loss: 1.32282526e-06
Iter: 1950 loss: 1.32076093e-06
Iter: 1951 loss: 1.32680373e-06
Iter: 1952 loss: 1.32017283e-06
Iter: 1953 loss: 1.3181666e-06
Iter: 1954 loss: 1.3185761e-06
Iter: 1955 loss: 1.31665934e-06
Iter: 1956 loss: 1.31527167e-06
Iter: 1957 loss: 1.3149413e-06
Iter: 1958 loss: 1.31402e-06
Iter: 1959 loss: 1.31243632e-06
Iter: 1960 loss: 1.31243632e-06
Iter: 1961 loss: 1.31081038e-06
Iter: 1962 loss: 1.323476e-06
Iter: 1963 loss: 1.3106893e-06
Iter: 1964 loss: 1.30901344e-06
Iter: 1965 loss: 1.31117781e-06
Iter: 1966 loss: 1.30827084e-06
Iter: 1967 loss: 1.30652552e-06
Iter: 1968 loss: 1.30584658e-06
Iter: 1969 loss: 1.30495471e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2.8
+ date
Wed Nov  4 15:53:09 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.4/300_300_300_1 --function f2 --psi 1 --alpha 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f474aac50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f474a9e2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4729491730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f472948a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f472948abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47294b76a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4729417bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4729436400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4729436598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47293a59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47293a5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47293a5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4729460510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47044778c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f470446b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47043fe9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47043feea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47044037b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47042cc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47044038c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4704403840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4704335510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4704394950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f470436b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f470436b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47042418c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f470422e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f470422e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f470422e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47041c4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47041baa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47041ac2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47042768c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4704276d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f47042a0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4704282620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.007257855
test_loss: 0.010064326
train_loss: 0.005813945
test_loss: 0.009326246
train_loss: 0.0061409753
test_loss: 0.009154677
train_loss: 0.0058206553
test_loss: 0.009123377
train_loss: 0.0054331883
test_loss: 0.009054583
train_loss: 0.005298276
test_loss: 0.009060665
train_loss: 0.0051721763
test_loss: 0.008903408
train_loss: 0.0054996917
test_loss: 0.008692909
train_loss: 0.0055836667
test_loss: 0.008860563
train_loss: 0.0054196073
test_loss: 0.008899639
train_loss: 0.0053271805
test_loss: 0.00882176
train_loss: 0.0058060642
test_loss: 0.008948528
train_loss: 0.0057302434
test_loss: 0.0091614835
train_loss: 0.00498488
test_loss: 0.008677694
train_loss: 0.0049668453
test_loss: 0.008623037
train_loss: 0.005440738
test_loss: 0.008977678
train_loss: 0.005201622
test_loss: 0.008775934
train_loss: 0.0058742445
test_loss: 0.008985847
train_loss: 0.005596772
test_loss: 0.008708518
train_loss: 0.00516498
test_loss: 0.008690545
train_loss: 0.005520825
test_loss: 0.008677484
train_loss: 0.0050159865
test_loss: 0.008469009
train_loss: 0.005189031
test_loss: 0.008572614
train_loss: 0.005184739
test_loss: 0.008849103
train_loss: 0.0056613763
test_loss: 0.008542963
train_loss: 0.0050291982
test_loss: 0.008642471
train_loss: 0.005030323
test_loss: 0.008684887
train_loss: 0.005185046
test_loss: 0.008571161
train_loss: 0.0049198424
test_loss: 0.008423977
train_loss: 0.0049202987
test_loss: 0.008505401
train_loss: 0.0050708395
test_loss: 0.008628437
train_loss: 0.0053629675
test_loss: 0.00864955
train_loss: 0.0051382254
test_loss: 0.008664045
train_loss: 0.0050142095
test_loss: 0.008428018
train_loss: 0.0049315407
test_loss: 0.008614799
train_loss: 0.00536256
test_loss: 0.008679151
train_loss: 0.0044508204
test_loss: 0.008489718
train_loss: 0.0050207875
test_loss: 0.008666449
train_loss: 0.004728947
test_loss: 0.008429609
train_loss: 0.005090346
test_loss: 0.008773869
train_loss: 0.0045822705
test_loss: 0.008387371
train_loss: 0.0048267986
test_loss: 0.008622452
train_loss: 0.005381557
test_loss: 0.008669384
train_loss: 0.004877184
test_loss: 0.008517409
train_loss: 0.005022276
test_loss: 0.008418893
train_loss: 0.0048678853
test_loss: 0.00856663
train_loss: 0.0050039818
test_loss: 0.008551741
train_loss: 0.0046311235
test_loss: 0.008431
train_loss: 0.004556131
test_loss: 0.008376835
train_loss: 0.0046594334
test_loss: 0.008400924
train_loss: 0.0051236106
test_loss: 0.008549263
train_loss: 0.005130235
test_loss: 0.00844459
train_loss: 0.004975083
test_loss: 0.008422173
train_loss: 0.004675977
test_loss: 0.00850502
train_loss: 0.0049620746
test_loss: 0.008460564
train_loss: 0.0046468135
test_loss: 0.008290794
train_loss: 0.0049699354
test_loss: 0.008560633
train_loss: 0.004928836
test_loss: 0.008414192
train_loss: 0.004915079
test_loss: 0.0086153615
train_loss: 0.0049353107
test_loss: 0.00836316
train_loss: 0.0044954685
test_loss: 0.008318979
train_loss: 0.004601678
test_loss: 0.008380858
train_loss: 0.0047849994
test_loss: 0.008434494
train_loss: 0.0048516453
test_loss: 0.008282562
train_loss: 0.0046187243
test_loss: 0.008390723
train_loss: 0.0048876563
test_loss: 0.008459348
train_loss: 0.0050359983
test_loss: 0.00840294
train_loss: 0.004949177
test_loss: 0.008294902
train_loss: 0.0051349937
test_loss: 0.008347779
train_loss: 0.004782037
test_loss: 0.008444319
train_loss: 0.004820386
test_loss: 0.008482937
train_loss: 0.0046229335
test_loss: 0.008344106
train_loss: 0.004518319
test_loss: 0.008243362
train_loss: 0.004570786
test_loss: 0.008338156
train_loss: 0.004901159
test_loss: 0.008571671
train_loss: 0.0048736343
test_loss: 0.0083297305
train_loss: 0.004425953
test_loss: 0.008248756
train_loss: 0.0048904796
test_loss: 0.008342428
train_loss: 0.0048095994
test_loss: 0.008301425
train_loss: 0.0047222623
test_loss: 0.008212637
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.8/300_300_300_1 --optimizer lbfgs --function f2 --psi 1 --alpha 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7d3fa56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7d3fa5158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7d3f73620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5569510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5569ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5597840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5552bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b54fb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5500488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5500b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b54a9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b55006a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5500730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b544c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5454bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b53ef7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b53efd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b53b0a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5376950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b53b0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b53b0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b534eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5300ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b534ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b52dd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5276840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b552e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5276400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5223488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b51ee6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b51eeea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b51ee730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b519fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b51ee488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b5175840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b518b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.95033967e-05
Iter: 2 loss: 3.65610322e-05
Iter: 3 loss: 3.28709e-05
Iter: 4 loss: 3.25775836e-05
Iter: 5 loss: 2.81965185e-05
Iter: 6 loss: 5.59060973e-05
Iter: 7 loss: 2.76970568e-05
Iter: 8 loss: 2.51386664e-05
Iter: 9 loss: 2.58205946e-05
Iter: 10 loss: 2.32812745e-05
Iter: 11 loss: 2.132593e-05
Iter: 12 loss: 4.62056632e-05
Iter: 13 loss: 2.13109106e-05
Iter: 14 loss: 2.00230679e-05
Iter: 15 loss: 2.15425134e-05
Iter: 16 loss: 1.93393425e-05
Iter: 17 loss: 1.82254589e-05
Iter: 18 loss: 1.99079841e-05
Iter: 19 loss: 1.76944159e-05
Iter: 20 loss: 1.65917263e-05
Iter: 21 loss: 2.51589627e-05
Iter: 22 loss: 1.65139063e-05
Iter: 23 loss: 1.57293343e-05
Iter: 24 loss: 1.64867852e-05
Iter: 25 loss: 1.52820794e-05
Iter: 26 loss: 1.44743308e-05
Iter: 27 loss: 1.82868243e-05
Iter: 28 loss: 1.43265515e-05
Iter: 29 loss: 1.37318884e-05
Iter: 30 loss: 1.75174137e-05
Iter: 31 loss: 1.36662757e-05
Iter: 32 loss: 1.321745e-05
Iter: 33 loss: 1.38701398e-05
Iter: 34 loss: 1.29999062e-05
Iter: 35 loss: 1.26314826e-05
Iter: 36 loss: 1.51376253e-05
Iter: 37 loss: 1.25949782e-05
Iter: 38 loss: 1.22488664e-05
Iter: 39 loss: 1.26033865e-05
Iter: 40 loss: 1.20559562e-05
Iter: 41 loss: 1.17589825e-05
Iter: 42 loss: 1.24356538e-05
Iter: 43 loss: 1.16476112e-05
Iter: 44 loss: 1.13517726e-05
Iter: 45 loss: 1.19040724e-05
Iter: 46 loss: 1.12258967e-05
Iter: 47 loss: 1.11022237e-05
Iter: 48 loss: 1.10615983e-05
Iter: 49 loss: 1.09424645e-05
Iter: 50 loss: 1.07964343e-05
Iter: 51 loss: 1.07831538e-05
Iter: 52 loss: 1.05749696e-05
Iter: 53 loss: 1.07387568e-05
Iter: 54 loss: 1.04492065e-05
Iter: 55 loss: 1.02590257e-05
Iter: 56 loss: 1.17527361e-05
Iter: 57 loss: 1.02458425e-05
Iter: 58 loss: 1.0056785e-05
Iter: 59 loss: 1.07150463e-05
Iter: 60 loss: 1.00071175e-05
Iter: 61 loss: 9.89496402e-06
Iter: 62 loss: 9.8297005e-06
Iter: 63 loss: 9.78200478e-06
Iter: 64 loss: 9.62737795e-06
Iter: 65 loss: 1.08874447e-05
Iter: 66 loss: 9.61742808e-06
Iter: 67 loss: 9.48703564e-06
Iter: 68 loss: 9.7873417e-06
Iter: 69 loss: 9.43858868e-06
Iter: 70 loss: 9.31426439e-06
Iter: 71 loss: 9.66360858e-06
Iter: 72 loss: 9.27429937e-06
Iter: 73 loss: 9.16956378e-06
Iter: 74 loss: 9.79797733e-06
Iter: 75 loss: 9.15634519e-06
Iter: 76 loss: 9.06110654e-06
Iter: 77 loss: 9.23687367e-06
Iter: 78 loss: 9.020172e-06
Iter: 79 loss: 8.93841116e-06
Iter: 80 loss: 8.94960249e-06
Iter: 81 loss: 8.87601709e-06
Iter: 82 loss: 8.77405e-06
Iter: 83 loss: 1.00124125e-05
Iter: 84 loss: 8.77284128e-06
Iter: 85 loss: 8.70289114e-06
Iter: 86 loss: 8.85396275e-06
Iter: 87 loss: 8.67619747e-06
Iter: 88 loss: 8.59806642e-06
Iter: 89 loss: 8.92175922e-06
Iter: 90 loss: 8.58128624e-06
Iter: 91 loss: 8.51253844e-06
Iter: 92 loss: 8.46847797e-06
Iter: 93 loss: 8.44189708e-06
Iter: 94 loss: 8.356531e-06
Iter: 95 loss: 8.66474511e-06
Iter: 96 loss: 8.33508602e-06
Iter: 97 loss: 8.25341158e-06
Iter: 98 loss: 8.28066914e-06
Iter: 99 loss: 8.19599e-06
Iter: 100 loss: 8.13285715e-06
Iter: 101 loss: 8.13212e-06
Iter: 102 loss: 8.07458e-06
Iter: 103 loss: 8.15671865e-06
Iter: 104 loss: 8.04626325e-06
Iter: 105 loss: 7.98395922e-06
Iter: 106 loss: 7.94979678e-06
Iter: 107 loss: 7.92203355e-06
Iter: 108 loss: 7.84524582e-06
Iter: 109 loss: 8.1243179e-06
Iter: 110 loss: 7.82618372e-06
Iter: 111 loss: 7.76131856e-06
Iter: 112 loss: 8.38922642e-06
Iter: 113 loss: 7.75903936e-06
Iter: 114 loss: 7.71184386e-06
Iter: 115 loss: 7.81460585e-06
Iter: 116 loss: 7.69361304e-06
Iter: 117 loss: 7.64066408e-06
Iter: 118 loss: 7.73169268e-06
Iter: 119 loss: 7.61713181e-06
Iter: 120 loss: 7.5669077e-06
Iter: 121 loss: 8.02647901e-06
Iter: 122 loss: 7.5648968e-06
Iter: 123 loss: 7.52854794e-06
Iter: 124 loss: 7.55921201e-06
Iter: 125 loss: 7.50719846e-06
Iter: 126 loss: 7.4597624e-06
Iter: 127 loss: 7.73204738e-06
Iter: 128 loss: 7.45314901e-06
Iter: 129 loss: 7.41642e-06
Iter: 130 loss: 7.37860682e-06
Iter: 131 loss: 7.37151095e-06
Iter: 132 loss: 7.31954469e-06
Iter: 133 loss: 7.51510561e-06
Iter: 134 loss: 7.3070205e-06
Iter: 135 loss: 7.26155577e-06
Iter: 136 loss: 7.54060875e-06
Iter: 137 loss: 7.25621112e-06
Iter: 138 loss: 7.21498463e-06
Iter: 139 loss: 7.27915e-06
Iter: 140 loss: 7.1957993e-06
Iter: 141 loss: 7.15519809e-06
Iter: 142 loss: 7.14053385e-06
Iter: 143 loss: 7.11768917e-06
Iter: 144 loss: 7.07165782e-06
Iter: 145 loss: 7.76789511e-06
Iter: 146 loss: 7.071656e-06
Iter: 147 loss: 7.03487558e-06
Iter: 148 loss: 7.14711859e-06
Iter: 149 loss: 7.02364105e-06
Iter: 150 loss: 6.99080283e-06
Iter: 151 loss: 6.97800533e-06
Iter: 152 loss: 6.96033112e-06
Iter: 153 loss: 6.9171474e-06
Iter: 154 loss: 7.02932721e-06
Iter: 155 loss: 6.90246316e-06
Iter: 156 loss: 6.85706027e-06
Iter: 157 loss: 7.03053547e-06
Iter: 158 loss: 6.84644601e-06
Iter: 159 loss: 6.81550137e-06
Iter: 160 loss: 7.08123162e-06
Iter: 161 loss: 6.81402526e-06
Iter: 162 loss: 6.77929756e-06
Iter: 163 loss: 6.81843085e-06
Iter: 164 loss: 6.76063064e-06
Iter: 165 loss: 6.72796432e-06
Iter: 166 loss: 6.89925218e-06
Iter: 167 loss: 6.72261376e-06
Iter: 168 loss: 6.69440124e-06
Iter: 169 loss: 6.69461633e-06
Iter: 170 loss: 6.67160566e-06
Iter: 171 loss: 6.63849414e-06
Iter: 172 loss: 6.83217604e-06
Iter: 173 loss: 6.63410719e-06
Iter: 174 loss: 6.60273508e-06
Iter: 175 loss: 6.63283254e-06
Iter: 176 loss: 6.58492e-06
Iter: 177 loss: 6.55550093e-06
Iter: 178 loss: 6.55558779e-06
Iter: 179 loss: 6.5320346e-06
Iter: 180 loss: 6.48946843e-06
Iter: 181 loss: 6.66469896e-06
Iter: 182 loss: 6.48012656e-06
Iter: 183 loss: 6.44721695e-06
Iter: 184 loss: 6.66435699e-06
Iter: 185 loss: 6.44396732e-06
Iter: 186 loss: 6.41043152e-06
Iter: 187 loss: 6.52028575e-06
Iter: 188 loss: 6.40134e-06
Iter: 189 loss: 6.37752191e-06
Iter: 190 loss: 6.3884554e-06
Iter: 191 loss: 6.36143068e-06
Iter: 192 loss: 6.32527281e-06
Iter: 193 loss: 6.50148831e-06
Iter: 194 loss: 6.31890771e-06
Iter: 195 loss: 6.29331817e-06
Iter: 196 loss: 6.33233594e-06
Iter: 197 loss: 6.28125e-06
Iter: 198 loss: 6.25437315e-06
Iter: 199 loss: 6.27223608e-06
Iter: 200 loss: 6.23741244e-06
Iter: 201 loss: 6.20969968e-06
Iter: 202 loss: 6.54364248e-06
Iter: 203 loss: 6.20935498e-06
Iter: 204 loss: 6.18381546e-06
Iter: 205 loss: 6.26146948e-06
Iter: 206 loss: 6.17624119e-06
Iter: 207 loss: 6.15769477e-06
Iter: 208 loss: 6.1401056e-06
Iter: 209 loss: 6.13594239e-06
Iter: 210 loss: 6.10739789e-06
Iter: 211 loss: 6.21761592e-06
Iter: 212 loss: 6.10076358e-06
Iter: 213 loss: 6.0716975e-06
Iter: 214 loss: 6.25084158e-06
Iter: 215 loss: 6.06820686e-06
Iter: 216 loss: 6.04797287e-06
Iter: 217 loss: 6.06109461e-06
Iter: 218 loss: 6.03496483e-06
Iter: 219 loss: 6.00659496e-06
Iter: 220 loss: 6.11776068e-06
Iter: 221 loss: 6.00007479e-06
Iter: 222 loss: 5.97697272e-06
Iter: 223 loss: 6.00326e-06
Iter: 224 loss: 5.96452628e-06
Iter: 225 loss: 5.9364e-06
Iter: 226 loss: 5.97336066e-06
Iter: 227 loss: 5.92218794e-06
Iter: 228 loss: 5.89103411e-06
Iter: 229 loss: 5.95379925e-06
Iter: 230 loss: 5.87841532e-06
Iter: 231 loss: 5.86064152e-06
Iter: 232 loss: 5.85929138e-06
Iter: 233 loss: 5.84355621e-06
Iter: 234 loss: 5.8209e-06
Iter: 235 loss: 5.82028315e-06
Iter: 236 loss: 5.79246262e-06
Iter: 237 loss: 5.86816032e-06
Iter: 238 loss: 5.78322761e-06
Iter: 239 loss: 5.76451521e-06
Iter: 240 loss: 5.76436423e-06
Iter: 241 loss: 5.74869046e-06
Iter: 242 loss: 5.75445574e-06
Iter: 243 loss: 5.7377456e-06
Iter: 244 loss: 5.71803957e-06
Iter: 245 loss: 5.83465817e-06
Iter: 246 loss: 5.71548662e-06
Iter: 247 loss: 5.70073735e-06
Iter: 248 loss: 5.67603092e-06
Iter: 249 loss: 5.67593543e-06
Iter: 250 loss: 5.64808715e-06
Iter: 251 loss: 5.7818479e-06
Iter: 252 loss: 5.64308584e-06
Iter: 253 loss: 5.6167537e-06
Iter: 254 loss: 5.6574595e-06
Iter: 255 loss: 5.60425906e-06
Iter: 256 loss: 5.58382453e-06
Iter: 257 loss: 5.91005028e-06
Iter: 258 loss: 5.58376541e-06
Iter: 259 loss: 5.56760733e-06
Iter: 260 loss: 5.57180101e-06
Iter: 261 loss: 5.55563201e-06
Iter: 262 loss: 5.53391965e-06
Iter: 263 loss: 5.61117031e-06
Iter: 264 loss: 5.52828533e-06
Iter: 265 loss: 5.50977074e-06
Iter: 266 loss: 5.56868827e-06
Iter: 267 loss: 5.50442655e-06
Iter: 268 loss: 5.48605294e-06
Iter: 269 loss: 5.50610639e-06
Iter: 270 loss: 5.47592481e-06
Iter: 271 loss: 5.45409966e-06
Iter: 272 loss: 5.44441718e-06
Iter: 273 loss: 5.43339183e-06
Iter: 274 loss: 5.42305452e-06
Iter: 275 loss: 5.41912868e-06
Iter: 276 loss: 5.4049151e-06
Iter: 277 loss: 5.38954464e-06
Iter: 278 loss: 5.38713539e-06
Iter: 279 loss: 5.36812786e-06
Iter: 280 loss: 5.51785342e-06
Iter: 281 loss: 5.36689686e-06
Iter: 282 loss: 5.35168783e-06
Iter: 283 loss: 5.45033e-06
Iter: 284 loss: 5.34987521e-06
Iter: 285 loss: 5.33736329e-06
Iter: 286 loss: 5.3191261e-06
Iter: 287 loss: 5.31853766e-06
Iter: 288 loss: 5.29786575e-06
Iter: 289 loss: 5.36169955e-06
Iter: 290 loss: 5.29165072e-06
Iter: 291 loss: 5.27384555e-06
Iter: 292 loss: 5.40902556e-06
Iter: 293 loss: 5.27241082e-06
Iter: 294 loss: 5.25509131e-06
Iter: 295 loss: 5.26163512e-06
Iter: 296 loss: 5.24302777e-06
Iter: 297 loss: 5.22291e-06
Iter: 298 loss: 5.2493433e-06
Iter: 299 loss: 5.21258471e-06
Iter: 300 loss: 5.19327114e-06
Iter: 301 loss: 5.2363157e-06
Iter: 302 loss: 5.18575143e-06
Iter: 303 loss: 5.16626278e-06
Iter: 304 loss: 5.32041668e-06
Iter: 305 loss: 5.16476439e-06
Iter: 306 loss: 5.14655403e-06
Iter: 307 loss: 5.16474029e-06
Iter: 308 loss: 5.13612031e-06
Iter: 309 loss: 5.11609414e-06
Iter: 310 loss: 5.18210072e-06
Iter: 311 loss: 5.1105817e-06
Iter: 312 loss: 5.09323809e-06
Iter: 313 loss: 5.12986799e-06
Iter: 314 loss: 5.08631774e-06
Iter: 315 loss: 5.06981905e-06
Iter: 316 loss: 5.25923906e-06
Iter: 317 loss: 5.06954211e-06
Iter: 318 loss: 5.05687285e-06
Iter: 319 loss: 5.05086427e-06
Iter: 320 loss: 5.04474292e-06
Iter: 321 loss: 5.03144065e-06
Iter: 322 loss: 5.22633218e-06
Iter: 323 loss: 5.03146111e-06
Iter: 324 loss: 5.02013063e-06
Iter: 325 loss: 5.02778767e-06
Iter: 326 loss: 5.01294198e-06
Iter: 327 loss: 5.00026317e-06
Iter: 328 loss: 4.99871612e-06
Iter: 329 loss: 4.98943882e-06
Iter: 330 loss: 4.97263727e-06
Iter: 331 loss: 5.11408734e-06
Iter: 332 loss: 4.9716823e-06
Iter: 333 loss: 4.95756467e-06
Iter: 334 loss: 4.96884331e-06
Iter: 335 loss: 4.9491282e-06
Iter: 336 loss: 4.93468178e-06
Iter: 337 loss: 4.91830269e-06
Iter: 338 loss: 4.91620085e-06
Iter: 339 loss: 4.90347247e-06
Iter: 340 loss: 4.90200182e-06
Iter: 341 loss: 4.88975411e-06
Iter: 342 loss: 4.88495334e-06
Iter: 343 loss: 4.87836223e-06
Iter: 344 loss: 4.8610118e-06
Iter: 345 loss: 4.91816718e-06
Iter: 346 loss: 4.85610417e-06
Iter: 347 loss: 4.84212e-06
Iter: 348 loss: 4.83525127e-06
Iter: 349 loss: 4.82836822e-06
Iter: 350 loss: 4.80999825e-06
Iter: 351 loss: 5.04799755e-06
Iter: 352 loss: 4.80986455e-06
Iter: 353 loss: 4.79354776e-06
Iter: 354 loss: 4.81489678e-06
Iter: 355 loss: 4.78544553e-06
Iter: 356 loss: 4.76975811e-06
Iter: 357 loss: 4.79894425e-06
Iter: 358 loss: 4.76291279e-06
Iter: 359 loss: 4.75097295e-06
Iter: 360 loss: 4.75088336e-06
Iter: 361 loss: 4.74088074e-06
Iter: 362 loss: 4.73898399e-06
Iter: 363 loss: 4.73229693e-06
Iter: 364 loss: 4.71864541e-06
Iter: 365 loss: 4.74260742e-06
Iter: 366 loss: 4.71254134e-06
Iter: 367 loss: 4.69855695e-06
Iter: 368 loss: 4.80544804e-06
Iter: 369 loss: 4.69747602e-06
Iter: 370 loss: 4.68822054e-06
Iter: 371 loss: 4.67504105e-06
Iter: 372 loss: 4.67473546e-06
Iter: 373 loss: 4.65627591e-06
Iter: 374 loss: 4.72620832e-06
Iter: 375 loss: 4.6518544e-06
Iter: 376 loss: 4.63939068e-06
Iter: 377 loss: 4.83273925e-06
Iter: 378 loss: 4.63941797e-06
Iter: 379 loss: 4.62939897e-06
Iter: 380 loss: 4.61442369e-06
Iter: 381 loss: 4.61388618e-06
Iter: 382 loss: 4.59658e-06
Iter: 383 loss: 4.67520476e-06
Iter: 384 loss: 4.59309558e-06
Iter: 385 loss: 4.57887381e-06
Iter: 386 loss: 4.6256032e-06
Iter: 387 loss: 4.57488795e-06
Iter: 388 loss: 4.56075441e-06
Iter: 389 loss: 4.61866966e-06
Iter: 390 loss: 4.55760483e-06
Iter: 391 loss: 4.5421948e-06
Iter: 392 loss: 4.56550924e-06
Iter: 393 loss: 4.53481061e-06
Iter: 394 loss: 4.52183485e-06
Iter: 395 loss: 4.52734139e-06
Iter: 396 loss: 4.51276856e-06
Iter: 397 loss: 4.49757545e-06
Iter: 398 loss: 4.6466007e-06
Iter: 399 loss: 4.49709842e-06
Iter: 400 loss: 4.48578339e-06
Iter: 401 loss: 4.56191e-06
Iter: 402 loss: 4.48465426e-06
Iter: 403 loss: 4.47489856e-06
Iter: 404 loss: 4.48428636e-06
Iter: 405 loss: 4.46935928e-06
Iter: 406 loss: 4.45764817e-06
Iter: 407 loss: 4.47307457e-06
Iter: 408 loss: 4.45164369e-06
Iter: 409 loss: 4.43907447e-06
Iter: 410 loss: 4.43354338e-06
Iter: 411 loss: 4.42709097e-06
Iter: 412 loss: 4.41717384e-06
Iter: 413 loss: 4.4165763e-06
Iter: 414 loss: 4.40663098e-06
Iter: 415 loss: 4.39767064e-06
Iter: 416 loss: 4.3950613e-06
Iter: 417 loss: 4.38014922e-06
Iter: 418 loss: 4.40587155e-06
Iter: 419 loss: 4.37335257e-06
Iter: 420 loss: 4.35867287e-06
Iter: 421 loss: 4.37194694e-06
Iter: 422 loss: 4.35028505e-06
Iter: 423 loss: 4.34046933e-06
Iter: 424 loss: 4.33906189e-06
Iter: 425 loss: 4.33128389e-06
Iter: 426 loss: 4.32043635e-06
Iter: 427 loss: 4.31990884e-06
Iter: 428 loss: 4.30495811e-06
Iter: 429 loss: 4.36792197e-06
Iter: 430 loss: 4.30179398e-06
Iter: 431 loss: 4.28944713e-06
Iter: 432 loss: 4.29861211e-06
Iter: 433 loss: 4.28196699e-06
Iter: 434 loss: 4.2679676e-06
Iter: 435 loss: 4.41404291e-06
Iter: 436 loss: 4.2675797e-06
Iter: 437 loss: 4.25677126e-06
Iter: 438 loss: 4.28291878e-06
Iter: 439 loss: 4.25296275e-06
Iter: 440 loss: 4.24395694e-06
Iter: 441 loss: 4.31744138e-06
Iter: 442 loss: 4.24326254e-06
Iter: 443 loss: 4.23463553e-06
Iter: 444 loss: 4.22424e-06
Iter: 445 loss: 4.22319317e-06
Iter: 446 loss: 4.21131517e-06
Iter: 447 loss: 4.25658709e-06
Iter: 448 loss: 4.20838614e-06
Iter: 449 loss: 4.19696698e-06
Iter: 450 loss: 4.23086e-06
Iter: 451 loss: 4.19335856e-06
Iter: 452 loss: 4.18068203e-06
Iter: 453 loss: 4.19690286e-06
Iter: 454 loss: 4.17404681e-06
Iter: 455 loss: 4.1620342e-06
Iter: 456 loss: 4.19173557e-06
Iter: 457 loss: 4.15786508e-06
Iter: 458 loss: 4.14439364e-06
Iter: 459 loss: 4.24162045e-06
Iter: 460 loss: 4.14312535e-06
Iter: 461 loss: 4.13336966e-06
Iter: 462 loss: 4.14753549e-06
Iter: 463 loss: 4.12847658e-06
Iter: 464 loss: 4.11913743e-06
Iter: 465 loss: 4.1096846e-06
Iter: 466 loss: 4.10768553e-06
Iter: 467 loss: 4.09113136e-06
Iter: 468 loss: 4.18042328e-06
Iter: 469 loss: 4.0886307e-06
Iter: 470 loss: 4.07654579e-06
Iter: 471 loss: 4.1449016e-06
Iter: 472 loss: 4.07498828e-06
Iter: 473 loss: 4.06336585e-06
Iter: 474 loss: 4.12263853e-06
Iter: 475 loss: 4.06154777e-06
Iter: 476 loss: 4.05260471e-06
Iter: 477 loss: 4.05262381e-06
Iter: 478 loss: 4.04526645e-06
Iter: 479 loss: 4.03783906e-06
Iter: 480 loss: 4.03774e-06
Iter: 481 loss: 4.02991463e-06
Iter: 482 loss: 4.02065507e-06
Iter: 483 loss: 4.01966736e-06
Iter: 484 loss: 4.00854333e-06
Iter: 485 loss: 4.05639912e-06
Iter: 486 loss: 4.00635145e-06
Iter: 487 loss: 3.99601277e-06
Iter: 488 loss: 4.02730075e-06
Iter: 489 loss: 3.99292549e-06
Iter: 490 loss: 3.98322118e-06
Iter: 491 loss: 3.98709199e-06
Iter: 492 loss: 3.97653093e-06
Iter: 493 loss: 3.96306314e-06
Iter: 494 loss: 4.00363479e-06
Iter: 495 loss: 3.95928373e-06
Iter: 496 loss: 3.94772178e-06
Iter: 497 loss: 4.02899832e-06
Iter: 498 loss: 3.94675862e-06
Iter: 499 loss: 3.93818027e-06
Iter: 500 loss: 3.94966446e-06
Iter: 501 loss: 3.93383289e-06
Iter: 502 loss: 3.92371294e-06
Iter: 503 loss: 3.96137102e-06
Iter: 504 loss: 3.92120864e-06
Iter: 505 loss: 3.91094727e-06
Iter: 506 loss: 3.93799837e-06
Iter: 507 loss: 3.90754849e-06
Iter: 508 loss: 3.8991875e-06
Iter: 509 loss: 3.89404113e-06
Iter: 510 loss: 3.8907865e-06
Iter: 511 loss: 3.87795217e-06
Iter: 512 loss: 3.92248785e-06
Iter: 513 loss: 3.87457385e-06
Iter: 514 loss: 3.86207466e-06
Iter: 515 loss: 3.91000049e-06
Iter: 516 loss: 3.85906378e-06
Iter: 517 loss: 3.85245e-06
Iter: 518 loss: 3.85197791e-06
Iter: 519 loss: 3.846656e-06
Iter: 520 loss: 3.8359367e-06
Iter: 521 loss: 4.03077047e-06
Iter: 522 loss: 3.83566658e-06
Iter: 523 loss: 3.82456892e-06
Iter: 524 loss: 3.99148666e-06
Iter: 525 loss: 3.8245289e-06
Iter: 526 loss: 3.81876271e-06
Iter: 527 loss: 3.80702136e-06
Iter: 528 loss: 4.02185378e-06
Iter: 529 loss: 3.80693746e-06
Iter: 530 loss: 3.79381891e-06
Iter: 531 loss: 3.84499663e-06
Iter: 532 loss: 3.79073617e-06
Iter: 533 loss: 3.78094728e-06
Iter: 534 loss: 3.88170702e-06
Iter: 535 loss: 3.78078175e-06
Iter: 536 loss: 3.77331503e-06
Iter: 537 loss: 3.78662662e-06
Iter: 538 loss: 3.77010701e-06
Iter: 539 loss: 3.76069465e-06
Iter: 540 loss: 3.78513141e-06
Iter: 541 loss: 3.75743093e-06
Iter: 542 loss: 3.74974957e-06
Iter: 543 loss: 3.797913e-06
Iter: 544 loss: 3.74883643e-06
Iter: 545 loss: 3.74106276e-06
Iter: 546 loss: 3.73300145e-06
Iter: 547 loss: 3.73141484e-06
Iter: 548 loss: 3.71893975e-06
Iter: 549 loss: 3.77179958e-06
Iter: 550 loss: 3.71638271e-06
Iter: 551 loss: 3.70725024e-06
Iter: 552 loss: 3.73688158e-06
Iter: 553 loss: 3.70472685e-06
Iter: 554 loss: 3.69483337e-06
Iter: 555 loss: 3.74947194e-06
Iter: 556 loss: 3.69343888e-06
Iter: 557 loss: 3.68626388e-06
Iter: 558 loss: 3.6981246e-06
Iter: 559 loss: 3.68291921e-06
Iter: 560 loss: 3.67422581e-06
Iter: 561 loss: 3.73743023e-06
Iter: 562 loss: 3.67348139e-06
Iter: 563 loss: 3.66776476e-06
Iter: 564 loss: 3.66166273e-06
Iter: 565 loss: 3.66073596e-06
Iter: 566 loss: 3.65065716e-06
Iter: 567 loss: 3.668418e-06
Iter: 568 loss: 3.64618177e-06
Iter: 569 loss: 3.63700224e-06
Iter: 570 loss: 3.74709271e-06
Iter: 571 loss: 3.6369579e-06
Iter: 572 loss: 3.62951891e-06
Iter: 573 loss: 3.62451692e-06
Iter: 574 loss: 3.6218712e-06
Iter: 575 loss: 3.61171897e-06
Iter: 576 loss: 3.63521713e-06
Iter: 577 loss: 3.60796412e-06
Iter: 578 loss: 3.59657815e-06
Iter: 579 loss: 3.61212756e-06
Iter: 580 loss: 3.59114574e-06
Iter: 581 loss: 3.57991121e-06
Iter: 582 loss: 3.61754019e-06
Iter: 583 loss: 3.57684394e-06
Iter: 584 loss: 3.56503892e-06
Iter: 585 loss: 3.66761924e-06
Iter: 586 loss: 3.56445526e-06
Iter: 587 loss: 3.55733459e-06
Iter: 588 loss: 3.55916518e-06
Iter: 589 loss: 3.55212478e-06
Iter: 590 loss: 3.54224812e-06
Iter: 591 loss: 3.62272317e-06
Iter: 592 loss: 3.54174358e-06
Iter: 593 loss: 3.53543214e-06
Iter: 594 loss: 3.52970505e-06
Iter: 595 loss: 3.5280832e-06
Iter: 596 loss: 3.51862195e-06
Iter: 597 loss: 3.57559429e-06
Iter: 598 loss: 3.51746985e-06
Iter: 599 loss: 3.50836285e-06
Iter: 600 loss: 3.57049521e-06
Iter: 601 loss: 3.50739265e-06
Iter: 602 loss: 3.50136202e-06
Iter: 603 loss: 3.50430923e-06
Iter: 604 loss: 3.49721904e-06
Iter: 605 loss: 3.48994945e-06
Iter: 606 loss: 3.51582275e-06
Iter: 607 loss: 3.4880984e-06
Iter: 608 loss: 3.48082e-06
Iter: 609 loss: 3.478792e-06
Iter: 610 loss: 3.47430205e-06
Iter: 611 loss: 3.4646223e-06
Iter: 612 loss: 3.47457626e-06
Iter: 613 loss: 3.45928765e-06
Iter: 614 loss: 3.44813589e-06
Iter: 615 loss: 3.51285848e-06
Iter: 616 loss: 3.4467098e-06
Iter: 617 loss: 3.43707507e-06
Iter: 618 loss: 3.44692285e-06
Iter: 619 loss: 3.43164743e-06
Iter: 620 loss: 3.42235376e-06
Iter: 621 loss: 3.50421578e-06
Iter: 622 loss: 3.42199746e-06
Iter: 623 loss: 3.41407122e-06
Iter: 624 loss: 3.45231047e-06
Iter: 625 loss: 3.41271812e-06
Iter: 626 loss: 3.40589418e-06
Iter: 627 loss: 3.40200108e-06
Iter: 628 loss: 3.39906592e-06
Iter: 629 loss: 3.39013968e-06
Iter: 630 loss: 3.47309333e-06
Iter: 631 loss: 3.38969903e-06
Iter: 632 loss: 3.38246673e-06
Iter: 633 loss: 3.38614882e-06
Iter: 634 loss: 3.37782149e-06
Iter: 635 loss: 3.37037636e-06
Iter: 636 loss: 3.43041211e-06
Iter: 637 loss: 3.36994503e-06
Iter: 638 loss: 3.36266885e-06
Iter: 639 loss: 3.37480833e-06
Iter: 640 loss: 3.3593833e-06
Iter: 641 loss: 3.35235336e-06
Iter: 642 loss: 3.37225401e-06
Iter: 643 loss: 3.35021332e-06
Iter: 644 loss: 3.34172705e-06
Iter: 645 loss: 3.35511822e-06
Iter: 646 loss: 3.33781281e-06
Iter: 647 loss: 3.33097501e-06
Iter: 648 loss: 3.33275329e-06
Iter: 649 loss: 3.32602144e-06
Iter: 650 loss: 3.31656383e-06
Iter: 651 loss: 3.34250581e-06
Iter: 652 loss: 3.31343426e-06
Iter: 653 loss: 3.30450348e-06
Iter: 654 loss: 3.38774908e-06
Iter: 655 loss: 3.30409944e-06
Iter: 656 loss: 3.29892146e-06
Iter: 657 loss: 3.29215959e-06
Iter: 658 loss: 3.29169e-06
Iter: 659 loss: 3.28216902e-06
Iter: 660 loss: 3.32436139e-06
Iter: 661 loss: 3.28017495e-06
Iter: 662 loss: 3.27136195e-06
Iter: 663 loss: 3.28412125e-06
Iter: 664 loss: 3.26713916e-06
Iter: 665 loss: 3.25646101e-06
Iter: 666 loss: 3.27400562e-06
Iter: 667 loss: 3.25165161e-06
Iter: 668 loss: 3.24390498e-06
Iter: 669 loss: 3.24379948e-06
Iter: 670 loss: 3.23748827e-06
Iter: 671 loss: 3.24533153e-06
Iter: 672 loss: 3.23432e-06
Iter: 673 loss: 3.22772871e-06
Iter: 674 loss: 3.2434325e-06
Iter: 675 loss: 3.22529695e-06
Iter: 676 loss: 3.21861671e-06
Iter: 677 loss: 3.26965073e-06
Iter: 678 loss: 3.21811285e-06
Iter: 679 loss: 3.2132391e-06
Iter: 680 loss: 3.21904918e-06
Iter: 681 loss: 3.21069092e-06
Iter: 682 loss: 3.203696e-06
Iter: 683 loss: 3.20405161e-06
Iter: 684 loss: 3.19826495e-06
Iter: 685 loss: 3.19067135e-06
Iter: 686 loss: 3.2228495e-06
Iter: 687 loss: 3.18908451e-06
Iter: 688 loss: 3.18244929e-06
Iter: 689 loss: 3.22024903e-06
Iter: 690 loss: 3.18155685e-06
Iter: 691 loss: 3.17583294e-06
Iter: 692 loss: 3.17566492e-06
Iter: 693 loss: 3.17129252e-06
Iter: 694 loss: 3.16262049e-06
Iter: 695 loss: 3.16390538e-06
Iter: 696 loss: 3.15601847e-06
Iter: 697 loss: 3.14743306e-06
Iter: 698 loss: 3.22310689e-06
Iter: 699 loss: 3.1469383e-06
Iter: 700 loss: 3.13973919e-06
Iter: 701 loss: 3.16279647e-06
Iter: 702 loss: 3.13790247e-06
Iter: 703 loss: 3.12983457e-06
Iter: 704 loss: 3.14274166e-06
Iter: 705 loss: 3.12601514e-06
Iter: 706 loss: 3.11856184e-06
Iter: 707 loss: 3.12354769e-06
Iter: 708 loss: 3.11391523e-06
Iter: 709 loss: 3.10548648e-06
Iter: 710 loss: 3.11801023e-06
Iter: 711 loss: 3.10148289e-06
Iter: 712 loss: 3.09360712e-06
Iter: 713 loss: 3.17362151e-06
Iter: 714 loss: 3.0934184e-06
Iter: 715 loss: 3.0860117e-06
Iter: 716 loss: 3.13051078e-06
Iter: 717 loss: 3.08501103e-06
Iter: 718 loss: 3.07983692e-06
Iter: 719 loss: 3.081459e-06
Iter: 720 loss: 3.07607229e-06
Iter: 721 loss: 3.06967104e-06
Iter: 722 loss: 3.10131622e-06
Iter: 723 loss: 3.06866718e-06
Iter: 724 loss: 3.06252468e-06
Iter: 725 loss: 3.06027596e-06
Iter: 726 loss: 3.05694221e-06
Iter: 727 loss: 3.04866944e-06
Iter: 728 loss: 3.12227917e-06
Iter: 729 loss: 3.04830633e-06
Iter: 730 loss: 3.04357536e-06
Iter: 731 loss: 3.04400078e-06
Iter: 732 loss: 3.0398387e-06
Iter: 733 loss: 3.03274192e-06
Iter: 734 loss: 3.04271725e-06
Iter: 735 loss: 3.02933972e-06
Iter: 736 loss: 3.02219678e-06
Iter: 737 loss: 3.10700943e-06
Iter: 738 loss: 3.02206081e-06
Iter: 739 loss: 3.01701357e-06
Iter: 740 loss: 3.01650243e-06
Iter: 741 loss: 3.01297314e-06
Iter: 742 loss: 3.00578586e-06
Iter: 743 loss: 3.00709644e-06
Iter: 744 loss: 3.0003971e-06
Iter: 745 loss: 2.99231715e-06
Iter: 746 loss: 3.03950878e-06
Iter: 747 loss: 2.99125804e-06
Iter: 748 loss: 2.98352961e-06
Iter: 749 loss: 3.01785235e-06
Iter: 750 loss: 2.98209056e-06
Iter: 751 loss: 2.97545648e-06
Iter: 752 loss: 2.99842213e-06
Iter: 753 loss: 2.9737198e-06
Iter: 754 loss: 2.96764119e-06
Iter: 755 loss: 2.96070766e-06
Iter: 756 loss: 2.9599114e-06
Iter: 757 loss: 2.95681366e-06
Iter: 758 loss: 2.95501309e-06
Iter: 759 loss: 2.95096788e-06
Iter: 760 loss: 2.95339669e-06
Iter: 761 loss: 2.94829442e-06
Iter: 762 loss: 2.9432274e-06
Iter: 763 loss: 2.93524e-06
Iter: 764 loss: 2.93520156e-06
Iter: 765 loss: 2.9273906e-06
Iter: 766 loss: 3.00595229e-06
Iter: 767 loss: 2.92720733e-06
Iter: 768 loss: 2.92051141e-06
Iter: 769 loss: 2.93574067e-06
Iter: 770 loss: 2.91796277e-06
Iter: 771 loss: 2.91125616e-06
Iter: 772 loss: 2.94546157e-06
Iter: 773 loss: 2.91017682e-06
Iter: 774 loss: 2.90486105e-06
Iter: 775 loss: 2.91560127e-06
Iter: 776 loss: 2.90283583e-06
Iter: 777 loss: 2.89622039e-06
Iter: 778 loss: 2.90903381e-06
Iter: 779 loss: 2.89353875e-06
Iter: 780 loss: 2.88831529e-06
Iter: 781 loss: 2.88581487e-06
Iter: 782 loss: 2.8832535e-06
Iter: 783 loss: 2.87646367e-06
Iter: 784 loss: 2.91160427e-06
Iter: 785 loss: 2.87542844e-06
Iter: 786 loss: 2.86901491e-06
Iter: 787 loss: 2.91261949e-06
Iter: 788 loss: 2.86841578e-06
Iter: 789 loss: 2.86274e-06
Iter: 790 loss: 2.87696798e-06
Iter: 791 loss: 2.86081513e-06
Iter: 792 loss: 2.85587339e-06
Iter: 793 loss: 2.85227384e-06
Iter: 794 loss: 2.85062538e-06
Iter: 795 loss: 2.84395173e-06
Iter: 796 loss: 2.84397834e-06
Iter: 797 loss: 2.83959071e-06
Iter: 798 loss: 2.84079078e-06
Iter: 799 loss: 2.83635882e-06
Iter: 800 loss: 2.8303923e-06
Iter: 801 loss: 2.87966122e-06
Iter: 802 loss: 2.83011696e-06
Iter: 803 loss: 2.82624433e-06
Iter: 804 loss: 2.81906091e-06
Iter: 805 loss: 2.98374152e-06
Iter: 806 loss: 2.81907478e-06
Iter: 807 loss: 2.8116915e-06
Iter: 808 loss: 2.84963426e-06
Iter: 809 loss: 2.81053735e-06
Iter: 810 loss: 2.80391032e-06
Iter: 811 loss: 2.81821895e-06
Iter: 812 loss: 2.80131758e-06
Iter: 813 loss: 2.79527512e-06
Iter: 814 loss: 2.86545264e-06
Iter: 815 loss: 2.79522328e-06
Iter: 816 loss: 2.7906483e-06
Iter: 817 loss: 2.79322421e-06
Iter: 818 loss: 2.78773405e-06
Iter: 819 loss: 2.78208154e-06
Iter: 820 loss: 2.78846278e-06
Iter: 821 loss: 2.77904655e-06
Iter: 822 loss: 2.77275831e-06
Iter: 823 loss: 2.81172925e-06
Iter: 824 loss: 2.77201752e-06
Iter: 825 loss: 2.76673723e-06
Iter: 826 loss: 2.76362471e-06
Iter: 827 loss: 2.76144215e-06
Iter: 828 loss: 2.75455909e-06
Iter: 829 loss: 2.76889023e-06
Iter: 830 loss: 2.75176626e-06
Iter: 831 loss: 2.74582681e-06
Iter: 832 loss: 2.82311748e-06
Iter: 833 loss: 2.74585636e-06
Iter: 834 loss: 2.74068952e-06
Iter: 835 loss: 2.75725506e-06
Iter: 836 loss: 2.73921569e-06
Iter: 837 loss: 2.73441083e-06
Iter: 838 loss: 2.74277613e-06
Iter: 839 loss: 2.73228534e-06
Iter: 840 loss: 2.72686862e-06
Iter: 841 loss: 2.7593394e-06
Iter: 842 loss: 2.72610259e-06
Iter: 843 loss: 2.72170223e-06
Iter: 844 loss: 2.71549311e-06
Iter: 845 loss: 2.71532e-06
Iter: 846 loss: 2.70983719e-06
Iter: 847 loss: 2.74532704e-06
Iter: 848 loss: 2.7092342e-06
Iter: 849 loss: 2.70313558e-06
Iter: 850 loss: 2.71819431e-06
Iter: 851 loss: 2.7009869e-06
Iter: 852 loss: 2.69622797e-06
Iter: 853 loss: 2.70255941e-06
Iter: 854 loss: 2.69387283e-06
Iter: 855 loss: 2.68862868e-06
Iter: 856 loss: 2.71045246e-06
Iter: 857 loss: 2.68752478e-06
Iter: 858 loss: 2.68246367e-06
Iter: 859 loss: 2.70995e-06
Iter: 860 loss: 2.68174381e-06
Iter: 861 loss: 2.67724272e-06
Iter: 862 loss: 2.67561495e-06
Iter: 863 loss: 2.67312612e-06
Iter: 864 loss: 2.66733059e-06
Iter: 865 loss: 2.68433155e-06
Iter: 866 loss: 2.66555389e-06
Iter: 867 loss: 2.65971221e-06
Iter: 868 loss: 2.68777e-06
Iter: 869 loss: 2.65858171e-06
Iter: 870 loss: 2.65293238e-06
Iter: 871 loss: 2.65474364e-06
Iter: 872 loss: 2.64888786e-06
Iter: 873 loss: 2.64327878e-06
Iter: 874 loss: 2.67416453e-06
Iter: 875 loss: 2.6425846e-06
Iter: 876 loss: 2.63833545e-06
Iter: 877 loss: 2.69200677e-06
Iter: 878 loss: 2.63832067e-06
Iter: 879 loss: 2.63473794e-06
Iter: 880 loss: 2.63114725e-06
Iter: 881 loss: 2.63041966e-06
Iter: 882 loss: 2.62581125e-06
Iter: 883 loss: 2.64373602e-06
Iter: 884 loss: 2.62470326e-06
Iter: 885 loss: 2.62009462e-06
Iter: 886 loss: 2.64125492e-06
Iter: 887 loss: 2.61912419e-06
Iter: 888 loss: 2.61477362e-06
Iter: 889 loss: 2.61287437e-06
Iter: 890 loss: 2.61063e-06
Iter: 891 loss: 2.60444176e-06
Iter: 892 loss: 2.61584455e-06
Iter: 893 loss: 2.60194474e-06
Iter: 894 loss: 2.59606713e-06
Iter: 895 loss: 2.63384891e-06
Iter: 896 loss: 2.59546914e-06
Iter: 897 loss: 2.59133913e-06
Iter: 898 loss: 2.62710068e-06
Iter: 899 loss: 2.59109856e-06
Iter: 900 loss: 2.58769023e-06
Iter: 901 loss: 2.58447267e-06
Iter: 902 loss: 2.58369846e-06
Iter: 903 loss: 2.57913098e-06
Iter: 904 loss: 2.62614276e-06
Iter: 905 loss: 2.57909528e-06
Iter: 906 loss: 2.57450279e-06
Iter: 907 loss: 2.57061197e-06
Iter: 908 loss: 2.56944213e-06
Iter: 909 loss: 2.56345743e-06
Iter: 910 loss: 2.57639613e-06
Iter: 911 loss: 2.56121052e-06
Iter: 912 loss: 2.55577788e-06
Iter: 913 loss: 2.60285788e-06
Iter: 914 loss: 2.55542045e-06
Iter: 915 loss: 2.55020723e-06
Iter: 916 loss: 2.56171347e-06
Iter: 917 loss: 2.54823317e-06
Iter: 918 loss: 2.54461247e-06
Iter: 919 loss: 2.6018065e-06
Iter: 920 loss: 2.54459405e-06
Iter: 921 loss: 2.5423858e-06
Iter: 922 loss: 2.53778717e-06
Iter: 923 loss: 2.61781065e-06
Iter: 924 loss: 2.53763756e-06
Iter: 925 loss: 2.53122926e-06
Iter: 926 loss: 2.53791109e-06
Iter: 927 loss: 2.52755285e-06
Iter: 928 loss: 2.52106474e-06
Iter: 929 loss: 2.56085741e-06
Iter: 930 loss: 2.52035784e-06
Iter: 931 loss: 2.51476604e-06
Iter: 932 loss: 2.55104624e-06
Iter: 933 loss: 2.51415622e-06
Iter: 934 loss: 2.50945936e-06
Iter: 935 loss: 2.51433357e-06
Iter: 936 loss: 2.50681433e-06
Iter: 937 loss: 2.50230596e-06
Iter: 938 loss: 2.50679886e-06
Iter: 939 loss: 2.49975051e-06
Iter: 940 loss: 2.49452341e-06
Iter: 941 loss: 2.51306028e-06
Iter: 942 loss: 2.49324739e-06
Iter: 943 loss: 2.48882839e-06
Iter: 944 loss: 2.53321241e-06
Iter: 945 loss: 2.48865945e-06
Iter: 946 loss: 2.48495553e-06
Iter: 947 loss: 2.48375613e-06
Iter: 948 loss: 2.48153538e-06
Iter: 949 loss: 2.47644743e-06
Iter: 950 loss: 2.48556262e-06
Iter: 951 loss: 2.47417893e-06
Iter: 952 loss: 2.4691285e-06
Iter: 953 loss: 2.5194829e-06
Iter: 954 loss: 2.46890954e-06
Iter: 955 loss: 2.46533205e-06
Iter: 956 loss: 2.46365653e-06
Iter: 957 loss: 2.4618314e-06
Iter: 958 loss: 2.45837646e-06
Iter: 959 loss: 2.45817046e-06
Iter: 960 loss: 2.4554729e-06
Iter: 961 loss: 2.45060892e-06
Iter: 962 loss: 2.56855355e-06
Iter: 963 loss: 2.45059186e-06
Iter: 964 loss: 2.4464332e-06
Iter: 965 loss: 2.48813512e-06
Iter: 966 loss: 2.44622356e-06
Iter: 967 loss: 2.442599e-06
Iter: 968 loss: 2.44267585e-06
Iter: 969 loss: 2.43970067e-06
Iter: 970 loss: 2.43456589e-06
Iter: 971 loss: 2.44054149e-06
Iter: 972 loss: 2.43182535e-06
Iter: 973 loss: 2.4262697e-06
Iter: 974 loss: 2.4460951e-06
Iter: 975 loss: 2.42476699e-06
Iter: 976 loss: 2.42028636e-06
Iter: 977 loss: 2.46912623e-06
Iter: 978 loss: 2.4201895e-06
Iter: 979 loss: 2.41599446e-06
Iter: 980 loss: 2.41503653e-06
Iter: 981 loss: 2.41255248e-06
Iter: 982 loss: 2.4077126e-06
Iter: 983 loss: 2.41530711e-06
Iter: 984 loss: 2.40551708e-06
Iter: 985 loss: 2.40068061e-06
Iter: 986 loss: 2.43275736e-06
Iter: 987 loss: 2.40017698e-06
Iter: 988 loss: 2.39609085e-06
Iter: 989 loss: 2.42065062e-06
Iter: 990 loss: 2.39554765e-06
Iter: 991 loss: 2.39258748e-06
Iter: 992 loss: 2.39165297e-06
Iter: 993 loss: 2.38987309e-06
Iter: 994 loss: 2.38533835e-06
Iter: 995 loss: 2.39315546e-06
Iter: 996 loss: 2.38344091e-06
Iter: 997 loss: 2.37970949e-06
Iter: 998 loss: 2.3797329e-06
Iter: 999 loss: 2.37656286e-06
Iter: 1000 loss: 2.37282734e-06
Iter: 1001 loss: 2.3723826e-06
Iter: 1002 loss: 2.36829646e-06
Iter: 1003 loss: 2.38554367e-06
Iter: 1004 loss: 2.36746291e-06
Iter: 1005 loss: 2.36270375e-06
Iter: 1006 loss: 2.37242875e-06
Iter: 1007 loss: 2.36080746e-06
Iter: 1008 loss: 2.35678613e-06
Iter: 1009 loss: 2.3601383e-06
Iter: 1010 loss: 2.35443849e-06
Iter: 1011 loss: 2.34982326e-06
Iter: 1012 loss: 2.36351411e-06
Iter: 1013 loss: 2.34832487e-06
Iter: 1014 loss: 2.34341019e-06
Iter: 1015 loss: 2.35773859e-06
Iter: 1016 loss: 2.34185563e-06
Iter: 1017 loss: 2.33699893e-06
Iter: 1018 loss: 2.37813e-06
Iter: 1019 loss: 2.33679611e-06
Iter: 1020 loss: 2.33382843e-06
Iter: 1021 loss: 2.33350193e-06
Iter: 1022 loss: 2.33139872e-06
Iter: 1023 loss: 2.32650018e-06
Iter: 1024 loss: 2.34481058e-06
Iter: 1025 loss: 2.32528123e-06
Iter: 1026 loss: 2.32137972e-06
Iter: 1027 loss: 2.32987304e-06
Iter: 1028 loss: 2.31985609e-06
Iter: 1029 loss: 2.31564445e-06
Iter: 1030 loss: 2.32339744e-06
Iter: 1031 loss: 2.31392141e-06
Iter: 1032 loss: 2.30920296e-06
Iter: 1033 loss: 2.34591084e-06
Iter: 1034 loss: 2.30881619e-06
Iter: 1035 loss: 2.30590808e-06
Iter: 1036 loss: 2.30323826e-06
Iter: 1037 loss: 2.3025184e-06
Iter: 1038 loss: 2.29793045e-06
Iter: 1039 loss: 2.36688902e-06
Iter: 1040 loss: 2.29794682e-06
Iter: 1041 loss: 2.29518173e-06
Iter: 1042 loss: 2.29136549e-06
Iter: 1043 loss: 2.2911654e-06
Iter: 1044 loss: 2.28592035e-06
Iter: 1045 loss: 2.28745103e-06
Iter: 1046 loss: 2.28207273e-06
Iter: 1047 loss: 2.27608302e-06
Iter: 1048 loss: 2.29712327e-06
Iter: 1049 loss: 2.27451028e-06
Iter: 1050 loss: 2.26961447e-06
Iter: 1051 loss: 2.3327525e-06
Iter: 1052 loss: 2.26962266e-06
Iter: 1053 loss: 2.26616794e-06
Iter: 1054 loss: 2.27333408e-06
Iter: 1055 loss: 2.26479688e-06
Iter: 1056 loss: 2.26079533e-06
Iter: 1057 loss: 2.26609177e-06
Iter: 1058 loss: 2.25879648e-06
Iter: 1059 loss: 2.25481062e-06
Iter: 1060 loss: 2.26547377e-06
Iter: 1061 loss: 2.25341091e-06
Iter: 1062 loss: 2.24887253e-06
Iter: 1063 loss: 2.27647524e-06
Iter: 1064 loss: 2.2483805e-06
Iter: 1065 loss: 2.24540258e-06
Iter: 1066 loss: 2.24304904e-06
Iter: 1067 loss: 2.24216546e-06
Iter: 1068 loss: 2.23753614e-06
Iter: 1069 loss: 2.26950078e-06
Iter: 1070 loss: 2.23711208e-06
Iter: 1071 loss: 2.23291522e-06
Iter: 1072 loss: 2.24984342e-06
Iter: 1073 loss: 2.23197435e-06
Iter: 1074 loss: 2.2285908e-06
Iter: 1075 loss: 2.23668076e-06
Iter: 1076 loss: 2.22730432e-06
Iter: 1077 loss: 2.22342032e-06
Iter: 1078 loss: 2.23794177e-06
Iter: 1079 loss: 2.22251106e-06
Iter: 1080 loss: 2.21909295e-06
Iter: 1081 loss: 2.2219424e-06
Iter: 1082 loss: 2.21705523e-06
Iter: 1083 loss: 2.21318805e-06
Iter: 1084 loss: 2.22738754e-06
Iter: 1085 loss: 2.21213622e-06
Iter: 1086 loss: 2.20868424e-06
Iter: 1087 loss: 2.20549873e-06
Iter: 1088 loss: 2.20465313e-06
Iter: 1089 loss: 2.19910589e-06
Iter: 1090 loss: 2.2172535e-06
Iter: 1091 loss: 2.19749177e-06
Iter: 1092 loss: 2.1926553e-06
Iter: 1093 loss: 2.19845833e-06
Iter: 1094 loss: 2.19008143e-06
Iter: 1095 loss: 2.18623359e-06
Iter: 1096 loss: 2.18611694e-06
Iter: 1097 loss: 2.18283367e-06
Iter: 1098 loss: 2.18226205e-06
Iter: 1099 loss: 2.17998354e-06
Iter: 1100 loss: 2.17574961e-06
Iter: 1101 loss: 2.18575065e-06
Iter: 1102 loss: 2.17424758e-06
Iter: 1103 loss: 2.16977583e-06
Iter: 1104 loss: 2.20064e-06
Iter: 1105 loss: 2.16938065e-06
Iter: 1106 loss: 2.16577155e-06
Iter: 1107 loss: 2.1698429e-06
Iter: 1108 loss: 2.16382455e-06
Iter: 1109 loss: 2.15985619e-06
Iter: 1110 loss: 2.16468152e-06
Iter: 1111 loss: 2.15787577e-06
Iter: 1112 loss: 2.15414639e-06
Iter: 1113 loss: 2.18605692e-06
Iter: 1114 loss: 2.15396722e-06
Iter: 1115 loss: 2.15029945e-06
Iter: 1116 loss: 2.15395e-06
Iter: 1117 loss: 2.14834154e-06
Iter: 1118 loss: 2.1447911e-06
Iter: 1119 loss: 2.16840158e-06
Iter: 1120 loss: 2.14435522e-06
Iter: 1121 loss: 2.14135935e-06
Iter: 1122 loss: 2.1486037e-06
Iter: 1123 loss: 2.14016973e-06
Iter: 1124 loss: 2.13710541e-06
Iter: 1125 loss: 2.14003626e-06
Iter: 1126 loss: 2.13531825e-06
Iter: 1127 loss: 2.13162639e-06
Iter: 1128 loss: 2.14040847e-06
Iter: 1129 loss: 2.13025123e-06
Iter: 1130 loss: 2.12654777e-06
Iter: 1131 loss: 2.12567761e-06
Iter: 1132 loss: 2.1232504e-06
Iter: 1133 loss: 2.11828956e-06
Iter: 1134 loss: 2.1298406e-06
Iter: 1135 loss: 2.11658062e-06
Iter: 1136 loss: 2.11162751e-06
Iter: 1137 loss: 2.12993336e-06
Iter: 1138 loss: 2.11038491e-06
Iter: 1139 loss: 2.10593862e-06
Iter: 1140 loss: 2.11561746e-06
Iter: 1141 loss: 2.10422832e-06
Iter: 1142 loss: 2.09968607e-06
Iter: 1143 loss: 2.13921567e-06
Iter: 1144 loss: 2.09940367e-06
Iter: 1145 loss: 2.09561e-06
Iter: 1146 loss: 2.10936264e-06
Iter: 1147 loss: 2.09461859e-06
Iter: 1148 loss: 2.09157292e-06
Iter: 1149 loss: 2.09300924e-06
Iter: 1150 loss: 2.08936262e-06
Iter: 1151 loss: 2.08531355e-06
Iter: 1152 loss: 2.10239659e-06
Iter: 1153 loss: 2.08444021e-06
Iter: 1154 loss: 2.08035544e-06
Iter: 1155 loss: 2.1005485e-06
Iter: 1156 loss: 2.07970629e-06
Iter: 1157 loss: 2.07735275e-06
Iter: 1158 loss: 2.0905627e-06
Iter: 1159 loss: 2.07694984e-06
Iter: 1160 loss: 2.0742259e-06
Iter: 1161 loss: 2.07030553e-06
Iter: 1162 loss: 2.07011612e-06
Iter: 1163 loss: 2.06641607e-06
Iter: 1164 loss: 2.10572898e-06
Iter: 1165 loss: 2.06629079e-06
Iter: 1166 loss: 2.06318e-06
Iter: 1167 loss: 2.0665143e-06
Iter: 1168 loss: 2.06151526e-06
Iter: 1169 loss: 2.05814626e-06
Iter: 1170 loss: 2.05738252e-06
Iter: 1171 loss: 2.0551613e-06
Iter: 1172 loss: 2.05016636e-06
Iter: 1173 loss: 2.09367045e-06
Iter: 1174 loss: 2.04994103e-06
Iter: 1175 loss: 2.04687922e-06
Iter: 1176 loss: 2.04829098e-06
Iter: 1177 loss: 2.04482376e-06
Iter: 1178 loss: 2.0405023e-06
Iter: 1179 loss: 2.04550815e-06
Iter: 1180 loss: 2.03820127e-06
Iter: 1181 loss: 2.03450736e-06
Iter: 1182 loss: 2.05229981e-06
Iter: 1183 loss: 2.03390232e-06
Iter: 1184 loss: 2.03031163e-06
Iter: 1185 loss: 2.04575e-06
Iter: 1186 loss: 2.02960496e-06
Iter: 1187 loss: 2.02611477e-06
Iter: 1188 loss: 2.04375215e-06
Iter: 1189 loss: 2.0255593e-06
Iter: 1190 loss: 2.02283422e-06
Iter: 1191 loss: 2.02250567e-06
Iter: 1192 loss: 2.02066394e-06
Iter: 1193 loss: 2.0176476e-06
Iter: 1194 loss: 2.05800256e-06
Iter: 1195 loss: 2.01759099e-06
Iter: 1196 loss: 2.01503076e-06
Iter: 1197 loss: 2.0161317e-06
Iter: 1198 loss: 2.01330022e-06
Iter: 1199 loss: 2.01042394e-06
Iter: 1200 loss: 2.01757e-06
Iter: 1201 loss: 2.00953718e-06
Iter: 1202 loss: 2.00597083e-06
Iter: 1203 loss: 2.01123862e-06
Iter: 1204 loss: 2.0042587e-06
Iter: 1205 loss: 2.00103773e-06
Iter: 1206 loss: 2.00603836e-06
Iter: 1207 loss: 1.99948045e-06
Iter: 1208 loss: 1.99581564e-06
Iter: 1209 loss: 2.01774674e-06
Iter: 1210 loss: 1.99532792e-06
Iter: 1211 loss: 1.9921506e-06
Iter: 1212 loss: 1.99154283e-06
Iter: 1213 loss: 1.98930047e-06
Iter: 1214 loss: 1.98569069e-06
Iter: 1215 loss: 1.98965358e-06
Iter: 1216 loss: 1.98374391e-06
Iter: 1217 loss: 1.97967302e-06
Iter: 1218 loss: 2.0074167e-06
Iter: 1219 loss: 1.97932332e-06
Iter: 1220 loss: 1.97585018e-06
Iter: 1221 loss: 1.98674888e-06
Iter: 1222 loss: 1.97482586e-06
Iter: 1223 loss: 1.97105e-06
Iter: 1224 loss: 1.98157932e-06
Iter: 1225 loss: 1.96993597e-06
Iter: 1226 loss: 1.96700262e-06
Iter: 1227 loss: 1.97509439e-06
Iter: 1228 loss: 1.96609312e-06
Iter: 1229 loss: 1.96194378e-06
Iter: 1230 loss: 1.9694603e-06
Iter: 1231 loss: 1.96021847e-06
Iter: 1232 loss: 1.95776806e-06
Iter: 1233 loss: 1.98345265e-06
Iter: 1234 loss: 1.95766302e-06
Iter: 1235 loss: 1.95518578e-06
Iter: 1236 loss: 1.95354414e-06
Iter: 1237 loss: 1.95260213e-06
Iter: 1238 loss: 1.9492702e-06
Iter: 1239 loss: 1.95840312e-06
Iter: 1240 loss: 1.94816835e-06
Iter: 1241 loss: 1.94514132e-06
Iter: 1242 loss: 1.95820553e-06
Iter: 1243 loss: 1.94456e-06
Iter: 1244 loss: 1.94141603e-06
Iter: 1245 loss: 1.94894892e-06
Iter: 1246 loss: 1.94033692e-06
Iter: 1247 loss: 1.93764799e-06
Iter: 1248 loss: 1.93523647e-06
Iter: 1249 loss: 1.93449523e-06
Iter: 1250 loss: 1.93044275e-06
Iter: 1251 loss: 1.98050498e-06
Iter: 1252 loss: 1.93038431e-06
Iter: 1253 loss: 1.9274371e-06
Iter: 1254 loss: 1.93059077e-06
Iter: 1255 loss: 1.92577454e-06
Iter: 1256 loss: 1.92233347e-06
Iter: 1257 loss: 1.92061088e-06
Iter: 1258 loss: 1.9189597e-06
Iter: 1259 loss: 1.91436357e-06
Iter: 1260 loss: 1.94903669e-06
Iter: 1261 loss: 1.91393201e-06
Iter: 1262 loss: 1.91060417e-06
Iter: 1263 loss: 1.91913205e-06
Iter: 1264 loss: 1.90942637e-06
Iter: 1265 loss: 1.90637297e-06
Iter: 1266 loss: 1.94147765e-06
Iter: 1267 loss: 1.90629623e-06
Iter: 1268 loss: 1.90364017e-06
Iter: 1269 loss: 1.90393894e-06
Iter: 1270 loss: 1.9016901e-06
Iter: 1271 loss: 1.89905597e-06
Iter: 1272 loss: 1.93188453e-06
Iter: 1273 loss: 1.89901209e-06
Iter: 1274 loss: 1.89700199e-06
Iter: 1275 loss: 1.89354728e-06
Iter: 1276 loss: 1.89353318e-06
Iter: 1277 loss: 1.88964157e-06
Iter: 1278 loss: 1.91253844e-06
Iter: 1279 loss: 1.88906017e-06
Iter: 1280 loss: 1.88570732e-06
Iter: 1281 loss: 1.89916875e-06
Iter: 1282 loss: 1.88494687e-06
Iter: 1283 loss: 1.8821172e-06
Iter: 1284 loss: 1.88666172e-06
Iter: 1285 loss: 1.88071499e-06
Iter: 1286 loss: 1.87765147e-06
Iter: 1287 loss: 1.89092214e-06
Iter: 1288 loss: 1.87700687e-06
Iter: 1289 loss: 1.87393107e-06
Iter: 1290 loss: 1.8740102e-06
Iter: 1291 loss: 1.87153319e-06
Iter: 1292 loss: 1.86807176e-06
Iter: 1293 loss: 1.87791329e-06
Iter: 1294 loss: 1.86705074e-06
Iter: 1295 loss: 1.86381226e-06
Iter: 1296 loss: 1.89320042e-06
Iter: 1297 loss: 1.86371767e-06
Iter: 1298 loss: 1.86114755e-06
Iter: 1299 loss: 1.85915712e-06
Iter: 1300 loss: 1.85830436e-06
Iter: 1301 loss: 1.85451881e-06
Iter: 1302 loss: 1.86671423e-06
Iter: 1303 loss: 1.85344527e-06
Iter: 1304 loss: 1.8508415e-06
Iter: 1305 loss: 1.88334343e-06
Iter: 1306 loss: 1.85075419e-06
Iter: 1307 loss: 1.84789963e-06
Iter: 1308 loss: 1.84975295e-06
Iter: 1309 loss: 1.84610928e-06
Iter: 1310 loss: 1.84332714e-06
Iter: 1311 loss: 1.85392969e-06
Iter: 1312 loss: 1.84263251e-06
Iter: 1313 loss: 1.83989243e-06
Iter: 1314 loss: 1.84660985e-06
Iter: 1315 loss: 1.8388879e-06
Iter: 1316 loss: 1.83596228e-06
Iter: 1317 loss: 1.83518273e-06
Iter: 1318 loss: 1.83339898e-06
Iter: 1319 loss: 1.82939755e-06
Iter: 1320 loss: 1.84187127e-06
Iter: 1321 loss: 1.82828785e-06
Iter: 1322 loss: 1.82544409e-06
Iter: 1323 loss: 1.86709599e-06
Iter: 1324 loss: 1.82539975e-06
Iter: 1325 loss: 1.82321605e-06
Iter: 1326 loss: 1.82047506e-06
Iter: 1327 loss: 1.8202619e-06
Iter: 1328 loss: 1.81674216e-06
Iter: 1329 loss: 1.83296629e-06
Iter: 1330 loss: 1.81607129e-06
Iter: 1331 loss: 1.81275595e-06
Iter: 1332 loss: 1.82426095e-06
Iter: 1333 loss: 1.81190148e-06
Iter: 1334 loss: 1.80826214e-06
Iter: 1335 loss: 1.81239329e-06
Iter: 1336 loss: 1.80623169e-06
Iter: 1337 loss: 1.80281802e-06
Iter: 1338 loss: 1.81390647e-06
Iter: 1339 loss: 1.80177869e-06
Iter: 1340 loss: 1.79869869e-06
Iter: 1341 loss: 1.81972109e-06
Iter: 1342 loss: 1.79838753e-06
Iter: 1343 loss: 1.79568133e-06
Iter: 1344 loss: 1.79798053e-06
Iter: 1345 loss: 1.79405095e-06
Iter: 1346 loss: 1.7920346e-06
Iter: 1347 loss: 1.7920247e-06
Iter: 1348 loss: 1.7904614e-06
Iter: 1349 loss: 1.78703442e-06
Iter: 1350 loss: 1.83735142e-06
Iter: 1351 loss: 1.78686059e-06
Iter: 1352 loss: 1.78322512e-06
Iter: 1353 loss: 1.80808024e-06
Iter: 1354 loss: 1.7828537e-06
Iter: 1355 loss: 1.77988761e-06
Iter: 1356 loss: 1.8005934e-06
Iter: 1357 loss: 1.7796782e-06
Iter: 1358 loss: 1.7777221e-06
Iter: 1359 loss: 1.77387915e-06
Iter: 1360 loss: 1.85163617e-06
Iter: 1361 loss: 1.7738879e-06
Iter: 1362 loss: 1.77048958e-06
Iter: 1363 loss: 1.77051083e-06
Iter: 1364 loss: 1.76812159e-06
Iter: 1365 loss: 1.77552306e-06
Iter: 1366 loss: 1.76748267e-06
Iter: 1367 loss: 1.76481956e-06
Iter: 1368 loss: 1.76260073e-06
Iter: 1369 loss: 1.76195226e-06
Iter: 1370 loss: 1.75841376e-06
Iter: 1371 loss: 1.77145387e-06
Iter: 1372 loss: 1.75760624e-06
Iter: 1373 loss: 1.75473065e-06
Iter: 1374 loss: 1.77331481e-06
Iter: 1375 loss: 1.75440596e-06
Iter: 1376 loss: 1.75156845e-06
Iter: 1377 loss: 1.75655123e-06
Iter: 1378 loss: 1.75032164e-06
Iter: 1379 loss: 1.74725687e-06
Iter: 1380 loss: 1.75370633e-06
Iter: 1381 loss: 1.74610523e-06
Iter: 1382 loss: 1.7435068e-06
Iter: 1383 loss: 1.77052095e-06
Iter: 1384 loss: 1.74345973e-06
Iter: 1385 loss: 1.74099785e-06
Iter: 1386 loss: 1.74182105e-06
Iter: 1387 loss: 1.73926219e-06
Iter: 1388 loss: 1.7367903e-06
Iter: 1389 loss: 1.74593674e-06
Iter: 1390 loss: 1.73626063e-06
Iter: 1391 loss: 1.73371018e-06
Iter: 1392 loss: 1.73460637e-06
Iter: 1393 loss: 1.73193644e-06
Iter: 1394 loss: 1.72941395e-06
Iter: 1395 loss: 1.74753939e-06
Iter: 1396 loss: 1.7291178e-06
Iter: 1397 loss: 1.72666046e-06
Iter: 1398 loss: 1.7283154e-06
Iter: 1399 loss: 1.72503849e-06
Iter: 1400 loss: 1.72171769e-06
Iter: 1401 loss: 1.72692137e-06
Iter: 1402 loss: 1.72018508e-06
Iter: 1403 loss: 1.71745262e-06
Iter: 1404 loss: 1.72689829e-06
Iter: 1405 loss: 1.71680801e-06
Iter: 1406 loss: 1.71421584e-06
Iter: 1407 loss: 1.73090564e-06
Iter: 1408 loss: 1.71386137e-06
Iter: 1409 loss: 1.71140482e-06
Iter: 1410 loss: 1.71070576e-06
Iter: 1411 loss: 1.70922851e-06
Iter: 1412 loss: 1.70594103e-06
Iter: 1413 loss: 1.70862336e-06
Iter: 1414 loss: 1.704032e-06
Iter: 1415 loss: 1.70058217e-06
Iter: 1416 loss: 1.71754971e-06
Iter: 1417 loss: 1.69993382e-06
Iter: 1418 loss: 1.69690702e-06
Iter: 1419 loss: 1.72566251e-06
Iter: 1420 loss: 1.69681823e-06
Iter: 1421 loss: 1.6946808e-06
Iter: 1422 loss: 1.69744396e-06
Iter: 1423 loss: 1.69367161e-06
Iter: 1424 loss: 1.69094756e-06
Iter: 1425 loss: 1.70595251e-06
Iter: 1426 loss: 1.69063298e-06
Iter: 1427 loss: 1.68867882e-06
Iter: 1428 loss: 1.68645306e-06
Iter: 1429 loss: 1.6861967e-06
Iter: 1430 loss: 1.68309145e-06
Iter: 1431 loss: 1.70200099e-06
Iter: 1432 loss: 1.68270026e-06
Iter: 1433 loss: 1.68002146e-06
Iter: 1434 loss: 1.68958172e-06
Iter: 1435 loss: 1.6793133e-06
Iter: 1436 loss: 1.67688677e-06
Iter: 1437 loss: 1.67973303e-06
Iter: 1438 loss: 1.67556e-06
Iter: 1439 loss: 1.67279268e-06
Iter: 1440 loss: 1.69176963e-06
Iter: 1441 loss: 1.67259441e-06
Iter: 1442 loss: 1.67074222e-06
Iter: 1443 loss: 1.66827454e-06
Iter: 1444 loss: 1.66807945e-06
Iter: 1445 loss: 1.66509903e-06
Iter: 1446 loss: 1.69339035e-06
Iter: 1447 loss: 1.66500604e-06
Iter: 1448 loss: 1.662258e-06
Iter: 1449 loss: 1.6716258e-06
Iter: 1450 loss: 1.66155462e-06
Iter: 1451 loss: 1.65945926e-06
Iter: 1452 loss: 1.65605411e-06
Iter: 1453 loss: 1.65599727e-06
Iter: 1454 loss: 1.65226334e-06
Iter: 1455 loss: 1.6914878e-06
Iter: 1456 loss: 1.6521924e-06
Iter: 1457 loss: 1.64958794e-06
Iter: 1458 loss: 1.65890447e-06
Iter: 1459 loss: 1.64898324e-06
Iter: 1460 loss: 1.64630251e-06
Iter: 1461 loss: 1.66433438e-06
Iter: 1462 loss: 1.6460109e-06
Iter: 1463 loss: 1.64409244e-06
Iter: 1464 loss: 1.64838104e-06
Iter: 1465 loss: 1.64337575e-06
Iter: 1466 loss: 1.64144888e-06
Iter: 1467 loss: 1.64400967e-06
Iter: 1468 loss: 1.64038147e-06
Iter: 1469 loss: 1.63816617e-06
Iter: 1470 loss: 1.63729237e-06
Iter: 1471 loss: 1.63615209e-06
Iter: 1472 loss: 1.63339405e-06
Iter: 1473 loss: 1.66375821e-06
Iter: 1474 loss: 1.6333438e-06
Iter: 1475 loss: 1.63127993e-06
Iter: 1476 loss: 1.63741515e-06
Iter: 1477 loss: 1.63063146e-06
Iter: 1478 loss: 1.62861943e-06
Iter: 1479 loss: 1.62829645e-06
Iter: 1480 loss: 1.6269197e-06
Iter: 1481 loss: 1.62415404e-06
Iter: 1482 loss: 1.64226799e-06
Iter: 1483 loss: 1.62380775e-06
Iter: 1484 loss: 1.62152628e-06
Iter: 1485 loss: 1.62241736e-06
Iter: 1486 loss: 1.61993137e-06
Iter: 1487 loss: 1.6173534e-06
Iter: 1488 loss: 1.63577715e-06
Iter: 1489 loss: 1.61709738e-06
Iter: 1490 loss: 1.61494756e-06
Iter: 1491 loss: 1.6195944e-06
Iter: 1492 loss: 1.61403909e-06
Iter: 1493 loss: 1.61147454e-06
Iter: 1494 loss: 1.61104549e-06
Iter: 1495 loss: 1.60939192e-06
Iter: 1496 loss: 1.60654361e-06
Iter: 1497 loss: 1.61301818e-06
Iter: 1498 loss: 1.60549121e-06
Iter: 1499 loss: 1.60276659e-06
Iter: 1500 loss: 1.6359013e-06
Iter: 1501 loss: 1.60272498e-06
Iter: 1502 loss: 1.60052275e-06
Iter: 1503 loss: 1.6093968e-06
Iter: 1504 loss: 1.60000445e-06
Iter: 1505 loss: 1.59824685e-06
Iter: 1506 loss: 1.59553838e-06
Iter: 1507 loss: 1.59547017e-06
Iter: 1508 loss: 1.59274714e-06
Iter: 1509 loss: 1.63330037e-06
Iter: 1510 loss: 1.59279534e-06
Iter: 1511 loss: 1.59083982e-06
Iter: 1512 loss: 1.58945579e-06
Iter: 1513 loss: 1.5887623e-06
Iter: 1514 loss: 1.58619775e-06
Iter: 1515 loss: 1.60293882e-06
Iter: 1516 loss: 1.58595844e-06
Iter: 1517 loss: 1.58369107e-06
Iter: 1518 loss: 1.59468027e-06
Iter: 1519 loss: 1.58327293e-06
Iter: 1520 loss: 1.58138562e-06
Iter: 1521 loss: 1.5789426e-06
Iter: 1522 loss: 1.57877014e-06
Iter: 1523 loss: 1.57620366e-06
Iter: 1524 loss: 1.61256457e-06
Iter: 1525 loss: 1.57621116e-06
Iter: 1526 loss: 1.57386216e-06
Iter: 1527 loss: 1.57235957e-06
Iter: 1528 loss: 1.57144677e-06
Iter: 1529 loss: 1.56898318e-06
Iter: 1530 loss: 1.60032914e-06
Iter: 1531 loss: 1.56891156e-06
Iter: 1532 loss: 1.5668e-06
Iter: 1533 loss: 1.56662065e-06
Iter: 1534 loss: 1.56503313e-06
Iter: 1535 loss: 1.56234341e-06
Iter: 1536 loss: 1.56512112e-06
Iter: 1537 loss: 1.56078045e-06
Iter: 1538 loss: 1.55801717e-06
Iter: 1539 loss: 1.58531452e-06
Iter: 1540 loss: 1.55790099e-06
Iter: 1541 loss: 1.55556813e-06
Iter: 1542 loss: 1.57003274e-06
Iter: 1543 loss: 1.55533223e-06
Iter: 1544 loss: 1.55390853e-06
Iter: 1545 loss: 1.55173e-06
Iter: 1546 loss: 1.55170369e-06
Iter: 1547 loss: 1.54927091e-06
Iter: 1548 loss: 1.56751821e-06
Iter: 1549 loss: 1.54906968e-06
Iter: 1550 loss: 1.54650706e-06
Iter: 1551 loss: 1.54880183e-06
Iter: 1552 loss: 1.54501913e-06
Iter: 1553 loss: 1.54268105e-06
Iter: 1554 loss: 1.55404132e-06
Iter: 1555 loss: 1.54233612e-06
Iter: 1556 loss: 1.54039242e-06
Iter: 1557 loss: 1.54853342e-06
Iter: 1558 loss: 1.53994404e-06
Iter: 1559 loss: 1.53781707e-06
Iter: 1560 loss: 1.53786959e-06
Iter: 1561 loss: 1.53609835e-06
Iter: 1562 loss: 1.53364e-06
Iter: 1563 loss: 1.53871542e-06
Iter: 1564 loss: 1.53255291e-06
Iter: 1565 loss: 1.53013832e-06
Iter: 1566 loss: 1.54817178e-06
Iter: 1567 loss: 1.52991765e-06
Iter: 1568 loss: 1.52775283e-06
Iter: 1569 loss: 1.53033443e-06
Iter: 1570 loss: 1.526593e-06
Iter: 1571 loss: 1.52440748e-06
Iter: 1572 loss: 1.53350356e-06
Iter: 1573 loss: 1.52397706e-06
Iter: 1574 loss: 1.5216699e-06
Iter: 1575 loss: 1.52562029e-06
Iter: 1576 loss: 1.52064604e-06
Iter: 1577 loss: 1.51844813e-06
Iter: 1578 loss: 1.51736469e-06
Iter: 1579 loss: 1.51625454e-06
Iter: 1580 loss: 1.51501763e-06
Iter: 1581 loss: 1.51446761e-06
Iter: 1582 loss: 1.51304096e-06
Iter: 1583 loss: 1.51076256e-06
Iter: 1584 loss: 1.51073789e-06
Iter: 1585 loss: 1.50835842e-06
Iter: 1586 loss: 1.51269523e-06
Iter: 1587 loss: 1.50729215e-06
Iter: 1588 loss: 1.5046985e-06
Iter: 1589 loss: 1.52293228e-06
Iter: 1590 loss: 1.50450433e-06
Iter: 1591 loss: 1.50236508e-06
Iter: 1592 loss: 1.50672815e-06
Iter: 1593 loss: 1.50148276e-06
Iter: 1594 loss: 1.49917969e-06
Iter: 1595 loss: 1.50293408e-06
Iter: 1596 loss: 1.49818197e-06
Iter: 1597 loss: 1.49633195e-06
Iter: 1598 loss: 1.51772292e-06
Iter: 1599 loss: 1.4963033e-06
Iter: 1600 loss: 1.49467837e-06
Iter: 1601 loss: 1.49188327e-06
Iter: 1602 loss: 1.5612394e-06
Iter: 1603 loss: 1.49189509e-06
Iter: 1604 loss: 1.48903405e-06
Iter: 1605 loss: 1.51475081e-06
Iter: 1606 loss: 1.4888351e-06
Iter: 1607 loss: 1.48682125e-06
Iter: 1608 loss: 1.49730317e-06
Iter: 1609 loss: 1.48644654e-06
Iter: 1610 loss: 1.48446748e-06
Iter: 1611 loss: 1.48630602e-06
Iter: 1612 loss: 1.4833513e-06
Iter: 1613 loss: 1.48146285e-06
Iter: 1614 loss: 1.48712934e-06
Iter: 1615 loss: 1.48088498e-06
Iter: 1616 loss: 1.47865023e-06
Iter: 1617 loss: 1.48500544e-06
Iter: 1618 loss: 1.47795026e-06
Iter: 1619 loss: 1.47583444e-06
Iter: 1620 loss: 1.48899312e-06
Iter: 1621 loss: 1.47564083e-06
Iter: 1622 loss: 1.47381843e-06
Iter: 1623 loss: 1.47580226e-06
Iter: 1624 loss: 1.47278251e-06
Iter: 1625 loss: 1.47095398e-06
Iter: 1626 loss: 1.46900595e-06
Iter: 1627 loss: 1.46864136e-06
Iter: 1628 loss: 1.46614946e-06
Iter: 1629 loss: 1.48425465e-06
Iter: 1630 loss: 1.46594175e-06
Iter: 1631 loss: 1.46380444e-06
Iter: 1632 loss: 1.4767204e-06
Iter: 1633 loss: 1.46353796e-06
Iter: 1634 loss: 1.46156094e-06
Iter: 1635 loss: 1.462613e-06
Iter: 1636 loss: 1.46022251e-06
Iter: 1637 loss: 1.45813988e-06
Iter: 1638 loss: 1.46892694e-06
Iter: 1639 loss: 1.45778972e-06
Iter: 1640 loss: 1.45590047e-06
Iter: 1641 loss: 1.46111961e-06
Iter: 1642 loss: 1.45522222e-06
Iter: 1643 loss: 1.45306524e-06
Iter: 1644 loss: 1.4524951e-06
Iter: 1645 loss: 1.45123e-06
Iter: 1646 loss: 1.44893056e-06
Iter: 1647 loss: 1.46105708e-06
Iter: 1648 loss: 1.44855824e-06
Iter: 1649 loss: 1.445934e-06
Iter: 1650 loss: 1.45504237e-06
Iter: 1651 loss: 1.4452919e-06
Iter: 1652 loss: 1.44339992e-06
Iter: 1653 loss: 1.44328783e-06
Iter: 1654 loss: 1.44184833e-06
Iter: 1655 loss: 1.43990565e-06
Iter: 1656 loss: 1.4398687e-06
Iter: 1657 loss: 1.43827833e-06
Iter: 1658 loss: 1.43816737e-06
Iter: 1659 loss: 1.43687976e-06
Iter: 1660 loss: 1.43475381e-06
Iter: 1661 loss: 1.4409502e-06
Iter: 1662 loss: 1.43407647e-06
Iter: 1663 loss: 1.4320733e-06
Iter: 1664 loss: 1.43449529e-06
Iter: 1665 loss: 1.43106058e-06
Iter: 1666 loss: 1.42865588e-06
Iter: 1667 loss: 1.43074794e-06
Iter: 1668 loss: 1.42717e-06
Iter: 1669 loss: 1.42486306e-06
Iter: 1670 loss: 1.43685588e-06
Iter: 1671 loss: 1.42448016e-06
Iter: 1672 loss: 1.42207023e-06
Iter: 1673 loss: 1.43329362e-06
Iter: 1674 loss: 1.42157955e-06
Iter: 1675 loss: 1.41944213e-06
Iter: 1676 loss: 1.42182591e-06
Iter: 1677 loss: 1.41836938e-06
Iter: 1678 loss: 1.41619523e-06
Iter: 1679 loss: 1.42379918e-06
Iter: 1680 loss: 1.41561623e-06
Iter: 1681 loss: 1.41339683e-06
Iter: 1682 loss: 1.42076385e-06
Iter: 1683 loss: 1.41279384e-06
Iter: 1684 loss: 1.4106804e-06
Iter: 1685 loss: 1.41079795e-06
Iter: 1686 loss: 1.40907377e-06
Iter: 1687 loss: 1.40687848e-06
Iter: 1688 loss: 1.42278031e-06
Iter: 1689 loss: 1.40665099e-06
Iter: 1690 loss: 1.40443171e-06
Iter: 1691 loss: 1.41183648e-06
Iter: 1692 loss: 1.40386123e-06
Iter: 1693 loss: 1.40253496e-06
Iter: 1694 loss: 1.40507609e-06
Iter: 1695 loss: 1.40193117e-06
Iter: 1696 loss: 1.39988072e-06
Iter: 1697 loss: 1.40798602e-06
Iter: 1698 loss: 1.39951248e-06
Iter: 1699 loss: 1.39782742e-06
Iter: 1700 loss: 1.39684312e-06
Iter: 1701 loss: 1.39620283e-06
Iter: 1702 loss: 1.39387362e-06
Iter: 1703 loss: 1.39924828e-06
Iter: 1704 loss: 1.39302631e-06
Iter: 1705 loss: 1.39066969e-06
Iter: 1706 loss: 1.40658915e-06
Iter: 1707 loss: 1.39048188e-06
Iter: 1708 loss: 1.38889254e-06
Iter: 1709 loss: 1.38909479e-06
Iter: 1710 loss: 1.38764312e-06
Iter: 1711 loss: 1.38520397e-06
Iter: 1712 loss: 1.39218355e-06
Iter: 1713 loss: 1.38439532e-06
Iter: 1714 loss: 1.38253358e-06
Iter: 1715 loss: 1.40682528e-06
Iter: 1716 loss: 1.38252926e-06
Iter: 1717 loss: 1.3810602e-06
Iter: 1718 loss: 1.37883148e-06
Iter: 1719 loss: 1.37879533e-06
Iter: 1720 loss: 1.3764145e-06
Iter: 1721 loss: 1.39470933e-06
Iter: 1722 loss: 1.37621146e-06
Iter: 1723 loss: 1.37376719e-06
Iter: 1724 loss: 1.37745837e-06
Iter: 1725 loss: 1.37266261e-06
Iter: 1726 loss: 1.37067559e-06
Iter: 1727 loss: 1.37402765e-06
Iter: 1728 loss: 1.36983374e-06
Iter: 1729 loss: 1.3676754e-06
Iter: 1730 loss: 1.38465202e-06
Iter: 1731 loss: 1.36754659e-06
Iter: 1732 loss: 1.36570554e-06
Iter: 1733 loss: 1.36769904e-06
Iter: 1734 loss: 1.36477115e-06
Iter: 1735 loss: 1.36325298e-06
Iter: 1736 loss: 1.38306518e-06
Iter: 1737 loss: 1.36323956e-06
Iter: 1738 loss: 1.362136e-06
Iter: 1739 loss: 1.36048641e-06
Iter: 1740 loss: 1.36042593e-06
Iter: 1741 loss: 1.35799473e-06
Iter: 1742 loss: 1.35764458e-06
Iter: 1743 loss: 1.35594189e-06
Iter: 1744 loss: 1.35340815e-06
Iter: 1745 loss: 1.38392625e-06
Iter: 1746 loss: 1.3534102e-06
Iter: 1747 loss: 1.35143932e-06
Iter: 1748 loss: 1.35680807e-06
Iter: 1749 loss: 1.35075811e-06
Iter: 1750 loss: 1.34854986e-06
Iter: 1751 loss: 1.34973789e-06
Iter: 1752 loss: 1.34703851e-06
Iter: 1753 loss: 1.34501693e-06
Iter: 1754 loss: 1.36471738e-06
Iter: 1755 loss: 1.34489221e-06
Iter: 1756 loss: 1.34305856e-06
Iter: 1757 loss: 1.34631046e-06
Iter: 1758 loss: 1.34230072e-06
Iter: 1759 loss: 1.34037055e-06
Iter: 1760 loss: 1.34279958e-06
Iter: 1761 loss: 1.33932849e-06
Iter: 1762 loss: 1.33722665e-06
Iter: 1763 loss: 1.34228867e-06
Iter: 1764 loss: 1.33649905e-06
Iter: 1765 loss: 1.3341878e-06
Iter: 1766 loss: 1.34596439e-06
Iter: 1767 loss: 1.33379888e-06
Iter: 1768 loss: 1.33190531e-06
Iter: 1769 loss: 1.33233107e-06
Iter: 1770 loss: 1.33055653e-06
Iter: 1771 loss: 1.32890182e-06
Iter: 1772 loss: 1.32894127e-06
Iter: 1773 loss: 1.32759703e-06
Iter: 1774 loss: 1.32747459e-06
Iter: 1775 loss: 1.32650985e-06
Iter: 1776 loss: 1.32454522e-06
Iter: 1777 loss: 1.33424987e-06
Iter: 1778 loss: 1.3242111e-06
Iter: 1779 loss: 1.32253012e-06
Iter: 1780 loss: 1.32292541e-06
Iter: 1781 loss: 1.32130185e-06
Iter: 1782 loss: 1.31931415e-06
Iter: 1783 loss: 1.31990464e-06
Iter: 1784 loss: 1.31788499e-06
Iter: 1785 loss: 1.31517925e-06
Iter: 1786 loss: 1.32195737e-06
Iter: 1787 loss: 1.31435104e-06
Iter: 1788 loss: 1.31249408e-06
Iter: 1789 loss: 1.33706794e-06
Iter: 1790 loss: 1.31250613e-06
Iter: 1791 loss: 1.3109925e-06
Iter: 1792 loss: 1.31169043e-06
Iter: 1793 loss: 1.30985688e-06
Iter: 1794 loss: 1.30787191e-06
Iter: 1795 loss: 1.31541333e-06
Iter: 1796 loss: 1.30745218e-06
Iter: 1797 loss: 1.30571766e-06
Iter: 1798 loss: 1.31502713e-06
Iter: 1799 loss: 1.3054123e-06
Iter: 1800 loss: 1.30375838e-06
Iter: 1801 loss: 1.30328681e-06
Iter: 1802 loss: 1.30223157e-06
Iter: 1803 loss: 1.30029593e-06
Iter: 1804 loss: 1.30508238e-06
Iter: 1805 loss: 1.29960142e-06
Iter: 1806 loss: 1.29729165e-06
Iter: 1807 loss: 1.31118281e-06
Iter: 1808 loss: 1.29702539e-06
Iter: 1809 loss: 1.29523596e-06
Iter: 1810 loss: 1.29698469e-06
Iter: 1811 loss: 1.2942478e-06
Iter: 1812 loss: 1.29285922e-06
Iter: 1813 loss: 1.30925582e-06
Iter: 1814 loss: 1.29281216e-06
Iter: 1815 loss: 1.29140426e-06
Iter: 1816 loss: 1.29100704e-06
Iter: 1817 loss: 1.29016257e-06
Iter: 1818 loss: 1.28838053e-06
Iter: 1819 loss: 1.30188027e-06
Iter: 1820 loss: 1.28820307e-06
Iter: 1821 loss: 1.28666682e-06
Iter: 1822 loss: 1.28529268e-06
Iter: 1823 loss: 1.28494139e-06
Iter: 1824 loss: 1.28281135e-06
Iter: 1825 loss: 1.28626516e-06
Iter: 1826 loss: 1.28184138e-06
Iter: 1827 loss: 1.27929843e-06
Iter: 1828 loss: 1.28453371e-06
Iter: 1829 loss: 1.27824978e-06
Iter: 1830 loss: 1.27570138e-06
Iter: 1831 loss: 1.29128534e-06
Iter: 1832 loss: 1.27542512e-06
Iter: 1833 loss: 1.27326302e-06
Iter: 1834 loss: 1.28358442e-06
Iter: 1835 loss: 1.27291219e-06
Iter: 1836 loss: 1.27139515e-06
Iter: 1837 loss: 1.27413296e-06
Iter: 1838 loss: 1.27076714e-06
Iter: 1839 loss: 1.26903592e-06
Iter: 1840 loss: 1.27680244e-06
Iter: 1841 loss: 1.26858754e-06
Iter: 1842 loss: 1.26699433e-06
Iter: 1843 loss: 1.26805571e-06
Iter: 1844 loss: 1.2660505e-06
Iter: 1845 loss: 1.26405325e-06
Iter: 1846 loss: 1.26698058e-06
Iter: 1847 loss: 1.26309089e-06
Iter: 1848 loss: 1.2612569e-06
Iter: 1849 loss: 1.27674048e-06
Iter: 1850 loss: 1.26118107e-06
Iter: 1851 loss: 1.25930933e-06
Iter: 1852 loss: 1.25834754e-06
Iter: 1853 loss: 1.25750535e-06
Iter: 1854 loss: 1.25560882e-06
Iter: 1855 loss: 1.27155715e-06
Iter: 1856 loss: 1.2555106e-06
Iter: 1857 loss: 1.25390534e-06
Iter: 1858 loss: 1.26001282e-06
Iter: 1859 loss: 1.25347822e-06
Iter: 1860 loss: 1.25215297e-06
Iter: 1861 loss: 1.25457973e-06
Iter: 1862 loss: 1.25150882e-06
Iter: 1863 loss: 1.24976168e-06
Iter: 1864 loss: 1.25915676e-06
Iter: 1865 loss: 1.24951407e-06
Iter: 1866 loss: 1.24832343e-06
Iter: 1867 loss: 1.24663836e-06
Iter: 1868 loss: 1.24658095e-06
Iter: 1869 loss: 1.24425219e-06
Iter: 1870 loss: 1.24968574e-06
Iter: 1871 loss: 1.24345763e-06
Iter: 1872 loss: 1.24129906e-06
Iter: 1873 loss: 1.25185068e-06
Iter: 1874 loss: 1.24094595e-06
Iter: 1875 loss: 1.23915356e-06
Iter: 1876 loss: 1.2435454e-06
Iter: 1877 loss: 1.23850759e-06
Iter: 1878 loss: 1.23632799e-06
Iter: 1879 loss: 1.24565611e-06
Iter: 1880 loss: 1.23589598e-06
Iter: 1881 loss: 1.23428924e-06
Iter: 1882 loss: 1.23531424e-06
Iter: 1883 loss: 1.23322025e-06
Iter: 1884 loss: 1.23149425e-06
Iter: 1885 loss: 1.24258781e-06
Iter: 1886 loss: 1.23128166e-06
Iter: 1887 loss: 1.22934375e-06
Iter: 1888 loss: 1.23316477e-06
Iter: 1889 loss: 1.22860524e-06
Iter: 1890 loss: 1.22696724e-06
Iter: 1891 loss: 1.22917731e-06
Iter: 1892 loss: 1.22612482e-06
Iter: 1893 loss: 1.22416941e-06
Iter: 1894 loss: 1.23689892e-06
Iter: 1895 loss: 1.22398808e-06
Iter: 1896 loss: 1.22258859e-06
Iter: 1897 loss: 1.22579468e-06
Iter: 1898 loss: 1.2220421e-06
Iter: 1899 loss: 1.22062966e-06
Iter: 1900 loss: 1.22864708e-06
Iter: 1901 loss: 1.22041388e-06
Iter: 1902 loss: 1.21905941e-06
Iter: 1903 loss: 1.21788571e-06
Iter: 1904 loss: 1.21745416e-06
Iter: 1905 loss: 1.21576068e-06
Iter: 1906 loss: 1.22234e-06
Iter: 1907 loss: 1.21542496e-06
Iter: 1908 loss: 1.21349763e-06
Iter: 1909 loss: 1.22049721e-06
Iter: 1910 loss: 1.21296705e-06
Iter: 1911 loss: 1.21165783e-06
Iter: 1912 loss: 1.21290645e-06
Iter: 1913 loss: 1.21091819e-06
Iter: 1914 loss: 1.20918276e-06
Iter: 1915 loss: 1.20912705e-06
Iter: 1916 loss: 1.20777941e-06
Iter: 1917 loss: 1.2056779e-06
Iter: 1918 loss: 1.2214125e-06
Iter: 1919 loss: 1.20547816e-06
Iter: 1920 loss: 1.20382629e-06
Iter: 1921 loss: 1.20982668e-06
Iter: 1922 loss: 1.20334255e-06
Iter: 1923 loss: 1.2015031e-06
Iter: 1924 loss: 1.20289974e-06
Iter: 1925 loss: 1.20035384e-06
Iter: 1926 loss: 1.19838774e-06
Iter: 1927 loss: 1.20166794e-06
Iter: 1928 loss: 1.19743436e-06
Iter: 1929 loss: 1.19590925e-06
Iter: 1930 loss: 1.19589095e-06
Iter: 1931 loss: 1.19464448e-06
Iter: 1932 loss: 1.1941878e-06
Iter: 1933 loss: 1.19344293e-06
Iter: 1934 loss: 1.19210938e-06
Iter: 1935 loss: 1.21320886e-06
Iter: 1936 loss: 1.19211768e-06
Iter: 1937 loss: 1.190847e-06
Iter: 1938 loss: 1.18876937e-06
Iter: 1939 loss: 1.18876596e-06
Iter: 1940 loss: 1.1872753e-06
Iter: 1941 loss: 1.18723938e-06
Iter: 1942 loss: 1.18610683e-06
Iter: 1943 loss: 1.18523519e-06
Iter: 1944 loss: 1.18482285e-06
Iter: 1945 loss: 1.18300045e-06
Iter: 1946 loss: 1.18726541e-06
Iter: 1947 loss: 1.18235266e-06
Iter: 1948 loss: 1.18066941e-06
Iter: 1949 loss: 1.18806429e-06
Iter: 1950 loss: 1.18027265e-06
Iter: 1951 loss: 1.17849982e-06
Iter: 1952 loss: 1.18579328e-06
Iter: 1953 loss: 1.17809304e-06
Iter: 1954 loss: 1.17686523e-06
Iter: 1955 loss: 1.17608226e-06
Iter: 1956 loss: 1.175595e-06
Iter: 1957 loss: 1.17385321e-06
Iter: 1958 loss: 1.18298726e-06
Iter: 1959 loss: 1.17357536e-06
Iter: 1960 loss: 1.17193667e-06
Iter: 1961 loss: 1.1779041e-06
Iter: 1962 loss: 1.17154377e-06
Iter: 1963 loss: 1.17000809e-06
Iter: 1964 loss: 1.17206275e-06
Iter: 1965 loss: 1.16921365e-06
Iter: 1966 loss: 1.16736362e-06
Iter: 1967 loss: 1.1694101e-06
Iter: 1968 loss: 1.16640706e-06
Iter: 1969 loss: 1.16467709e-06
Iter: 1970 loss: 1.18504636e-06
Iter: 1971 loss: 1.16469073e-06
Iter: 1972 loss: 1.1629852e-06
Iter: 1973 loss: 1.1656889e-06
Iter: 1974 loss: 1.16220303e-06
Iter: 1975 loss: 1.16077467e-06
Iter: 1976 loss: 1.16122931e-06
Iter: 1977 loss: 1.15976809e-06
Iter: 1978 loss: 1.15798093e-06
Iter: 1979 loss: 1.17033585e-06
Iter: 1980 loss: 1.15782257e-06
Iter: 1981 loss: 1.15644389e-06
Iter: 1982 loss: 1.15610953e-06
Iter: 1983 loss: 1.1551333e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi3
+ date
Wed Nov  4 16:34:57 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi2.8/300_300_300_1 --function f2 --psi 1 --alpha 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e760e3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e50133598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e500bb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e500bbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e500bb7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e50114400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e50070510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e50070620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5009c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e300227b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e30022ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e300bc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e30022730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de475c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de47e19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de47e17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de47e1d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e50051ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4734730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e500519d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e50051c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4698048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de46a3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de46a3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4667840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de47947b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e760bb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4792ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de45b4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de45b4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de45526a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4552950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de461a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de46159d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de45cbbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de451c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0053847996
test_loss: 0.0086638415
train_loss: 0.005006306
test_loss: 0.00851362
train_loss: 0.0047263214
test_loss: 0.008521106
train_loss: 0.0050008744
test_loss: 0.008473456
train_loss: 0.0046533886
test_loss: 0.008421507
train_loss: 0.0048255995
test_loss: 0.008658059
train_loss: 0.00470465
test_loss: 0.008548572
train_loss: 0.004713702
test_loss: 0.0084308535
train_loss: 0.0051787663
test_loss: 0.008494719
train_loss: 0.0044903113
test_loss: 0.008418041
train_loss: 0.004899409
test_loss: 0.008485949
train_loss: 0.0049740183
test_loss: 0.00839771
train_loss: 0.0047057266
test_loss: 0.008456627
train_loss: 0.0048108934
test_loss: 0.008411574
train_loss: 0.0050829253
test_loss: 0.008830518
train_loss: 0.0046848482
test_loss: 0.008450104
train_loss: 0.0045852936
test_loss: 0.0082105175
train_loss: 0.0047880164
test_loss: 0.008401414
train_loss: 0.00464318
test_loss: 0.008370244
train_loss: 0.0044955555
test_loss: 0.008268388
train_loss: 0.00442269
test_loss: 0.008261324
train_loss: 0.005104494
test_loss: 0.008326841
train_loss: 0.004430293
test_loss: 0.008376162
train_loss: 0.004597878
test_loss: 0.008231902
train_loss: 0.0044018817
test_loss: 0.008370842
train_loss: 0.0044833818
test_loss: 0.008208791
train_loss: 0.0043315627
test_loss: 0.008241953
train_loss: 0.004863806
test_loss: 0.008480527
train_loss: 0.004958006
test_loss: 0.008599871
train_loss: 0.0043312446
test_loss: 0.008301911
train_loss: 0.004501754
test_loss: 0.008349958
train_loss: 0.004494937
test_loss: 0.008344599
train_loss: 0.0043834867
test_loss: 0.008329999
train_loss: 0.0045914794
test_loss: 0.008445244
train_loss: 0.004773387
test_loss: 0.008490369
train_loss: 0.004630678
test_loss: 0.008334214
train_loss: 0.0045601465
test_loss: 0.008285052
train_loss: 0.0043946295
test_loss: 0.008344162
train_loss: 0.0042793574
test_loss: 0.008217675
train_loss: 0.0043364866
test_loss: 0.0081403
train_loss: 0.004537006
test_loss: 0.008288363
train_loss: 0.004738048
test_loss: 0.008301697
train_loss: 0.0046753082
test_loss: 0.008381009
train_loss: 0.0045228857
test_loss: 0.008392874
train_loss: 0.004630248
test_loss: 0.008254078
train_loss: 0.0047103334
test_loss: 0.0084863
train_loss: 0.004319144
test_loss: 0.008253633
train_loss: 0.005135297
test_loss: 0.008287021
train_loss: 0.0048433794
test_loss: 0.008290496
train_loss: 0.004406103
test_loss: 0.008253571
train_loss: 0.004282389
test_loss: 0.008170526
train_loss: 0.0043487935
test_loss: 0.008235561
train_loss: 0.004470355
test_loss: 0.008255814
train_loss: 0.004379717
test_loss: 0.008183367
train_loss: 0.0046480047
test_loss: 0.008359666
train_loss: 0.0045397305
test_loss: 0.008268087
train_loss: 0.0044016717
test_loss: 0.008304358
train_loss: 0.004270861
test_loss: 0.00817787
train_loss: 0.004503316
test_loss: 0.0082602305
train_loss: 0.004353117
test_loss: 0.00815327
train_loss: 0.0047592805
test_loss: 0.008318829
train_loss: 0.0048160953
test_loss: 0.0082836095
train_loss: 0.0045255884
test_loss: 0.00810816
train_loss: 0.004439688
test_loss: 0.008105818
train_loss: 0.0044688815
test_loss: 0.008311523
train_loss: 0.004230836
test_loss: 0.008177745
train_loss: 0.004402382
test_loss: 0.008276534
train_loss: 0.004260762
test_loss: 0.008225068
train_loss: 0.0042961254
test_loss: 0.008223825
train_loss: 0.0041269762
test_loss: 0.008106154
train_loss: 0.0044992687
test_loss: 0.008172473
train_loss: 0.0042144866
test_loss: 0.007995426
train_loss: 0.004467111
test_loss: 0.008236051
train_loss: 0.0042167464
test_loss: 0.008245014
train_loss: 0.0042828573
test_loss: 0.00815121
train_loss: 0.0045561246
test_loss: 0.008133666
train_loss: 0.004248158
test_loss: 0.008116043
train_loss: 0.0040932954
test_loss: 0.008306414
train_loss: 0.004404463
test_loss: 0.008176786
train_loss: 0.0044766637
test_loss: 0.008314678
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi3/300_300_300_1 --optimizer lbfgs --function f2 --psi 1 --alpha 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi1_phi3/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777c5fd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777c5fdb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777c563488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777c563268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777c560ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777c51f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777c4f36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777c4f3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775f2c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775ed1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777c560048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775ed1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775efbc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777c6148c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775e55950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775e13730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775e13620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775dcb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775e01378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775dcb268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775dcbc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775d58a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775d83d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775d83730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775d32950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775d83840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f772582bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7775cdcb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7725860158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7725860598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7725860378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f772584a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77257afc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77257af9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77257af950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f772574cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.0253e-05
Iter: 2 loss: 3.6077e-05
Iter: 3 loss: 2.96080652e-05
Iter: 4 loss: 2.95126119e-05
Iter: 5 loss: 2.4584464e-05
Iter: 6 loss: 8.50962824e-05
Iter: 7 loss: 2.45287119e-05
Iter: 8 loss: 2.23531824e-05
Iter: 9 loss: 2.36786927e-05
Iter: 10 loss: 2.09566897e-05
Iter: 11 loss: 1.89252169e-05
Iter: 12 loss: 3.26224545e-05
Iter: 13 loss: 1.87202895e-05
Iter: 14 loss: 1.76049907e-05
Iter: 15 loss: 1.93783035e-05
Iter: 16 loss: 1.70864896e-05
Iter: 17 loss: 1.59765677e-05
Iter: 18 loss: 1.96481324e-05
Iter: 19 loss: 1.56724418e-05
Iter: 20 loss: 1.45661943e-05
Iter: 21 loss: 1.96936016e-05
Iter: 22 loss: 1.43586476e-05
Iter: 23 loss: 1.36887156e-05
Iter: 24 loss: 1.45148342e-05
Iter: 25 loss: 1.33393833e-05
Iter: 26 loss: 1.27136664e-05
Iter: 27 loss: 1.71993361e-05
Iter: 28 loss: 1.26597333e-05
Iter: 29 loss: 1.21421435e-05
Iter: 30 loss: 1.33792528e-05
Iter: 31 loss: 1.19544093e-05
Iter: 32 loss: 1.14956556e-05
Iter: 33 loss: 1.30788703e-05
Iter: 34 loss: 1.13742899e-05
Iter: 35 loss: 1.10422206e-05
Iter: 36 loss: 1.38832602e-05
Iter: 37 loss: 1.10242527e-05
Iter: 38 loss: 1.07768019e-05
Iter: 39 loss: 1.0557811e-05
Iter: 40 loss: 1.04936553e-05
Iter: 41 loss: 1.02031954e-05
Iter: 42 loss: 1.27782932e-05
Iter: 43 loss: 1.01888372e-05
Iter: 44 loss: 9.95807113e-06
Iter: 45 loss: 1.20614368e-05
Iter: 46 loss: 9.94783477e-06
Iter: 47 loss: 9.79115612e-06
Iter: 48 loss: 9.61248952e-06
Iter: 49 loss: 9.59007593e-06
Iter: 50 loss: 9.36488505e-06
Iter: 51 loss: 1.02506729e-05
Iter: 52 loss: 9.31295835e-06
Iter: 53 loss: 9.12394e-06
Iter: 54 loss: 1.06301659e-05
Iter: 55 loss: 9.11090683e-06
Iter: 56 loss: 8.97130667e-06
Iter: 57 loss: 9.05158413e-06
Iter: 58 loss: 8.88036e-06
Iter: 59 loss: 8.72661622e-06
Iter: 60 loss: 9.44273779e-06
Iter: 61 loss: 8.6981272e-06
Iter: 62 loss: 8.57187388e-06
Iter: 63 loss: 9.12039286e-06
Iter: 64 loss: 8.54602877e-06
Iter: 65 loss: 8.42741429e-06
Iter: 66 loss: 8.44425631e-06
Iter: 67 loss: 8.33745617e-06
Iter: 68 loss: 8.21946378e-06
Iter: 69 loss: 8.89213e-06
Iter: 70 loss: 8.20326386e-06
Iter: 71 loss: 8.07401102e-06
Iter: 72 loss: 8.46917828e-06
Iter: 73 loss: 8.03551211e-06
Iter: 74 loss: 7.95354208e-06
Iter: 75 loss: 8.06906e-06
Iter: 76 loss: 7.91298e-06
Iter: 77 loss: 7.81040399e-06
Iter: 78 loss: 8.28518569e-06
Iter: 79 loss: 7.79119091e-06
Iter: 80 loss: 7.70862516e-06
Iter: 81 loss: 8.00036287e-06
Iter: 82 loss: 7.68724931e-06
Iter: 83 loss: 7.61419324e-06
Iter: 84 loss: 8.15954627e-06
Iter: 85 loss: 7.60837975e-06
Iter: 86 loss: 7.55966357e-06
Iter: 87 loss: 7.48029106e-06
Iter: 88 loss: 7.47958e-06
Iter: 89 loss: 7.37813707e-06
Iter: 90 loss: 7.60510511e-06
Iter: 91 loss: 7.33942534e-06
Iter: 92 loss: 7.27400129e-06
Iter: 93 loss: 8.24989093e-06
Iter: 94 loss: 7.2738203e-06
Iter: 95 loss: 7.21539e-06
Iter: 96 loss: 7.28508394e-06
Iter: 97 loss: 7.18435422e-06
Iter: 98 loss: 7.12123574e-06
Iter: 99 loss: 7.15409305e-06
Iter: 100 loss: 7.07960498e-06
Iter: 101 loss: 7.02091256e-06
Iter: 102 loss: 7.68046084e-06
Iter: 103 loss: 7.01965473e-06
Iter: 104 loss: 6.96595362e-06
Iter: 105 loss: 6.96457937e-06
Iter: 106 loss: 6.92235244e-06
Iter: 107 loss: 6.87013471e-06
Iter: 108 loss: 7.17325111e-06
Iter: 109 loss: 6.86324711e-06
Iter: 110 loss: 6.81565052e-06
Iter: 111 loss: 7.03488513e-06
Iter: 112 loss: 6.80655739e-06
Iter: 113 loss: 6.76633499e-06
Iter: 114 loss: 6.77868275e-06
Iter: 115 loss: 6.73763316e-06
Iter: 116 loss: 6.69553356e-06
Iter: 117 loss: 7.02008947e-06
Iter: 118 loss: 6.69243491e-06
Iter: 119 loss: 6.65336393e-06
Iter: 120 loss: 6.80998573e-06
Iter: 121 loss: 6.64452364e-06
Iter: 122 loss: 6.61100557e-06
Iter: 123 loss: 6.59801572e-06
Iter: 124 loss: 6.57977489e-06
Iter: 125 loss: 6.53351435e-06
Iter: 126 loss: 6.76320269e-06
Iter: 127 loss: 6.52563767e-06
Iter: 128 loss: 6.486242e-06
Iter: 129 loss: 6.50656284e-06
Iter: 130 loss: 6.45999e-06
Iter: 131 loss: 6.41151655e-06
Iter: 132 loss: 6.51724531e-06
Iter: 133 loss: 6.39287646e-06
Iter: 134 loss: 6.35139e-06
Iter: 135 loss: 6.73169916e-06
Iter: 136 loss: 6.34933167e-06
Iter: 137 loss: 6.31130933e-06
Iter: 138 loss: 6.36994491e-06
Iter: 139 loss: 6.29350552e-06
Iter: 140 loss: 6.2571612e-06
Iter: 141 loss: 6.25255689e-06
Iter: 142 loss: 6.22661e-06
Iter: 143 loss: 6.18147669e-06
Iter: 144 loss: 6.5986651e-06
Iter: 145 loss: 6.17951e-06
Iter: 146 loss: 6.14578585e-06
Iter: 147 loss: 6.29746319e-06
Iter: 148 loss: 6.13917655e-06
Iter: 149 loss: 6.1087535e-06
Iter: 150 loss: 6.08783967e-06
Iter: 151 loss: 6.07679203e-06
Iter: 152 loss: 6.04119396e-06
Iter: 153 loss: 6.04123761e-06
Iter: 154 loss: 6.01649936e-06
Iter: 155 loss: 6.02098135e-06
Iter: 156 loss: 5.99810301e-06
Iter: 157 loss: 5.96767131e-06
Iter: 158 loss: 6.28328462e-06
Iter: 159 loss: 5.9668896e-06
Iter: 160 loss: 5.94845642e-06
Iter: 161 loss: 5.91669414e-06
Iter: 162 loss: 5.91668959e-06
Iter: 163 loss: 5.8770238e-06
Iter: 164 loss: 6.01716465e-06
Iter: 165 loss: 5.86655551e-06
Iter: 166 loss: 5.83547762e-06
Iter: 167 loss: 5.96735026e-06
Iter: 168 loss: 5.82874327e-06
Iter: 169 loss: 5.79814878e-06
Iter: 170 loss: 5.93971754e-06
Iter: 171 loss: 5.79226162e-06
Iter: 172 loss: 5.76530965e-06
Iter: 173 loss: 5.74825708e-06
Iter: 174 loss: 5.73747639e-06
Iter: 175 loss: 5.70738302e-06
Iter: 176 loss: 5.70726388e-06
Iter: 177 loss: 5.6851286e-06
Iter: 178 loss: 5.66551807e-06
Iter: 179 loss: 5.65985374e-06
Iter: 180 loss: 5.62641253e-06
Iter: 181 loss: 5.71979308e-06
Iter: 182 loss: 5.615595e-06
Iter: 183 loss: 5.5895066e-06
Iter: 184 loss: 5.95610481e-06
Iter: 185 loss: 5.58937245e-06
Iter: 186 loss: 5.56890882e-06
Iter: 187 loss: 5.55483484e-06
Iter: 188 loss: 5.5472392e-06
Iter: 189 loss: 5.52276515e-06
Iter: 190 loss: 5.79261041e-06
Iter: 191 loss: 5.5223727e-06
Iter: 192 loss: 5.49934839e-06
Iter: 193 loss: 5.52684196e-06
Iter: 194 loss: 5.48731623e-06
Iter: 195 loss: 5.46522824e-06
Iter: 196 loss: 5.54196731e-06
Iter: 197 loss: 5.45940202e-06
Iter: 198 loss: 5.4383745e-06
Iter: 199 loss: 5.45982039e-06
Iter: 200 loss: 5.42679118e-06
Iter: 201 loss: 5.40242445e-06
Iter: 202 loss: 5.41461e-06
Iter: 203 loss: 5.3859967e-06
Iter: 204 loss: 5.35679737e-06
Iter: 205 loss: 5.44454633e-06
Iter: 206 loss: 5.34789342e-06
Iter: 207 loss: 5.3227177e-06
Iter: 208 loss: 5.47306081e-06
Iter: 209 loss: 5.31962087e-06
Iter: 210 loss: 5.29696672e-06
Iter: 211 loss: 5.37996402e-06
Iter: 212 loss: 5.29102635e-06
Iter: 213 loss: 5.26799386e-06
Iter: 214 loss: 5.29376712e-06
Iter: 215 loss: 5.25569158e-06
Iter: 216 loss: 5.23481322e-06
Iter: 217 loss: 5.35454274e-06
Iter: 218 loss: 5.23201061e-06
Iter: 219 loss: 5.20965841e-06
Iter: 220 loss: 5.20919639e-06
Iter: 221 loss: 5.19164496e-06
Iter: 222 loss: 5.17011085e-06
Iter: 223 loss: 5.29228782e-06
Iter: 224 loss: 5.16732052e-06
Iter: 225 loss: 5.14763451e-06
Iter: 226 loss: 5.27300972e-06
Iter: 227 loss: 5.14549538e-06
Iter: 228 loss: 5.13045597e-06
Iter: 229 loss: 5.13690611e-06
Iter: 230 loss: 5.12023871e-06
Iter: 231 loss: 5.09829397e-06
Iter: 232 loss: 5.22087157e-06
Iter: 233 loss: 5.09517577e-06
Iter: 234 loss: 5.08152061e-06
Iter: 235 loss: 5.06257129e-06
Iter: 236 loss: 5.06163269e-06
Iter: 237 loss: 5.03893125e-06
Iter: 238 loss: 5.23888502e-06
Iter: 239 loss: 5.03780393e-06
Iter: 240 loss: 5.01797149e-06
Iter: 241 loss: 5.05216804e-06
Iter: 242 loss: 5.00917713e-06
Iter: 243 loss: 4.99176622e-06
Iter: 244 loss: 4.98804366e-06
Iter: 245 loss: 4.97655583e-06
Iter: 246 loss: 4.9536211e-06
Iter: 247 loss: 5.08281119e-06
Iter: 248 loss: 4.95066342e-06
Iter: 249 loss: 4.93079824e-06
Iter: 250 loss: 5.08807261e-06
Iter: 251 loss: 4.92938852e-06
Iter: 252 loss: 4.91401352e-06
Iter: 253 loss: 4.92722847e-06
Iter: 254 loss: 4.90481e-06
Iter: 255 loss: 4.88512705e-06
Iter: 256 loss: 4.90226921e-06
Iter: 257 loss: 4.87354737e-06
Iter: 258 loss: 4.85488954e-06
Iter: 259 loss: 5.06305514e-06
Iter: 260 loss: 4.85440341e-06
Iter: 261 loss: 4.8391862e-06
Iter: 262 loss: 4.83880922e-06
Iter: 263 loss: 4.82694577e-06
Iter: 264 loss: 4.810262e-06
Iter: 265 loss: 5.04385753e-06
Iter: 266 loss: 4.81024563e-06
Iter: 267 loss: 4.79800292e-06
Iter: 268 loss: 4.79316623e-06
Iter: 269 loss: 4.78667334e-06
Iter: 270 loss: 4.76902824e-06
Iter: 271 loss: 4.86702174e-06
Iter: 272 loss: 4.76633431e-06
Iter: 273 loss: 4.75154366e-06
Iter: 274 loss: 4.74632361e-06
Iter: 275 loss: 4.73804903e-06
Iter: 276 loss: 4.72003785e-06
Iter: 277 loss: 4.78568654e-06
Iter: 278 loss: 4.7154067e-06
Iter: 279 loss: 4.69680799e-06
Iter: 280 loss: 4.73003092e-06
Iter: 281 loss: 4.6887626e-06
Iter: 282 loss: 4.66662959e-06
Iter: 283 loss: 4.789782e-06
Iter: 284 loss: 4.66369693e-06
Iter: 285 loss: 4.64931509e-06
Iter: 286 loss: 4.6390337e-06
Iter: 287 loss: 4.63407332e-06
Iter: 288 loss: 4.61276522e-06
Iter: 289 loss: 4.79771643e-06
Iter: 290 loss: 4.61151e-06
Iter: 291 loss: 4.59683315e-06
Iter: 292 loss: 4.68558892e-06
Iter: 293 loss: 4.59483908e-06
Iter: 294 loss: 4.58086424e-06
Iter: 295 loss: 4.57494843e-06
Iter: 296 loss: 4.56758698e-06
Iter: 297 loss: 4.55154395e-06
Iter: 298 loss: 4.61608352e-06
Iter: 299 loss: 4.54803e-06
Iter: 300 loss: 4.53232678e-06
Iter: 301 loss: 4.67701557e-06
Iter: 302 loss: 4.53183566e-06
Iter: 303 loss: 4.52122367e-06
Iter: 304 loss: 4.52599488e-06
Iter: 305 loss: 4.51425922e-06
Iter: 306 loss: 4.49900881e-06
Iter: 307 loss: 4.54456494e-06
Iter: 308 loss: 4.49442041e-06
Iter: 309 loss: 4.48038963e-06
Iter: 310 loss: 4.48079936e-06
Iter: 311 loss: 4.46919103e-06
Iter: 312 loss: 4.45212072e-06
Iter: 313 loss: 4.56202542e-06
Iter: 314 loss: 4.45028309e-06
Iter: 315 loss: 4.43428416e-06
Iter: 316 loss: 4.4375588e-06
Iter: 317 loss: 4.42247938e-06
Iter: 318 loss: 4.40596159e-06
Iter: 319 loss: 4.44544094e-06
Iter: 320 loss: 4.3999089e-06
Iter: 321 loss: 4.38319239e-06
Iter: 322 loss: 4.47406728e-06
Iter: 323 loss: 4.38068855e-06
Iter: 324 loss: 4.36422215e-06
Iter: 325 loss: 4.41019e-06
Iter: 326 loss: 4.3588243e-06
Iter: 327 loss: 4.34551566e-06
Iter: 328 loss: 4.36337541e-06
Iter: 329 loss: 4.3386226e-06
Iter: 330 loss: 4.32487514e-06
Iter: 331 loss: 4.40572603e-06
Iter: 332 loss: 4.32298566e-06
Iter: 333 loss: 4.30738328e-06
Iter: 334 loss: 4.32123534e-06
Iter: 335 loss: 4.29818374e-06
Iter: 336 loss: 4.28558315e-06
Iter: 337 loss: 4.33515925e-06
Iter: 338 loss: 4.28272961e-06
Iter: 339 loss: 4.26902579e-06
Iter: 340 loss: 4.34999401e-06
Iter: 341 loss: 4.2671827e-06
Iter: 342 loss: 4.25654298e-06
Iter: 343 loss: 4.25474718e-06
Iter: 344 loss: 4.2475458e-06
Iter: 345 loss: 4.23555321e-06
Iter: 346 loss: 4.31965464e-06
Iter: 347 loss: 4.2344418e-06
Iter: 348 loss: 4.22304856e-06
Iter: 349 loss: 4.21874029e-06
Iter: 350 loss: 4.21252662e-06
Iter: 351 loss: 4.19693561e-06
Iter: 352 loss: 4.24706195e-06
Iter: 353 loss: 4.19271237e-06
Iter: 354 loss: 4.17921774e-06
Iter: 355 loss: 4.21217919e-06
Iter: 356 loss: 4.1745925e-06
Iter: 357 loss: 4.15935619e-06
Iter: 358 loss: 4.23109532e-06
Iter: 359 loss: 4.15666727e-06
Iter: 360 loss: 4.14547958e-06
Iter: 361 loss: 4.14567648e-06
Iter: 362 loss: 4.13649695e-06
Iter: 363 loss: 4.12130839e-06
Iter: 364 loss: 4.18029185e-06
Iter: 365 loss: 4.11773772e-06
Iter: 366 loss: 4.10215944e-06
Iter: 367 loss: 4.19095522e-06
Iter: 368 loss: 4.10004259e-06
Iter: 369 loss: 4.08925189e-06
Iter: 370 loss: 4.11158271e-06
Iter: 371 loss: 4.0847367e-06
Iter: 372 loss: 4.07420521e-06
Iter: 373 loss: 4.11510609e-06
Iter: 374 loss: 4.07170592e-06
Iter: 375 loss: 4.05893206e-06
Iter: 376 loss: 4.0783e-06
Iter: 377 loss: 4.05296669e-06
Iter: 378 loss: 4.04297953e-06
Iter: 379 loss: 4.14547048e-06
Iter: 380 loss: 4.04258481e-06
Iter: 381 loss: 4.03426e-06
Iter: 382 loss: 4.02432488e-06
Iter: 383 loss: 4.02324486e-06
Iter: 384 loss: 4.01119632e-06
Iter: 385 loss: 4.03651165e-06
Iter: 386 loss: 4.00650151e-06
Iter: 387 loss: 3.9919687e-06
Iter: 388 loss: 4.09421682e-06
Iter: 389 loss: 3.99060218e-06
Iter: 390 loss: 3.98124121e-06
Iter: 391 loss: 3.97911299e-06
Iter: 392 loss: 3.97306e-06
Iter: 393 loss: 3.95911502e-06
Iter: 394 loss: 3.98252723e-06
Iter: 395 loss: 3.95286588e-06
Iter: 396 loss: 3.93847131e-06
Iter: 397 loss: 3.99863802e-06
Iter: 398 loss: 3.93539085e-06
Iter: 399 loss: 3.92243783e-06
Iter: 400 loss: 4.01020043e-06
Iter: 401 loss: 3.92123729e-06
Iter: 402 loss: 3.9118222e-06
Iter: 403 loss: 3.91176945e-06
Iter: 404 loss: 3.90424248e-06
Iter: 405 loss: 3.89185698e-06
Iter: 406 loss: 3.95221e-06
Iter: 407 loss: 3.88966e-06
Iter: 408 loss: 3.87900536e-06
Iter: 409 loss: 3.95055304e-06
Iter: 410 loss: 3.87795717e-06
Iter: 411 loss: 3.86965485e-06
Iter: 412 loss: 3.87344335e-06
Iter: 413 loss: 3.86417651e-06
Iter: 414 loss: 3.85337489e-06
Iter: 415 loss: 3.94585686e-06
Iter: 416 loss: 3.85285648e-06
Iter: 417 loss: 3.84600253e-06
Iter: 418 loss: 3.84187933e-06
Iter: 419 loss: 3.83912266e-06
Iter: 420 loss: 3.82711278e-06
Iter: 421 loss: 3.88005947e-06
Iter: 422 loss: 3.8247781e-06
Iter: 423 loss: 3.81669906e-06
Iter: 424 loss: 3.81860536e-06
Iter: 425 loss: 3.81072391e-06
Iter: 426 loss: 3.79774428e-06
Iter: 427 loss: 3.8338153e-06
Iter: 428 loss: 3.79355924e-06
Iter: 429 loss: 3.78208688e-06
Iter: 430 loss: 3.86665397e-06
Iter: 431 loss: 3.7810205e-06
Iter: 432 loss: 3.7726777e-06
Iter: 433 loss: 3.76616754e-06
Iter: 434 loss: 3.76347862e-06
Iter: 435 loss: 3.74987644e-06
Iter: 436 loss: 3.76378966e-06
Iter: 437 loss: 3.7421064e-06
Iter: 438 loss: 3.73005241e-06
Iter: 439 loss: 3.82303551e-06
Iter: 440 loss: 3.72924615e-06
Iter: 441 loss: 3.71677743e-06
Iter: 442 loss: 3.75828608e-06
Iter: 443 loss: 3.71338638e-06
Iter: 444 loss: 3.70225598e-06
Iter: 445 loss: 3.73085913e-06
Iter: 446 loss: 3.69832765e-06
Iter: 447 loss: 3.68960082e-06
Iter: 448 loss: 3.75764171e-06
Iter: 449 loss: 3.68900919e-06
Iter: 450 loss: 3.68028759e-06
Iter: 451 loss: 3.68600627e-06
Iter: 452 loss: 3.67469738e-06
Iter: 453 loss: 3.66564268e-06
Iter: 454 loss: 3.715807e-06
Iter: 455 loss: 3.66417589e-06
Iter: 456 loss: 3.6554602e-06
Iter: 457 loss: 3.65549522e-06
Iter: 458 loss: 3.64850121e-06
Iter: 459 loss: 3.63963682e-06
Iter: 460 loss: 3.65827145e-06
Iter: 461 loss: 3.63609888e-06
Iter: 462 loss: 3.62445712e-06
Iter: 463 loss: 3.66985387e-06
Iter: 464 loss: 3.62185187e-06
Iter: 465 loss: 3.61277353e-06
Iter: 466 loss: 3.62626747e-06
Iter: 467 loss: 3.60829335e-06
Iter: 468 loss: 3.59891e-06
Iter: 469 loss: 3.65247524e-06
Iter: 470 loss: 3.59764408e-06
Iter: 471 loss: 3.58877514e-06
Iter: 472 loss: 3.59072283e-06
Iter: 473 loss: 3.58203306e-06
Iter: 474 loss: 3.57133604e-06
Iter: 475 loss: 3.58806551e-06
Iter: 476 loss: 3.56615737e-06
Iter: 477 loss: 3.55459338e-06
Iter: 478 loss: 3.58333637e-06
Iter: 479 loss: 3.55070847e-06
Iter: 480 loss: 3.53752671e-06
Iter: 481 loss: 3.57496378e-06
Iter: 482 loss: 3.53333417e-06
Iter: 483 loss: 3.52342158e-06
Iter: 484 loss: 3.52339066e-06
Iter: 485 loss: 3.5168041e-06
Iter: 486 loss: 3.52271513e-06
Iter: 487 loss: 3.51299309e-06
Iter: 488 loss: 3.50426762e-06
Iter: 489 loss: 3.53364385e-06
Iter: 490 loss: 3.50181e-06
Iter: 491 loss: 3.49367474e-06
Iter: 492 loss: 3.49485276e-06
Iter: 493 loss: 3.48750473e-06
Iter: 494 loss: 3.47837317e-06
Iter: 495 loss: 3.54019835e-06
Iter: 496 loss: 3.47743662e-06
Iter: 497 loss: 3.46943921e-06
Iter: 498 loss: 3.47187733e-06
Iter: 499 loss: 3.46388e-06
Iter: 500 loss: 3.4530326e-06
Iter: 501 loss: 3.46767e-06
Iter: 502 loss: 3.44773298e-06
Iter: 503 loss: 3.43844567e-06
Iter: 504 loss: 3.55799193e-06
Iter: 505 loss: 3.43837291e-06
Iter: 506 loss: 3.43079728e-06
Iter: 507 loss: 3.42540125e-06
Iter: 508 loss: 3.42279e-06
Iter: 509 loss: 3.41292161e-06
Iter: 510 loss: 3.45382114e-06
Iter: 511 loss: 3.4107702e-06
Iter: 512 loss: 3.40027304e-06
Iter: 513 loss: 3.4375887e-06
Iter: 514 loss: 3.39745247e-06
Iter: 515 loss: 3.38871132e-06
Iter: 516 loss: 3.39968756e-06
Iter: 517 loss: 3.38419954e-06
Iter: 518 loss: 3.37450069e-06
Iter: 519 loss: 3.39131611e-06
Iter: 520 loss: 3.37023539e-06
Iter: 521 loss: 3.36264065e-06
Iter: 522 loss: 3.36262815e-06
Iter: 523 loss: 3.35575214e-06
Iter: 524 loss: 3.35555978e-06
Iter: 525 loss: 3.35020331e-06
Iter: 526 loss: 3.34164633e-06
Iter: 527 loss: 3.36444828e-06
Iter: 528 loss: 3.33890102e-06
Iter: 529 loss: 3.32960053e-06
Iter: 530 loss: 3.36274297e-06
Iter: 531 loss: 3.32730156e-06
Iter: 532 loss: 3.32007448e-06
Iter: 533 loss: 3.31509409e-06
Iter: 534 loss: 3.31248521e-06
Iter: 535 loss: 3.30138892e-06
Iter: 536 loss: 3.40094653e-06
Iter: 537 loss: 3.30091916e-06
Iter: 538 loss: 3.29204408e-06
Iter: 539 loss: 3.30169041e-06
Iter: 540 loss: 3.2873e-06
Iter: 541 loss: 3.28036504e-06
Iter: 542 loss: 3.30891e-06
Iter: 543 loss: 3.27873772e-06
Iter: 544 loss: 3.27005614e-06
Iter: 545 loss: 3.28303054e-06
Iter: 546 loss: 3.26586769e-06
Iter: 547 loss: 3.25788e-06
Iter: 548 loss: 3.27912085e-06
Iter: 549 loss: 3.25520523e-06
Iter: 550 loss: 3.24694315e-06
Iter: 551 loss: 3.24417988e-06
Iter: 552 loss: 3.23934228e-06
Iter: 553 loss: 3.22974847e-06
Iter: 554 loss: 3.22976803e-06
Iter: 555 loss: 3.22313076e-06
Iter: 556 loss: 3.22393066e-06
Iter: 557 loss: 3.21810558e-06
Iter: 558 loss: 3.21049197e-06
Iter: 559 loss: 3.30644e-06
Iter: 560 loss: 3.21040397e-06
Iter: 561 loss: 3.20433219e-06
Iter: 562 loss: 3.20535082e-06
Iter: 563 loss: 3.19961782e-06
Iter: 564 loss: 3.19217816e-06
Iter: 565 loss: 3.19673791e-06
Iter: 566 loss: 3.18736238e-06
Iter: 567 loss: 3.17944409e-06
Iter: 568 loss: 3.24661687e-06
Iter: 569 loss: 3.17912941e-06
Iter: 570 loss: 3.17170907e-06
Iter: 571 loss: 3.17277636e-06
Iter: 572 loss: 3.1659506e-06
Iter: 573 loss: 3.15764e-06
Iter: 574 loss: 3.1612758e-06
Iter: 575 loss: 3.15193347e-06
Iter: 576 loss: 3.14446788e-06
Iter: 577 loss: 3.14447675e-06
Iter: 578 loss: 3.1388563e-06
Iter: 579 loss: 3.1338127e-06
Iter: 580 loss: 3.13246073e-06
Iter: 581 loss: 3.12349584e-06
Iter: 582 loss: 3.15878106e-06
Iter: 583 loss: 3.12138832e-06
Iter: 584 loss: 3.11300573e-06
Iter: 585 loss: 3.16038359e-06
Iter: 586 loss: 3.11185318e-06
Iter: 587 loss: 3.10541236e-06
Iter: 588 loss: 3.10016298e-06
Iter: 589 loss: 3.09834149e-06
Iter: 590 loss: 3.08904214e-06
Iter: 591 loss: 3.12839e-06
Iter: 592 loss: 3.08703261e-06
Iter: 593 loss: 3.0790452e-06
Iter: 594 loss: 3.19257697e-06
Iter: 595 loss: 3.07903247e-06
Iter: 596 loss: 3.07399455e-06
Iter: 597 loss: 3.08097083e-06
Iter: 598 loss: 3.07148139e-06
Iter: 599 loss: 3.06597167e-06
Iter: 600 loss: 3.07006258e-06
Iter: 601 loss: 3.06264656e-06
Iter: 602 loss: 3.05458343e-06
Iter: 603 loss: 3.06029983e-06
Iter: 604 loss: 3.04953551e-06
Iter: 605 loss: 3.04257037e-06
Iter: 606 loss: 3.11162921e-06
Iter: 607 loss: 3.04237665e-06
Iter: 608 loss: 3.03634079e-06
Iter: 609 loss: 3.04203763e-06
Iter: 610 loss: 3.03294564e-06
Iter: 611 loss: 3.02579497e-06
Iter: 612 loss: 3.03196339e-06
Iter: 613 loss: 3.0217268e-06
Iter: 614 loss: 3.01398404e-06
Iter: 615 loss: 3.03640172e-06
Iter: 616 loss: 3.01160367e-06
Iter: 617 loss: 3.0034721e-06
Iter: 618 loss: 3.05937783e-06
Iter: 619 loss: 3.0026797e-06
Iter: 620 loss: 2.99595536e-06
Iter: 621 loss: 2.99570684e-06
Iter: 622 loss: 2.99050907e-06
Iter: 623 loss: 2.9825469e-06
Iter: 624 loss: 3.00862212e-06
Iter: 625 loss: 2.98031819e-06
Iter: 626 loss: 2.97255156e-06
Iter: 627 loss: 3.01528689e-06
Iter: 628 loss: 2.97151e-06
Iter: 629 loss: 2.96444864e-06
Iter: 630 loss: 2.97351926e-06
Iter: 631 loss: 2.96083e-06
Iter: 632 loss: 2.95584573e-06
Iter: 633 loss: 2.95585e-06
Iter: 634 loss: 2.95128029e-06
Iter: 635 loss: 2.94357687e-06
Iter: 636 loss: 2.94355095e-06
Iter: 637 loss: 2.93519179e-06
Iter: 638 loss: 2.96232724e-06
Iter: 639 loss: 2.93273388e-06
Iter: 640 loss: 2.92539062e-06
Iter: 641 loss: 2.95839436e-06
Iter: 642 loss: 2.92395e-06
Iter: 643 loss: 2.91646484e-06
Iter: 644 loss: 2.94540928e-06
Iter: 645 loss: 2.91483229e-06
Iter: 646 loss: 2.90875232e-06
Iter: 647 loss: 2.9078833e-06
Iter: 648 loss: 2.90369735e-06
Iter: 649 loss: 2.89631453e-06
Iter: 650 loss: 2.97486849e-06
Iter: 651 loss: 2.89609898e-06
Iter: 652 loss: 2.89077025e-06
Iter: 653 loss: 2.88529645e-06
Iter: 654 loss: 2.88429646e-06
Iter: 655 loss: 2.87662851e-06
Iter: 656 loss: 2.94998677e-06
Iter: 657 loss: 2.87635567e-06
Iter: 658 loss: 2.87012131e-06
Iter: 659 loss: 2.88913725e-06
Iter: 660 loss: 2.86813338e-06
Iter: 661 loss: 2.86214913e-06
Iter: 662 loss: 2.8637487e-06
Iter: 663 loss: 2.85788747e-06
Iter: 664 loss: 2.85052965e-06
Iter: 665 loss: 2.86337854e-06
Iter: 666 loss: 2.8472864e-06
Iter: 667 loss: 2.84070165e-06
Iter: 668 loss: 2.84061e-06
Iter: 669 loss: 2.83615896e-06
Iter: 670 loss: 2.83709278e-06
Iter: 671 loss: 2.83288591e-06
Iter: 672 loss: 2.82620499e-06
Iter: 673 loss: 2.8418192e-06
Iter: 674 loss: 2.82390988e-06
Iter: 675 loss: 2.81769894e-06
Iter: 676 loss: 2.815671e-06
Iter: 677 loss: 2.8121508e-06
Iter: 678 loss: 2.80428185e-06
Iter: 679 loss: 2.83884674e-06
Iter: 680 loss: 2.80269842e-06
Iter: 681 loss: 2.79634605e-06
Iter: 682 loss: 2.8575032e-06
Iter: 683 loss: 2.79609276e-06
Iter: 684 loss: 2.79066671e-06
Iter: 685 loss: 2.78907919e-06
Iter: 686 loss: 2.78573589e-06
Iter: 687 loss: 2.77884533e-06
Iter: 688 loss: 2.80261065e-06
Iter: 689 loss: 2.77709159e-06
Iter: 690 loss: 2.77060349e-06
Iter: 691 loss: 2.78361154e-06
Iter: 692 loss: 2.7680826e-06
Iter: 693 loss: 2.76072365e-06
Iter: 694 loss: 2.80584709e-06
Iter: 695 loss: 2.75972889e-06
Iter: 696 loss: 2.75471075e-06
Iter: 697 loss: 2.75614366e-06
Iter: 698 loss: 2.75094249e-06
Iter: 699 loss: 2.74493141e-06
Iter: 700 loss: 2.79643336e-06
Iter: 701 loss: 2.74469221e-06
Iter: 702 loss: 2.73949354e-06
Iter: 703 loss: 2.73612841e-06
Iter: 704 loss: 2.7341523e-06
Iter: 705 loss: 2.73046226e-06
Iter: 706 loss: 2.72985403e-06
Iter: 707 loss: 2.72645957e-06
Iter: 708 loss: 2.71935187e-06
Iter: 709 loss: 2.844336e-06
Iter: 710 loss: 2.71920294e-06
Iter: 711 loss: 2.71254521e-06
Iter: 712 loss: 2.75019647e-06
Iter: 713 loss: 2.71157205e-06
Iter: 714 loss: 2.70568967e-06
Iter: 715 loss: 2.72919806e-06
Iter: 716 loss: 2.7043443e-06
Iter: 717 loss: 2.69841848e-06
Iter: 718 loss: 2.70094711e-06
Iter: 719 loss: 2.69435714e-06
Iter: 720 loss: 2.6873804e-06
Iter: 721 loss: 2.70063106e-06
Iter: 722 loss: 2.68441954e-06
Iter: 723 loss: 2.67782025e-06
Iter: 724 loss: 2.7521146e-06
Iter: 725 loss: 2.67770565e-06
Iter: 726 loss: 2.67213181e-06
Iter: 727 loss: 2.67213704e-06
Iter: 728 loss: 2.66751249e-06
Iter: 729 loss: 2.66161396e-06
Iter: 730 loss: 2.67313021e-06
Iter: 731 loss: 2.65904782e-06
Iter: 732 loss: 2.65216431e-06
Iter: 733 loss: 2.66989809e-06
Iter: 734 loss: 2.64986534e-06
Iter: 735 loss: 2.64404889e-06
Iter: 736 loss: 2.71285035e-06
Iter: 737 loss: 2.64395726e-06
Iter: 738 loss: 2.63940456e-06
Iter: 739 loss: 2.63596257e-06
Iter: 740 loss: 2.63436891e-06
Iter: 741 loss: 2.6284938e-06
Iter: 742 loss: 2.68639405e-06
Iter: 743 loss: 2.62831873e-06
Iter: 744 loss: 2.6221926e-06
Iter: 745 loss: 2.62950971e-06
Iter: 746 loss: 2.61903915e-06
Iter: 747 loss: 2.61407672e-06
Iter: 748 loss: 2.61531841e-06
Iter: 749 loss: 2.61052105e-06
Iter: 750 loss: 2.60464844e-06
Iter: 751 loss: 2.62656977e-06
Iter: 752 loss: 2.60317825e-06
Iter: 753 loss: 2.59665057e-06
Iter: 754 loss: 2.61552168e-06
Iter: 755 loss: 2.59458398e-06
Iter: 756 loss: 2.58904129e-06
Iter: 757 loss: 2.60546426e-06
Iter: 758 loss: 2.58743694e-06
Iter: 759 loss: 2.58247519e-06
Iter: 760 loss: 2.59128683e-06
Iter: 761 loss: 2.58020236e-06
Iter: 762 loss: 2.57437682e-06
Iter: 763 loss: 2.61481364e-06
Iter: 764 loss: 2.57384772e-06
Iter: 765 loss: 2.56951216e-06
Iter: 766 loss: 2.5650329e-06
Iter: 767 loss: 2.564233e-06
Iter: 768 loss: 2.55806026e-06
Iter: 769 loss: 2.61805872e-06
Iter: 770 loss: 2.55787791e-06
Iter: 771 loss: 2.55233772e-06
Iter: 772 loss: 2.55559644e-06
Iter: 773 loss: 2.54884435e-06
Iter: 774 loss: 2.54277984e-06
Iter: 775 loss: 2.56286921e-06
Iter: 776 loss: 2.54108613e-06
Iter: 777 loss: 2.53622102e-06
Iter: 778 loss: 2.58666273e-06
Iter: 779 loss: 2.53603275e-06
Iter: 780 loss: 2.53211601e-06
Iter: 781 loss: 2.53682128e-06
Iter: 782 loss: 2.52989e-06
Iter: 783 loss: 2.52624523e-06
Iter: 784 loss: 2.54762472e-06
Iter: 785 loss: 2.52584732e-06
Iter: 786 loss: 2.52201698e-06
Iter: 787 loss: 2.51478082e-06
Iter: 788 loss: 2.66822963e-06
Iter: 789 loss: 2.5147815e-06
Iter: 790 loss: 2.50824223e-06
Iter: 791 loss: 2.54007455e-06
Iter: 792 loss: 2.50704898e-06
Iter: 793 loss: 2.50094172e-06
Iter: 794 loss: 2.51518145e-06
Iter: 795 loss: 2.49861023e-06
Iter: 796 loss: 2.49292225e-06
Iter: 797 loss: 2.53668259e-06
Iter: 798 loss: 2.49239406e-06
Iter: 799 loss: 2.48746801e-06
Iter: 800 loss: 2.50480502e-06
Iter: 801 loss: 2.48614879e-06
Iter: 802 loss: 2.4819426e-06
Iter: 803 loss: 2.4832641e-06
Iter: 804 loss: 2.47888647e-06
Iter: 805 loss: 2.47381058e-06
Iter: 806 loss: 2.5123054e-06
Iter: 807 loss: 2.47342518e-06
Iter: 808 loss: 2.46935292e-06
Iter: 809 loss: 2.47138951e-06
Iter: 810 loss: 2.46659488e-06
Iter: 811 loss: 2.46103696e-06
Iter: 812 loss: 2.46373384e-06
Iter: 813 loss: 2.45735373e-06
Iter: 814 loss: 2.45343472e-06
Iter: 815 loss: 2.45334058e-06
Iter: 816 loss: 2.44964804e-06
Iter: 817 loss: 2.44953253e-06
Iter: 818 loss: 2.44669945e-06
Iter: 819 loss: 2.44170815e-06
Iter: 820 loss: 2.46236232e-06
Iter: 821 loss: 2.44062221e-06
Iter: 822 loss: 2.43598106e-06
Iter: 823 loss: 2.44184821e-06
Iter: 824 loss: 2.43362388e-06
Iter: 825 loss: 2.42893884e-06
Iter: 826 loss: 2.43227964e-06
Iter: 827 loss: 2.42617261e-06
Iter: 828 loss: 2.42096053e-06
Iter: 829 loss: 2.46294439e-06
Iter: 830 loss: 2.42064516e-06
Iter: 831 loss: 2.4161352e-06
Iter: 832 loss: 2.4114911e-06
Iter: 833 loss: 2.41059388e-06
Iter: 834 loss: 2.40410077e-06
Iter: 835 loss: 2.43572367e-06
Iter: 836 loss: 2.40301551e-06
Iter: 837 loss: 2.39844735e-06
Iter: 838 loss: 2.44998159e-06
Iter: 839 loss: 2.39830615e-06
Iter: 840 loss: 2.39407677e-06
Iter: 841 loss: 2.39356768e-06
Iter: 842 loss: 2.39060387e-06
Iter: 843 loss: 2.38457915e-06
Iter: 844 loss: 2.39763676e-06
Iter: 845 loss: 2.3823286e-06
Iter: 846 loss: 2.3772825e-06
Iter: 847 loss: 2.39436e-06
Iter: 848 loss: 2.37587983e-06
Iter: 849 loss: 2.37043605e-06
Iter: 850 loss: 2.39856195e-06
Iter: 851 loss: 2.36958772e-06
Iter: 852 loss: 2.36515643e-06
Iter: 853 loss: 2.37319546e-06
Iter: 854 loss: 2.36319511e-06
Iter: 855 loss: 2.35737889e-06
Iter: 856 loss: 2.38284792e-06
Iter: 857 loss: 2.35628136e-06
Iter: 858 loss: 2.3527e-06
Iter: 859 loss: 2.35266771e-06
Iter: 860 loss: 2.34978734e-06
Iter: 861 loss: 2.34564277e-06
Iter: 862 loss: 2.37946642e-06
Iter: 863 loss: 2.34535401e-06
Iter: 864 loss: 2.3415771e-06
Iter: 865 loss: 2.33977926e-06
Iter: 866 loss: 2.33797823e-06
Iter: 867 loss: 2.33328592e-06
Iter: 868 loss: 2.34737581e-06
Iter: 869 loss: 2.3319094e-06
Iter: 870 loss: 2.32682987e-06
Iter: 871 loss: 2.33470246e-06
Iter: 872 loss: 2.32447292e-06
Iter: 873 loss: 2.31959e-06
Iter: 874 loss: 2.36481446e-06
Iter: 875 loss: 2.31940248e-06
Iter: 876 loss: 2.31524109e-06
Iter: 877 loss: 2.31661716e-06
Iter: 878 loss: 2.31220884e-06
Iter: 879 loss: 2.30712044e-06
Iter: 880 loss: 2.33341279e-06
Iter: 881 loss: 2.30627961e-06
Iter: 882 loss: 2.30146225e-06
Iter: 883 loss: 2.31242598e-06
Iter: 884 loss: 2.299578e-06
Iter: 885 loss: 2.2953368e-06
Iter: 886 loss: 2.29476927e-06
Iter: 887 loss: 2.29182479e-06
Iter: 888 loss: 2.28740942e-06
Iter: 889 loss: 2.35549533e-06
Iter: 890 loss: 2.28745e-06
Iter: 891 loss: 2.28318049e-06
Iter: 892 loss: 2.28819954e-06
Iter: 893 loss: 2.28089334e-06
Iter: 894 loss: 2.27699365e-06
Iter: 895 loss: 2.2816198e-06
Iter: 896 loss: 2.27491728e-06
Iter: 897 loss: 2.27048736e-06
Iter: 898 loss: 2.29452166e-06
Iter: 899 loss: 2.26990983e-06
Iter: 900 loss: 2.26588281e-06
Iter: 901 loss: 2.26274233e-06
Iter: 902 loss: 2.26146585e-06
Iter: 903 loss: 2.25550593e-06
Iter: 904 loss: 2.2953368e-06
Iter: 905 loss: 2.25486701e-06
Iter: 906 loss: 2.25053395e-06
Iter: 907 loss: 2.27360738e-06
Iter: 908 loss: 2.24992755e-06
Iter: 909 loss: 2.24606674e-06
Iter: 910 loss: 2.24043492e-06
Iter: 911 loss: 2.24023188e-06
Iter: 912 loss: 2.23364941e-06
Iter: 913 loss: 2.27689225e-06
Iter: 914 loss: 2.23279449e-06
Iter: 915 loss: 2.22852054e-06
Iter: 916 loss: 2.27365285e-06
Iter: 917 loss: 2.2283366e-06
Iter: 918 loss: 2.22468952e-06
Iter: 919 loss: 2.22888229e-06
Iter: 920 loss: 2.22262224e-06
Iter: 921 loss: 2.2182287e-06
Iter: 922 loss: 2.21916207e-06
Iter: 923 loss: 2.21493156e-06
Iter: 924 loss: 2.21052687e-06
Iter: 925 loss: 2.24908626e-06
Iter: 926 loss: 2.21033088e-06
Iter: 927 loss: 2.20544803e-06
Iter: 928 loss: 2.2127781e-06
Iter: 929 loss: 2.20302741e-06
Iter: 930 loss: 2.19928393e-06
Iter: 931 loss: 2.21945538e-06
Iter: 932 loss: 2.19873755e-06
Iter: 933 loss: 2.19536082e-06
Iter: 934 loss: 2.19248841e-06
Iter: 935 loss: 2.1914841e-06
Iter: 936 loss: 2.18643186e-06
Iter: 937 loss: 2.21309097e-06
Iter: 938 loss: 2.18550508e-06
Iter: 939 loss: 2.18181549e-06
Iter: 940 loss: 2.20634274e-06
Iter: 941 loss: 2.18129298e-06
Iter: 942 loss: 2.17811203e-06
Iter: 943 loss: 2.17889715e-06
Iter: 944 loss: 2.17576599e-06
Iter: 945 loss: 2.1713513e-06
Iter: 946 loss: 2.1719402e-06
Iter: 947 loss: 2.16795252e-06
Iter: 948 loss: 2.16358876e-06
Iter: 949 loss: 2.16356102e-06
Iter: 950 loss: 2.16009084e-06
Iter: 951 loss: 2.158288e-06
Iter: 952 loss: 2.15664522e-06
Iter: 953 loss: 2.15096225e-06
Iter: 954 loss: 2.16942431e-06
Iter: 955 loss: 2.14942361e-06
Iter: 956 loss: 2.14544593e-06
Iter: 957 loss: 2.18834498e-06
Iter: 958 loss: 2.14538431e-06
Iter: 959 loss: 2.14203487e-06
Iter: 960 loss: 2.14234865e-06
Iter: 961 loss: 2.13951353e-06
Iter: 962 loss: 2.13604403e-06
Iter: 963 loss: 2.15231785e-06
Iter: 964 loss: 2.13535986e-06
Iter: 965 loss: 2.13103499e-06
Iter: 966 loss: 2.1412875e-06
Iter: 967 loss: 2.12942223e-06
Iter: 968 loss: 2.12666464e-06
Iter: 969 loss: 2.12325e-06
Iter: 970 loss: 2.12296527e-06
Iter: 971 loss: 2.11785027e-06
Iter: 972 loss: 2.15038153e-06
Iter: 973 loss: 2.11726206e-06
Iter: 974 loss: 2.11276051e-06
Iter: 975 loss: 2.13193107e-06
Iter: 976 loss: 2.11194333e-06
Iter: 977 loss: 2.10807184e-06
Iter: 978 loss: 2.11342694e-06
Iter: 979 loss: 2.10624103e-06
Iter: 980 loss: 2.10233293e-06
Iter: 981 loss: 2.10946882e-06
Iter: 982 loss: 2.10065173e-06
Iter: 983 loss: 2.09626228e-06
Iter: 984 loss: 2.12210216e-06
Iter: 985 loss: 2.09574e-06
Iter: 986 loss: 2.09236873e-06
Iter: 987 loss: 2.09160271e-06
Iter: 988 loss: 2.08940855e-06
Iter: 989 loss: 2.08477491e-06
Iter: 990 loss: 2.10210237e-06
Iter: 991 loss: 2.08356596e-06
Iter: 992 loss: 2.07889138e-06
Iter: 993 loss: 2.10836242e-06
Iter: 994 loss: 2.07834773e-06
Iter: 995 loss: 2.07420794e-06
Iter: 996 loss: 2.07415269e-06
Iter: 997 loss: 2.07082485e-06
Iter: 998 loss: 2.06745426e-06
Iter: 999 loss: 2.06742334e-06
Iter: 1000 loss: 2.06491222e-06
Iter: 1001 loss: 2.06482855e-06
Iter: 1002 loss: 2.06300615e-06
Iter: 1003 loss: 2.05887636e-06
Iter: 1004 loss: 2.07182597e-06
Iter: 1005 loss: 2.05767151e-06
Iter: 1006 loss: 2.05453557e-06
Iter: 1007 loss: 2.0589905e-06
Iter: 1008 loss: 2.05311e-06
Iter: 1009 loss: 2.04972434e-06
Iter: 1010 loss: 2.04801199e-06
Iter: 1011 loss: 2.04644e-06
Iter: 1012 loss: 2.04128628e-06
Iter: 1013 loss: 2.06350114e-06
Iter: 1014 loss: 2.04013304e-06
Iter: 1015 loss: 2.03624336e-06
Iter: 1016 loss: 2.0944467e-06
Iter: 1017 loss: 2.03619152e-06
Iter: 1018 loss: 2.0336206e-06
Iter: 1019 loss: 2.03031095e-06
Iter: 1020 loss: 2.02998217e-06
Iter: 1021 loss: 2.02513684e-06
Iter: 1022 loss: 2.04431103e-06
Iter: 1023 loss: 2.02407159e-06
Iter: 1024 loss: 2.02027286e-06
Iter: 1025 loss: 2.02752426e-06
Iter: 1026 loss: 2.01875355e-06
Iter: 1027 loss: 2.01476519e-06
Iter: 1028 loss: 2.05502693e-06
Iter: 1029 loss: 2.01466355e-06
Iter: 1030 loss: 2.01140324e-06
Iter: 1031 loss: 2.00993804e-06
Iter: 1032 loss: 2.00833256e-06
Iter: 1033 loss: 2.00489058e-06
Iter: 1034 loss: 2.05668039e-06
Iter: 1035 loss: 2.0048592e-06
Iter: 1036 loss: 2.00194222e-06
Iter: 1037 loss: 2.00057207e-06
Iter: 1038 loss: 1.9991553e-06
Iter: 1039 loss: 1.99517467e-06
Iter: 1040 loss: 2.03137597e-06
Iter: 1041 loss: 1.99498936e-06
Iter: 1042 loss: 1.99237e-06
Iter: 1043 loss: 1.99053056e-06
Iter: 1044 loss: 1.98965063e-06
Iter: 1045 loss: 1.98544444e-06
Iter: 1046 loss: 1.99059014e-06
Iter: 1047 loss: 1.98325733e-06
Iter: 1048 loss: 1.97881536e-06
Iter: 1049 loss: 2.0052621e-06
Iter: 1050 loss: 1.97828331e-06
Iter: 1051 loss: 1.97489317e-06
Iter: 1052 loss: 1.99309488e-06
Iter: 1053 loss: 1.97430609e-06
Iter: 1054 loss: 1.97136819e-06
Iter: 1055 loss: 1.97119516e-06
Iter: 1056 loss: 1.9689387e-06
Iter: 1057 loss: 1.96485735e-06
Iter: 1058 loss: 1.99168653e-06
Iter: 1059 loss: 1.96448036e-06
Iter: 1060 loss: 1.96074825e-06
Iter: 1061 loss: 1.96585552e-06
Iter: 1062 loss: 1.95890584e-06
Iter: 1063 loss: 1.95521329e-06
Iter: 1064 loss: 1.95470557e-06
Iter: 1065 loss: 1.95205257e-06
Iter: 1066 loss: 1.94777067e-06
Iter: 1067 loss: 1.98871658e-06
Iter: 1068 loss: 1.94762288e-06
Iter: 1069 loss: 1.94391328e-06
Iter: 1070 loss: 1.96277051e-06
Iter: 1071 loss: 1.94336e-06
Iter: 1072 loss: 1.9402105e-06
Iter: 1073 loss: 1.94378663e-06
Iter: 1074 loss: 1.93861979e-06
Iter: 1075 loss: 1.93537517e-06
Iter: 1076 loss: 1.96389374e-06
Iter: 1077 loss: 1.93525193e-06
Iter: 1078 loss: 1.93310052e-06
Iter: 1079 loss: 1.93059509e-06
Iter: 1080 loss: 1.93033065e-06
Iter: 1081 loss: 1.92656489e-06
Iter: 1082 loss: 1.94949189e-06
Iter: 1083 loss: 1.9261322e-06
Iter: 1084 loss: 1.92278912e-06
Iter: 1085 loss: 1.9248323e-06
Iter: 1086 loss: 1.92072866e-06
Iter: 1087 loss: 1.91703975e-06
Iter: 1088 loss: 1.9214e-06
Iter: 1089 loss: 1.91507752e-06
Iter: 1090 loss: 1.91125491e-06
Iter: 1091 loss: 1.93723372e-06
Iter: 1092 loss: 1.91081836e-06
Iter: 1093 loss: 1.90751598e-06
Iter: 1094 loss: 1.91988602e-06
Iter: 1095 loss: 1.90674245e-06
Iter: 1096 loss: 1.90333776e-06
Iter: 1097 loss: 1.9056132e-06
Iter: 1098 loss: 1.90129117e-06
Iter: 1099 loss: 1.89761681e-06
Iter: 1100 loss: 1.91445179e-06
Iter: 1101 loss: 1.89699347e-06
Iter: 1102 loss: 1.8934328e-06
Iter: 1103 loss: 1.89965829e-06
Iter: 1104 loss: 1.891848e-06
Iter: 1105 loss: 1.88792058e-06
Iter: 1106 loss: 1.89459297e-06
Iter: 1107 loss: 1.88624563e-06
Iter: 1108 loss: 1.883717e-06
Iter: 1109 loss: 1.92207494e-06
Iter: 1110 loss: 1.88370302e-06
Iter: 1111 loss: 1.88127501e-06
Iter: 1112 loss: 1.88051354e-06
Iter: 1113 loss: 1.87907335e-06
Iter: 1114 loss: 1.87613114e-06
Iter: 1115 loss: 1.89089496e-06
Iter: 1116 loss: 1.87555804e-06
Iter: 1117 loss: 1.87214914e-06
Iter: 1118 loss: 1.87147623e-06
Iter: 1119 loss: 1.86920533e-06
Iter: 1120 loss: 1.86568457e-06
Iter: 1121 loss: 1.87575517e-06
Iter: 1122 loss: 1.86456487e-06
Iter: 1123 loss: 1.86093871e-06
Iter: 1124 loss: 1.8714702e-06
Iter: 1125 loss: 1.85977751e-06
Iter: 1126 loss: 1.85619956e-06
Iter: 1127 loss: 1.87346632e-06
Iter: 1128 loss: 1.85552892e-06
Iter: 1129 loss: 1.85235353e-06
Iter: 1130 loss: 1.85116596e-06
Iter: 1131 loss: 1.84946555e-06
Iter: 1132 loss: 1.84524674e-06
Iter: 1133 loss: 1.8625451e-06
Iter: 1134 loss: 1.84440523e-06
Iter: 1135 loss: 1.84036344e-06
Iter: 1136 loss: 1.87478327e-06
Iter: 1137 loss: 1.84007956e-06
Iter: 1138 loss: 1.83766679e-06
Iter: 1139 loss: 1.83697148e-06
Iter: 1140 loss: 1.8353951e-06
Iter: 1141 loss: 1.83168447e-06
Iter: 1142 loss: 1.84100099e-06
Iter: 1143 loss: 1.83029579e-06
Iter: 1144 loss: 1.82697875e-06
Iter: 1145 loss: 1.85737599e-06
Iter: 1146 loss: 1.82676695e-06
Iter: 1147 loss: 1.82399208e-06
Iter: 1148 loss: 1.82825033e-06
Iter: 1149 loss: 1.82262215e-06
Iter: 1150 loss: 1.81924975e-06
Iter: 1151 loss: 1.83640043e-06
Iter: 1152 loss: 1.81871633e-06
Iter: 1153 loss: 1.81636017e-06
Iter: 1154 loss: 1.81550672e-06
Iter: 1155 loss: 1.81414703e-06
Iter: 1156 loss: 1.81063945e-06
Iter: 1157 loss: 1.82923861e-06
Iter: 1158 loss: 1.81012115e-06
Iter: 1159 loss: 1.80662755e-06
Iter: 1160 loss: 1.80989423e-06
Iter: 1161 loss: 1.80467737e-06
Iter: 1162 loss: 1.80145457e-06
Iter: 1163 loss: 1.79913309e-06
Iter: 1164 loss: 1.79798792e-06
Iter: 1165 loss: 1.79401945e-06
Iter: 1166 loss: 1.85776071e-06
Iter: 1167 loss: 1.79401786e-06
Iter: 1168 loss: 1.79118342e-06
Iter: 1169 loss: 1.79742915e-06
Iter: 1170 loss: 1.79011e-06
Iter: 1171 loss: 1.78677101e-06
Iter: 1172 loss: 1.79013352e-06
Iter: 1173 loss: 1.78488028e-06
Iter: 1174 loss: 1.78139771e-06
Iter: 1175 loss: 1.79138067e-06
Iter: 1176 loss: 1.78026e-06
Iter: 1177 loss: 1.77748848e-06
Iter: 1178 loss: 1.80748521e-06
Iter: 1179 loss: 1.77740776e-06
Iter: 1180 loss: 1.7749303e-06
Iter: 1181 loss: 1.7712498e-06
Iter: 1182 loss: 1.77114134e-06
Iter: 1183 loss: 1.76735034e-06
Iter: 1184 loss: 1.80537143e-06
Iter: 1185 loss: 1.76723711e-06
Iter: 1186 loss: 1.76443723e-06
Iter: 1187 loss: 1.78925677e-06
Iter: 1188 loss: 1.76432809e-06
Iter: 1189 loss: 1.76224466e-06
Iter: 1190 loss: 1.76017818e-06
Iter: 1191 loss: 1.75976288e-06
Iter: 1192 loss: 1.75714399e-06
Iter: 1193 loss: 1.78357413e-06
Iter: 1194 loss: 1.75706441e-06
Iter: 1195 loss: 1.75466607e-06
Iter: 1196 loss: 1.75393802e-06
Iter: 1197 loss: 1.75248442e-06
Iter: 1198 loss: 1.74951379e-06
Iter: 1199 loss: 1.76239053e-06
Iter: 1200 loss: 1.74890101e-06
Iter: 1201 loss: 1.74560978e-06
Iter: 1202 loss: 1.75048308e-06
Iter: 1203 loss: 1.74405e-06
Iter: 1204 loss: 1.74096533e-06
Iter: 1205 loss: 1.74412526e-06
Iter: 1206 loss: 1.73930857e-06
Iter: 1207 loss: 1.73571652e-06
Iter: 1208 loss: 1.75060973e-06
Iter: 1209 loss: 1.73508124e-06
Iter: 1210 loss: 1.73225089e-06
Iter: 1211 loss: 1.7479332e-06
Iter: 1212 loss: 1.73177432e-06
Iter: 1213 loss: 1.72899081e-06
Iter: 1214 loss: 1.73379374e-06
Iter: 1215 loss: 1.72785496e-06
Iter: 1216 loss: 1.72496357e-06
Iter: 1217 loss: 1.72765363e-06
Iter: 1218 loss: 1.72336559e-06
Iter: 1219 loss: 1.72012324e-06
Iter: 1220 loss: 1.74835213e-06
Iter: 1221 loss: 1.7200266e-06
Iter: 1222 loss: 1.71745e-06
Iter: 1223 loss: 1.71865747e-06
Iter: 1224 loss: 1.7158161e-06
Iter: 1225 loss: 1.71320551e-06
Iter: 1226 loss: 1.74483694e-06
Iter: 1227 loss: 1.71317288e-06
Iter: 1228 loss: 1.71103875e-06
Iter: 1229 loss: 1.70912938e-06
Iter: 1230 loss: 1.7086079e-06
Iter: 1231 loss: 1.70564363e-06
Iter: 1232 loss: 1.70849285e-06
Iter: 1233 loss: 1.70395197e-06
Iter: 1234 loss: 1.70057831e-06
Iter: 1235 loss: 1.73616343e-06
Iter: 1236 loss: 1.70048725e-06
Iter: 1237 loss: 1.69816212e-06
Iter: 1238 loss: 1.69994576e-06
Iter: 1239 loss: 1.69671853e-06
Iter: 1240 loss: 1.69411396e-06
Iter: 1241 loss: 1.69832424e-06
Iter: 1242 loss: 1.69290911e-06
Iter: 1243 loss: 1.69001783e-06
Iter: 1244 loss: 1.69873783e-06
Iter: 1245 loss: 1.68911436e-06
Iter: 1246 loss: 1.68571489e-06
Iter: 1247 loss: 1.69596399e-06
Iter: 1248 loss: 1.68475992e-06
Iter: 1249 loss: 1.68213455e-06
Iter: 1250 loss: 1.68167742e-06
Iter: 1251 loss: 1.67988424e-06
Iter: 1252 loss: 1.67693861e-06
Iter: 1253 loss: 1.7236473e-06
Iter: 1254 loss: 1.67693736e-06
Iter: 1255 loss: 1.67471148e-06
Iter: 1256 loss: 1.67707253e-06
Iter: 1257 loss: 1.67352596e-06
Iter: 1258 loss: 1.6708467e-06
Iter: 1259 loss: 1.67357132e-06
Iter: 1260 loss: 1.66945654e-06
Iter: 1261 loss: 1.66692814e-06
Iter: 1262 loss: 1.70359806e-06
Iter: 1263 loss: 1.66691643e-06
Iter: 1264 loss: 1.66511688e-06
Iter: 1265 loss: 1.66586256e-06
Iter: 1266 loss: 1.66387485e-06
Iter: 1267 loss: 1.66161817e-06
Iter: 1268 loss: 1.66979919e-06
Iter: 1269 loss: 1.66104667e-06
Iter: 1270 loss: 1.6589114e-06
Iter: 1271 loss: 1.65699453e-06
Iter: 1272 loss: 1.65640915e-06
Iter: 1273 loss: 1.65319796e-06
Iter: 1274 loss: 1.66897644e-06
Iter: 1275 loss: 1.65260167e-06
Iter: 1276 loss: 1.64944277e-06
Iter: 1277 loss: 1.66902055e-06
Iter: 1278 loss: 1.64904475e-06
Iter: 1279 loss: 1.64680318e-06
Iter: 1280 loss: 1.64586936e-06
Iter: 1281 loss: 1.6447301e-06
Iter: 1282 loss: 1.64157404e-06
Iter: 1283 loss: 1.64671405e-06
Iter: 1284 loss: 1.64013659e-06
Iter: 1285 loss: 1.63669017e-06
Iter: 1286 loss: 1.66059226e-06
Iter: 1287 loss: 1.63636514e-06
Iter: 1288 loss: 1.63355708e-06
Iter: 1289 loss: 1.6416011e-06
Iter: 1290 loss: 1.63272171e-06
Iter: 1291 loss: 1.62985111e-06
Iter: 1292 loss: 1.63313109e-06
Iter: 1293 loss: 1.62834976e-06
Iter: 1294 loss: 1.62550396e-06
Iter: 1295 loss: 1.6422465e-06
Iter: 1296 loss: 1.62510423e-06
Iter: 1297 loss: 1.62227502e-06
Iter: 1298 loss: 1.62909828e-06
Iter: 1299 loss: 1.62123729e-06
Iter: 1300 loss: 1.61916273e-06
Iter: 1301 loss: 1.63463085e-06
Iter: 1302 loss: 1.61899686e-06
Iter: 1303 loss: 1.61701053e-06
Iter: 1304 loss: 1.61749347e-06
Iter: 1305 loss: 1.61553612e-06
Iter: 1306 loss: 1.61320577e-06
Iter: 1307 loss: 1.61500441e-06
Iter: 1308 loss: 1.6118338e-06
Iter: 1309 loss: 1.60916943e-06
Iter: 1310 loss: 1.62817469e-06
Iter: 1311 loss: 1.60894399e-06
Iter: 1312 loss: 1.60672676e-06
Iter: 1313 loss: 1.60619322e-06
Iter: 1314 loss: 1.60485081e-06
Iter: 1315 loss: 1.60213108e-06
Iter: 1316 loss: 1.62165e-06
Iter: 1317 loss: 1.601858e-06
Iter: 1318 loss: 1.59924286e-06
Iter: 1319 loss: 1.60358456e-06
Iter: 1320 loss: 1.59809e-06
Iter: 1321 loss: 1.59554281e-06
Iter: 1322 loss: 1.59904471e-06
Iter: 1323 loss: 1.5942444e-06
Iter: 1324 loss: 1.59135391e-06
Iter: 1325 loss: 1.5912857e-06
Iter: 1326 loss: 1.58896944e-06
Iter: 1327 loss: 1.58571879e-06
Iter: 1328 loss: 1.60919888e-06
Iter: 1329 loss: 1.58541297e-06
Iter: 1330 loss: 1.58258172e-06
Iter: 1331 loss: 1.60072352e-06
Iter: 1332 loss: 1.58225509e-06
Iter: 1333 loss: 1.57994714e-06
Iter: 1334 loss: 1.58468288e-06
Iter: 1335 loss: 1.57896079e-06
Iter: 1336 loss: 1.57663112e-06
Iter: 1337 loss: 1.58635441e-06
Iter: 1338 loss: 1.57610953e-06
Iter: 1339 loss: 1.57353952e-06
Iter: 1340 loss: 1.57859722e-06
Iter: 1341 loss: 1.57238753e-06
Iter: 1342 loss: 1.5700391e-06
Iter: 1343 loss: 1.57485772e-06
Iter: 1344 loss: 1.56913768e-06
Iter: 1345 loss: 1.56627266e-06
Iter: 1346 loss: 1.56999795e-06
Iter: 1347 loss: 1.56480928e-06
Iter: 1348 loss: 1.56246097e-06
Iter: 1349 loss: 1.56366127e-06
Iter: 1350 loss: 1.56086662e-06
Iter: 1351 loss: 1.55797784e-06
Iter: 1352 loss: 1.58730302e-06
Iter: 1353 loss: 1.55790883e-06
Iter: 1354 loss: 1.55567921e-06
Iter: 1355 loss: 1.55893485e-06
Iter: 1356 loss: 1.55465534e-06
Iter: 1357 loss: 1.55222301e-06
Iter: 1358 loss: 1.55623661e-06
Iter: 1359 loss: 1.55116516e-06
Iter: 1360 loss: 1.54843462e-06
Iter: 1361 loss: 1.55873954e-06
Iter: 1362 loss: 1.54775375e-06
Iter: 1363 loss: 1.54512054e-06
Iter: 1364 loss: 1.54949612e-06
Iter: 1365 loss: 1.54398151e-06
Iter: 1366 loss: 1.54138593e-06
Iter: 1367 loss: 1.5381313e-06
Iter: 1368 loss: 1.53795781e-06
Iter: 1369 loss: 1.53452584e-06
Iter: 1370 loss: 1.5819586e-06
Iter: 1371 loss: 1.53450242e-06
Iter: 1372 loss: 1.53231167e-06
Iter: 1373 loss: 1.56053261e-06
Iter: 1374 loss: 1.53234794e-06
Iter: 1375 loss: 1.53059909e-06
Iter: 1376 loss: 1.5307819e-06
Iter: 1377 loss: 1.52926168e-06
Iter: 1378 loss: 1.52671828e-06
Iter: 1379 loss: 1.53721214e-06
Iter: 1380 loss: 1.52610369e-06
Iter: 1381 loss: 1.52397081e-06
Iter: 1382 loss: 1.52644043e-06
Iter: 1383 loss: 1.52283792e-06
Iter: 1384 loss: 1.52064138e-06
Iter: 1385 loss: 1.52154712e-06
Iter: 1386 loss: 1.51900804e-06
Iter: 1387 loss: 1.51596282e-06
Iter: 1388 loss: 1.53335895e-06
Iter: 1389 loss: 1.51550876e-06
Iter: 1390 loss: 1.5126393e-06
Iter: 1391 loss: 1.51523386e-06
Iter: 1392 loss: 1.51096447e-06
Iter: 1393 loss: 1.50849485e-06
Iter: 1394 loss: 1.52465122e-06
Iter: 1395 loss: 1.50813457e-06
Iter: 1396 loss: 1.50574579e-06
Iter: 1397 loss: 1.51251857e-06
Iter: 1398 loss: 1.50496908e-06
Iter: 1399 loss: 1.5029226e-06
Iter: 1400 loss: 1.50261803e-06
Iter: 1401 loss: 1.50118217e-06
Iter: 1402 loss: 1.49865366e-06
Iter: 1403 loss: 1.51696679e-06
Iter: 1404 loss: 1.49845414e-06
Iter: 1405 loss: 1.49595814e-06
Iter: 1406 loss: 1.49757295e-06
Iter: 1407 loss: 1.49436812e-06
Iter: 1408 loss: 1.49170296e-06
Iter: 1409 loss: 1.50018423e-06
Iter: 1410 loss: 1.49077539e-06
Iter: 1411 loss: 1.48851814e-06
Iter: 1412 loss: 1.50281096e-06
Iter: 1413 loss: 1.48825188e-06
Iter: 1414 loss: 1.48609513e-06
Iter: 1415 loss: 1.49971675e-06
Iter: 1416 loss: 1.48585013e-06
Iter: 1417 loss: 1.48454058e-06
Iter: 1418 loss: 1.48281424e-06
Iter: 1419 loss: 1.48275649e-06
Iter: 1420 loss: 1.4800421e-06
Iter: 1421 loss: 1.4977004e-06
Iter: 1422 loss: 1.47968444e-06
Iter: 1423 loss: 1.47787057e-06
Iter: 1424 loss: 1.47823584e-06
Iter: 1425 loss: 1.47655714e-06
Iter: 1426 loss: 1.47411538e-06
Iter: 1427 loss: 1.47751621e-06
Iter: 1428 loss: 1.47287346e-06
Iter: 1429 loss: 1.47024161e-06
Iter: 1430 loss: 1.49085781e-06
Iter: 1431 loss: 1.47009109e-06
Iter: 1432 loss: 1.46784885e-06
Iter: 1433 loss: 1.47262165e-06
Iter: 1434 loss: 1.46701154e-06
Iter: 1435 loss: 1.46492152e-06
Iter: 1436 loss: 1.46665639e-06
Iter: 1437 loss: 1.46355444e-06
Iter: 1438 loss: 1.46095726e-06
Iter: 1439 loss: 1.47948822e-06
Iter: 1440 loss: 1.4606436e-06
Iter: 1441 loss: 1.4589624e-06
Iter: 1442 loss: 1.4567014e-06
Iter: 1443 loss: 1.45654212e-06
Iter: 1444 loss: 1.4531937e-06
Iter: 1445 loss: 1.47178639e-06
Iter: 1446 loss: 1.45275271e-06
Iter: 1447 loss: 1.45027798e-06
Iter: 1448 loss: 1.4731861e-06
Iter: 1449 loss: 1.45022022e-06
Iter: 1450 loss: 1.44849446e-06
Iter: 1451 loss: 1.45012859e-06
Iter: 1452 loss: 1.44746241e-06
Iter: 1453 loss: 1.44487103e-06
Iter: 1454 loss: 1.45676108e-06
Iter: 1455 loss: 1.44435376e-06
Iter: 1456 loss: 1.44258183e-06
Iter: 1457 loss: 1.44193768e-06
Iter: 1458 loss: 1.44093474e-06
Iter: 1459 loss: 1.43858233e-06
Iter: 1460 loss: 1.45102581e-06
Iter: 1461 loss: 1.43821057e-06
Iter: 1462 loss: 1.43606303e-06
Iter: 1463 loss: 1.44260912e-06
Iter: 1464 loss: 1.43543093e-06
Iter: 1465 loss: 1.43314765e-06
Iter: 1466 loss: 1.43317197e-06
Iter: 1467 loss: 1.43139619e-06
Iter: 1468 loss: 1.42896408e-06
Iter: 1469 loss: 1.43425859e-06
Iter: 1470 loss: 1.42802241e-06
Iter: 1471 loss: 1.4256243e-06
Iter: 1472 loss: 1.45508307e-06
Iter: 1473 loss: 1.42564818e-06
Iter: 1474 loss: 1.42379622e-06
Iter: 1475 loss: 1.42262411e-06
Iter: 1476 loss: 1.42195154e-06
Iter: 1477 loss: 1.41947521e-06
Iter: 1478 loss: 1.42920896e-06
Iter: 1479 loss: 1.418897e-06
Iter: 1480 loss: 1.41625401e-06
Iter: 1481 loss: 1.42428735e-06
Iter: 1482 loss: 1.41545434e-06
Iter: 1483 loss: 1.41305622e-06
Iter: 1484 loss: 1.42009185e-06
Iter: 1485 loss: 1.41236433e-06
Iter: 1486 loss: 1.41041573e-06
Iter: 1487 loss: 1.41467092e-06
Iter: 1488 loss: 1.40974419e-06
Iter: 1489 loss: 1.40757561e-06
Iter: 1490 loss: 1.42756016e-06
Iter: 1491 loss: 1.40750967e-06
Iter: 1492 loss: 1.40593681e-06
Iter: 1493 loss: 1.40446446e-06
Iter: 1494 loss: 1.40410884e-06
Iter: 1495 loss: 1.40180964e-06
Iter: 1496 loss: 1.41413625e-06
Iter: 1497 loss: 1.40144471e-06
Iter: 1498 loss: 1.399241e-06
Iter: 1499 loss: 1.40140219e-06
Iter: 1500 loss: 1.3979917e-06
Iter: 1501 loss: 1.39588269e-06
Iter: 1502 loss: 1.40571285e-06
Iter: 1503 loss: 1.39549149e-06
Iter: 1504 loss: 1.39356234e-06
Iter: 1505 loss: 1.40043244e-06
Iter: 1506 loss: 1.39299118e-06
Iter: 1507 loss: 1.39097665e-06
Iter: 1508 loss: 1.39024212e-06
Iter: 1509 loss: 1.38904738e-06
Iter: 1510 loss: 1.3865772e-06
Iter: 1511 loss: 1.39792701e-06
Iter: 1512 loss: 1.386142e-06
Iter: 1513 loss: 1.38408564e-06
Iter: 1514 loss: 1.40272971e-06
Iter: 1515 loss: 1.38393557e-06
Iter: 1516 loss: 1.38207338e-06
Iter: 1517 loss: 1.37930147e-06
Iter: 1518 loss: 1.37921393e-06
Iter: 1519 loss: 1.37662232e-06
Iter: 1520 loss: 1.3911764e-06
Iter: 1521 loss: 1.37621896e-06
Iter: 1522 loss: 1.37391135e-06
Iter: 1523 loss: 1.38749579e-06
Iter: 1524 loss: 1.37362781e-06
Iter: 1525 loss: 1.37168058e-06
Iter: 1526 loss: 1.37829761e-06
Iter: 1527 loss: 1.37116911e-06
Iter: 1528 loss: 1.36917924e-06
Iter: 1529 loss: 1.37626341e-06
Iter: 1530 loss: 1.36871495e-06
Iter: 1531 loss: 1.36671883e-06
Iter: 1532 loss: 1.36797496e-06
Iter: 1533 loss: 1.36545589e-06
Iter: 1534 loss: 1.3635896e-06
Iter: 1535 loss: 1.36447488e-06
Iter: 1536 loss: 1.36235781e-06
Iter: 1537 loss: 1.36001859e-06
Iter: 1538 loss: 1.37232189e-06
Iter: 1539 loss: 1.35963137e-06
Iter: 1540 loss: 1.35717903e-06
Iter: 1541 loss: 1.36423796e-06
Iter: 1542 loss: 1.35644916e-06
Iter: 1543 loss: 1.35456207e-06
Iter: 1544 loss: 1.35974983e-06
Iter: 1545 loss: 1.35390542e-06
Iter: 1546 loss: 1.35213054e-06
Iter: 1547 loss: 1.35645269e-06
Iter: 1548 loss: 1.35153516e-06
Iter: 1549 loss: 1.34915945e-06
Iter: 1550 loss: 1.35095979e-06
Iter: 1551 loss: 1.3476963e-06
Iter: 1552 loss: 1.34558968e-06
Iter: 1553 loss: 1.35441951e-06
Iter: 1554 loss: 1.34514994e-06
Iter: 1555 loss: 1.34288598e-06
Iter: 1556 loss: 1.35329674e-06
Iter: 1557 loss: 1.34251411e-06
Iter: 1558 loss: 1.34057882e-06
Iter: 1559 loss: 1.33956951e-06
Iter: 1560 loss: 1.33868571e-06
Iter: 1561 loss: 1.33621313e-06
Iter: 1562 loss: 1.34925222e-06
Iter: 1563 loss: 1.33582705e-06
Iter: 1564 loss: 1.33423737e-06
Iter: 1565 loss: 1.33423828e-06
Iter: 1566 loss: 1.33284129e-06
Iter: 1567 loss: 1.33089611e-06
Iter: 1568 loss: 1.33086189e-06
Iter: 1569 loss: 1.32845889e-06
Iter: 1570 loss: 1.34061179e-06
Iter: 1571 loss: 1.32807872e-06
Iter: 1572 loss: 1.3259546e-06
Iter: 1573 loss: 1.3298793e-06
Iter: 1574 loss: 1.32500031e-06
Iter: 1575 loss: 1.32285982e-06
Iter: 1576 loss: 1.32473929e-06
Iter: 1577 loss: 1.32164291e-06
Iter: 1578 loss: 1.31945171e-06
Iter: 1579 loss: 1.3360301e-06
Iter: 1580 loss: 1.31929721e-06
Iter: 1581 loss: 1.317336e-06
Iter: 1582 loss: 1.32261141e-06
Iter: 1583 loss: 1.31662796e-06
Iter: 1584 loss: 1.31491561e-06
Iter: 1585 loss: 1.31412071e-06
Iter: 1586 loss: 1.31330876e-06
Iter: 1587 loss: 1.31120441e-06
Iter: 1588 loss: 1.33516642e-06
Iter: 1589 loss: 1.31116e-06
Iter: 1590 loss: 1.30925218e-06
Iter: 1591 loss: 1.3110855e-06
Iter: 1592 loss: 1.3082838e-06
Iter: 1593 loss: 1.30620288e-06
Iter: 1594 loss: 1.31269758e-06
Iter: 1595 loss: 1.30560181e-06
Iter: 1596 loss: 1.30356682e-06
Iter: 1597 loss: 1.31015167e-06
Iter: 1598 loss: 1.30298622e-06
Iter: 1599 loss: 1.30106196e-06
Iter: 1600 loss: 1.30395847e-06
Iter: 1601 loss: 1.30006538e-06
Iter: 1602 loss: 1.29845807e-06
Iter: 1603 loss: 1.31918955e-06
Iter: 1604 loss: 1.29842635e-06
Iter: 1605 loss: 1.29691557e-06
Iter: 1606 loss: 1.29551768e-06
Iter: 1607 loss: 1.29509044e-06
Iter: 1608 loss: 1.29319153e-06
Iter: 1609 loss: 1.29535192e-06
Iter: 1610 loss: 1.29210753e-06
Iter: 1611 loss: 1.28974943e-06
Iter: 1612 loss: 1.30329192e-06
Iter: 1613 loss: 1.28950819e-06
Iter: 1614 loss: 1.28757051e-06
Iter: 1615 loss: 1.29446494e-06
Iter: 1616 loss: 1.28711417e-06
Iter: 1617 loss: 1.28516137e-06
Iter: 1618 loss: 1.28508805e-06
Iter: 1619 loss: 1.28351826e-06
Iter: 1620 loss: 1.28164697e-06
Iter: 1621 loss: 1.3043184e-06
Iter: 1622 loss: 1.28163401e-06
Iter: 1623 loss: 1.27980843e-06
Iter: 1624 loss: 1.27893782e-06
Iter: 1625 loss: 1.27810108e-06
Iter: 1626 loss: 1.27565136e-06
Iter: 1627 loss: 1.2811953e-06
Iter: 1628 loss: 1.27477642e-06
Iter: 1629 loss: 1.27287615e-06
Iter: 1630 loss: 1.29349633e-06
Iter: 1631 loss: 1.27283533e-06
Iter: 1632 loss: 1.2712087e-06
Iter: 1633 loss: 1.2730751e-06
Iter: 1634 loss: 1.27028193e-06
Iter: 1635 loss: 1.26865143e-06
Iter: 1636 loss: 1.26987095e-06
Iter: 1637 loss: 1.26761006e-06
Iter: 1638 loss: 1.26581574e-06
Iter: 1639 loss: 1.28859119e-06
Iter: 1640 loss: 1.265788e-06
Iter: 1641 loss: 1.26440079e-06
Iter: 1642 loss: 1.2661385e-06
Iter: 1643 loss: 1.26371265e-06
Iter: 1644 loss: 1.26197517e-06
Iter: 1645 loss: 1.26424914e-06
Iter: 1646 loss: 1.26113844e-06
Iter: 1647 loss: 1.25936469e-06
Iter: 1648 loss: 1.26070825e-06
Iter: 1649 loss: 1.2583547e-06
Iter: 1650 loss: 1.25606743e-06
Iter: 1651 loss: 1.25888528e-06
Iter: 1652 loss: 1.25477072e-06
Iter: 1653 loss: 1.25270776e-06
Iter: 1654 loss: 1.2662174e-06
Iter: 1655 loss: 1.25252586e-06
Iter: 1656 loss: 1.2504828e-06
Iter: 1657 loss: 1.25895451e-06
Iter: 1658 loss: 1.25001623e-06
Iter: 1659 loss: 1.24844667e-06
Iter: 1660 loss: 1.24798123e-06
Iter: 1661 loss: 1.2470648e-06
Iter: 1662 loss: 1.24509256e-06
Iter: 1663 loss: 1.27007524e-06
Iter: 1664 loss: 1.24508495e-06
Iter: 1665 loss: 1.24375936e-06
Iter: 1666 loss: 1.24331063e-06
Iter: 1667 loss: 1.24250323e-06
Iter: 1668 loss: 1.24057442e-06
Iter: 1669 loss: 1.2463438e-06
Iter: 1670 loss: 1.2399903e-06
Iter: 1671 loss: 1.23807229e-06
Iter: 1672 loss: 1.25177053e-06
Iter: 1673 loss: 1.23791142e-06
Iter: 1674 loss: 1.23635346e-06
Iter: 1675 loss: 1.23653217e-06
Iter: 1676 loss: 1.23507027e-06
Iter: 1677 loss: 1.23334144e-06
Iter: 1678 loss: 1.24891233e-06
Iter: 1679 loss: 1.2332132e-06
Iter: 1680 loss: 1.23157702e-06
Iter: 1681 loss: 1.23467544e-06
Iter: 1682 loss: 1.2308758e-06
Iter: 1683 loss: 1.22943993e-06
Iter: 1684 loss: 1.22780318e-06
Iter: 1685 loss: 1.22752613e-06
Iter: 1686 loss: 1.22548931e-06
Iter: 1687 loss: 1.24706537e-06
Iter: 1688 loss: 1.22543884e-06
Iter: 1689 loss: 1.22374945e-06
Iter: 1690 loss: 1.2255673e-06
Iter: 1691 loss: 1.22284473e-06
Iter: 1692 loss: 1.22071469e-06
Iter: 1693 loss: 1.22216261e-06
Iter: 1694 loss: 1.21945857e-06
Iter: 1695 loss: 1.21761741e-06
Iter: 1696 loss: 1.24207281e-06
Iter: 1697 loss: 1.21759581e-06
Iter: 1698 loss: 1.21613289e-06
Iter: 1699 loss: 1.21880851e-06
Iter: 1700 loss: 1.21554467e-06
Iter: 1701 loss: 1.21396124e-06
Iter: 1702 loss: 1.21320329e-06
Iter: 1703 loss: 1.21250673e-06
Iter: 1704 loss: 1.210655e-06
Iter: 1705 loss: 1.21064556e-06
Iter: 1706 loss: 1.20931577e-06
Iter: 1707 loss: 1.20800803e-06
Iter: 1708 loss: 1.2076523e-06
Iter: 1709 loss: 1.20593472e-06
Iter: 1710 loss: 1.2264336e-06
Iter: 1711 loss: 1.20596155e-06
Iter: 1712 loss: 1.20452091e-06
Iter: 1713 loss: 1.20746813e-06
Iter: 1714 loss: 1.20402149e-06
Iter: 1715 loss: 1.2024484e-06
Iter: 1716 loss: 1.20675634e-06
Iter: 1717 loss: 1.2019932e-06
Iter: 1718 loss: 1.20041238e-06
Iter: 1719 loss: 1.20546042e-06
Iter: 1720 loss: 1.19998811e-06
Iter: 1721 loss: 1.19852939e-06
Iter: 1722 loss: 1.19648428e-06
Iter: 1723 loss: 1.19645642e-06
Iter: 1724 loss: 1.19405388e-06
Iter: 1725 loss: 1.20747e-06
Iter: 1726 loss: 1.19375613e-06
Iter: 1727 loss: 1.19193885e-06
Iter: 1728 loss: 1.19987158e-06
Iter: 1729 loss: 1.1916195e-06
Iter: 1730 loss: 1.18986668e-06
Iter: 1731 loss: 1.19612878e-06
Iter: 1732 loss: 1.18940716e-06
Iter: 1733 loss: 1.18779019e-06
Iter: 1734 loss: 1.18854507e-06
Iter: 1735 loss: 1.18660648e-06
Iter: 1736 loss: 1.18464345e-06
Iter: 1737 loss: 1.19760091e-06
Iter: 1738 loss: 1.18441858e-06
Iter: 1739 loss: 1.18266757e-06
Iter: 1740 loss: 1.18778098e-06
Iter: 1741 loss: 1.18212836e-06
Iter: 1742 loss: 1.18062849e-06
Iter: 1743 loss: 1.17997729e-06
Iter: 1744 loss: 1.17919217e-06
Iter: 1745 loss: 1.17716922e-06
Iter: 1746 loss: 1.19572269e-06
Iter: 1747 loss: 1.1770428e-06
Iter: 1748 loss: 1.17534307e-06
Iter: 1749 loss: 1.17968e-06
Iter: 1750 loss: 1.17470017e-06
Iter: 1751 loss: 1.17313425e-06
Iter: 1752 loss: 1.17895092e-06
Iter: 1753 loss: 1.1727808e-06
Iter: 1754 loss: 1.17108948e-06
Iter: 1755 loss: 1.17574564e-06
Iter: 1756 loss: 1.17056265e-06
Iter: 1757 loss: 1.16917386e-06
Iter: 1758 loss: 1.17162199e-06
Iter: 1759 loss: 1.1685911e-06
Iter: 1760 loss: 1.16698516e-06
Iter: 1761 loss: 1.17007949e-06
Iter: 1762 loss: 1.1663094e-06
Iter: 1763 loss: 1.16454453e-06
Iter: 1764 loss: 1.16581214e-06
Iter: 1765 loss: 1.16348951e-06
Iter: 1766 loss: 1.16145179e-06
Iter: 1767 loss: 1.1639471e-06
Iter: 1768 loss: 1.16044828e-06
Iter: 1769 loss: 1.15822888e-06
Iter: 1770 loss: 1.16261435e-06
Iter: 1771 loss: 1.15737737e-06
Iter: 1772 loss: 1.15541911e-06
Iter: 1773 loss: 1.18520938e-06
Iter: 1774 loss: 1.15543321e-06
Iter: 1775 loss: 1.15393073e-06
Iter: 1776 loss: 1.15492037e-06
Iter: 1777 loss: 1.15302714e-06
Iter: 1778 loss: 1.1513506e-06
Iter: 1779 loss: 1.15755529e-06
Iter: 1780 loss: 1.15093871e-06
Iter: 1781 loss: 1.14942782e-06
Iter: 1782 loss: 1.1553891e-06
Iter: 1783 loss: 1.1490547e-06
Iter: 1784 loss: 1.1475455e-06
Iter: 1785 loss: 1.14708837e-06
Iter: 1786 loss: 1.1462123e-06
Iter: 1787 loss: 1.14490422e-06
Iter: 1788 loss: 1.14489239e-06
Iter: 1789 loss: 1.14374075e-06
Iter: 1790 loss: 1.14438376e-06
Iter: 1791 loss: 1.1430302e-06
Iter: 1792 loss: 1.1416513e-06
Iter: 1793 loss: 1.14632019e-06
Iter: 1794 loss: 1.14123645e-06
Iter: 1795 loss: 1.13985288e-06
Iter: 1796 loss: 1.14157251e-06
Iter: 1797 loss: 1.13915121e-06
Iter: 1798 loss: 1.13760552e-06
Iter: 1799 loss: 1.13785e-06
Iter: 1800 loss: 1.13645274e-06
Iter: 1801 loss: 1.13439546e-06
Iter: 1802 loss: 1.14830891e-06
Iter: 1803 loss: 1.13421549e-06
Iter: 1804 loss: 1.13258147e-06
Iter: 1805 loss: 1.13510669e-06
Iter: 1806 loss: 1.13185024e-06
Iter: 1807 loss: 1.13017904e-06
Iter: 1808 loss: 1.1314404e-06
Iter: 1809 loss: 1.12913403e-06
Iter: 1810 loss: 1.12699047e-06
Iter: 1811 loss: 1.13034912e-06
Iter: 1812 loss: 1.12604516e-06
Iter: 1813 loss: 1.1243385e-06
Iter: 1814 loss: 1.12437067e-06
Iter: 1815 loss: 1.12292389e-06
Iter: 1816 loss: 1.12161592e-06
Iter: 1817 loss: 1.12124678e-06
Iter: 1818 loss: 1.11956604e-06
Iter: 1819 loss: 1.1322029e-06
Iter: 1820 loss: 1.11947679e-06
Iter: 1821 loss: 1.11781083e-06
Iter: 1822 loss: 1.12115094e-06
Iter: 1823 loss: 1.11712131e-06
Iter: 1824 loss: 1.11562258e-06
Iter: 1825 loss: 1.12507564e-06
Iter: 1826 loss: 1.1154782e-06
Iter: 1827 loss: 1.11414829e-06
Iter: 1828 loss: 1.11725808e-06
Iter: 1829 loss: 1.1136467e-06
Iter: 1830 loss: 1.11224404e-06
Iter: 1831 loss: 1.11199324e-06
Iter: 1832 loss: 1.11103839e-06
Iter: 1833 loss: 1.10935127e-06
Iter: 1834 loss: 1.12271664e-06
Iter: 1835 loss: 1.10922917e-06
Iter: 1836 loss: 1.10775557e-06
Iter: 1837 loss: 1.1077417e-06
Iter: 1838 loss: 1.10662347e-06
Iter: 1839 loss: 1.10480482e-06
Iter: 1840 loss: 1.10775136e-06
Iter: 1841 loss: 1.10399856e-06
Iter: 1842 loss: 1.10223323e-06
Iter: 1843 loss: 1.11672023e-06
Iter: 1844 loss: 1.10212409e-06
Iter: 1845 loss: 1.10063138e-06
Iter: 1846 loss: 1.1022637e-06
Iter: 1847 loss: 1.09975053e-06
Iter: 1848 loss: 1.09804762e-06
Iter: 1849 loss: 1.09754797e-06
Iter: 1850 loss: 1.09659425e-06
Iter: 1851 loss: 1.09472546e-06
Iter: 1852 loss: 1.11846657e-06
Iter: 1853 loss: 1.09472603e-06
Iter: 1854 loss: 1.09327993e-06
Iter: 1855 loss: 1.10000735e-06
Iter: 1856 loss: 1.09303448e-06
Iter: 1857 loss: 1.09179712e-06
Iter: 1858 loss: 1.09204234e-06
Iter: 1859 loss: 1.0908783e-06
Iter: 1860 loss: 1.08936467e-06
Iter: 1861 loss: 1.09750044e-06
Iter: 1862 loss: 1.0891614e-06
Iter: 1863 loss: 1.08782774e-06
Iter: 1864 loss: 1.09600956e-06
Iter: 1865 loss: 1.08766017e-06
Iter: 1866 loss: 1.08657969e-06
Iter: 1867 loss: 1.08573647e-06
Iter: 1868 loss: 1.08538848e-06
Iter: 1869 loss: 1.08361905e-06
Iter: 1870 loss: 1.09486291e-06
Iter: 1871 loss: 1.0834093e-06
Iter: 1872 loss: 1.08228164e-06
Iter: 1873 loss: 1.08232837e-06
Iter: 1874 loss: 1.0813535e-06
Iter: 1875 loss: 1.07963797e-06
Iter: 1876 loss: 1.08752795e-06
Iter: 1877 loss: 1.07937444e-06
Iter: 1878 loss: 1.07774702e-06
Iter: 1879 loss: 1.07943106e-06
Iter: 1880 loss: 1.07687981e-06
Iter: 1881 loss: 1.07526489e-06
Iter: 1882 loss: 1.0787262e-06
Iter: 1883 loss: 1.07457902e-06
Iter: 1884 loss: 1.07311985e-06
Iter: 1885 loss: 1.07895494e-06
Iter: 1886 loss: 1.07273468e-06
Iter: 1887 loss: 1.07119843e-06
Iter: 1888 loss: 1.07814253e-06
Iter: 1889 loss: 1.07080859e-06
Iter: 1890 loss: 1.06962034e-06
Iter: 1891 loss: 1.0680144e-06
Iter: 1892 loss: 1.06786729e-06
Iter: 1893 loss: 1.06572418e-06
Iter: 1894 loss: 1.09244615e-06
Iter: 1895 loss: 1.06573611e-06
Iter: 1896 loss: 1.06414484e-06
Iter: 1897 loss: 1.07102187e-06
Iter: 1898 loss: 1.06383368e-06
Iter: 1899 loss: 1.0626552e-06
Iter: 1900 loss: 1.06378684e-06
Iter: 1901 loss: 1.06195273e-06
Iter: 1902 loss: 1.06056973e-06
Iter: 1903 loss: 1.07165113e-06
Iter: 1904 loss: 1.06046218e-06
Iter: 1905 loss: 1.05924528e-06
Iter: 1906 loss: 1.05861773e-06
Iter: 1907 loss: 1.05802792e-06
Iter: 1908 loss: 1.05676156e-06
Iter: 1909 loss: 1.06258881e-06
Iter: 1910 loss: 1.0565393e-06
Iter: 1911 loss: 1.0549777e-06
Iter: 1912 loss: 1.05574816e-06
Iter: 1913 loss: 1.05397737e-06
Iter: 1914 loss: 1.05208983e-06
Iter: 1915 loss: 1.05601055e-06
Iter: 1916 loss: 1.05138452e-06
Iter: 1917 loss: 1.04981211e-06
Iter: 1918 loss: 1.06176037e-06
Iter: 1919 loss: 1.04968944e-06
Iter: 1920 loss: 1.04838489e-06
Iter: 1921 loss: 1.04917888e-06
Iter: 1922 loss: 1.04751257e-06
Iter: 1923 loss: 1.04591379e-06
Iter: 1924 loss: 1.04730248e-06
Iter: 1925 loss: 1.04487663e-06
Iter: 1926 loss: 1.04339142e-06
Iter: 1927 loss: 1.05757147e-06
Iter: 1928 loss: 1.04329865e-06
Iter: 1929 loss: 1.04202854e-06
Iter: 1930 loss: 1.0447194e-06
Iter: 1931 loss: 1.04152377e-06
Iter: 1932 loss: 1.04011951e-06
Iter: 1933 loss: 1.04047842e-06
Iter: 1934 loss: 1.03903619e-06
Iter: 1935 loss: 1.03760055e-06
Iter: 1936 loss: 1.05581739e-06
Iter: 1937 loss: 1.03756986e-06
Iter: 1938 loss: 1.03625598e-06
Iter: 1939 loss: 1.03645641e-06
Iter: 1940 loss: 1.03524587e-06
Iter: 1941 loss: 1.03374782e-06
Iter: 1942 loss: 1.04668777e-06
Iter: 1943 loss: 1.03363993e-06
Iter: 1944 loss: 1.03240473e-06
Iter: 1945 loss: 1.03355751e-06
Iter: 1946 loss: 1.03167508e-06
Iter: 1947 loss: 1.03029777e-06
Iter: 1948 loss: 1.03075092e-06
Iter: 1949 loss: 1.02929448e-06
Iter: 1950 loss: 1.0280894e-06
Iter: 1951 loss: 1.03866603e-06
Iter: 1952 loss: 1.02799879e-06
Iter: 1953 loss: 1.02669856e-06
Iter: 1954 loss: 1.02724596e-06
Iter: 1955 loss: 1.02580611e-06
Iter: 1956 loss: 1.02422337e-06
Iter: 1957 loss: 1.02541935e-06
Iter: 1958 loss: 1.02330296e-06
Iter: 1959 loss: 1.021618e-06
Iter: 1960 loss: 1.03465209e-06
Iter: 1961 loss: 1.02149806e-06
Iter: 1962 loss: 1.02000013e-06
Iter: 1963 loss: 1.02197987e-06
Iter: 1964 loss: 1.01919727e-06
Iter: 1965 loss: 1.01755654e-06
Iter: 1966 loss: 1.02015451e-06
Iter: 1967 loss: 1.01671208e-06
Iter: 1968 loss: 1.0151989e-06
Iter: 1969 loss: 1.01883074e-06
Iter: 1970 loss: 1.01464468e-06
Iter: 1971 loss: 1.01265823e-06
Iter: 1972 loss: 1.02276977e-06
Iter: 1973 loss: 1.01236026e-06
Iter: 1974 loss: 1.01120418e-06
Iter: 1975 loss: 1.01480657e-06
Iter: 1976 loss: 1.01087574e-06
Iter: 1977 loss: 1.00956333e-06
Iter: 1978 loss: 1.01511159e-06
Iter: 1979 loss: 1.00925672e-06
Iter: 1980 loss: 1.00803118e-06
Iter: 1981 loss: 1.0094335e-06
Iter: 1982 loss: 1.00734383e-06
Iter: 1983 loss: 1.00603313e-06
Iter: 1984 loss: 1.01286798e-06
Iter: 1985 loss: 1.0058551e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi1_phi3/300_300_300_1
